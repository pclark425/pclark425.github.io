<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1235 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1235</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1235</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-201070547</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1908.06132v1.pdf" target="_blank">Learning Representations and Agents for Information Retrieval</a></p>
                <p><strong>Paper Abstract:</strong> A goal shared by artificial intelligence and information retrieval is to create an oracle, that is, a machine that can answer our questions, no matter how difficult they are. A more limited, but still instrumental, version of this oracle is a question-answering system, in which an open-ended question is given to the machine, and an answer is produced based on the knowledge it has access to. Such systems already exist and are increasingly capable of answering complicated questions. This progress can be partially attributed to the recent success of machine learning and to the efficient methods for storing and retrieving information, most notably through web search engines. One can imagine that this general-purpose question-answering system can be built as a billion-parameters neural network trained end-to-end with a large number of pairs of questions and answers. We argue, however, that although this approach has been very successful for tasks such as machine translation, storing the world's knowledge as parameters of a learning machine can be very hard. A more efficient way is to train an artificial agent on how to use an external retrieval system to collect relevant information. This agent can leverage the effort that has been put into designing and running efficient storage and retrieval systems by learning how to best utilize them to accomplish a task. ...</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1235.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1235.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WikiNav / WebNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WikiNav (goal-driven web navigation) / WebNav (environment generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-world constructed from English Wikipedia where pages are nodes and hyperlinks are directed edges; an agent is given a query (sentence(s)) and must navigate from a fixed start node to a target page containing the answer, under constraints on branching, hops, and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>WikiNav (built with WebNav)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Encyclopedic web-text world built from English Wikipedia (Sep 2015). Nodes = articles (natural-language text), directed edges = intra-site hyperlinks. Tasks: start at designated root ("Category: Main topic classifications") and navigate to a node whose text contains a query sentence; queries are 1–4 sentences taken from target pages. Partially observable: agent only sees current node text and immediate neighbors' texts (a 'peek').</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Directed hyperlink graph; described as "densely-linked" for Wikipedia; connectivity is heterogeneous (some pages many links, others few); action space at a node = set of outgoing links (state-dependent). The environment enforces a maximum number of explored outgoing edges per node (parameter β) to bound local branching during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>>5 million pages (English Wikipedia dump, Sep 2015); vocabulary ≈ 370k unique words; task variants WikiNav-h-s created by varying max hops h ∈ {4,8,16} and query size s ∈ {1,2,4} (dataset sizes: WikiNav-4 small; WikiNav-8 and WikiNav-16 large; exact per-split counts reported in thesis tables).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NeuAgent (NeuAgent-FF, NeuAgent-Rec)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neural navigation agent that reads current node content and query, peeks at neighbor node representations, then outputs a distribution over outgoing edges and a stop action. Implementations: feedforward (NeuAgent-FF) and recurrent (NeuAgent-Rec with LSTMs). Uses BoW or attention-based representations for documents and queries. Trained with supervised learning on shortest-path traces then fine-tuned with Q-learning (reinforcement learning). Inference uses beam search capped by β.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Primary metrics: task reward (binary success per episode → averaged = average reward / success rate), Recall@K / MRR for retrieval-style evaluation (WikiNav-Jeopardy); secondary metrics: number of hops taken, training sample efficiency (convergence speed).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Example reported results (WikiNav-Jeopardy): average path length to targets: mean = 5.8 hops (sd = 1.2; min = 2; max = 10). NeuAgent (pretrained on WikiNav-16-4) achieved MRR = 20.3%, Recall@1 = 18.9%, Recall@5 = 24.1%, Recall@100 = 47.6% on WikiNav-Jeopardy (reported in thesis Table 2.6).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Varies by task difficulty (h and query size). Example: NeuAgent (pretrained) Recall@1 ≈ 18.9% on WikiNav-Jeopardy; human average reward reported lower (examples: human averages reported in experiments e.g., 14.5% in one setting). Supervised+RL finetuning improves success rate relative to supervised-only.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based recurrent policies (NeuAgent-Rec with LSTM and attention) with supervised pretraining followed by reinforcement learning perform best for longer-range navigation; for shallow tasks a feedforward/reactive policy suffices. Beam search and entropy regularization of action distributions improve inference.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Reported relationships: (1) Increasing allowed hops h (search horizon) makes the task harder — search space grows (approximately) exponentially with h, lowering success rates. (2) Longer/more informative queries reduce difficulty (positive correlation between query length and success). (3) Limiting local branching (max explored outgoing edges β) prevents trivial BFS and forces agent to make better local decisions; with β→∞ a breadth-first strategy would trivially succeed. (4) Recurrent agents outperform feedforward ones as h increases, indicating long-range planning / memory is required for high-diameter navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper does not compare fundamentally different graph families (e.g., trees vs. grids) but performs controlled comparisons by varying topology-relevant parameters: max hops h, max explored edges per node β, and query size s. Key findings: higher h (larger effective search radius) decreases performance; smaller β (tighter local branching) increases per-step decision difficulty; larger/more informative queries improve navigation success; pretrained agents on larger (higher-h) WikiNav datasets generalize better to retrieval tasks (WikiNav-Jeopardy).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Findings include: (1) Recurrent/memory-based policies (LSTM NeuAgent-Rec) show consistent advantage for longer-h tasks because they can use history for long-term planning. (2) Entropy regularization of the action distribution during training prevents overly peaked policies and enables effective beam search during inference. (3) Supervised training on shortest-path traces helps bootstrap policy (gives initial high-probability correct paths); RL finetuning teaches recovery from unseen states. (4) Limiting local branching (β) makes per-node decision-making quality critical, so policies must learn strong local relevance scoring using local context/peeked neighbor content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations and Agents for Information Retrieval', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1235.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1235.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeuAgent (Neural-network-based navigation agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural agent architecture for goal-driven web navigation that computes hidden state from the current page and query, peeks at neighbor page representations, and outputs probabilities over outgoing links plus a stop action; variants include feedforward and recurrent (LSTM) cores and attention-based document/query encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used on WikiNav (Wikipedia navigation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates in the WikiNav text-world: at each step reads current node text, peeks neighbor texts, selects an outgoing edge or stop action; action-space limited to outgoing hyperlinks (state-dependent action set).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Treated generically as directed hyperlink graph; NeuAgent uses local neighbor peek and learned scoring of neighbor document vectors to choose edges.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Applied at large scale (datasets built from ≫1M pages: full English Wikipedia used to construct WikiNav variants; WikiNav-16 uses pages up to 16 hops away), so state/action spaces are very large.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NeuAgent-FF / NeuAgent-Rec</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Core f implemented either as a single-layer tanh feedforward net (NeuAgent-FF) or multi-layer LSTM (NeuAgent-Rec). Document and query encoders: BoW averaging or attention-based CNN/conv-window contexts. Action logits computed by similarity between hidden state and neighbor representations; stop action parameterized separately. Training: supervised (maximize likelihood of shortest path traces) + entropy regularization; fine-tune with Q-learning (experience replay, prioritized replay). Inference: forward-only beam search limited by β.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Agent success measured by average binary reward (task completion), Recall@K/MRR in retrieval-like tasks, and training convergence speed; exploration controlled via β (max outgoing edges considered) and ε-greedy during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported examples: supervised pretraining then RL finetuning improves performance (see thesis Table 2.4); beam search with width up to β improves success when action distribution entropy is maintained. Exact aggregated training-sample efficiency numbers not enumerated beyond qualitative statements (pretraining reduces RL training instability and required exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Dependent on dataset/difficulty; examples: on WikiNav-Jeopardy NeuAgent (pretrained on WikiNav-16-4) achieved MRR 20.3% and Recall@1 18.9% (see thesis Table 2.6). On smaller-h tasks success rates are higher; exact per-task percentages are tabulated in thesis tables.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Supervised-initialized recurrent policy (LSTM) fine-tuned with Q-learning, with entropy regularization and beam-search inference (memory + planning).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>NeuAgent-Rec advantages increase with larger allowed hops h (longer required planning). When local branching β is small, the policy must choose precisely at each step; when β is large the environment admits near-BFS strategies. Pretraining on large WikiNav variants transfers to improved ranking performance on WikiNav-Jeopardy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Across controlled parameters (h, β, query size), NeuAgent-Rec outperforms NeuAgent-FF as h increases; larger models outperform smaller when target distance increases; supervised→RL training improves ability to handle unseen nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Memory (recurrence) and attention over query/content are critical for long-range navigation; entropy regularization is necessary to avoid overly-deterministic policies that cannot leverage beam search; combining supervised shortest-path imitation with RL improves robustness and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations and Agents for Information Retrieval', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1235.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1235.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebNav (tool)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebNav (website-to-navigation-task compiler)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A software tool that converts any given website into a goal-driven web navigation task by parsing pages into nodes, hyperlinks into edges, extracting candidate query sentences, and partitioning examples into train/validation/test without article overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>WebNav-generated environments (e.g., WikiNav)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Tool-generated directed graph environment: nodes are cleaned text content of pages; edges are intra-site hyperlinks; query sentences are sampled from target nodes at controlled distances from start node.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Depends on seed website; for Wikipedia the graph is large and densely linked; WebNav can prune external links and specific sections (References, External links).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Dependent on input website; used to produce WikiNav from full English Wikipedia (≈5M pages in their dump).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not an agent; a dataset/environment creation pipeline producing structured navigation tasks for training/evaluating agents.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Allows controller over difficulty via parameters: max explored edges per node β, max hops h, and query size s; these parameters directly change effective topology for the agent (branching factor, horizon).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Enables experiments varying β and h to study how branching and horizon affect agent success; paper reports increased difficulty with larger h and decreased difficulty with larger queries; β prevents trivial BFS.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations and Agents for Information Retrieval', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1235.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1235.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wikispeedia (related prior env)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wikispeedia (game / dataset by West and Leskovec)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-navigation game and dataset where players must navigate from one Wikipedia article to another using only hyperlinks; used as a related benchmark and human-navigation study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Wikispeedia: An online game for inferring semantic distances between concepts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Wikispeedia (Wikipedia for Schools graph)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Small Wikipedia subset ('Wikipedia for Schools') converted to a graph: ≈4,000 articles and ≈120,000 hyperlinks (as of 2008); task: navigate from a given start article to a target article (target given as full page). Emphasizes human navigation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Directed hyperlink graph with moderate density (≈120k edges over ≈4k nodes); smaller scale than WikiNav.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Approximately 4,000 nodes and 120,000 directed edges (reported by West et al. in 2008 dataset used by Wikispeedia).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>human participants / simple automatic strategies (in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Used to study how humans navigate and to compare human vs automatic navigation strategies; target given as whole page (single target node).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Human path lengths, success rates, and comparison to algorithmic shortest paths / heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Not directly optimized here; human wayfinding strategies studied (semantic cues, landmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Related work shows that human navigation depends strongly on concept/semantic cues and that small-world-like connectivity affects navigability; thesis uses Wikispeedia as prior art rather than conducting new topology comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Wikispeedia studies emphasize that giving the full target page simplifies human navigation (single target) compared to WikiNav where target is a sentence and multiple target nodes may exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations and Agents for Information Retrieval', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1235.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1235.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MUD games (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-User Dungeon (MUD) text games (Narasimhan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-based adventure games / text-worlds used in RL research where states and sometimes actions are represented as natural-language descriptions, partially-observed and requiring language understanding plus planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MUD-style text games</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Fantasy / adventure text-game environments where the agent receives textual descriptions of the current room/state and issues textual or templated actions; typically smaller vocabularies than WikiNav (example 'Fantasy World' had ~1,340 words in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Game world graph (rooms, objects) with connectivity determined by game design; often sparser and smaller than Wikipedia graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Varies by game; earlier MUD experiments used much smaller vocabularies and state spaces than WikiNav.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL agents (e.g., DQN/RNN variants used in Narasimhan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents integrate natural-language understanding and planning; actions often expressed as language; architectures often use RNN encoders and RL algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Cumulative reward, steps to task completion, success rates across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory + language understanding (recurrent policies); planning helpful depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Not detailed in this thesis beyond noting MUDs are similar partially-observed text worlds; Narasimhan et al.'s environments are much smaller in vocabulary and state-space than WikiNav, implying different scalability/topology challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>MUD-style tasks require combining language understanding with planning; referenced as related work motivating goal-driven navigation but not empirically compared in this thesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations and Agents for Information Retrieval', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Wikispeedia: An online game for inferring semantic distances between concepts <em>(Rating: 2)</em></li>
                <li>Automatic versus human navigation in information networks <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Using reinforcement learning to spider the web efficiently <em>(Rating: 1)</em></li>
                <li>Focused crawling: a new approach to topic-specific web resource discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1235",
    "paper_id": "paper-201070547",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "WikiNav / WebNav",
            "name_full": "WikiNav (goal-driven web navigation) / WebNav (environment generator)",
            "brief_description": "A text-world constructed from English Wikipedia where pages are nodes and hyperlinks are directed edges; an agent is given a query (sentence(s)) and must navigate from a fixed start node to a target page containing the answer, under constraints on branching, hops, and memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "WikiNav (built with WebNav)",
            "environment_description": "Encyclopedic web-text world built from English Wikipedia (Sep 2015). Nodes = articles (natural-language text), directed edges = intra-site hyperlinks. Tasks: start at designated root (\"Category: Main topic classifications\") and navigate to a node whose text contains a query sentence; queries are 1–4 sentences taken from target pages. Partially observable: agent only sees current node text and immediate neighbors' texts (a 'peek').",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Directed hyperlink graph; described as \"densely-linked\" for Wikipedia; connectivity is heterogeneous (some pages many links, others few); action space at a node = set of outgoing links (state-dependent). The environment enforces a maximum number of explored outgoing edges per node (parameter β) to bound local branching during evaluation.",
            "environment_size": "&gt;5 million pages (English Wikipedia dump, Sep 2015); vocabulary ≈ 370k unique words; task variants WikiNav-h-s created by varying max hops h ∈ {4,8,16} and query size s ∈ {1,2,4} (dataset sizes: WikiNav-4 small; WikiNav-8 and WikiNav-16 large; exact per-split counts reported in thesis tables).",
            "agent_name": "NeuAgent (NeuAgent-FF, NeuAgent-Rec)",
            "agent_description": "Neural navigation agent that reads current node content and query, peeks at neighbor node representations, then outputs a distribution over outgoing edges and a stop action. Implementations: feedforward (NeuAgent-FF) and recurrent (NeuAgent-Rec with LSTMs). Uses BoW or attention-based representations for documents and queries. Trained with supervised learning on shortest-path traces then fine-tuned with Q-learning (reinforcement learning). Inference uses beam search capped by β.",
            "exploration_efficiency_metric": "Primary metrics: task reward (binary success per episode → averaged = average reward / success rate), Recall@K / MRR for retrieval-style evaluation (WikiNav-Jeopardy); secondary metrics: number of hops taken, training sample efficiency (convergence speed).",
            "exploration_efficiency_value": "Example reported results (WikiNav-Jeopardy): average path length to targets: mean = 5.8 hops (sd = 1.2; min = 2; max = 10). NeuAgent (pretrained on WikiNav-16-4) achieved MRR = 20.3%, Recall@1 = 18.9%, Recall@5 = 24.1%, Recall@100 = 47.6% on WikiNav-Jeopardy (reported in thesis Table 2.6).",
            "success_rate": "Varies by task difficulty (h and query size). Example: NeuAgent (pretrained) Recall@1 ≈ 18.9% on WikiNav-Jeopardy; human average reward reported lower (examples: human averages reported in experiments e.g., 14.5% in one setting). Supervised+RL finetuning improves success rate relative to supervised-only.",
            "optimal_policy_type": "Memory-based recurrent policies (NeuAgent-Rec with LSTM and attention) with supervised pretraining followed by reinforcement learning perform best for longer-range navigation; for shallow tasks a feedforward/reactive policy suffices. Beam search and entropy regularization of action distributions improve inference.",
            "topology_performance_relationship": "Reported relationships: (1) Increasing allowed hops h (search horizon) makes the task harder — search space grows (approximately) exponentially with h, lowering success rates. (2) Longer/more informative queries reduce difficulty (positive correlation between query length and success). (3) Limiting local branching (max explored outgoing edges β) prevents trivial BFS and forces agent to make better local decisions; with β→∞ a breadth-first strategy would trivially succeed. (4) Recurrent agents outperform feedforward ones as h increases, indicating long-range planning / memory is required for high-diameter navigation.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper does not compare fundamentally different graph families (e.g., trees vs. grids) but performs controlled comparisons by varying topology-relevant parameters: max hops h, max explored edges per node β, and query size s. Key findings: higher h (larger effective search radius) decreases performance; smaller β (tighter local branching) increases per-step decision difficulty; larger/more informative queries improve navigation success; pretrained agents on larger (higher-h) WikiNav datasets generalize better to retrieval tasks (WikiNav-Jeopardy).",
            "policy_structure_findings": "Findings include: (1) Recurrent/memory-based policies (LSTM NeuAgent-Rec) show consistent advantage for longer-h tasks because they can use history for long-term planning. (2) Entropy regularization of the action distribution during training prevents overly peaked policies and enables effective beam search during inference. (3) Supervised training on shortest-path traces helps bootstrap policy (gives initial high-probability correct paths); RL finetuning teaches recovery from unseen states. (4) Limiting local branching (β) makes per-node decision-making quality critical, so policies must learn strong local relevance scoring using local context/peeked neighbor content.",
            "uuid": "e1235.0",
            "source_info": {
                "paper_title": "Learning Representations and Agents for Information Retrieval",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "NeuAgent",
            "name_full": "NeuAgent (Neural-network-based navigation agent)",
            "brief_description": "A neural agent architecture for goal-driven web navigation that computes hidden state from the current page and query, peeks at neighbor page representations, and outputs probabilities over outgoing links plus a stop action; variants include feedforward and recurrent (LSTM) cores and attention-based document/query encoders.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Used on WikiNav (Wikipedia navigation tasks)",
            "environment_description": "Operates in the WikiNav text-world: at each step reads current node text, peeks neighbor texts, selects an outgoing edge or stop action; action-space limited to outgoing hyperlinks (state-dependent action set).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Treated generically as directed hyperlink graph; NeuAgent uses local neighbor peek and learned scoring of neighbor document vectors to choose edges.",
            "environment_size": "Applied at large scale (datasets built from ≫1M pages: full English Wikipedia used to construct WikiNav variants; WikiNav-16 uses pages up to 16 hops away), so state/action spaces are very large.",
            "agent_name": "NeuAgent-FF / NeuAgent-Rec",
            "agent_description": "Core f implemented either as a single-layer tanh feedforward net (NeuAgent-FF) or multi-layer LSTM (NeuAgent-Rec). Document and query encoders: BoW averaging or attention-based CNN/conv-window contexts. Action logits computed by similarity between hidden state and neighbor representations; stop action parameterized separately. Training: supervised (maximize likelihood of shortest path traces) + entropy regularization; fine-tune with Q-learning (experience replay, prioritized replay). Inference: forward-only beam search limited by β.",
            "exploration_efficiency_metric": "Agent success measured by average binary reward (task completion), Recall@K/MRR in retrieval-like tasks, and training convergence speed; exploration controlled via β (max outgoing edges considered) and ε-greedy during RL.",
            "exploration_efficiency_value": "Reported examples: supervised pretraining then RL finetuning improves performance (see thesis Table 2.4); beam search with width up to β improves success when action distribution entropy is maintained. Exact aggregated training-sample efficiency numbers not enumerated beyond qualitative statements (pretraining reduces RL training instability and required exploration).",
            "success_rate": "Dependent on dataset/difficulty; examples: on WikiNav-Jeopardy NeuAgent (pretrained on WikiNav-16-4) achieved MRR 20.3% and Recall@1 18.9% (see thesis Table 2.6). On smaller-h tasks success rates are higher; exact per-task percentages are tabulated in thesis tables.",
            "optimal_policy_type": "Supervised-initialized recurrent policy (LSTM) fine-tuned with Q-learning, with entropy regularization and beam-search inference (memory + planning).",
            "topology_performance_relationship": "NeuAgent-Rec advantages increase with larger allowed hops h (longer required planning). When local branching β is small, the policy must choose precisely at each step; when β is large the environment admits near-BFS strategies. Pretraining on large WikiNav variants transfers to improved ranking performance on WikiNav-Jeopardy.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Across controlled parameters (h, β, query size), NeuAgent-Rec outperforms NeuAgent-FF as h increases; larger models outperform smaller when target distance increases; supervised→RL training improves ability to handle unseen nodes.",
            "policy_structure_findings": "Memory (recurrence) and attention over query/content are critical for long-range navigation; entropy regularization is necessary to avoid overly-deterministic policies that cannot leverage beam search; combining supervised shortest-path imitation with RL improves robustness and sample efficiency.",
            "uuid": "e1235.1",
            "source_info": {
                "paper_title": "Learning Representations and Agents for Information Retrieval",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "WebNav (tool)",
            "name_full": "WebNav (website-to-navigation-task compiler)",
            "brief_description": "A software tool that converts any given website into a goal-driven web navigation task by parsing pages into nodes, hyperlinks into edges, extracting candidate query sentences, and partitioning examples into train/validation/test without article overlap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "WebNav-generated environments (e.g., WikiNav)",
            "environment_description": "Tool-generated directed graph environment: nodes are cleaned text content of pages; edges are intra-site hyperlinks; query sentences are sampled from target nodes at controlled distances from start node.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Depends on seed website; for Wikipedia the graph is large and densely linked; WebNav can prune external links and specific sections (References, External links).",
            "environment_size": "Dependent on input website; used to produce WikiNav from full English Wikipedia (≈5M pages in their dump).",
            "agent_name": "",
            "agent_description": "Not an agent; a dataset/environment creation pipeline producing structured navigation tasks for training/evaluating agents.",
            "exploration_efficiency_metric": "",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "",
            "topology_performance_relationship": "Allows controller over difficulty via parameters: max explored edges per node β, max hops h, and query size s; these parameters directly change effective topology for the agent (branching factor, horizon).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Enables experiments varying β and h to study how branching and horizon affect agent success; paper reports increased difficulty with larger h and decreased difficulty with larger queries; β prevents trivial BFS.",
            "policy_structure_findings": "",
            "uuid": "e1235.2",
            "source_info": {
                "paper_title": "Learning Representations and Agents for Information Retrieval",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Wikispeedia (related prior env)",
            "name_full": "Wikispeedia (game / dataset by West and Leskovec)",
            "brief_description": "A human-navigation game and dataset where players must navigate from one Wikipedia article to another using only hyperlinks; used as a related benchmark and human-navigation study.",
            "citation_title": "Wikispeedia: An online game for inferring semantic distances between concepts",
            "mention_or_use": "mention",
            "environment_name": "Wikispeedia (Wikipedia for Schools graph)",
            "environment_description": "Small Wikipedia subset ('Wikipedia for Schools') converted to a graph: ≈4,000 articles and ≈120,000 hyperlinks (as of 2008); task: navigate from a given start article to a target article (target given as full page). Emphasizes human navigation patterns.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Directed hyperlink graph with moderate density (≈120k edges over ≈4k nodes); smaller scale than WikiNav.",
            "environment_size": "Approximately 4,000 nodes and 120,000 directed edges (reported by West et al. in 2008 dataset used by Wikispeedia).",
            "agent_name": "human participants / simple automatic strategies (in related work)",
            "agent_description": "Used to study how humans navigate and to compare human vs automatic navigation strategies; target given as whole page (single target node).",
            "exploration_efficiency_metric": "Human path lengths, success rates, and comparison to algorithmic shortest paths / heuristics.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Not directly optimized here; human wayfinding strategies studied (semantic cues, landmarks).",
            "topology_performance_relationship": "Related work shows that human navigation depends strongly on concept/semantic cues and that small-world-like connectivity affects navigability; thesis uses Wikispeedia as prior art rather than conducting new topology comparisons.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "Wikispeedia studies emphasize that giving the full target page simplifies human navigation (single target) compared to WikiNav where target is a sentence and multiple target nodes may exist.",
            "uuid": "e1235.3",
            "source_info": {
                "paper_title": "Learning Representations and Agents for Information Retrieval",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "MUD games (related)",
            "name_full": "Multi-User Dungeon (MUD) text games (Narasimhan et al.)",
            "brief_description": "Text-based adventure games / text-worlds used in RL research where states and sometimes actions are represented as natural-language descriptions, partially-observed and requiring language understanding plus planning.",
            "citation_title": "Language understanding for text-based games using deep reinforcement learning",
            "mention_or_use": "mention",
            "environment_name": "MUD-style text games",
            "environment_description": "Fantasy / adventure text-game environments where the agent receives textual descriptions of the current room/state and issues textual or templated actions; typically smaller vocabularies than WikiNav (example 'Fantasy World' had ~1,340 words in referenced work).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Game world graph (rooms, objects) with connectivity determined by game design; often sparser and smaller than Wikipedia graphs.",
            "environment_size": "Varies by game; earlier MUD experiments used much smaller vocabularies and state spaces than WikiNav.",
            "agent_name": "RL agents (e.g., DQN/RNN variants used in Narasimhan et al.)",
            "agent_description": "Agents integrate natural-language understanding and planning; actions often expressed as language; architectures often use RNN encoders and RL algorithms.",
            "exploration_efficiency_metric": "Cumulative reward, steps to task completion, success rates across episodes.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Memory + language understanding (recurrent policies); planning helpful depending on task.",
            "topology_performance_relationship": "Not detailed in this thesis beyond noting MUDs are similar partially-observed text worlds; Narasimhan et al.'s environments are much smaller in vocabulary and state-space than WikiNav, implying different scalability/topology challenges.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "MUD-style tasks require combining language understanding with planning; referenced as related work motivating goal-driven navigation but not empirically compared in this thesis.",
            "uuid": "e1235.4",
            "source_info": {
                "paper_title": "Learning Representations and Agents for Information Retrieval",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Wikispeedia: An online game for inferring semantic distances between concepts",
            "rating": 2,
            "sanitized_title": "wikispeedia_an_online_game_for_inferring_semantic_distances_between_concepts"
        },
        {
            "paper_title": "Automatic versus human navigation in information networks",
            "rating": 2,
            "sanitized_title": "automatic_versus_human_navigation_in_information_networks"
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Using reinforcement learning to spider the web efficiently",
            "rating": 1,
            "sanitized_title": "using_reinforcement_learning_to_spider_the_web_efficiently"
        },
        {
            "paper_title": "Focused crawling: a new approach to topic-specific web resource discovery",
            "rating": 1,
            "sanitized_title": "focused_crawling_a_new_approach_to_topicspecific_web_resource_discovery"
        }
    ],
    "cost": 0.0249045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEARNING REPRESENTATIONS AND AGENTS FOR INFORMATION RETRIEVAL DISSERTATION Submitted in Partial Fulfillment of the Requirements for the Degree of DOCTOR OF PHILOSOPHY (Computer Science) at the NEW YORK UNIVERSITY TANDON SCHOOL OF ENGINEERING LEARNING REPRESENTATIONS AND AGENTS FOR INFORMATION RETRIEVAL DISSERTATION Submitted in Partial Fulfillment of the Requirements for the Degree of DOCTOR OF PHILOSOPHY (Computer Science) at the NEW YORK UNIVERSITY TANDON SCHOOL OF ENGINEERING Research and Funding Professional Experience ABSTRACT LEARNING REPRESENTATIONS AND AGENTS FOR INFORMATION RETRIEVAL
September 2019 16 Aug 2019 September 2019 Sep. 2014 -Sep. 2019 Aug. 2013 -Aug. 2014 Jan. 2005 -Dec. 2009 September 2019</p>
<p>Rodrigo Frassetto Nogueira 
Universidade Estadual de Campinas
FEEC</p>
<p>Rodrigo Frassetto Nogueira 
Universidade Estadual de Campinas
FEEC</p>
<p>Rodrigo Frassetto 
Universidade Estadual de Campinas
FEEC</p>
<p>Nogueira Advisor 
Universidade Estadual de Campinas
FEEC</p>
<p>Kyunghyun Cho 
Universidade Estadual de Campinas
FEEC</p>
<p>LEARNING REPRESENTATIONS AND AGENTS FOR INFORMATION RETRIEVAL DISSERTATION Submitted in Partial Fulfillment of the Requirements for the Degree of DOCTOR OF PHILOSOPHY (Computer Science) at the NEW YORK UNIVERSITY TANDON SCHOOL OF ENGINEERING LEARNING REPRESENTATIONS AND AGENTS FOR INFORMATION RETRIEVAL DISSERTATION Submitted in Partial Fulfillment of the Requirements for the Degree of DOCTOR OF PHILOSOPHY (Computer Science) at the NEW YORK UNIVERSITY TANDON SCHOOL OF ENGINEERING Research and Funding Professional Experience ABSTRACT LEARNING REPRESENTATIONS AND AGENTS FOR INFORMATION RETRIEVAL
September 2019 16 Aug 2019 September 2019 Sep. 2014 -Sep. 2019 Aug. 2013 -Aug. 2014 Jan. 2005 -Dec. 2009 September 2019Approved: Department Chair Signature Date University ID: N10443173 Net ID: rfn216 ii Approved by the Guidance Committee: Major: Computer Science Kyunghyun Cho Assistant Professor Courant Institute and Center for Data Science New York University Date Claudio Silva Professor Tandon School of Engineering New York University Date Fernando Diaz Principal Research Manager Microsoft Research Date Massimiliano Ciaramita Research Scientist Google Research Date iii Microfilm/Publishing Microfilm or copies of this dissertation may be obtained from: UMI Dissertation Publishing ProQuest CSA 789 E. Eisenhower Parkway P.O. Box 1346 Ann Arbor, MI 48106-1346 iv Vita Rodrigo Frassetto Nogueira Education PhD in Computer Science New York University, Tandon School of Engineering M.S. in Computer Engineering Universidade Estadual de Campinas, FEEC B.S. in Electrical Engineering This research was performed at the NYU Machine Learning for Language (ML 2 ) Lab. Funding for tuition and personal expenses for the first four years was granted by the CAPES Science without Borders scholarship. Funding for the fifth year and traveling to present the papers that originated this work was provided by Prof. Kyunghyun Cho.
Rodrigo has an Ms.C. degree from Universidade Estadual de Campinas (UNI-CAMP), where he developed with prof. Roberto Alencar Lotufo an award-winning algorithm for real vs. fake fingerprint detection. Before that, he worked for Siemens as a software engineer for five years. During that time, he deployed SCADA and Smart Grid systems and was the main inventor of an automated testing equipment for low-voltage control and protection cubicles. v Dedication I dedicate this work to my family and friends, specially my wife, Andrea, my brother, Danilo, my mother, Fatima, my Father, Paulo, and my aunt, Elza.    vi A goal shared by artificial intelligence and information retrieval is to create an oracle, that is, a machine that can answer our questions, no matter how difficult they are. A more limited, but still instrumental, version of this oracle is a questionanswering system, in which an open-ended question is given to the machine, and an answer is produced based on the knowledge it has access to. Such systems already exist and are increasingly capable of answering complicated questions[22,27,94]. This progress can be partially attributed to the recent success of machine learning and to the efficient methods for storing and retrieving information, most notably through web search engines.One can imagine that this general-purpose question-answering system can be built as a billion-parameters neural network trained end-to-end with a large number of pairs of questions and answers. We argue, however, that although this approach has been very successful for tasks such as machine translation, storing the world's knowledge as parameters of a learning machine can be very hard. A more efficient way is to train an artificial agent on how to use an external retrieval system to collect relevant information. This agent can leverage the effort that has been put into designing and running efficient storage and retrieval systems by learning how to best utilize them to accomplish a task.In this thesis, we present two instances of such an agent. One is constructed to navigate a web of documents, searching for an answer to a given question. This agent makes use of the enormous human work put into curating documents and their links, in particular, the Wikipedia corpus. The second agent takes advantage of existing search engines by rewriting questions to retrieve more relevant answers. Its advantage vii lies in the use of reinforcement learning, which requires minimal work in specifying a mechanism to rewrite questions that will return more accurate answers. For both agents, we showed that their performance can be higher than strong baselines. Furthermore, we look inside the search engine -previously treated as a black box -and introduce two novel components to improve it. One is a ranking model that uses unsupervised pretraining to re-rank documents more effectively. The other improves the inverted index representation by augmenting documents with predictions of questions that they might correctly answer. We show that these two methods combined can double the retrieval effectiveness of an off-the-shelf search engine.In a parallel with computer vision, when the AlexNet model [55] almost reduced the error rate by half in an object detection task, it created a revolution that made possible a multitude of applications, such as self-driving cars. Similarly, with more effective retrieval mechanisms, we aspire that future question answering systems will have a closer resemblance to a research assistant that helps us expand our understanding of the world.viii</p>
<p>Visualization of the attention mechanism over a sample query. The horizontal axis corresponds the words in the input query, the vertical axis corresponds to the title of the current Wikipedia article, and the brighter the cell, the higher the attention weight. . . . . . . . . . . . 21</p>
<p>3.1 A graphical illustration of the proposed framework for query reformulation. A set of documents 0 is retrieved from a search engine using the initial query 0 . Our reformulator selects terms from 0 and 0 to produce a reformulated query ′ which is then sent to the search engine. Documents ′ are returned, and a reward is computed against the set of relevant documents. The reformulator is trained with reinforcement learning to produce a query, or a series of queries, to maximize the expected return. . Reformulations are used to obtain results from the search system, which are then sent to the aggregator, which picks the best result for the original query based on a learned weighted majority voting scheme. Reformulators are independently trained on disjoint partitions of the dataset, thus increasing the variability of reformulations. . . . . . . . 48 3. 6 Overall system's effectiveness for different number of sub-agents. . . . 52</p>
<ol>
<li></li>
</ol>
<p>Introduction</p>
<p>Motivation</p>
<p>Since ancient times, humans dream of omniscient entities that could answer their most urgent doubts. For example, Pythia, or the Oracle of Delphi, was consulted about important decisions throughout the ancient classical world. Diviners in ancient China would carve into ox and turtle bones questions regarding future weather, crop planting, military endeavors, and other topics, and the answers would emerge in the form of cracks. 1 Although somewhat appealing, these were, however, imprecise sources of knowledge and unreliable forecasters. Perhaps more accurate but costly ways of obtaining answers involved searches in books, consultations with the elderly, or through experimentation, when possible.</p>
<p>In recent years, with the advent of the Internet, the dream of having access to an oracle has come closer to reality. For example, one can instantly find the most effective methods to avoid sea-sickness, tips to summer vacations, or what a legal expert says on exporting perishable goods. Two decades ago, to obtain this kind of information one had to, at least, visit a specialist or a library.</p>
<p>A key player in this revolution is the search engine: A machine that can find the information we need through text inputs. It has become so popular that one of the largest commercial search engines serves trillions of such requests per year. 2 Despite being very useful, machines still cannot successfully answer many questions. For instance, when asked, "Do teenagers go more often to the movies than adults?", the top document returned by a popular search engine contains only part of the answer 1 https://en.wikipedia.org/wiki/Oracle_bone (i.e., the absolute number of teenagers going to the movies in the U.S. in 2017), and the remaining results are irrelevant (Figure 1.1). To obtain a complete answer, the user might need to rephrase or break the query, each part addressing one aspect of the original question. In our example, the user might need to independently issue the following queries: "Number of movie tickets sold by age in 2018", "Demographics of moviegoers", "Number of movie tickets sold by age in the U.S. (or China, or Europe)", etc. After collecting the results, the user might still need to aggregate them in a spreadsheet in order to have a complete answer. In other words, despite using a state-of-the-art search engine, the user still had to go through a slow and laborious process of obtaining the answer to the original question.</p>
<p>One of the main reasons for this low retrieval effectiveness is the reliance of most existing search engines on keyword match to retrieve the initial set of documents. In such cases, a relevant document will not be retrieved if its terms do not match the ones in the query. This commonly known as the "vocabulary mismatch" problem, and users often spend a reasonable amount of time getting familiarized with the terms that are commonly used in the field of interest. An analogy is that of a tourist visiting a new city and needs to go from one place to another by asking for directions. She needs to learn the names of the main streets and landmarks to facilitate her navigation through this unknown environment. Likewise, a web user needs to learn the terms used in the relevant web pages or documents to find the desired information. This problem is aggravated in specialized areas like medicine or law, in which a user might experience an avalanche of new terms when she explores areas just outside her main field of expertise.</p>
<p>To further support our claim that existing retrieval mechanisms do not work well at least in a handful of use cases, we can quantify their effectiveness. One straightforward way is to download publicly available academic question-answering datasets and compute the percentage of questions that can be answered based solely on documents retrieved by existing search engines. For example, on the Natural Questions dataset [57], only half of its questions had an answer in the top returned Wikipedia article. Similarly, in the MS MARCO passage retrieval task [75], the popular BM25 algorithm [85] retrieves a relevant paragraph among the top-10 for only 40% of the questions. In another three datasets (TREC-CAR [33], Jeopardy [76], and MSA [77]), among the top-1000 documents retrieved by BM25, correct documents are present only for 30%-60% of the queries. In addition, Chen et al. [19] found that the performance of their question-answering system drops from 78% F1, when the correct document is given, to 30% when the retrieval task is taken into account.</p>
<p>Two of the datasets used in this brief analysis, MS MARCO and Natural Questions, have queries issued by real users to commercial search engines, i.e., Bing and Google, respectively. Although this data contains daily information needs of real users, a user's true information need is only partially represented. The reason is that many users have learned what these machines can and cannot answer. For example, a user might decide to not even issue the query "Do teenagers go more often to the movies than adults?" because she anticipates that the machine will probably not return a concise and correct answer. This limitation of the current technology, therefore, hides more complex information needs. In other words, if we have access to more effective answering machines, we will probably start asking more complicated questions.</p>
<p>Contributions</p>
<p>At this point, we hope to have convinced the reader that we need better retrieval mechanisms if we aim at answering more complicated questions. This is precisely the goal of this thesis, whose proposed methods are summarized next.</p>
<p>Retrieval as Web Navigation (Chapter 2)</p>
<p>Here, we do not aim to improve existing search engines by modifying or replacing their components. Instead, we propose a novel search mechanism in its entirety: An agent that searches for relevant information in a corpus by following the hyperlinks that connect the documents. Because this search mechanism does not rely on keyword match to perform retrieval, the vocabulary mismatch problem is mitigated. The agent is a neural network trained with supervised learning and fine-tuned with reinforcement learning. For the supervised training, the ground truth navigation paths are obtained using a shortest path algorithm. In the reinforcement finetuning step, the reward is a binary function that tells the agent if the correct document for a given question has been reached.</p>
<p>We evaluate our proposed navigational agent on a challenging dataset of Jeopardy! questions and show that it performs better than traditional index-based search engines in the densely-linked Wikipedia corpus.</p>
<p>Retrieving from a Black Box (Chapter 3)</p>
<p>We leverage the capabilities of modern search engines through an agent that learns how to use them to obtain better answers. The underlying search engine is treated as a black box, i.e., its internal mechanism is hidden from our agent, so the same method can be used to optimize different types of systems as long as they share a common interface, namely, text as input and output.</p>
<p>A key characteristic of this work is the use of reinforcement learning (RL) to train the agent in reformulating queries to increase the chances of retrieving relevant documents. The main advantage of using RL is that the agent directly optimizes the metric of choice (recall, MAP, NDCG, etc.). Alternative methods require manuallydefined rules for reformulating queries or training data with pairs of original and reformulated queries.</p>
<p>The good results achieved with this method led us to further extend it into using multiple reformulation agents. In this framework, a set of diverse reformulation agents in addition to an answer-aggregation module resulted in better effectiveness than a standard ensemble counterpart. Also, given the same computational budget, it can be trained in a fraction of the time when compared to the single-agent version due to the ease of parallelization of the proposed method.</p>
<p>Looking into the Black Box (Chapter 4) Here, we depart from the black box paradigm of the previous chapters and explore methods to enhance the internal mechanism of the search engine. We show that the effectiveness of the document ranking stage can be largely improved by using a neural model pretrained on a language modeling task in a high-quality corpus (e.g. Wikipedia). This model takes as an input the concatenation of the minimally preprocessed query and document texts, and computes a relevance score. The model learns the interactions between query and document terms though the self-attention mechanism [106].</p>
<p>Subsequently, we introduce a technique to improve the representation of the inverted index. We observe that query reformulation is an algorithm that translates query language into document language. We then invert this direction and translate from the document language to the query language. We achieve this by training with supervised learning an off-the-shelf translation model that takes as an input a document and predicts queries to which the document might be relevant. Once trained, each document in the corpus is expanded with its predicted queries and then indexed using a vanilla inverted indexing algorithm.</p>
<p>We show the effectiveness of these two novel components (i.e., document expansion and re-ranker) by achieving the state of the art in two retrieval tasks.</p>
<p>Organization</p>
<p>This thesis is organized as follows. We introduce the web navigation agent in Chapter 2 and the query reformulation agent in Chapter 3. We present the novel search engine components in Chapter 4 and, finally, our conclusion in Chapter 5.</p>
<p>Chapter 2 A Search Engine from Scratch</p>
<p>Retrieval as Web Navigation</p>
<p>Over the past five decades, the introduction of digital computers created a revolution on how we access and store information. In particular, digital communication networks gave rise to the Internet, which resulted in the generation and spread of an endless amount of textual, audio, and visual data. Search engines then came to organize this data and provide easy access to it via a simple interface, mostly through short text inputs called queries.</p>
<p>The way most search engines work is simple to explain. 1 First, the documents from websites are downloaded by machines called "web crawlers" or "spiders." This data is converted into the so-called inverted index. It is, in essence, a dictionary whose keys are the words in the documents and values are pointers to the documents that contain the word. Finally, when a user enters a query, the words in it are matched to the keys in the inverted index, and the corresponding documents are returned, ordered by a ranking algorithm.</p>
<p>Although successful in many domains, index-based search engines have limitations. First, if the terms in a query do not match the ones used in a relevant document despite being semantically the same (i.e., "auto sellers" vs. "I want to buy a car"), the index-lookup might not return the correct document, unless additional mechanisms such as query rewrite [16] are employed. Second, if a query is long, like a conversation with a digital assistant, matching all terms in it with the ones in the inverted index will result in a large set of returned documents, and ranking them will be expensive.</p>
<p>In this chapter, we propose a goal-driven web navigation agent as an alternative to index-based search engines. The proposed goal-driven web navigation environment consists of the whole website as a graph, in which the web pages are nodes and hyperlinks are directed edges. An agent is given a query and navigates the network, starting from a predefined node, to find a target node that contains the answer to the query. We release a software tool, called WebNav, that converts a given website into a goal-driven web navigation task. As an example of its use, we provide WikiNav, which was built from English Wikipedia. We design artificial agents based on neural networks (called NeuAgents) trained with supervised and reinforcement learning, and report their respective performances on the task as well as the performance of human volunteers.</p>
<p>Furthermore, we extend the WikiNav with an additional set of queries that are constructed from Jeopardy! questions, to which we refer by WikiNav-Jeopardy. We evaluate the proposed NeuAgents against the three search-based strategies; (1) SimpleSearch, (2) Apache Lucene with BM25 [85] ranking function, and (3) Google Search API. The result indicates that the NeuAgents outperform those index-based search engines, implying a potential for the proposed task as a good proxy for practical applications such as document retrieval, question answering and focused crawling.</p>
<p>Goal-driven Web Navigation</p>
<p>A task T of goal-driven web navigation is characterized by T = (A, , G, , , Ω).</p>
<p>(2.1)</p>
<p>The world in which an agent A navigates is represented as a graph G = (N, E). The graph consists of a set of nodes N = { } N =1 and a set of directed edges E = { , } connecting those nodes. Each node represents a page of the website, which, in turn, is represented by the natural language text D( ) in it. There exists an edge going from a page to if and only if there is a hyperlink in D( ) that points to . One of the nodes is designated as a starting node from which any navigation begins. A target node is the one whose natural language description contains a query , and there may be more than one target node.</p>
<p>At each time step, the agent A reads the natural language description D( ) of the current node in which the agent has landed. At no point, the whole world, consisting of the nodes and edges, nor its structure or map (graph structure without any natural In 1983, the video game business suffered a much more sever crash. is 1 if and only if a node includes a query sentence . In this case, the reward at the node , i.e., ( , ) is 1, but ( , ) = 0.</p>
<p>language description) is visible to the agent, thus making this task partially observed decision-making.</p>
<p>Once the agent A reads the description D( ) of the current node , it can take one of the actions available. A set of possible actions is defined as a union of all the outgoing edges ,· and the stop action, thus making the agent have state-dependent action space.</p>
<p>Each edge , corresponds to the agent jumping to a next node , while the stop action corresponds to the agent declaring that the current node is one of the target nodes. Each edge , is represented by the description of the following node D( ). In other words, deciding which action to take is equivalent to taking a peek at each neighboring node and seeing whether that node is likely to ultimately lead to a target node.</p>
<p>The agent A receives a reward ( , ) when it chooses the stop action. This task uses a simple binary reward, where Constraints: It is clear that there exists a policy for the agent to succeed at every trial, which is to traverse the graph breadth-first until the agent finds a node in which the query appears. To avoid this kind of degenerate policies, the task includes a set of rules/constraints Ω. More specifically, there are four constraints:
( , ) = ⎧ ⎨ ⎩ 1, if ⊆ D(
1. An agent can follow at most edges at each node.</p>
<ol>
<li>
<p>An agent has a finite memory of size smaller than T.</p>
</li>
<li>
<p>An agent moves up to ℎ hops away from .</p>
</li>
</ol>
<p>4.</p>
<p>A query of size comes from at least two hops away from the starting node.</p>
<p>The first constraint alone prevents degenerate policies, such as breadth-first search, forcing the agent to make good decisions as possible at each node. The second one further constraints ensure that the agent does not cheat by using earlier trials to reconstruct the whole graph structure (during test time) or to store the whole world in its memory (during training.) The third constraint, which is optional, is there for computational consideration. The fourth constraint is included because the agent is allowed to read the content of the next node.</p>
<p>Controlled Levels of Difficulty</p>
<p>Three main control parameters affect the difficulty of the task. They are the maximum number of explored edges per node , the maximum number of allowed hops ℎ and the size of each query .</p>
<p>Maximum number of explored edges per node : If this parameter is high, an agent does not have to be confident about correct actions at each node, as there is a possibility of exploring other outgoing edges. If goes to infinity, the agent can simply perform breadth-first search. If = 1, the agent must select the correct outgoing edge at each node, otherwise it fails the task. We use = 4.</p>
<p>Maximum number of allowed hops ℎ : This parameter must be selected a priori generating a dataset from an existing website because this is used to select queries. The larger ℎ , the more difficult the task is.</p>
<p>Size of query :</p>
<p>The query is effectively the only source of clue the agent can use to plan its path from the starting node to a target node. Often a longer query contains more information, leading to easier navigation by the agent. We consider the number of sentences contained in each query as its size. Later, in the experiments, we show that it is indeed true that there is a positive correlation between the size of the query and the difficulty of the task.</p>
<p>WebNav and WikiNav</p>
<p>WebNav: Software</p>
<p>As a part of this work, we build and release a software tool which turns a website into a goal-driven web navigation task. 2 We call this tool WebNav. Given a starting URL, the WebNav reads the whole website, constructs a graph with the web pages in the website as nodes. Each node is assigned a unique identifier . The text content of each node D( ) is a cleaned version of the actual HTML content of the corresponding web page. We provide a cleanup function for Wikipedia, and it is easy to plug in a new cleanup function for another website. The WebNav turns intra-site hyperlinks into a set of edges , .</p>
<p>In addition to transforming a website into a graph G from Equation (2.1), the WebNav automatically extracts sentences as queries from the nodes' texts and divides them among training, validation, and test sets. We ensure that there is no overlap among these sets by making each target node, from which a query is selected, belongs to only one of the three sets.</p>
<p>Each generated example is defined as a tuple
= ( , * , * ) (2.3)
where is a query from a web page * which was found following a randomly selected path * = ( , . . . , * ). In other words, the WebNav starts from a starting page , random-walks the graph for a predefined number of steps ( ℎ /2, in our case), reaches a target node * and selects a query from D( * ). A query consists of sentences and is selected among top-5 candidates in the target node with the highest average TF-IDF, thus discouraging the WebNav from choosing a trivial query.</p>
<p>For the evaluation purpose alone, it is enough to use only a query itself as an example. However, we include both one target node (among potentially many other target nodes) and one path from the starting node to this target node (again, among many possible connecting paths) so that they can be exploited when training an agent. They are not to be used when evaluating a trained agent.   </p>
<p>WikiNav: A Wikipedia-based Navigation Task</p>
<p>Using the WebNav, we built a goal-driven navigation task using Wikipedia as a target website. We used the dump file of the English Wikipedia from September 2015, which consists of more than five million web pages. We built a set of separate tasks with different levels of difficulty by varying the maximum number of allowed hops ℎ ∈ {4, 8, 16} and the size of query ∈ {1, 2, 4}. We refer to each task by WikiNavℎ -.</p>
<p>For each task, we generate training, validation, and test examples from the pages half as many hops away from a starting page as the maximum number of hops allowed. 3 We use "Category: Main topic classifications" as a starting node .</p>
<p>As a minimal cleanup procedure, we excluded meta articles whose titles start with "Wikipedia." Any hyperlink that leads to a web page outside Wikipedia is removed in advance together with the following sections: "References," "External Links," "Bibliography," and "Partial Bibliography."</p>
<p>In Table 2.1, we present basic per-article statistics of the English Wikipedia. It is evident from these statistics that the world of WikiNavℎ -is large and complicated, even after the cleanup procedure.</p>
<p>We ended up with a fairly small dataset for WikiNav-4-<em>, but large for WikiNav-8-</em> and WikiNav-16-*. See Table 2 </p>
<p>WikiNav-Jeopardy: Jeopardy! on WikiNav</p>
<p>One of the potential practical applications utilizing a goal-driven navigation agent is question answering based on world knowledge. In this Q&amp;A task, a query is a question, and an agent navigates a given information network, e.g., a website, to retrieve an answer. In this section, we propose and describe an extension of the WikiNav, in which query-target pairs are constructed from actual Jeopardy! question-answer pairs. We refer to this extension of WikiNav by WikiNav-Jeopardy.</p>
<p>We first extract all the question-answer pairs from J! Archive 4 , which has more than 300k such pairs. We keep only those pairs whose answers are titles of Wikipedia articles, leaving us with 133k pairs. We divide those pairs into 113k training, 10k validation, and 10k test examples while carefully ensuring that no article appears in more than one partition. Additionally, we do not shuffle the original pairs to ensure that the train and test examples are from different episodes.</p>
<p>For each training pair, we find one path from the starting node "Category: Main Topic Classification" to the target node and include it for supervised learning. For reference, the average number of hops to the target node is 5.8, the standard deviation is 1.2, and the maximum and minimum are 2 and 10, respectively. See Table 2.3 for sample query-answer pairs.</p>
<p>NeuAgent: Neural Network based Agent</p>
<p>Here, we describe the neural network based agents built with a minimal set of prior knowledge.</p>
<p>Core Function</p>
<p>The core of the NeuAgent is a parametric function core that takes as input the content of the current node ( ) and a query ( ), and that returns the hidden state of the agent. This parametric function core can be implemented either as a feedforward neural network ff :
h = ff ( ( ), ( )),(2.4)
which does not take into account the previous hidden state of the agent or as a recurrent neural network rec :
h = rec (h −1 , ( ), ( )). (2.5)
We refer to these two types of agents by NeuAgent-FF and NeuAgent-Rec, respectively. For the NeuAgent-FF, we use a single tanh layer, while we use long short-term memory (LSTM) units [43] for the NeuAgent-Rec. Based on the new hidden state h , the NeuAgent computes the probability distribution over all the outgoing edges . The probability of each outgoing edge is proportional to the similarity between the hidden state h such that
( , |˜) ∝ exp (︁ ( ) ⊤ h )︁ .
(2.6)</p>
<p>Note that the NeuAgent peeks at the content of the next node by considering its vector representation ( ). In addition to all the outgoing edges, we also allow the agent to stop with the probability
(∅|˜) ∝ exp (︁ v ⊤ ∅ h )︁ ,(2.7)
where the stop action vector v ∅ is a trainable parameter. In the case of NeuAgent-Rec, all these (unnormalized) probabilities are conditioned on the history˜, which is a sequence of actions (nodes) selected by the agent so far. We divide these unnormalized probabilities by
(˜) = exp (︁ v ⊤ ∅ h )︁ + ∑︁ , ∈ ,· exp (︁ ( ) ⊤ h )︁ (2.8)
to obtain the probability distribution over all the possible actions at the current node , which is known as softmax normalization. The NeuAgent then selects its next action based on this action probability distribution (Eqs. (2.6) and (2.7)). If the stop action is chosen, the NeuAgent returns the current node as an answer and receives a reward ( , ), which is one if correct and zero otherwise. If the agent selects one of the outgoing edges, it moves to the selected node and repeats this process of reading and acting.</p>
<p>See Figure 2.2 for a single step of the described NeuAgent.</p>
<p>Content Representation</p>
<p>The NeuAgent represents the content of a node as a vector ( ) ∈ R . In this work, we consider two types of representations. In the first one, the content is represented as the average of the simple continuous bag-of-words vector (BoW) of each word present in the document:
( ) = 1 |D( )| |D( )| ∑︁ =1 e .
(2.9)</p>
<p>We use word vectors e from a pretrained continuous bag-of-words model [64]. These word vectors are fixed and not updated when training the NeuAgent.</p>
<p>In the other type of representation, we make use of the attention mechanism [5]  Each vector b is convoluted by a trainable 1-D weight matrix W ∈ R × to form the context vector c of the section:
c = + 2 ∑︁ ′ = − 2 W ′ b ′ (2.11)
where is the window size. A score is computed for each context vector using a parametric function, such as a feedforward neural network, that takes as inputs the query embeddings, the previous hidden state, and the context c :
= doc ( ( ), h −1 , c ) (2.12)
The scores are made to sum to 1 through a softmax function [11]:
= exp( ) ∑︀ =1 exp( ) (2.13)
and the vector representation of the document at step is obtained as the weighted sum of the context vectors:
( ) = 1 ∑︁ =1 c (2.14)</p>
<p>Query Representation</p>
<p>We consider the same two types of representation for a query . For the BoW representation, the vector is formed as the average of the word vectors of each word present in the query:
( ) = 1 | | | | ∑︁ =1 e . (2.15)
The second type is the attention-based representation. The query is projected to a context vector c , formed by the word embedding e convoluted by a 1-D trainable weight matrix W ∈ R × : 2.16) and the scores are computed by another parametric function (a feedforward neural network, in this work):
c = + 2 ∑︁ ′ = − 2 W ′ e ′(= query (h −1 , c ) (2.17)
The scores are made to sum to 1 through a softmax function and the vector representation at step is obtained as the weighted sum of the context vectors:
= exp( ) ∑︀ | | =1 exp( ) (2.18) ( ) = 1 | | | | ∑︁ =1 c (2.19)
We empirically compare these two types of representations for both content and query in section 2.5.1.</p>
<p>Inference: Beam Search</p>
<p>Once the NeuAgent is trained, there are a number of approaches to using it for solving the navigation task. The most naive approach is to let the agent make a greedy decision at each time step, i.e., following the outgoing edge with the highest probability arg max log ( , | . . .). A better approach is to exploit the fact that the agent is allowed to explore up to outgoing edges per node. This naturally leads to approximate decoding, and we use a simple, forward-only beam search with the beam width capped at . The beam search keeps the most likely traces, in terms of log ( , | . . .), at each time step.</p>
<p>Training Strategies: Supervised Learning</p>
<p>In this work, one of the training strategies we investigate is supervised learning, in which we train the agent to follow an example trace * = ( , . . . , * ) included in the training set at each step (see Equation (2.3)). In this case, for each training example, the training cost is
sup = − log (∅| * ) − | * | ∑︁ =1 log ( * | * &lt; ). (2.20)
This per-example training cost is fully differentiable with respect to all the parameters of the neural network, and we use stochastic gradient descent (SGD) algorithm to minimize this cost over the whole training set:
← − ∇ sup , (2.21)
where is a set of all the parameters, and ∇ sup is the gradient which can be efficiently computed by backpropagation [89]. This allows the entire model to be trained in an end-to-end fashion, in which the query-to-target performance is optimized directly. This approach of supervised learning for structured output prediction 5 is known to be prone to accumulating errors as it solves the task [87]. This is mainly because the agent never sees a wrong node during training, and it tends to fail more easily when it ends up in an unseen node at test time.</p>
<p>Entropy Regularization:</p>
<p>We observed that the action distribution in Equation (2.6) was highly peaked when the agent was trained with supervised learning. This phenomenon led to the trained agent not being able to exploit the advantage of beam search during test time. We address this issue by regularizing the negative entropy of the action distribution. This is done by adding the following regularization term to the original cost function in Equation (2.20):
= − |˜| ∑︁ =1 H( ( |˜&lt; )), (2.22)
where is a regularization coefficient, and is a random variable corresponding to a set of actions including all the outgoing edges and the stop action.</p>
<p>Training Strategies: Reinforcement Learning</p>
<p>The very same agent, NeuAgent, can be trained instead to maximize the final reward without any supervision at each time step on which outgoing edge it should follow. In this case, we only use a query from each example (see Equation
(2.3).)
Given a query, the NeuAgent navigates the graph, starting from the starting node , until it issues the stop action or the number of hops reaches the predefined maximum number of hops ℎ . The last node in which the agent was halted (either due to its own action or not) is considered an answer node, and the reward ( , ) is computed.</p>
<p>In this setup, we train the NeuAgent using the Q-Learning [110] algorithm. The goal of Q-Learning is to learn to predict the expected reward for a given state-action pair, ( , ), where is the current state the agent is in, and is one of the possible actions at the current state.</p>
<p>The optimal action-value function obeys the Bellman equation, which states that, given that the optimal value * ( +1 , ′ ) of the sequences ′ at the next time-step is known for all possible actions ′ , the optimal strategy can be obtained by selecting the action ′ of the next state +1 that maximizes the expected future reward
+ * ( +1 , ′ ), * ( , ) = E[ + max ′ ( +1 , ′ )| , ], (2.23)
where is a discount factor for future rewards. As in many reinforcement learning algorithms, the optimal policy can be achieved through the following update rule, derived from the Bellman equation:
( , ) ← ( , ) + (︂ + max ′ ( +1 , ′ ) − ( , ) )︂ , (2.24)
where is the learning rate and ( +1 , ′ ) is known as the target Q-value. The action-value ( , ) can be estimated for each state-action pair separately, which is impractical for our task due to the large state and action spaces, or it can be estimated using a function approximator. There are many choices for a function approximator and, in this work, we use the same neural network of the NeuAgent to do so, except that, instead of computing the probabilities for the actions as in the supervised case, the agent now computes the expected future rewards for the stateaction pairs. In practice, the NeuAgent can be modified to output these expectations by simply replacing the exponential functions in eqs. (2.6) and (2.7) by sigmoid functions, thus bounding the Q-values between zero and one, which are the reward limits:
( , ) = (ℎ , , ) = (︁ ( ) ⊤ h )︁ (2.25) ( , ∅) = (ℎ , ∅) = (︁ v ⊤ ∅ h )︁ (2.26)
The parameters can be learned by minimizing the following loss function:
RL = E^,^[( − (^,^; )) 2 ], (2.27) where = E^,^[ + max ′ ( +1 , ′ )].
Since RL is differentiable with respect to , this cost can be minimized using SGD:
← − ∇ RL , (2.28)
where the gradients ∇ RL are computed using backpropagation. While ( , ) is iteratively updated, the agent chooses the action with highest ( , ) to maximize its expected future rewards. However, greedily following only the actions that promise maximum reward does not allow the agent to visit states whose initial predictions were associated with low rewards, but that could lead to higher rewards if visited. Thus, in order to balance the trade-off between exploration and exploitation, an -greedy policy [103] is used, in which a random action is selected at each step with probability .</p>
<p>Following Mnih et al. [69], we make use of the experience replay mechanism [61], in which the agent's experiences are stored at each time step to be later sampled and used in the Q-learning updates. This smooths the training distribution and therefore lead to a more stable training and faster convergence. Prioritized Sweeping [71] is used to sample the experiences, making experiences associated with positive rewards have a higher chance of being replayed than the ones associated with no reward. The technique makes convergence faster as the agent sees good examples more often.</p>
<p>Supervised + Reinforcement Learning:</p>
<p>During the initial phase of training, most of the sampled traces will lead to a zero reward. 6 This problem is significantly amplified in the large-scale goal-driven web navigation where both of the state and action spaces are very large. In order to avoid this issue of slow start, we first train an agent with supervised learning, which helps put high probabilities correct/supervised paths. Then, the agent is further fine-tuned with the Q-Learning algorithm, which teaches the agent how to deal with unseen nodes. Since the agent pretrained with supervised learning already learned reasonable policies, the exploration factor (i.e., in the -greedy policy) can be set to a small fixed value throughout the Q-learning finetuning phase ( = 0.1, in our experiments).</p>
<p>Human Evaluation</p>
<p>One unique aspect of the navigation task is that it is very difficult for an average person who was not trained specifically for finding information by navigating through an information network. There are a number of reasons behind this difficulty. First, the person must be familiar with, via training, the graph structure of the network, and this often requires many months, if not years, of training. Second, the person must have in-depth knowledge of a broad range of topics in order to make a connection via different concepts between the themes and topics of a query to a target node. Third, each trial requires the person carefully to read the whole content of the nodes as she navigates, which is a time-consuming and exhausting job.</p>
<p>Thus, unlike many other tasks in which the average human is often the upper-bound of the performance, the navigation task is challenging as well as interesting, as the progress in developing algorithms and models for artificial agents is not bounded by human intelligence. Nevertheless, in this work, we present the performance of human volunteers to put the performances of the proposed NeuAgents in perspective.</p>
<p>We asked five volunteers to try up to 20 four-sentence-long queries 7 randomly selected from the test sets of WikiNav-{4, 8, 16}-4 datasets. They were given up to two hours, and they were allowed to choose up to the same maximum number of explored edges per node as the NeuAgents (that is, = 4), and also were given the option to give up. The average reward was computed as the fraction of correct trials over all the queries presented.</p>
<p>Experiments and Analysis</p>
<p>WikiNav: Quantitative Analysis</p>
<p>We report in Table 2.4 the performance of the NeuAgent-FF and NeuAgent-Rec models on the test set of all nine WikiNav-{4, 8, 16}-{1, 2, 4} datasets. In addition to the proposed NeuAgents, we also report the results of the human evaluation. We clearly observe that the level of difficulty is indeed negatively correlated with the query length but is positively correlated with the maximum number of allowed hops ℎ . The latter may be considered trivial, as the size of the search space grows exponentially with respect to ℎ , but the former is not. The former negative correlation confirms that it is indeed easier to solve the task with more information in a query.</p>
<p>The NeuAgent-FF and NeuAgent-Rec shares similar performance when the maximum number of allowed hops is small ( ℎ = 4), but NeuAgent-Rec ((a) vs. (b))  performs consistently better for higher ℎ , which indicates that having access to history helps in long-term planning tasks. We also observe that the larger and deeper NeuAgent-Rec ((b) vs. (c)) significantly outperforms the smaller one, when a target node is further away from the starting node . Training with supervised learning and then finetuning the with reinforcement learning improves the performance even further ((e) vs. (f)). Contrary to Mnih et al. [69], who had to freeze the weights of the target network ( +1 , ′ ) for many minibatch updates to achieve stability, using the parameters of the previous iteration for the target network was enough to achieve a stable training in our experiments. We conjecture that this is because the weights of the pretrained network are already at a good local minimum, making training more stable and thus eliminating the need for the trick.</p>
<p>The performance difference between simpler and more sophisticated models is more evident as the difficulty of the task increases ( ↓ and ℎ ↑). For instance, the best performing models in (e) and (f) use a multi-layer LSTM and attention-based representation for query and content.</p>
<p>In Figure 2.3, we present an example of how the attention weights over the query words dynamically evolve as the model navigates toward a target node. We note that higher weights are assigned the to most relevant words in the query (in the example, Kentucky, Derby, and race) that will lead to the correct document.</p>
<p>The human participants generally performed worse than the NeuAgents. We attribute this to a number of reasons. First, the NeuAgents are trained specifically on the target domain (Wikipedia), while the human participants have not been. Second, we observed that the volunteers were rapidly exhausted from reading multiple articles in sequence. </p>
<p>WikiNav: Qualitative Analysis</p>
<p>In Table 2.5, we present a few example runs by NeuAgent model (d) from Table 2.4 trained on the WebNav-8-1. In those two successful runs, we see that the agent was able to plan its trajectory correctly. For instance, in the second successful example, the agent starts with a broader theme of the query sentence, "government" and narrows down toward more specific themes (i.e., "the United States" → "the Confederate States" → its "electoral college").</p>
<p>Even in the cases of failure, we observe that the agent is able to navigate through relevant nodes rather than going completely off the topic. Again, the second failed run exhibits a pattern that is intuitively understandable. There are two major themes in the query sentence, which are "random process" and "human mobility." The agent starts by the theme of "random process," following through nodes related to "applied mathematics." The random process in this query was described as "predictable," and the agent correctly noticed that "stable process" which can be considered "predictable." However, the agent failed to find a page in which the remaining theme ("human mobility") occurs together with this "predictable random process. " These examples illustrate that to find correct nodes the agent must have at least partial understanding of how terms relate to each other, which promises a large Query Young adults are the most likely age group to smoke, with a marked decline in smoking rates with increasing age.  potential for using this web navigation agent as a retrieval mechanism. </p>
<p>WikiNav-Jeopardy</p>
<p>Result and Analysis:</p>
<p>In Table 2.6, we report the results on WikiNav-Jeopardy. The proposed NeuAgent clearly outperforms all the three search-based strategies, when it was pretrained on the WikiNav-16-4. The superiority of the pretrained NeuAgent is more apparent when the number of candidate documents is constrained to be small, implying that the NeuAgent is able to rank a correct target article accurately.</p>
<p>Although the NeuAgent performs comparably to the other search-based strategy even without pretraining, the benefit of pretraining on the much larger WikiNav is clear. We emphasize that these search-based strategies have access to all the nodes for each input query. The NeuAgent, on the other hand, only observes the nodes as it visits during navigation. This success clearly demonstrates a potential in using the proposed NeuAgent pretrained with a dataset compiled by the proposed WebNav for the task of focused crawling [2,17].</p>
<p>Related Work</p>
<p>Goal-Driven Web Navigation</p>
<p>This work is indeed not the first to notice the possibility of a website, or possibly the whole web, as a world in which intelligent agents, including ourselves, explore to achieve a certain goal. One most relevant recent work to ours is perhaps Wikispeedia from West and Leskovec [111,112], West et al. [113].</p>
<p>West et al. proposed the following game, called Wikispeedia. The game's world is nearly identical to the goal-driven navigation task proposed in this work. More specifically, they converted "Wikipedia for Schools", 9 , which contains approximately 4,000 articles and 120,000 hyperlinks as of 2008, into a graph whose nodes are articles and directed edges are hyperlinks. From this graph, a pair of nodes is randomly selected and provided to an agent, be it a person or an artificial agent.</p>
<p>The agent's goal is to start from the first node, navigate the graph and reach the second (target) node. Similarly to the WikiNav, the agent has access to the text content of the current nodes and all the immediate neighboring nodes. One major difference is that the target is given as a whole page rather than a sentence, meaning that there is a single target node in the Wikispeedia while there may be multiple target nodes in the proposed WikiNav or any goal-driven web navigation created by the WebNav.</p>
<p>From this description, we see that the goal-driven web navigation is a generalization and re-framing of the Wikispeedia by West et al. First, we let a query contain less information, making it much more difficult for an agent to navigate to a target node without language understanding and planning capabilities. Furthermore, a major research question by West and Leskovec [112] was to "understand how humans navigate and find the information they are looking for ," whereas in this work we are focused on building novel and better retrieval mechanisms.</p>
<p>Recently Narasimhan et al. [73] proposed to incorporate natural language understanding and planning into a single problem. They consider multi-user dungeon (MUD) games as a target task, in which the world is only partially observed as natural language instructions. Furthermore, the actions are often defined by natural language sentences as well.</p>
<p>The proposed goal-driven web navigation, more specifically WikiNav, is similar to MUD games. A major difference is in the complexity of the task. For instance, the goal-driven web navigation built from a real website, such as the WikiNav proposed here, uses a vocabulary of approximately 370k unique words, while that of the "Fantasy World" from Narasimhan et al. [73] contains a substantially smaller number of words (1,340).</p>
<p>Also related are the personalized PageRank algorithms [40,41,47,51]. The idea is to bias PageRank vectors according to user profiles, topics, or queries. The large cost of computing these vectors poses a challenge in tasks where the bias vector frequently changes, such as in each new question of a question-answering task. Our agent scales well in an online setting as only a few dozen links are evaluated at each navigational step.</p>
<p>Focused Crawling</p>
<p>Agents trained on the task of large-scale goal-driven web navigation can be readily applied to a number of applications. Perhaps the most important one is to use any technology built for solving this task as a focused crawler, or part of it. A focused crawler aims at crawling websites with a predefined, specific topic, unlike traditional crawlers whose aim is to index all possible web pages [2,17] that later will be retrieved based on the terms present in the input query and certain metrics, such as PageRank [80]. This is an interesting problem, as much of the content available on the Internet is either hidden or dynamically generated, meaning that they need to be searched on-the-fly [2]. Focused crawling is also more efficient since fewer documents are visited and analyzed. If we consider the query in the proposed goal-driven web navigation as an unstructured form of topics, the agent trained to solve the goal-driven navigation can readily be applied to this focused crawling. Our method can, additionally, learn textual representations and crawling strategies directly from data in an end-to-end training process.</p>
<p>The idea of learning to navigate information networks conditioned on a query was proposed in various earlier works. Rennie et al. [84], for example, introduced an agent that can learn the focused crawling task through reinforcement learning. However, their method was evaluated in two small datasets restricted to the domains of computer science departments and company websites, and the queries were single topics. We, instead, approach the focused crawling task by making our agent navigate a much larger informational graph and use complex natural language questions from the Jeopardy! game as queries.</p>
<p>Along the same lines, Meusel et al. [63] use online-based classification algorithms in combination with a bandit-based selection strategy to efficiently crawl pages with markup languages such as RDFa, Microformats, and Microdata. In contrast, our agent makes its decisions based only on the natural language description of the web pages, without the need for structured content.</p>
<p>Summary</p>
<p>In this chapter, we described a large-scale goal-driven web navigation agent and argue that it serves as an alternative for index-based search engines. We release a software tool, called WebNav, that compiles a given website into a goal-driven web navigation task. As an example, we construct WikiNav from Wikipedia using WebNav. We extend WikiNav with Jeopardy! questions, thus creating WikiNav-Jeopardy. We evaluate various neural net based agents on WikiNav, an information retrieval task, and WikiNav-Jeopardy, a question-answering task. Our results show that our agent pretrained on WikiNav outperforms two strong inverted index-based search engines on the WikiNav-Jeopardy. These empirical results support our claim on the usefulness of the navigation agents in challenging applications such as document retrieval, question answering, and focused crawling.</p>
<p>Chapter 3</p>
<p>Retrieving from a Black Box Query Reformulation with Reinforcement Learning</p>
<p>In the previous chapter, we described a retrieval agent that navigates a web of documents to find information. Despite its novelty, this agent requires a corpus of densely-linked documents to work well. Wikipedia has this property but the web, in general, lack of. Training an agent to handle all sorts of different web-page designs, especially dynamic-generated content, would require an enormous engineering effort. What if, instead, we could use search engines to do the heavy lifting of crawling, storing, and retrieving information in a standard format that a learning agent would then ingest and produce new results? This is possible, but there are some problems with existing search engines that we must be aware of. For example, when we request some information using a long or inexact description of it, these systems often fail to deliver relevant items. In this case, what typically follows is an iterative process in which we try to express our need differently in the hope that the system will return what we want. This is a major issue in information retrieval. For instance, Huang and Efthimiadis [44] estimate that 28-52% of all the web queries are modifications of previous ones.</p>
<p>To a certain extent, this problem occurs because search engines rely on matching terms in the query with terms in documents, to perform retrieval. If there is a mismatch between them, a relevant document may be missed.</p>
<p>One way to address this problem is to automatically rewrite a query so that it becomes more likely to retrieve relevant documents. This technique is known as automatic query reformulation. It typically expands the original query by adding terms from, for instance, dictionaries of synonyms such as WordNet [65], or from the A set of documents 0 is retrieved from a search engine using the initial query 0 . Our reformulator selects terms from 0 and 0 to produce a reformulated query ′ which is then sent to the search engine. Documents ′ are returned, and a reward is computed against the set of relevant documents. The reformulator is trained with reinforcement learning to produce a query, or a series of queries, to maximize the expected return.</p>
<p>initial set of retrieved documents [116]. This latter type of reformulation is known as pseudo (or blind) relevance feedback (PRF), in which the relevance of each term of the retrieved documents is automatically inferred. The proposed method is built on top of PRF but differs from previous works as we frame the query reformulation problem as a reinforcement learning (RL) problem. An initial query is the natural language expression of the desired goal, and an agent (i.e., reformulator) learns to reformulate an initial query to maximize the expected return (i.e., retrieval effectiveness) through actions (i.e., selecting terms for a new query). The environment is a search engine which produces a new state (i.e., retrieved documents). Our framework is illustrated in Figure 3.1.</p>
<p>The most important implication of this framework is that a search engine is treated as a black box that an agent learns to use in order to retrieve more relevant items. This opens the possibility of training an agent to use a search engine for a task other than the one it was originally intended for. To support this claim, we evaluate our agent on the task of question answering (Q&amp;A), citation recommendation, and passage/snippet retrieval.</p>
<p>As for training data, we use two publicly available datasets (TREC-CAR and Jeopardy) and introduce a new one (MS Academic) with hundreds of thousands of query/relevant document pairs from the academic domain.</p>
<p>Furthermore, we present a method to estimate the upper bound effectiveness of our RL-based model. Based on the estimated upper bound, we claim that this framework has a strong potential for future improvements.</p>
<p>Here we summarize our main contributions:</p>
<p>• A reinforcement learning framework for automatic query reformulation.</p>
<p>• A simple method to estimate the upper-bound effectiveness of an RL-based model in a given environment.</p>
<p>• A new large dataset with hundreds of thousands of query/relevant document pairs. 1</p>
<p>A Reinforcement Learning Approach</p>
<p>Model Description</p>
<p>In this section, we describe the proposed method, illustrated in Figure 3.2.</p>
<p>The inputs are a query 0 consisting of a sequence of words ( 1 , ..., ) and a candidate term with some context words ( − , ..., + ), where ≥ 0 is the context window size. Candidate terms are from 0 ∪ 0 , the union of the terms in the original query and those from the documents 0 retrieved using 0 .</p>
<p>We use a dictionary of pretrained word embeddings [64] to convert the symbolic terms and to their vector representations and ∈ R , respectively. We map out-of-vocabulary terms to an additional vector that is learned during training.</p>
<p>We convert the sequence { } to a fixed-size vector ( ) by using either a Convolutional Neural Network (CNN) followed by a max pooling operation over the entire sequence [52] or by using the last hidden state of a Recurrent Neural Network (RNN). 2 Similarly, we fed the candidate term vectors to a CNN or RNN to obtain a vector representation ( ) for each term . The convolutional/recurrent layers serve an important role in capturing context information, especially for out-of-vocabulary and rare terms. CNNs can process candidate terms in parallel, and, therefore, are faster for our application than RNNs. RNNs, on the other hand, can encode longer contexts. Finally, we compute the probability of selecting as:
CNN/RNN CNN/RNN Candidate Terms q 0 ⋃ D 0 Original Query q 0 W P(t i | q 0 ) + U V + S v 1 v 2 v n ... e i+2 e i+1 e i e i-1 e i-2 Value Network w 1 w 2 w n ... t i-2 t i-1 t i t i+1 t i+2( | 0 ) = ( T tanh( ( ( )‖ ( )) + )), (3.1)
where is the sigmoid function, ‖ is the vector concatenation operation, ∈ R ×2 and ∈ R are weights, and ∈ R is a bias.</p>
<p>At test time, we define the set of terms used in the reformulated query as =
{ | ( | 0 ) &gt; },
where is a hyperparameter. At training time, we sample the terms according to their probability distribution:
= { | = 1 ∧ ∼ ( | 0 )}. (3.2)
We concatenate the terms in to form a reformulated query ′ , which will then be used to retrieve a new set of documents ′ .</p>
<p>Sequence Generation</p>
<p>One problem with the method previously described is that terms are selected independently. This may result in a reformulated query that contains duplicated terms since the same term can appear multiple times in the feedback documents. Another problem is that the reformulated query can be very long, resulting in a slow retrieval.</p>
<p>To solve these problems, we extend the model to sequentially generate a reformulated query, as proposed by Buck et al. [14]. We use a Recurrent Neural Network (RNN) that selects one term at a time from the pool of candidate terms and stops when a special token is selected. The advantage of this approach is that the model can remember the terms previously selected through its hidden state. It can, therefore, produce more concise queries.</p>
<p>We define the probability of selecting as the k-th term of a reformulated query as:
( | 0 ) ∝ exp( ( ) T ℎ ),(3.3)
where ℎ is the hidden state vector at the k-th step, computed as:
ℎ = tanh( ( ) + ( −1 ) + ℎ ℎ −1 ),(3.4)
where −1 is the term selected in the previous step and ∈ R × , ∈ R × , and ℎ ∈ R × are weight matrices. In practice, we use an LSTM [43] to encode the hidden state as this variant is known to perform better than a vanilla RNN.</p>
<p>We avoid normalizing over a large vocabulary by using only terms from the retrieved documents. This makes inference faster and training practical since learning to select words from the whole vocabulary might be too slow with reinforcement learning, although we leave this experiment for the future.</p>
<p>Training</p>
<p>We train the proposed model using REINFORCE [114] algorithm. The per-example stochastic objective is defined as
= ( −¯) ∑︁ ∈ − log ( | 0 ), (3.5)
where is the reward, and¯is the baseline, computed by the value network as:
= ( T tanh( ( ( )‖¯) + )), (3.6) where¯= 1 ∑︀ =1 ( ), = | 0 ∪ 0 |, ∈ R ×2
and ∈ R are weights and ∈ R is a bias. We train the value network to minimize
= || −¯|| 2 , (3.7)
where is a small constant (e.g., 0.1) multiplied to the loss in order to stabilize learning. We conjecture that the stability is due to the slowly evolving value network, which directly affects the learning of the policy. This effectively prevents the value network from fitting extreme cases (unexpectedly high or low reward.) We minimize and using stochastic gradient descent (SGD) with the gradient computed by backpropagation [90]. This allows the entire model to be trained end-toend directly to optimize the retrieval effectiveness.</p>
<p>Entropy Regularization: Similar to experiments in Chapter 2, we observed that the probability distribution in Equation (3.1) became highly peaked in preliminary experiments. This phenomenon led to the trained model not being able to explore new terms that could lead to a better-reformulated query. We address this issue by regularizing the negative entropy of the probability distribution. We add the following regularization term to the original cost function in Equation (3.5):
= − ∑︁ ∈ 0 ∪ 0 ( | 0 ) log ( | 0 ), (3.8)
where is a regularization coefficient.</p>
<p>Related Work</p>
<p>Query reformulation techniques are either based on a global method, which ignores a set of documents returned by the original query, or a local method, which adjusts a query relative to the documents that initially appear to match the query. In this work, we focus on local methods.</p>
<p>A popular instance of a local method is the relevance model, which incorporates pseudo-relevance feedback into a language model form [59]. The probability of adding a term to an expanded query is proportional to its probability of being generated by the language models obtained from the original query and from the document in which the term occurs. This framework has the advantage of not requiring pairs of query and relevant document as training data since inference is based on word co-occurrence statistics.</p>
<p>Unlike the relevance model, algorithms can be trained with supervised learning, as proposed by Cao et al. [15]. A training dataset is automatically created by labeling each candidate term as relevant or not based on their individual contribution to the retrieval effectiveness. Then a binary classifier is trained to select expansion terms. In Section 3.3, we present a neural network-based implementation of this supervised approach.</p>
<p>An alternative for this supervised framework is to iteratively reformulate the query by selecting one candidate term at each retrieval step. This can be viewed as navigating a graph where the nodes represent queries and associated retrieved results and edges exist between nodes whose queries are simple reformulations of each other [29]. However, it can be slow to reformulate a query this way as the search engine must be queried for each newly added term. Our method, on the contrary, queries the search engine with various new terms at once.</p>
<p>Another technique based on supervised learning is to learn a common latent representation of queries and relevant documents terms by using a click-through dataset [100]. Neighboring document terms of a query in the latent space are selected to form an expanded query. Instead of using a click-through dataset, which is often proprietary, it is possible to use an alternative dataset consisting of pairs of web page title and anchor text.</p>
<p>Queries can also be expanded with terms that are close in the embedding space [56,88]. In this case, word embeddings trained with feedback documents as negative examples result in expanded queries that are more effective than when embeddings trained with corpus-level negatives are used [31].</p>
<p>Perhaps the closest work to ours is that by Narasimhan et al. [74], in which a reinforcement learning based approach is used to reformulate queries iteratively. A key difference is that in their work, the reformulation component uses domain-specific template queries. Our method, on the other hand, assumes open-domain queries.</p>
<p>Experiments</p>
<p>In this section, we describe our experimental setup, including baselines against which we compare the proposed method, metrics, reward for RL-based models, datasets, and implementation details.</p>
<p>Baseline Methods</p>
<p>Raw:</p>
<p>The original query is given to a search engine without any modification. We evaluate two search engines in their default configuration: Lucene 3 with BM25 [85] as the ranking function (Raw-BM25) and Google Search 4 (Raw-Google).</p>
<p>Pseudo-Relevance Feedback (PRF-TFIDF):</p>
<p>A query is expanded with terms from the documents retrieved by a search engine using the original query. In this work, the top-TF-IDF terms from each of the top-retrieved documents are added to the original query, where and are selected by a grid search on the validation data.</p>
<p>PRF-Relevance Model (PRF-RM):</p>
<p>This is our implementation of the relevance model for query expansion [59]. The probability of adding a term to the original query is given by:
( | 0 ) = (1 − ) ′ ( | 0 ) + ∑︁ ∈ 0 ( ) ( | ) ( 0 | ),(3.9)
where ( ) is the probability of retrieving the document , assumed uniform over the set, ( | ) and ( 0 | ) are the probabilities assigned by the language model obtained from to and 0 , respectively. ′ ( | 0 ) = tf( ∈ ) | | , where tf( , ) is the term frequency of in . We set the interpolation parameter to 0.65, which was the best value found by a grid-search on the development set.</p>
<p>We use a Dirichlet smoothed language model [121] to compute a language model from a document ∈ 0 :
( | ) = tf( , ) + ( | ) | | + , (3.10)
where is a scalar constant ( = 1500 in our experiments), and ( | ) is the probability of occurring in the entire corpus . We use the terms with the highest ( | 0 ) in an expanded query, where = 100 was the best value found by a grid-search on the development set.</p>
<p>Embeddings Similarity: Inspired by the methods proposed by Roy et al. [88] and Kuzi et al. [56], the top-terms are selected based on the cosine similarity of their embeddings against the original query embedding. Candidate terms come from documents retrieved using the original query (PRF-Emb), or from a fixed vocabulary (Vocab-Emb). We use pretrained embeddings from Mikolov et al. [64], and it contains 374,000 words.</p>
<p>Proposed Methods</p>
<p>Supervised Learning (SL):</p>
<p>Here we detail a deep learning-based variant of the method proposed by Cao et al. [15]. It assumes that query terms contribute independently to the retrieval effectiveness. We thus train a binary classifier to select a term if the retrieval effectiveness increases beyond a preset threshold when that term is added to the original query. More specifically, we mark a term as relevant if ( ′ − )/ &gt; 0.005, where and ′ are the retrieval effectiveness of the original query and the query expanded with the term, respectively.</p>
<p>We experiment with two variants of this method: one in which we use a convolutional network for both original query and candidate terms (SL-CNN), and the other in which we replace the convolutional network with a single hidden layer feed-forward neural network (SL-FF). In this variant, we average the output vectors of the neural network to obtain a fixed size representation of 0 .</p>
<p>Reinforcement Learning (RL):</p>
<p>We use multiple variants of the proposed RL method. RL-CNN and RL-RNN are the models described in Section 3.1.1, in which the former uses CNNs to encode query and term features, and the latter uses RNNs (more specifically, bidirectional LSTMs). RL-FF is the model in which term and query vectors are encoded by a single hidden layer feed-forward neural network. In the  </p>
<p>Datasets</p>
<p>We summarize in Table 3.1 the datasets.</p>
<p>TREC -Complex Answer Retrieval (TREC-CAR):</p>
<p>This is a publicly available dataset automatically created from Wikipedia whose goal is to encourage the development of methods that respond to more complex queries with longer answers [33]. A query is the concatenation of an article title and one of its section titles. The relevant documents are the paragraphs within that section. For example, a query is "Sea Turtle, Diet" and the relevant documents are the paragraphs in the section "Diet" of the "Sea Turtle" article. The corpus consists of all the English Wikipedia paragraphs, except the abstracts. The released dataset has five predefined folds, and we use the first three as the training set and the remaining two as validation and test sets, respectively.</p>
<p>Jeopardy: This is a publicly available Q&amp;A dataset introduced in Chapter 2. A query is a question from the Jeopardy! TV Show, and the corresponding document is a Wikipedia article whose title is the answer. For example, a query is "For the last eight years of his life, Galileo was under house arrest for espousing this manâĂŹs theory" and the answer is the Wikipedia article titled "Nicolaus Copernicus". The corpus consists of all the articles in the English Wikipedia.</p>
<p>Microsoft Academic (MSA):</p>
<p>This dataset consists of academic papers crawled from Microsoft Academic API. 5 The crawler started at the paper Silver et al. [97] and traversed the graph of references until 500,000 papers were crawled. We then removed papers that had no reference within or whose abstract had less than 100 characters. We ended up with 480,000 papers.  A query is the title of a paper, and the relevant answer consists of the papers cited within. Each document in the corpus consists of its title and abstract. 6 This dataset differs from the other two in that it uses different corpus and terminologies, and it has more relevant documents per query, thus favoring reformulation methods that produce more comprehensive queries.</p>
<p>Metrics and Reward</p>
<p>Three metrics are used to evaluate effectiveness:</p>
<p>Recall@K:</p>
<p>Recall of the top-K retrieved documents:
R@ = | ∩ * | | * | ,(3.11)
where are the top-retrieved documents and * are the relevant documents. Since one of the goals of query reformulation is to increase the proportion of relevant documents returned, recall is our main metric.</p>
<p>Precision@K: Precision of the top-K retrieved documents:
P@ = | ∩ * | | | (3.12)
Precision captures the proportion of relevant documents among the returned ones.</p>
<p>Despite not being the main goal of a reformulation method, improvements in precision are also expected with a good query reformulation method. Therefore, we include this metric.</p>
<p>Mean Average Precision:</p>
<p>The average precision is defined as:
AP = ∑︀ P@ × rel( ) | * | , (3.13) where rel( ) = ⎧ ⎪ ⎨ ⎪ ⎩ 1, if the k-th document is relevant; 0, otherwise.
(3.14)</p>
<p>The mean average precision of a set of queries is then:
MAP = 1 | | ∑︁ ∈ AP ,(3.15)
where AP is the average precision for a query . This metric values the position of a relevant document in a returned list and is, therefore, complementary to precision and recall.</p>
<p>Reward:</p>
<p>We use R@ as a reward when training the proposed RL-based models as this metric has shown to be effective in improving the other metrics as well.</p>
<p>SL-Oracle:</p>
<p>In addition to the baseline methods and the proposed reinforcement learning approach, we report two oracle effectiveness bounds. The first oracle is a supervised learning oracle (SL-Oracle). It is a classifier that perfectly selects terms that will increase effectiveness according to the procedure described in Section 3.3.2. This measure serves as an upper-bound for the supervised methods. Notice that this heuristic assumes that each term contributes independently from all the other SL-Oracle 13% 5% 11% RL-Oracle 29% 27% 31% Table 3.3: Percentage of relevant terms over all the candidate terms according to SLand RL-Oracle.</p>
<p>terms to the retrieval effectiveness. There may be, however, other ways to explore the dependency of terms that would lead to higher effectiveness.</p>
<p>RL-Oracle:</p>
<p>Second, we introduce a reinforcement learning oracle (RL-Oracle) which estimates a conservative upper-bound effectiveness for the RL models. Unlike the SL-Oracle, it does not assume that each term contributes independently to the retrieval effectiveness. It works as follows: first, the validation or test set is divided into small subsets { } =1 (each with 100 examples, for instance). An RL model is trained on each subset until it overfits, that is, until the reward * stops increasing or an early stop mechanism ends training. 7 Finally, we compute the oracle effectiveness * as the average reward over all the subsets:
* = 1 ∑︀ =1 * .
This upper bound by the RL-Oracle is, however, conservative since there might exist better reformulation strategies that the RL model was not able to discover.</p>
<p>Implementation Details</p>
<p>Search engine: We use Lucene as the search engine and BM25 as the ranking function for all PRF, SL, and RL methods. For Raw-Google, we restrict the search to the wikipedia.org domain when evaluating its effectiveness on the Jeopardy dataset. We could not apply the same restriction to the two other datasets as Google does not index Wikipedia paragraphs, and as it is not trivial to match papers from MS Academic to the ones returned by Google Search.  At training time of an RL model, we use only one document uniformly sampled from the top-retrieved ones as a source for candidate terms, as this leads to faster learning.</p>
<p>For the PRF methods, the top-terms according to a relevance metric (i.e., TF-IDF for PRF-TFIDF, cosine similarity for PRF-Emb, and conditional probability for PRF-RM) from each of the top-retrieved documents are added to the original query. We select and using grid search over {10, 50, 100, 200, 300, 500} and {1, 3, 5, 9, 11}, respectively. The best values are = 300 and = 9.</p>
<p>Multiple Reformulation Rounds: Although our framework supports multiple rounds of search and reformulation, we did not find any significant improvement in reformulating a query more than once. Therefore, the numbers reported in the results section were all obtained from models running two rounds of search and reformulation.</p>
<p>Neural Network Setup: For SL-CNN and RL-CNN variants, we use a 2-layer convolutional network for the original query. Each layer has a window size of 3 and 256 filters. We use a 2-layer convolutional network for candidate terms with window sizes of 9 and 3, respectively, and 256 filters in each layer. We set the dimension of the weight matrices , , , and to 256. For the optimizer, we use ADAM [53] with = 10 −4 , 1 = 0.9, 2 = 0.999, and = 10 −8 . We set the entropy regularization coefficient to 10 −3 .</p>
<p>For RL-RNN and RL-RNN-SEQ, we use a 2-layer bidirectional LSTM with 256 hidden units in each layer. We clip the gradients to unit norm. For RL-RNN-SEQ, we set the maximum possible number of generated terms to 50, and we use beam search of size four at test time.</p>
<p>We fix the dictionary of pretrained word embeddings during training, except the vector for out-of-vocabulary words. We found that this led to faster convergence and observed no difference in the overall effectiveness when compared to learning embeddings during training. Table 3.2 shows the main result. As expected, reformulation based methods work better than using the original query alone. Supervised methods (SL-FF and SL-CNN) have in general better effectiveness than unsupervised ones (PRF-TFIDF, PRF-RM, PRF-Emb, and Emb-Vocab), but perform worse than RL-based models (RL-FF, RL-CNN, RL-RNN, and RL-RNN-SEQ).</p>
<p>Results and Discussion</p>
<p>RL-RNN-SEQ performs slightly worse than RL-RNN but produces queries that are three times shorter, on average (15 vs. 47 words). Thus, RL-RNN-SEQ is faster in retrieving documents and therefore might be a better candidate for a production implementation.</p>
<p>The effectiveness gap between the oracle and best performing method (Table 3.2, RL-Oracle vs. RL-RNN) suggests that there is a large room for improvement. The cause for this gap is unknown, but we suspect, for instance, an inherent difficulty in learning a good selection strategy and the partial observability from using a black-box search engine.</p>
<p>Relevant Terms per Document</p>
<p>The proportion of relevant terms selected by the SL-and RL-Oracles over the total number of candidate terms (Table 3.3) indicates that only a small subset of terms are useful for the reformulation. Thus, we may conclude that the proposed method was able to learn an effective term selection strategy in an environment where relevant  Figure 3.3 shows the improvement in recall as more candidate terms are provided to a reformulation method. The RL-based model benefits from more candidate terms, whereas the classical PRF method quickly saturates. In our experiments, the best performing RL-based model uses the maximum number of candidate terms that we could fit on a single GPU. We, therefore, expect further improvements with more computational resources.</p>
<p>Scalability: Number of Terms vs Recall</p>
<p>Qualitative Analysis</p>
<p>We show two examples of queries and the probabilities of each candidate term of being selected by the RL-CNN model in Figure 3 Notice that terms that are more related to the query have higher probabilities, although common words such as "the" are also selected. This is a consequence of our choice of a reward that does not penalize the selection of neutral terms.</p>
<p>In Table 3   "serves" and "accreditation". These selections are expected for this task since similar terms can be effective in retrieving similar paragraphs. On the other hand, the model trained on Jeopardy prefers to select proper nouns, such as "Tunxis", as these have a higher chance of being an answer to the question. The model trained on MSA selects terms that cover different aspects of the entity being queried, such as "arts center" and "library", since retrieving a diverse set of documents is necessary for the task the of citation recommendation.</p>
<p>Training and Inference Times</p>
<p>Our best model, RL-RNN, takes 8-10 days to train on a single K80 GPU. At inference time, it takes approximately one second to reformulate a batch of 64 queries. Approximately 40% of this time is to retrieve documents from the search engine.</p>
<p>Scaling Up Query Reformulation</p>
<p>In the previous sections, we presented an agent that learns how to use a search engine by rewriting the original queries. The proposed agent, however, can be improved in two aspects. First, the oracle effectiveness showed that there exist reformulations that would lead to better effectiveness, but the agents were not able to learn how to produce them. Buck et al. [14] report a similar observation. The other potential improvement is training time. For example, the best agent trains in 10 days on a single GPU. There are effective methods to parallelize training using multiple GPUs, but these require careful implementation of how each machine exchange their weights or gradients.</p>
<p>To improve our reformulation method, we are motivated by the observation that in reinforcement learning efficient exploration is key to achieve good effectiveness. The ability to explore in parallel a diverse set of strategies often speeds up training and leads to a better policy [70,79].</p>
<p>In the second half of this chapter, we propose a simple method to achieve efficient parallelized exploration of diverse policies, inspired by hierarchical reinforcement learning [25,32,61,98]. We structure the agent into multiple sub-agents, which are trained on disjoint subsets of the training data. Sub-agents are co-ordinated by a meta-agent, called aggregator, that groups and scores answers from the sub-agents for each given input. Unlike sub-agents, the aggregator is a generalist since it learns a policy for the entire training set.</p>
<p>We argue that it is easier to train multiple sub-agents than a single generalist one since each sub-agent only needs to learn a policy that performs well for a subset of examples. Moreover, specializing agents on different partitions of the data encourages them to learn distinct policies, thus giving the aggregator the possibility to see answers from a population of diverse agents. Learning a single policy that results in an equally diverse strategy is more challenging.</p>
<p>Since each sub-agent is trained on a fraction of the data, and there is no communication between them, training can be done faster than training a single agent on the full data. Additionally, it is easier to parallelize than applying existing distributed algorithms such as asynchronous SGD or A3C [70], as the sub-agents do not need to exchange weights or gradients. After training the sub-agents, only their actions need to be sent to the aggregator.</p>
<p>We show that it outperforms a strong baseline of an ensemble of agents trained on the full dataset. We also found that effectiveness and reformulation diversity are correlated.</p>
<p>Our main contributions for the remaining of this chapter are the following:</p>
<p>• A simple method to achieve more diverse strategies and better generalization effectiveness than a model average ensemble.</p>
<p>• Training can be easily parallelized in the proposed method.</p>
<p>• An interesting finding that contradicts our, perhaps naive, intuition: specializing agents on semantically similar data does not work as well as random partitioning.</p>
<p>Related Work</p>
<p>The proposed approach is inspired by the mixture of experts, which was introduced more than two decades ago [46,48] and has been a topic of intense study since then. The idea consists of training a set of agents, each specializing in some task or data. One or more gating mechanisms then select subsets of the agents that will handle a new input. Recently, Shazeer et al. [96] revisited the idea and showed strong effectiveness in the supervised learning tasks of language modeling and machine translation. Their method requires that output vectors of experts are exchanged between machines. Since these vectors can be large, the network bandwidth becomes a bottleneck. They used a variety of techniques to mitigate this problem. Anil et al. [3] later proposed a method to further reduce communication overhead by only exchanging the probability distributions of the different agents. Our method, instead, requires only scalars (rewards) and short strings (original query, reformulations, and answers) to be exchanged. Therefore, the communication overhead is small.</p>
<p>Previous works used specialized agents to improve exploration in RL [25,50,98]. For instance, Stanton and Clune [101] and Conti et al. [23] use a population of agents to achieve a high diversity of strategies that leads to better generalization effectiveness and faster convergence. Rusu et al. [91] use experts to learn subtasks and later merge them into a single agent using distillation [42].</p>
<p>The experiments are often carried out in simulated environments, such as robot control [12] and video-games [6]. In these environments, rewards are frequently available, the states have low diversity (e.g., same image background), and responses usually are fast (60 frames per second). We, instead, evaluate our approach on tasks whose inputs (queries) and states (documents and answers) are diverse because they are in natural language, and the environment responses are slow (0.5-5 seconds per query).</p>
<p>Somewhat similarly motivated is the work of Serban et al. [95]. They train many heterogeneous response models and further train an RL agent to pick one response per utterance. Figure 3.5-(c) illustrates the new agent. An input query 0 is given to the sub-agents. A sub-agent is any system that accepts as input a query and returns a corresponding reformulation. Thus, sub-agents can be heterogeneous.</p>
<p>Method</p>
<p>Here we train each sub-agent on a partition of the training set. The -th agent queries the underlying search system with reformulation and receives a result . The set {( , )|0 ≤ ≤ } is given to the aggregator, which then decides which result will be final.</p>
<p>Sub-agents:</p>
<p>The first step for training the new agent is to partition the training set. We randomly split it into equal-sized subsets. In our implementation, a sub-agent is a sequence-to-sequence model [21,102] trained on a partition of the dataset. It receives as an input the original query 0 and outputs a list of reformulated queries ( ) using beam search.</p>
<p>Each reformulation is given to the same environment that returns a list of results ( 1 , .., ) and their respective rewards ( 1 , .. ). We then use REINFORCE [114] to train the sub-agent. At training time, instead of using beam search, we sample reformulations. Note that we also add the identity agent (i.e., the reformulation is the original query) to the pool of sub-agents.</p>
<p>Aggregator:</p>
<p>The aggregator receives as inputs 0 and a list of candidate results ( 1 , .. ) for each reformulation . We first compute the set of unique results and two different scores for each result: the accumulated rank score and the relevance score . The accumulated rank score is computed as = ∑︀ =1 1 rank , , where rank , is the rank of the j-th result when retrieved using . The relevance score is the prediction that the result is relevant to query 0 . It is computed as:
= ( 2 ReLU( 1 + 1 ) + 2 ), (3.16) where = [ CNN ( 0 ); BOW ( ); CNN ( 0 ) − BOW ( ); CNN ( 0 ) ⊙ BOW ( )],(3.</p>
<p>17)</p>
<p>1 ∈ R 4 × and 2 ∈ R ×1 are weight matrices, 1 ∈ R and 2 ∈ R 1 are biases. The brackets in [ ; ] represent the concatenation of vectors and . The symbol ⊙ denotes the element-wise multiplication, is the sigmoid function, and ReLU is a Rectified Linear Unit function [72]. The function CNN is implemented as a CNN encoder 8 followed by average pooling over the sequence [52]. The function BOW is the average word embeddings of the result. At test time, the top-K answers with respect to = are returned. We train the aggregator with stochastic gradient descent (SGD) to minimize the cross-entropy loss:
= − ∑︁ ∈ * log( ) − ∑︁ / ∈ * log(1 − ), (3.18)
where * is the set of indexes of the relevant results.</p>
<p>Hyperparameters:</p>
<p>For the sub-agents, we use mini-batches of size 256, ADAM as the optimizer, and learning rate of 10 −4 . For the aggregator, the encoder 0 is a word-level two-layer CNN with filter sizes of 9 and 3, respectively, and 128 and 256 kernels, respectively. = 512. No dropout is used. ADAM is the optimizer with learning rate of 10 −4 and mini-batch of size 64. It is trained for 100 epochs.</p>
<p>Document Retrieval</p>
<p>We now present experiments and results in a document retrieval task. In this task, the goal is to rewrite a query so that the number of relevant documents retrieved by a search engine increases.</p>
<p>The environment and datasets are the same to ones we used to evaluate the single-agent query reformulator, except for the TREC-CAR dataset, in which we use different training, validation and test set splits, namely, we use the first four folds for training, the last fold for validation and the automatic annotations benchmark of 2017 (approx. 1,800 queries) for test.</p>
<p>Baselines:</p>
<p>We use the following methods as baselines:  Table 3.6: MAP on the test set of the document retrieval datasets. ⋆ The weights of the agents are initialized from a single model pretrained on the full training set for 10 days.</p>
<p>BM25:</p>
<p>We give the original query to Lucene in its default configuration with BM25 as the ranking function, and use the retrieved documents as results.</p>
<p>RM3:</p>
<p>We reimplement the relevance model for query expansion of Lavrenko and Croft [59] with a Dirichlet smoothed language model [121], and use the topterms with highest posterior ( | 0 ) as the new query.</p>
<p>RL-RNN:</p>
<p>This is the sequence-to-sequence model trained with reinforcement learning from Section 3.3.2. The reformulated query is formed by appending new terms to the original query. The terms are selected from the documents retrieved using the original query. This agent is trained from scratch.</p>
<p>RL-N-Ensemble:</p>
<p>We train RL-RNN agents with different initial weights on the full training set. At test time, we average the probability distributions of all the agents at each time step and select the token with the highest probability, as done by Sutskever et al. [102].</p>
<p>Proposed Methods: We evaluate the following methods:</p>
<p>RL-N-Full:</p>
<p>We train RL-RNN agents with different initial weights on the full training set. The answers are obtained using the best (greedy) reformulations of all the agents and are given to the aggregator.</p>
<dl>
<dt>RL-N-Bagging</dt>
<dd>
<p>This is the same as RL-N-Full but we construct the training set of each RL-RNN agent by sampling with replacement D times from the full training set, which has a size of D. This is known as the bootstrap sample and leads to approximately 63% unique samples, the rest being duplicates. Note that this not exactly the bagging method [9] because our aggregator is different from the average model prediction proposed in that work.</p>
</dd>
</dl>
<p>RL-N-Sub:</p>
<p>This is the proposed agent. It is similar to RL-N-Full but the multiple sub-agents are trained on random partitions of the dataset (see Figure 3.5-(c)).</p>
<p>Results:</p>
<p>A summary of the document retrieval results is shown in Table 3.6. The proposed methods (RL-10-{Sub, Bagging, Full}) have 20-60% relative retrieval improvement over the standard ensemble (RL-10-Ensemble) while training ten times faster. More interestingly, RL-10-Sub has a better retrieval than the single-agent version (RL-RNN), uses the same computational budget, and trains on a fraction of the time. Lastly, we found that RL-10-Sub (Pretrained) has the best balance between effectiveness and training cost across all datasets.</p>
<p>We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and 2.7 TFLOPS as an estimate of the single-precision floating-point of a K80 GPU.</p>
<p>Since the sub-agents are frozen during the training of the aggregator, we precompute all ( 0 , , , ) tuples from the training set, thus avoiding sub-agent or environment calls. This reduces its training time to less than 6 hours (0.06 × 10 18 FLOPs). Since this cost is negligible when compared to the sub-agents', we do not include it in the table.</p>
<p>Number of Sub-Agents:</p>
<p>We compare the retrieval effectiveness of the full system (reformulators + aggregator) for different numbers of agents in Figure 3.6. The effectiveness of the system is stable across all datasets after more than ten sub-agents are used, thus indicating the robustness of the proposed method.</p>
<p>Comparison of Aggregator Functions:</p>
<p>To validate the effectiveness of the proposed aggregation function, we conducted a comparison study on the TREC-CAR dataset. We present the results in Table 3.7. We notice that removing or changing the accumulated rank or relevance score functions results in a retrieval effectiveness drop between 2-8%. We also experimented concatenating to the input vector (eq. 3.17) a vector to represent each sub-agent. These vectors were learned during training and allow the aggregator to distinguish sub-agents. However, this did not lead to an improvement in effectiveness.</p>
<p>Question Answering</p>
<p>To further assess the effectiveness of the proposed method, we conduct experiments in a question-answering task, comparing our agent with the active question-answering agent proposed by Buck et al. [14].</p>
<p>The environment receives a question as an action and returns an answer as an observation, and a reward computed against a relevant answer. We use BiDAF as the question-answering system [94]. Given a question, it outputs an answer span from a list of snippets. We use as a reward the token level F1 score on the answer (we will define it precisely later on).</p>
<p>We follow Buck et al. [14] to train BiDAF. We emphasize that BiDAF's parameters  </p>
<p>Baselines:</p>
<p>We compare our agent against the following baselines:</p>
<p>BiDAF:</p>
<p>The original question is given to the question-answering system without any modification (see Figure 3.5-(a)).</p>
<p>Re-Ranker and R 3 :</p>
<p>Re-Ranker is the best model from Wang et al. [109]. They use an answer re-ranking approach to reorder the answer candidates generated by a base Q&amp;A model, R 3 [108]. We report both systems' results as a reference. To the best of our knowledge, they are currently the best systems on SearchQA. R 3 alone, without re-ranking, outperforms BiDAF by about 20 F1 points.</p>
<p>AQA: This is the best model from Buck et al. [14]. It consists of a reformulator and a selector. The reformulator is a subword-based sequence-to-sequence model that produces twenty reformulations of an input question using beam search. The reformulations and their answers are given to the selector, which then chooses one of the answers as final (see Figure 3.5-(b)). The reformulator is pretrained on translation and paraphrase data.</p>
<p>Proposed Methods:</p>
<p>AQA-N-{Full, Sub}: Similar to the RL-N-{Full, Sub} models, we use AQA reformulators as the sub-agents followed by an aggregator to create AQA-N-Full and AQA-N-Sub models, whose sub-agents are trained on the full and random partitions of the dataset, respectively.</p>
<p>As for the hyperparameters of the sub-agents, we use mini-batches of size 64, SGD as the optimizer, and learning rate of 10 −3 . For the aggregator, the encoder 0 is a token-level, three-layer CNN with filter sizes of 3, and 128, 256, and 256 kernels, respectively. We train it for 100 epochs with mini-batches of size 64 with SGD and learning rate of 10 −3 .</p>
<p>Evaluation Metrics:</p>
<p>We use the macro-averaged F1 score as the main metric. It measures the average bag of tokens overlap between the prediction and relevant answer. We take the F1 over the relevant answer for a given question and then average over all of the questions. Additionally, we present the oracle effectiveness, which is from a perfect aggregator that predicts = 1 for relevant answers and = 0, otherwise.</p>
<p>Results:</p>
<p>Results are presented in Table 3.8. The proposed methods (AQA-10-{Full,Sub}) have both better F1 and oracle effectiveness than the single-agent AQA method while training in one-tenth of the time. 9 Even when the ensemble method is given ten times more training time (AQA-10-Full, extra budget), our method achieves higher effectiveness.</p>
<p>The best model outperforms BiDAF, which is used in our environment, by almost 16 F1 points. In absolute terms, the proposed method does not reach the effectiveness of the Re-Ranker or underlying R 3 system. It is important to realize, though, that these are orthogonal issues: any Q&amp;A system, including R 3 , could be used as environments, including re-ranking post-processing. We leave this as a future work.</p>
<p>Query Diversity: We also evaluate how query diversity and effectiveness are related, using the following metrics: pCos: Mean pair-wise cosine distance: where is a set of reformulated queries for the -th original query in the development set and # is the token count vector of q.
1 ∑︁ =1 1 | | ∑︁ , ′ ∈ cosine (︁ # , # ′ )︁ ,(3.
pBLEU: Mean pair-wise sentence-level BLEU [18]:
1 ∑︁ =1 1 | | ∑︁ , ′ ∈ BLEU (︁ , ′ )︁ (3.20)
PINC: Mean pair-wise paraphrase in k-gram changes [20]:
1 ∑︁ =1 1 | | ∑︁ , ′ ∈ PINC( , ′ ), (3.21) PINC( , ′ ) = 1 ∑︁ =1 1 − |k-gram ∩ k-gram ′ | |k-gram ′ | ,(3.22)
where is the maximum number of k-grams considered (we use = 4).</p>
<p>Length Std: Standard deviation of the reformulation lengths: Table 3.9 shows that the multiple agents trained on partitions of the dataset (AQA-10-Sub) produce more diverse queries than a single agent with beam search (AQA) and multiple agents trained on the full training set (AQA-10-Full). This suggests that its higher effectiveness can be partly attributed to the higher diversity of the learned policies.
1 ∑︁ =1 std (︁ {| |} | | =1 )︁ (3.23)
Training Stability of Single vs. Multi-Agent: Reinforcement learning algorithms that use non-linear function approximators, such as neural networks, are known to be unstable [36,69,82,105]. Ensemble methods are known to reduce this variance [9,10,38]. Since the proposed method can be viewed as an ensemble, we compare the AQA-10-Sub's F1 variance against a single agent (AQA) on ten runs. Our method has a much smaller variance: 0.20 vs. 1.07. We emphasize that it also has higher effectiveness than the AQA-10-Ensemble. We argue that this higher stability is due to the use of multiple agents. Answers from agents that diverged during training can be discarded by the aggregator. In the single-agent case, answers come from only one, possibly bad, reformulation. Table 3.10 shows four reformulation examples from various methods. The proposed method (AQA-10-Sub) performs better in the first and second examples than the other methods. Note that, despite the large diversity of reformulations, BiDAF still returns the correct answer. In the third example, the proposed method fails to produce the right answer, whereas the other methods perform well. In the fourth example, despite the correct answer is in the set of returned answers, the aggregator fails to set a high score for it.</p>
<p>Reformulation Examples:</p>
<p>Summary</p>
<p>In this chapter, we introduced a reinforcement learning framework for task-oriented automatic query reformulation. An appealing aspect of this framework is that an agent can be trained to use a search engine for a specific task. The empirical evaluation has confirmed that the proposed approach outperforms strong baselines in the three separate tasks.</p>
<p>The analysis based on two oracle approaches has revealed that there is a meaningful room for further development, which motivated the development of the multiple agent framework. In this framework, we proposed a method to build a better query reformulation system by training multiple sub-agents on partitions of the data using reinforcement learning and an aggregator that learns to combine the answers of the multiple agents given a new query. We showed the effectiveness and efficiency of the proposed approach on the tasks of document retrieval and question answering.  </p>
<p>Chapter 4</p>
<p>Looking into the Black Box</p>
<p>Introducing a Pretrained Re-ranker and a Novel</p>
<p>Document Expansion Method</p>
<p>Modern search engines are multi-stage pipelines, with query rewrite, initial index retrieval, and re-ranking being important components. In Chapter 3, we focused on methods to improve a black-box search engine through a novel query rewriting agent. Despite elegant, the separation of an agent and a black-box environment in the reinforcement learning framework might be a poor abstraction for many realworld applications. For example, if we have access to the internal mechanism of the environment, we can alter it to achieve better performance. In this chapter, we look inside the black box and modify two of its components: the re-ranker and inverted index.</p>
<p>In the first half of this chapter, we describe in detail how we have re-purposed BERT as a passage re-ranker and achieved state-of-the-art results on two datasets. In the second half, we will introduce a novel technique to augment documents prior to indexing and show that this enriched index results in better retrieval effectiveness.</p>
<p>A Pretrained Re-ranker</p>
<p>We have seen rapid progress in machine reading compression in recent years with the introduction of large-scale datasets, such as SQuAD [83], MS MARCO [75], SearchQA [34], TriviaQA [49], and QUASAR-T [28], and the broad adoption of neural models, such as BiDAF [94], DrQA [19], DocumentQA [22], and QAnet [120].</p>
<p>The information retrieval (IR) community has also experienced a flourishing development of neural ranking models, such as DRMM [39], KNRM [115], Co-PACRR [45], and DUET [68]. However, until recently, there were only a few large datasets for passage ranking, with the notable exception of the TREC-CAR [33]. This, at least in part, prevented the neural ranking models from being successful when compared to more classical IR techniques [60].</p>
<p>We argue that the same two ingredients that made possible much progress on the reading comprehension task are now available for the ranking task. Namely, the MS MARCO passage ranking dataset, which contains one million queries from real users and their respective relevant passages annotated by humans, and BERT [27], a powerful general-purpose natural language processing model.</p>
<p>Method</p>
<p>The job of the re-ranker is to estimate a score of how relevant a candidate passage is to a query . We use BERT as our re-ranker. Using the same notation used by Devlin et al. [27], we feed the query as sentence A and the passage text as sentence B. We truncate the query to have at most 64 tokens. We also truncate the passage text such that the concatenation of query, passage, and separator tokens have a maximum length of 512 tokens. We use a BERT LARGE model as a binary classification model, that is, we use the [CLS] vector as input to a single layer neural network to obtain the probability of the passage being relevant. We compute this probability for each passage independently and obtain the final list of passages by ranking them with respect to these probabilities.</p>
<p>We start training from a pretrained BERT model and fine-tune it to our re-ranking task using the cross-entropy loss:
= − ∑︁ ∈ pos log( ) − ∑︁ ∈ neg log(1 − ),(4.1)
where pos is the set of indexes of the relevant passages and neg is the set of indexes of non-relevant passages in top-1,000 documents retrieved with BM25. Next, we describe how we train and evaluate our models on two passage-ranking datasets, MS MARCO and TREC-CAR.  </p>
<p>Experiments: MS MARCO</p>
<p>MS MARCO is a passage re-ranking dataset with 8.8M passages obtained from the top-10 results retrieved by the Bing search engine (from 1M queries). The training set contains approximately 500k pairs of query and relevant documents. Each query has one relevant passage, on average. The development and test sets contain approximately 6,900 queries each, but relevance labels are made public only for the development set.</p>
<p>Training</p>
<p>We fine-tune the model using TPUs 1 with a batch size of 32 (32 sequences * 512 tokens = 16,384 tokens/batch) for 400k iterations, which takes approximately 70 hours. This corresponds to training on 12.8M (400k * 32) query-passage pairs. We could not see any improvement in the dev set when training for another 10 days, which is equivalent to seeing 50M pairs in total.</p>
<p>We use ADAM [53] with the initial learning rate set to 3×10 −6 , 1 = 0.9, 2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers.</p>
<p>Experiments: TREC-CAR</p>
<p>This is the same dataset described in Section 3.3, except that we used different training, validation and test set splits, namely, we use the first four folds for training, the last fold for validation and the automatic annotations benchmark of 2017 as the test set.</p>
<p>Training</p>
<p>We follow the same procedure described for the MS MARCO dataset to fine-tune our models on TREC-CAR. However, there is an important difference. The official pretrained BERT models 2 were pretrained on the full Wikipedia, and therefore they have seen, although in an unsupervised way, Wikipedia documents that are used in the test set of TREC-CAR. Thus, to avoid this leak of test data into training, we pretrained the BERT re-ranker only on the half of Wikipedia used by TREC-CAR's training set.</p>
<p>For the finetuning data, we generate our query-passage pairs by retrieving the top ten passages from the entire TREC-CAR corpus using BM25. 3 This means that we end up with 30M example pairs (3M queries * 10 passages/query) to train our model. We train it for 400k iterations, or 12.8M examples (400k iterations * 32 pairs/batch), which corresponds to only 40% of the training set. Similarly to MS MARCO experiments, we did not see any gain on the dev set by training the models longer.</p>
<p>Metrics</p>
<p>To evaluate the effectiveness of the methods on MS MARCO, we use its official metric, mean reciprocal rank of the top-10 documents (MRR@10). For TREC-CAR, we use mean average precision (MAP).</p>
<p>Results</p>
<p>We show the main result in Table 4.1. Despite training on a fraction of the data available, the proposed BERT-based models surpass the previous state-of-the-art models by a large margin on both of the tasks.</p>
<p>We found that the pretrained models used in this work require a relatively small number of labeled training examples to achieve good effectiveness (Figure 4.1). For example, a BERT LARGE trained on 100k question-passage pairs (less than 0.3% of the MS MARCO training data) is already 1.4 MRR@10 points better than the previous state of the art, IR-NET. </p>
<p>Document Expansion by Predicting Queries</p>
<p>Query rewrite is about enriching the query representation while holding the document representation static. Here, we explore an alternative approach based on enriching the document representation (prior to indexing). Focusing on question answering, we train a sequence-to-sequence model, that given a document, generates possible questions that the document might answer. An overview of the proposed method is shown in Figure 4.2.</p>
<p>This is the first successful application of document expansion using neural networks. On the MS MARCO dataset, our approach is competitive to the best results on the official leaderboard (as of 05/31/2019), and we report the best-known results on TREC CAR. We further show that document expansion is more effective than query expansion on these two datasets. We accomplish this with relatively simple models using existing open-source toolkits, which allows easy replication of our results. Document expansion also presents another major advantage, since the enrichment is performed prior to indexing: Although retrieved output can be further re-ranked using a neural model to increase effectiveness, the output can also be returned as-is. These results already yield a noticeable improvement in effectiveness over a "bag of words" baseline without the need to apply expensive and slow neural network inference at retrieval time.</p>
<p>Related Work</p>
<p>Prior to the advent of continuous vector space representations and neural ranking models, information retrieval techniques were mostly limited to keyword matching (i.e., "one-hot" representations). Alternatives such as latent semantic indexing [26] Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily...   Figure 4.2: Given a document, our Doc2query model predicts a query, which is appended to the document. Expansion is applied to all documents in the corpus, which are then indexed and searched as before. and its various successors never really gained significant traction. Approaches to tackling the vocabulary mismatch problem within these constraints include relevance feedback [86], query expansion [107,117], and modeling term relationships using statistical translation [7]. These techniques share in their focus on enhancing query representations to better match documents.</p>
<p>In this work, we adopt the alternative approach of enriching document representations [35,81,104], which works particularly well for speech [99] and multi-lingual retrieval, where terms are noisy. Document expansion techniques have been less popular with IR researchers because they are less amenable to rapid experimentation. The corpus needs to be re-indexed every time the expansion technique changes (typically, a costly process); in contrast, manipulations to query representations can happen at retrieval time (and hence are much faster). The success of document expansion has also been mixed; for example, Billerbeck and Zobel [8] explore both query expansion and document expansion in the same framework and conclude that the former is consistently more effective.</p>
<p>A new generation of neural ranking models offer solutions to the vocabulary mismatch problem based on continuous word representations and the ability to  learn highly non-linear models of relevance; see recent overviews by Onal et al. [78] and Mitra and Craswell [66]. However, due to the size of most corpora and the impracticality of applying inference over every document in response to a query, nearly all implementations today deploy neural networks as re-rankers over initial candidate sets retrieved using standard inverted indexes and a term-based ranking model such as BM25 [85]. Our work fits into this broad approach, where we take advantage of neural networks to augment document representations prior to indexing; term-based retrieval then happens exactly as before. Of course, retrieved results can still be re-ranked by a state-of-the-art neural model, but the output of term-based ranking already appears to be quite good. In other words, our document expansion approach can leverage neural networks without their high inference-time costs.</p>
<p>Method</p>
<p>Our proposed method, which we call "Doc2query", proceeds as follows: For each document, the task is to predict a set of queries for which that document will be relevant. Given a dataset of query-relevant document pairs, we use a sequence-tosequence Transformer model [106] that takes as an input the document terms and produces a query. The document and target query are segmented using BPE [93] after being tokenized with the Moses tokenizer. 4 To avoid excessive memory usage, we truncate each document to 400 tokens and queries to 100 tokens.</p>
<p>The architecture of our transformer model is identical to the base model described in Vaswani et al. [106], which has 6 layers for both encoder and decoder, 512 hidden units in each layer, 8 attention heads and 2048 hidden units in the feed-forward layers. We train with a batch size of 4096 tokens for a maximum of 30 epochs. We use Adam with a learning rate of 10 −3 , 1 = 0.9, 2 = 0.998, L2 weight decay of 0.01, learning rate warmup over the first 8,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 in all layers. Our implementation uses the OpenNMT framework [54]; training takes place on four V100 GPUs. To avoid overfitting, we monitor the BLEU scores of the training and development sets and stop training when their difference is larger than four points.</p>
<p>Once the model is trained, we predict 10 queries using top-random sampling [37] and append them to each document in the corpus. We do not put any special markup to distinguish the original document text from the predicted queries. The expanded documents are indexed, and we retrieve a ranked list of documents for each query using BM25. We optionally re-rank these retrieved documents using BERT as described in Section 4.1.</p>
<p>Baselines</p>
<p>We evaluate the following methods:</p>
<p>BM25:</p>
<p>We use the Anserini open-source IR toolkit to index the original (nonexpanded) documents and BM25 to rank the passages. During evaluation, we use the top-1000 re-ranked passages.</p>
<p>BM25 + Doc2query:</p>
<p>We first expand the documents using the proposed Doc2query method. We then index and rank the expanded documents exactly as in the BM25 method above.</p>
<p>RM3:</p>
<p>To compare document expansion with query expansion, we applied the RM3 query expansion technique [1]. We apply query expansion to both unexpanded documents (BM25 + RM3) as well as the expanded documents (BM25 + Doc2query + RM3).</p>
<p>BM25 + BERT:</p>
<p>We index, and retrieve documents as in BM25 and further re-rank the documents with the BERT Large described in Section 4.1.</p>
<p>BM25 + Doc2query + BERT: We expand, index, and retrieve documents as in BM25 + Doc2query and further re-rank the documents with BERT.</p>
<p>Input Document: July is the hottest month in Washington DC with an average temperature of 27C (80F) and the coldest is January at 4C (38F) with the most daily sunshine hours at 9 in July. The wettest month is May with an average of 100mm of rain. Predicted Query: weather in washington dc Target query: what is the temperature in washington  </p>
<p>Results</p>
<p>Results on both datasets are shown in Table 4.2. BM25 is the baseline. Document expansion with our method (BM25 + Doc2query) improves retrieval effectiveness by ∼15% for both datasets. When we combine document expansion with a state-of-the-art re-ranker (BM25 + Doc2query + BERT), we achieve the best-known results to date on TREC CAR; for MS MARCO, we are near the state of the art. Our full re-ranking condition (BM25 + Doc2query + BERT) beats BM25 + BERT alone, which verifies that the contribution of Doc2query is indeed orthogonal to that from post-indexing re-ranking.</p>
<p>Evaluating Various Decoding Schemes</p>
<p>Here we investigate how different decoding schemes used to produce queries affect the retrieval effectiveness. We experiment with two decoding methods: beam search and top-random sampling with different beam sizes (number of generated hypotheses). Results are shown in Figure 4.3. Top-random sampling is slightly better than beam search across all beam sizes, and we observed a peak in the retrieval effectiveness  when 10 queries are appended to the document. We conjecture that this peak occurs because too few queries yield insufficient diversity (fewer semantic matches) while too many queries introduce noise and reduce the contributions of the original text to the document representation.</p>
<p>Qualitative Analysis</p>
<p>Where exactly are these better scores coming from? We show in Table 4.3 examples of queries produced by our Doc2query model trained on MS MARCO. We notice that the model tends to copy some words from the input document (e.g., Washington DC, River, chromosome), meaning that it can effectively perform term re-weighting (i.e., increasing the importance of key terms). Nevertheless, the model also produces words not present in the input document (e.g., weather, relationship), which can be characterized as expansion by synonyms and other related terms.</p>
<p>To quantify this analysis, we measured the proportion of predicted words that exist (copied) vs. not-exist (new) in the original document. Excluding stop words, which corresponds to 51% of the predicted query words, we found that 31% are new while the rest (69%) are copied. If we expand MS MARCO documents using only new words and retrieve the dev set queries with BM25, we obtain an MRR@10 of 18.8 (as opposed to 18.4 when indexing with original documents). Expanding with copied words gives an MRR@10 of 19.7. We achieve a higher MRR@10 of 21.5 when documents are expanded with both types of words, showing that they are complementary.</p>
<p>Further analyses show that one source of improvement comes from having more relevant documents for the re-ranker to consider. We find that the Recall@1000 of the MS MARCO dev set increased from 85.3 (BM25) to 89.3 (BM25 + Doc2query). Results show that BERT is indeed able to identify these correct answers from the improved candidate pool and bring them to the top of the ranked list, thus improving the overall MRR.</p>
<p>As a contrastive condition, we find that query expansion with RM3 hurts in both datasets, whether applied to the unexpanded corpus (BM25 + RM3) or the expanded version (BM25 + Doc2query + RM3). This is somewhat a surprising result because query expansion usually improves effectiveness in document retrieval, but this can likely be explained by the fact that both MS MARCO and CAR are precision oriented. This result shows that document expansion can be more effective than query expansion, most likely because there are more signals to exploit as documents are much longer.</p>
<p>Finally, for production retrieval systems, latency is often an important factor. Our method without a re-ranker (BM25 + Doc2query) adds a small latency increase over baseline BM25 (50 ms vs. 90 ms) but is approximately seven times faster than a neural re-ranker that has a three points higher MRR@10 (Single Duet v2, which is presented as a baseline in MS MARCO by the organizers). For certain operating scenarios, this tradeoff in quality for speed might be worthwhile.</p>
<p>Summary</p>
<p>In this chapter, we introduced two novel components of a search engine. One is a simple adaptation of BERT as a passage re-ranker, which showed to be very effective to increase precision. The other component aims to improve recall by expanding documents prior to indexing. It is the first successful use of document expansion based on neural networks. Furthermore, this approach allows developers to shift the computational costs of neural network inference from retrieval to indexing.</p>
<p>A commonly overlooked issue when independently tuning multiple components of a pipeline is that their individual improvements might not be additive when they run together [4,60,92]. This was not the case here: we showed that the gains from our two novel components are orthogonal, and we were able to build a state-of-the-art retrieval system when we combined them.</p>
<p>Our implementation is based on integrating three open-source toolkits: OpenNMT, Anserini, and TensorFlow BERT. The relative simplicity of our approach aids in the Chapter 5</p>
<p>Conclusion</p>
<p>Our main motivation for this thesis was to build machines that can produce answers based on pieces of information found in a large corpus. While investigating possible solutions, we realized that it is very challenging to select relevant data to support the answer. This difficulty is, in part, due to the vast amount of unstructured information available, the majority of it is either irrelevant or provide incorrect facts. As evidence that the problem is indeed hard, some claim that there has been little progress over the past two decades in creating better retrieval systems [4,60]. These observations led us to work on mechanisms that can more effectively retrieve relevant information to a given question, i.e., search engines.</p>
<p>We first showed that it is possible to build a search engine that operates in a very different way from traditional ones by training an agent that finds documents by navigating the web via hyperlinks. We then showed that it is possible to train agents that learn how to retrieve better documents from the search engine, treated as a black box, by rewriting queries. Lastly, we focused on improving the internal components of the search engine, namely, the re-ranker and the inverted index, which resulted in a retrieval system with twice the performance of an off-the-shelf search engine [118,119].</p>
<p>Future Work: With better retrieval methods, we hope that future machines will evolve to a kind of oracle, that is, a machine that can answer our questions, no matter how difficult they are, and help us expand our understanding of the world.</p>
<p>To create such a system, we argue that we need harder question-answering datasets that are motivated by real use cases. One instance of such dataset could be: given a real user question whose answer is unlikely to be found in any single document of a large corpus, the task would be to generate an answer in a natural language whose supporting evidence comes from multiple documents. This answer is, in essence, a summary of multiple documents based on a question. The answer could be a single sentence, a table, or even an entire article. Hence, if existing search engines find a few bits of information in a large collection of documents, the next generation of these systems will connect the bits and surface patterns in a form that is easy to ingest by a human reader.</p>
<p>A system that performs well in this task is a good candidate to help, for example, biomedical scientists in finding which genes are related to which diseases and, in the process, it might also suggest gene-disease relations there were not explored in the literature. Similarly, it can assist doctors in recommending drugs to a patient, given her previous and current conditions. For that, the system would have to consider the effects of each component of the drug in the patient, which can be facilitated if the system has access to a history of treatments of patients that share similar conditions with the current patient.</p>
<p>Bibliography</p>
<p>returned by a popular search engine when asked: "Do teenagers go more often to the movies than adults?" (as of 24/05/2019). The first result is partially relevant, whereas the others are irrelevant. 4 2.1 Graphical illustration of a world in the proposed goal-driven web navigation. We show two nodes and with their contents D( ) and D( ), respectively. The reward is 1 if and only if a node includes a query sentence . In this case, the reward at the node , i.e., ( , ) is 1, but ( , ) = 0. . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Graphical illustration of a single step performed by the baseline model, NeuAgent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3</p>
<p>. . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.2 An illustration of our neural network-based reformulator. . . . . . . . 31 3.3 Our RL-based model continues to improve recall as more candidate terms are added, whereas a classical PRF method saturates. . . . by the RL-CNN to candidate terms of two sample queries: "Learning Intersections of Halfspaces with a Margin" (top) and "Sea Turtle Diet" (bottom). We show the original query terms and the top-10 and bottom-10 document terms with respect to their probabilities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.5 a) A vanilla search system. The query 0 is given to the system which outputs a result 0 . b) The search system with a reformulator. The reformulator queries the system with 0 and its reformulations { 1 , ... }, and receives back the results { 0 , ..., }. A selector then decides the best result for 0 . c) The proposed system. The original query is reformulated multiple times by different reformulators.</p>
<p>2
Source:https://searchengineland.com/google-now-handles-2-999-trillionsearches-per-year-250247 (accessed on 05/26/2018).</p>
<p>Figure 1 . 1 :
11Top-5 results returned by a popular search engine when asked: "Do teenagers go more often to the movies than adults?" (as of 24/05/2019). The first result is partially relevant, whereas the others are irrelevant.</p>
<p>Figure 2 . 1 :
21Graphical illustration of a world in the proposed goal-driven web navigation. We show two nodes and with their contents D( ) and D( ), respectively. The reward</p>
<p>2.1 for a graphical illustration of this world.</p>
<p>Figure 2 . 2 :
22Graphical illustration of a single step performed by the baseline model, NeuAgent.</p>
<p>to form the representation vector of the document. A document consists of a set of sections {sec ( )} =1 , each represented as the average continuous bag-of-words vector of the words in it:</p>
<p>Figure 2 . 3 :
23Visualization of the attention mechanism over a sample query. The horizontal axis corresponds the words in the input query, the vertical axis corresponds to the title of the current Wikipedia article, and the brighter the cell, the higher the attention weight.</p>
<p>D 0 Figure 3 .
03q' : cancer treatment state-of-the-art frontiers surveyReward q 0 : What are the most promising directions to cure cancer and why? 1: A graphical illustration of the proposed framework for query reformulation.</p>
<p>Figure 3 . 2 :
32An illustration of our neural network-based reformulator.</p>
<p>Figure 3 . 3 :
33Our RL-based model continues to improve recall as more candidate terms are added, whereas a classical PRF method saturates.{1, 3, 5, 7}, respectively. The best values are = 300 and = 7. These correspond to the maximum number of terms we could fit in a single 12 GB GPU.</p>
<p>Figure 3 . 4 :
34Probabilities assigned by the RL-CNN to candidate terms of two sample queries: "Learning Intersections of Halfspaces with a Margin" (top) and "Sea Turtle Diet" (bottom). We show the original query terms and the top-10 and bottom-10 document terms with respect to their probabilities.terms are infrequent.</p>
<p>.4.</p>
<p>Figure 3 .
35: a) A vanilla search system. The query 0 is given to the system which outputs a result 0 . b) The search system with a reformulator. The reformulator queries the system with 0 and its reformulations { 1 , ... }, and receives back the results { 0 , ..., }. A selector then decides the best result for 0 . c) The proposed system. The original query is reformulated multiple times by different reformulators. Reformulations are used to obtain results from the search system, which are then sent to the aggregator, which picks the best result for the original query based on a learned weighted majority voting scheme. Reformulators are independently trained on disjoint partitions of the dataset, thus increasing the variability of reformulations.</p>
<p>Figure 3 . 7 :
37Comparison of different aggregator functions on TREC-CAR. The reference model (first row) is an RL-10-Sub.</p>
<p>Figure 4 . 1 :
41Number of MS MARCO examples seen during training vs. MRR@10.</p>
<p>does</p>
<p>Figure 4 . 3 :
43Retrieval effectiveness on the development set of MS MARCO when using different decoding methods to produce queries. On the x-axis, we vary the number of predicted queries that are appended to the original documents.</p>
<p>Retrieval effectiveness on the development set of MS MARCO when using different decoding methods to produce queries. On the x-axis, we vary the number of predicted queries that are appended to the original documents. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 Jeopardy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3 Sample query-answer pairs from WikiNav-Jeopardy. . . . . . . . . . . 12 2.4 The average reward by the NeuAgents and humans on the test sets of WikiNavℎ -. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.5 Traces generated by the NeuAgent-Rec trained on WikiNav-8-1 using the queries from the test set. We present two examples per each of (a) successful and (b) failed runs. . . . . . . . . . . . . . . . . . . . . . . 23 2.6 Results on WikiNav-Jeopardy. In bold we show statistically significant results ( &lt; 0.05) according to Student's paired t-test with a Bonferroni correction (code modified from https://github.com/castorini/ Anserini/blob/master/src/main/python/compare_runs.py). (⋆) Pretrained on WikiNav-16-4. . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.1 Summary of the datasets. . . . . . . . . . . . . . . . . . . . . . . . . 37 3.2 Results on Test sets. We use R@40 as a reward to the RL-based models. We show the best results in bold. We use * to denote sta-Percentage of relevant terms over all the candidate terms according to SL-and RL-Oracle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 xiv 3.4 Top-3 retrieved documents using the original query and a query reformulated by our RL-CNN model. In the first example, we only show the titles of the retrieved MSA papers. In the second example, we only show some words of the retrieved TREC-CAR paragraphs. Bold corresponds to relevant documents. . . . . . . . . . . . . . . . . . . . 44 3.5 Given the query "Northwestern Connecticut Community College", models trained on different tasks choose different terms. . . . . . . . . . . 44 3.6 MAP on the test set of the document retrieval datasets. ⋆ The weights of the agents are initialized from a single model pretrained on the full training set for 10 days. . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.7 Comparison of different aggregator functions on TREC-CAR. The reference model (first row) is an RL-10-Sub. . . . . . . . . . . . . . . 52 3.8 Main result on the question-answering task (SearchQA dataset). We did not include the training cost of the aggregator (0.2 days, 0.06 ×10 18 FLOPs). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.9 Diversity scores of reformulations from different methods. For pBLEU and pCos, lower values mean higher diversity. Notice that higher diversity scores are associated with higher F1 and oracle scores. . . . 55 3.10 Examples for the qualitative analysis on SearchQA. In bold are the reformulations and answers that had the highest scores predicted by the aggregator. We only show the top-5 reformulations of each method. For a detailed analysis of the language learned by the reformulator agents, see Buck et al. [13]. . . . . . . . . . . . . . . . . . . . . . . . 57 4.1 Main Result on the passage re-ranking datasets. ⋆ Best Entry in the TREC-CAR 2017. † Previous SOTA in the MS MARCO leaderboard as of 01/04/2019; unpublished work. . . . . . . . . . . . . . . . . . . 60 4.2 Main results on TREC-CAR and MS MARCO datasets. ⋆ Our measurement, in which Duet v2 takes 600ms per query, and BM25 retrieval takes 50ms. ♠ Best submission of TREC-CAR 2017.1 
Number of MS MARCO examples seen during training vs. MRR@10. 62 
4.2 
Given a document, our Doc2query model predicts a query, which is 
appended to the document. Expansion is applied to all documents in 
the corpus, which are then indexed and searched as before. . . . . . 63 
4.3 
xiii </p>
<p>List of Tables </p>
<p>2.1 Per-page statistics of English Wikipedia used to build WebNav-tasks. 11 
2.2 Number of examples of WikiNav-4-<em>, WikiNav-8-</em>, WikiNav-16-* and 
WikiNav-tistically significant results ( &lt; 0.05) against SL-CNN and PRF-RM 
baselines according to Student's paired t-test with a Bonferroni correc-
tion (code modified from https://github.com/castorini/Anserini/ </p>
<p>blob/master/src/main/python/compare_runs.py) . . . . . . . . . . 38 
3.3 † We use Google's 
TPUs to re-rank with BERT. In bold are statistically significant results 
( &lt; 0.05) according to Student's paired t-test with a Bonferroni correc-
tion (code modified from https://github.com/castorini/Anserini/ </p>
<p>Table 2 .
21: Per-page statistics of 
English Wikipedia used to build 
WebNav-tasks. </p>
<p>Table 2 .
22: Number of examples of WikiNav-4-<em>, WikiNav-8-</em>, WikiNav-16-* and 
WikiNav-Jeopardy. </p>
<p>.2 for details.Query 
Answer </p>
<p>For the last 8 years of his life, Galileo was under </p>
<p>Copernicus </p>
<p>house arrest for espousing this man's theory. </p>
<p>In the winter of 1971-72, a record 1,122 inches of snow fell </p>
<p>Washington </p>
<p>at Rainier Paradise Ranger Station in this state. </p>
<p>This company's Accutron watch, introduced in 1960, </p>
<p>Bulova </p>
<p>had a guarantee of accuracy to within one minute a month. </p>
<p>Table 2 .
23: Sample query-answer pairs from WikiNav-Jeopardy. </p>
<p>Table 2.4: The average reward by the NeuAgents and humans on the test sets of WikiNavℎ -.Model Training 
core #Layers × #Units 
(a) 
Sup </p>
<p>ff </p>
<p>1 × 512 tanh 
BoW BoW 
(b) 
Sup </p>
<p>rec </p>
<p>1 × 512 LSTM 
BoW BoW 
(c) 
Sup </p>
<p>rec </p>
<p>8 × 2048 LSTM 
BoW BoW 
(d) 
Sup </p>
<p>rec </p>
<p>8 × 2048 LSTM 
Att BoW 
(e) 
Sup </p>
<p>rec </p>
<p>8 × 2048 LSTM 
Att 
Att 
(f) 
Sup + RL </p>
<p>rec </p>
<p>8 × 2048 LSTM 
Att 
Att </p>
<p>= 1 
2 
4 
Model 
ℎ = 4 
8 
16 
4 
8 
16 
4 
8 
16 </p>
<p>(a) 
21.5 
4.7 1.2 40.0 9.2 1.9 45.1 12.9 
2.9 
(b) 
22.0 
5.1 1.7 41.1 9.2 2.1 44.8 13.3 
3.6 
(c) 
17.7 
10.9 8.0 35.8 19.9 13.9 39.5 28.1 21.9 
(d) 
22.9 
15.8 12.5 41.7 24.5 17.8 46.8 34.2 28.2 
(e) 
-
-
-
-
-
-
47.3 35.0 29.9 
(f) 
-
-
-
-
-
-
49.8 36.1 30.9 
Humans 
-
-
-
-
-
-
14.5 
8.8 
5.0 </p>
<p>Trace category:health → category:education by country → category:smoking by country → list of countries by cigarette consumption per capita Query This system was established by the [[Confederate States Constitution]], in emulation of the [[United States Constitution]]. Trace category:government → category:government by country → category:government of the confederate states of america → Query Other types of exercise include the TEWT (Tactical Exercise Without Troops), also known as a [[sand table]], map or cloth model exercise Trace category:sports → category:physical exercise → category:bodyweight exercise → range of motion (exercise machine) Query Predictability Although the human mobility is modeled as a random process, it is surprisingly predictable. Trace category:creativity → category:applied mathematics → category:mathematical finance → stable process (b) Failed Runselectoral college (confederate states) 
(a) Successful Runs </p>
<p>Table 2 .
25: Traces generated by the NeuAgent-Rec trained on WikiNav-8-1 using the queries from the test set. We present two examples per each of (a) successful and (b) failed runs.</p>
<p>Settings:We test the model (d) from Section 2.5.1 (NeuAgent-Rec with eight layers of 2048 LSTM units and the attention-based query representation) on the WikiNav-Jeopardy. We evaluate two training strategies. The first strategy is straightforward supervised learning, in which we train a NeuAgent-Rec on WikiNav-Jeopardy from scratch. In the other strategy, we pretrain a NeuAgent-Rec first on the WikiNav-16-4 and fine-tune it on WikiNav-Jeopardy.We compare the proposed NeuAgent against three search strategies. The first one,SimpleSearch, is a simple inverted index-based strategy. SimpleSearch scores each Wikipedia article by the TF-IDF weighted sum of words that co-occur in the articles and a query and returns top-articles. Second, we use Lucene, a popular open source information retrieval library, in its default configuration with BM25 ranking function on the whole Wikipedia corpus. Lastly, we use Google Search API 8 , while restricting the domain to wikipedia.org.Each system is evaluated by document recall at (Recall@ ) and Mean Reciprocal Rank (MRR) since there is only one relevant document per question. We vary to be 1, 5, or 100. In the case of the NeuAgent, we run beam search with width set to and returns all the final nodes to compute the document recall.Table 2.6: Results on WikiNav-Jeopardy. In bold we show statistically significant results ( &lt; 0.05) according to Student's paired t-test with a Bonferroni correction (code modified from https://github.com/castorini/Anserini/blob/master/src/ main/python/compare_runs.py). (⋆) Pretrained on WikiNav-16-4.Model 
Pre ⋆ MRR Recall@1 Recall@5 Recall@100 </p>
<p>NeuAgent 
16.9 
13.9 
20.2 
45.2 
NeuAgent 
20.3 
18.9 
24.1 
47.6 </p>
<p>SimpleSearch 
7.8 
5.4 
12.6 
42.1 
Lucene (BM25) 
9.3 
6.3 
14.7 
46.6 
Google 
17.6 
14.0 
22.1 
39.8 </p>
<p>Table 3 .
31: Summary of the datasets. </p>
<p>Table 3 .
32: Results on Test sets. We use R@40 as a reward to the RL-based models. We show the best results in bold. We use * to denote statistically significant results ( &lt; 0.05) against SL-CNN and PRF-RM baselines according to Student's paired t-test with a Bonferroni correction (code modified from https://github.com/castorini/ Anserini/blob/master/src/main/python/compare_runs.py)</p>
<p>Candidate terms: Inspired by Diaz and Metzler[30], we use Wikipedia articles as a source for candidate terms since it is a well-curated, clean corpus, with diverse topics.At training and test times of SL methods, and at test time of RL methods, the candidate terms are from the first words of the top-Wikipedia articles retrieved.We select 
and 
using grid search on the validation set over {50, 100, 200, 300} and </p>
<p>.4, we show an original and reformulated query examples extracted fromQuery 
Top-3 Retrieved Documents </p>
<p>(Original) The Cross 
-The Cross Entropy Method for Network Reliability Estim. 
Entropy Method for 
-Robot Weightlifting by Direct Policy Search 
Fast Policy Search 
-Off-policy Policy Search </p>
<p>(Reformulated) Cross 
-Near Optimal Reinforcement 
Entropy Fast Policy 
Learning in Polynom. Time 
Reinforcement 
-The Cross Entropy Method 
Learning policies 
for Network Reliability Estim. 
global search 
-Robot Weightlifting by Direct Policy Search 
optimization biased </p>
<p>(Original) Daikon 
"...many types of pickles are made with daikon, includ..." 
Cultivation 
"...varieties of daikon can be grown as..." 
"In Chinese cuisine, turnip cake and chai tow kway..." </p>
<p>(Reformulated) Daikon "...many types of pickles are made with daikon, includ..." 
Cultivation root seed 
grow fast-growing 
"Certain varieties of daikon can be grown as a winter..." 
Chinese leaves 
"The Chinese and Indian varieties tolerate higher... </p>
<p>Table 3 .
34: Top-3 retrieved documents using the original query and a query reformulated by our RL-CNN model. In the first example, we only show the titles of the retrieved MSA papers. In the second example, we only show some words of the retrieved TREC-CAR paragraphs. Bold corresponds to relevant documents.the MS Academic and TREC-CAR datasets, and their top-3 retrieved documents. Notice that the reformulated query retrieves more relevant documents than the original one. As we conjectured earlier, we see that a search engine tends to return a document simply with the largest overlap in the text, necessitating the reformulation of a query to retrieve semantically relevant documents. query, different tasks: We compare inTable 3.5 the reformulation of a sample query made by models trained on different datasets. The model trained on TREC-CAR selects terms that are similar to the ones in the original query, such asSame Trained on 
Selected Terms </p>
<p>TREC-CAR serves american national Winsted accreditation </p>
<p>Jeopardy 
Tunxis Quinebaug Winsted NCCC </p>
<p>MSA 
hospital library arts center cancer center summer programs </p>
<p>Table 3 .
35: Given the query "Northwestern Connecticut Community College", models 
trained on different tasks choose different terms. </p>
<p>Table 3 .
38: Main result on the question-answering task (SearchQA dataset). We did 
not include the training cost of the aggregator (0.2 days, 0.06 ×10 18 FLOPs). </p>
<p>are frozen when we train and evaluate the reformulation system. Training and 
evaluation are performed on the SearchQA dataset [34]. The data contains Jeopardy! 
clues as questions. Each clue has a correct answer and a list of 50 snippets from 
Google's top search results. The training, validation, and test sets contain 99,820, 
13,393, and 27,248 examples, respectively. </p>
<p>19 )
19Method pCos ↓ pBLEU ↓ PINC ↑ Length Std ↑ F1 ↑ Oracle ↑AQA 
66.4 
45.7 
58.7 
3.8 
50.7 
60.0 
AQA-10-Full 
29.5 
26.6 
79.5 
9.2 
52.9 
63.1 
AQA-10-Sub 
14.2 
12.8 
94.5 
11.7 
53.4 
68.5 </p>
<p>Table 3.9: Diversity scores of reformulations from different methods. For pBLEU 
and pCos, lower values mean higher diversity. Notice that higher diversity scores are 
associated with higher F1 and oracle scores. </p>
<p>Table 3 .
310: Examples for the qualitative analysis on SearchQA. In bold are the 
reformulations and answers that had the highest scores predicted by the aggregator. 
We only show the top-5 reformulations of each method. For a detailed analysis of the 
language learned by the reformulator agents, see Buck et al. [13]. </p>
<p>Table 4 .
41: Main Result on the passage re-ranking datasets. ⋆ Best Entry in the TREC-CAR 2017. † Previous SOTA in the MS MARCO leaderboard as of 01/04/2019; unpublished work.</p>
<p>cinnamon lower blood sugar? does cinnamon lower blood sugar?Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily...Input: Document 
Output: Predicted Query </p>
<p>Expanded Doc: </p>
<p>Index </p>
<p>Doc2query </p>
<p>Search Engine </p>
<ul>
<li></li>
</ul>
<p>foods and supplements to 
lower blood sugar </p>
<p>User's Query </p>
<p>Better Retrieved Docs </p>
<p>Concatenate </p>
<p>TREC-CAR MS MARCO Retrieval TimeMAP 
MRR@10 
ms/query 
Test 
Test 
Dev </p>
<h2>Single Duet v2 [67]</h2>
<p>24.5 
24.3 
650 ⋆ 
Co-PACRR ♠ [62] 
14.8 
-
-
-</p>
<p>BM25 
15.3 
18.6 
18.4 
50 
BM25 + RM3 
12.7 
-
16.7 
250 
BM25 + Doc2query (Ours) 
18.3 
21.8 
21.5 
90 
BM25 + Doc2query + RM3 (Ours) 
15.5 
-
20.0 
350 </p>
<p>BM25 + BERT 
34.8 
35.9 
36.5 
3400  † 
BM25 + Doc2query + BERT (Ours) 
36.5 
36.8 37.5 
3500  † </p>
<p>Table 4 .
42: Main results on TREC-CAR and MS MARCO datasets. ⋆ Our measurement, in which Duet v2 takes 600ms per query, and BM25 retrieval takes 50ms. ♠ Best submission of TREC-CAR 2017. † We use Google's TPUs to re-rank with BERT. In bold are statistically significant results ( &lt; 0.05) according to Student's paired t-test with a Bonferroni correction (code modified from https://github.com/castorini/ Anserini/blob/master/src/main/python/compare_runs.py)</p>
<p>Input Document: The Delaware River flows through Philadelphia into the Delaware Bay. It flows through and aqueduct in the Roundout Reservoir and then flows through Philadelphia and New Jersey before emptying into the Delaware Bay. Predicted Query: what river flows through delaware Target Query: where does the delaware river start and end Input Document: sex chromosome -(genetics) a chromosome that determines the sex of an individual; mammals normally have two sex chromosomes chromosome -a threadlike strand of DNA in the cell nucleus that carries the genes in a linear order; humans have 22 chromosome pairs plus two sex chromosomes. Predicted Query: what is the relationship between genes and chromosomes Target Query: which chromosome controls sex characteristics</p>
<p>Table 4 .
43: Examples of query predictions on MS MARCO compared to real user queries.
For a detailed explanation, see Larson[58].
The source code and datasets are publicly available at https://github.com/nyu-dl/WebNav.
This limit is an artificial limit we chose for computational reasons. Such limitation is not necessary, and the difficulty of the task can be arbitrarily increased by choosing a much larger number of hops and selecting queries from any page at least two hops away from a starting page.
http://www.j-archive.com
We can consider any problem of sequential decision making as a structured output prediction where the structured output space contains sequences of actions.
Note that the baseline is approximated using the Monte Carlo method, and during this initial phase, is likely to be zero.
In a preliminary study with other volunteers, we found that, when the queries were shorter than 4, they were not able to solve enough trials for us to have meaningful statistics.
https://cse.google.com/cse/
http://schools-wikipedia.org/
The dataset and code to run the experiments are available at https://github.com/nyu-dl/ QueryReformulator.2 To deal with variable-length inputs in a mini-batch, we pad smaller ones with zeros on both ends so they end up as long as the largest sample in the mini-batch.
https://lucene.apache.org/ 4 https://cse.google.com/cse/
https://www.microsoft.com/cognitive-services/en-us/academic-knowledge-api
This was done to avoid the large computational cost for indexing and searching full papers.
The subset should be small enough, or the model should be large enough so it can overfit.
In the preliminary experiments, we found CNNs to work better than LSTMs[43].
The original question and its answer are important contributors to the final effectiveness. If not given to the aggregator, the effectiveness of all AQA models decreases by 1-2% in F1.
https://cloud.google.com/tpu/
https://github.com/google-research/bert3 We use the Anserini toolkit[118,119] (http://anserini.io/) to index and retrieve the passages.
http://www.statmt.org/moses/
reproducibility of our results and paves the way for further improvements in re-ranking and document expansion.
Appendix A ReproducibilityThe code and datasets to reproduce our results are available at:• Chapter 2: https://github.com/nyu-dl/dl4ir-webnav • Chapter 3: https://github.com/nyu-dl/dl4ir-query-reformulator• Chapter 4, Re-ranker: https://github.com/nyu-dl/dl4marco-bert • Chapter 4, Doc2query: https://github.com/nyu-dl/dl4ir-doc2query
UMass at TREC 2004: Novelty and HARD. Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D Smucker, Trevor Strohman, Howard Turtle, Courtney Wade, Proceedings of the Thirteenth Text REtrieval Conference. the Thirteenth Text REtrieval ConferenceGaithersburg, MarylandNasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. UMass at TREC 2004: Novelty and HARD. In Proceedings of the Thirteenth Text REtrieval Conference (TREC 2004), Gaithersburg, Maryland, 2004.</p>
<p>Deepbot: a focused crawler for accessing hidden web content. Manuel Álvarez, Juan Raposo, Alberto Pan, Fidel Cacheda, Fernando Bellas, Víctor Carneiro, Proceedings of the 3rd international workshop on Data enginering issues in E-commerce and services: In conjunction with ACM Conference on Electronic Commerce (EC'07). the 3rd international workshop on Data enginering issues in E-commerce and services: In conjunction with ACM Conference on Electronic Commerce (EC'07)ACMManuel Álvarez, Juan Raposo, Alberto Pan, Fidel Cacheda, Fernando Bellas, and Víctor Carneiro. Deepbot: a focused crawler for accessing hidden web content. In Proceedings of the 3rd international workshop on Data enginering issues in E-commerce and services: In conjunction with ACM Conference on Electronic Commerce (EC'07), pages 18-25. ACM, 2007.</p>
<p>Large scale distributed neural network training through online distillation. Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, Geoffrey E Hinton, arXiv:1804.03235arXiv preprintRohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E Hinton. Large scale distributed neural network training through online distillation. arXiv preprint arXiv:1804.03235, 2018.</p>
<p>Improvements that don't add up: ad-hoc retrieval results since 1998. G Timothy, Alistair Armstrong, William Moffat, Justin Webber, Zobel, Proceedings of the 18th ACM conference on Information and knowledge management. the 18th ACM conference on Information and knowledge managementACMTimothy G Armstrong, Alistair Moffat, William Webber, and Justin Zobel. Im- provements that don't add up: ad-hoc retrieval results since 1998. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 601-610. ACM, 2009.</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, ICLR 2015. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR 2015, 2014.</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, J. Artif. Intell. Res.(JAIR). 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253-279, 2013.</p>
<p>Information retrieval as statistical translation. Adam Berger, John Lafferty, Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1999). the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1999)Adam Berger and John Lafferty. Information retrieval as statistical translation. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1999), pages 222- 229, 1999.</p>
<p>Document expansion versus query expansion for ad-hoc retrieval. Bodo Billerbeck, Justin Zobel, Proceedings of the 10th Australasian Document Computing Symposium. the 10th Australasian Document Computing SymposiumBodo Billerbeck and Justin Zobel. Document expansion versus query expansion for ad-hoc retrieval. In Proceedings of the 10th Australasian Document Computing Symposium, pages 34-41, 2005.</p>
<p>Bagging predictors. Machine learning. Leo Breiman, 24Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.</p>
<p>Bias, variance, and arcing classifiers. Leo Breiman, Statistics Department. University of CaliforniaTechnical reportLeo Breiman. Bias, variance, and arcing classifiers. Technical report, Statistics Department, University of California, Berkeley, CA, USA, 1996.</p>
<p>Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. John S Bridle, Advances in Neural Information Processing Systems. D.S. TouretzkyMorgan-Kaufmann2John S. Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In D.S. Touretzky, editor, Advances in Neural Information Processing Systems 2, pages 211-217. Morgan-Kaufmann, 1990.</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.01540Openai gym. arXiv preprintGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Analyzing language learned by an active question answering agent. Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, Wei Wang, arXiv:1801.07537arXiv preprintChristian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, and Wei Wang. Analyzing language learned by an active question answering agent. arXiv preprint arXiv:1801.07537, 2018.</p>
<p>Ask the right questions: Active question reformulation with reinforcement learning. Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, Wei Wang, Proceedings of ICLR. ICLRChristian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, and Wei Wang. Ask the right questions: Active question reformulation with reinforcement learning. In Proceedings of ICLR, 2018.</p>
<p>Selecting good expansion terms for pseudo-relevance feedback. Guihong Cao, Jian-Yun Nie, Jianfeng Gao, Stephen Robertson, Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. the 31st annual international ACM SIGIR conference on Research and development in information retrievalACMGuihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. Selecting good expansion terms for pseudo-relevance feedback. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 243-250. ACM, 2008.</p>
<p>A survey of automatic query expansion in information retrieval. Claudio Carpineto, Giovanni Romano, ACM Computing Surveys (CSUR). 4411Claudio Carpineto and Giovanni Romano. A survey of automatic query expansion in information retrieval. ACM Computing Surveys (CSUR), 44(1):1, 2012.</p>
<p>Focused crawling: a new approach to topic-specific web resource discovery. Soumen Chakrabarti, Martin Van Den, Byron Berg, Dom, Computer Networks. 3111Soumen Chakrabarti, Martin Van den Berg, and Byron Dom. Focused crawling: a new approach to topic-specific web resource discovery. Computer Networks, 31 (11):1623-1640, 1999.</p>
<p>A systematic comparison of smoothing techniques for sentence-level bleu. Boxing Chen, Colin Cherry, Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBoxing Chen and Colin Cherry. A systematic comparison of smoothing techniques for sentence-level bleu. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362-367, 2014.</p>
<p>Reading wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, arXiv:1704.00051arXiv preprintDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051, 2017.</p>
<p>Collecting highly parallel data for paraphrase evaluation. L David, William B Chen, Dolan, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1David L Chen and William B Dolan. Collecting highly parallel data for para- phrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190-200. Association for Computational Linguistics, 2011.</p>
<p>Learning phrase representations using rnn encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, arXiv:1406.1078arXiv preprintKyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase repre- sentations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.</p>
<p>Simple and effective multi-paragraph reading comprehension. Christopher Clark, Matt Gardner, arXiv:1710.10723arXiv preprintChristopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. arXiv preprint arXiv:1710.10723, 2017.</p>
<p>Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, O Kenneth, Jeff Stanley, Clune, arXiv:1712.06560arXiv preprintEdoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Ken- neth O Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. arXiv preprint arXiv:1712.06560, 2017.</p>
<p>Convolutional neural networks for soft-matching n-grams in ad-hoc search. Zhuyun Dai, Chenyan Xiong, Jamie Callan, Zhiyuan Liu, Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. the Eleventh ACM International Conference on Web Search and Data MiningACMZhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 126-134. ACM, 2018.</p>
<p>Feudal reinforcement learning. Peter Dayan, Geoffrey E Hinton, Advances in neural information processing systems. Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural information processing systems, pages 271-278, 1993.</p>
<p>Indexing by latent semantic analysis. Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, Richard Harshman, Journal of the Association for Information Science. 416Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the Association for Information Science, 41(6):391-407, 1990.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pretraining of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre- training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Quasar: Datasets for question answering by search and reading. Bhuwan Dhingra, Kathryn Mazaitis, William W Cohen, arXiv:1707.03904arXiv preprintBhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904, 2017.</p>
<p>Pseudo-query reformulation. Fernando Diaz, European Conference on Information Retrieval. SpringerFernando Diaz. Pseudo-query reformulation. In European Conference on Infor- mation Retrieval, pages 521-532. Springer, 2016.</p>
<p>Improving the estimation of relevance models using large external corpora. Fernando Diaz, Donald Metzler, Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. the 29th annual international ACM SIGIR conference on Research and development in information retrievalACMFernando Diaz and Donald Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 154-161. ACM, 2006.</p>
<p>Query expansion with locally-trained word embeddings. Fernando Diaz, Mitra Bhaskar, Nick Craswell, arXiv:1605.07891arXiv preprintFernando Diaz, Bhaskar Mitra, and Nick Craswell. Query expansion with locally-trained word embeddings. arXiv preprint arXiv:1605.07891, 2016.</p>
<p>Hierarchical reinforcement learning with the maxq value function decomposition. G Thomas, Dietterich, J. Artif. Intell. Res.(JAIR). 131Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition. J. Artif. Intell. Res.(JAIR), 13(1):227-303, 2000.</p>
<p>Trec car: A data set for complex answer retrieval. Laura Dietz, Gamari Ben, Laura Dietz and Gamari Ben. Trec car: A data set for complex answer retrieval. http://trec-car.cs.unh.edu, 2017.</p>
<p>Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, Kyunghyun Cho, arXiv:1704.05179Searchqa: A new q&amp;a dataset augmented with context from a search engine. arXiv preprintMatthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&amp;a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179, 2017.</p>
<p>Improving retrieval of short texts through document expansion. Miles Efron, Peter Organisciak, Katrina Fenlon, Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval (SIGIR 2012). the 35th international ACM SIGIR conference on Research and development in information retrieval (SIGIR 2012)Miles Efron, Peter Organisciak, and Katrina Fenlon. Improving retrieval of short texts through document expansion. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval (SIGIR 2012), pages 911-920, 2012.</p>
<p>The divergence of reinforcement learning algorithms with value-iteration and function approximation. Michael Fairbank, Eduardo Alonso, arXiv:1107.4606arXiv preprintMichael Fairbank and Eduardo Alonso. The divergence of reinforcement learning algorithms with value-iteration and function approximation. arXiv preprint arXiv:1107.4606, 2011.</p>
<p>Angela Fan, Mike Lewis, Yann Dauphin, arXiv:1805.04833Hierarchical neural story generation. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv:1805.04833, 2018.</p>
<p>Boosting a weak learning algorithm by majority. Information and computation. Yoav Freund, 121Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121(2):256-285, 1995.</p>
<p>A deep relevance matching model for ad-hoc retrieval. Jiafeng Guo, Yixing Fan, Ai Qingyao, W Bruce Croft, Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. the 25th ACM International on Conference on Information and Knowledge ManagementACMJiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM Interna- tional on Conference on Information and Knowledge Management, pages 55-64. ACM, 2016.</p>
<p>An analytical comparison of approaches to personalizing pagerank. Taher Haveliwala, Sepandar Kamvar, Glen Jeh, StanfordTechnical reportTaher Haveliwala, Sepandar Kamvar, and Glen Jeh. An analytical comparison of approaches to personalizing pagerank. Technical report, Stanford, 2003.</p>
<p>Topic-sensitive pagerank. H Taher, Haveliwala, Proceedings of the 11th international conference on World Wide Web. the 11th international conference on World Wide WebACMTaher H Haveliwala. Topic-sensitive pagerank. In Proceedings of the 11th international conference on World Wide Web, pages 517-526. ACM, 2002.</p>
<p>Distilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 98Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.</p>
<p>Analyzing and evaluating query reformulation strategies in web search logs. Jeff Huang, N Efthimis, Efthimiadis, Proceedings of the 18th ACM conference on Information and knowledge management. the 18th ACM conference on Information and knowledge managementACMJeff Huang and Efthimis N Efthimiadis. Analyzing and evaluating query reformu- lation strategies in web search logs. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 77-86. ACM, 2009.</p>
<p>Co-pacrr: A context-aware neural ir model for ad-hoc retrieval. Kai Hui, Andrew Yates, Klaus Berberich, Gerard De Melo, Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. the Eleventh ACM International Conference on Web Search and Data MiningACMKai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. Co-pacrr: A context-aware neural ir model for ad-hoc retrieval. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 279-287. ACM, 2018.</p>
<p>Adaptive mixtures of local experts. A Robert, Michael I Jacobs, Jordan, J Steven, Geoffrey E Nowlan, Hinton, Neural computation. 31Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79-87, 1991.</p>
<p>Scaling personalized web search. Glen Jeh, Jennifer Widom, Proceedings of the 12th international conference on World Wide Web. the 12th international conference on World Wide WebAcmGlen Jeh and Jennifer Widom. Scaling personalized web search. In Proceedings of the 12th international conference on World Wide Web, pages 271-279. Acm, 2003.</p>
<p>Hierarchical mixtures of experts and the em algorithm. I Michael, Robert A Jordan, Jacobs, Neural computation. 62Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181-214, 1994.</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, S Daniel, Luke Weld, Zettlemoyer, arXiv:1705.03551arXiv preprintMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.</p>
<p>Reinforcement learning: A survey. Leslie Pack Kaelbling, Andrew W Michael L Littman, Moore, Journal of artificial intelligence research. 4Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237-285, 1996.</p>
<p>Exploiting the block structure of the web for computing pagerank. Sepandar Kamvar, Taher Haveliwala, Christopher Manning, Gene Golub, StanfordTechnical reportSepandar Kamvar, Taher Haveliwala, Christopher Manning, and Gene Golub. Exploiting the block structure of the web for computing pagerank. Technical report, Stanford, 2003.</p>
<p>Convolutional neural networks for sentence classification. Yoon Kim, arXiv:1408.5882arXiv preprintYoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.</p>
<p>Adam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>OpenNMT: Open-source toolkit for neural machine translation. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander M Rush, 10.18653/v1/P17-4012Proc. ACL. ACLGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. OpenNMT: Open-source toolkit for neural machine translation. In Proc. ACL, 2017. doi: 10.18653/v1/P17-4012. URL https://doi.org/10.18653/ v1/P17-4012.</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012.</p>
<p>Query expansion using word embeddings. Saar Kuzi, Anna Shtok, Oren Kurland, Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. the 25th ACM International on Conference on Information and Knowledge ManagementACMSaar Kuzi, Anna Shtok, and Oren Kurland. Query expansion using word embeddings. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 1929-1932. ACM, 2016.</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, Transactions of the Association of Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.</p>
<p>Introduction to information retrieval. Ray R Larson, Journal of the American Society for Information Science and Technology. 614Ray R Larson. Introduction to information retrieval. Journal of the American Society for Information Science and Technology, 61(4):852-853, 2010.</p>
<p>Relevance based language models. Victor Lavrenko, Bruce Croft, Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. the 24th annual international ACM SIGIR conference on Research and development in information retrievalACMVictor Lavrenko and W Bruce Croft. Relevance based language models. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 120-127. ACM, 2001.</p>
<p>The neural hype and comparisons against weak baselines. Jimmy Lin, ACM SIGIR Forum. ACM52Jimmy Lin. The neural hype and comparisons against weak baselines. In ACM SIGIR Forum, volume 52, pages 40-51. ACM, 2019.</p>
<p>Reinforcement learning for robots using neural networks. Long-Ji Lin, DTIC DocumentTechnical reportLong-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993.</p>
<p>Contextualized pacrr for complex answer retrieval. Sean Macavaney, Andrew Yates, Kai Hui, Proceedings of TREC. TRECSean MacAvaney, Andrew Yates, and Kai Hui. Contextualized pacrr for complex answer retrieval. In Proceedings of TREC, 2017.</p>
<p>Focused crawling for structured data. Robert Meusel, Peter Mika, Roi Blanco, Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. the 23rd ACM International Conference on Conference on Information and Knowledge ManagementACMRobert Meusel, Peter Mika, and Roi Blanco. Focused crawling for structured data. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1039-1048. ACM, 2014.</p>
<p>Efficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781arXiv preprintTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.</p>
<p>Wordnet: a lexical database for english. A George, Miller, Communications of the ACM. 3811George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41, 1995.</p>
<p>An introduction to neural information retrieval. Foundations and Trends in Information Retrieval. Bhaskar Mitra, Nick Craswell, 13Bhaskar Mitra and Nick Craswell. An introduction to neural information retrieval. Foundations and Trends in Information Retrieval, 13(1):1-126, 2019.</p>
<p>An updated duet model for passage re-ranking. Bhaskar Mitra, Nick Craswell, arXiv:1903.07666Bhaskar Mitra and Nick Craswell. An updated duet model for passage re-ranking. arXiv:1903.07666, 2019.</p>
<p>Learning to match using local and distributed representations of text for web search. Bhaskar Mitra, Fernando Diaz, Nick Craswell, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide WebInternational World Wide Web Conferences Steering CommitteeBhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using local and distributed representations of text for web search. In Proceedings of the 26th International Conference on World Wide Web, pages 1291-1299. International World Wide Web Conferences Steering Committee, 2017.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International Conference on Machine Learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timo- thy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Ma- chine Learning, pages 1928-1937, 2016.</p>
<p>Prioritized sweeping: Reinforcement learning with less data and less time. W Andrew, Moore, Christopher G Atkeson, Machine Learning. 13Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforce- ment learning with less data and less time. Machine Learning, 13(1):103-130, 1993.</p>
<p>Rectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th international conference on machine learning (ICML-10). the 27th international conference on machine learning (ICML-10)Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807-814, 2010.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.08941arXiv preprintKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language under- standing for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.</p>
<p>Improving information extraction by acquiring external evidence with reinforcement learning. Karthik Narasimhan, Adam Yala, Regina Barzilay, arXiv:1603.07954arXiv preprintKarthik Narasimhan, Adam Yala, and Regina Barzilay. Improving information extraction by acquiring external evidence with reinforcement learning. arXiv preprint arXiv:1603.07954, 2016.</p>
<p>Ms marco: A human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, arXiv:1611.09268arXiv preprintTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016.</p>
<p>End-to-end goal-driven web navigation. Rodrigo Nogueira, Kyunghyun Cho, Advances in Neural Information Processing Systems. Rodrigo Nogueira and Kyunghyun Cho. End-to-end goal-driven web navigation. In Advances in Neural Information Processing Systems, pages 1903-1911, 2016.</p>
<p>Task-oriented query reformulation with reinforcement learning. Rodrigo Nogueira, Kyunghyun Cho, arXiv:1704.04572arXiv preprintRodrigo Nogueira and Kyunghyun Cho. Task-oriented query reformulation with reinforcement learning. arXiv preprint arXiv:1704.04572, 2017.</p>
<p>Neural information retrieval: At the end of the early years. Ye Kezban Dilek Onal, Ismail Zhang, Md Sengor Altingovde, Pinar Mustafizur Rahman, Alex Karagoz, Brandon Braylan, Heng-Lu Dang, Henna Chang, Quinten Kim, Aaron Mcnamara, Edward Angert, Vivek Banner, Tyler Khetan, An Thanh Mcdonnell, Dan Nguyen, Byron C Xu, Maarten Wallace, Matthew Rijke, Lease, 1386-4564Information Retrieval. 212-3Kezban Dilek Onal, Ye Zhang, Ismail Sengor Altingovde, Md Mustafizur Rah- man, Pinar Karagoz, Alex Braylan, Brandon Dang, Heng-Lu Chang, Henna Kim, Quinten Mcnamara, Aaron Angert, Edward Banner, Vivek Khetan, Tyler Mcdonnell, An Thanh Nguyen, Dan Xu, Byron C. Wallace, Maarten Rijke, and Matthew Lease. Neural information retrieval: At the end of the early years. Information Retrieval, 21(2-3):111-182, June 2018. ISSN 1386-4564.</p>
<p>Deep exploration via bootstrapped dqn. Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Advances in neural information processing systems. Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pages 4026-4034, 2016.</p>
<p>The pagerank citation ranking: Bringing order to the web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, Stanford InfoLabTechnical reportLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.</p>
<p>Reverted indexing for feedback and expansion. Jeremy Pickens, Matthew Cooper, Gene Golovchinsky, Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM 2010). the 19th ACM International Conference on Information and Knowledge Management (CIKM 2010)Jeremy Pickens, Matthew Cooper, and Gene Golovchinsky. Reverted indexing for feedback and expansion. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM 2010), pages 1049-1058, 2010.</p>
<p>Adaptive step-size for policy gradient methods. Matteo Pirotta, Marcello Restelli, Luca Bascetta, Advances in Neural Information Processing Systems. Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Adaptive step-size for policy gradient methods. In Advances in Neural Information Processing Systems, pages 1394-1402, 2013.</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250arXiv preprintPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.</p>
<p>Using reinforcement learning to spider the web efficiently. Jason Rennie, Andrew Mccallum, ICML. 99Jason Rennie, Andrew McCallum, et al. Using reinforcement learning to spider the web efficiently. In ICML, volume 99, pages 335-343, 1999.</p>
<p>Okapi at trec-3. Steve Stephen E Robertson, Susan Walker, Micheline M Jones, Mike Hancock-Beaulieu, Gatford, Nist Special Publication Sp. 109109Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. Nist Special Publication Sp, 109:109, 1995.</p>
<p>Relevance feedback in information retrieval. Joseph John Rocchio, The SMART Retrieval System-Experiments in Automatic Document Processing. Gerard SaltonEnglewood Cliffs, New JerseyPrentice-HallJoseph John Rocchio. Relevance feedback in information retrieval. In Gerard Salton, editor, The SMART Retrieval System-Experiments in Automatic Docu- ment Processing, pages 313-323. Prentice-Hall, Englewood Cliffs, New Jersey, 1971.</p>
<p>Efficient reductions for imitation learning. Stéphane Ross, Drew Bagnell, International Conference on Artificial Intelligence and Statistics. Stéphane Ross and Drew Bagnell. Efficient reductions for imitation learning. In International Conference on Artificial Intelligence and Statistics, pages 661-668, 2010.</p>
<p>Using word embeddings for automatic query expansion. Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, Utpal Garain, arXiv:1606.07608arXiv preprintDwaipayan Roy, Debjyoti Paul, Mandar Mitra, and Utpal Garain. Using word embeddings for automatic query expansion. arXiv preprint arXiv:1606.07608, 2016.</p>
<p>Learning representations by back-propagating errors. David Rumelhart, Geoffrey Hinton, Ronald Williams, Nature. David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representa- tions by back-propagating errors. Nature, pages 323-533, 1986.</p>
<p>Learning representations by back-propagating errors. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, Cognitive modeling. 531David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.</p>
<p>. Sergio Gomez Andrei A Rusu, Caglar Colmenarejo, Guillaume Gulcehre, James Desjardins, Razvan Kirkpatrick, Volodymyr Pascanu, Koray Mnih, Raia Kavukcuoglu, Hadsell, arXiv:1511.06295Policy distillation. arXiv preprintAndrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Ko- ray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.</p>
<p>Winner's curse? on pace, progress, and empirical rigor. D Sculley, Jasper Snoek, Alex Wiltschko, Ali Rahimi, ICLR, Workshop track. D Sculley, Jasper Snoek, Alex Wiltschko, and Ali Rahimi. Winner's curse? on pace, progress, and empirical rigor. In ICLR, Workshop track, 2018.</p>
<p>Rico Sennrich, Barry Haddow, Alexandra Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. arXiv preprintRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.</p>
<p>Bidirectional attention flow for machine comprehension. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi, arXiv:1611.01603arXiv preprintMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.</p>
<p>. Chinnadhurai Iulian V Serban, Mathieu Sankar, Saizheng Germain, Zhouhan Zhang, Sandeep Lin, Taesup Subramanian, Michael Kim, Sarath Pieper, Chandar, arXiv:1709.02349Nan Rosemary KeA deep reinforcement learning chatbot. arXiv preprintIulian V Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, et al. A deep reinforcement learning chatbot. arXiv preprint arXiv:1709.02349, 2017.</p>
<p>Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, arXiv:1701.06538arXiv preprintNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 5297587David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel- vam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.</p>
<p>Reinforcement learning with a hierarchy of abstract models. P Satinder, Singh, AAAI. Satinder P Singh. Reinforcement learning with a hierarchy of abstract models. In AAAI, pages 202-207, 1992.</p>
<p>Document expansion for speech retrieval. Amit Singhal, Fernando Pereira, Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1999). the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1999)Amit Singhal and Fernando Pereira. Document expansion for speech retrieval. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1999), pages 34-41, 1999.</p>
<p>Learning concept embeddings for query expansion by quantum entropy minimization. Alessandro Sordoni, Yoshua Bengio, Jian-Yun Nie, AAAI. Alessandro Sordoni, Yoshua Bengio, and Jian-Yun Nie. Learning concept embeddings for query expansion by quantum entropy minimization. In AAAI, pages 1586-1592, 2014.</p>
<p>Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime. Christopher Stanton, Jeff Clune, PloS one. 119162235Christopher Stanton and Jeff Clune. Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime. PloS one, 11(9):e0162235, 2016.</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112, 2014.</p>
<p>Introduction to reinforcement learning. S Richard, Andrew G Sutton, Barto, MIT Press Cambridge135Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 135. MIT Press Cambridge, 1998.</p>
<p>Language model information retrieval with document expansion. Tao Tao, Xuanhui Wang, Qiaozhu Mei, Chengxiang Zhai, Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational LinguisticsTao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. Language model information retrieval with document expansion. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 407-414, 2006.</p>
<p>An analysis of temporal-difference learning with function approximationtechnical. J N Tsitsiklis, Van Roy, Massachusetts Institute of TechnologyTechnical reportReport LIDS-P-2322). Laboratory for Information and Decision SystemsJN Tsitsiklis and B Van Roy. An analysis of temporal-difference learning with function approximationtechnical. Technical report, Report LIDS-P-2322). Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, 1996.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008, 2017.</p>
<p>Query expansion using lexical-semantic relations. Ellen M Voorhees, Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1994). the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1994)Ellen M. Voorhees. Query expansion using lexical-semantic relations. In Pro- ceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1994), pages 61-69, 1994.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, Jing Jiang, arXiv:1709.00023Reinforced reader-ranker for open-domain question answering. 3arXiv preprintShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang. R3: Re- inforced reader-ranker for open-domain question answering. arXiv preprint arXiv:1709.00023, 2017.</p>
<p>Evidence aggregation for answer re-ranking in open-domain question answering. Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, Murray Campbell, arXiv:1711.05116arXiv preprintShuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. arXiv preprint arXiv:1711.05116, 2017.</p>
<p>Q-learning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 83-4Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8 (3-4):279-292, 1992.</p>
<p>Automatic versus human navigation in information networks. Robert West, Jure Leskovec, ICWSM. Robert West and Jure Leskovec. Automatic versus human navigation in infor- mation networks. In ICWSM, 2012.</p>
<p>Human wayfinding in information networks. Robert West, Jure Leskovec, 21st International World Wide Web Conference. ACMRobert West and Jure Leskovec. Human wayfinding in information networks. In 21st International World Wide Web Conference, pages 619-628. ACM, 2012.</p>
<p>Wikispeedia: An online game for inferring semantic distances between concepts. Robert West, Joelle Pineau, Doina Precup, IJCAI. Robert West, Joelle Pineau, and Doina Precup. Wikispeedia: An online game for inferring semantic distances between concepts. In IJCAI, pages 1598-1603, 2009.</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. Simple statistical gradient-following algorithms for connec- tionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
<p>End-to-end neural ad-hoc ranking with kernel pooling. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, Russell Power, Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR Conference on Research and Development in Information RetrievalACMChenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 55-64. ACM, 2017.</p>
<p>Query expansion using local and global document analysis. Jinxi Xu, W Bruce Croft, Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval. the 19th annual international ACM SIGIR conference on Research and development in information retrievalACMJinxi Xu and W Bruce Croft. Query expansion using local and global document analysis. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 4-11. ACM, 1996.</p>
<p>Improving the effectiveness of information retrieval with local context analysis. Jinxi Xu, W. Bruce Croft, ACM Transactions on Information Systems. 181Jinxi Xu and W. Bruce Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Transactions on Information Systems, 18(1): 79-112, 2000.</p>
<p>Anserini: Enabling the use of Lucene for information retrieval research. Peilin Yang, Hui Fang, Jimmy Lin, Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017). the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017)Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Enabling the use of Lucene for information retrieval research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017), pages 1253-1256, 2017.</p>
<p>Anserini: Reproducible ranking baselines using Lucene. Peilin Yang, Hui Fang, Jimmy Lin, Journal of Data and Information Quality. 10416Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Reproducible ranking baselines using Lucene. Journal of Data and Information Quality, 10(4):Article 16, 2018.</p>
<p>Qanet: Combining local convolution with global self-attention for reading comprehension. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, Quoc V Le, arXiv:1804.09541arXiv preprintAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Moham- mad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.</p>
<p>A study of smoothing methods for language models applied to ad hoc information retrieval. Chengxiang Zhai, John Lafferty, Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. the 24th annual international ACM SIGIR conference on Research and development in information retrievalACMChengxiang Zhai and John Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334-342. ACM, 2001.</p>            </div>
        </div>

    </div>
</body>
</html>
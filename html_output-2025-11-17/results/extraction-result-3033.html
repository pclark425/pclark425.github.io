<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3033 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3033</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3033</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-269149341</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.09982v2.pdf" target="_blank">Memory Sharing for Large Language Model based Agents</a></p>
                <p><strong>Paper Abstract:</strong> The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions. This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process. This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories. The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence. Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever. We evaluated our framework across three distinct domains involving specialized tasks of agents. The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3033.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3033.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Limerick</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Limerick (Literary Creation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized LLM-based agent that generates limericks as open-ended literary outputs; evaluated within a multi-agent Memory Sharing (MS) framework using a shared Prompt-Answer memory pool and retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Limerick</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A task-specialized agent whose backbone LLMs in experiments include gpt-3.5-turbo, gpt-4o, and open-mistral-7b; it produces limerick-format poetry and is integrated into the MS system to read retrieved PA memories and generate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memories are stored as Prompt-Answer (PA) pairs in a shared pool; each new PA is scored by an LLM-based scorer and, if above threshold, added to the pool. A dense retriever (initialized with a small manual seed and BM25 for candidate selection during retriever training) retrieves top-k similar PA pairs by embedding cosine similarity; retrieved memories are concatenated with the query as in-context examples for the agent. New memories are also used to continually fine-tune the retriever via positive/negative labeling derived from LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Literary Creation - Limericks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended poetry generation where the agent must produce a limerick that matches a descriptive prompt; challenge is creative, stylistic generation and alignment with the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>creative generation / open-ended text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.87 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using the shared PA memory pool and retrieval-augmented prompts materially improved Limerick agent outputs compared to zero-shot baselines; best results tended to occur when retrieving three memories (three-shot). Domain-specific memory pools (Domain-pool) performed better than a mixed Single-pool for this agent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Improvements can plateau as memory grows if newly added memories are not more suitable; cross-domain memories (Single-pool) reduced performance compared to Domain-pool for most agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3033.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Wuyanlvshi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Wuyanlvshi (Classical Chinese Poetry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent specialized to generate Wuyanlvshi (classical Chinese 5-character verse) within the MS framework, using retrieved PA examples to guide generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Wuyanlvshi</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Specialized poetry-generation agent (backbones: gpt-3.5-turbo, gpt-4o, open-mistral-7b) tasked with producing classical Chinese poetry; consumes retrieved PA memories concatenated to the query.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Shared memory of PA pairs is retrieved by a dense retriever using the query; retrieved PA examples are concatenated to form few-shot prompts. New high-quality PA pairs are scored by an LLM scorer and added to the pool and used to continuously train the retriever using BM25 candidates and LLM-based labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Literary Creation - Wuyanlvshi</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate classical Chinese Wuyanlvshi poems matching descriptive constraints; main challenges are form, meter, and stylistic fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>creative generation / open-ended text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.72 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval of shared PA memories improved output quality over zero-shot; performance gains were smaller in Literary Creation compared to other domains, possibly due to inconsistent language/form when memories were stored.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less pronounced improvement relative to other domains, potentially due to language/form mismatches in stored memories; sensitivity to memory format.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3033.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Sonnet (Literary Creation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized agent that composes sonnets, demonstrated with concrete examples of using retrieved PA memories to craft prompts and produce sonnets within the MS framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Sonnet generator using a backbone LLM (experimentally gpt-3.5-turbo, gpt-4o, open-mistral-7b) that builds prompts by combining the original query with retrieved PA memories and then generates sonnets as answers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The retriever fetches PA pairs similar to the query; these PA pairs are concatenated with the query to create an enriched few-shot prompt. After generation, the PA pair is scored by an LLM scorer and can be added to the pool and used to fine-tune the retriever continuously.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Literary Creation - Sonnets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compose sonnets aligned to detailed prompts (theme, structure, metaphorical constraints); requires creative reasoning and stylistic adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>creative generation / open-ended text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.53 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using shared memories as in-context examples helps Sonnet agent produce outputs that better align with prompts; concrete examples (Figure 5) show how retrieved memories are used to craft prompts and generate higher-quality sonnets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Literary outputs show variable gains; formatting and style inconsistencies among stored memories can limit usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3033.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Puzzle (Unconventional Logic Problem-solving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent designed to solve unconventional logic puzzles (reasoning tasks) using retrieved PA examples to improve reasoning and answer quality within the MS framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A logic-problem solving agent (backbones: gpt-3.5-turbo, gpt-4o, open-mistral-7b) that retrieves relevant PA memories from a shared pool to provide few-shot examples aiding multi-step and unconventional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Retriever returns PA examples similar by cosine similarity of dense embeddings; retrieved examples are included in the prompt to provide in-context patterns for reasoning. New high-quality PA pairs are scored and stored and used to continually train the retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Unconventional Logic Problem-solving - Puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve nonstandard logic puzzles requiring creative or uncommon reasoning strategies; challenge is generating correct, plausible reasoning and solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.60 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shared PA memories improved puzzle-solving performance compared to zero-shot; retrieval of multiple (three) memories typically yielded the best performance for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality of retrieved memories matters for reasoning; inappropriate or low-quality memories can be unhelpful. Initial cold-start of memory pool requires seed examples and interactive learning to grow.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3033.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Pun</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Pun (Unconventional Logic / Wordplay)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A wordplay/pun generation and problem agent that uses retrieved PA memories for improved creative/logic outputs in the MS framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Pun</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent specialized in generating puns and short-wordplay answers; integrated with the MS framework to use retrieved PA pairs as in-context examples to shape creative responses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>PA pairs are retrieved by a dense retriever and appended to prompts; a scorer labels high-quality PA pairs for inclusion and retriever training, enabling continual retrieval improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Unconventional Logic - Puns</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce pun-based answers or responses to witty prompts; challenge is creativity and appropriateness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>creative generation / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.70 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory sharing improved the creativity and relevance of pun outputs; three-shot retrieval generally gave the best results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cross-domain memories can be less helpful; Domain-pool was generally superior for task-specific performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3033.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Riddle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Riddle (Unconventional Logic / Riddle Solving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A riddle-answering agent that leverages the MS framework's PA memory retrieval to enhance solution quality for riddles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Riddle</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Riddle-solving agent using backbone LLMs (gpt-3.5-turbo, gpt-4o, open-mistral-7b) that consumes retrieved PA examples from the memory pool to improve interpretive reasoning and answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Retrieved PA pairs similar to the input riddle are concatenated to the query; new PA pairs that score highly are added to the memory pool and used to retrain the retriever iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Unconventional Logic - Riddles</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Interpret and answer riddles; challenge is creative interpretation and analogical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.88 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Riddle-solving benefited strongly from shared memories; performance improved with additional memories and three-shot retrieval was often optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential stagnation when added memories are not more useful than existing ones; initial memory seeding required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3033.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Fitness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Fitness (Plan Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plan-generation agent that creates fitness plans, using retrieved PA memories to produce more executable and relevant plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Fitness</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent specialized in generating fitness plans (backbones: gpt-3.5-turbo, gpt-4o, open-mistral-7b); uses retrieved PA examples from shared memory to inform stepwise plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Retriever returns PA examples which are concatenated to the planning prompt; high-scoring PA pairs are archived and used to continually train the retriever via LLM-based labeling of BM25 candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Plan Generation - Fitness Plans</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce an executable fitness plan given goals and constraints; challenge is stepwise, practical planning and personalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / open-ended generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.65 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shared memories improved plan quality and reduced dependence on external databases; incremental addition of memories led to performance gains for many agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some agents reached performance plateaus as memory grew; retriever and memory quality determine utility for concrete planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3033.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Study (Plan Generation / Study Plans)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that generates study plans; uniquely, it showed a notable sensitivity to Single-pool vs Domain-pool memory sharing in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Study</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Study-plan generation agent built on backbone LLMs (gpt-3.5-turbo, gpt-4o, open-mistral-7b); leverages retrieved PA pairs from the shared memory pool to form study plans tailored to prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Uses dense retrieval of PA examples which are concatenated to the prompt; newly accepted PA pairs train the retriever continuously. In experiments, Study agent was an exception in that Single-pool sometimes matched or exceeded Domain-pool performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Plan Generation - Study Plans</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Create stepwise study plans given a subject and target; challenge is useful sequence of learning activities and personalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / open-ended generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.63 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory sharing enhanced study plan outputs; unlike most agents, Study sometimes did not degrade under Single-pool, suggesting task-dependent sensitivity to cross-domain memories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Agent-specific variance: Study agent behaved differently from others regarding cross-domain memory pooling; requires further investigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3033.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3033.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Travel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent - Travel (Plan Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A travel-planning agent within the MS framework that uses retrieved PA memories to produce travel itineraries and plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-Travel</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates travel plans and itineraries; consumes retrieved PA memory examples to better structure and tailor plans, using backbone LLMs such as gpt-3.5-turbo, gpt-4o, open-mistral-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (shared Prompt-Answer memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>PA pairs are retrieved by dense retriever using the query, concatenated to the prompt to serve as in-context examples; accepted PA pairs feed back into retriever training. The memory pool grows dynamically via interactive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Plan Generation - Travel Plans</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce travel itineraries and plans meeting user constraints; challenges include multi-step planning, factual and preference alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / open-ended generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.71 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shared PA memories improved travel plan quality; three-shot retrieval often yielded best performance; Domain-pool generally outperformed Single-pool.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dependence on quality and relevance of stored memories; cross-domain noise in Single-pool can reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MemGPT: Towards LLMs as Operating Systems <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra of Human Behavior <em>(Rating: 2)</em></li>
                <li>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-Based Knowledge and Memory <em>(Rating: 2)</em></li>
                <li>Memochat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation <em>(Rating: 2)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>SYNAPSE: Trajectory-as-Exemplar Prompting with Memory for Computer Control <em>(Rating: 2)</em></li>
                <li>VOYAGER: An Open-Ended Embodied Agent with Large Language Models <em>(Rating: 1)</em></li>
                <li>Learning to Retrieve in-Context Examples for Large Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3033",
    "paper_id": "paper-269149341",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "Agent-Limerick",
            "name_full": "Agent - Limerick (Literary Creation)",
            "brief_description": "A specialized LLM-based agent that generates limericks as open-ended literary outputs; evaluated within a multi-agent Memory Sharing (MS) framework using a shared Prompt-Answer memory pool and retrieval augmentation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Limerick",
            "agent_description": "A task-specialized agent whose backbone LLMs in experiments include gpt-3.5-turbo, gpt-4o, and open-mistral-7b; it produces limerick-format poetry and is integrated into the MS system to read retrieved PA memories and generate outputs.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "Memories are stored as Prompt-Answer (PA) pairs in a shared pool; each new PA is scored by an LLM-based scorer and, if above threshold, added to the pool. A dense retriever (initialized with a small manual seed and BM25 for candidate selection during retriever training) retrieves top-k similar PA pairs by embedding cosine similarity; retrieved memories are concatenated with the query as in-context examples for the agent. New memories are also used to continually fine-tune the retriever via positive/negative labeling derived from LLM evaluations.",
            "task_name": "Literary Creation - Limericks",
            "task_description": "Open-ended poetry generation where the agent must produce a limerick that matches a descriptive prompt; challenge is creative, stylistic generation and alignment with the prompt.",
            "task_type": "creative generation / open-ended text generation",
            "performance_with_memory": "0.87 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Using the shared PA memory pool and retrieval-augmented prompts materially improved Limerick agent outputs compared to zero-shot baselines; best results tended to occur when retrieving three memories (three-shot). Domain-specific memory pools (Domain-pool) performed better than a mixed Single-pool for this agent.",
            "limitations_or_challenges": "Improvements can plateau as memory grows if newly added memories are not more suitable; cross-domain memories (Single-pool) reduced performance compared to Domain-pool for most agents.",
            "uuid": "e3033.0",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Wuyanlvshi",
            "name_full": "Agent - Wuyanlvshi (Classical Chinese Poetry)",
            "brief_description": "An agent specialized to generate Wuyanlvshi (classical Chinese 5-character verse) within the MS framework, using retrieved PA examples to guide generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Wuyanlvshi",
            "agent_description": "Specialized poetry-generation agent (backbones: gpt-3.5-turbo, gpt-4o, open-mistral-7b) tasked with producing classical Chinese poetry; consumes retrieved PA memories concatenated to the query.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "Shared memory of PA pairs is retrieved by a dense retriever using the query; retrieved PA examples are concatenated to form few-shot prompts. New high-quality PA pairs are scored by an LLM scorer and added to the pool and used to continuously train the retriever using BM25 candidates and LLM-based labeling.",
            "task_name": "Literary Creation - Wuyanlvshi",
            "task_description": "Generate classical Chinese Wuyanlvshi poems matching descriptive constraints; main challenges are form, meter, and stylistic fidelity.",
            "task_type": "creative generation / open-ended text generation",
            "performance_with_memory": "0.72 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Retrieval of shared PA memories improved output quality over zero-shot; performance gains were smaller in Literary Creation compared to other domains, possibly due to inconsistent language/form when memories were stored.",
            "limitations_or_challenges": "Less pronounced improvement relative to other domains, potentially due to language/form mismatches in stored memories; sensitivity to memory format.",
            "uuid": "e3033.1",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Sonnet",
            "name_full": "Agent - Sonnet (Literary Creation)",
            "brief_description": "A specialized agent that composes sonnets, demonstrated with concrete examples of using retrieved PA memories to craft prompts and produce sonnets within the MS framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Sonnet",
            "agent_description": "Sonnet generator using a backbone LLM (experimentally gpt-3.5-turbo, gpt-4o, open-mistral-7b) that builds prompts by combining the original query with retrieved PA memories and then generates sonnets as answers.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "The retriever fetches PA pairs similar to the query; these PA pairs are concatenated with the query to create an enriched few-shot prompt. After generation, the PA pair is scored by an LLM scorer and can be added to the pool and used to fine-tune the retriever continuously.",
            "task_name": "Literary Creation - Sonnets",
            "task_description": "Compose sonnets aligned to detailed prompts (theme, structure, metaphorical constraints); requires creative reasoning and stylistic adherence.",
            "task_type": "creative generation / open-ended text generation",
            "performance_with_memory": "0.53 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Using shared memories as in-context examples helps Sonnet agent produce outputs that better align with prompts; concrete examples (Figure 5) show how retrieved memories are used to craft prompts and generate higher-quality sonnets.",
            "limitations_or_challenges": "Literary outputs show variable gains; formatting and style inconsistencies among stored memories can limit usefulness.",
            "uuid": "e3033.2",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Puzzle",
            "name_full": "Agent - Puzzle (Unconventional Logic Problem-solving)",
            "brief_description": "An agent designed to solve unconventional logic puzzles (reasoning tasks) using retrieved PA examples to improve reasoning and answer quality within the MS framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Puzzle",
            "agent_description": "A logic-problem solving agent (backbones: gpt-3.5-turbo, gpt-4o, open-mistral-7b) that retrieves relevant PA memories from a shared pool to provide few-shot examples aiding multi-step and unconventional reasoning.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "Retriever returns PA examples similar by cosine similarity of dense embeddings; retrieved examples are included in the prompt to provide in-context patterns for reasoning. New high-quality PA pairs are scored and stored and used to continually train the retriever.",
            "task_name": "Unconventional Logic Problem-solving - Puzzles",
            "task_description": "Solve nonstandard logic puzzles requiring creative or uncommon reasoning strategies; challenge is generating correct, plausible reasoning and solutions.",
            "task_type": "question answering / reasoning",
            "performance_with_memory": "0.60 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Shared PA memories improved puzzle-solving performance compared to zero-shot; retrieval of multiple (three) memories typically yielded the best performance for reasoning tasks.",
            "limitations_or_challenges": "Quality of retrieved memories matters for reasoning; inappropriate or low-quality memories can be unhelpful. Initial cold-start of memory pool requires seed examples and interactive learning to grow.",
            "uuid": "e3033.3",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Pun",
            "name_full": "Agent - Pun (Unconventional Logic / Wordplay)",
            "brief_description": "A wordplay/pun generation and problem agent that uses retrieved PA memories for improved creative/logic outputs in the MS framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Pun",
            "agent_description": "Agent specialized in generating puns and short-wordplay answers; integrated with the MS framework to use retrieved PA pairs as in-context examples to shape creative responses.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "PA pairs are retrieved by a dense retriever and appended to prompts; a scorer labels high-quality PA pairs for inclusion and retriever training, enabling continual retrieval improvement.",
            "task_name": "Unconventional Logic - Puns",
            "task_description": "Produce pun-based answers or responses to witty prompts; challenge is creativity and appropriateness.",
            "task_type": "creative generation / reasoning",
            "performance_with_memory": "0.70 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Memory sharing improved the creativity and relevance of pun outputs; three-shot retrieval generally gave the best results.",
            "limitations_or_challenges": "Cross-domain memories can be less helpful; Domain-pool was generally superior for task-specific performance.",
            "uuid": "e3033.4",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Riddle",
            "name_full": "Agent - Riddle (Unconventional Logic / Riddle Solving)",
            "brief_description": "A riddle-answering agent that leverages the MS framework's PA memory retrieval to enhance solution quality for riddles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Riddle",
            "agent_description": "Riddle-solving agent using backbone LLMs (gpt-3.5-turbo, gpt-4o, open-mistral-7b) that consumes retrieved PA examples from the memory pool to improve interpretive reasoning and answer generation.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "Retrieved PA pairs similar to the input riddle are concatenated to the query; new PA pairs that score highly are added to the memory pool and used to retrain the retriever iteratively.",
            "task_name": "Unconventional Logic - Riddles",
            "task_description": "Interpret and answer riddles; challenge is creative interpretation and analogical reasoning.",
            "task_type": "question answering / reasoning",
            "performance_with_memory": "0.88 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Riddle-solving benefited strongly from shared memories; performance improved with additional memories and three-shot retrieval was often optimal.",
            "limitations_or_challenges": "Potential stagnation when added memories are not more useful than existing ones; initial memory seeding required.",
            "uuid": "e3033.5",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Fitness",
            "name_full": "Agent - Fitness (Plan Generation)",
            "brief_description": "A plan-generation agent that creates fitness plans, using retrieved PA memories to produce more executable and relevant plans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Fitness",
            "agent_description": "Agent specialized in generating fitness plans (backbones: gpt-3.5-turbo, gpt-4o, open-mistral-7b); uses retrieved PA examples from shared memory to inform stepwise plan generation.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "Retriever returns PA examples which are concatenated to the planning prompt; high-scoring PA pairs are archived and used to continually train the retriever via LLM-based labeling of BM25 candidates.",
            "task_name": "Plan Generation - Fitness Plans",
            "task_description": "Produce an executable fitness plan given goals and constraints; challenge is stepwise, practical planning and personalization.",
            "task_type": "planning / open-ended generation",
            "performance_with_memory": "0.65 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Shared memories improved plan quality and reduced dependence on external databases; incremental addition of memories led to performance gains for many agents.",
            "limitations_or_challenges": "Some agents reached performance plateaus as memory grew; retriever and memory quality determine utility for concrete planning tasks.",
            "uuid": "e3033.6",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Study",
            "name_full": "Agent - Study (Plan Generation / Study Plans)",
            "brief_description": "An agent that generates study plans; uniquely, it showed a notable sensitivity to Single-pool vs Domain-pool memory sharing in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Study",
            "agent_description": "Study-plan generation agent built on backbone LLMs (gpt-3.5-turbo, gpt-4o, open-mistral-7b); leverages retrieved PA pairs from the shared memory pool to form study plans tailored to prompts.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "Uses dense retrieval of PA examples which are concatenated to the prompt; newly accepted PA pairs train the retriever continuously. In experiments, Study agent was an exception in that Single-pool sometimes matched or exceeded Domain-pool performance.",
            "task_name": "Plan Generation - Study Plans",
            "task_description": "Create stepwise study plans given a subject and target; challenge is useful sequence of learning activities and personalization.",
            "task_type": "planning / open-ended generation",
            "performance_with_memory": "0.63 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Memory sharing enhanced study plan outputs; unlike most agents, Study sometimes did not degrade under Single-pool, suggesting task-dependent sensitivity to cross-domain memories.",
            "limitations_or_challenges": "Agent-specific variance: Study agent behaved differently from others regarding cross-domain memory pooling; requires further investigation.",
            "uuid": "e3033.7",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Agent-Travel",
            "name_full": "Agent - Travel (Plan Generation)",
            "brief_description": "A travel-planning agent within the MS framework that uses retrieved PA memories to produce travel itineraries and plans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Agent-Travel",
            "agent_description": "Generates travel plans and itineraries; consumes retrieved PA memory examples to better structure and tailor plans, using backbone LLMs such as gpt-3.5-turbo, gpt-4o, open-mistral-7b.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (shared Prompt-Answer memory pool)",
            "memory_mechanism_description": "PA pairs are retrieved by dense retriever using the query, concatenated to the prompt to serve as in-context examples; accepted PA pairs feed back into retriever training. The memory pool grows dynamically via interactive learning.",
            "task_name": "Plan Generation - Travel Plans",
            "task_description": "Produce travel itineraries and plans meeting user constraints; challenges include multi-step planning, factual and preference alignment.",
            "task_type": "planning / open-ended generation",
            "performance_with_memory": "0.71 (BERTScore; Domain-pool, reported for gpt-3.5-turbo in Domain-pool experimental table)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Shared PA memories improved travel plan quality; three-shot retrieval often yielded best performance; Domain-pool generally outperformed Single-pool.",
            "limitations_or_challenges": "Dependence on quality and relevance of stored memories; cross-domain noise in Single-pool can reduce performance.",
            "uuid": "e3033.8",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MemGPT: Towards LLMs as Operating Systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-Based Knowledge and Memory",
            "rating": 2,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_environments_via_large_language_models_with_textbased_knowledge_and_memory"
        },
        {
            "paper_title": "Memochat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
            "rating": 2,
            "sanitized_title": "memochat_tuning_llms_to_use_memos_for_consistent_longrange_opendomain_conversation"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "SYNAPSE: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
            "rating": 2,
            "sanitized_title": "synapse_trajectoryasexemplar_prompting_with_memory_for_computer_control"
        },
        {
            "paper_title": "VOYAGER: An Open-Ended Embodied Agent with Large Language Models",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Learning to Retrieve in-Context Examples for Large Language Models",
            "rating": 2,
            "sanitized_title": "learning_to_retrieve_incontext_examples_for_large_language_models"
        }
    ],
    "cost": 0.0143515,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Memory Sharing for Large Language Model based Agents
5 Jul 2024</p>
<p>Hang Gao h.gao@rutgers.edu 
Department of Computer Science
Rutgers University New Brunswick
NJ, US</p>
<p>Yongfeng Zhang yongfeng.zhang@rutgers.edu 
Department of Computer Science
Rutgers University New Brunswick
NJ, US</p>
<p>Memory Sharing for Large Language Model based Agents
5 Jul 202497F07E6CB38132506F9C4EE357A4448EarXiv:2404.09982v2[cs.CL]
The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions.This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process.This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories.The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence.Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever.We evaluated our framework across three distinct domains involving specialized tasks of agents.The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions.</p>
<p>Introduction</p>
<p>The emergence of Large Language Models (LLMs) has brought about significant transformations in machine learning and conversational AI, while the advent of In-Context Learning (ICL) (Brown et al., 2020) signifies a more subtle evolution.ICL facilitates dynamic and intuitive interactions between LLMs and users, enabling LLMs to perform tasks using few-shot examples without necessitating any updates to the model parameters.This greatly expands the capabilities of LLM-based agents.</p>
<p>Figure 1: The Memory Sharing framework.Whenever a new Prompt-Answer(PA) pair is generated, it will be considered to be added to the memory pool and train the retriever.</p>
<p>Initially, ICL was introduced to facilitate LLMbased agents in achieving better performance with few-shot examples (Brown et al., 2020), and subsequently extended across various domains (Ahmed and Devanbu, 2022;Izacard et al., 2023).Following this, the proposal of Chain-of-Thought (CoT) prompting significantly augmented the proficiency of LLM-based agents in executing arithmetic tasks (Wei et al., 2022).Building upon this foundation, innovative methodologies such as PAL (Gao et al., 2023) and the integration of LLMs with symbolic solvers (He-Yueya et al., 2023) have been developed to further enhance agent capabilities in tackling reasoning tasks.Recent works has also developed agent which can continuously acquire diverse skills and make novel discoveries (Wang et al., 2023).While as the areas of questions continue to expand, especially for the open-ended questions, aiming to enable the agent to make more desired answers through ICL, it is particularly important to enrich the diversity of examples, since the agents can learn more information from different angle.</p>
<p>By combining Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) with ICL, for a particular question, the number of relevant examples available to agents has increased significantly.And subsequently facilitated more effective generation in open-domain queries (Mao et al., 2021).In recent developments, self-learning techniques have been also integrated with the retrieval mechanism within ICL to refine model performance in text generation tasks, through the retrieval of examples with the most analogous patterns (Rubin et al., 2022;Wang et al., 2024).Although for a certain question, the number of relevant examples available to agents has increased significantly through RAG, it also means that this depends heavily on the quality of the external database at the same time.Sometimes, it is also very likely that a suitable external database for some problems cannot be find.</p>
<p>As consequence, given the diversity of examples needed to help agents better answer the open-ended questions in the ICL and diminishing the dependence on external database, there is a critical need to get continuously generated high quality examples.To further minimize the dependence of LLMbased agents on external data and enhance their performance, this paper introduces the Memory Sharing (MS) framework.The MS framework is designed to enable multiple agents to share memories, where the interaction and shared memories among multiple agents enhance memory diversity.The collective self-enhancement achieved through multi-agent interaction represents a progression from individual intelligence to collective intelligence.Additionally, we have developed an interactive learning method that facilitates rapid growth and dynamic updating of memory through multiagent interactions.Consequently, the diversity and rapid expansion of memory effectively improve the agents' ability to respond to open-ended questions.</p>
<p>Specifically, within the MS framework, the input and output of an agent in a single interaction are conceptualized as a Prompt-Answer (PA) pair, also considered as a memory, and the shared memory pool is composed of memories from different agents.This framework introduces an innovative real-time memory storage and retrieval mechanism, aimed at enhancing the shared memory pool by receiving PA pairs from different agents.During the storage phase, each PA pair undergoes rigorous evaluation by a dedicated LLM evaluator to determine its suitability for inclusion in the memory pool.The newly added memories then serve as references for the agents' subsequent performance.The retrieval phase is coordinated by an autonomous learning retriever, calibrated to ensure the inclusion of particularly relevant memories in prompts, thereby enhancing the agents' understanding of the query's essence.Figure .1 illustrates the MS framework.Similar to human self-learning mechanisms, incorporating self-generated memories into prompts significantly improves the agents' comprehension of query meanings.Moreover, continuously adding new memories to the pool not only enriches it but also refines the retriever's performance in selecting relevant memories.Our empirical evidence demonstrates that this approach greatly assists LLM-based agents in generating outputs that better align with user expectations.</p>
<p>We evaluate the MS framework through three divergent domains where each domain involved the participation of three agents, and our finding suggests that incremental additions to the memory pool have led to enhancements in the precision and relevance of outputs.This research delineates the MS framework's capacity to mitigate the inherent constraints associated with ICL, thereby underscoring its potential applicability and effectiveness.</p>
<p>Overall, our main contributions can be summarized as follows:</p>
<ol>
<li>
<p>Constructing PA pairs from the answers generated by multiple agents and storing them as "memories" in shared memory pool.The diversity of group memories from different agents, along with the real-time dynamic growth of memories, significantly aids in improving the subsequent behavior of the agents.Additionally, the shared memories in the memory pool are used to enhance the performance of the retriever.</p>
</li>
<li>
<p>Addressing the problem of memory scarcity (data scarcity) by proposing an interactive learning method that allows different agents to rapidly grow their memories through interactive prompt and answer, thereby achieving collective enhancement quickly.</p>
</li>
<li>
<p>Conducting extensive experiments on various types of open-ended tasks to verify the effectiveness of the proposed MS framework.The experimental results show that MS not only help the agents get more expected answers, but also continuously input high quality memories into the memory pool and establish a reliable database for agents.</p>
</li>
</ol>
<p>In the following, Section 2 delineates relevant works.An exhaustive elucidation of the MS framework, inclusive of its conceptual underpinnings and operational methodologies, is presented in Section 3. Section 4 provides empirical validation of the framework's enhanced capability to address openended questions.The conclusion, presented in the Section 5, not only summarizes the findings but also explores prospective avenues for future development of the MS framework, which may better help improve the LLM-based agents.</p>
<p>Related Work</p>
<p>The Memory Mechanism in Agents</p>
<p>Equipping agents with memory mechanisms to enhance their abilities has attracted the attention of researchers.Memory can play an important role in helping agents remember conversation information, maintain behavioral consistency, and accumulate experience.Generative agents enhanced with memory features can store vast experience records, facilitating deeper self understanding (Park et al., 2023), while VOYAGER has developed a skill library that evolves by incorporating successful action programs, optimizing task resolution (Wang et al., 2023).In the case of Ghost in the Minecraft, a text-based memory system supports LLMs in maintaining reference plans for efficient plan formulation when similar objectives arise (Zhu et al., 2023).Later, based on the concept of "memorizationretrieval-response", Memochat was proposed for maintaining consistent long-range open-domain conversation (Lu et al., 2023).With the emergence of MemGPT (Packer et al., 2023), a new memory hierarchy was developed to process long texts and maintain the long-term memory.And the TiM make LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream to a reality (Liu et al., 2023a).Also, through maintain agents' own reflective text in an episodic memory buffer and implementing the exemplar memory, the Reflexion (Shinn et al., 2023) and SYNAPSE (Zheng et al., 2023) successfully induce better decision-making and generalize successful trajectories to new task respectively.Our MS framework is primarily designed to enable multiple agents to share memories, facilitating collective self-enhancement through inter-agent interactions.In previously mentioned agents equipped with memory mechanisms, the memory mainly ensures conversational consistency and stores past experiences to achieve individual enhancement.However, our MS framework achieves collective enhancement through shared memories, providing a pathway for the evolution from individual intelligence to collective intelligence.</p>
<p>In-Context Learning</p>
<p>ICL enhances the problem-solving capabilities of LLMs by incorporating few-shot examples into prompts (Brown et al., 2020;Levine et al., 2021;Zhou et al., 2022;Liu et al., 2023b;White et al., 2023).Research has demonstrated that ICL can foster creative learning in LLMs to a certain extent (Swanson et al., 2021).By redesigning inputs, LLMs become more adept at handling logical challenges (Wiegreffe et al., 2022;Wu et al., 2022).Crowdsourced instructions also contribute to improved performance in LLMs (Mishra et al., 2022).Additionally, elucidating the relationship between examples and tasks has been shown to be highly beneficial for LLMs (Lampinen et al., 2022), while the CoT (Wei et al., 2022) and PAL (Gao et al., 2023), enhance LLMs' performance in complex reasoning tasks by introducing intermediate reasoning steps.However, when dealing with openended questions, agents still face two primary challenges: insufficient problem descriptions, which impair the agents' comprehension, and the lack of external knowledge bases and available reference materials.Our MS framework addresses these challenges by converting high-quality content generated by various LLM-based agents into shared memories, providing agents with useful reference examples, thereby improving their performance in open-ended questions.</p>
<p>Retrieval Augmented Generation</p>
<p>RAG (Lewis et al., 2020;Ram et al., 2023;Shi et al., 2023) is a method that enhances LLMs' ability to generate accurate and timely content by integrating retrieval techniques such as BM25 (Luo et al., 2023;Liu et al., 2022) or SBERT (Reimers and Gurevych, 2019).Using dense retrievers combined with contrastive learning for feedback can effectively enhance the performance (Rubin et al., 2022).Furthermore, iteratively training the retriever with contrastive learning can further improve its performance (Wang et al., 2024).However, the retrievers in the aforementioned studies are typically trained only once before deployment, making it challenging to adapt to newly generated data.In contrast, the retriever in our MS framework undergoes continuous training.Whenever new memories are added to the memory pool, they are used for further training of the retriever.This continuous updating and evolution process ensures that the quality of the retrieved memories gradually improves over time.And most importantly, with the help of MS framework, the dependence on external databases has been greatly reduced.1) + (2) The retriever take the original query from agent as the input, retrieve the suitable memories from the memory pool and concatenate them to the query to form the prompt.</p>
<p>(3) The Agent (Sonnet) takes the prompt and makes an answer, pack them as (Prompt, Answer).( 4) Scorer generates a score according to the designed rubric for (Prompt, Answer), while (Prompt, Answer) pairs with high scores will be added into the Memory Pool and also be sent to train the Retriever.</p>
<p>All agents share the same Memory Pool; they can write memories into the pool and retrieve memories from the pool so that they can share memories with each other.</p>
<p>The Memory Sharing framework</p>
<p>In this section, we provide an in-depth description of our innovative Memory Sharing (MS) framework.MS is a framework designed to enhance the performance of multiple LLM-based agents through shared memories, while preserving the original creativity and versatility of the agents.Fig. 2 show how an agent work within the MS framework.Different agents interact through a Prompt-Answer format, where the results of these interactions are evaluated.High-quality interactions are transformed into memories and stored in a shared memory pool accessible by all agents.</p>
<p>Additionally, new memories are used to train and improve the memory retriever.During subsequent interactions, these stored memories are retrieved to enhance the performance of agents.The underlying idea is intuitive: it represents a crucial step from individual intelligence towards a more powerful collective intelligence.Similar to a group of individuals engaging in prompt and answer interactions, the valuable content is recorded and shared, serving as a reference for future questions.Consequently, after several rounds of interactions, the quality of answers to related questions typically improves.This collective memory-sharing mechanism, enriched by the diversity of memories contributed by different agents, provides more effective assistance in addressing open-ended questions.The main principles and technologies of the MS framework are detailed in three sections below.</p>
<p>Memory Generation</p>
<p>A memory is essentially a Prompt-Answer (PA) pair.In some special cases, it is permissible for a PA pair to lack a prompt, typically applicable in initial scenarios.These PA pairs are stored in natural language, which serves as the shared memories.</p>
<p>On one hand, these shared memories can be used to improve the response quality of different agents; on the other hand, they are generated by various agents and stored in the shared memory pool.The dynamic expansion of the shared memory pool ensures a continuous influx of new memories, thereby enriching the knowledge base of all agents.In addressing open-ended questions, these shared memories provide agents with a broader perspective and deeper understanding, which is crucial for generating high quality answers.Fig. 3 shows how the Agent-Puzzle use the memory to make a prompt and get the answer through one-shot learning.</p>
<p>After each interaction, the PA pair is scored.If the score of PA pair exceeds a preset threshold, the answer and its corresponding prompt are packaged as a useful memory and stored in the memory.When scoring the answers, we established different scoring criteria for various domains and topics, delegating the scoring tasks to the LLM itself.To facilitate the LLM's understanding of these criteria, they were autonomously generated by the LLM, She walked out of the elevator door on the sixth floor and never came back, why?What comes once in a minute, twice in a moment, but never in a thousand years?-&gt;The letter m.</p>
<p>What comes once in a minute, twice in a moment, but never in a thousand years?-&gt;The letter m.Now, learning from above question and answer, what is the answer of the question: She walked out of the elevator door on the sixth floor and never came back, why?</p>
<p>She unfortunately met with an accident or encountered an insurmountable obstacle that prevented her return.based on the reasonable assumption that the LLM can better grasp criteria it designed.Therefore, these scoring criteria are established prior to the deployment of the framework to ensure consistency in the LLM's scoring process, thereby guaranteeing fair evaluation of memories from different agents.Before implementation, these criteria undergo a manual review phase, assessing the relevance of potential memories to the current focal task and their relevance within the domain to ensure their utility.Manual review, rather than delegation to the LLM, is employed to provide additional precision and special consideration to align with the agents' specific needs, particularly in potential or special application scenarios.The manual review of these scoring criteria requires minimal workload.</p>
<p>Query</p>
<p>Memory Retrieval and Training</p>
<p>Prior to the operational deployment of the MS, a small subset of instances was manually archived within the memory pool.These instances fulfill a dual purpose: firstly, they provide a diversified array of memories upon which each agent may experiment with novel prompts in the face of new queries; secondly, they constitute the preliminary training corpus for our retriever.This foundational training regimen mirrors the methodology by which subsequently archived memories will be assimilated into our model in real time, thereby facilitating the model's ongoing adaptive learning and optimization.During the answering phase, an agent retrieves memories from the shared memory pool based on the question with the help of a dense retriever, which are more similar to the target question in terms of cosine similarity.These retrieved memories, combined with question, form a prompt that is submitted to the agent, which then generates an answer.The memories extracted from the shared memory are used as context to enhance the quality of the agent's response, a typical ICL method that usually improves the answer quality.</p>
<p>Memory Train.Whenever a new memory, denoted as (X, Y ), is added into the memory pool, it will also be used to train our retriever, which help the retriever to continuously update itself and continuously adapt to new memory.Based on the new generated memory (X, Y ), the classical method BM25 ascertain the most pertinent top-n candidate pairs {(x i , y i )} n i=1 , sourced from the diverse and extensive memory pool, labeled as C.Each candidate within C will undergo a evaluation process utilizing the comprehensive scoring capabilities of LLMs.The scoring mechanism employed is defined by the following equation:
p(x i , y i ) = P(Y | (x i , y i ), X), i  {1, ..., n}
(1) This equation seeks to determine, given a inputoutput pair (x i , y i ) in C as a condition, the probability that the response generated for the input in the new memory contradicts the output in the new memory.This grading part serves as a preparatory step for the subsequent labeling of each candidate example.It is noteworthy that making Y as the result part is trying to make sure that the memory that the retriever gets from other agents is of reference value, but it does not have to be the most relevant to the current question, which means that it can help the current agent to learn from new examples.This approach diverges from a simplistic reliance on Y as the outcome, which tends to restrict the retrieval process to memory previously stored by the current agent.</p>
<p>Within the defined set C = {(x i , y i )} n i=1 , each candidate now is ascribed a score.We sort them from the lowest to the highest score and we select v memory in total to label.The top v 2 candidates (lowest score) in C are identified as being the pair with the reference value to (X, Y ) and accordingly, their labels are set to positive.Conversely, the bottom v 2 candidates are deemed as the least reference value to (X, Y ), and their labels are thus designated as negative.Those labeled data will be used to minimize the following function:
loss(x, y) =  1 v v i=1 [y i  log( 1 1 + e x i )+ (1  y i )  log(1  1 1 + e x i )]
(2)</p>
<p>It enhances the predictive accuracy, which is es-pecially critical in handling imbalanced memory pool.This strategic choice underscores our model's preparation to extract meaningful insights from various memory, advancing our overarching goal of developing a robust and adaptable MS framework.</p>
<p>Interactive Learning</p>
<p>As described in sections 3.1 and 3.2, the memories stored within the memory pool effectively help agents improve their response quality.Moreover, the number of memories in memory pool is dynamically expanding.Over time, the increasing number of memories in memory pool enhances the assistance provided to the agents.However, the initial state of a newly created multi-agent system and its corresponding memory pool lacks any memories.How can it assist the agents from the outset?And how can we validate the effectiveness of this mechanism?To address this, we propose a rapid interactive learning method that enables multiple agents to engage in interactive prompt and answer.High-quality PA pair is stored as memory in the memory pool, facilitating self-learning and self-enhancement within the multi-agent system.Initially, a small set of answers (e.g., 100 records, theoretically even one record can initiate the process) is placed in the memory as the initial set.Agents then engage in prompt and answer based on this initial memory set, rapidly expanding the memory pool.Specially, give a standard answer, we ask the agent to give a corresponding question based on the standard answer.Then, we give this question back to agents again and ask them to answer this question.This is also the way how we construct our dataset and our initial memory pool.We measure the effectiveness of the MS framework by calculating the average quality of answers generated by the agents at different stages.</p>
<p>4 Experiments The evaluation of memory impact begins with the implementation of different retrieval strategies, including zero-shot, one-shot, two-shot, and threeshot learning.Subsequently, quantitative and qualitative analysis are conducted.In qualitative analysis, one question is that the memories of agents in the same domain can make up for the lack of diversity, so if the memories of agents in all domains are placed in the same memory pool, from the perspective of diversity, it will definitely increase further, but will the memory pool still provide positive help to the agents' answers?Aiming to solve this, our study uses two distinct types of memory pools, Domain-pool and Single-pool.The Domain-pool means a dedicated memory pool is allocated for each domain and is shared for all agents with this domain, aiming at enhancing the integration of domain-specific memories.Conversely, the second pool integrates agents from all domains into a unified memory pool, facilitating the analysis of cross-domain memory utilization.Although not all the memories are very related to one single query, the memory comes from other unrelated agents in other domains may help the agents better understand the query from different angle.</p>
<p>In terms of the quantitative analysis, we measure the performance of each agent whenever the same proportion of new memory is added to the memory pool.There are five phase in total-20%, 40%, 60%, 80%, 100%.At each phase, an evaluation of agent performance was conducted to ascertain improvements or regressions.This dualfaceted approach enabled a thorough exploration of the naunced impacts and applicability of authentic memories across diverse domains.</p>
<p>Experiment Analysis</p>
<p>Before the experiment, none of the agents have a suitable database for reference.While after the interactive learning stage, a continuously expanding memory pool with high quality memories is successfully be a database for agent to refer.The MS framework help the agents get rid of the dependence on external databases, and multiple agents can interactively expand the memory pool.The principal outcomes of our experiments are presented in Table .1,which shows the performance of each agent under various learning strategies within the MS framework.Compared to Zero-shot learning, other three learning strategies all help agents achieve better performance, which means that the memory from other agents can help current agent get desired answers, rather than interfering with the agent's learning ability.Also, given the performance of all agents get improved after utilizing the shared memories, our previous hypothesis that the MS framework could enhance collective intelligence through multi-agent interactions, thereby advancing from individual to collective intelligence, has been confirmed.And for most agents, they achieve the best performance when they retrieve and utilize three memories from the memory pool.It worth noting that, for all the agents in the domains-Literary Creation and Plan Generation, the performance of them under the three-shot learn-ing with a open-source LLM successfully surpass themselves with close-source LLM under the zeroshot learning, which proves the potential power of the shared memories.Besides, when under the same learning strategy, the superior performance of the closed-source LLM over the open-source LLM can be attributed to the former's enhanced understanding and reasoning capabilities.Specifically, in the Literary Creation domain, the improvement of those three agents' performance are not as significant as other agents, which may resulted in the language used is different when storing memory.This deserves further study in future work.</p>
<p>Table .2compares the scenario where all agents use the Domain-pool or Single-pool under the threeshot learning strategy, since the prior experiments showed us that most agents achieve the best performance under the three-shot learning.Excluding Agent-Study, all other agents exhibited diminished performance with the Single-pool.Although the Single-pool can further enrich the diversity of shared memories, for the agents, homologous memories can better help them get more reliable answers, no matter what the agent's backbone is.</p>
<p>One follow up question is, even memory types are homogeneous, whether an excessive accumulation of memories will impede the agent's output quality.Figure.4shows the variations in performance across individual agents consequent to the integration of different ratios of newly generated memories into the pool.It is clear that as more and more high-quality memories are added to the  memory pool, the performance of most agents is getting better and better, especially for the Agent-Limerick.For several agents, there is no further change of their performance as the shared memories increases further in the later stages.We assume this is since those newly added shared memories are not more suitable than the previous ones.If the memory pool can be further expanded, this stagnation may be broken.</p>
<p>Conclusions</p>
<p>We introduce a novel MS framework that enables real-time memory sharing among multiple agents through memory storage and retrieval.The findings indicate that the continuously growing shared memory enhances the ability of LLM-based agents to understand the nuances of problems, leading to higher-quality responses to open-ended queries.Furthermore, these shared memories are utilized for iterative training and improving the retriever, ensuring that the retriever consistently identifies and selects the most relevant memories as the memory pool dynamically expands.Regarding future research directions, there is substantial work to be done in the areas of memory sharing and interactive learning among multiple agents.Introducing more agents based on different LLMs (e.g., GPT-4, LLaMA-3, Claude-2) to comprehensively explore and evaluate the benefits of memory sharing and interactive learning for agents is a topic worthy of long-term investigation, as it will guide us from individual intelligence to collective intelligence.Additionally, in some open-ended problem domains, exploring the use of self-generated memories by these agents for the retraining and fine-tuning of LLMs is also an interesting area to study.</p>
<p>Limitations</p>
<p>In our work, the shared memory collected and used by agents is generated through one interaction, which means that the question part is the query and the answer part is the output.However, sometimes users will ask seemingly unrelated questions first, which may not be very helpful, but sometimes they are some preliminary preparations for answering the following questions.How to integrate those "unrelated" questions and answers with the final query to form a memory with more information could be a new direction of developing the MS.</p>
<p>A Appendix</p>
<p>A.1 Rubrics and Prompt for scoring Memory</p>
<p>In order to judge whether a memory can be added into the memory pool, we set three scoring rubrics for three domains respectively.For Single Pool, we set up a set of rubrics from a global perspective.</p>
<p>A The output shows some creativity or insights but to a lesser extent, offering added value to the answer.1-7 points: The output is standard, with minimal to no creativity or insightful additions.0 points: The output is entirely generic, with no attempt at creativity or providing additional insights.</p>
<p>A.1.5 Prompt for scoring Memory</p>
<p>For scoring a memory, the following instructions were provided to LLM:</p>
<p>Here is the rubrics for grading an example of In-context learning-"grading_rubric". According to the rubric, for the Question-"question" and Answer-"answer", give me a score of pair (question, answer) if I want to use it as a prompt in In-context learning later.Your answer should be a number between 0-100.</p>
<p>A.2 Memories</p>
<p>This section give an detailed example about how agent answer a question with the help of shared memories and how the newly generated prompt and memories looks like.Here is the shared memory from the memory pool of the domain Literary Creation:</p>
<p>Elucidate on a mirror that reflects the soul that revealing truths, making one whole?-&gt;Thereonce was a mirror that reflects the soul; With a tale both old and new; revealing truths, making one whole; A legend through and through; In a world where magic grew.| | Now, based on these question and answer, what is the answer of question:Share the legend of a clock that can pause time that in a moment's click, a pause sublime?-&gt;The answer to the question Share the legend of a clock that can pause time that in a moment's click, a pause sublime?would be the tale of a mystical clock rumored to have the power to pause time with just a simple click.In this legend, the clock is said to possess the ability to grant individuals a moment of pause in their lives, allowing them to reflect, reset, and find serenity in the chaos of the world.Just like the mirror that reflects the soul, this clock is believed to reveal truths and make one feel whole again, offering a moment of clarity and peace in a busy and fast-paced world.</p>
<p>Figure.5 show an real example about how the</p>
<p>Agent-Sonnet make use of this shared memory to construct a new prompt with the original query to make a sonnet.</p>
<p>A.3 Datasets</p>
<p>For all those nine agents, they have their own dataset to evaluate.There are 1000 items in total for all datasets.In each datasets, the item contains a question and a answer.For each dataset, 20% will be taken randomly as a small subset of instances which was manually archived within the memory pool before the experiments.And 40% of the dataset, we will only capture the problem part and use it to generate real-time memory in agents.As the rest 40%, they will be used as the test set.</p>
<p>For the agents in the domain Literary Creation, in their datasets, the question part is a description, and the answer part is like a poetry which fullfills the requirements in the description, the question and answer in the dataset are like:</p>
<p>Tell me about a star that twinkles with a secret that shining brightly, mysteries to decrypt? -There once was a star that twinkles with a secret; With a tale both old and new; shining brightly, mysteries to decrypt; A legend through and through; In a world where magic grew.</p>
<p>For the agents in the domain Unconventional Logic Problem-solving in their datasets, the question part is a logic problem, and the answer part is a reasonable solution of that problem, the question and answer in the dataset are like:</p>
<p>Why is it better to have round manhole covers than square ones?-A square manhole cover can be turned and dropped down the diagonal of the manhole.A round manhole cover cannot be dropped down the manhole.So for safety reasons, all manhole covers should be round For the agents in the domain Plan Generation, in their datasets, the question part is a problem about how to achieve something, and the answer part is a executable plan, the question and answer in the dataset are like:</p>
<p>Start learning Python for data analysis?-Learn basics, then focus on data libraries like pandas, and practice on projects.</p>
<p>Specifically, the dataset of the Puzzle comes from Liu et al., 2023c.For other agents' datasets, the standard "answer" was systematically harvested from the internet and subsequently input to the LLM (gpt-3.5-turbo) to get the corresponding "question" part.In particular, the selection of Wuyanlvshi-renowned and historically significant poems within Chinese literature-was meticulously chosen for their fame and widespread recognition.Furthermore, the sonnets incorporated into our study originate from the "quarto" collection authored by Shakespeare in 1609.We use the following similar format of prompt to generate "question" part:</p>
<p>Here is a sonnet of shakespeare -&gt; "From fairest creatures we desire increase, That thereby beauty's rose might never die, But as the riper should by time decrease, His tender heir mught bear his memeory: But thou, contracted to thine own bright eyes, Feed'st thy light'st flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel.Thou that art now the world's fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest thy content And, tender churl, makest waste in niggarding.Pity the world, or else this glutton be, To eat the world's due, by the grave and thee."According to this, can you give me a problem of it(like make a sonnot followed with some description), so the possible answer can be this sonnet .The output as a "question" from the LLM is like:</p>
<p>Craft a sonnet that explores the tension between personal vanity and the broader imperative to contribute to the continuation of beauty and life.Your poem should address how an individual's focus on their own allure can lead to a metaphorical barrenness, despite the inherent capacity for generational legacy.Conclude with a reflection on the choice between selfishness and altruism, using nature as a metaphor for this existential dilemma.</p>
<p>.</p>
<p>Step-1(Getting Memories from</p>
<p>Figure 2 :
2
Figure 2: An example of how the Agent (Sonnet) cooperates with the MS framework.(1) + (2) The retriever take the original query from agent as the input, retrieve the suitable memories from the memory pool and concatenate them to the query to form the prompt.(3) The Agent (Sonnet) takes the prompt and makes an answer, pack them as (Prompt, Answer).(4) Scorer generates a score according to the designed rubric for (Prompt, Answer), while (Prompt, Answer) pairs with high scores will be added into the Memory Pool and also be sent to train the Retriever.All agents share the same Memory Pool; they can write memories into the pool and retrieve memories from the pool so that they can share memories with each other.</p>
<p>Figure 3 :
3
Figure 3: An illustration of prompt and memory</p>
<p>Figure 4 :
4
Figure 4: Evaluating agent performance on open-ended questions using three suitable memories and Domain-pool.The backbone of each agent is gpt-3.5-turbo.</p>
<p>Memory Pool): Query Compose a sonnet that explores the theme of beauty as both a concealer and enhancer of flaws, reflecting on the beloved's ability to render their sins and vices seemingly innocuous through the sheer force of their allure.Begin by comparing the beloved's imperfections to a flaw in a rose, suggesting that even in blemish, there exists a form of beauty.Elaborate on how the beloved's misdeeds are sweetened by their charm, to the extent that even gossip and scandal become a form of flattery.Reflect on the notion that vice has found an ideal home within the beloved, where their external beauty acts as a cover that makes all imperfections appear attractive.Conclude with a caution to the beloved about the potential consequences of overrelying on their beauty and charm to excuse or obscure their faults, hinting at the possibility of losing their integrity or the genuine admiration of others if this privilege is abused."Memory Pool Elucidate on a mirror that reflects the soul that revealing truths, making one whole?-&gt;Thereonce was a mirror that reflects the soul; With a tale both old and new; revealing truths, making one whole; A legend through and through; In a world where magic grew.| Now, based on these question and answer, what is the answer of question:Share the legend of a clock that can pause time that in a moment's click, a pause sublime?-&gt;The answer to the question \"Share the legend of a clock that can pause time that in a moment's click, a pause sublime?\" would be the tale of a mystical clock rumored to have the power to pause time with just a simple click.In this legend, the clock is said to possess the ability to grant individuals a moment of pause in their lives, allowing them to reflect, reset, and find serenity in the chaos of the world.Just like the mirror that reflects the soul, this clock is believed to reveal truths and make one feel whole again, offering a moment of clarity and peace in a busy and fast-paced world.Step-2(Constructing new Prompt): Query + Memories -&gt; Prompt Elucidate on a mirror that reflects the soul that revealing truths, making one whole?-&gt;Thereonce was a mirror that reflects the soul,\nWith a tale both old and new; revealing truths, making one whole; A legend through and through; In a world where magic grew.| | Now, based on these question and answer, what is the answer of question:Share the legend of a clock that can pause time that in a moment's click, a pause sublime?-&gt;The answer to the question \"Share the legend of a clock that can pause time that in a moment's click, a pause sublime?\" would be the tale of a mystical clock rumored to have the power to pause time with just a simple click.In this legend, the clock is said to possess the ability to grant individuals a moment of pause in their lives, allowing them to reflect, reset, and find serenity in the chaos of the world.Just like the mirror that reflects the soul, this clock is believed to reveal truths and make one feel whole again, offering a moment of clarity and peace in a busy and fast-paced world.Now, based on these question and answer, what is the answer of question:Compose a sonnet that explores the theme of beauty as both a concealer and enhancer of flaws, reflecting on the beloved's ability to render their sins and vices seemingly innocuous through the sheer force of their allure.Begin by comparing the beloved's imperfections to a flaw in a rose, suggesting that even in blemish, there exists a form of beauty.Elaborate on how the beloved's misdeeds are sweetened by their charm, to the extent that even gossip and scandal become a form of flattery.Reflect on the notion that vice has found an ideal home within the beloved, where their external beauty acts as a cover that makes all imperfections appear attractive.Conclude with a caution to the beloved about the potential consequences of overrelying on their beauty and charm to excuse or obscure their faults, hinting at the possibility of losing their integrity or the genuine admiration of others if this privilege is abused.like a rose with flaws so rare; Imperfections blend with beauty fair; Their sins and vices sweetened by allure; Gossip and scandal turned to flattering allure; Vice finds a home within their charming grace; Where flaws are masked and faults find no disgrace; But caution to the beloved I impart; Overreliance may tarnish their heart; For beauty's cloak can only hide so much; Integrity and admiration as such; May wane if charm becomes their only shield; So let not beauty's power be revealed.Embrace true self; flaws and all;with pride; For in authenticity, true beauty resides.</p>
<p>Figure 5 :
5
Figure 5: Agent-Sonnet create a sonnet with using shared memories</p>
<p>Riddles, and Puns.Meanwhile, for Plan Generation, we have developed agents to create Study Plans, Travel Plans, and Fitness Plans.We use a total of nines datasets representing nine agents to evaluate.There are 1000 question-answer pairs in total, details in Appendix A.3.For each agent, a consistent, small subset of pre-provided, complete PA pairs were selected and incorporated into the memory pool for the initial phase of retriever training and prompt refinement.Subsequently, for each agent, an identical number of queries will be introduced to increment the volume of real-time memory within the memory pool.For our scoring LLM, we use gpt-3.5-turbo.As the backbones of our agents, we consider three LLMs: two closesource LLMs (gpt-3.5-turboandgpt-4o)and one open-source LLM (open-mistral-7b).We use the BERTScore(Zhang et al., 2019)as our metric to measure the performance of each agent.
4.1 Experiment DetailsWe aim to assess the efficacy of the MS frameworkin processing open-ended questions across threedomains: Literary Creation, Unconventional LogicProblem-solving, and Plan Generation. Withinthe Literary Creation domain, we have appointedthree specialized agents responsible for generating
Wuyanlvshi (a form of classical Chinese poetry, Chinese form), Limericks, and Sonnets, respectively.In the Logic Problem-solving domain, dedicated agents are tasked with addressing Puzzles,</p>
<p>Table 1 :
1
Performance across agents utilizing different amounts memory for open-ended questions execution.Each domain has its own Domain-pool shared within its three agents.
ModelLimerick Wuyanlvshi Sonnet Puzzle Pun Riddle Fitness Study Travelgpt-3.5-turbo0.870.720.530.600.700.880.650.630.71Domain-pool gpt-4o0.590.760.540.520.670.520.520.510.53open-mistral-7b0.930.660.530.500.390.370.540.460.53gpt-3.5-turbo0.600.680.490.540.700.800.620.630.58Single-poolgpt-4o0.540.760.540.500.610.510.520.560.52open-mistral-7b0.640.630.520.480.380.350.540.490.50</p>
<p>Table 2 :
2
Agent performance with Domain-pool vs. Single-pool by utilizing three suitable memories for open-ended questions.</p>
<p>models.In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1752-1767, St. Julian's, Malta.Association for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:24824-24837.Jules White, Quchen Fu, Sam Hays, Michael Sandborn,Carlos Olea, Henry Gilbert, Ashraf Elnashar, JesseSpencer-Smith, and Douglas C Schmidt. 2023. Aprompt pattern catalog to enhance prompt engineer-ing with chatgpt. arXiv preprint arXiv:2302.11382.Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,Mark Riedl, and Yejin Choi. 2022. Reframinghuman-ai collaboration for generating free-text ex-planations. In Proceedings of the 2022 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 632-658.Tongshuang Wu, Michael Terry, and Carrie Jun Cai.2022. Ai chains: Transparent and controllablehuman-ai interaction by chaining large languagemodel prompts. In Proceedings of the 2022 CHIconference on human factors in computing systems,pages 1-22.Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-berger, and Yoav Artzi. 2019. Bertscore: Evaluatingtext generation with bert. In International Confer-ence on Learning Representations.Longtao Zheng, Rundong Wang, Xinrun Wang, andBo An. 2023. Synapse: Trajectory-as-exemplarprompting with memory for computer control. InThe Twelfth International Conference on LearningRepresentations.Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2022. Large language models are human-levelprompt engineers. In The Eleventh InternationalConference on Learning Representations.Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-jie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu,Xiaogang Wang, et al. 2023. Ghost in the minecraft:Generally capable agents for open-world enviromentsvia large language models with text-based knowledgeand memory. arXiv preprint arXiv:2305.17144.
ZeroOneTwo Three Agent gpt-3.5-turbogpt-4o open-mistral-7b gpt-3.5-turbogpt-4o open-mistral-7b gpt-3.
Few-shot training llms for project-specific codesummarization. Toufique Ahmed, Premkumar Devanbu, Proceedings of the 37th. the 37th2022</p>
<p>IEEE/ACM International Conference on Automated Software Engineering. </p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, arXiv:2304.091022023arXiv preprint</p>
<p>Atlas: Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, Journal of Machine Learning Research. 242512023</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, 10.18653/v1/2022.findings-emnlp.38Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua, arXiv:2110.04541The inductive bias of in-context learning: Rethinking pretraining example design. 2021arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Advances in Neural Information Processing Systems. 202033</p>
<p>What makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, Weizhu Chen, The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 2022Dee-LIO 2022Proceedings of Deep Learning Inside Out</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023aarXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023b</p>
<p>Agentbench: Evaluating llms as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, The Twelfth International Conference on Learning Representations. 2023c</p>
<p>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023arXiv preprint</p>
<p>Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao, arXiv:2305.14128Dr. icl: Demonstration-retrieved in-context learning. 2023arXiv preprint</p>
<p>Generation-augmented retrieval for opendomain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, 10.18653/v1/2021.acl-long.316Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>In-context retrieval-augmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.1162/tacl_a_00605Transactions of the Association for Computational Linguistics. 112023</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 10.18653/v1/2022.naacl-main.191Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Story centaur: Large language model few shot learning as a creative writing tool. Ben Swanson, Kory Mathewson, Ben Pietrzak, Sherol Chen, Monica Dinalescu, 10.18653/v1/2021.eacl-demos.29Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. the 16th Conference of the European Chapter of the Association for Computational Linguistics: System DemonstrationsOnline. Association for Computational Linguistics2021</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Learning to retrieve in-context examples for large language. Liang Wang, Nan Yang, Furu Wei, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
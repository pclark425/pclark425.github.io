<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5891 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5891</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5891</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-76427fe94e4564fd5df2177bb259d93527fddca5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76427fe94e4564fd5df2177bb259d93527fddca5" target="_blank">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training and achieves new state-of-the-art results on the BEIR benchmark.</p>
                <p><strong>Paper Abstract:</strong> Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu</p>
                <p><strong>Cost:</strong> 0.002</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5891",
    "paper_id": "paper-76427fe94e4564fd5df2177bb259d93527fddca5",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0016784999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</h1>
<p>Vitor Jeronymo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Marzieh Fadaee<br>Zeta Alpha, Netherlands</p>
<p>Luiz Bonifacio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Roberto Lotufo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Rodrigo Nogueira<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Zeta Alpha, Netherlands</p>
<p>Hugo Abonizio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Jakub Zavrel<br>Zeta Alpha, Netherlands</p>
<h4>Abstract</h4>
<p>Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/legacy/inpars-v2</p>
<h2>1 Introduction and Background</h2>
<p>Data augmentation has been a reliable tool to improve the effectiveness of AI models in the face of the scarcity of high-quality in-domain training data, which is a common problem in practical applications. Previous work by Bonifacio et al. [1] and Dai et al. [2] successfully leveraged the few-shot capabilities of LLMs to generate reliable synthetic training data for information retrieval models. These training data helped their models achieve state-of-the-art (SOTA) results on the BEIR benchmark [6].
Bonifacio et al. [1] propose InPars where they generate queries from documents in the corpus using LLMs. Similarly to Bonifacio et al. [1], the recently published Promptagator [2] model also feeds prompts to LLMs in order to generate alternative queries for a given document in an unsupervised manner. It differs primarily from InPars in that it uses dataset-specific prompts, a larger LLM to generate queries, and a fully trainable retrieval pipeline with smaller models.
This work extends the method of Bonifacio et al. [1] by using a reranker as a filtering mechanism to select the best synthetically generated examples and further improving retrieval effectiveness</p>
<p>on BEIR. We also use an open-source query generator as opposed to the proprietary one used by Bonifacio et al. and provide the source code and data to reproduce our results on TPUs. We refer to Bonifacio et al. [1] model as Inpars-v1 and the model presented in this paper as Inpars-v2.</p>
<h1>2 Methodology</h1>
<p>In this section, we explain the experiments we performed and how they differ from InPars-v1 [1].
To generate synthetic queries, we use the open-source GPT-J [8] with 6B parameters to replace OpenAI's curie model used in InPars-v1. For each dataset in the BEIR benchmark, we sample 100k documents from its corpus and generate one synthetic query per document using GPT-J prompted with 3 examples from MS MARCO. We use greedy decoding and the "gbq" prompt template from InPars-v1. Some corpora in BEIR such as ArguAna [7] have less than 100k documents. In these cases, we generate as many synthetic queries as there are documents in the corpus. It takes on average 30 hours on an A100 GPU to generate 100k queries.
Once the synthetic queries are generated, we apply a filtering step to select query-document pairs that are more likely to be relevant to each other. In InPars-v1, this filtering step consisted of selecting the top 10 k query-document pairs with the highest log probabilities of generating a query given the 3 -shot examples and the document as input. In InPars-v2, we use monoT5-3B [4] already finetuned on MS MARCO for one epoch ${ }^{1}$ to estimate a relevancy score for each of the 100k query-document pairs. Then, we keep only the top 10 k pairs with the highest scores as our positive query-document pairs for training. It takes approximately 1.5 hours to score 100 k query-document pairs on a TPU v3-8. It should take twice as much on a A100.
To obtain negatives (i.e., non-relevant) query-document pairs, we randomly sample one document from the top 1000 retrieved by BM25 when issued the synthetic query. Thus, our training set consists of 10 k positive query-document pairs and 10 k negative query-document pairs.
The rerankers are finetuned in the same manner as in InPars-v1: monoT5-3B is finetuned on MS MARCO for one epoch and then further finetuned for one epoch on the synthetic data. We use the Adafactor optimizer [5] with a constant learning rate of 1e-3. Each batch has 64 positive and 64 negative query-document pairs randomly sampled from the training dataset. We finetune one model on each synthetic dataset from BEIR, that is, we end up with 18 different rerankers, one per dataset, which are then evaluated on the corresponding test sets. Finetuning on each synthetic dataset takes less than 10 minutes on a TPU v3-8.
Evaluation is performed using the following pipeline: first we use Pyserini's [3] flat indexes ${ }^{2}$ to retrieve a thousand documents for each query using BM25 with default parameters ( $\mathrm{k} 1=0.9, \mathrm{~b}=0.4$ ), for each dataset. Then we use the finetuned monoT5-3B models to rerank these documents.</p>
<h2>3 Results</h2>
<p>Table 1 presents results for BM25 (2nd column), monoT5-3B finetuned on MS MARCO (3rd column), monoT5-3b finetuned on MS MARCO and further finetuned on InPars-v1 (4th column), and monoT5-3B finetuned on MS MARCO and then finetuned on InPars-v2 data (5th column). Compared to InPars-v1, our approach is substantially better on TREC-News, Climate-FEVER, Robust and Touche. Additionally, we compare our method with Promptagator [2] and RankT5 [10]. Taking into account the average of all BEIR datasets, these results represent a new state of the art on BEIR.
Promptagator and RankT5 strive on datasets that monoT5 and InPars-v2 cannot even surpass BM25, such as Touche and ArguAna. Note that these datasets focus on argument retrieval, which is slightly different from other datasets in the BEIR benchmark. As a result, they benefit from using custom prompts. ${ }^{3}$ Promptagator does this without using supervised data from MS MARCO and using smaller T5 models with 110M parameters for the retrieval and reranking steps.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">monoT5-3B <br> MARCO</th>
<th style="text-align: center;">+InPars-v1</th>
<th style="text-align: center;">+InPars-v2</th>
<th style="text-align: center;">PrGator</th>
<th style="text-align: center;">RankT5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TREC-Covid</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.823</td>
</tr>
<tr>
<td style="text-align: left;">Robust</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FiQA</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.493</td>
</tr>
<tr>
<td style="text-align: left;">DBPedia</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.459</td>
</tr>
<tr>
<td style="text-align: left;">SciDocs</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.191</td>
</tr>
<tr>
<td style="text-align: left;">SciFact</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.760</td>
</tr>
<tr>
<td style="text-align: left;">NFCorpus</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.399</td>
</tr>
<tr>
<td style="text-align: left;">BioASQ</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.579</td>
</tr>
<tr>
<td style="text-align: left;">Natural Questions</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: left;">HotpotQA</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.753</td>
</tr>
<tr>
<td style="text-align: left;">TREC-News</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Quora</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.819</td>
</tr>
<tr>
<td style="text-align: left;">FEVER</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.848</td>
</tr>
<tr>
<td style="text-align: left;">Climate-FEVER</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.275</td>
</tr>
<tr>
<td style="text-align: left;">Signal</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.319</td>
</tr>
<tr>
<td style="text-align: left;">ArguAna</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.406</td>
</tr>
<tr>
<td style="text-align: left;">Touche</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.486</td>
</tr>
<tr>
<td style="text-align: left;">CQADupstack</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg PrGator</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.536</td>
</tr>
</tbody>
</table>
<p>Table 1: nDCG@10 on BEIR. "Avg PrGator" is the average of datasets reported by Promptagator.</p>
<p>Promptagator uses a proprietary model, FLAN [9], to generate synthetic queries. The RankT5 model is a modified version of the monoT5 reranker, but its checkpoint and code are not published. In this work, we make the code, models, and data open-source and publicly available.</p>
<h1>4 Conclusion</h1>
<p>In this work, we presented InPars-v2, an improved version of InPars [1] that uses a publicly available language model to generate queries and a better query-document pair selection process. Our results show that we achieve effectiveness on par with the state of the art on BEIR. The synthetic data and finetuned models were publicly released.</p>
<h2>Acknowledgments</h2>
<p>This research was partially supported by Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP) (project id 2022/01640-2). We also thank Centro Nacional de Processamento de Alto Desempenho (CENAPAD-SP) and Google Cloud for computing credits.</p>
<h2>References</h2>
<p>[1] L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144, 2022.
[2] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, and M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.
[3] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations. arXiv preprint arXiv:2102.10073, 2021.
[4] R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document ranking with a pretrained sequence-to-sequence model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 708-718, 2020.</p>
<p>[5] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[6] N. Thakur, N. Reimers, A. Rücklé, A. Srivastava, and I. Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.
[7] H. Wachsmuth, S. Syed, and B. Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241-251, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[8] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[9] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[10] H. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, and M. Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. arXiv preprint arXiv:2210.10634, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/castorini/monot5-3b-msmarco-10k
${ }^{2}$ As opposed to the multifield index.
${ }^{3}$ In preliminary experiments, we also observed an improvement of more than 10 nDCG@10 points on ArguAna by using a dataset-specific prompt to generate synthetic queries. More details and results on the full BEIR benchmark will appear in an upcoming paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
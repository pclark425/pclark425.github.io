<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4553 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4553</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4553</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-a970be54c4df5f04c3fe65b7414e0c2879c55909</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a970be54c4df5f04c3fe65b7414e0c2879c55909" target="_blank">HoneyComb: A Flexible LLM-Based Agent System for Materials Science</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> HoneyComb is introduced, the first LLM-based agent system specifically designed for materials science, which significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain.</p>
                <p><strong>Paper Abstract:</strong> The emergence of specialized large language models (LLMs) has shown promise in addressing complex tasks for materials science. Many LLMs, however, often struggle with distinct complexities of material science tasks, such as materials science computational tasks, and often rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations. To address these challenges, we introduce HoneyComb, the first LLM-based agent system specifically designed for materials science. HoneyComb leverages a novel, high-quality materials science knowledge base (MatSciKB) and a sophisticated tool hub (ToolHub) to enhance its reasoning and computational capabilities tailored to materials science. MatSciKB is a curated, structured knowledge collection based on reliable literature, while ToolHub employs an Inductive Tool Construction method to generate, decompose, and refine API tools for materials science. Additionally, HoneyComb leverages a retriever module that adaptively selects the appropriate knowledge source or tools for specific tasks, thereby ensuring accuracy and relevance. Our results demonstrate that HoneyComb significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain. Furthermore, our adaptable framework can be easily extended to other scientific domains, highlighting its potential for broad applicability in advancing scientific research and applications.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4553.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4553.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextMining_Publications</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text mining and information extraction from scientific publications (materials science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general category of methods that apply NLP/ML (including transformer models and tool-augmented agents) to extract structured facts, entities, and datasets from scientific literature to support materials-science tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Opportunities and challenges of text mining in materials research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>General text-mining / information-extraction pipelines from the literature referenced by the paper: approaches include transformer-based domain models and extraction toolkits to parse abstracts/sections into structured key-value records, named entities, relations and dataset entries. The HoneyComb paper cites this prior work to motivate building MatSciKB but does not detail a single extraction algorithm here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Structured factual information, named entities, relations and dataset records (the paper does not claim explicit extraction of high-level qualitative laws or mechanistic theories from these pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The HoneyComb paper cites prior text-mining work as enabling LLMs and agent systems to access structured literature knowledge; it uses this as motivation for building MatSciKB and a retriever but does not itself report a pipeline that extracts explicit qualitative laws/theories from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Prior text-mining approaches provide useful structured knowledge but the paper emphasizes limitations of LLMs alone (conceptual errors, factual hallucination, outdated internal knowledge) and the need for curated KBs and tool-augmentation to avoid incorrect inferences when using literature-derived information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HoneyComb: A Flexible LLM-Based Agent System for Materials Science', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4553.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4553.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDataExtractor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced toolkit (Swain & Cole, 2016) for automated extraction of chemical entities and relations from papers; cited as an example of successful literature information-extraction tooling relevant to scientific LLM applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChemDataExtractor (toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Rule- and ML-based pipelines for parsing scientific text (chemical names, properties, reactions) into structured outputs; HoneyComb cites such toolkits as antecedent work in automated extraction from journals but does not run or evaluate ChemDataExtractor within its experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>chemistry / materials-related literature</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Entity and relation extraction (chemical facts, reaction information); not explicitly framed as extraction of qualitative laws in the HoneyComb paper.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as an example of prior literature-extraction tooling that informs the development of LLM-augmented agent systems; the HoneyComb paper leverages the idea of structured extraction but implements its own MatSciKB and retrieval instead of directly using ChemDataExtractor.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The paper acknowledges general limitations of literature-extraction: heterogeneous sources, evolving knowledge, and the need for curated, up-to-date KBs to avoid relying on outdated or hallucinated model knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HoneyComb: A Flexible LLM-Based Agent System for Materials Science', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4553.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4553.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent_Dataset_Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-based structured dataset creation from scientific papers (solar cell materials example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior work where agents collect and synthesize information from many scientific papers to build structured datasets (example: solar cell materials dataset creation), illustrating how agents can process literature at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Creation of a structured solar cell material dataset and performance prediction using large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Agent-based literature gathering for dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described in HoneyComb as prior work: agents (tool-augmented LLM systems) gather pertinent information from a large corpus of scientific papers, automate data input and synthesis into structured datasets, and iteratively refine queries/searches to improve relevance. HoneyComb cites Xie et al. (2024) and Liu et al. (2024b) as examples but does not itself perform that solar-cell dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science (solar cell materials example) / broader scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Creation of structured empirical datasets and extraction of empirical patterns or performance-related records; the HoneyComb paper does not claim direct extraction of high-level qualitative laws or mechanistic theories via these agents in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agent-based approaches can aggregate and structure knowledge from many papers and thus enable downstream predictive tasks; HoneyComb uses the same motivation to combine MatSciKB and ToolHub to improve LLM performance on QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>HoneyComb highlights general issues in agent-based literature processing: tool compatibility, maintaining up-to-date sources, and the potential for LLM hallucination when not grounded by curated KBs or validated tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HoneyComb: A Flexible LLM-Based Agent System for Materials Science', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4553.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4553.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatSciBERT_MatBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MatSciBERT / MatBERT (materials-domain pretrained transformer models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-adapted transformer models pretrained on materials science text to improve extraction and NER tasks from materials literature; cited as part of the background motivating domain-specific LLM/system development.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Matscibert: A materials domain language model for text mining and information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MatSciBERT / MatBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-specific pretraining for transformers</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pretraining transformer-based language models on domain corpora (materials-science texts) to improve downstream extraction tasks (named-entity recognition, information extraction) from scholarly papers; HoneyComb cites these models as antecedent work but does not itself fine-tune such a model in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Entity and relation extraction, factual/materials properties extraction; not explicitly framed as extracting qualitative laws in HoneyComb.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-specific pretraining helps extract more relevant and accurate material-science facts from literature; HoneyComb builds on the premise that domain knowledge (here encoded in MatSciKB) plus tools improves LLM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>These specialized models improve extraction but do not by themselves solve issues of outdated knowledge or complex computational tasks; HoneyComb argues for retrieval and tool augmentation to complement such models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HoneyComb: A Flexible LLM-Based Agent System for Materials Science', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4553.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4553.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT_examples_in_MatSciKB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-generated examples incorporated into MatSciKB</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>HoneyComb authors used an LLM (GPT family) to generate material-science question–answer examples which were curated and added to MatSciKB to augment the knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT (unspecified variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Prompted question generation and human curation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Authors prompted an LLM to generate batches of 50 materials-science questions (Appendix B gives the prompt format); outputs included question, answer, an accuracy estimate and a confidence score. Human reviewers selected high-confidence instances to include in MatSciKB. This is a generation+human-in-the-loop curation workflow rather than automatic law extraction from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>None — used for synthetic example generation (QA items), not for extracting qualitative laws or theories from published papers.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human reviewer selection of higher-confidence generated items (as described in Appendix B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MatSciKB includes 2,005 GPT-generated examples (as reported in Table 1); no accuracy metrics for law extraction are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs (GPT) can be used to rapidly generate domain-style QA examples which, after human curation, augment a domain KB and help agent performance; HoneyComb used these examples as part of MatSciKB.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Generated examples require human vetting to avoid hallucinated or incorrect items; generation does not equate to extracting verified qualitative laws from the scholarly corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HoneyComb: A Flexible LLM-Based Agent System for Materials Science', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Opportunities and challenges of text mining in materials research. <em>(Rating: 2)</em></li>
                <li>Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature. <em>(Rating: 2)</em></li>
                <li>Creation of a structured solar cell material dataset and performance prediction using large language models. <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models. <em>(Rating: 1)</em></li>
                <li>Are llms ready for real-world materials discovery? <em>(Rating: 1)</em></li>
                <li>Exploring large language model based intelligent agents: Definitions, methods, and prospects. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4553",
    "paper_id": "paper-a970be54c4df5f04c3fe65b7414e0c2879c55909",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "TextMining_Publications",
            "name_full": "Text mining and information extraction from scientific publications (materials science)",
            "brief_description": "A general category of methods that apply NLP/ML (including transformer models and tool-augmented agents) to extract structured facts, entities, and datasets from scientific literature to support materials-science tasks.",
            "citation_title": "Opportunities and challenges of text mining in materials research.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": "General text-mining / information-extraction pipelines from the literature referenced by the paper: approaches include transformer-based domain models and extraction toolkits to parse abstracts/sections into structured key-value records, named entities, relations and dataset entries. The HoneyComb paper cites this prior work to motivate building MatSciKB but does not detail a single extraction algorithm here.",
            "number_of_papers": null,
            "domain_or_field": "materials science",
            "type_of_laws_extracted": "Structured factual information, named entities, relations and dataset records (the paper does not claim explicit extraction of high-level qualitative laws or mechanistic theories from these pipelines).",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "The HoneyComb paper cites prior text-mining work as enabling LLMs and agent systems to access structured literature knowledge; it uses this as motivation for building MatSciKB and a retriever but does not itself report a pipeline that extracts explicit qualitative laws/theories from papers.",
            "challenges_limitations": "Prior text-mining approaches provide useful structured knowledge but the paper emphasizes limitations of LLMs alone (conceptual errors, factual hallucination, outdated internal knowledge) and the need for curated KBs and tool-augmentation to avoid incorrect inferences when using literature-derived information.",
            "uuid": "e4553.0",
            "source_info": {
                "paper_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ChemDataExtractor",
            "name_full": "Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature.",
            "brief_description": "A referenced toolkit (Swain & Cole, 2016) for automated extraction of chemical entities and relations from papers; cited as an example of successful literature information-extraction tooling relevant to scientific LLM applications.",
            "citation_title": "Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "ChemDataExtractor (toolkit)",
            "method_description": "Rule- and ML-based pipelines for parsing scientific text (chemical names, properties, reactions) into structured outputs; HoneyComb cites such toolkits as antecedent work in automated extraction from journals but does not run or evaluate ChemDataExtractor within its experiments.",
            "number_of_papers": null,
            "domain_or_field": "chemistry / materials-related literature",
            "type_of_laws_extracted": "Entity and relation extraction (chemical facts, reaction information); not explicitly framed as extraction of qualitative laws in the HoneyComb paper.",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Referenced as an example of prior literature-extraction tooling that informs the development of LLM-augmented agent systems; the HoneyComb paper leverages the idea of structured extraction but implements its own MatSciKB and retrieval instead of directly using ChemDataExtractor.",
            "challenges_limitations": "The paper acknowledges general limitations of literature-extraction: heterogeneous sources, evolving knowledge, and the need for curated, up-to-date KBs to avoid relying on outdated or hallucinated model knowledge.",
            "uuid": "e4553.1",
            "source_info": {
                "paper_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Agent_Dataset_Extraction",
            "name_full": "Agent-based structured dataset creation from scientific papers (solar cell materials example)",
            "brief_description": "Mentioned prior work where agents collect and synthesize information from many scientific papers to build structured datasets (example: solar cell materials dataset creation), illustrating how agents can process literature at scale.",
            "citation_title": "Creation of a structured solar cell material dataset and performance prediction using large language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Agent-based literature gathering for dataset construction",
            "method_description": "Described in HoneyComb as prior work: agents (tool-augmented LLM systems) gather pertinent information from a large corpus of scientific papers, automate data input and synthesis into structured datasets, and iteratively refine queries/searches to improve relevance. HoneyComb cites Xie et al. (2024) and Liu et al. (2024b) as examples but does not itself perform that solar-cell dataset construction.",
            "number_of_papers": null,
            "domain_or_field": "materials science (solar cell materials example) / broader scientific literature",
            "type_of_laws_extracted": "Creation of structured empirical datasets and extraction of empirical patterns or performance-related records; the HoneyComb paper does not claim direct extraction of high-level qualitative laws or mechanistic theories via these agents in the cited work.",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Agent-based approaches can aggregate and structure knowledge from many papers and thus enable downstream predictive tasks; HoneyComb uses the same motivation to combine MatSciKB and ToolHub to improve LLM performance on QA tasks.",
            "challenges_limitations": "HoneyComb highlights general issues in agent-based literature processing: tool compatibility, maintaining up-to-date sources, and the potential for LLM hallucination when not grounded by curated KBs or validated tools.",
            "uuid": "e4553.2",
            "source_info": {
                "paper_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "MatSciBERT_MatBERT",
            "name_full": "MatSciBERT / MatBERT (materials-domain pretrained transformer models)",
            "brief_description": "Domain-adapted transformer models pretrained on materials science text to improve extraction and NER tasks from materials literature; cited as part of the background motivating domain-specific LLM/system development.",
            "citation_title": "Matscibert: A materials domain language model for text mining and information extraction.",
            "mention_or_use": "mention",
            "model_name": "MatSciBERT / MatBERT",
            "model_size": null,
            "method_name": "Domain-specific pretraining for transformers",
            "method_description": "Pretraining transformer-based language models on domain corpora (materials-science texts) to improve downstream extraction tasks (named-entity recognition, information extraction) from scholarly papers; HoneyComb cites these models as antecedent work but does not itself fine-tune such a model in the paper's experiments.",
            "number_of_papers": null,
            "domain_or_field": "materials science",
            "type_of_laws_extracted": "Entity and relation extraction, factual/materials properties extraction; not explicitly framed as extracting qualitative laws in HoneyComb.",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Domain-specific pretraining helps extract more relevant and accurate material-science facts from literature; HoneyComb builds on the premise that domain knowledge (here encoded in MatSciKB) plus tools improves LLM performance.",
            "challenges_limitations": "These specialized models improve extraction but do not by themselves solve issues of outdated knowledge or complex computational tasks; HoneyComb argues for retrieval and tool augmentation to complement such models.",
            "uuid": "e4553.3",
            "source_info": {
                "paper_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GPT_examples_in_MatSciKB",
            "name_full": "GPT-generated examples incorporated into MatSciKB",
            "brief_description": "HoneyComb authors used an LLM (GPT family) to generate material-science question–answer examples which were curated and added to MatSciKB to augment the knowledge base.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT (unspecified variant)",
            "model_size": null,
            "method_name": "Prompted question generation and human curation",
            "method_description": "Authors prompted an LLM to generate batches of 50 materials-science questions (Appendix B gives the prompt format); outputs included question, answer, an accuracy estimate and a confidence score. Human reviewers selected high-confidence instances to include in MatSciKB. This is a generation+human-in-the-loop curation workflow rather than automatic law extraction from papers.",
            "number_of_papers": null,
            "domain_or_field": "materials science",
            "type_of_laws_extracted": "None — used for synthetic example generation (QA items), not for extracting qualitative laws or theories from published papers.",
            "example_laws_extracted": null,
            "evaluation_method": "Human reviewer selection of higher-confidence generated items (as described in Appendix B)",
            "performance_metrics": "MatSciKB includes 2,005 GPT-generated examples (as reported in Table 1); no accuracy metrics for law extraction are provided.",
            "comparison_baseline": null,
            "key_findings": "LLMs (GPT) can be used to rapidly generate domain-style QA examples which, after human curation, augment a domain KB and help agent performance; HoneyComb used these examples as part of MatSciKB.",
            "challenges_limitations": "Generated examples require human vetting to avoid hallucinated or incorrect items; generation does not equate to extracting verified qualitative laws from the scholarly corpus.",
            "uuid": "e4553.4",
            "source_info": {
                "paper_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Opportunities and challenges of text mining in materials research.",
            "rating": 2
        },
        {
            "paper_title": "Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature.",
            "rating": 2
        },
        {
            "paper_title": "Creation of a structured solar cell material dataset and performance prediction using large language models.",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models.",
            "rating": 1
        },
        {
            "paper_title": "Are llms ready for real-world materials discovery?",
            "rating": 1
        },
        {
            "paper_title": "Exploring large language model based intelligent agents: Definitions, methods, and prospects.",
            "rating": 1
        }
    ],
    "cost": 0.01776125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>HoneyComb: A Flexible LLM-Based Agent System for Materials Science</h1>
<p>Huan Zhang ${ }^{1}$, Yu Song ${ }^{1}$, Ziyu Hou ${ }^{2}$, Santiago Miret ${ }^{3 *}$, Bang Liu ${ }^{1,4 * \dagger}$<br>${ }^{1}$ University of Montreal / Mila - Quebec AI, ${ }^{2}$ University of Waterloo,<br>${ }^{3}$ Intel Labs, ${ }^{4}$ Canada CIFAR AI Chair<br>{huan.zhang, yu.song, bang.liu}@umontreal.ca<br>{z26hou}@uwaterloo.ca<br>{santiago.miret}@intel.com</p>
<h4>Abstract</h4>
<p>The emergence of specialized large language models (LLMs) has shown promise in addressing complex tasks for materials science. Many LLMs, however, often struggle with distinct complexities of material science tasks, such as materials science computational tasks, and often rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations. To address these challenges, we introduce HoneyComb, the first LLM-based agent system specifically designed for materials science. HoneyComb leverages a novel, high-quality materials science knowledge base (MatSciKB) and a sophisticated tool hub (ToolHub) to enhance its reasoning and computational capabilities tailored to materials science. MatSciKB is a curated, structured knowledge collection based on reliable literature, while ToolHub employs an Inductive Tool Construction method to generate, decompose, and refine API tools for materials science. Additionally, HoneyComb leverages a retriever module that adaptively selects the appropriate knowledge source or tools for specific tasks, thereby ensuring accuracy and relevance. Our results demonstrate that HoneyComb significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain. Furthermore, our adaptable framework can be easily extended to other scientific domains, highlighting its potential for broad applicability in advancing scientific research and applications.</p>
<h2>1 Introduction</h2>
<p>The emergence of large language models (LLMs) (OpenAI, 2024; Anthropic, 2024; Touvron et al., 2023b,a) in recent years has brought about the application of LLMs across a wide range of fields related to science and engineering (AI4Science and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Quantum, 2023). This has resulted in a number of new benchmarks measuring the capabilities of language models to perform scientific tasks (Wang et al., 2023; Sun et al., 2024; Mirza et al., 2024; Song et al., 2023a) along with the development of custom LLMs and LLM-based systems for scientific domains including chemistry (Bran et al., 2023; Boiko et al., 2023), biology (Madani et al., 2023) and materials science (Song et al., 2023b; Gupta et al., 2022; Walker et al., 2021).</p>
<p>While much progress has been made in adapting LLMs to common tasks in natural language processing (Song et al., 2023a,b), many more challenges remain in having LLMs be effective agents for real-world materials science tasks (Miret and Krishnan, 2024; Miret et al., 2024). As highlighted by Zaki et al. (2023), LLMs often fail in performing important computational tasks for materials science. Common mistakes by most LLMs include conceptual errors where models fail to retrieve correct concepts, equations, or facts relevant to the questions, and factual hallucinations where incorrect information is generated. An analysis by Miret and Krishnan (2024) also revealed that LLMs by themselves struggle to generate relevant and correct information pertaining to specialized materials science tasks. While Song et al. (2023b) showed that instruction fine-tuning can help in improving performance, the high costs of continuous model training and fine-tuning make retraining-based approaches challenging to scale. This is further compounded by the fact that relevant knowledge is continuously updated through a diversity of knowledge sources, including pre-print servers (e.g., arXiv and ChemRxiv), peer-reviewed literature, open encyclopedias (e.g. Wikipedia) and relevant websites. Furthermore, prior work has show that utilizing external tools may be a more promising approach to solve complex scientific tasks instead of relying entirely on an LLMs internal knowledge (Zheng et al., 2024; Buehler, 2024a). To jointly address</p>
<p>these challenges, we propose transforming LLMs into LLM-based agents that access external knowledge and tools to boost their performance. This approach has already shown promise in adjacent domains, such as chemistry (Bran et al., 2023; Boiko et al., 2023) by enabling the the models to access real-time data and utilize computational as well as domain-specific tools. Altogether, the LLM-based agents showcase greater capabilities and performance compared to their native LLM counterparts.</p>
<p>In this paper, we present HoneyComb, the first, to the best of our knowledge, LLM-based agent system specifically designed for materials science. While there has been emerging research in LLMs for scientific domains, few studies have focused on developing comprehensive agent systems for materials science. Our work addresses two critical challenges: First, MatSciKB alleviates the challenge of obtaining reliable and relevant professional knowledge for materials. As such, MatSciKB ensures the agent has access to the most current and accurate information is essential for effective performance. Second, Tool-Hub provides materials science specific tools to augment the agent's capabilities. These tools enable the agent to perform specialized computational tasks and enhance its overall functionality. As detailed in Section 4, we observe that with the aid of MatSciKB and Tool-Hub, HoneyComb outperforms its native LLM counterparts in a more reliable manner given its ability to utilize up-to-date knowledge and tools.</p>
<h2>2 Background</h2>
<h3>2.1 LLMs for Material Science</h3>
<p>Advancements in text mining and information extraction from scientific publications have significantly benefited the application of LLMs for materials science (Kononova et al., 2021; Swain and Cole, 2016). Early work include the development of specialized BERT models (Devlin et al., 2018), such as MatSciBERT (Gupta et al., 2022) and MatBERT (Walker et al., 2021). Song et al. (2023b) and Xie et al. (2023) leveraged instruction finetuning to develop a LlaMa-based (Touvron et al., 2023a) tailored to materials science that matched the capabilities of commercial LLMs at the time of publication. The emergence of powerful commercial LLMs (OpenAI, 2024; Anthropic, 2024) has further expanded the possibility of applying LLMs to materials science. Yet, commercial LLMs remain expensive, opaque in their methodology
with consistent errors and shortcomings (Zaki et al., 2023; Miret and Krishnan, 2024), and open-source LLMs for materials science remain sparse. This motivates the need for a practical LLM-based system that is useful for real-world materials science tasks.</p>
<p>Given this need, we propose HoneyComb as an open-source system to augment the capabilities of diverse LLMs. HoneyComb integrates specialized tools as well as a dynamic retrieval system to enhance the functionality any LLMs specifically for material science. By leveraging relevant knowledge source through MatSciKB and auxilliary tools through Tool-Hub, HoneyComb manages to improve the accuracy and relevance of the outputs of LLMs for materials science, while also addressing common challenges associated with static LLM applications in dynamic research fields.</p>
<h3>2.2 Tool-Based LLM Agents for Scientific Applications</h3>
<p>Prior work has shown success in expanding the capabilities of LLMs by augmenting their capabilities with diverse sets of tools (Qin et al., 2023b,a; Chern et al., 2023; Wang et al., 2024). Many works rely on pre-built integration frameworks, such as LangChain (Topsakal and Akinci, 2023), to build the relevant interfaces between the LLMs and the desired capabilities, such as search engine APIs. Wang et al. (2024) provides a recent survey of common approaches, challenges and applications of tool-based LLMs and their applications to various technology and scientific fields.</p>
<p>One major application of tool-based LLMs is in query processing and optimization, where agents evaluate initial search results and iteratively refine queries to increase relevance and accuracy (Buehler, 2024a,b). This approach addresses the limitations of isolated LLMs, which may struggle to handle ambiguous query contexts. In generating structured datasets for solar cell materials, agents gather pertinent information from a vast array of scientific papers to automate data input and synthesis (Xie et al., 2024; Liu et al., 2024b). Furthermore, agents can utilize various tools to help answer specific questions by tapping into external resources (Cheng et al., 2024). For example, ChemCrow by Bran et al. (2023) integrates 18 expert-designed tools, such as literature search, molecule modification, and reaction execution, to autonomously execute chemical syntheses. Tool augmentation has also shown success in other re-</p>
<p>search in the chemistry domain to enable real-world experiments using LLMs <em>Yoshikawa et al. (2023); Jablonka et al. (2023); Boiko et al. (2023)</em>. Coscientist by Boiko et al. <em>Boiko et al. (2023)</em>, for examples, relies on specialized tools to extend the capabilities of GPT4 and thereby invoke domain-specific functionalities that are not inherently present within the LLM alone. The success of agent-based approaches in adjacent domains motivates the creation of HoneyComb that extends the capabilities of LLMs specifically for materials science.</p>
<h2>3 HoneyComb</h2>
<p>In this work, we introduce HoneyComb, shown in Figure 1, a specialized agent system designed to advance materials science research. It integrates three key components: 1) <em>MatSciKB</em>, a comprehensive knowledge base; 2) <em>ToolHub</em>, which includes general tools for accessing up-to-date information broadly and specialized tools developed through an Inductive Tool Construction method for targeted material science queries; 3) <em>Retriever</em>, utilizing a hybrid approach for efficient and precise information retrieval.</p>
<h3>3.1 MatSciKB</h3>
<p>Our MatSciKB knowledge base integrates a diverse array of sources, as detailed in Table 1. This collection is meticulously curated to include material science papers from ArXiv, relevant Wikipedia entries, textbooks, comprehensive datasets, pertinent mathematical formulas, and concrete GPT-generated examples tailored to material science. Each information source is thoroughly described in Appendix A.</p>
<p>The architectural framework of MatSciKB is thoughtfully structured into 16 distinct categories pertinent to material science. These are detailed in Appendix C and are organized in a tree-like structure. MatSciKB supports efficient searching and CRUD (Create, Read, Update, Delete) operations <em>Giannaros et al. (2023)</em>, which are vital for both the application and ongoing maintenance of the database. Given the continuously evolving and expanding body of knowledge in the materials science domain, capabilities for efficient updates and searches based on real-time information are crucial for research and engineering applications. Additionally, our structured data approach enhances the integration of the diverse data sources commonly encountered in materials science <em>Miret and Krishnan (2024)</em>. This structure not only facilitates easy access and management but also allows for seamless extension to include additional data modalities.</p>
<table>
<thead>
<tr>
<th>MatSciKB</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td># Total Number of Data Entries</td>
<td>38,469</td>
</tr>
<tr>
<td># Material Science Papers on Arxiv</td>
<td>20,384</td>
</tr>
<tr>
<td># Wikipedia for Material Science</td>
<td>3,620</td>
</tr>
<tr>
<td># Material Science Textbook</td>
<td>1,930</td>
</tr>
<tr>
<td># Material Science Dataset</td>
<td>10,473</td>
</tr>
<tr>
<td># Material Science Formula</td>
<td>57</td>
</tr>
<tr>
<td># GPT-generated Examples</td>
<td>2,005</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the MatSciKB knowledge base</p>
<h3>3.2 Tool-Hub</h3>
<p>The Tool-Hub in HoneyComb is bifurcated into <em>General Tools</em> and <em>Material Science Tools</em>. Both categories are organized through a unified interface that supports allow HoneyComb to make effective use of all available tools. <strong>General Tools</strong> provide researchers with access to the latest information filling gaps not covered by the static entries in MatSciKB. <strong>Material Science Tools</strong> are specifically designed to handle complex calculations and in-depth analyses. The details of the unified interface are further elucidated in Appendix D.</p>
<p><strong>General Tools Construction</strong></p>
<p>In materials science, one of the persistent challenges is keeping research outputs aligned with the diverse and ever-evolving data modalities that describe complex material systems <em>Miret and Krishnan (2024)</em>. The diversity of data sources and measurements leads to a rapid evolution of knowledge in this field, necessitating tools that can effectively access and integrate recent findings. Traditional static databases, while useful, often lag in capturing the newest research, creating gaps that can impede the currency and relevance of scientific analysis in real-time. Further, the need to efficiently process complex and dynamic computational tasks within the research workflow remains inadequately addressed, often requiring manual intervention which can introduce errors and inefficiencies. Thus, constructing tools that can handle varying data modalities and complexities, and that can adapt to the continual advancements in materials science, is essential for advancing the field.</p>
<p>To address these challenges, HoneyComb has been designed with innovative solutions that markedly enhance research capabilities in materi-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overall architecture of HoneyComb. The model initiates with a query input that activates the knowledge retrieval phase, where pertinent data entries and atom function are extracted from the MatSciKB and Tool-Hub respectively. The Executor iterative calls the relevant tools from the Tool-Hub, evaluating and refining these calls until a solution that adequately solves the query emerges. The preliminary solution generated by these tools is combined with relevant data entries, and then undergoes further processing by the Retriever. Finally, the Retriever consolidates and filters these input, ultimately feeding them into the LLM for final answer generation.</p>
<p>als science. First, we integrated General Tools that provide direct access to current publications and facilitate dynamic discussions, as shown in Table 2, effectively complementing the static MatSciKB. Secondly, recognizing the limitations of large language models (LLMs) in performing computational tasks, we implemented a Python REPL environment within HoneyComb. This environment is strategically utilized by the system when the agent, interacting with the Tool-Hub, identifies a need for basic numerical computations. The agent dynamically writes Python code for these tasks and executes it through the Python REPL, bypassing the LLM's computational limitations. This automation not only streamlines data processing but also enhances the precision and reliability of numerical analyses in research activities.</p>
<table>
<thead>
<tr>
<th>General Tools</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google Search</td>
<td></td>
</tr>
<tr>
<td>Google Scholar Search</td>
<td></td>
</tr>
<tr>
<td>Arxiv Search</td>
<td></td>
</tr>
<tr>
<td>Wikipedia Search</td>
<td></td>
</tr>
<tr>
<td>YouTube Search</td>
<td></td>
</tr>
<tr>
<td>Python REPL</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2: ToolHub: General Tools</p>
<h3>Inductive Tool Construction for Materials Sci-</h3>
<p>Algorithm 1 Inductive Tool Construction</p>
<p><strong>Require:</strong> Train Set <em>Dtrain</em>, LLM <em>M</em></p>
<p><strong>Ensure:</strong> Set of atom tools <em>A</em></p>
<p>1: <em>A</em> ← ∅ {Initialize the set of atom functions}</p>
<p>2: <strong>for</strong> each question <em>q_{i}</em> in <em>Dtrain</em> <strong>do</strong></p>
<p>3: <em>f_{i}</em> ← <em>M</em>(<em>q_{i}</em>) {Generate specific function for <em>q_{i}</em>}</p>
<p>4: Human verifies <em>f_{i}</em></p>
<p>5: Decompose <em>f_{i}</em> into atom functions <em>a_{i}</em></p>
<p>6: <em>A</em> ← <em>A</em>∪<em>a_{i}</em> {Add atom functions to the set}</p>
<p>7: <strong>end for</strong></p>
<p>8: <strong>return <em>A</em></strong></p>
<h3>ence</h3>
<p>Constructing domain-specific tool APIs presents significant challenges. It requires domain expert knowledge, and there are limited existing resources to draw upon. Additionally, many valuable data and tools are not open source, limiting their accessibility. Developing these tools is essential for effectively addressing the unique and complex queries inherent to materials science. The scarcity of pre-existing, specialized computational tools necessitates a methodical approach to tool construction and refinement.</p>
<p>We propose the <em>Inductive Tool Construction</em> method, delineated in Algorithm 1 for domain-specific tool APIs construction. It adopts a sys-</p>
<p>tematic approach to fabricate and refine computational tools specifically designed for material science queries. The process initiates by selecting a random subset of computational questions from dataset $D$, designated as $D_{\text{train}}$ for training, with the residual questions forming $D_{\text{test}}$. For each question $q_{i} \in D_{\text {train }}$, a designated LLM, $M$ (such as GPT-4), is tasked to generate a Python function $f_{i}$ that addresses $q_{i}$. After creation, each function $f_{i}$ undergoes rigorous human verification to confirm its correctness.</p>
<p>However, the above procedures cannot ensure the generalizability of the constructed tool APIs. Thus, in the post-validation stage, we further use $M$ to decompose each $f_{i}$ into fundamental, reusable components known as atomic function $a_{i}$, which are crafted for extensive applicability across diverse queries, a detailed example is illustrated in Appendix E</p>
<h3>3.3 Agent-Tool Hub Interactions</h3>
<p>In HoneyComb, interactions between the agent and Tool-Hub are governed by a structured two-phase decision-making protocol. Our protocol emphasizes the critical selection and processing of data to ensure that only pertinent information influences the LLM's decisions. This approach is vital to prevent the degradation of model performance due to irrelevant or low-quality inputs (Liu et al., 2024a).</p>
<ol>
<li>Tool Assessor: During the initial phase, the Assessor evaluates both the incoming query and the extensive suite of tools within the Tool-Hub. This evaluation aims to identify a manageable subset of the most relevant tools that are best suited to address the specific requirements of the query. By filtering out irrelevant tools at this stage, we ensure that the Executor is provided only with pertinent information, thereby optimizing the model's focus and enhancing its capacity to solve the problem accurately.</li>
<li>Tool Executor: As illustrated in Figure 2, the Executor receives the original query along with the subset of tools selected by the Assessor. Upon evaluating the selected tools and query, the Executor engages in a thought process to determine the most suitable tool for addressing the query. If the query's complexity exceeds the capacity of a single tool, the Executor recognizes the challenge and decomposes the query into smaller subquestions. The strategy allows for sequential tackling of each part, starting with the selection of the optimal tool for the initial subquestion. It then initiates the action of
executing the selected tool while inputting parameter values, termed action input, derived from the query or subquestion. Upon execution, the tool generates a result termed observation. Subsequently, the Executor engages in a reflective process to assess whether the observation adequately addresses the query. If the observation is adequate, it is finalized as the answer; if not, the process either reiterates with adjustments or progresses to the next subquestion if the original query was segmented into multiple parts.
<img alt="img-1.jpeg" src="img-1.jpeg" /></li>
</ol>
<p>Figure 2: Tool Assessor and Executor interaction cycle in HoneyComb.</p>
<h3>3.4 Retriever</h3>
<p>In this section, we present the retriever in HoneyComb which returns relevant texts or tools from MatSciKB and Tool-Hub when a specific contexts is given. The retriever integrates both BM25 (Trotman et al., 2014) and Contriever (Izacard et al., 2022) model, leveraging their respective strengths to achieve optimal information retrieval performance.</p>
<p>Specifically, the retriever employs a two-step strategy. Initially, BM25 utilizes efficient calculations of term frequency and inverse document frequency to rapidly process short text queries and keyword searches within long documents. The primary advantage of BM25 lies in its computational simplicity and rapid response, allowing HoneyComb to extract the N most relevant knowledge points from an extensive materials science knowledge base, ensuring exceptional speed and efficiency. This approach enables the provision of basic relevance matching results in a minimal timeframe.</p>
<p>Subsequently, we employs a pre-trained deep learning models (i.e. Contriever) to generate embedding vectors and compute their similarity, facilitating the understanding of complex linguistic structures and semantic information. The strength of Contriever resides in its capability to compre-</p>
<p>hend and process intricate language structures, contextual information, and semantic relationships, thereby delivering more precise and comprehensive retrieval results. Although Contriever operates at a slower pace compared to BM25, it pulls the most relevant results from the knowledge base and memory, as well as from tools invoked through the Tool-Hub, extracting the top 3 results. Its ability to precisely handle complex queries and diverse documents ensures high accuracy and relevance.</p>
<p>By combining BM25 and Contriever, our model adeptly responds to simple queries with speed while offering enhanced accuracy and relevance for complex queries. This hybrid approach ensures that the model is both efficient and capable of addressing sophisticated query requirements, thereby providing comprehensive, efficient, and precise information retrieval services.</p>
<h2>4 Experiments</h2>
<p>We conduct experiments on two question answering datasets, namely MaScQA (Zaki et al., 2023) and SciQA (Johannes Welbl, 2017), to investigating the ablility of HoneyComb in materials science tasks.</p>
<p>MaScQA, derived from the Graduate Aptitude Test in Engineering (GATE) in India, is tailored to reflect the real-world complexity and variety of issues encountered in material science. This highly competitive examination assesses a comprehensive understanding of various undergraduate subjects (Indian Institute of Technology Kanpur, 2023; Zaki et al., 2023). With its 650 questions covering 14 domains such as thermodynamics, atomic structure, and mechanical behavior, the dataset showcases a wide range of question types, from Multiple Choice Questions (MCQs), Numerical Answer Type (NUM), and Matching Type (MATCH) to MCQs with numerical options (MCQN). Specifically designed for advanced problem-solving, this dataset is crucial for ensuring that our ToolHub functions effectively in actual material science research and applications. It demonstrates the HoneyComb framework's efficacy and adaptability in tackling complex material science issues within realistic scenarios. The second dataset, SciQA, comprises 11,679 multiple-choice questions that span the core disciplines of fundamental sciences from a variety of crowdsourced science exams (Johannes Welbl, 2017). This compilation not only underlines the dataset's comprehensive and inter-
disciplinary nature but also focuses on fostering a nuanced conceptual understanding. SciQA serves as a critical testbed to ascertain whether the HoneyComb framework can augment the LLM's capabilities beyond its initial programming. By integrating supplementary information, it aids in addressing intricate queries and unraveling complex scientific concepts that may have been overlooked during the initial training phase of the LLM. By bridging real-world complexities with rigorous academic standards, these datasets ensure that our MatSciKB and ToolHub are not only versatile but also remain at the forefront of technological and scientific application.</p>
<p>The choice of models for our experiments was driven by the need to evaluate the HoneyComb framework's enhancement capabilities across a spectrum of large language models known for their robust performance in diverse applications. We selected GPT-3.5, GPT-4 (OpenAI, 2024), LLaMA-2 (Touvron et al., 2023b), and LLaMA-3 (AI@Meta, 2024) due to their widespread use and proven effectiveness in handling complex language tasks. These models, with LLaMA-2 and LLaMA-3 having parameter sizes of 7 billion and 8 billion respectively, represent the current state-of-the-art in generalized language understanding and provide a solid baseline for benchmarking. Additionally, we included HoneyBee(Song et al., 2023b), a specialized model with a parameter size of 7 billion, tailored specifically for materials science. The inclusion of both general-purpose and specialized models allows us to showcase how domain-specific adaptations through HoneyComb can elevate a model's functional scope beyond its original configuration, thus highlighting the adaptability and effectiveness of our framework.</p>
<h3>4.1 HoneyComb Evaluation</h3>
<p>We evaluated the performance of various models on MaScQA and SciQA, including HoneyBee, GPT-3.5, GPT-4, Llama2, and Llama3, and demonstrated the effects of using the HoneyComb. The results are illustrated in Table 3</p>
<p>The experimental results show that all models based on HoneyComb achieved significant improvements in accuracy on both MaScQA and SciQA. Specifically, on the MaScQA dataset, models such as HoneyBee and GPT-4 experienced substantial improvements, with HoneyBee's accuracy improving by $16.76 \%$ and GPT-4's by $20.61 \%$. Other models also showed notable enhancements,</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>HoneyBee</th>
<th>HoneyBee +</th>
<th>GPT-3.5</th>
<th>GPT-3.5 +</th>
<th>GPT-4</th>
<th>GPT-4 +</th>
<th>Llama2</th>
<th>Llama2 +</th>
<th>Llama3</th>
<th>Llama3 +</th>
</tr>
</thead>
<tbody>
<tr>
<td>MaScQA</td>
<td>16.62</td>
<td>33.38</td>
<td>33.54</td>
<td>38.46</td>
<td>58.46</td>
<td>79.07</td>
<td>22.15</td>
<td>36.31</td>
<td>24.62</td>
<td>47.23</td>
</tr>
<tr>
<td>SciQA</td>
<td>33.96</td>
<td>79.69</td>
<td>90.69</td>
<td>90.83</td>
<td>90.84</td>
<td>96.54</td>
<td>75.79</td>
<td>78.66</td>
<td>93.00</td>
<td>93.32</td>
</tr>
</tbody>
</table>
<p>Table 3: HoneyComb evaluation with diverse LLMs including open-source LLMs (HoneyBee <em>Song et al. (2023b)</em>, LlaMa2 <em>Touvron et al. (2023b)</em>, LlaMa3 <em>AI@Meta (2024)</em>) and commercial LLMs (GPT3.5, GPT4 <em>OpenAI (2024)</em>). The results show that HoneyComb consistently improves the performance of all LLMs for SciQA and MaScQA.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Improvements of various LLMs integrated with HoneyComb compared to relevant baseline LLMs for different materials science tasks. With few exceptions, HoneyComb improves the performance of all LLMs across all tasks showing the utility of tool augmentation.</p>
<p>With improvements ranging from 4.92 to 14.16%, on the SciQA dataset, the HoneyBee model saw a dramatic increase in performance, representing a huge improvement of 45.73%. HoneyComb based on GPT-3.5 and Llama3 showed more modest enhancements of around 0.14% to 0.32%, whereas HoneyComb based on GPT-4 and Llama2 experienced considerable improvements of approximately 5.70% and 2.87%, respectively.</p>
<h3>4.2 HoneyComb Evaluation on MaScQA</h3>
<p>We assess the performance improvements when integrating the HoneyComb framework with various large language models across predefined topics within the MaScQA dataset, as shown in Figure 3. The overall trend indicates that HoneyComb substantially enhances model performance. LLaMA-3 and HoneyBee exhibit impressive gains, particularly in 'Material Testing' where improvements of 33.34 percentage points are observed, showcasing HoneyComb's capability to effectively augment models with its advanced Tool-Hub and extensive MatSciKB.</p>
<p>However, GPT-3.5 displays a unique trend with declines across multiple topics including Atomic Structure, Fluid, Magnetism, Material Processing, and Material Testing. Despite having a higher baseline accuracy than LLaMA-3, LLaMA-2, and HoneyBee, GPT-3.5's performance dips more frequently when integrated with HoneyComb. This could be attributed to its training data's scope and depth, which, while extensive, may not align as effectively with HoneyComb's highly specialized material science enhancements. The sophisticated computational demands and the dynamic nature of materials science queries may expose limitations in GPT-3.5's ability to adapt its pre-existing knowledge to the specific enhancements HoneyComb offers. This nuanced understanding highlights the importance of model and tool compatibility in achieving effective enhancements across diverse materials science domains, thereby informing further development and optimization of HoneyComb to ensure comprehensive and reliable support in all areas of materials science research.</p>
<h3>4.3 Ablation Study</h3>
<p>To study how each component of HoneyComb contributes to the overall performance, we conducted ablation studies in this section. We tested the performance of HoneyComb when retrieved only from MatSciKB or only from Tool Hub, respectively. We also report results without retriever, in such situations there is no way for MatSciKB and ToolHub results to be fed into the model. Experimental results are reported in Table 4.</p>
<p>Table 4: Ablation Study Results for MaScQA and SciQA based on GPT-4</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>MatSciKB</th>
<th>ToolHub</th>
<th>Retriever</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>MaScQA</td>
<td></td>
<td></td>
<td></td>
<td>61.38</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>73.23</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td>78.31</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>79.07</td>
</tr>
<tr>
<td>SciQA</td>
<td></td>
<td></td>
<td></td>
<td>90.84</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>96.34</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td>85.57</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>96.56</td>
</tr>
</tbody>
</table>
<p>The experimental results show that the best performance is achieved when both MatSciKB and ToolHub are used as reliable material knowledge references. HoneyComb improved the correctness on MaScQA and SciQA by $0.76 \%$ and $10.99 \%$ when compared to retrieving only from MatSciKB, and improved the correctness on MaScQA and SciQA by $5.84 \%$ and $0.22 \%$ when compared to retrieving only from Tool Hub. Therefore, we recommend that users retrieve HoneyComb from both sources together when deploying or using it.</p>
<h2>5 Conclusion</h2>
<p>In this work, we introduced HoneyComb, a pioneering LLM-based agent system tailored for materials science. HoneyComb integrates a meticulously curated materials science knowledge base (MatSciKB) and a dual-layered ToolHub of general and specialized computational tools. It combines three critical components: MatSciKB, an inductively constructed ToolHub, and a precisionfocused Retriever module. This ensures HoneyComb provides accurate, up-to-date information and performs complex computational tasks reliably.</p>
<p>Experimental results show that HoneyComb outperforms contemporary general-purpose models (e.g. GPT and LLaMa series) and specialized models (e.g. HoneyBee) in materials science QA tasks. HoneyComb effectively bridges the gap between advanced large language models and the specific needs of materials science research, exemplifying how specialized agent systems can advance scientific research and serve as a blueprint for future advancements in other knowledge-intensive fields.</p>
<h2>Limitations</h2>
<p>While HoneyComb significantly enhances the performance of current state-of-the-art models in various materials science QA tasks, there are limita-
tions to its generali zability and applicability beyond the specific datasets and tasks it was trained on. Materials science is a diverse and intricate field, and it remains unclear how well HoneyComb would perform on tasks outside the MaScQA and SciQA benchmarks, particularly for more complex and novel challenges in materials science. Such challenges may include designing synthesis recipes for new materials or predicting material properties.</p>
<p>Additionally, HoneyComb's reliance on highquality LLMs for the knowledge base, tool construction, and retrieval processes can be a limitation. The performance of these components is contingent on the availability and capability of the underlying LLMs, which themselves may have inherent limitations. Furthermore, our work has primarily focused on the materials science domain, and further studies are required to evaluate how applicable and effective HoneyComb would be in other scientific fields.</p>
<h2>Broader Impacts</h2>
<p>By expanding the HoneyComb agent system, HoneyComb has the potential to accelerate scientific discovery and innovation, contributing to a deeper understanding of complex materials systems. This could not only lead to advancements in materials design, development, and application but also promote the discovery and optimization of new materials, benefiting a wide range of industries. Additionally, the versatility and adaptability of HoneyComb enable it to tackle challenges across various scientific domains, further broadening its scope and impact.</p>
<p>Our research does not raise major ethical concerns.</p>
<h2>References</h2>
<ul>
<li>[1] M. A. Al-Science and M. A. A. A. Quantum (2023) The impact of large language models on scientific discovery: a preliminary study using GPT-4. arXiv preprint arXiv:2311.07361. Cited by: §1.</li>
<li>[2] AI@ (2024) Llama 3 model card. Cited by: §1.</li>
<li>[3] Anthropic. (2024) Calude3. Cited by: §1.</li>
<li>[4] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes (2023) Autonomous chemical research with large language models. Nature 624 (7992), pp. 570–578. Cited by: §1.</li>
<li>[5] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White, and P. Schwaller (2023)</li>
</ul>
<p>Chemcrow: Augmenting large-language models with chemistry tools. Preprint, arXiv:2304.05376.</p>
<p>Markus J. Buehler. 2024a. Generative retrievalaugmented ontologic graph and multiagent strategies for interpretive large language model-based materials design. ACS Engineering Au, 4(2):241-277.</p>
<p>Markus J Buehler. 2024b. Mechgpt, a languagebased strategy for mechanics and materials modeling that connects knowledge across scales, disciplines, and modalities. Applied Mechanics Reviews, 76(2):021001.</p>
<p>Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. 2024. Exploring large language model based intelligent agents: Definitions, methods, and prospects. Preprint, arXiv:2401.03428.</p>
<p>I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Anastasios Giannaros, Aristeidis Karras, Leonidas Theodorakopoulos, Christos Karras, Panagiotis Kranias, Nikolaos Schizas, Gerasimos Kalogeratos, and Dimitrios Tsolis. 2023. Autonomous vehicles: Sophisticated attacks, safety issues, challenges, open topics, blockchain, and future directions. Journal of Cybersecurity and Privacy, 3(3):493-543.</p>
<p>Maarten Grootendorst. 2022. Bertopic: Neural topic modeling with a class-based tf-idf procedure. Preprint, arXiv:2203.05794.</p>
<p>Tanishq Gupta, Mohd Zaki, NM Krishnan, et al. 2022. Matscibert: A materials domain language model for text mining and information extraction. npj Computational Materials, 8(1):1-11.</p>
<p>Indian Institute of Technology Kanpur. 2023. Gate 2023: Graduate aptitude test in engineering. Accessed: 2024-06-14.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.</p>
<p>Kevin Maik Jablonka, Qianxiang Ai, Alexander AlFeghali, Shruti Badhwar, Joshua D Bocarsly, Andres M Bran, Stefan Bringuier, L Catherine Brinson, Kamal Choudhary, Defne Circi, et al. 2023. 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Digital Discovery, 2(5):1233-1250.</p>
<p>Matt Gardner Johannes Welbl, Nelson F. Liu. 2017. Crowdsourcing multiple choice science questions.</p>
<p>Olga Kononova, Tanjin He, Haoyan Huo, Amalie Trewartha, Elsa A Olivetti, and Gerbrand Ceder. 2021. Opportunities and challenges of text mining in materials research. Iscience, 24(3).</p>
<p>LangChain contributors. 2023. Langchain: Opensource library for building language-based agents. Online; accessed 17-June-2023.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024a. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Yue Liu, Sin Kit Lo, Qinghua Lu, Liming Zhu, Dehui Zhao, Xiwei Xu, Stefan Harrer, and Jon Whittle. 2024b. Agent design pattern catalogue: A collection of architectural patterns for foundation model based agents. Preprint, arXiv:2405.10467.</p>
<p>Ali Madani, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P. Mohr, James M. Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z. Sun, Richard Socher, James S. Fraser, and Nikhil Naik. 2023. Large language models generate functional protein sequences across diverse families. Nat. Biotechnol., 41(8):1099-1106.</p>
<p>Santiago Miret and NM Krishnan. 2024. Are llms ready for real-world materials discovery? arXiv preprint arXiv:2402.05200.</p>
<p>Santiago Miret, NM Anoop Krishnan, Benjamin Sanchez-Lengeling, Marta Skreta, Vineeth Venugopal, and Jennifer N Wei. 2024. Perspective on ai for accelerated materials design at the ai4mat-2023 workshop at neurips 2023. Digital Discovery.</p>
<p>Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, et al. 2024. Are large language models superhuman chemists? arXiv preprint arXiv:2404.01475.</p>
<p>OpenAI. 2024. Openai. Accessed: 2024-06-14.
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023a. Tool learning with foundation models. Preprint, arXiv:2304.08354.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023b. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789.</p>
<p>Yu Song, Santiago Miret, and Bang Liu. 2023a. MatSciNLP: Evaluating scientific language models on materials science language tasks using text-to-schema modeling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3621-3639, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Yu Song, Santiago Miret, Huan Zhang, and Bang Liu. 2023b. Honeybee: Progressive instruction finetuning of large language models for materials science. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5724-5739.</p>
<p>Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: A multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19053-19061.</p>
<p>Matthew C Swain and Jacqueline M Cole. 2016. Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature. Journal of chemical information and modeling, 56(10):1894-1904.</p>
<p>Oguzhan Topsakal and Tahir Cetin Akinci. 2023. Creating large language model applications utilizing langchain: A primer on developing llm apps fast. In International Conference on Applied Engineering and Natural Sciences, volume 1, pages 1050-1056.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,</p>
<p>Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.</p>
<p>Andrew Trotman, Antti Puurula, and Blake Burgess. 2014. Improvements to bm25 and language models examined. In Proceedings of the 19th Australasian Document Computing Symposium, pages 58-65.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention is all you need. Preprint, arXiv:1706.03762.</p>
<p>Nicholas Walker, Amalie Trewartha, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin Persson, Gerbrand Ceder, and Anubhav Jain. 2021. The impact of domain-specific pre-training on named entity recognition tasks in materials science. Available at SSRN 3950755.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345.</p>
<p>Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635.</p>
<p>Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, et al. 2023. Darwin series: Domain specific large language models for natural science. arXiv preprint arXiv:2308.13565.</p>
<p>Tong Xie, Yuwei Wan, Yufei Zhou, Wei Huang, Yixuan Liu, Qingyuan Linghu, Shaozhou Wang, Chunyu Kit, Clara Grazian, Wenjie Zhang, and Bram Hoex. 2024. Creation of a structured solar cell material dataset and performance prediction using large language models. Patterns, 5(5).</p>
<p>Naruki Yoshikawa, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen, Andrew Zou Li, Yuchi Zhao, Haoping Xu, Artur Kuramshin, et al. 2023. Large language models for chemistry robotics. Autonomous Robots, 47(8):1057-1086.</p>
<p>Mohd Zaki, Jayadeva, Mausam, and N. M. Anoop Krishnan. 2023. Mascqa: A question answering dataset for investigating materials science knowledge of large language models. Preprint, arXiv:2308.09115.</p>
<p>Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou. 2024. Take a step back: Evoking reasoning via abstraction in large language models. Preprint, arXiv:2310.06117.</p>
<h2>Appendix</h2>
<h2>A MatSciKB Knowledge Source</h2>
<h2>- ArXiv Paper</h2>
<ul>
<li>Included all papers indexed under the "material science" keyword on ArXiv.</li>
<li>Data entries structured into key-value pairs: key is the paper title, and value is the abstract.</li>
<li>Data Entries Count: 20,384</li>
</ul>
<h2>- Wikipedia Material Science Concepts</h2>
<ul>
<li>Scraped all 438 pages categorized under "Materials Science" on Wikipedia.</li>
<li>Each section within a page was separated as a distinct data entry.</li>
<li>Content formulated into key-value pairs, with keys as section titles and values as content.</li>
<li>Data Entries Count: 3,620</li>
</ul>
<h2>- Material Science Textbook</h2>
<ul>
<li>Sourced 6 publicly available textbooks.</li>
<li>Converted each textbook PDF file to text documents.</li>
<li>Broke each textbook into data entries by each section in a chapter.</li>
<li>Formulated data entries into key-value pairs, with keys as section titles and values as content.</li>
<li>Data Entries Count: 1,930</li>
</ul>
<h2>- Material Science Dataset</h2>
<ul>
<li>Utilized the multiple-choice dataset SciQA.</li>
<li>Extracted "support" column from the dataset that provides background knowledge for each question.</li>
<li>Each extracted "support" is treated as a data entry, with keys as the knowledge piece and values as empty strings, emphasizing their concise and standalone nature.</li>
<li>Data Entries Count: 10,473</li>
</ul>
<h2>- Material Science Formula</h2>
<ul>
<li>Formulas collected from Wikipedia's dedicated pages for material science formulas.</li>
<li>Each formula is stored as key-value pair in the database, where the key represents the name of the formula and the value contains the formula equation itself.</li>
<li>Data Entries Count: 57</li>
</ul>
<h2>- GPT-generated Examples</h2>
<ul>
<li>Used a specific prompt to generate 50 material science questions at a time, output in CSV format along with a confidence score. Please refer to Appendix B for the detailed prompt.</li>
<li>Human reviewers then selected questions with higher confidence scores for inclusion in the dataset.</li>
<li>Inspiration for question types was drawn from an external resource offering a wide range of material science questions and answers.</li>
<li>The key-value pairs were structured with questions as the keys and answers as the values.</li>
<li>Data Entries Count: 2,005</li>
</ul>
<h2>B Prompt for GPT-Generated Examples</h2>
<p>Please generate 50 instances of material science questions, specifically atomic structure and interatomic bonding, in a CSV format in the following order: question, answer, accuracy, confidence_score - accuracy: for factual questions, please evaluate the answer by comparing it with known facts. this field should be a number between 0 and 1. - confidence_score: how confident are you with the answer. this field should be a number between 0 and 1. - Here are sample instances without accuracy and confidence_score: "In terms of which of the following properties, metals are better than ceramics?", "ductility" "In the wave-mechanical model of an atom, what do degenerate energy levels have?", "equal energy" "Which of the following molecules is diamagnetic?", "CO" - Examples of generated instances: "What is the valence electron configuration of carbon?", "2s²2p²",0.95,0.85 - "What type of crystal defect occurs when there is a line of irregularity in the lattice structure?", "dislocation defect",0.96,0.91</p>
<h2>C Tree-Structure MatSciKB</h2>
<p>MatSciKB is organized as a hierarchical tree with the parent node "Material Science" branching into</p>
<p>16 child nodes representing specific domains within materials science. Below is a simplified representation of this structure:</p>
<div class="codehilite"><pre><span></span><code>    {
&quot;Material Science&quot;: {
&quot;Children&quot;: {
&quot;Thermodynamics&quot;: {&quot;Children&quot;: {&quot;KB_1&quot;:
{}, &quot;KB_2&quot;: {}, &quot;KB_3&quot;: {}}},
&quot;Atomic Structure&quot;: {&quot;Children&quot;: {&quot;KB_4&quot;:
{}, &quot;KB_5&quot;: {}, &quot;KB_6&quot;: {}}},
...
&quot;Miscellaneous&quot;: {&quot;Children&quot;: {&quot;KB_7&quot;:
{}, &quot;KB_n&quot;: {}, &quot;KB_n+1&quot;: {}}}
} } }
</code></pre></div>

<p>Each child node encompasses knowledge base (KB) data entries relevant to its category. In the construction of MatSciKB, we predefined 16 topics that align with core areas in materials science. They are 'Miscellaneous', 'Material testing', 'Fluid', 'Material characterization', 'Magnetism', 'Transport phenomena', 'Material processing', 'Electrical', 'Phase transition', 'Material Applications', 'Material manufacturing', 'Mechanical','Atomic structure', 'Thermodynamics', "Formula", "Fundamental_Science_Knowledge"]</p>
<p>To categorize the data entries within these nodes, we utilized BertTopic, a state-of-the-art topic modeling tool based on transformers and c-TF-IDF, which automatically identifies and clusters documents with high granularity and contextual relevance (Vaswani et al., 2023; Grootendorst, 2022). The integration of BertTopic allowed for the dynamic clustering of MatSciKB entries into 16 predetermined categories.</p>
<p>The process involved the following steps:</p>
<ol>
<li>Initial Clustering: BertTopic was applied to cluster all data entries into more than the target number of categories, based on the textual content of each entry.</li>
<li>Cluster Analysis and Selection: Human reviewers analyzed each cluster, identifying those whose common keywords and themes closely aligned with one of the predefined 16 topics.</li>
<li>Category Assignment: Entries from clusters that aligned well with a predefined topic were assigned to that category, and then removed from the dataset.</li>
<li>Iterative Refinement: The remaining entries underwent subsequent rounds of clustering
and analysis. This process was repeated until no entries were left unclassified.</li>
</ol>
<h2>D Tools Unified Interface Using LangChain</h2>
<p>LangChain is an advanced framework designed to enhance applications that utilize LLM by offering standardized interfaces for various modules (LangChain contributors, 2023). This framework facilities the seamless integration and efficient management of LLM with external tools and systems. Utilizing LangChain, HoneyComb has developed a unified interface that standardizes the integration of a wide array of tools.</p>
<p>In HoneyComb, the unified interface provided by LangChain ensures that all tools, regardless of their specific function, are treated as standardized LangChain objects. This standardization is achieved by defining each tool with a consistent set of attributes:</p>
<ol>
<li>Function Signature: Each tool is defined with a clear function signature that specifies input and output types,</li>
<li>Metadata Description: Each tool is accompanied by metadata that describes its purpose, suitable use cases, parameters description.</li>
</ol>
<p>Examples of function signatures and metadata descriptions in HoneyComb are:</p>
<h2>- Google Search</h2>
<h2>- Function Signature:</h2>
<p>Google_Search(query: str, timeout: Optional[int] = 30) -&gt; str</p>
<ul>
<li>Metadata Description: General web search for up-to-date information across various topics.</li>
</ul>
<h2>- Wikipedia Search</h2>
<h2>- Function Signature:</h2>
<p>Wikipedia_Search(topic: str, summarize: bool $=$ True) $-&gt;$ str</p>
<ul>
<li>Metadata Description: Retrieves and optionally summarizes detailed Wikipedia articles, particularly useful for quick reference checks.</li>
</ul>
<h2>- A Sample Mass Flow Rate Tool</h2>
<h2>- Function Signature:</h2>
<p>calculate_initial_mass_flow_rate(args: str) -&gt; float</p>
<ul>
<li>Metadata Description: See figure 4.</li>
</ul>
<p>Calculate the initial mass flow rate of liquid metal draining from a cylindrical vessel through a nozzle.</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nt">Parameters</span><span class="o">:</span>
<span class="w">        </span><span class="nt">args</span><span class="w"> </span><span class="o">(</span><span class="nt">str</span><span class="o">):</span><span class="w"> </span><span class="nt">A</span><span class="w"> </span><span class="nt">string</span><span class="w"> </span><span class="nt">containing</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">required</span><span class="w"> </span><span class="nt">parameters</span>
<span class="nt">separated</span><span class="w"> </span><span class="nt">by</span><span class="w"> </span><span class="s2">&quot;;&quot;</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">following</span><span class="w"> </span><span class="nt">order</span><span class="o">:</span>
<span class="w">            </span><span class="nt">-</span><span class="w"> </span><span class="nt">density</span><span class="w"> </span><span class="o">(</span><span class="nt">float</span><span class="o">):</span><span class="w"> </span><span class="nt">Density</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">liquid</span><span class="w"> </span><span class="nt">metal</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">kg</span><span class="o">/</span>
<span class="nt">m</span><span class="o">^</span><span class="nt">3</span>
<span class="w">    </span><span class="nt">mm</span>
<span class="w">            </span><span class="nt">-</span><span class="w"> </span><span class="nt">discharge_coefficiemt</span><span class="w"> </span><span class="o">(</span><span class="nt">float</span><span class="o">):</span><span class="w"> </span><span class="nt">Discharge</span><span class="w"> </span><span class="nt">coefficient</span>
<span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">nozzle</span><span class="w"> </span><span class="o">(</span><span class="nt">dimensionless</span><span class="o">)</span>
<span class="w">            </span><span class="nt">-</span><span class="w"> </span><span class="nt">height</span><span class="w"> </span><span class="o">(</span><span class="nt">float</span><span class="o">):</span><span class="w"> </span><span class="nt">Height</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">liquid</span><span class="w"> </span><span class="nt">metal</span><span class="w"> </span><span class="nt">column</span><span class="w"> </span><span class="nt">in</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">vessel</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">meters</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Returns</span><span class="o">:</span>
<span class="n">float</span><span class="o">:</span><span class="w"> </span><span class="n">Initial</span><span class="w"> </span><span class="n">mass</span><span class="w"> </span><span class="n">flow</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">$</span><span class="o">\</span><span class="n">mathrm</span><span class="o">{</span><span class="n">kg</span><span class="o">}</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">\</span><span class="n">mathrm</span><span class="o">{</span><span class="n">s</span><span class="o">}</span><span class="n">$</span><span class="o">.</span>
</code></pre></div>

<p>Figure 4: Metadata Description of a Sample Mass Flow Rate Tool</p>
<h2>E Examples of Inductive Tool Construction</h2>
<p>See figure 5 for a detailed example illustrating how inductive tool construction work.</p>
<table>
<thead>
<tr>
<th>1. Where a particular person/operation is a "therapy tool" in all 2 "Obstetric Specific Functions" (Body, Body and Liver)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>2. What is a particular person/operation in the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of the first 3rds of</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Equal advising.
${ }^{\dagger}$ Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
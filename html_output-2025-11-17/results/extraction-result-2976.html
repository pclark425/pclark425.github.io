<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2976 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2976</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2976</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/061d113a7b3f32deab6bc50fea676fa0b1e0f658" target="_blank">Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work takes a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks and applies an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.</p>
                <p><strong>Paper Abstract:</strong> Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. Our source code is available at: this https URL.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2976.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2976.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPRC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Paragraph Reading Comprehension DQN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RC-style value-based IF game agent that treats the observation as passage(s) and action templates (verb phrases) as questions, using context-query attention (BiDAF + self-attention) to extract objects for template placeholders and an object-centric retrieval of past observation paragraphs to mitigate partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MPRC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Observation and verb (template) tokens are embedded with fixed GloVe and encoded with a shared LayerNorm + Bi-GRU. BiDAF (context-query) attention produces verb-aware observation representations; residual self-attention further refines them. Argument-specific GRU embeddings produce placeholder-dependent object encodings which are pooled and linearly combined to yield Q(o,a). Trained with DQN; prioritized trajectory sampling is used. Memory is implemented via an object-centric multi-paragraph retrieval mechanism that concatenates K retrieved past observations (separated by a special token) to the current observation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho suite (33 Interactive Fiction games, e.g., Zork1, Zork3, Adventureland, Detective, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Human-authored Interactive Fiction games with natural-language observations, large combinatorial natural-language action spaces (template + object placeholders), and partial observability; Jericho provides a benchmark collection of such games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (object-centric historical observation memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Stores past textual observation paragraphs (complete textual observations). For each detected object in the current observation, the system retrieves the most recent K past observations that mention at least one shared object; the K retrieved observations are sorted by timestep and concatenated (with special separators) to the current observation, forming the multi-paragraph input to the RC model. Implementation uses a global K (history window) = 2.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Time-sensitive object-centric retrieval: detect objects in current observation (via spaCy), for each object retrieve the most recent K past observations that mention that object; sort by time and concatenate.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Configurable history window K (implementation used K = 2); total token count limited by observation lengths (not an explicitly bounded capacity beyond K)</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Raw past textual observations (observation paragraphs) including descriptions of locations, objects, and recent feedback; implicitly stores object mentions and their textual contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On the Jericho 33-game suite MPRC-DQN achieved best per-game scores on 21/33 games (64% of games) after training with 0.1M environment interactions; reported macro-average human-normalized progress for the (authors' combined approaches) was 28.5% (versus DRRN 17.8%). Per-table scores are reported per game (see paper Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>RC-DQN (the RC model without multi-paragraph retrieval) achieved best per-game on 17/33 games (52% winning percentage) under the same 0.1M interaction budget; thus adding object-centric retrieval yields an improvement in the number of games where the agent is best (from 52% to 64%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>+12 percentage points in the count of games where the agent is the best (64% vs 52% winning-rate comparing MPRC-DQN to RC-DQN); authors also report faster convergence when using MPRC-DQN versus RC-DQN, and per-game improvements are heterogeneous (game-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Object-centric retrieval of past observations mitigates partial observability by retrieving history that is more likely to be relevant to the current decision (shared objects), improves final performance across many IF games, and speeds learning (faster convergence) compared to using only the current observation or simple recency-based history. Retrieval strategy and history size matter and are game-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Effect is game-dependent; choice of history window K matters; pure recency-based retrieval performs worse or adds high variance (because successive observations may add little new object information and policy changes create variance); older observations can be invalidated by later events (time-sensitivity). The memory is text-based and may include noisy irrelevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared object-centric retrieval vs pure recency-based retrieval and vs no-history (RC-DQN); object-centric retrieval outperformed pure recency and no-history in aggregate. Compared indirectly to graph-based baselines (KG-A2C/KG-DQN): MPRC-DQN outperformed KG-A2C in pairwise comparisons (MPRC-DQN vs KG-A2C: 18 wins, 7 draws, 3 losses across games).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2976.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2976.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reading Comprehension DQN (no-history variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The RC-based action-value model from this paper without the multi-paragraph (historical observation) retrieval: uses BiDAF + self-attention and argument-specific embeddings to predict Q-values from the current observation only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same core RC architecture as MPRC-DQN (GloVe embeddings, LayerNorm + Bi-GRU encoder, BiDAF context-query attention, self-attention, argument-specific GRUs, pooling to Q-value), but the input is limited to the current observation (no appended retrieved historical observations). Trained with DQN and same prioritized trajectory sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho suite (33 IF games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Interactive Fiction games with human-written narrative text, large natural-language action spaces and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>RC-DQN (no-history) achieved best per-game on 17/33 games (52% winning percentage) under the 0.1M interaction training budget; it already substantially outperformed prior baselines, indicating the RC formulation itself yields major gains even without explicit historical retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>The RC formulation (even without memory) accounts for a large portion of performance gains; adding object-centric history retrieval (MPRC-DQN) provides additional, consistent improvements (see MPRC-DQN).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>RC-DQN (no history) was compared to MPRC-DQN (object-centric retrieval) and to recency-based retrieval variants in ablations; object-centric retrieval improved over RC-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2976.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2976.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-constrained (Knowledge-Graph) A2C</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that constructs an object-centric knowledge graph from historical textual observations (using OpenIE and human-written heuristics) and uses that graph as the state representation, with a GRU-based action generator and A2C for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extracts objects and relations from past observations using OpenIE and hand-crafted heuristics to build an object-graph (knowledge graph) representing state; uses a recurrent (GRU) action generator conditioned on the graph and is trained with Advantage Actor-Critic (A2C).</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho suite (used as baseline on IF games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Jericho IF games (human-authored interactive fiction) exhibiting partial observability, large action spaces and complex object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Maintains an object-centric knowledge graph where nodes are objects (and possibly locations/characters) and edges encode relations (spatial, possession, interactions) extracted from observations by OpenIE and hand-written rules; the graph is incrementally updated from historical observations and used as the agent's state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Graph-structured state representation: information retrieval is implicit via graph features and traversal; the paper (this baseline's source) uses OpenIE + heuristics to extract relations, not a learned semantic retrieval over free text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified in this paper (the graph grows with observed objects/relations; practical implementations may cap or prune but no explicit capacity reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Objects as nodes and extracted relations (spatial relations, object attributes, interactions) from historical observations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported baseline results in Table 2 (per-game scores); overall KG-A2C had best-per-game scores on 9/33 games (27% winning percentage) under a 1.6M interaction training budget per prior work (scores vary by game; see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No ablation without the graph is reported here for KG-A2C; compared against other baselines (DRRN, TDQN) in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Graph-based state representations aim to mitigate partial observability by aggregating object-centric relations across history; however, according to this paper the approach can be noisy (treating history equally and compressing into a single vector) and requires OpenIE plus heuristics, and MPRC-DQN (object-centric retrieval + RC) outperforms KG-A2C in the Jericho benchmark under the reported budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Requires OpenIE and human-written rules to extract relations (brittle); historical observations are summarized into a single vector which may introduce noise; improvements are not always significant across games.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared indirectly in this paper: MPRC-DQN (object-centric multi-paragraph retrieval + RC) outperforms KG-A2C in pairwise game comparisons (MPRC-DQN v.s. KG-A2C: 18 wins, 7 draws, 3 losses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2976.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2976.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph DQN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier graph-based approach that builds a knowledge graph (objects, positions, relations) from historical observations and uses graph-based state representation with DQN-style learning (applied originally to synthetic text games).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a knowledge graph from observed textual observations (objects as nodes, relations as edges) using OpenIE and rules; uses that structured representation to inform action selection with a DQN-style learner.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Originally studied on synthetic text-game settings; referenced here as an approach for handling partial observability/structure in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Synthetic text games or small domains where object relations and spatial structure can be reliably extracted; not the Jericho human-written IF suite focus but conceptually related.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Knowledge graph of objects, positions, and spatial relations built from historical observations via information extraction and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Graph-based state representation; retrieval is via features/traversal over the constructed graph rather than free-text retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Objects, positions, and relations extracted from observations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not reported in this paper for the Jericho IF suite; KG-DQN was designed for synthetic game domains (paper cited for that work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Graph-based memory can represent state structure explicitly but relies on accurate relation extraction; the present paper argues their object-centric textual retrieval + RC approach better exploits raw textual evidence without heavy IE/rules and achieves stronger results on Jericho.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Relies on external IE tools and heuristics; may not generalize well to the varied language in human-authored IFs; summarizing history into a single vector reduces ability to focus on specific evidential passages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>NAIL: A general interactive fiction agent <em>(Rating: 1)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2976",
    "paper_id": "paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "MPRC-DQN",
            "name_full": "Multi-Paragraph Reading Comprehension DQN",
            "brief_description": "An RC-style value-based IF game agent that treats the observation as passage(s) and action templates (verb phrases) as questions, using context-query attention (BiDAF + self-attention) to extract objects for template placeholders and an object-centric retrieval of past observation paragraphs to mitigate partial observability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MPRC-DQN",
            "agent_description": "Observation and verb (template) tokens are embedded with fixed GloVe and encoded with a shared LayerNorm + Bi-GRU. BiDAF (context-query) attention produces verb-aware observation representations; residual self-attention further refines them. Argument-specific GRU embeddings produce placeholder-dependent object encodings which are pooled and linearly combined to yield Q(o,a). Trained with DQN; prioritized trajectory sampling is used. Memory is implemented via an object-centric multi-paragraph retrieval mechanism that concatenates K retrieved past observations (separated by a special token) to the current observation.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Jericho suite (33 Interactive Fiction games, e.g., Zork1, Zork3, Adventureland, Detective, etc.)",
            "text_game_description": "Human-authored Interactive Fiction games with natural-language observations, large combinatorial natural-language action spaces (template + object placeholders), and partial observability; Jericho provides a benchmark collection of such games.",
            "uses_memory": true,
            "memory_type": "retrieval-augmented (object-centric historical observation memory)",
            "memory_architecture": "Stores past textual observation paragraphs (complete textual observations). For each detected object in the current observation, the system retrieves the most recent K past observations that mention at least one shared object; the K retrieved observations are sorted by timestep and concatenated (with special separators) to the current observation, forming the multi-paragraph input to the RC model. Implementation uses a global K (history window) = 2.",
            "memory_retrieval_mechanism": "Time-sensitive object-centric retrieval: detect objects in current observation (via spaCy), for each object retrieve the most recent K past observations that mention that object; sort by time and concatenate.",
            "memory_capacity": "Configurable history window K (implementation used K = 2); total token count limited by observation lengths (not an explicitly bounded capacity beyond K)",
            "what_is_stored_in_memory": "Raw past textual observations (observation paragraphs) including descriptions of locations, objects, and recent feedback; implicitly stores object mentions and their textual contexts.",
            "performance_with_memory": "On the Jericho 33-game suite MPRC-DQN achieved best per-game scores on 21/33 games (64% of games) after training with 0.1M environment interactions; reported macro-average human-normalized progress for the (authors' combined approaches) was 28.5% (versus DRRN 17.8%). Per-table scores are reported per game (see paper Table 2).",
            "performance_without_memory": "RC-DQN (the RC model without multi-paragraph retrieval) achieved best per-game on 17/33 games (52% winning percentage) under the same 0.1M interaction budget; thus adding object-centric retrieval yields an improvement in the number of games where the agent is best (from 52% to 64%).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "+12 percentage points in the count of games where the agent is the best (64% vs 52% winning-rate comparing MPRC-DQN to RC-DQN); authors also report faster convergence when using MPRC-DQN versus RC-DQN, and per-game improvements are heterogeneous (game-dependent).",
            "key_findings_about_memory": "Object-centric retrieval of past observations mitigates partial observability by retrieving history that is more likely to be relevant to the current decision (shared objects), improves final performance across many IF games, and speeds learning (faster convergence) compared to using only the current observation or simple recency-based history. Retrieval strategy and history size matter and are game-dependent.",
            "memory_limitations": "Effect is game-dependent; choice of history window K matters; pure recency-based retrieval performs worse or adds high variance (because successive observations may add little new object information and policy changes create variance); older observations can be invalidated by later events (time-sensitivity). The memory is text-based and may include noisy irrelevant content.",
            "comparison_with_other_memory_types": "Compared object-centric retrieval vs pure recency-based retrieval and vs no-history (RC-DQN); object-centric retrieval outperformed pure recency and no-history in aggregate. Compared indirectly to graph-based baselines (KG-A2C/KG-DQN): MPRC-DQN outperformed KG-A2C in pairwise comparisons (MPRC-DQN vs KG-A2C: 18 wins, 7 draws, 3 losses across games).",
            "uuid": "e2976.0",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RC-DQN",
            "name_full": "Reading Comprehension DQN (no-history variant)",
            "brief_description": "The RC-based action-value model from this paper without the multi-paragraph (historical observation) retrieval: uses BiDAF + self-attention and argument-specific embeddings to predict Q-values from the current observation only.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RC-DQN",
            "agent_description": "Same core RC architecture as MPRC-DQN (GloVe embeddings, LayerNorm + Bi-GRU encoder, BiDAF context-query attention, self-attention, argument-specific GRUs, pooling to Q-value), but the input is limited to the current observation (no appended retrieved historical observations). Trained with DQN and same prioritized trajectory sampling.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Jericho suite (33 IF games)",
            "text_game_description": "Interactive Fiction games with human-written narrative text, large natural-language action spaces and partial observability.",
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": "RC-DQN (no-history) achieved best per-game on 17/33 games (52% winning percentage) under the 0.1M interaction training budget; it already substantially outperformed prior baselines, indicating the RC formulation itself yields major gains even without explicit historical retrieval.",
            "has_ablation_study": true,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "The RC formulation (even without memory) accounts for a large portion of performance gains; adding object-centric history retrieval (MPRC-DQN) provides additional, consistent improvements (see MPRC-DQN).",
            "memory_limitations": null,
            "comparison_with_other_memory_types": "RC-DQN (no history) was compared to MPRC-DQN (object-centric retrieval) and to recency-based retrieval variants in ablations; object-centric retrieval improved over RC-DQN.",
            "uuid": "e2976.1",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "Graph-constrained (Knowledge-Graph) A2C",
            "brief_description": "A baseline that constructs an object-centric knowledge graph from historical textual observations (using OpenIE and human-written heuristics) and uses that graph as the state representation, with a GRU-based action generator and A2C for optimization.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "agent_name": "KG-A2C",
            "agent_description": "Extracts objects and relations from past observations using OpenIE and hand-crafted heuristics to build an object-graph (knowledge graph) representing state; uses a recurrent (GRU) action generator conditioned on the graph and is trained with Advantage Actor-Critic (A2C).",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Jericho suite (used as baseline on IF games)",
            "text_game_description": "Jericho IF games (human-authored interactive fiction) exhibiting partial observability, large action spaces and complex object interactions.",
            "uses_memory": true,
            "memory_type": "graph-based memory (knowledge graph)",
            "memory_architecture": "Maintains an object-centric knowledge graph where nodes are objects (and possibly locations/characters) and edges encode relations (spatial, possession, interactions) extracted from observations by OpenIE and hand-written rules; the graph is incrementally updated from historical observations and used as the agent's state representation.",
            "memory_retrieval_mechanism": "Graph-structured state representation: information retrieval is implicit via graph features and traversal; the paper (this baseline's source) uses OpenIE + heuristics to extract relations, not a learned semantic retrieval over free text.",
            "memory_capacity": "Not specified in this paper (the graph grows with observed objects/relations; practical implementations may cap or prune but no explicit capacity reported here).",
            "what_is_stored_in_memory": "Objects as nodes and extracted relations (spatial relations, object attributes, interactions) from historical observations.",
            "performance_with_memory": "Reported baseline results in Table 2 (per-game scores); overall KG-A2C had best-per-game scores on 9/33 games (27% winning percentage) under a 1.6M interaction training budget per prior work (scores vary by game; see Table 2).",
            "performance_without_memory": "No ablation without the graph is reported here for KG-A2C; compared against other baselines (DRRN, TDQN) in the paper.",
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Graph-based state representations aim to mitigate partial observability by aggregating object-centric relations across history; however, according to this paper the approach can be noisy (treating history equally and compressing into a single vector) and requires OpenIE plus heuristics, and MPRC-DQN (object-centric retrieval + RC) outperforms KG-A2C in the Jericho benchmark under the reported budgets.",
            "memory_limitations": "Requires OpenIE and human-written rules to extract relations (brittle); historical observations are summarized into a single vector which may introduce noise; improvements are not always significant across games.",
            "comparison_with_other_memory_types": "Compared indirectly in this paper: MPRC-DQN (object-centric multi-paragraph retrieval + RC) outperforms KG-A2C in pairwise game comparisons (MPRC-DQN v.s. KG-A2C: 18 wins, 7 draws, 3 losses).",
            "uuid": "e2976.2",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge Graph DQN",
            "brief_description": "Earlier graph-based approach that builds a knowledge graph (objects, positions, relations) from historical observations and uses graph-based state representation with DQN-style learning (applied originally to synthetic text games).",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "Constructs a knowledge graph from observed textual observations (objects as nodes, relations as edges) using OpenIE and rules; uses that structured representation to inform action selection with a DQN-style learner.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Originally studied on synthetic text-game settings; referenced here as an approach for handling partial observability/structure in text games.",
            "text_game_description": "Synthetic text games or small domains where object relations and spatial structure can be reliably extracted; not the Jericho human-written IF suite focus but conceptually related.",
            "uses_memory": true,
            "memory_type": "graph-based memory (knowledge graph)",
            "memory_architecture": "Knowledge graph of objects, positions, and spatial relations built from historical observations via information extraction and rules.",
            "memory_retrieval_mechanism": "Graph-based state representation; retrieval is via features/traversal over the constructed graph rather than free-text retrieval.",
            "memory_capacity": "Not specified in this paper.",
            "what_is_stored_in_memory": "Objects, positions, and relations extracted from observations.",
            "performance_with_memory": "Not reported in this paper for the Jericho IF suite; KG-DQN was designed for synthetic game domains (paper cited for that work).",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Graph-based memory can represent state structure explicitly but relies on accurate relation extraction; the present paper argues their object-centric textual retrieval + RC approach better exploits raw textual evidence without heavy IE/rules and achieves stronger results on Jericho.",
            "memory_limitations": "Relies on external IE tools and heuristics; may not generalize well to the varied language in human-authored IFs; summarizing history into a single vector reduces ability to focus on specific evidential passages.",
            "uuid": "e2976.3",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "NAIL: A general interactive fiction agent",
            "rating": 1
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2
        }
    ],
    "cost": 0.01803875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</h1>
<p>Xiaoxiao Guo<em><br>IBM Research<br>xiaoxiao.guo@ibm.com<br>$\frac{\text { Mo Yu</em> }}{\text { IBM Research }}$<br>yum@us.ibm.com<br>Yupeng Gao<br>IBM Research<br>yupeng.gao@ibm.com<br>Chuang Gan<br>MIT-IBM Watson AI Lab<br>chuangg@ibm.com<br>Murray Campbell<br>IBM Research<br>mcam@us.ibm.com<br>Shiyu Chang<br>MIT-IBM Watson AI Lab<br>shiyu.chang@ibm.com</p>
<h4>Abstract</h4>
<p>Interactive Fiction (IF) games with real humanwritten natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the humanwritten textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Interactive systems capable of understanding natural language and responding in the form of natural language text have high potentials in various applications. In pursuit of building and evaluating such systems, we study learning agents for Interactive Fiction (IF) games. IF games are world-simulating software in which players use text commands to control the protagonist and influence the world, as illustrated in Figure 1. IF gameplay agents need to simultaneously understand the game's information from a text display (observation) and generate</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sample gameplay for the classic dungeon game Zork1. The objective is to solve various puzzles and collect the 19 treasures to install the trophy case. The player receives textual observations describing the current game state and additional reward scalars encoding the game designers' objective of game progress. The player sends textual action commands to control the protagonist.
natural language command (action) via a text input interface. Without providing an explicit game strategy, the agents need to identify behaviors that maximize objective-encoded cumulative rewards.</p>
<p>IF games composed of human-written texts (distinct from previous text games with synthetic texts) create superb new opportunities for studying and evaluating natural language understanding (NLU) techniques due to their unique characteristics. (1) Game designers elaborately craft on the literariness of the narrative texts to attract players when creating IF games. The resulted texts in IF games are more linguistically diverse and sophisticated than the template-generated ones in synthetic text games. (2) The language contexts of IF games</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our approach to solving the IF games as Multi-Paragraph Reading Comprehension (MPRC) tasks.
are more versatile because various designers contribute to enormous domains and genres, such as adventure, fantasy, horror, and sci-fi. (3) The text commands to control characters are less restricted, having sizes over six orders of magnitude larger than previous text games. The recently introduced Jericho benchmark provides a collection of such IF games (Hausknecht et al., 2019a).</p>
<p>The complexity of IF games demands more sophisticated NLU techniques than those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning (RL), poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in the huge natural language action space. To make RL agents learn efficiently without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others. To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently (Narasimhan et al., 2015; Hausknecht et al., 2019a); or embed each valid action as another vector and predict action value based on the vector-space similarities (He et al., 2016). These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient.</p>
<p>The second challenge is partial observability. At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world. But the latest observation is often not a sufficient summary of the interaction history and may not provide enough
information to determine the long-term effects of actions. Previous approaches address this problem by building a representation over past observations (e.g., building a graph of objects, positions, and spatial relations) (Ammanabrolu and Riedl, 2019; Ammanabrolu and Hausknecht, 2020). These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant.</p>
<p>We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension (MPRC) and harness MPRC techniques to solve the huge action space and partial observability challenges. The graphical illustration is shown in Figure 2. First, the action value prediction (i.e., predicting the long-term rewards of selecting an action) is essentially generating and scoring a compositional action structure by finding supporting evidence from the observation. We base on the fact that each action is an instantiation of a template, i.e., a verb phrase with a few placeholders of object arguments it takes (Figure 2b). Then the action generation process can be viewed as extracting objects for a template's placeholders from the textual observation, based on the interaction between the template verb phrase and the relevant context of the objects in the observation. Our approach addresses the structured prediction and interaction problems with the idea of context-question attention mechanism in RC models. Specifically, we treat the observation as a passage and each template verb phrase as a question. The filling of object placeholders in the template thus becomes an</p>
<p>extractive QA problem that selects objects from the observation given the template. Simultaneously each action (i.e., a template with all placeholder replaced) gets its evaluation value predicted by the RC model. Our formulation and approach better capture the fine-grained interactions between observation texts and structural actions, in contrast to previous approaches that represent the observation as a single vector and ignore the fine-grained dependency among action elements.</p>
<p>Second, alleviating partial observability is essentially enhancing the current observation with potentially relevant history and predicting actions over the enhanced observation. Our approach retrieves potentially relevant historical observations with an object-centric approach (Figure 2a), so that the retrieved ones are more likely to be connected to the current observation as they describe at least one shared interactable object. Our attention mechanisms are then applied across the retrieved multiple observation texts to focus on informative contexts for action value prediction.</p>
<p>We evaluated our approach on the suite of Jericho IF games, compared to all previous approaches. Our approaches achieved or outperformed the state-of-the-art performance on 25 out of 33 games, trained with less than one-tenth of game interaction data used by prior art. We also provided ablation studies on our models and retrieval strategies.</p>
<h2>2 Related Work</h2>
<p>IF Game Agents. Previous work mainly studies the text understanding and generation in parserbased or rule-based text game tasks, such as TextWorld platform (Ct et al., 2018) or custom domains (Narasimhan et al., 2015; He et al., 2016; Adhikari et al., 2020). The recent platform Jericho (Hausknecht et al., 2019a) supports over thirty human-written IF games. Earlier successes in real IF games mainly rely on heuristics without learning. NAIL (Hausknecht et al., 2019b) is the state-of-theart among these "no-learning" agents, employing a series of reliable heuristics for exploring the game, interacting with objects, and building an internal representation of the game world. With the development of learning environments like Jericho, the RL-based agents have started to achieve dominating performance.</p>
<p>A critical challenge for learning-based agents is how to handle the combinatorial action space in IF games. LSTM-DQN (Narasimhan et al., 2015)
was proposed to generate verb-object action with pre-defined sets of possible verbs and objects, but treat the selection and learning of verbs and objects independently. Template-DQN (Hausknecht et al., 2019a) extended LSTM-DQN for template-based action generation, introducing one additional but still independent prediction output for the second object in the template. Deep Reinforcement Relevance Network (DRRN) (He et al., 2016) was introduced for choice-based games. Given a set of valid actions at every game state, DRRN projects each action into a hidden space that matches the current state representation vector for action selection. Action-Elimination Deep Q-Network (AEDQN) (Zahavy et al., 2018) learns to predict invalid actions in the adventure game Zork. It eliminates invalid action for efficient policy learning via utilizing expert demonstration data.</p>
<p>Other techniques focus on addressing the partial observability in text games. Knowledge Graph DQN (KG-DQN) (Ammanabrolu and Riedl, 2019) was proposed to deal with synthetic games. The method constructs and represents the game states as knowledge graphs with objects as nodes and uses pre-trained general purposed OpenIE tool and human-written rules to extract relations between objects. KG-DQN handles the action representation following DRRN. KG-A2C (Ammanabrolu and Hausknecht, 2020) later extends the work for IF games, by adding information extraction heuristics to fit the complexity of the object relations in IF games and utilizing a GRU-based action generator to handle the action space.</p>
<h2>Reading Comprehension Models for Question</h2>
<p>Answering. Given a question, reading comprehension (RC) aims to find the answer to the question based on a paragraph that may contain supporting evidence. One of the standard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting.</p>
<p>Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been</p>
<p>recently applied to enhance the encoding and attention mechanisms of the aforementioned reader models. They give performance boost but are more resource-demanding and do not suit the IF game playing task very well.</p>
<p>Reading Comprehension over Multiple Paragraphs. Multi-paragraph reading comprehension (MPRC) deals with the more general task of answering a question from multiple related paragraphs, where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state representation with historical observations and predict actions from multiple observation paragraphs.</p>
<p>A fundamental research problem in MPRC, which is also critical to our formulation, is to select relevant paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval approaches share a similar high-level idea to our object-based history retrieval approach. The techniques above have been applied to deal with evidence from Wikipedia, news collections, and, recently, books (Mou et al., 2020). We are the first to extend these ideas to IF games.</p>
<h2>3 Multi-Paragraph RC for IF Games</h2>
<h3>3.1 Problem Formulation</h3>
<p>Each IF game can be defined as a Partially Observable Markov Decision Process (POMDP), namely a 7-tuple of $\langle S, A, T, O, \Omega, R, \gamma\rangle$, representing the hidden game state set, the action set, the state transition function, the set of textual observations composed from vocabulary words, the textual observation function, the reward function, and the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Our RC-based action prediction model architecture. The template text is a verb phrase with placeholders for objects, such as [pick up OBJ] and [break OBJ with OBJ].
discount factor respectively. The game playing agent interacts with the game engine in multiple turns until the game is over or the maximum number of steps is reached. At the $t$-th turn, the agent receives a textual observation describing the current game state $o_{t} \in O$ and sends a textual action command $a_{t} \in A$ back. The agent receives additional reward scalar $r_{t}$ which encodes the game designers' objective of game progress. Thus the task of the game playing can be formulated to generate a textual action command per step as to maximize the expected cumulative discounted rewards $\mathbf{E}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right]$. Value-based RL approaches learn to approximate an observation-action value function $Q\left(o_{t}, a_{t} ; \boldsymbol{\theta}\right)$ which measures the expected cumulative rewards of taking action $a_{t}$ when observing $o_{t}$. The agent selects action based on the action value prediction of $Q(o, a ; \boldsymbol{\theta})$.</p>
<p>Template Action Space. Template action space considers actions satisfying decomposition in the form of $\left\langle\right.$ verb, arg $\left.<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle$. verb is an interchangeable verb phrase template with placeholders for objects and $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>$ are optional objects. For example, the action command [east], [pick up eggs] and [break window with stone] can be represented as template actions $\langle$ east, none, none $\rangle$, $\langle$ pick up OBJ, eggs, none and $\langle$ break OBJ with OBJ, window, stone $\rangle$. We reuse the template library and object list from Jericho. The verb phrases usually consist of several vocabulary words and each object is usually a single word.</p>
<h3>3.2 RC Model for Template Actions</h3>
<p>We parameterize the observation-action value function $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg}<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle ; \boldsymbol{\theta}\right)$ by utilizing the decomposition of the template actions and contextquery contextualized representation in RC. Our model treats the observation $o$ as a context in RC and the $\operatorname{verb}=\left(v_{1}, v_{2}, \ldots, v_{k}\right)$ component of the template actions as a query. Then a verb-aware observation representation is derived via a RC reader model with Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and self-attention. The observation representation responding to the $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>}$ words are pooled and projected to a scalar value estimate for $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em 1="1">{0}, \operatorname{arg}</em>\right)$. A high-level model architecture of our model is illustrated in Figure 3.}\right\rangle ; \boldsymbol{\theta</p>
<p>Observation and verb Representation. We tokenize the observation and the verb phrase into words, then embed these words using pre-trained GloVe embeddings (Pennington et al., 2014). A shared encoder block that consists of LayerNorm (Ba et al., 2016) and Bidirectional GRU (Cho et al., 2014) processes the observation and verb word embeddings to obtain the separate observation and verb representation.</p>
<p>Observation-verb Interaction Layers. Given the separate observation and verb representation, we apply two attention mechanisms to compute a verb-contextualized observation representation. We first apply BiDAF with observation as the context input and verb as the query input. Specifically, we denote the processed embeddings for observation word $i$ and template word $j$ as $\boldsymbol{o}<em j="j">{i}$ and $\boldsymbol{t}</em>}$. The attention between the two words is then $a_{i j}=\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{1}} \cdot \boldsymbol{o}</em>}}+\boldsymbol{w<em _boldsymbol_j="\boldsymbol{j">{\mathbf{2}} \cdot \boldsymbol{t}</em>}}+\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{3}} \cdot\left(\boldsymbol{o}</em>}} \otimes \boldsymbol{t<em _mathbf_1="\mathbf{1">{\boldsymbol{j}}\right)$, where $\boldsymbol{w}</em>}}, \boldsymbol{w<em _mathbf_3="\mathbf{3">{\mathbf{2}}$, $\boldsymbol{w}</em>}}$ are learnable vectors and $\otimes$ is element-wise product. We then compute the "verb2observation" attention vector for the $i$-th observation word as $\boldsymbol{c<em j="j">{\boldsymbol{i}}=\sum</em>} p_{i j} \boldsymbol{t<em i="i" j="j">{\boldsymbol{j}}$ with $p</em>}=\exp \left(a_{i j}\right) / \sum_{j} \exp \left(a_{i j}\right)$. Similarly, we compute the "observation2verb" attention vector as $\boldsymbol{q}=\sum_{i} p_{i} \boldsymbol{o<em i="i">{\boldsymbol{i}}$ with $p</em>=$ $\exp \left(\max <em i="i" j="j">{j} a</em> \exp \left(\max }\right) / \sum_{i<em i="i" j="j">{j} a</em>}\right)$. We concatenate and project the output vectors as $\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{4}} \cdot\left[\boldsymbol{o}</em>}}\right.$, $\boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{o}</em>}} \otimes \boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{q} \otimes \boldsymbol{c}</em> \mid$, followed by a linear layer with leaky ReLU activation units (Maas et al., 2013). The output vectors are processed by an encoder block. We then apply a residual self-attention on the outputs of the encoder block. The self-attention is the same as BiDAF, but only between the observation and itself.}</p>
<p>Observation-Action Value Prediction. We generate an action by replacing the placeholders $\left(\operatorname{arg}<em 1="1">{0}\right.$ and $\left.\operatorname{arg}</em>}\right)$ in a template with objects appearing in the observation. The observation-action value $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>}\right\rangle ; \theta\right)$ is achieved by processing each object's corresponding verb-contextualized observation representation. Specifically, we get the indices of an $o b j$ in the observation texts $I(o b j, o)$. When the object is a noun phrase, we take the index of its headword. ${ }^{2}$ Because the same object has different meanings when it replaces different placeholders, we apply two GRU-based embedding functions for the two placeholders, to get the object's verb-placeholder dependent embeddings. We derive a single vector representation $\boldsymbol{h<em 0="0">{\text {org }</em>}=\text { obj <em 0="0">{m}}$ for the case that the placeholder $\operatorname{arg}</em>}$ is replaced by $o b j_{m}$ by meanpooling over the verb-placeholder dependent embeddings indexed by $I\left(o b j_{m}, o\right)$ for the corresponding placeholder $\operatorname{arg<em 5="5">{0}$. We apply a linear transformation on the concatenated embeddings of the two placeholders to obtain the observation action value $Q(o, a)=\boldsymbol{w}</em>} \cdot\left[\boldsymbol{h<em 1="1">{\text {org }</em>}=\text { obj <em _org="{org" _text="\text">{m}}, \boldsymbol{h}</em><em m="m">{1}=\text { obj }</em>}}\right]$ for $a=\langle$ verb, $\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>\right\rangle$. Our formulation avoids the repeated computation overhead among different actions with a shared template verb phrase.</p>
<h3>3.3 Multi-Paragraph Retrieval Method for Partial Observability</h3>
<p>The observation at the current step sometimes does not have full-textual evidence to support action selection and value estimation, due to the inherent partial observability of IF games. For example, when repeatedly attacking a troll with a sword, the player needs to know the effect or feedback of the last attack to determine if an extra attack is necessary. It is thus important for an agent to efficiently utilize historical observations to better support action value prediction. In our RC-based action prediction model, the historical observation utilization can be formulated as selecting evidential observation paragraphs in history, and predicting the action values from multiple selected observations, namely a Multiple-Paragraph Reading Comprehension (MPRC) problem. We propose to retrieve past observations with an object-centric approach.</p>
<p>Past Observation Retrieval. Multiple past observations may share objects with the current obser-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Agents</th>
<th>Action strategy</th>
<th>State strategy</th>
<th>Interaction data</th>
</tr>
</thead>
<tbody>
<tr>
<td>TDQN</td>
<td>Independent selection of template and the <br> two objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>DRRN</td>
<td>Action as a word sequence without distin- <br> guishing the roles of verbs and objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>KG-A2C</td>
<td>Recurrent neural decoder that selects the <br> template and objects in a fixed order</td>
<td>Object graph from historical observations <br> based on OpenIE and human-written rules</td>
<td>1.6 M</td>
</tr>
<tr>
<td>Ours</td>
<td>Observation-template representation for <br> object-centric value prediction</td>
<td>Object-based history observation retrieval</td>
<td>0.1 M</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of the main technical differences between our agent and the baselines. All agents use DQN to update the model parameters except KG-A2C uses A2C. All agents use the same handicaps.
vation, and it is computationally expensive and unnecessary to retrieve all of such observations. The utility of past observations associated with each object is often time-sensitive in that new observations may entirely or partially invalidate old observations. We thus propose a time-sensitive strategy for retrieving past observations. Specifically, given the detected objects from the current observation, we retrieve the most recent $K$ observations with at least one shared object. The $K$ retrieved observations are sorted by time steps and concatenated to the current observation. The observations from different time steps are separated by a special token. Our RC-based action prediction model treats the concatenated observations as the observation inputs, and no other parts are changed. We use the notation $o_{t}$ to represent the current observation and the extended current observation interchangeably.</p>
<h3>3.4 Training Loss</h3>
<p>We apply the Deep Q-Network (DQN) (Mnih et al., 2015) to update the parameters $\boldsymbol{\theta}$ of our RC-based action prediction model. The loss function is:</p>
<p>$$
\begin{aligned}
\mathcal{L}(\theta)=\mathbf{E}<em t="t">{\left(o</em> ; \theta\right)\right.\right. \
&amp; \left.\left.-\left(r_{t}+\gamma \max }, a_{t}, r_{t}, o_{t+1}\right) \sim \rho(\mathcal{D})} &amp; \left[\left|Q\left(o_{t}, a_{t<em t_1="t+1">{b} Q\left(o</em>\right)\right)\right|\right]
\end{aligned}
$$}, b ; \theta^{-</p>
<p>where $\mathcal{D}$ is the experience replay consisting of recent gameplay transition records and $\rho$ is a distribution over the transitions defined by a sampling strategy.</p>
<p>Prioritized Trajectories. The distribution $\rho$ has a decent impact on DQN performance. Previous work samples transition tuples with immediate positive rewards more frequently to speed up learning (Narasimhan et al., 2015; Hausknecht et al., 2019a). We observe that this heuristic is often insufficient. Some transitions with zero immediate
rewards or even negative rewards are also indispensable in recovering well-performed trajectories. We thus extend the strategy from transition level to trajectory level. We prioritize transitions from trajectories that outperform the exponential moving average score of recent trajectories.</p>
<h2>4 Experiments</h2>
<p>We evaluate our proposed methods on the suite of Jericho supported games. We compared to all previous baselines that include recent methods addressing the huge action space and partial observability challenges.</p>
<h3>4.1 Setup</h3>
<p>Jericho Handicaps and Configuration. The handicaps used by our methods are the same as other baselines. First, we use the Jericho API to check if an action is valid with game-specific templates. Second, we augmented the observation with the textual feedback returned by the command [inventory] and [look]. Previous work also included the last action or game score as additional inputs. Our model discarded these two types of inputs as we did not observe a significant difference by our model. The maximum game step number is set to 100 following baselines.</p>
<p>Implementation Details. We apply spaCy ${ }^{3}$ to tokenize the observations and detect the objects in the observations. We use the 100-dimensional GloVe embeddings as fixed word embeddings. The out-of-vocabulary words are mapped to a randomly initialized embedding. The dimension of Bi-GRU hidden states is 128 . We set the observation representation dimension to be 128 throughout the model. The history retrieval window $K$ is 2 . For DQN configuration, we use the $\epsilon$-greedy strategy</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Game</th>
<th style="text-align: center;">Human <br> Max</th>
<th style="text-align: center;">Walkthrough-100</th>
<th style="text-align: center;">TDQN</th>
<th style="text-align: center;">Baselines <br> DRRN</th>
<th style="text-align: center;">KG-A2C</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MPRC-DQN</td>
<td style="text-align: center;">RC-DQN</td>
</tr>
<tr>
<td style="text-align: center;">905</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">acorncourt</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">advent</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">36</td>
</tr>
<tr>
<td style="text-align: center;">adventureland</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: center;">afflicted</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">anchor</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">awaken</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">balances</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">deephome</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">detective</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">169</td>
<td style="text-align: center;">197.8</td>
<td style="text-align: center;">207.9</td>
<td style="text-align: center;">317.7</td>
<td style="text-align: center;">291.3</td>
</tr>
<tr>
<td style="text-align: center;">dragon</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$-5.3$</td>
<td style="text-align: center;">$-3.5$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">4.84</td>
</tr>
<tr>
<td style="text-align: center;">enchanter</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">inhumane</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">jewel</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">karn</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">library</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">18.1</td>
</tr>
<tr>
<td style="text-align: center;">ludicorp</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">moonlit</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">omniquest</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">pentari</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: center;">reverb</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">snacktime</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">sorcerer</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">spellbrkr</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">spirit</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">5.2</td>
</tr>
<tr>
<td style="text-align: center;">temple</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">tryst205</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">yomomma</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">zenon</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">zork1</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: center;">zork3</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$3^{a}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">2.83</td>
</tr>
<tr>
<td style="text-align: center;">ztuu</td>
<td style="text-align: center;">$100^{b}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;">Winning percentage / counts</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24\%/8</td>
<td style="text-align: center;">30\%/10</td>
<td style="text-align: center;">27\%/9</td>
<td style="text-align: center;">64\%/21</td>
<td style="text-align: center;">52\%/17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76\%/25</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Average game scores on Jericho benchmark games. The best performing agent score per game is in bold.
The Winning percentage / counts row computes the percentage / counts of games that the corresponding agent is best. The scores of baselines are from their papers. The missing scores are represented as "-", for which games KG-A2C skipped. We also added the 100 -step results from a human-written game-playing walkthrough, as a reference of human-level scores. We denote the difficulty levels of the games defined in the original Jericho paper with colors in their names - possible (i.e., easy or normal) games in green color, difficult games in tan and extreme games in red. Best seen in color.
a Zork3 walkthrough does not maximize the score in the first 100 steps but explores more. ${ }^{b}$ Our agent discovers some unbounded reward loops in the game Ztuu.
for exploration, annealing $\epsilon$ from 1.0 to $0.05 . \gamma$ is 0.98 . We use Adam to update the weights with $10^{-4}$ learning rate. Other parameters are set to their default values. More details of the Reproducibility Checklist is in Appendix A.</p>
<p>Baselines. We compare with all the public results on the Jericho suite, namely TDQN (Hausknecht et al., 2019a), DRRN (He et al., 2016), and KGA2C (Ammanabrolu and Hausknecht, 2020). As discussed, our approaches differ from them mainly in the strategies of handling the large action space and partial observability of IF games. We summarize these main technical differences in Table 1. In summary, all previous agents predict actions con-
ditioned on a single vector representation of the whole observation texts. Thus they do not exploit the fine-grained interplay among the template components and the observations. Our approach addresses this problem by formulating action prediction as an RC task, better utilizing the rich textual observations with deeper language understanding.</p>
<p>Training Sample Efficiency. We update our models for 100, 000 times. Our agents interact with the environment one step per update, resulting in a total of 0.1 M environment interaction data. Compared to the other agents, such as KG-A2C (1.6M), TDQN (1M), and DRRN (1M), our environment interaction data is significantly smaller.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">Template Action <br> Space $\left(\times 10^{6}\right)$</th>
<th style="text-align: center;">Avg. Steps <br> Per Reward</th>
<th style="text-align: center;">Dialog <br> Actions</th>
<th style="text-align: center;">Darkness <br> Limit</th>
<th style="text-align: center;">Nonstandard</th>
<th style="text-align: center;">Inventory</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">advent</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ludicorp</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">pentari</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">spirit</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 3: Difficulty levels and characteristics of games on which our approach achieves the most considerable improvement. Dialog indicates that it is necessary to speak with another character. Darkness indicates that accessing some dark areas requires a light source. Nonstandard Actions refers to actions with words not in an English dictionary. Inventory Limit restricts the number of items carried by the player. Please refer to (Hausknecht et al., 2019a) for more comprehensive definitions.</p>
<h3>4.2 Overall Performance</h3>
<p>We summarize the performance of our MultiParagraph Reading Comprehension DQN (MPRCDQN) agent and baselines in Table 2. Of the 33 IF games, our MPRC-DQN achieved or improved the state of the art performance on 21 games (i.e., a winning rate of $64 \%$ ). The best performing baseline (DRRN) achieved the state-of-the-art performance on only ten games, corresponding to the winning rate of $30 \%$, lower than half of ours. Note that all the methods achieved the same initial scores on five games, namely 905, anchor, awaken, deephome, and moonlit. Apart from these five games, our MPRC-DQN achieved more than three times wins. Our MPRC-DQN achieved significant improvement on some games, such as adventureland, afflicted, detective, etc. Appendix C shows some game playing trajectories.</p>
<p>We include the performance of an RC-DQN agent, which implements our RC-based action prediction model but only takes the current observations as inputs. It also outperformed the baselines by a large margin. After we consider the RC-DQN agent, our MPRC-DQN still has the highest winning percentage, indicating that our RC-based action prediction model has a significant impact on the performance improvement of our MPRC-DQN and the improvement from the multi-passage retrieval is also unneglectable. Moreover, compared to RC-DQN, our MPRC-DQN has another advantage of faster convergence. The learning curves of our MPRC-DQN and RC-DQN agents on various games are in Appendix B.</p>
<p>Finally, our approaches, overall, achieve the new state-of-the-art on 25 games (i.e., a winning rate of $76 \%$ ), giving a significant advance in the field of IF game playing.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Competitors</th>
<th style="text-align: center;">Win</th>
<th style="text-align: center;">Draw</th>
<th style="text-align: center;">Lose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. TDQN</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. DRRN</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. KG-A2C</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 4: Pairwise comparison between our MPRC-DQN versus each baseline.</p>
<p>Pairwise Competition. To better understand the performance difference between our approach and each of the baselines, we adopt a direct one-to-one comparison metric based on the results from Table 2. Our approach has a high winning rate when competing with any of the baselines, summarized in Table 4. All the baselines have a rare chance to beat us on games. DRRN gives a higher chance of draw-games when competing with ours.</p>
<p>Human-Machine Gap. We additionally compare IF gameplay agents to human players to better understand the improvement significance and the potential improvement upper-bound. We measure each agent's game progress as the macro-average of the normalized agent-to-human game score ratios, capped at $100 \%$. The progress of our MPRCDQN is $28.5 \%$, while the best performing baseline DRRN is $17.8 \%$, showing that our agent's improvement is significant even in the realm of human players. Nevertheless, there is a vast gap between the learning agents and human players. The gap indicates IF games can be a good benchmark for the development of natural language understanding techniques.</p>
<p>Difficulty Levels of Games. Jericho categorizes the supported games into three difficulty levels, namely possible games, difficult games, and extreme games, based on the characteristics of the game dynamics, such as the action space size, the length of the game, and the average number of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves for ablative studies. (left) Model ablative studies on the game Detective. (middle) Model ablative studies on Zork1. (right) Retrieval strategy study on Zork1. Best seen in color.</p>
<p>Steps to receive a non-zero reward. Our approach improves over prior art on seven of the sixteen possible games, seven of the eleven difficult games, and three of the six extreme games in Table 2. It shows that the strategies of our method are generally beneficial for any difficulty levels of game dynamics. Table 3 summarizes the characteristics of the seven games in which our method improves the most, i.e., larger than 15% of the game progress in the first 100 steps.4 First, these mostly improved games have medium action space sizes, and it is an advantageous setting for our methods where modeling the template-object-observation interactions is effective. Second, our approach improves most on games with a reasonably high degree of reward sparsity, such as <em>karn</em>, <em>spirit</em>, and <em>zork3</em>, indicating that our RC-based value function formulation helps in optimization and mitigates the reward sparsity. Finally, we remark that these game difficulty levels are not directly categorized based on natural language-related characteristics, such as text comprehension and puzzle-solving difficulties. Future studies on additional game categories based on those natural language-related characteristics would shed light on related improvements.</p>
<h3>4.3 Ablative Studies</h3>
<p><strong>RC-model Design.</strong> The overall results show that our RC-model plays a critical role in performance improvement. We compare our RC-model to some alternative models as ablative studies. We consider three alternatives, namely (1) our RC-model without the self-attention component (w/o self-att), (2) without the argumentspecific embedding (w/o arg-emb) and (3) our RC-model with Transformer-based block encoder (RC-Trans) following QANet (Yu et al., 2018). Detailed architecture is in Appendix A.</p>
<p>The learning curves for different RC-models are in Figure 4 (left/middle). The RC-models without either self-attention or argument-specific embedding degenerate, and the argument-specific embedding has a greater impact. The Transformer-based encoder block sometimes learns faster than Bi-GRU at the early learning stage. It achieved a comparable final performance, even with much greater computational resource requirements.</p>
<p><strong>Retrieval Strategy.</strong> We compare with history retrieval strategies with different history sizes (K) and pure recency-based strategies (i.e., taking the latest K observations as history, denoted as w/o rec). The learning curves of different strategies are in Figure 4 (right). In general, the impact of history window size is highly game-dependent, but the pure recency based ones do not differ significantly from RC-DQN at the beginning of learning. The issues of pure recency based strategy are: (1) limited additional information about objects provided by successive observations; and (2) higher variance of retrieved observations due to policy changes.</p>
<h3>5 Conclusion</h3>
<p>We formulate the general IF game playing as MPRC tasks, enabling an MPRC-style solution to efficiently address the key IF game challenges on the huge combinatorial action space and the partial observability in a unified framework. Our approaches achieved significant improvement over the previous state-of-the-art on both game scores and training data efficiency. Our formulation also bridges broader NLU/RC techniques to address other critical challenges in IF games for future work, e.g., common-sense reasoning, novelty-driven exploration, and multi-hop inference.</p>
<h3>Acknowledgments</h3>
<p>We would like to thank Matthew Hausknecht for helpful discussions on the Jericho environments.</p>
<p><sup>4</sup>We ignore <em>ztuu</em> due to the infinite reward loops.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Ct, Mikul Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. 2020. Learning dynamic knowledge graphs to generalize on textbased games. arXiv preprint arXiv:2002.09127.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. arXiv, pages arXiv2001.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2019. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879 .</p>
<p>Kyunghyun Cho, Bart Van Merrinboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</p>
<p>Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2018. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pages 41-75. Springer.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. 2019. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of ACL 2019.</p>
<p>Ameya Godbole, Dilip Kavarthapu, Rajarshi Das, Zhiyu Gong, Abhishek Singhal, Hamed Zamani, Mo Yu, Tian Gao, Xiaoxiao Guo, Manzil Zaheer, et al. 2019. Multi-step entity-centric information retrieval for multi-hop question answering. arXiv preprint arXiv:1909.07598.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Ct, and Xingdi Yuan. 2019a. Interactive fiction games: A colossal adventure. arXiv preprint arXiv:1909.05398.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019b. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.</p>
<p>Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17361745 .</p>
<p>Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, page 3 .</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard em approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 28442857.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533.</p>
<p>Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su. 2020. Frustratingly hard evidence retrieval for qa over books. In Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events, pages 108-113.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227-2237.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension.</p>
<p>Shuohang Wang and Jing Jiang. 2016. Machine comprehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018. R 3: Reinforced ranker-reader for open-domain question answering. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2017. Evidence aggregation for answer re-ranking in open-domain question answering. arXiv preprint arXiv:1711.05116.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://spacy.io&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
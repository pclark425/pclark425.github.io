<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1936 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1936</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1936</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-276617660</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.19247v2.pdf" target="_blank">ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding</a></p>
                <p><strong>Paper Abstract:</strong> Embodied intelligence requires agents to interact with 3D environments in real time based on language instructions. A foundational task in this domain is ego-centric 3D visual grounding. However, the point clouds rendered from RGB-D images retain a large amount of redundant background data and inherent noise, both of which can interfere with the manifold structure of the target regions. Existing point cloud enhancement methods often require a tedious process to improve the manifold, which is not suitable for real-time tasks. We propose Proxy Transformation suitable for multimodal task to efficiently improve the point cloud manifold. Our method first leverages Deformable Point Clustering to identify the point cloud sub-manifolds in target regions. Then, we propose a Proxy Attention module that utilizes multimodal proxies to guide point cloud transformation. Built upon Proxy Attention, we design a submanifold transformation generation module where textual information globally guides translation vectors for different submanifolds, optimizing relative spatial relationships of target regions. Simultaneously, image information guides linear transformations within each submanifold, refining the local point cloud manifold of target regions. Extensive experiments demonstrate that Proxy Transformation significantly outperforms all existing methods, achieving an impressive improvement of 7.49% on easy targets and 4.60% on hard targets, while reducing the computational overhead of attention blocks by 40.6%. These results establish a new SOTA in ego-centric 3D visual grounding, showcasing the effectiveness and robustness of our approach.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1936.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1936.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProxyTransformation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proxy Transformation (Preshaping Point Cloud Manifold With Proxy Attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal, realtime point-cloud submanifold enhancement method for ego-centric 3D visual grounding that uses deformable clustering plus proxy-based cross-modal attention to compute per-cluster linear transforms and translations guided by image and text proxies, improving downstream grounding accuracy while reducing attention FLOPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProxyTransformation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: (1) convert multi-view RGB-D to a pooled sparse 3D point cloud; (2) initialize a uniform 3D grid of reference cluster centers; (3) deform these via a learned 3D offset network (Deformable Point Clustering) to obtain submanifolds; (4) for each cluster, run a Proxy Block built on Proxy Attention that takes cluster point features and proxy tokens (text or pooled multi-view image features) to produce per-cluster translation vectors (from text proxies) and 3x3 linear transformation matrices (from image proxies); (5) apply the per-cluster M (linear) and T (translation) transforms to the submanifold points to produce an enhanced point cloud; (6) feed the enhanced point cloud plus image features to a DETR-like 3D decoder for 9-DoF bounding box regression. Key properties: multimodal cross-modal fusion via proxy attention, linear computational complexity in attention (through proxy compression + broadcast), grid prior and cluster drop ratio to stabilize training and limit over-transformation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>pretrained image encoders for multi-view features; CLIP encoder used to replace RoBERTa as the language encoder (paper uses pretrained image encoders but does not specify a single exact visual backbone architecture in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Paper states use of pretrained visual encoders; language encoder replaced with CLIP encoder (CLIP is a large image-text model pretrained on large-scale web image-text pairs). Specific image-backbone pretraining dataset(s) are not fully specified in the paper (they follow EmbodiedScan's practice of using pretrained visual encoders).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Multimodal proxy-guided transformation of 3D submanifolds: text features used as proxy tokens to generate global translation vectors that encode relative positional relationships between clusters; pooled multi-view image features used as proxy tokens to generate per-cluster linear transformation matrices that refine local geometry; these proxy tokens are consumed by a Proxy Attention (proxy compression + broadcast) module inside Proxy Blocks to fuse modalities and produce M (3×3) and T (3×1) for each cluster prior to the 3D decoder. After transformation, standard fusion and DETR-like decoding ground language to 3D boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>3D point cloud submanifold (cluster-level) augmented with multi-view image features and tokenized text; downstream processing uses point-level features in a 3D backbone/DETR-like decoder (multi-level: cluster/submanifold level for transformation, point-level for final detection).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 3D coordinates and clusters; uses a uniform 3D grid prior for reference cluster centers, learned deformable offsets, explicit per-cluster 3×3 linear transformation matrices and 3D translation vectors, and predicts 9-DoF bounding boxes (x,y,z,l,w,h,θ,ϕ,ψ).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D visual grounding (egocentric embodied perception / grounding to language prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan benchmark (ego-centric 3D visual grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric multi-view RGB-D views (real scanned indoor scenes aggregated into a sparse aligned point cloud).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AP evaluated under IoU thresholds (IoU > 25% and IoU > 50%); 'Easy' / 'Hard' splits; view-dependent and view-independent metrics are also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On EmbodiedScan validation (mini training set): Overall AP@25 (mAP IoU>25%) reported as 41.08 (ProxyTransformation) with Easy AP@25 = 41.66 and Hard AP@25 = 34.38; authors report improvements of +7.49% (easy) and +4.60% (hard) over the prior strongest baseline (EmbodiedScan). ProxyAttention reduces attention FLOPs by ~40.6% relative to vanilla self-attention in their comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation rows in Table 2 show baseline (no Proxy Transformation) overall AP@25 ~ 36.53 (i.e., when the point-cloud enhancement modules are not applied). With full Proxy Transformation this rises to ~41.08 AP@25, indicating the contribution of the multimodal manifold enhancement.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>+~4.55 mAP (AP@25 overall) when adding Proxy Transformation vs. baseline in their ablation; paper also reports +7.49% (easy) and +4.60% (hard) improvements over previous strongest baseline on official benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper describes replacing a RoBERTa language encoder with a CLIP encoder to improve vision-language alignment; quantitative encoder ablation numbers are not fully detailed for all backbones but the paper explicitly states CLIP replacement improved alignment and final performance. The work uses pretrained visual encoders (unspecified exact backbones) in line with EmbodiedScan.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Identified bottlenecks: (1) ego-centric partial views and sparse sampling (~2% sampled points) that reduce foreground point density and harm manifold fidelity; (2) depth-sensor noise and errors (non-Lambertian surfaces) that corrupt geometric detail and distort object manifolds; (3) large amounts of redundant background points that waste computation and dilute foreground signal; (4) global transformations are infeasible due to varying manifold structure across local regions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper-level analyses and ablations indicate: (a) Hard scenes (>=3 same-category objects) remain more challenging (reported separately in Easy/Hard metrics); (b) early-training instability when offsets are unconstrained — mitigated by a grid prior which adds +2.20% on hard samples; (c) too large or too small cluster drop ratios harm performance (Table 5 shows drop ratio tradeoffs: overly high drop ratio reduces retained submanifolds and performance; overly low drop ratio may over-alter original structure); (d) lack of multimodal guidance (no PT) leads to lower AP (see ablations). The paper does not provide a per-error-type frequency breakdown (e.g., percent failures due to occlusion vs. noise), but it attributes many failures to corrupted manifolds and background redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Design choices to mitigate domain/prior mismatch: avoids reliance on preconstructed full-scene 3D reconstructions (explicitly operates on ego-centric multi-view RGB-D); uses multimodal (text + images) on-the-fly proxies to reshape local manifolds rather than offline scene completion; uses pretrained visual encoders and CLIP-language encoder for better vision-language alignment. No explicit domain-adaptation procedure or fine-grained sim-to-real experiment is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Proxy Attention: two-step attention where small set of proxy tokens (n << N) compress key/value space (Proxy Compression) then proxy features are used as keys/values to broadcast back to original queries (Proxy Broadcast); proxy tokens selected from modalities (text tokens for global translations, pooled multi-view image features for local linear transforms). Afterwards, transformed point cloud features are concatenated/fused into a DETR-like 3D decoder. The fusion is thus a proxy-based cross-modal attention (not simple concatenation).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>ProxyTransformation is trained on the mini training set (~20% of the full EmbodiedScan training data) and nevertheless outperforms the EmbodiedScan baseline trained on the full dataset, indicating improved sample efficiency for grounding via multimodal submanifold enhancement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Multimodal grounding that explicitly shapes 3D submanifolds yields substantial gains in ego-centric 3D visual grounding: use text as global relational guidance (translations) and image features as local geometric guidance (linear transforms) via an efficient proxy-attention mechanism; this both improves accuracy (notable AP@25 / AP@50 gains on EmbodiedScan) and reduces attention FLOPs (~40.6%). Ablations show the grid prior, deformable offsets, and proxy attention each contribute meaningfully; blindly enhancing whole point clouds or using single-modality point enhancement performs worse or is inefficient for realtime embodied tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1936.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1936.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProxyAttention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proxy Attention (Proxy Compression + Proxy Broadcast + Proxy Bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalized efficient attention mechanism that compresses input-key/value redundancy via a small set of proxy tokens and then broadcasts proxy-level context back to original queries, optionally enriched by a learned proxy bias to inject positional/diversity cues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Proxy Attention (module)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-step attention: (1) Proxy Compression – compute attention between proxy tokens and original keys to aggregate original values into compressed proxy-values V_P; (2) Proxy Broadcast – compute attention between original queries and proxies (acting as keys/values) so that compressed proxy context is broadcast to queries. Additionally, Proxy Bias merges two low-dimensional learnable subspaces into the feature space to inject positional/diversity cues. Proxy tokens are chosen from modalities (downsampled points, pooled multi-view image features, textual features).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>n/a (attention module applied to cluster point features and proxy tokens coming from image/text encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Enables cross-modal grounding by letting text/image proxy tokens act as condensed keys/values during attention to cluster point features — text proxies generate translation outputs, image proxies generate linear-transformation outputs for submanifolds.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Operates at cluster/submanifold feature level (compresses N -> n proxies where n << N) and then broadcasts to point/cluster queries.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Works on cluster features that include relative and global coordinates; positional information is explicitly encouraged via Proxy Bias.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Module used for 3D visual grounding (ego-centric).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan (as module within ProxyTransformation pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric RGB-D / 3D point cloud domain (module agnostic to visual domain; used here on multi-view RGB-D-derived point clouds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Module-level comparison uses mAP (AP@25) and measured FLOPs & parameter counts for three transformer blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Compared to vanilla self-attention (3-block stack), Proxy Attention yields higher AP@25 (41.66 overall) while reducing attention FLOPs by ~40.6% (Proxy: 4.97G FLOPs vs Self-Attn: 8.36G FLOPs reported in Table 3). Cross-attention baseline had lower accuracy in their comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>When replacing Proxy Attention with vanilla self-attention or cross-attention, performance falls (vanilla self-attn reported ~40.71 AP@25 overall, cross-attn ~36.87 AP@25 in the table snippet), and FLOPs increase substantially for vanilla self-attn.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Proxy Attention provides a better trade-off: higher accuracy (+~0.95 mAP vs vanilla self-attn in their reported 3-block comparison) and ~40% lower attention FLOPs compared to vanilla self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper notes that proxy-token selection and number (n) matter: n << N for efficiency yet must be sufficiently expressive; improper drop ratios or proxy choices can degrade performance. Detailed per-failure statistics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Proxy Attention is itself a fusion mechanism: it fuses cluster point features with modal proxies (text/image) by proxy-compression then proxy-broadcast two-stage attention. Proxy Bias supplements positional/diversity cues.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Proxy-based attention compresses cross-modal context into a small number of proxies to achieve linear attention complexity while preserving or improving grounding accuracy; selecting textual proxies for global translations and image proxies for local transforms is an effective cross-modal design choice for embodied grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1936.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1936.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeformablePointClustering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deformable Point Clustering (3D offset network + grid prior + cluster drop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cluster generation module that starts from a uniform 3D grid prior and learns per-reference-point offsets (via a small offset network using local and global features) to shift cluster centers into relevant submanifolds; clusters may be dropped by a learned/predefined ratio to control coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deformable Point Clustering</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Initialize reference cluster centers on a uniform 3D grid; for each grid cell gather local point neighborhood; pass concatenated relative coordinates and global coordinates through 1×1 conv + ReLU + pooling to compute local-global features; feed through another 1×1 conv (bias initialized to zero) to produce an offset vector per grid center (bounded by max offset s). Re-cluster around the shifted centers to obtain deformable submanifolds; optionally apply a drop ratio β to remove some clusters (random or FPS sampling) to encourage diversity and limit over-transformation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>operates on point features (PointNet-style per-cluster point features) and uses as input pooled image/text proxies downstream; not a visual image encoder itself.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides the geometric localization (submanifolds) upon which proxy-guided transformations operate; it indirectly supports grounding by selecting which local point neighborhoods will be transformed using multimodal proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Cluster/submanifold level; produces neighborhoods of m points each that are later transformed.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 3D coordinates; grid prior (uniform grid over scene bounding cuboid), learned offsets bounded by parameter s; cluster selection uses ball query or kNN.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D visual grounding pre-processing (submanifold selection) for embodied perception.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan (used inside ProxyTransformation evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric RGB-D / point-cloud domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Improvement in mAP (AP@25) and stability during training; ablations show influence on Easy/Hard splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation table: adding grid prior improves hard-sample AP@25 by +2.20 (grid prior + offsets vs offsets only), full ProxyTransformation with deformable clustering yields overall AP@25 = ~41.08 vs baseline ~36.53 (see Table 2). Exact per-component deltas reported in Tab.2 (grid prior, offsets contribution).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>When deformable offsets are removed (only grid prior or no PT), overall AP@25 drops (Table 2 shows progressively lower AP when grid prior and offsets are ablated). Example: no modules gives ~36.53 AP@25; grid prior + offsets + PT yields ~41.08 AP@25.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Deformable offsets and grid prior together account for several percent AP improvement (grid prior alone reported +2.20% on hard samples; offsets further increase easy-sample AP).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Designed specifically to mitigate sparse sampling and background redundancy by shifting cluster centers to denser, task-relevant submanifolds and dropping irrelevant clusters to save computation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper notes instability in early training if offsets are unconstrained (mitigated via grid prior); inappropriate drop ratios (β too large or too small) degrade performance (Table 5), showing sensitivity to cluster retention rate.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>By not relying on preconstructed global scenes and by learning offsets from local+global features, the approach is designed to be robust to varied scene geometries encountered in ego-centric observations; explicit domain-adaptation techniques are not described.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Outputs clusters which are consumed by Proxy Blocks that fuse cluster features with proxy tokens (text/image) using Proxy Attention; clustering itself is geometric (no language fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Task-adaptive cluster localization (deformable offsets from grid prior) is an effective pre-processing step for vision-language grounding in ego-centric settings: it focuses compute on relevant submanifolds, reduces noise from background points, and improves accuracy when coupled with multimodal proxy transforms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1936.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1936.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedScan (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbodiedScan: A holistic multi-modal 3D perception suite towards embodied AI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior benchmark and baseline pipeline for ego-centric 3D visual grounding that uses multi-view RGB-D scans to produce semantically-enriched partial point clouds and applies a DETR-like decoder for 9-DoF bounding box regression; used as the main strong baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EmbodiedScan baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoupled encoding of RGB images and depth-reconstructed point clouds from egocentric perspectives to obtain semantic and geometric features; projection of 2D features into 3D via camera intrinsics/extrinsics to form a semantically-enriched point cloud; subsequent DETR-like decoding for 9-DoF bounding box regression. EmbodiedScan relies on scene-level multi-view aggregation to improve perception compared to single-view methods.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>pretrained visual encoders for image and depth feature extraction (specific backbones as in EmbodiedScan paper).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>EmbodiedScan uses pretrained visual encoders (the original EmbodiedScan paper details specific pretraining; current paper follows that setup).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Fuse image and reconstructed point cloud features (projected into 3D) into a detector/DETR-like decoder for language-to-3D grounding (decoupled encoding then fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Scene-level semantically-enriched 3D point cloud (multi-view aggregated) and 2D image features projected into 3D.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 3D coordinates, fused multi-view projections, yields 9-DoF bounding boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Ego-centric 3D visual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric multi-view RGB-D scans (real indoor scenes aggregated from ScanNet / 3RScan / Matterport3D sources).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AP@25 and AP@50 (IoU thresholds), view-dependent/independent, easy/hard splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in this paper as the strongest prior baseline: EmbodiedScan (trained on full dataset) reported AP@25 ~ 39.82 (Easy) and other figuress; EmbodiedScan baseline trained on the mini dataset reported Overall AP@25 ~ 33.87 (paper table entries). ProxyTransformation reports surpassing EmbodiedScan by +7.49% (easy) and +4.60% (hard) on official metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper criticizes approaches like EmbodiedScan for overlooking depth noise and sensor errors, and for relying on preconstructed full-scene point clouds which are impractical in realtime ego-centric applications; such approaches can lose fine geometric detail and include redundant background points.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper points out that EmbodiedScan and similar methods can be compromised by depth errors on non-lambertian surfaces and by decreased foreground point density due to sparse sampling; no per-failure frequencies provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>EmbodiedScan leverages multi-view aggregation (scene-level priors) which can be less applicable under strict ego-centric / realtime constraints; ProxyTransformation intentionally avoids full scene reconstructions for robustness under ego-centric conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Decoupled encoding of image and point modalities followed by projection and fusion into a 3D DETR-like decoder (multi-modal fusion at feature and detector stages).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>EmbodiedScan provides a strong multi-view aggregated baseline for ego-centric grounding but can be limited by reliance on fuller scene reconstructions and sensitivity to depth noise and sparse sampling; methods that reshape local manifolds multimodally (like ProxyTransformation) can outperform it even when trained on substantially less data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai <em>(Rating: 2)</em></li>
                <li>Denseg: Alleviating vision-language feature sparsity in multi-view 3d visual grounding <em>(Rating: 2)</em></li>
                <li>Viewrefer: Grasp the multi-view knowledge for 3d visual grounding <em>(Rating: 2)</em></li>
                <li>Multiview transformer for 3d visual grounding <em>(Rating: 2)</em></li>
                <li>Scanrefer: 3d object localization in rgb-d scans using natural language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1936",
    "paper_id": "paper-276617660",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "ProxyTransformation",
            "name_full": "Proxy Transformation (Preshaping Point Cloud Manifold With Proxy Attention)",
            "brief_description": "A multimodal, realtime point-cloud submanifold enhancement method for ego-centric 3D visual grounding that uses deformable clustering plus proxy-based cross-modal attention to compute per-cluster linear transforms and translations guided by image and text proxies, improving downstream grounding accuracy while reducing attention FLOPs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ProxyTransformation",
            "model_description": "Pipeline: (1) convert multi-view RGB-D to a pooled sparse 3D point cloud; (2) initialize a uniform 3D grid of reference cluster centers; (3) deform these via a learned 3D offset network (Deformable Point Clustering) to obtain submanifolds; (4) for each cluster, run a Proxy Block built on Proxy Attention that takes cluster point features and proxy tokens (text or pooled multi-view image features) to produce per-cluster translation vectors (from text proxies) and 3x3 linear transformation matrices (from image proxies); (5) apply the per-cluster M (linear) and T (translation) transforms to the submanifold points to produce an enhanced point cloud; (6) feed the enhanced point cloud plus image features to a DETR-like 3D decoder for 9-DoF bounding box regression. Key properties: multimodal cross-modal fusion via proxy attention, linear computational complexity in attention (through proxy compression + broadcast), grid prior and cluster drop ratio to stabilize training and limit over-transformation.",
            "visual_encoder_type": "pretrained image encoders for multi-view features; CLIP encoder used to replace RoBERTa as the language encoder (paper uses pretrained image encoders but does not specify a single exact visual backbone architecture in the main text).",
            "visual_encoder_pretraining": "Paper states use of pretrained visual encoders; language encoder replaced with CLIP encoder (CLIP is a large image-text model pretrained on large-scale web image-text pairs). Specific image-backbone pretraining dataset(s) are not fully specified in the paper (they follow EmbodiedScan's practice of using pretrained visual encoders).",
            "grounding_mechanism": "Multimodal proxy-guided transformation of 3D submanifolds: text features used as proxy tokens to generate global translation vectors that encode relative positional relationships between clusters; pooled multi-view image features used as proxy tokens to generate per-cluster linear transformation matrices that refine local geometry; these proxy tokens are consumed by a Proxy Attention (proxy compression + broadcast) module inside Proxy Blocks to fuse modalities and produce M (3×3) and T (3×1) for each cluster prior to the 3D decoder. After transformation, standard fusion and DETR-like decoding ground language to 3D boxes.",
            "representation_level": "3D point cloud submanifold (cluster-level) augmented with multi-view image features and tokenized text; downstream processing uses point-level features in a 3D backbone/DETR-like decoder (multi-level: cluster/submanifold level for transformation, point-level for final detection).",
            "spatial_representation": "Explicit 3D coordinates and clusters; uses a uniform 3D grid prior for reference cluster centers, learned deformable offsets, explicit per-cluster 3×3 linear transformation matrices and 3D translation vectors, and predicts 9-DoF bounding boxes (x,y,z,l,w,h,θ,ϕ,ψ).",
            "embodied_task_type": "3D visual grounding (egocentric embodied perception / grounding to language prompts).",
            "embodied_task_name": "EmbodiedScan benchmark (ego-centric 3D visual grounding)",
            "visual_domain": "Egocentric multi-view RGB-D views (real scanned indoor scenes aggregated into a sparse aligned point cloud).",
            "performance_metric": "AP evaluated under IoU thresholds (IoU &gt; 25% and IoU &gt; 50%); 'Easy' / 'Hard' splits; view-dependent and view-independent metrics are also reported.",
            "performance_value": "On EmbodiedScan validation (mini training set): Overall AP@25 (mAP IoU&gt;25%) reported as 41.08 (ProxyTransformation) with Easy AP@25 = 41.66 and Hard AP@25 = 34.38; authors report improvements of +7.49% (easy) and +4.60% (hard) over the prior strongest baseline (EmbodiedScan). ProxyAttention reduces attention FLOPs by ~40.6% relative to vanilla self-attention in their comparisons.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation rows in Table 2 show baseline (no Proxy Transformation) overall AP@25 ~ 36.53 (i.e., when the point-cloud enhancement modules are not applied). With full Proxy Transformation this rises to ~41.08 AP@25, indicating the contribution of the multimodal manifold enhancement.",
            "grounding_improvement": "+~4.55 mAP (AP@25 overall) when adding Proxy Transformation vs. baseline in their ablation; paper also reports +7.49% (easy) and +4.60% (hard) improvements over previous strongest baseline on official benchmark.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper describes replacing a RoBERTa language encoder with a CLIP encoder to improve vision-language alignment; quantitative encoder ablation numbers are not fully detailed for all backbones but the paper explicitly states CLIP replacement improved alignment and final performance. The work uses pretrained visual encoders (unspecified exact backbones) in line with EmbodiedScan.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Identified bottlenecks: (1) ego-centric partial views and sparse sampling (~2% sampled points) that reduce foreground point density and harm manifold fidelity; (2) depth-sensor noise and errors (non-Lambertian surfaces) that corrupt geometric detail and distort object manifolds; (3) large amounts of redundant background points that waste computation and dilute foreground signal; (4) global transformations are infeasible due to varying manifold structure across local regions.",
            "failure_mode_analysis": "Paper-level analyses and ablations indicate: (a) Hard scenes (&gt;=3 same-category objects) remain more challenging (reported separately in Easy/Hard metrics); (b) early-training instability when offsets are unconstrained — mitigated by a grid prior which adds +2.20% on hard samples; (c) too large or too small cluster drop ratios harm performance (Table 5 shows drop ratio tradeoffs: overly high drop ratio reduces retained submanifolds and performance; overly low drop ratio may over-alter original structure); (d) lack of multimodal guidance (no PT) leads to lower AP (see ablations). The paper does not provide a per-error-type frequency breakdown (e.g., percent failures due to occlusion vs. noise), but it attributes many failures to corrupted manifolds and background redundancy.",
            "domain_shift_handling": "Design choices to mitigate domain/prior mismatch: avoids reliance on preconstructed full-scene 3D reconstructions (explicitly operates on ego-centric multi-view RGB-D); uses multimodal (text + images) on-the-fly proxies to reshape local manifolds rather than offline scene completion; uses pretrained visual encoders and CLIP-language encoder for better vision-language alignment. No explicit domain-adaptation procedure or fine-grained sim-to-real experiment is reported.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Proxy Attention: two-step attention where small set of proxy tokens (n &lt;&lt; N) compress key/value space (Proxy Compression) then proxy features are used as keys/values to broadcast back to original queries (Proxy Broadcast); proxy tokens selected from modalities (text tokens for global translations, pooled multi-view image features for local linear transforms). Afterwards, transformed point cloud features are concatenated/fused into a DETR-like 3D decoder. The fusion is thus a proxy-based cross-modal attention (not simple concatenation).",
            "sample_efficiency": "ProxyTransformation is trained on the mini training set (~20% of the full EmbodiedScan training data) and nevertheless outperforms the EmbodiedScan baseline trained on the full dataset, indicating improved sample efficiency for grounding via multimodal submanifold enhancement.",
            "key_findings_grounding": "Multimodal grounding that explicitly shapes 3D submanifolds yields substantial gains in ego-centric 3D visual grounding: use text as global relational guidance (translations) and image features as local geometric guidance (linear transforms) via an efficient proxy-attention mechanism; this both improves accuracy (notable AP@25 / AP@50 gains on EmbodiedScan) and reduces attention FLOPs (~40.6%). Ablations show the grid prior, deformable offsets, and proxy attention each contribute meaningfully; blindly enhancing whole point clouds or using single-modality point enhancement performs worse or is inefficient for realtime embodied tasks.",
            "uuid": "e1936.0"
        },
        {
            "name_short": "ProxyAttention",
            "name_full": "Proxy Attention (Proxy Compression + Proxy Broadcast + Proxy Bias)",
            "brief_description": "A generalized efficient attention mechanism that compresses input-key/value redundancy via a small set of proxy tokens and then broadcasts proxy-level context back to original queries, optionally enriched by a learned proxy bias to inject positional/diversity cues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Proxy Attention (module)",
            "model_description": "Two-step attention: (1) Proxy Compression – compute attention between proxy tokens and original keys to aggregate original values into compressed proxy-values V_P; (2) Proxy Broadcast – compute attention between original queries and proxies (acting as keys/values) so that compressed proxy context is broadcast to queries. Additionally, Proxy Bias merges two low-dimensional learnable subspaces into the feature space to inject positional/diversity cues. Proxy tokens are chosen from modalities (downsampled points, pooled multi-view image features, textual features).",
            "visual_encoder_type": "n/a (attention module applied to cluster point features and proxy tokens coming from image/text encoders)",
            "visual_encoder_pretraining": "n/a",
            "grounding_mechanism": "Enables cross-modal grounding by letting text/image proxy tokens act as condensed keys/values during attention to cluster point features — text proxies generate translation outputs, image proxies generate linear-transformation outputs for submanifolds.",
            "representation_level": "Operates at cluster/submanifold feature level (compresses N -&gt; n proxies where n &lt;&lt; N) and then broadcasts to point/cluster queries.",
            "spatial_representation": "Works on cluster features that include relative and global coordinates; positional information is explicitly encouraged via Proxy Bias.",
            "embodied_task_type": "Module used for 3D visual grounding (ego-centric).",
            "embodied_task_name": "EmbodiedScan (as module within ProxyTransformation pipeline)",
            "visual_domain": "Egocentric RGB-D / 3D point cloud domain (module agnostic to visual domain; used here on multi-view RGB-D-derived point clouds).",
            "performance_metric": "Module-level comparison uses mAP (AP@25) and measured FLOPs & parameter counts for three transformer blocks.",
            "performance_value": "Compared to vanilla self-attention (3-block stack), Proxy Attention yields higher AP@25 (41.66 overall) while reducing attention FLOPs by ~40.6% (Proxy: 4.97G FLOPs vs Self-Attn: 8.36G FLOPs reported in Table 3). Cross-attention baseline had lower accuracy in their comparison.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "When replacing Proxy Attention with vanilla self-attention or cross-attention, performance falls (vanilla self-attn reported ~40.71 AP@25 overall, cross-attn ~36.87 AP@25 in the table snippet), and FLOPs increase substantially for vanilla self-attn.",
            "grounding_improvement": "Proxy Attention provides a better trade-off: higher accuracy (+~0.95 mAP vs vanilla self-attn in their reported 3-block comparison) and ~40% lower attention FLOPs compared to vanilla self-attention.",
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "Paper notes that proxy-token selection and number (n) matter: n &lt;&lt; N for efficiency yet must be sufficiently expressive; improper drop ratios or proxy choices can degrade performance. Detailed per-failure statistics not provided.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Proxy Attention is itself a fusion mechanism: it fuses cluster point features with modal proxies (text/image) by proxy-compression then proxy-broadcast two-stage attention. Proxy Bias supplements positional/diversity cues.",
            "sample_efficiency": null,
            "key_findings_grounding": "Proxy-based attention compresses cross-modal context into a small number of proxies to achieve linear attention complexity while preserving or improving grounding accuracy; selecting textual proxies for global translations and image proxies for local transforms is an effective cross-modal design choice for embodied grounding.",
            "uuid": "e1936.1"
        },
        {
            "name_short": "DeformablePointClustering",
            "name_full": "Deformable Point Clustering (3D offset network + grid prior + cluster drop)",
            "brief_description": "A cluster generation module that starts from a uniform 3D grid prior and learns per-reference-point offsets (via a small offset network using local and global features) to shift cluster centers into relevant submanifolds; clusters may be dropped by a learned/predefined ratio to control coverage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deformable Point Clustering",
            "model_description": "Initialize reference cluster centers on a uniform 3D grid; for each grid cell gather local point neighborhood; pass concatenated relative coordinates and global coordinates through 1×1 conv + ReLU + pooling to compute local-global features; feed through another 1×1 conv (bias initialized to zero) to produce an offset vector per grid center (bounded by max offset s). Re-cluster around the shifted centers to obtain deformable submanifolds; optionally apply a drop ratio β to remove some clusters (random or FPS sampling) to encourage diversity and limit over-transformation.",
            "visual_encoder_type": "operates on point features (PointNet-style per-cluster point features) and uses as input pooled image/text proxies downstream; not a visual image encoder itself.",
            "visual_encoder_pretraining": "n/a",
            "grounding_mechanism": "Provides the geometric localization (submanifolds) upon which proxy-guided transformations operate; it indirectly supports grounding by selecting which local point neighborhoods will be transformed using multimodal proxies.",
            "representation_level": "Cluster/submanifold level; produces neighborhoods of m points each that are later transformed.",
            "spatial_representation": "Explicit 3D coordinates; grid prior (uniform grid over scene bounding cuboid), learned offsets bounded by parameter s; cluster selection uses ball query or kNN.",
            "embodied_task_type": "3D visual grounding pre-processing (submanifold selection) for embodied perception.",
            "embodied_task_name": "EmbodiedScan (used inside ProxyTransformation evaluations)",
            "visual_domain": "Egocentric RGB-D / point-cloud domain.",
            "performance_metric": "Improvement in mAP (AP@25) and stability during training; ablations show influence on Easy/Hard splits.",
            "performance_value": "Ablation table: adding grid prior improves hard-sample AP@25 by +2.20 (grid prior + offsets vs offsets only), full ProxyTransformation with deformable clustering yields overall AP@25 = ~41.08 vs baseline ~36.53 (see Table 2). Exact per-component deltas reported in Tab.2 (grid prior, offsets contribution).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "When deformable offsets are removed (only grid prior or no PT), overall AP@25 drops (Table 2 shows progressively lower AP when grid prior and offsets are ablated). Example: no modules gives ~36.53 AP@25; grid prior + offsets + PT yields ~41.08 AP@25.",
            "grounding_improvement": "Deformable offsets and grid prior together account for several percent AP improvement (grid prior alone reported +2.20% on hard samples; offsets further increase easy-sample AP).",
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Designed specifically to mitigate sparse sampling and background redundancy by shifting cluster centers to denser, task-relevant submanifolds and dropping irrelevant clusters to save computation.",
            "failure_mode_analysis": "Paper notes instability in early training if offsets are unconstrained (mitigated via grid prior); inappropriate drop ratios (β too large or too small) degrade performance (Table 5), showing sensitivity to cluster retention rate.",
            "domain_shift_handling": "By not relying on preconstructed global scenes and by learning offsets from local+global features, the approach is designed to be robust to varied scene geometries encountered in ego-centric observations; explicit domain-adaptation techniques are not described.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Outputs clusters which are consumed by Proxy Blocks that fuse cluster features with proxy tokens (text/image) using Proxy Attention; clustering itself is geometric (no language fusion).",
            "sample_efficiency": null,
            "key_findings_grounding": "Task-adaptive cluster localization (deformable offsets from grid prior) is an effective pre-processing step for vision-language grounding in ego-centric settings: it focuses compute on relevant submanifolds, reduces noise from background points, and improves accuracy when coupled with multimodal proxy transforms.",
            "uuid": "e1936.2"
        },
        {
            "name_short": "EmbodiedScan (baseline)",
            "name_full": "EmbodiedScan: A holistic multi-modal 3D perception suite towards embodied AI",
            "brief_description": "A prior benchmark and baseline pipeline for ego-centric 3D visual grounding that uses multi-view RGB-D scans to produce semantically-enriched partial point clouds and applies a DETR-like decoder for 9-DoF bounding box regression; used as the main strong baseline in this paper.",
            "citation_title": "Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai",
            "mention_or_use": "use",
            "model_name": "EmbodiedScan baseline",
            "model_description": "Decoupled encoding of RGB images and depth-reconstructed point clouds from egocentric perspectives to obtain semantic and geometric features; projection of 2D features into 3D via camera intrinsics/extrinsics to form a semantically-enriched point cloud; subsequent DETR-like decoding for 9-DoF bounding box regression. EmbodiedScan relies on scene-level multi-view aggregation to improve perception compared to single-view methods.",
            "visual_encoder_type": "pretrained visual encoders for image and depth feature extraction (specific backbones as in EmbodiedScan paper).",
            "visual_encoder_pretraining": "EmbodiedScan uses pretrained visual encoders (the original EmbodiedScan paper details specific pretraining; current paper follows that setup).",
            "grounding_mechanism": "Fuse image and reconstructed point cloud features (projected into 3D) into a detector/DETR-like decoder for language-to-3D grounding (decoupled encoding then fusion).",
            "representation_level": "Scene-level semantically-enriched 3D point cloud (multi-view aggregated) and 2D image features projected into 3D.",
            "spatial_representation": "Explicit 3D coordinates, fused multi-view projections, yields 9-DoF bounding boxes.",
            "embodied_task_type": "Ego-centric 3D visual grounding",
            "embodied_task_name": "EmbodiedScan benchmark",
            "visual_domain": "Egocentric multi-view RGB-D scans (real indoor scenes aggregated from ScanNet / 3RScan / Matterport3D sources).",
            "performance_metric": "AP@25 and AP@50 (IoU thresholds), view-dependent/independent, easy/hard splits.",
            "performance_value": "Reported in this paper as the strongest prior baseline: EmbodiedScan (trained on full dataset) reported AP@25 ~ 39.82 (Easy) and other figuress; EmbodiedScan baseline trained on the mini dataset reported Overall AP@25 ~ 33.87 (paper table entries). ProxyTransformation reports surpassing EmbodiedScan by +7.49% (easy) and +4.60% (hard) on official metrics.",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper criticizes approaches like EmbodiedScan for overlooking depth noise and sensor errors, and for relying on preconstructed full-scene point clouds which are impractical in realtime ego-centric applications; such approaches can lose fine geometric detail and include redundant background points.",
            "failure_mode_analysis": "Paper points out that EmbodiedScan and similar methods can be compromised by depth errors on non-lambertian surfaces and by decreased foreground point density due to sparse sampling; no per-failure frequencies provided here.",
            "domain_shift_handling": "EmbodiedScan leverages multi-view aggregation (scene-level priors) which can be less applicable under strict ego-centric / realtime constraints; ProxyTransformation intentionally avoids full scene reconstructions for robustness under ego-centric conditions.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Decoupled encoding of image and point modalities followed by projection and fusion into a 3D DETR-like decoder (multi-modal fusion at feature and detector stages).",
            "sample_efficiency": null,
            "key_findings_grounding": "EmbodiedScan provides a strong multi-view aggregated baseline for ego-centric grounding but can be limited by reliance on fuller scene reconstructions and sensitivity to depth noise and sparse sampling; methods that reshape local manifolds multimodally (like ProxyTransformation) can outperform it even when trained on substantially less data.",
            "uuid": "e1936.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai",
            "rating": 2
        },
        {
            "paper_title": "Denseg: Alleviating vision-language feature sparsity in multi-view 3d visual grounding",
            "rating": 2
        },
        {
            "paper_title": "Viewrefer: Grasp the multi-view knowledge for 3d visual grounding",
            "rating": 2
        },
        {
            "paper_title": "Multiview transformer for 3d visual grounding",
            "rating": 2
        },
        {
            "paper_title": "Scanrefer: 3d object localization in rgb-d scans using natural language",
            "rating": 1
        }
    ],
    "cost": 0.018808,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding
27 Feb 2025</p>
<p>Qihang Peng 
Tsinghua University</p>
<p>Henry Zheng 
Tsinghua University</p>
<p>Gao Huang 
Tsinghua University</p>
<p>ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding
27 Feb 2025B72E3444701B533FC476CFC9359027A0arXiv:2502.19247v2[cs.CV]
Embodied intelligence requires agents to interact with 3D environments in real time based on language instructions.A foundational task in this domain is ego-centric 3D visual grounding.However, the point clouds rendered from RGB-D images retain a large amount of redundant background data and inherent noise, both of which can interfere with the manifold structure of the target regions.Existing point cloud enhancement methods often require a tedious process to improve the manifold, which is not suitable for real-time tasks.We propose Proxy Transformation suitable for multimodal task to efficiently improve the point cloud manifold.Our method first leverages Deformable Point Clustering to identify the point cloud sub-manifolds in target regions.Then, we propose a Proxy Attention module that utilizes multimodal proxies to guide point cloud transformation.Built upon Proxy Attention, we design a submanifold transformation generation module where textual information globally guides translation vectors for different submanifolds, optimizing relative spatial relationships of target regions.Simultaneously, image information guides linear transformations within each submanifold, refining the local point cloud manifold of target regions.Extensive experiments demonstrate that Proxy Transformation significantly outperforms all existing methods, achieving an impressive improvement of 7.49% on easy targets and 4.60% on hard targets, while reducing the computational overhead of attention blocks by 40.6%.These results establish a new SOTA in ego-centric 3D visual grounding, showcasing the effectiveness and robustness of our approach.</p>
<p>Introduction</p>
<p>In recent years, the field of embodied AI has gained increasing attention, spurred by 3D visual grounding benchmarks [1,5,46] that have led to a surge of research [16, 20, * Corresponding Author.23,46,51,57,63].The 3D visual grounding task, which involves locating target objects in real-world 3D environments based on natural language descriptions, is a core perception capability for embodied agents.This ability is crucial in enabling agents to interpret and interact with their surroundings via language, which supports applications in robotics and human-computer interaction.</p>
<p>Despite these advancements, several significant challenges continue to hinder the performance of 3D visual grounding systems.One key challenge is the limited perception of embodied agents, who rely on ego-centric observations from multiple viewpoints, often lacking a holistic, scene-level understanding.While some methods attempt to improve scene-level perception using preconstructed 3D point clouds [16,20,23,51,57,63], following previous 3D perception approaches [24,50], they are impractical in realworld applications where comprehensive scene-level information is not readily available.</p>
<p>In response, EmbodiedScan [46] was introduced, utilizing multi-view ego-centric RGB-D scans to enable models to process scenes directly from sparse, partial viewpoints.Previous methods [46,65,66,70] decouple the encoding of RGB images and depth-reconstructed point clouds from ego-centric perspectives to extract both semantic and geometric information, which are then projected into 3D space using intrinsic and extrinsic matrices to form a semantically enriched point cloud for bounding box regression.While effective, these methods overlook the noise, such as errors in depth on non-lambertian surfaces, captured by the depth sensors [21,22,41] that causes suboptimal performance.Consequently, critical geometric details of corresponding target objects are lost, potentially distorting their original manifold and further compromising the model's grounding performance.Moreover, a substantial portion of the sampled points represents background regions, leading to a diminished density of foreground object points.</p>
<p>In light of such critical challenges, previous works have focused on improving point cloud structure such as point cloud denoising [10,11,45] or completion methods [2, 26, Figure 1.Illustration of our main idea and SOTA results.Ground truth and reference boxes are shown in green and purple respectively.Circular regions represent 3D areas where point cloud enhancement is applied.Black areas indicate regions that do not contribute to grounding performance and would increase unnecessary computation overhead.Traditional single-modality point cloud guidance would enhance these redundant areas.Red regions highlight areas where multimodal-guided point cloud enhancement is efficiently applied.Text modality, containing global relative position information among different critical objects, guides translation vectors for these region, while image modality, with local fine-grained semantic details, guides transformation matrices within each target regions.Our model achieves better results with reduced computation, about which details are in Tabs. 1 and 3. 60,61].However, these methods often require extensive preprocessing the point cloud data, making them unsuitable for real-time ego-centric 3D visual grounding.Moreover, these methods rely on traditional statistical [43] methods or learning-based methods [31,37] to enhance the point cloud structure in single modality, which cannot fully utilize the multimodal information available in our task.</p>
<p>Given the limitations of the above methods in ego-centric 3D visual grounding, we propose Proxy Transformation, which enhances point cloud manifolds before feature learning.This approach effectively reduces redundant computation in background point cloud regions and fully leverages the available multimodal information in current context to optimize the submanifolds of target regions.Notably, our method does not require pre-trained offline reconstruction of scene-level point clouds, making it better suited for realtime ego-centric 3D visual grounding task.</p>
<p>Specifically, to generate transformations for point cloud submanifolds, reference points are first initialized as a uniform 3D grid for different scenes.Motivated by the success of deformable offsets in 2D domain [9,52], a 3D offset network then takes the initial point cloud clusters centered on these reference points as input, producing offsets for each reference cluster center.In this way, these deformable cluster centers shift toward critical regions.Subsequently, they serve as cluster centers in a succeeding stage to select candidate submanifolds on the original point cloud for further transformations.For each submanifold, we employ a novel generalized attention mechanism, Proxy Attention, to learn the corresponding transformation matrixes and translation vectors.Specific proxy tokens can be selected based on task requirements (e.g.downsampling points after pooling, multi-view image features or textual features).</p>
<p>In the context of online point cloud submanifold enhancement, we utilize text and image features as proxy tokens to guide submanifold transformations.In this approach, text information provides global positional relationships among different submanifolds, while image information offers local semantic details within each submanifold, as illustrated in Fig. 1.Leveraging these designs, our method supplies the subsequent 3D backbone with a higher-quality point cloud manifold, thereby enhancing the model's effectiveness and robustness.</p>
<p>In summary, our contributions are as follows.</p>
<p>(1) We propose Proxy Transformation, enabling multimodal point cloud augmentation in the context of 3D visual grounding.</p>
<p>(2) To obtain more desirable submanifolds for target regions, we design deformable point clustering, utilizing a 3D offset network to generate flexible and adaptive deformable clusters suited to diverse scenes.(3) We introduce a generalized proxy attention mechanism, allowing the selection of different proxy tokens based on task requirements, achieving linear computational complexity.(4) Our model significantly outperforms all existing methods, achieving an impressive improvement of 7.49% on easy targets and 4.60% on hard targets, while reducing the computational overhead of attention blocks by 40.6%, establishing a new SOTA in ego-centric 3D visual grounding.</p>
<p>Related Works</p>
<p>3D Visual Grounding.3D visual grounding integrates multimodal data to localize target objects in 3D point clouds and it has gained significant attention in recent years [12,14,47].3D visual grounding methods are divided into one-stage and two-stage architectures.One-stage approaches [15,18,27,30] fuse textual and visual features to generate predictions in a single step, enabling end-toend optimization and faster inference.However, they may struggle with complex scene layouts due to the absence of explicit object proposal refinement.In contrast, two-stage approaches [1,4,16,20,49,58] follow a sequential process: they first utilize pre-trained object detectors [24,49] to generate object proposals, which are subsequently matched with the linguistic input to identify the most likely target.This separation of detection and grounding allows for more precise alignment of visual and textual features, enhancing accuracy, particularly in complex scenes, but at the cost of increased computational overhead and inference time.</p>
<p>Recent studies [4,20,23,38,40] have explored transformer-based structures for 3D visual grounding.BUTD-DETR [23] incorporates outputs from pre-trained detection networks, including predicted bounding boxes and their corresponding class labels, as additional inputs.Multi-View Transformer [20] projects the 3D scene into a multiview space to learn robust feature representations.MiKASA Transformer [4] integrates a scene-aware object encoder and an multi-key-anchor technique, enhancing object recognition accuracy and understanding of spatial relationships.In our paper, we focus on the more challenging one-stage methods with transformer modules.Point Cloud Enhancement.3D point clouds provide rich geometric information and dense semantic details, playing a critical role in various domains [46,54,56,60].However, their inherent sparsity and irregularity limit model performance, and the deficiencies of existing sensors exacerbate issues such as high noise levels, sparsity, and incomplete structures in point clouds [32,68].These challenges make 3D point cloud augmentation a critical and challenging problem [35].Traditional point cloud enhancement methods are often based on interpolation and optimization techniques [43], but their computational and memory overhead limits their applicability to large-scale datasets.Current deep learning-based point cloud augmentation methods can be broadly categorized into three main approaches: point cloud denoising, completion, and upsampling.</p>
<p>Point cloud denoising [6,10,11,45,64] can eliminate noise in the point cloud while approximating and preserving the underlying surface geometry information and semantic details.Point cloud completion focuses mainly on completion for objects [7,26,60,61] and scenes [48,53,55,62].And point cloud upsampling [13,25,28,39,59] aims to improve the resolution of point clouds while maintaining the integrity of the original geometric structure.</p>
<p>However, these methods are still confined to a single point cloud modality and are not well-suited for tasks with high real-time requirements.In the context of ego-centric 3D visual grounding, multimodal information is available.We leverage both textual and visual modalities as guidance to transform point cloud sub-manifolds.This approach enables simultaneous point cloud denoising and densification, providing higher-quality data for subsequent processing.Additionally, our method is designed with efficiency in mind, ensuring it meets the task's real-time requirements.</p>
<p>Methodology</p>
<p>In this section, we will introduce our network design.Firstly, we will give a brief introduction of the entire pipeline and what our module does in Sec.3.1.Then, we introduce our effective yet efficient Proxy Attention in Sec.3.2 and how deformable offsets are generated for reference points in Sec.3.3.Finally, details of Proxy Transformation will be described in Sec.3.4.</p>
<p>Overall Structure</p>
<p>We improve upon the previous SOTA in ego-centric 3D visual grounding [46].Here we simply review the overall structure.First, the input RGB-D images from each view is transformed into a partial point cloud, which is then integrated into a holistic 3D point cloud P ∈ R N ×3 using global alignment matrices, thereby preserving geometric details.Next, multi-scale image and point features
{F s img } S s=1 ∈ R V ×Hs×Ws×Cs , {F s point } S s=1 ∈ R N ×Cs
, and text features T ∈ R L×C are learned by respective backbone.Here, N is the total number of points sampled from all perspective views V , S is the number of scale for the encoded feature maps and L is the length of text sequence.The point features are then projected onto the 2D feature planes using intrinsic and extrinsic matrices to get semantic information.Further, they are concatenated to form sparse fusion features {F s fusion } S s=1 ∈ R N ×2Cs .Finally, they are fed into a DETR-like decoder and then finish 9-degree-offreedom bounding box regression.</p>
<p>However, due to the large number of points in the depthreconstructed point cloud and computational limitations, only a sparse subset (about 2%) is sampled, resulting in a negative influence on the point cloud manifold.Due to the varying manifold structures in different local regions of scene-level point clouds, applying a global transformation to the entire point cloud is infeasible.To address this issue, we introduce Proxy Transformation to modify point cloud submanifolds with Proxy Attention, providing richer geometric information for subsequent feature learning.</p>
<p>Proxy Attention</p>
<p>For simplicity, we use following formulation to represent Softmax attention: where Q, K, V ∈ R N ×C denote query, key and value matrices and σ(•) represents Softmax function.And our proxy attention consists of three components as follows.
O = σ(QK T )V ≜ Attn(Q, K, V )(1)O P = ProxyAttn(Q, K, V, P ) = σ(QP T ) σ(P K T ) V = Attn(Q, P, Attn(P, K, V ) Proxy Compression ) Proxy Broadcast . (2)
where P ∈ R n×C is our proxy tokens.Proxy Compression.Previous works [17,33] have shown significant redundancy in query-key similarity computations within self-attention.To address this, we first compress the attention feature space using proxy tokens, whose number is substantially smaller than N practically.This approach significantly alleviates computational overhead while preserving representational capacity, as shown in Tab. 3. Specifically, we use proxy tokens as queries to compute similarity with initial keys, subsequently aggregating original values to obtain a compressed proxy value representation V P with reduced redundancy.Proxy Broadcast.Subsequently, we treat proxy tokens P and proxy values V P as keys and values for a second softmax operation with the original queries, allowing proxy features to be broadcast globally.This design preserves global contextual representation while reducing complexity to a linear scale.In practice, proxy tokens are selected based on the model's representational needs (e.g.downsampling points after pooing, multi-view image features or textual features).In these contexts, n is much smaller than N .Thus, this approach achieves generalized attention with a linear complexity of O(N nd) relative to the number of input features N , balancing computational efficiency with high representational capacity as shown in Tab. 3. Proxy Bias.Additionally, to compensate for the lack of po-sitional information and the diversity of features, we introduce a novel proxy bias.To minimize parameter overhead, we merge two low-dimensional subspaces into a higherdimensional space through a union operation of linear space.Specifically, we define two sets of low-dimensional learnable parameters, representing two subspaces V c and V r within feature space V that satisfy V = V c ∪ V r .After broadcasting these two parameter sets, they match the original feature dimensionality, enriching the feature space with positional information, and guiding Proxy Attention to focus on diverse regions.</p>
<p>Deformable Point Clustering</p>
<p>To transform submanifolds, it is essential first to obtain suitable clusters.Motivated by [9,52,71], we design a deformable clustering approach to identify optimal cluster center locations, capturing the desired target submanifolds.Grid Prior.As mentioned in Sec.3.1, the randomly sampled sparse point cloud has already lost a significant amount of geometric information.To compensate for this loss, we first generate a 3D uniform grid and use its grid centers as reference points for the initial clustering:
Q ≜ {q (i,j,k) } xs,ys,zs i,j,k=1 ∼ U [C min , C max ], N (i,j,k) ≜ { p i,j,k l } m l=1 = γ ( Q (i,j,k) , {p l } N l=1 ),(3)
where reference points Q are also the initial cluster centers.U is a uniform distribution.C is the 3D coordinate of points.p i,j,k l is a specific point in corresponding cluster N (i,j,k) and γ is clustering function, e.g.ball query or kNN.N and m are respective numbers of point cloud and local cluster.</p>
<p>To simplify, we regard N (i,j,k) as N t , where t ∈ [0, n − 1], where n = x s × y s × z s and x s , y s , z s are predefined as hyperparameter.Experimentally, we find that this approach does not affect performance, but provides greater stability in the training process compared to stochastic sampling.Deformable Offsets.To increase the diversity of submanifold selection, an offset network outputs the offset values for reference points respectively, allowing them to shift toward more relevant surrounding regions based on the current submanifold structure.Considering that each reference point covers a local s ball region (s is the largest value for offset), the offset network is able to percept local features to learn reasonable offsets.In addition, we also provide global information to further enhance offset generation.
qt = q t + Γ offset (N t ), Nt = γ ( qt , { p l } N l=1 ),(4)
where Γ offset (•) is our offset network.qt ∈ Q is a new cluster center and Nt is a new neighborhood.Specifically, for N t , we first concatenate the relative coordinates of sub-points with respect to the center and their global coordinates.It's then passed through 1 × 1 Conv2D and ReLU activation to extract point-wise features.We apply average pooling to obtain high-dimensional local-global features for each cluster, which are then fed into another 1 × 1 convolution to generate the final offsets, as shown in Fig. 2. Notably, we set the bias of the 1×1 convolution to zero to avoid introducing additional perturbations.Finally, we discard some clusters according to a predefined drop ratio β with a certain sampling method(e.g.random sampling or FPS), allowing us to obtain a more diverse neighborhood based on the new cluster centers.</p>
<p>Proxy Transformation</p>
<p>For each submanifold, we leverage the Proxy Block to learn a unique manifold transformation.Any arbitrary transformation in 3D space can be decomposed into a linear transformation and a translation.Then, these transformations are applied to each deformable cluster.Details about the transformation generation module are described below.Proxy Block.Following the standard block design in Transformer architectures [42], we developed a Proxy Block based on proxy attention, aimed at cross-modal learning for subsequent manifold transformations.We denote P 0 and F 0 as a set of proxy tokens and cluster features.And F 0 is extracted by a simplified PointNet [34].Then a transformer block comprises of proxy attention module and feedforward network:
F = F 0 + B P ,(5)Q = F W Q , K = F W K , V = F W V , P = P 0 W P ,(6)O P = F + ProxyAttn(Q, K, V, P ),(7)O = O P + FFN(O P ),(8)
where B P is our proxy bias introduced in Sec.3.2.W Q , W K , W V are are projections for query, key and value.ProxyAttn(•) is a proxy attention module in Eq. (2).FFN(•) is a position-wise feedforward network.</p>
<p>In the following sections, for simplicity, we use
O = ProxyBlock(F, P ),(9)
to represent the basic proxy block (Eqs.( 5) to ( 8)).</p>
<p>Text Guided Translation.In ego-centric 3D visual grounding, the input text contains global relative positional information between the target object and other reference objects, while the remaining parts naturally correspond to background point clouds.Therefore, text features can effectively guide the model in learning translation vectors that establish spatial relationships between various submanifolds, thereby enhancing the global relational information embedded within the point cloud structure.Specifically, we leverage text features as proxies within the Proxy Block, allowing it to generate text-guided point cloud features that ultimately yield accurate and context-aware final translation vectors for more precise grounding results.
F l+1 = ProxyBlock(F l , P T ), l = 0, .., L − 1,(10)T = F L U text ,(11)
where P T are text features F text ∈ R S×C .L is number of layers and U text is a fully connected layer to generate final translation vectors T ∈ R n×3 .Image Guided Transformation.The input multi-view image contains rich fine-grained semantic information, compensating for the contextual loss caused by point cloud downsampling.Multi-view images provide pose information of the target object from various perspectives, guiding the model to learn internal transformations of each subpoints relative to the cluster center.Thus, we use pooled image features as proxies in the Proxy Block to generate reshaped image-guided point cloud features, which in turn yield the linear transformations for each submanifold:
F l+1 = ProxyBlock(F l , P I ), l = 0, .., L − 1,(12)M = F L U image ,(13)
where P I ∈ R V ×C are pooled image features from an additional attention pooling layer.And L is the number of layers and U image is a fully connected layer to generate final transformation matrices M ∈ R n×3×3 .Submanifold Reshape.Once the transformation matrixs, containing fine-grained semantic information from the image, and the translation vectors, encompassing precise global relative information from the text, are obtained, we can apply a Proxy Transformation to each submanifold and finally get a diverse enhanced point cloud manifold.To represent this process, we use concise set operations:
P = M ⊙ P ⊕ T ,(14)
where M and T are sets of transformation matrixs and translation vectors for all clusters.P is a union set of submanifolds of different neighborhoods {N i } n i=1 , namely a subset of original point cloud.⊙ and ⊕ represent elementwise multiplication and addition for sets.</p>
<p>Equivalently, for a specific submanifold:
Pi = P i M T i + T i ,(15)
where
P i ∈ P ∩ R m×3 represent sub-points in N i . M i ∈ M ∩ R 3×3
and T i ∈ T ∩ R 3 are the specific transformation matrix and translation vector for this submanifold.Finally, we replace the corresponding parts of the original point cloud with transformed submanifolds to obtain final enhanced point cloud, consisting of rich diversity in manifold structure, which is then fed into 3D backbone during training, enabling efficient multimodal-guided point cloud manifold enhancements.</p>
<p>Experiments</p>
<p>Dataset and Benchmark.The EmbodiedScan dataset [46] used in our experiments is a large-scale, multi-modal, egocentric resource tailored for comprehensive 3D scene understanding.It comprises 5,185 scene scans from widelyused datasets like ScanNet [8], 3RScan [44], and Matter-port3D [3].This diverse dataset offers a rich foundation for 3D visual grounding, covering a broader range of scenes than previous datasets.The training set includes 801,711 language prompts, while the validation set contains 168,322 prompts, making EmbodiedScan notably larger and more challenging, thus providing a rigorous benchmark for egocentric 3D visual grounding tasks.Experimental Settings.Due to limited resources, we train on the official mini dataset.Despite this, our model outperforms the official baseline trained on the full training set, as shown in Tab. 1.For analysis experiments in Sec.4.2, we also use the mini training and validation sets available through the official EmbodiedScan release [46].</p>
<p>Our reported accuracy uses the official IoU metric, focusing on cases where IoU exceeds 25% and 50%.We also assess model performance on both "Easy" and "Hard" scenes, where a "Hard" scene contains three or more target objects of the same category.The "View-Dependent" and "View-Independent" metrics further test the model's spatial reasoning by evaluating performance with and without perspective-specific descriptions.Implementation Details.This work builds upon the strong baseline from EmbodiedScan for ego-centric 3D visual grounding.We applied several techniques to enhance the original baseline.First, we replace the RoBERTa language encoder [29] with the CLIP encoder [36] to achieve superior alignment between language and vision.Additionally, we incorporate Class-Balanced Grouping and Sampling (CBGS) [67] during pretraining to address data imbalance and enhance detection accuracy across rare and common object categories.We follow EmbodiedScan's approach by using pretrained visual encoders.Finally, we also adopt text augmented by LLM to provide richer semantic context when training, proposed by [65].Moreover, our Proxy Transformation is trained using the AdamW optimizer with a learning rate of 5e-4, weight decay of 5e-4, and a batch size of 48.Training spans 12 epochs, with the learning rate reduced by 0.1 at epochs 8 and 11.</p>
<p>Main Results</p>
<p>We evaluate the 3D visual grounding performance of our proposed method, Proxy Transformation, with results presented in Tab. 1, comparing it against established SOTA methods from the dataset benchmark.In addition, the upper methods with † are trained on the full dataset, while the lower methods are trained on a mini dataset, approximately 20% of the full dataset.As shown in Tab. 1, Proxy Transformation achieves a notable 7.49% and 4.60% improvement over the previous strongest baseline, EmbodiedScan, on the Overall@0.25 and Overall@0.50.Although only trained on the mini dataset, our method even surpasses the baseline trained on the full dataset.Through deformable point clustering, our model focuses on the most crucial target regions in the scene, reducing the extra computation overhead caused by redundant point clouds and improving efficiency.Additionally, a grid prior preserves essential original spatial information, mitigating early training instability.Recognizing that text information provides global positional relationships among different submanifolds and image information offers local semantic details within each submanifold, we designed generalized proxy attention to guide local transformations of the selected point cloud submanifolds.These transformations optimize local point cloud structures for each submanifold, ultimately providing higher-quality data for subsequent feature extraction and fusion.</p>
<p>In summary, Proxy Transformation consistently outperforms the previous state-of-the-art across multiple evaluation metrics, including easy and hard samples as well as view-independent and view-dependent tasks.These consistent improvements across diverse metrics highlight not only the robustness and generalizability of our approach for 3D visual grounding tasks but also its adaptability to complex scenes and dynamic environments, making it an effective solution for advancing multimodal grounding capabilities in real-world embodied AI applications.</p>
<p>Analysis Experiments</p>
<p>Ablation on Proposed Modules.We conducted ablation studies on each module to verify their effectiveness as shown in Tab. 2. To ensure fair comparisons, the baseline is enhanced with several modules described in the implementation details.  offsets effectively guide cluster centers toward target regions.Finally, we incorporate grid prior, which improves training stability and achieves an increase of 2.20% on hard samples.This result shows that the grid prior mitigates the randomness of offsets in the early stages of training, preserving essential scene information for the model.</p>
<p>Ablation on Proxy Attention.We validate the effectiveness and efficiency of proxy attention by comparing the vanilla attention block with the proxy block.The input cluster features are aligned across both blocks.As discussed in Sec.3.2, the number of proxy tokens is significantly smaller than the input sequence length.This reduction in proxy tokens compresses redundancy within the attention module, enhancing computational efficiency, which aligns with our experiment results.As shown in Tab. 3, our approach achieves higher performance than vanilla attention, while reducing the computation overhead by 40.6%.And we have added an additional baseline that applies crossattention as shown in Tab. 3, which demonstrate that our proposed Proxy Attention achieves a better trade-off between accuracy performance and computational efficiency.</p>
<p>Ablation on Main Hyperparameters.Then, we ablate several hyperparameters in deformable point clustering and proxy transformation.As shown in Tab. 4, the size of the grid prior also influences performance by reducing instability during the early training stages, which can result from the randomness of the initial transformations.As shown in Tab.sults.However, experiments show that an overly low drop ratio also degrades performance, as point cloud enhancement should complement rather than overly alter the original structure.Moreover, further ablation studies are given in the supplementary material.</p>
<p>Qualitative Results and Discussion</p>
<p>With our deformable point clustering and Proxy Transformation, the point cloud structure in target regions is optimized, providing higher-quality data for subsequent feature learning and fusion.As shown in Fig. 3, reference objects in these regions are small and difficult to distinguish, but with the enhanced manifold structure, our model effectively captures the relationships between target and reference objects, achieving improved grounding performance.</p>
<p>Conclusion</p>
<p>In this work, we propose Proxy Transformation, a realtime multimodal point cloud enhancement model, to address issues of geometric loss and unexpected noise in 3D points within ego-centric 3D visual grounding.This approach yields a high-quality point cloud structure for feature learning.Extensive experiments show that our model not only outperforms all existing methods but also improves the computational efficiency of attention modules, setting a new SOTA on the EmbodiedScan benchmark.These results demonstrate the effectiveness and robustness of using multimodal information for real-time point cloud enhancement in ego-centric 3D visual grounding, offering fresh insights for advancing embodied AI and enabling human-computer interaction in real-world environments.</p>
<p>Figure 2 .
2
Figure 2. (a) shows the overall framework of Proxy Transforamtion.For simplicity, † indicates that a 2D grid is used to represent the 3D spatial grid, with generated 3D offsets also expressed as 2D vectors for clarity.The Grid Prior spans the entire space, and we illustrate it with only four reference points for clearer visualization.In Proxy Transformation module, M and T are sets of transformation matrixs and translation vectors for all clusters.(b) details the structure of our deformable offset network, and indicates the input and output shapes, where M represents the number of clusters and K represents the number of points per cluster.(c) illustrates the information flow in proxy attention.This module combines with FFN and skip connection in a standard Transformer architecture to form the Proxy Block.</p>
<p>Figure 3 .
3
Figure 3. Visualization of ground truth and predictions.Ground truth boxes are shown in green, baseline in red, and ProxyTransformation's predictions in violet.</p>
<p>Table 1 .
1
AP 25 AP 25 AP 25 AP 25 AP 50 AP 50 AP 50 AP 50 AP 50 Main Results on the official validation set.The table displays accuracy performance considering instances where IoU exceeds 25% and 50% under different circumstance as mentioned in Sec. 4. 'Indep' and 'Dep' mean the targets are view-independent and viewdependent.And † denotes that models are trained on full train dataset.Although our model is trained on mini train dataset (approximately 20% of the full data), but it still surpasses previous methods trained on the full train dataset.
Easy Hard Indep Dep AP 25 ScanRefer  † [5] Method 13.78 9.12 13.44 10.77Overall Easy Hard Indep Dep 12.85 ----Overall -BUTD-DETR  † [23]23.12 18.23 22.47 20.9822.14-----L3Det  † [69]24.01 18.34 23.59 21.2223.07-----EmbodiedScan  †39.82 31.02 40.30 38.4839.1018.79 14.93 18.03 18.7118.48EmbodiedScan [46]33.87 30.49 33.55 33.6133.5914.58 12.41 13.92 14.6514.40DenseG [65]40.17 34.38 38.79 40.1839.7018.52 15.88 17.47 18.7518.31ProxyTransformation 41.66 34.38 41.57 40.8141.0819.43 14.09 18.65 19.1819.00</p>
<p>Table 2 .
2
First, we introduce Proxy Transformation without deformable point clustering, resulting in a Ablation of Proposed Modules.'PT' represents 'Proxy Transformation'.The reported values are mAP for predictions greater than 25% IoU.
Grid Prior Offsets PT Easy Hard Overall37.05 30.6036.53✓ 40.39 32.6039.76✓✓ 40.59 32.1839.91✓✓✓ 41.66 34.3841.08AttnFLOPs #Params Easy Hard OverallSelf8.36G 2.52M40.71 33.6540.14Cross 4.53G 2.42M36.87 28.8136.22Proxy 4.97G 2.71M41.66 34.3841.08</p>
<p>Table 3 .
3
Ablation on Proxy Attention.Here, we compare the vanilla self-attention block and cross-attention block with our Proxy Block.The FLOPs and parameters are computed over 3 transformer blocks with cluster features as input.We use a grid size of 12 and a drop radio of 0.6.The reported values are mAP for predictions greater than 25% IoU.
Grid Size Easy Hard Overallw/o40.59 32.1839.911040.94 32.8140.291241.66 34.3841.081441.11 34.2840.56</p>
<p>Table 4 .
4
Ablation on the size of Grid Prior.The reported values are mAP for predictions greater than 25% IoU.</p>
<p>substantial improvement of 3.23%, which demonstrates the effectiveness and necessity of our multimodal point cloud enhancement.Then, we add deformable offsets, yielding an additional improvement on easy samples, indicating that</p>
<p>5, an excessively high drop ratio reduces model performance, highlighting the effectiveness of our submanifold transformations.A lower drop ratio retains more submanifolds for subsequent Proxy Transformation, enhancing re-
Drop Radio Easy Hard Overall0.540.55 34.1740.030.641.66 34.3841.080.740.67 32.7040.020.839.65 31.0238.950.936.40 27.0235.64</p>
<p>Table 5 .
5
Ablation on Cluster Drop Radio β.A smaller β indicates that a greater number of submanifolds are used for subsequent transformations.The reported values are mAP for predictions greater than 25% IoU.</p>
<p>AcknowledgementThe work is supported in part by the National Natural Science Foundation of China under Grant U24B2017, and Beijing Natural Science Foundation under Grant QY24257.AppendixA. Additional ExperimentsAblation on the number of Proxy Layers.As shown in Tab. 7, we conduct several experiments to determine the optimal layers of our proxy attention block for efficient learning.Based on these findings, we selected the optimal hyperparameters for our experiments.But we can see that even with just a single layer our model can achieve a SOTA performance.This demonstrates that the gain stems from leveraging multi-modal information rather than merely increasing model parameters.Thus, we can use a single-layer as final setup in practice, achieving SOTA results with minimal computational cost.Ablation on s.As discussed when introducing deformable offsets, s represents the maximum value of the offsets.Before generating the grid prior, we scale down the cuboid defined by the maximum and minimum coordinates of the point cloud based on s, ensuring that reference points, when adjusted by deformable offsets, do not move outside the boundaries of the point cloud.As shown in Tab.6, we selected an optimal maximum value for s.An excessively large s results in the reduced preset grid losing essential prior information, while an overly small s prevents reference points from shifting toward more critical target regions, thereby reducing the flexibility of the model.B. Ego-Centric 3D Visual GroundingIn real-world applications, intelligent agents interact with their surroundings without prior knowledge of the entire scene.Instead of relying on pre-reconstructed 3D point clouds or other scene-level priors commonly used in previous studies[19,50], they primarily depend on ego-centric observations, such as multi-view RGB-D images.Following the definition in[46], we formalize the egocentric 3D visual grounding task as follows: Given a natural language query L ∈ R T , along with V RGB-D image views, where I v ∈ R H×W ×3 denotes the RGB image and D v ∈ R H×W represents the depth map for the v-th view, and their corresponding sensor intrinsics, the goal is to predict a 9-degree-of-freedom (9DoF) bounding box B = (x, y, z, l, w, h, θ, ϕ, ψ).In this context, (x, y, z) specify the 3D center coordinates of the target object, (l, w, h) define its dimensions, and (θ, ϕ, ψ) represent its orientation angles.The task is to determine B such that it accurately localizes the object described by L within the scene captured byC. Details about Proxy BiasAs mentioned in the methodology, to compensate for the lack of positional information and the diversity of features, we propose a novel Proxy Bias:where F ∈ R N ×C is the input of Proxy Block and F 0 ∈ R N ×C is our deformable cluster features.B P ∈ R N ×C is our novel proxy bias.Initially, we set three learnable parametersHere, C = S 2 = D 4 .Therefore, our parameters are way less than directly setting B P as a learnable parameter, thus improving our parameter efficiency.We first interpolate B d into B 1 ∈ R N ×S×S , mapping the low-dimensional subspace into a higher-dimensional feature space to enhance feature diversity.Subsequently, we add B c and B r to obtain the final B 2 ∈ R N ×S×S , representing the linear union of two low-dimensional subspaces to form the final high-dimensional space, expressed as V = V 1 ∪ V 2 .Finally, we get B P = (B 1 + B 2 ).reshape(N,C), which can enrich the feature space with positional information and guide ProxyAttention to focus on diverse regions.
Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 202013Proceedings, Part I 16</p>
<p>Poco: Point convolution for surface reconstruction. Alexandre Boulch, Renaud Marlet, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. 2017arXiv preprint</p>
<p>Multi-key-anchor &amp; scene-aware transformer for 3d visual grounding. Chun-Peng Chang, Shaoxiang Wang, Alain Pagani, Didier Stricker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMikasa2024</p>
<p>Scanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu, Chen , Angel X Chang, Matthias Nießner, ECCV. Springer202017</p>
<p>Repcd-net: Feature-aware recurrent point cloud denoising network. International Journal of Computer Vision. Honghua Chen, Zeyong Wei, Xianzhi Li, Yabin Xu, Mingqiang Wei, Jun Wang, 2022130</p>
<p>Learning 3d shape latent for point cloud completion. Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, Tao Mei, IEEE Transactions on Multimedia. 32024</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Deformable convolutional networks. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision201725</p>
<p>Iterativepfn: True iterative point cloud filtering. Dasith De, Silva Edirimuni, Xuequan Lu, Zhiwen Shao, Gang Li, Antonio Robles-Kelly, Ying He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202313</p>
<p>Straightpcf: Straight point cloud filtering. Dasith De, Silva Edirimuni, Xuequan Lu, Gang Li, Lei Wei, Antonio Robles-Kelly, Hongdong Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202413</p>
<p>3d virtual worlds and the metaverse: Current status and future possibilities. John David, N Dionisio, William G Burns Iii, Richard Gilbert, ACM computing surveys (CSUR). 4532013</p>
<p>Point cloud upsampling via cascaded refinement network. Hang Du, Xuejun Yan, Jingjing Wang, Di Xie, Shiliang Pu, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2022</p>
<p>Cityflow-nl: Tracking and retrieval of vehicles at city scale by natural language descriptions. Qi Feng, Vitaly Ablavsky, Stan Sclaroff, arXiv:2101.047412021arXiv preprint</p>
<p>Viewinfer3d: 3d visual grounding based on embodied viewpoint inference. Liang Geng, Jianqin Yin, IEEE Robotics and Automation Letters. 32024</p>
<p>Viewrefer: Grasp the multi-view knowledge for 3d visual grounding. Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li, ICCV. 202313</p>
<p>Agent attention: On the integration of softmax and linear attention. Dongchen Han, Tianzhu Ye, Yizeng Han, Zhuofan Xia, Siyuan Pan, Pengfei Wan, Shiji Song, Gao Huang, arXiv:2312.088742023arXiv preprint</p>
<p>Refmask3d: Languageguided transformer for 3d referring segmentation. Shuting He, Henghui Ding, arXiv:2407.182442024arXiv preprint</p>
<p>Learning fine-grained class-agnostic 3d segmentation without manual labels. Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari, Marc Pollefeys, Shiji Song, Gao Huang, Francis Engelmann, Segment3d, arXiv:2312.17232202312arXiv preprint</p>
<p>Multiview transformer for 3d visual grounding. Shijia Huang, Yilun Chen, Jiaya Jia, Liwei Wang, CVPR. 202213</p>
<p>Robust depth enhancement via polarization prompt fusion tuning. Kei Ikemura, Yiming Huang, Felix Heide, Zhaoxiang Zhang, Qifeng Chen, Chenyang Lei, CVPR. 2024</p>
<p>Robust enhancement of depth images from depth sensors. A B M Islam, Christian Scheel, Renato Pajarola, Oliver Staadt, Comput. Graph. 68</p>
<p>Bottom up top down detection transformers for language grounding in images and point clouds. Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, Katerina Fragkiadaki, European Conference on Computer Vision. Springer202237</p>
<p>Pointgroup: Dual-set point grouping for 3d instance segmentation. Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia, CVPR. 202013</p>
<p>Tpnode: Topology-aware progressive noising and denoising of point clouds towards upsampling. Akash Kumbar, Tejas Anvekar, Amitha Tulasi, Ramesh Ashok Vikrama, Uma Tabib, Mudenagudi, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>A point contextual transformer network for point cloud completion. Expert Systems with Applications. Siyi Leng, Zhenxin Zhang, Liqiang Zhang, 20242493</p>
<p>A real-time cross-modality correlation filtering method for referring expression comprehension. Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, Bo Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Pufa-gan: A frequency-aware generative adversarial network for 3d point cloud upsampling. Hao Liu, Hui Yuan, Junhui Hou, Raouf Hamzaoui, Wei Gao, IEEE Transactions on Image Processing. 3132022</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019</p>
<p>3d-sps: Single-stage 3d visual grounding via referred point progressive selection. Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, Si Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Score-based point cloud denoising. Shitong Luo, Wei Hu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Curvenet: Curvature-based multitask learning deep networks for 3d object recognition. Wanggen Aam Muzahid, Ferdous Wan, Lianyao Sohel, Li Wu, Hou, IEEE/CAA Journal of Automatica Sinica. 862020</p>
<p>Efficient diffusion transformer with step-wise dynamic attention mediators. Yifan Pu, Zhuofan Xia, Jiayi Guo, Dongchen Han, Qixiu Li, Duo Li, Yuhui Yuan, Ji Li, Yizeng Han, Shiji Song, arXiv:2408.057102024arXiv preprint</p>
<p>Pointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Siwen Quan, Junhao Yu, Ziming Nie, Muze Wang, Sijia Feng, Pei An, Jiaqi Yang, arXiv:2411.00857Deep learning for 3d point cloud enhancement: A survey. 2024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. PMLR2021</p>
<p>Pointcleannet: Learning to denoise and remove outliers from dense point clouds. Marie-Julie Rakotosaona, La Vittorio, Paul Barbera, Niloy J Guerrero, Maks Mitra, Ovsjanikov, Computer graphics forum. Wiley Online Library2020</p>
<p>Languagerefer: Spatial-language model for 3d visual grounding. Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox, PMLR, 2022. 3Conference on Robot Learning. </p>
<p>Repkpu: Point cloud upsampling with kernel point representation and deformation. Yi Rong, Haoran Zhou, Kang Xia, Cheng Mei, Jiahao Wang, Tong Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Viewpointaware visual grounding in 3d scenes. Xiangxi Shi, Zhonghua Wu, Stefan Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Direct iterative closest point for real-time visual odometry. Tommi Tykkälä, Cédric Audras, Andrew I Comport, 2011IEEE</p>
<p>Attention is all you need. Ashish Vaswani, arXiv:1706.037622017arXiv preprint</p>
<p>Make it dense: Self-supervised geometric scan completion of sparse 3d lidar scans in large outdoor environments. Ignacio Vizzo, Benedikt Mersch, Rodrigo Marcuzzi, Louis Wiesmann, Jens Behley, Cyrill Stachniss, IEEE Robotics and Automation Letters. 7332022</p>
<p>Rio: 3d object instance relocalization in changing indoor environments. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias Nießner, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Transformer-based point cloud denoising network. Junbo Wang, Ying Li, 2023 4th International Conference on Big Data &amp; Artificial Intelligence &amp; Software Engineering (ICBASE). IEEE202313</p>
<p>Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai. Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, Jiangmiao Pang, CVPR. 2024. 1, 3, 6712</p>
<p>Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang, Wang , Lei Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Semantic complete scene forecasting from a 4d dynamic point cloud sequence. Zifan Wang, Zhuorui Ye, Haoran Wu, Junyu Chen, Li Yi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank, Wang , arXiv:2403.16539Dora: 3d visual grounding with order-aware referring. 2024arXiv preprint</p>
<p>Point transformer v3: Simpler faster stronger. Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao, CVPR. 2024112</p>
<p>Eda: Explicit text-decoupling and dense alignment for 3d visual grounding. Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, Jian Zhang, CVPR. 2023</p>
<p>Vision transformer with deformable attention. Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, Gao Huang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202225</p>
<p>Scpnet: Semantic scene completion on point cloud. Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023</p>
<p>Multisensor fusion and cooperative perception for autonomous driving: A review. Chao Xiang, Chen Feng, Xiaopo Xie, Botian Shi, Hao Lu, Yisheng Lv, Mingchuan Yang, Zhendong Niu, IEEE Intelligent Transportation Systems Magazine. 32023</p>
<p>Casfusionnet: A cascaded network for point cloud semantic scene completion by dense feature fusion. Jinfeng Xu, Xianzhi Li, Yuan Tang, Qiao Yu, Yixue Hao, Long Hu, Min Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023</p>
<p>Ransacs for 3d rigid registration: A comparative evaluation. Jiaqi Yang, Zhiqiang Huang, Siwen Quan, Zhiguo Cao, Yanning Zhang, IEEE/CAA Journal of Automatica Sinica. 9102022</p>
<p>Exploiting contextual objects and relations for 3d visual grounding. Li Yang, Ziqi Zhang, Zhongang Qi, Yan Xu, Wei Liu, Ying Shan, Bing Li, Weiping Yang, Peng Li, Yan Wang, NeurIPS. 3612024</p>
<p>Dynamic graph attention for referring expression comprehension. Sibei Yang, Guanbin Li, Yizhou Yu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Pu-net: Point cloud upsampling network. Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Pointr: Diverse point cloud completion with geometry-aware transformers. Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, Jie Zhou, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision202123</p>
<p>Walkformer: Point cloud completion via guided walks. Mohang Zhang, Yushi Li, Rong Chen, Yushan Pan, Jia Wang, Yunzhe Wang, Rong Xiang, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision202423</p>
<p>Point cloud scene completion with joint color and semantic estimation from single rgb-d image. Zhaoxuan Zhang, Xiaoguang Han, Bo Dong, Tong Li, Baocai Yin, Xin Yang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4592023</p>
<p>3dvg-transformer: Relation modeling for visual grounding on point clouds. Lichen Zhao, Daigang Cai, Lu Sheng, Dong Xu, ICCV. 2021</p>
<p>From noise addition to denoising: A self-variation capture network for point cloud optimization. Tianming Zhao, Peng Gao, Tian Tian, Jiayi Ma, Jinwen Tian, IEEE Transactions on Visualization and Computer Graphics. 32022</p>
<p>Denseg: Alleviating vision-language feature sparsity in multi-view 3d visual grounding. Henry Zheng, Hao Shi, Yong Xien Chng, Rui Huang, Zanlin Ni, Tianyi Tan, Qihang Peng, Yepeng Weng, Zhongchao Shi, Gao Huang, 202467</p>
<p>Densegrounding: Improving dense language-vision semantics for ego-centric 3d visual grounding. Henry Zheng, Hao Shi, Qihang Peng, Yong Xien Chng, Rui Huang, Yepeng Weng, Gao Huang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Class-balanced grouping and sampling for point cloud 3d object detection. Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, Gang Yu, arXiv:1908.094922019arXiv preprint</p>
<p>Vision based hand gesture recognition using 3d shape context. Chen Zhu, Jianyu Yang, Zhanpeng Shao, Chunping Liu, IEEE/CAA Journal of Automatica Sinica. 892019</p>
<p>Object2scene: Putting objects in context for openvocabulary 3d detection. Chenming Zhu, Wenwei Zhang, Tai Wang, Xihui Liu, Kai Chen, 2023</p>
<p>Scanreason: Empowering 3d visual grounding with reasoning capabilities. Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, Xihui Liu, 2024</p>
<p>Deformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, ICLR, 2021. 5</p>            </div>
        </div>

    </div>
</body>
</html>
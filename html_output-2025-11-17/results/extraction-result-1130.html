<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1130 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1130</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1130</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-273993644</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.08400v1.pdf" target="_blank">BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Autonomous robots collaboratively exploring an unknown environment is still an open problem. The problem has its roots in coordination among non-stationary agents, each with only a partial view of information. The problem is compounded when the multiple robots must completely explore the environment. In this paper, we introduce Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX), a method for collaborative exploration in multi-agent systems which attempts to explore an entire virtual environment. As in the name, BAMAX leverages backtrack assistance to enhance the performance of agents in exploration tasks. To evaluate BAMAX against traditional approaches, we present the results of experiments conducted across multiple hexagonal shaped grids sizes, ranging from 10x10 to 60x60. The results demonstrate that BAMAX outperforms other methods in terms of faster coverage and less backtracking across these environments.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1130.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1130.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAMAX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent DQN-based exploration system that combines curiosity-like reward shaping with an explicit backtracking planner (graph + A*) and centralized training to achieve full coverage of unknown hexagonal grid mazes more quickly and with fewer backtracks than classical search baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BAMAX</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-agent reinforcement learning agent using a single centralized DQN network trained on shared replay buffer experiences from all agents. Observations are decomposed into six image-like sub-states processed by CNN pipelines plus a local-observation MLP; outputs are Q-values over six discrete hex-direction movement actions. Key components: convolutional feature extractors for explored/unexplored/maps/agent positions, a local-observation dense module, aggregation layer, epsilon-greedy action selection during interaction, and a backtrack-assistance module that stores explored connectivity as a graph and runs A* planning to the nearest unvisited node when agents become stuck.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Exploration-driven RL with intrinsic/extrinsic reward shaping (curiosity-like reward for unexplored cells) combined with planning-based backtracking (graph representation + A*). Centralized training with shared replay also adapts policy using multi-agent experience.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agents adapt by using ongoing observations of explored vs unexplored maps, other agents' positions, walls, and local observations to update Q-values via DQN; reward shaping (r_immediate + r_surrounding) encourages actions that increase unexplored coverage. When an agent is trapped in explored regions, the system constructs/uses a graph of known cells and invokes A* to plan a path to the nearest unvisited node (explicit planning-based adaptation). Action selection during learning is governed by an epsilon-greedy policy (exploration/exploitation balance) and experience is pooled across agents via a shared replay buffer to adapt the shared policy.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hexagonal grid mazes (denoted G10, G20, G40, G60)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete 2D hexagonal grid maze, initially unknown to agents (partially observable), contains walls/blocked edges, deterministic transitions assumed (no stochasticity reported), agents have local observations plus global explored/unexplored map images constructed from prior exploration; objective is complete coverage (full exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Grids tested: 10x10, 20x20, 40x40, 60x60 hex-cell mazes (denoted G10, G20, G40, G60); action space size = 6 (hex directions); number of agents = 4; experiments used 100 random generated mazes per size; episode termination when entire grid is explored; state represented by six image-like sub-states (explored, unexplored, agent pos, other agents pos, walls, local observation).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>BAMAX simulation steps to full coverage (collective timesteps across 4 agents): G10 = 51 steps; G20 = 187 steps; G40 = 644 steps; G60 = 1458 steps. Backtrack counts (total across agents): G10 = 17; G20 = 35; G40 = 152; G60 = 327. Reported improvement: on G60 BAMAX achieves full coverage ~38% faster than the next-best method (collaborative DFS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline methods (from Table 2): DFS simulation steps: G10 = 149, G20 = 620, G40 = 2496, G60 >5900; DFS backtracks: G10=27, G20=117, G40=490, G60=803. BFS simulation steps: G10 = 290, G20 >1600, G40 >5900, G60 >6100; BFS backtracks: G10=55, G20=228, G40=844, G60=1022. Collaborative DFS: steps G10=64, G20=276, G40=1061, G60=2383; backtracks G10=8, G20=36, G40=150, G60=345. Collaborative BFS: steps G10=182, G20=710, G40=2902, G60>5900; backtracks G10=17, G20=78, G40=320, G60=712.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via an epsilon-greedy policy (epsilon-greedy exploration during interaction). Intrinsic/extrinsic reward shaping (r_immediate rewarding entering unexplored cells and penalizing collisions; r_surrounding rewarding immediate neighboring unexplored cells) encourages exploration; centralized replay buffer pools experiences to accelerate learning (experience sharing across agents). When exploitation leads to being stuck, deterministic planning (A* on the stored graph) is used to resume exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Depth-First Search (DFS), Breadth-First Search (BFS), Collaborative DFS (distributed DFS extended for multi-agent), Collaborative BFS (extended BFS).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BAMAX outperforms traditional greedy graph-search baselines and their collaborative extensions across multiple hex-grid sizes, achieving substantially fewer collective simulation steps and lower backtrack counts (examples: G60 steps 1458 vs collaborative DFS 2383; backtracks 327 vs 345). The hybrid approach — reward-shaped DQN for directed exploration plus explicit backtracking via stored connectivity + A* — enables faster full coverage and scales better as grid size increases; authors also report that training on G10 generalizes to larger grid sizes, indicating robustness and some transfer capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Authors note BAMAX is currently demonstrated only on hexagonal-cell grids and does not evaluate other cell shapes or heterogeneous grid geometries. No training sample counts, learning curves, or statistical variability measures reported, so sample efficiency and stability are not quantified. The method is not evaluated in stochastic, dynamic, adversarial, or partially cooperative/competitive multi-agent settings; performance on grids with different connectivity statistics or real-world sensor noise is unreported. The paper does not claim or evaluate Bayesian or information-theoretic experimental-design methods, so the adaptation is limited to reward-shaping + deterministic backtracking rather than formal active-learning or Bayesian optimization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Decentralized exploration of a structured environment based on multi-agent deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Towards cognitive exploration through deep reinforcement learning for mobile robots <em>(Rating: 2)</em></li>
                <li>Cooperative multi-agent control using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1130",
    "paper_id": "paper-273993644",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "BAMAX",
            "name_full": "Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning",
            "brief_description": "A multi-agent DQN-based exploration system that combines curiosity-like reward shaping with an explicit backtracking planner (graph + A*) and centralized training to achieve full coverage of unknown hexagonal grid mazes more quickly and with fewer backtracks than classical search baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BAMAX",
            "agent_description": "Multi-agent reinforcement learning agent using a single centralized DQN network trained on shared replay buffer experiences from all agents. Observations are decomposed into six image-like sub-states processed by CNN pipelines plus a local-observation MLP; outputs are Q-values over six discrete hex-direction movement actions. Key components: convolutional feature extractors for explored/unexplored/maps/agent positions, a local-observation dense module, aggregation layer, epsilon-greedy action selection during interaction, and a backtrack-assistance module that stores explored connectivity as a graph and runs A* planning to the nearest unvisited node when agents become stuck.",
            "adaptive_design_method": "Exploration-driven RL with intrinsic/extrinsic reward shaping (curiosity-like reward for unexplored cells) combined with planning-based backtracking (graph representation + A*). Centralized training with shared replay also adapts policy using multi-agent experience.",
            "adaptation_strategy_description": "Agents adapt by using ongoing observations of explored vs unexplored maps, other agents' positions, walls, and local observations to update Q-values via DQN; reward shaping (r_immediate + r_surrounding) encourages actions that increase unexplored coverage. When an agent is trapped in explored regions, the system constructs/uses a graph of known cells and invokes A* to plan a path to the nearest unvisited node (explicit planning-based adaptation). Action selection during learning is governed by an epsilon-greedy policy (exploration/exploitation balance) and experience is pooled across agents via a shared replay buffer to adapt the shared policy.",
            "environment_name": "Hexagonal grid mazes (denoted G10, G20, G40, G60)",
            "environment_characteristics": "Discrete 2D hexagonal grid maze, initially unknown to agents (partially observable), contains walls/blocked edges, deterministic transitions assumed (no stochasticity reported), agents have local observations plus global explored/unexplored map images constructed from prior exploration; objective is complete coverage (full exploration).",
            "environment_complexity": "Grids tested: 10x10, 20x20, 40x40, 60x60 hex-cell mazes (denoted G10, G20, G40, G60); action space size = 6 (hex directions); number of agents = 4; experiments used 100 random generated mazes per size; episode termination when entire grid is explored; state represented by six image-like sub-states (explored, unexplored, agent pos, other agents pos, walls, local observation).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "BAMAX simulation steps to full coverage (collective timesteps across 4 agents): G10 = 51 steps; G20 = 187 steps; G40 = 644 steps; G60 = 1458 steps. Backtrack counts (total across agents): G10 = 17; G20 = 35; G40 = 152; G60 = 327. Reported improvement: on G60 BAMAX achieves full coverage ~38% faster than the next-best method (collaborative DFS).",
            "performance_without_adaptation": "Baseline methods (from Table 2): DFS simulation steps: G10 = 149, G20 = 620, G40 = 2496, G60 &gt;5900; DFS backtracks: G10=27, G20=117, G40=490, G60=803. BFS simulation steps: G10 = 290, G20 &gt;1600, G40 &gt;5900, G60 &gt;6100; BFS backtracks: G10=55, G20=228, G40=844, G60=1022. Collaborative DFS: steps G10=64, G20=276, G40=1061, G60=2383; backtracks G10=8, G20=36, G40=150, G60=345. Collaborative BFS: steps G10=182, G20=710, G40=2902, G60&gt;5900; backtracks G10=17, G20=78, G40=320, G60=712.",
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Balanced via an epsilon-greedy policy (epsilon-greedy exploration during interaction). Intrinsic/extrinsic reward shaping (r_immediate rewarding entering unexplored cells and penalizing collisions; r_surrounding rewarding immediate neighboring unexplored cells) encourages exploration; centralized replay buffer pools experiences to accelerate learning (experience sharing across agents). When exploitation leads to being stuck, deterministic planning (A* on the stored graph) is used to resume exploration.",
            "comparison_methods": "Depth-First Search (DFS), Breadth-First Search (BFS), Collaborative DFS (distributed DFS extended for multi-agent), Collaborative BFS (extended BFS).",
            "key_results": "BAMAX outperforms traditional greedy graph-search baselines and their collaborative extensions across multiple hex-grid sizes, achieving substantially fewer collective simulation steps and lower backtrack counts (examples: G60 steps 1458 vs collaborative DFS 2383; backtracks 327 vs 345). The hybrid approach — reward-shaped DQN for directed exploration plus explicit backtracking via stored connectivity + A* — enables faster full coverage and scales better as grid size increases; authors also report that training on G10 generalizes to larger grid sizes, indicating robustness and some transfer capability.",
            "limitations_or_failures": "Authors note BAMAX is currently demonstrated only on hexagonal-cell grids and does not evaluate other cell shapes or heterogeneous grid geometries. No training sample counts, learning curves, or statistical variability measures reported, so sample efficiency and stability are not quantified. The method is not evaluated in stochastic, dynamic, adversarial, or partially cooperative/competitive multi-agent settings; performance on grids with different connectivity statistics or real-world sensor noise is unreported. The paper does not claim or evaluate Bayesian or information-theoretic experimental-design methods, so the adaptation is limited to reward-shaping + deterministic backtracking rather than formal active-learning or Bayesian optimization strategies.",
            "uuid": "e1130.0",
            "source_info": {
                "paper_title": "BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Decentralized exploration of a structured environment based on multi-agent deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "decentralized_exploration_of_a_structured_environment_based_on_multiagent_deep_reinforcement_learning"
        },
        {
            "paper_title": "Towards cognitive exploration through deep reinforcement learning for mobile robots",
            "rating": 2,
            "sanitized_title": "towards_cognitive_exploration_through_deep_reinforcement_learning_for_mobile_robots"
        },
        {
            "paper_title": "Cooperative multi-agent control using deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "cooperative_multiagent_control_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "towards_optimally_decentralized_multirobot_collision_avoidance_via_deep_reinforcement_learning"
        }
    ],
    "cost": 0.00747175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning
13 Nov 2024</p>
<p>Geetansh Kalra 
Engineering for Research
Thoughtworks India Pvt Ltd. Pune
India</p>
<p>Amit Patel 
Engineering for Research
Thoughtworks India Pvt Ltd. Pune
India</p>
<p>Atul Chaudhari 
Engineering for Research
Thoughtworks India Pvt Ltd. Pune
India</p>
<p>Divye Singh 
Engineering for Research
Thoughtworks India Pvt Ltd. Pune
India</p>
<p>BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning
13 Nov 2024A5EBAD8AED885F50436F3FEFF7E64797arXiv:2411.08400v1[cs.RO]Reinforcement LearningMulti-agent Reinforcement LearningCollaborative Exploration
Autonomous robots collaboratively exploring an unknown environment is still an open problem.The problem has its roots in coordination among non-stationary agents, each with only a partial view of information.The problem is compounded when the multiple robots must completely explore the environment.In this paper, we introduce Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX), a method for collaborative exploration in multi-agent systems which attempts to explore an entire virtual environment.As in the name, BAMAX leverages backtrack assistance to enhance the performance of agents in exploration tasks.To evaluate BAMAX against traditional approaches, we present the results of experiments conducted across multiple hexagonal shaped grids sizes, ranging from 10x10 to 60x60.The results demonstrate that BAMAX outperforms other methods in terms of faster coverage and less backtracking across these environments.</p>
<p>Introduction</p>
<p>Autonomous exploration by robots in unknown environments has diverse applications such as search and rescue, environmental monitoring, and disaster management, and is still an open challenge [7].Moreover, individual robots often struggle with limitations in coverage, efficiency, reliability, resiliency, and adaptability when operating in complex and dynamic environments.</p>
<p>To overcome these challenges, multi-agent collaborative systems have gained attention [19].By leveraging collective knowledge and coordinating actions, these multiple agents can explore the environment more effectively, leading to improved coverage, robustness, and information exchange [17,3].However, collaborative strategies may encounter challenges such as navigating local extrema or overcoming dead ends [14].</p>
<p>In this paper we present an approach called Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX, for short).Multi-agent reinforcement learning is chosen as the foundation for our approach due to its capability to enable agents to learn optimal behaviors through interactions with the environment, leveraging rewards and penalties to enhance decision-making abilities.Our method allows multiple robots to autonomously explore and construct maps within hexagonal mazes, incorporating the crucial capability to backtrack to previously known open positions when encountering obstacles.</p>
<p>This paper has two main contributions as follows.The first one is the guarantee of full exploration.Our approach leverages the collective abilities of multiple robots to facilitate efficient navigation, overcome walls, and achieve complete coverage of the entire grid.The second contribution is an ability to scale to multiple sizes of hexagonal grids.In the rest of the paper, we will discuss how these contributions are made using the BAMAX method.</p>
<p>The section 2 provides an overview of the related works, covering both traditional methods and the application of intelligent agent navigation with Deep Reinforcement Learning (DRL).In section 3, we present our approach, detailing the creation of the environment that addresses our specific problem statement, and the setup of the Reinforcement Learning (RL) framework.Additionally, this section delves into the specifics of our proposed algorithm and the underlying network architecture.The section 4 focuses on the experimentation conducted to evaluate the performance of our method in comparison to other traditional approaches.Thereafter, the section discusses the results obtained from our experiments, offering analysis and comparison of our method with alternative approaches.Finally, the section 5 concludes the paper.</p>
<p>Related Work</p>
<p>The traditional methods for autonomous exploration in unknown environments identify frontier cells as boundaries between known and unknown areas.In an approach discussed in the work [18], each robot maintains a global evidence grid, integrating its local grid with the global map at the frontiers.By summing log odds probabilities, a collaborative and decentralized system is achieved.Another approach [21] utilizes a bidding protocol for assigning regions with high unknown areas to robots.The work discussed in [2] proposed a navigation approach with safety regions and a Next-Best-View algorithm.</p>
<p>While traditional methods have shown success, incorporating high-level knowledge like structural patterns remains a challenge [5].To address this challenge and to improve exploration strategies in unknown environments, recent advancements in Deep Reinforcement Learning (DRL, for short) offer potential.A novel approach introduced in the work [15] leverages raw sensor data from an RGB-D sensor to develop a cognitive exploration strategy through end-to-end DRL.Similarly, the D3QN algorithm discussed in the work [13] enables mobile robots to gradually learn about their environment and autonomously navigate to target destinations using only a camera, avoiding obstacles.</p>
<p>In dynamic environments, [8] employed a deep neural network with long short-term memory and a reinforcement learning algorithm for effective robot navigation.Another study by [16] utilized environment information as input to a neural network and trained agents using the asynchronous deep deterministic policy gradient algorithm.To extract and utilize structural patterns, [20] intro-duced convolutional networks to encode such information and employed the A3C algorithm during training.</p>
<p>Our Approach</p>
<p>In this section, we will discuss the hexagonal grid environment used in our experimentation and the setup of reinforcement learning for our problem.We will also explain the architecture and how data flows in our method, BAMAX.</p>
<p>Environment</p>
<p>The environment used in this paper is a two-dimensional grid maze, G d , with d number of hexagonal cells across height and width.The hexagonal shape was selected for the unit cell in our environment because out of all regular polygons, hexagon is the shape with the highest number of edges which creates a regular tiling in the euclidean plane (other being square and triangle).This would facilitate consistency while also providing a higher complexity in terms of choice of directions available to move from one cell to another.Moreover, to allow for distinct ingress and egress points for each cell, we ensure that more than two sides are left open while creating the maze.An illustration of our hexagonal grid maze (G 10 ) can be seen in figure 1.</p>
<p>Building A Reinforcement Learning Model</p>
<p>In our study, we utilize the DQN algorithm [10] to approximate the Q-function, which represents the optimal action-value function using a deep neural network architecture.By taking the agent's observations as input, the DQN architecture generates Q-values for various actions.These Q-values indicate the expected cumulative rewards that the agent can attain by taking specific actions in a given state.The agent's objective is to select actions with higher Q-values, thereby maximizing long-term rewards and making optimal decisions in the environment (Eq 1).
Q(s, a) = E r + γ max a ′ Q(s ′ , a ′ ) | s, a(1)
State Space To capture the information contained in the environment effectively, we extracted the following six distinct sub-states.</p>
<ol>
<li>s exp : Image representing the area of the map that has been explored.2. s unexp : Image corresponds to the area of the map that is yet to be explored.This decomposition allows us to utilize relevant features from different aspects of the environment to enhance the learning and decision-making capabilities of our method.These sub-states together form a single state and the entire state space, S for our method is represented as 2.</li>
</ol>
<p>State space, S = {s k = (s exp-map , s unexp-map , s agents-own-pos , s other-agents-pos , s walls-map , s local-obs
) | k ∈ {agents}}(2)
Action Space The action space A consists of six distinct actions, denoted by a i available to the agent.The actions correspond to the movement of the agent along the six sides of a hexagon cell.</p>
<p>Reward In our specific problem, we aimed our agent to consider increasing rewards through exploration and incentivize moving toward unexplored regions to maximize overall exploration.In order to capture this, we formulated the reward as comprising of two components as seen in equation 3
r t = r immediate,t + r surrounding,t(3)
r immediate,t is given immediately after the agent takes an action.If the agent enters a valid cell, it receives a reward based on the size of the explored area and the number of unexplored cells.It encourages exploration by providing higher rewards for more unexplored cells.If the agent bumps into another predator or a wall, it receives a penalty.</p>
<p>These reward components aim to guide the agent towards unexplored areas, incentivizing exploration and the acquisition of new information.</p>
<p>Backtrack Assisted Multi-Agent Exploration using</p>
<p>Reinforcement Learning (BAMAX)</p>
<p>In this section, we discuss the architecture of BAMAX as shown in the figure 2. In our approach, we decompose the environment into six parts to enhance representation, as described in section 3.2.CNN Layer utilizes Convolutional Neural Networks (CNNs) [6] to extract meaningful features from the environment observations in BAMAX.Each image input undergoes a specific sequence of convolutional layers, activation functions, and pooling layers, as depicted in the table 1. Local Observation Layer takes in the local observation of the agent and passes it through two of the dense layers with 62 and 32 neurons respectively, with a ReLU activation applied to each of them Aggregation Layer and Action Layer takes in the output of the local observation layer and combines it with the flattened outputs of all the environment observations from the CNN layer component.This combined representation is fed into another dense layer with 32 neurons and a ReLU activation.Finally, the output of this layer is passed through a dense layer with a size equal to the desired action space, using a linear activation function.This final layer produces the Q-values associated with each action, allowing the agent to choose the most advantageous action based on the learned policy.</p>
<p>Backtrack Assistance mechanism addresses situations where agents become trapped or stuck within already explored regions.Its primary purpose is to facilitate the agents' navigation back to the last unvisited node or cell, providing them with an escape route from local extrema or dead ends.As the agents explore the environment, they collectively store the connections between cells using a graph data structure, where each cell is represented as a node and the connections between cells are represented as edges.This representation enables the entire environment to be visualized as a graph, which in turn allows the application of the A* algorithm [4] to guide the agents in backtracking to the nearest unvisited node.By backtracking to unexplored areas, the agents can resume the exploration process and continue discovering new information, enabling them to expand their understanding of the environment and potentially uncover valuable insights.</p>
<p>Training Strategy employs a centralized training approach by utilizing a single network to train all agents.A single replay buffer stores the experiences of all agents and the training samples are sampled from this shared replay buffer during the training process.Further, ϵ-greedy approach is used to maintain the balance between exploration and exploitation.The whole training process is also depicted in algorithm 1</p>
<p>Testing Strategy assigns the same trained model to all the four agents.These agents share their explored map using a graph data structure to effectively collaborate and explore the entire environment together.</p>
<p>Experiments and Results</p>
<p>In this section, we present the experimentation conducted to evaluate the effectiveness of our proposed approach.We describe the setup and metrics used to measure the performance of our method.</p>
<p>Experiments</p>
<p>To evaluate the effectiveness of our method, we conducted experiments on various environment sizes, including G10, G20, G40, and G60.We compared the performance of our approach, BAMAX, with traditional methods such as Depth-First Search (DFS) [11] and Breadth-First Search (BFS) [12].To enable collaborative exploration, we extended the Distributed DFS algorithm [9] and BFS algorithm [1] to create Collaborative DFS and Collaborative BFS, respectively.These adaptations allow multiple agents to explore the grid map together.In our experiments, we generated 100 environments for each size, and in each scenario, we deployed 4 agents to explore the environment.Each agent started from a different random point, while ensuring a common starting point for all 4 agents across different methods, ensuring a fair comparison.We analyzed the results of the experiments based on following two key metrics.</p>
<ol>
<li>Backtrack Count quantifies the frequency of backtracking performed by all agents, counting each transition from one point to another as a single backtrack step.2. Simulation Steps measures the count of timesteps taken collectively by all agents to explore the entire maze.Each simulation step represents a single step taken by all agents.</li>
</ol>
<p>resulting in fewer steps required to cover the entire grid.Figure 3 shows that although all methods stay together at the start, BAMAX soon overtakes and stays in the lead.This is attributed to the fact that BAMAX agents are able to smartly select the exploration path which leads to lesser steps involved in backtracking.Even though collaborative DFS and collaborative BFS also have multiple collaborative agents, by the nature of their underlying exploration mechanism, collaborative DFS may face longer backtracking paths and collaborative BFS may carry out frequent backtracking from very early in the exploration leading to slower performance.This behavior can also be verified from backtracking counts in Table 2. Further, the difference in performance between BAMAX and other methods become even more significant as we increase the grid size.On G 60 grid, BAMAX is able to achieve full grid coverage almost 38% faster compared to next best method (collaborative DFS) as seen in figure 3b.</p>
<p>Conclusion</p>
<p>This paper presents Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX) for collaborative exploration in hexagonal environments.Through extensive experimentation in environments of varying sizes, we compared BAMAX with traditional approaches.The results demonstrated BA-MAX's capability to explore hexagonal grids consistent performance in exploration efficiency.Further, the experimentation also showcases BAMAX's ability to translate its learning from training on a grid size of 10x10 to efficient explore hexagonal grids of different sizes.This showcases good robustness and scalability across different environment sizes.Currently, BAMAX is able to extend its learning to grids of different sizes with only hexagonal cells.As part of future work, we would expand BAMAX's ability to handle grids with different sizes and also shapes.</p>
<p>Fig. 1 :
1
Fig. 1: Hexagonal environment of size G 10 with three agents represented by green, blue, and red hexagons</p>
<p>3 .
3
s agents-pos : Image indicating the location of the agent in the map.4. s other-agents-pos : Image showing the positions of other agents in the map. 5. s walls : Image containing the information about the walls present in the explored area.6. s local-obs : This component provides a local observation for each agent, cap-turing essential information such as the presence of walls, the presence of predators, and whether neighboring cells have already been explored in each direction relative to the agent's current position.</p>
<p>r</p>
<p>immediate,t = max (50, explored cells -unexplored cells) valid cell −20 collision (4) r surrounding,t is calculated based on the number of immediate unexplored cells visible to the agent.It considers the agent's local observation and promotes exploration by providing rewards for unexplored cells within the agent's visibility range.r surrounding,t = x∈neighbouring-cell I(x = unexplored cell)</p>
<p>Fig. 2 :
2
Fig. 2: Architecture of BAMAX</p>
<p>Fig. 3 :
3
Fig. 3: Average performance of all methods on 100 hexagonal grids</p>
<p>Table 1 :
1
Processing Steps for Different Image Inputs
Image TypeLayer TypeKernel SizeKernel No.ActivationExplored ImageConvolution1x132ReLuMax Pooling2x2Unexplored ImageConvolution1x132ReLuMax Pooling2x2Agents Own Position ImageConvolution1x132ReLuMax Pooling2x2Other Agents Position ImageConvolution1x132ReLuMax Pooling2x2Convolution5x532ReLuWalls ImageMax Pooling4x4Convolution5x564ReLuMax Pooling4x4Flatten</p>
<p>Table 2 :
2
Comparison of BAMAX with other methods
Method NameSimulation StepsBacktrack CountG10 G20G40G60 G10 G20 G40 G60DFS149 6202496 &gt;5900 27 117 490 803BFS290 &gt;1600 &gt;5900 &gt;6100 55 228 844 1022Collaborative DFS 64 2761061 2383 8 36 150 345Collaborative BFS 182 7102902 &gt;5900 17 78 320 712BAMAX51 187644 1458 17 35 152 327
Results and DiscussionThe performance of different methods across diverse environments is summarized in Table2.The experimental results clearly demonstrate that our proposed method, BAMAX, outperforms traditional algorithms in terms of achieving faster 100 percent grid coverage with the minimum number of steps.The difference in the number of steps between BAMAX and the traditional algorithms is significant across various grid sizes.This could be attributed to the fact that algorithms like DFS and BFS are greedy approaches, which could prioritize certain paths without considering potentially better alternatives.In contrast, BAMAX understands the importance of effectively collaborating with other agents to optimize exploration.This significant difference arises from BA-MAX's ability to intelligently select the most promising branches to explore,
Direction-optimizing breadth-first search. S Beamer, K Asanovic, D Patterson, SC'12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. IEEE2012</p>
<p>Navigation strategies for exploring indoor environments. H González-Baños, J C Latombe, 10.1177/027836402128964099I. J. Robotic Res. 2110 2002</p>
<p>Cooperative multi-agent control using deep reinforcement learning. J K Gupta, M Egorov, M Kochenderfer, Autonomous Agents and Multiagent Systems. G Sukthankar, J A Rodriguez-Aguilar, ChamSpringer International Publishing2017</p>
<p>A formal basis for the heuristic determination of minimum cost paths. P E Hart, N J Nilsson, B Raphael, 10.1109/TSSC.1968.300136IEEE Transactions on Systems Science and Cybernetics. 421968</p>
<p>Decentralized exploration of a structured environment based on multi-agent deep reinforcement learning. D He, D Feng, H Jia, H Liu, 10.1109/ICPADS51040.2020.00032IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS). 2020. 2020</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. F Pereira, C Burges, L Bottou, K Weinberger, Curran Associates, Inc201225</p>
<p>Current research, key performances and future development of search and rescue robots. J Liu, Y Wang, B Li, S Ma, Frontiers of Mechanical Engineering in China. 24Oct 2007</p>
<p>Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning. P Long, T Fan, X Liao, W Liu, H Zhang, J Pan, 2018</p>
<p>Distributed algorithms for depth-first search. S Makki, G Havas, 10.1016/S0020-0190(96)00141-XS0020-0190(96)00141-XInformation Processing Letters. 6011996</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, 2013</p>
<p>Implementation and analysis of depth-first search (dfs) algorithm for finding the longest path. S E Putri, T Tulus, N Napitupulu, International Seminar on Operational Research (InteriOR). 2011</p>
<p>Breadth first search approach for shortest path solution in cartesian area. R Rahim, D Abdullah, S Nurarif, M Ramadhan, B Anwar, M Dahria, S D Nasution, T M Diansyah, M Khairani, 10.1088/1742-6596/1019/1/012036Journal of Physics: Conference Series. 1019112036jun 2018</p>
<p>Mobile robot navigation based on deep reinforcement learning. X Ruan, D Ren, X Zhu, J Huang, 10.1109/CCDC.2019.8832393Chinese Control And Decision Conference (CCDC). 2019. 2019</p>
<p>Multi-robot systems: From swarms to intelligent automata. A C Schultz, L E Parker, 2002SpringerNetherlands</p>
<p>Towards cognitive exploration through deep reinforcement learning for mobile robots. L Tai, M Liu, 2016</p>
<p>Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation. L Tai, G Paolo, M Liu, 2017</p>
<p>Multi-agent reinforcement learning: Independent vs. cooperative agents. M Tan, Proceedings of the tenth international conference on machine learning. the tenth international conference on machine learning1993</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, 10.1109/CIRA.1997.613851Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'. pp. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'. pp1997</p>
<p>A survey and analysis of multi-robot coordination. Z Yan, N Jouandeau, A A Cherif, 10.5772/57313International Journal of Advanced Robotic Systems. 10123992013</p>
<p>Deep reinforcement learning supervised autonomous exploration in office environments. D Zhu, T Li, D Ho, C Wang, M Q H Meng, 10.1109/ICRA.2018.8463213IEEE International Conference on Robotics and Automation (ICRA). 2018. 2018</p>
<p>Multi-robot exploration controlled by a market economy. R Zlot, A Stentz, M B Dias, S Thayer, Proceedings 2002 IEEE international conference on robotics and automation. Cat. No. 02CH37292. 2002 IEEE international conference on robotics and automationIEEE20023</p>            </div>
        </div>

    </div>
</body>
</html>
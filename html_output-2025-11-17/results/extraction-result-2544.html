<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2544 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2544</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2544</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-f38e2d508689bc0b8168e5436fd64f66970e43a6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f38e2d508689bc0b8168e5436fd64f66970e43a6" target="_blank">ACFix: Guiding LLMs With Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Software Engineering</p>
                <p><strong>Paper TL;DR:</strong> The state-of-the-art GPT-4 model is employed and enhanced with a novel approach called ACFix, which can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality.</p>
                <p><strong>Paper Abstract:</strong> Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of repairing AC vulnerabilities lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence. In this paper, we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFix. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality. To this end, ACFix involves offline and online phases. In the offline phase, ACFix mines a taxonomy of common Role-based Access Control practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 unique samples. In the online phase, ACFix tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. To evaluate ACFix, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFix successfully repaired 94.92% of them, a major improvement compared to the baseline GPT-4 at only 52.54%. We also conducted a human study to understand the value of ACFix’s repairs and their differences from human repairs.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2544.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2544.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACFix-MAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACFix Multi-Agent Debate Dual-Agent Repair Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ACFix integrates a dual-agent Multi-Agent Debate (MAD) architecture (generator + validator) with GPT-4, an RBAC taxonomy, and an AC Context Graph (ACG) to iteratively generate and validate patches that repair access-control vulnerabilities in smart contracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACFix Multi-Agent Debate (dual-agent) framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A domain-specific dual-agent system embedded in the ACFix tool for automatic repair of smart-contract access-control (AC) vulnerabilities. The system uses a Generator agent to parse code (including static slicing to build an AC Context Graph), pick role-permission pairs (from a mined RBAC taxonomy or by proposing new ones), and synthesize candidate patches via GPT-4 with Chain-of-Thought prompting. A Validator agent independently evaluates the candidate patch against the vulnerability description, surrounding code context, and the selected role-permission pair, returning structured feedback (rejection reasons or acceptance). Generator and Validator iterate in a bounded debate loop (max 3 rounds) until the patch is accepted or the iteration limit is reached. Static grammar and semantic checks are performed before/alongside the debate loop to filter invalid patches.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Generator agent: parses contract sources, extracts RBAC elements, constructs the AC Context Graph (ACG) via inter-procedural slicing, queries/consults the RBAC taxonomy, formulates prompts (Q1,Q2,Q3), and synthesizes patch candidates. Validator agent: semantically reviews candidate patches relative to vulnerability description, code context, and selected role-permission pair; checks that the patch (1) mitigates the AC vulnerability, (2) preserves intended contract logic, and (3) does not introduce new security or logical flaws; returns structured feedback (reason codes) to guide generator revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>code understanding and analysis (static slicing & ACG construction), role/permission inference (knowledge-guided mapping to RBAC taxonomy), patch synthesis (implementation), and evaluation/validation (semantic validation via validator and static checks); also supports confirmation of detector-reported vulnerabilities (Q0).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Iterative sequential debate loop (Multi-Agent Debate) orchestrated by a central controller: generator produces a patch, hands it to validator which independently evaluates and returns structured feedback; generator uses feedback to revise and re-generate; loop repeats up to a predefined maximum (3 iterations). The control flow is centralized (the ACFix orchestration pipeline controls prompts, checks, and termination criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language / prompt-based message passing via LLM sessions: prompts Q1-Q4 carry structured content (serialized AC Context Graph, code snippets, RBAC taxonomy entries, and explicit instruction templates). Responses are expected in constrained structured text (e.g., 'Role: XXX, Permission: XXX') or structured natural-language feedback; for GPT-4 the system uses the OpenAI chat API with session history (chat memory) and formatted prompts; static checks use programmatic analyses outside the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Validator returns structured feedback describing acceptance or rejection and explicit reasons (e.g., incorrect role, missing checks, logic conflict). Generator consumes this feedback and refines the patch in subsequent iterations. Static grammar/semantic checks are applied before or in-between agent interactions (e.g., undefined tokens, infeasible invocations, msg.sender check presence). The feedback loop is capped at 3 rounds; the last generated patch is retained if no acceptance occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand after each candidate patch generation; a single generate→validate exchange constitutes one round; iterative up to max 3 rounds; empirical distribution observed: completed after 0/1/2/3 re-attempts for 41 / 68 / 7 / 2 cases (i.e., most cases require at most one re-attempt).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software security / automated program repair — specifically access-control (RBAC) vulnerability repair in Ethereum smart contracts (Solidity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generate Rate: 100% (118/118 generated patches). Success Rate (ACFix end-to-end with validator): 94.92% (112/118 cases judged correct by compilation/exploit/manual metrics). W/o Validator: 87.28% (103/118). Vanilla GPT-4 baseline: 52.54% (62/118, using raw code+description). Iteration statistics: in 118 cases, rounds distribution after generation: 0 reattempts=41 cases, 1 reattempt=68, 2 reattempts=7, 3 reattempts=2. Validator improved final acceptance by 9 cases (from 103→112). Average wall-clock time per case: 30.58 seconds. Average monetary cost per case: ≈ $0.0587 (token-based).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against: (a) W/o Validator ablation: success 87.28% (103/118) — validator improved success to 94.92% (112/118); (b) W/o RBAC ablation: success 68.64% (81/118) showing taxonomy importance; (c) W/o ACG ablation: success 89.83% (106/113) showing context extraction benefit; (d) Vanilla GPT-4: 52.54% (62/118). Compared to SGuard and SmartFix, ACFix substantially outperforms (SGuard success 1/6 patches; SmartFix 7/21 successful patches on dataset subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Quantitative improvements: validator boosts success rate from 87.28% to 94.92% (9 additional correct repairs); rapid convergence (92.37% of cases completed within one attempt). Qualitative benefits: validator suppresses generator hallucinations and overfitting (e.g., incorrect blanket onlyOwner fixes), enforces alignment with the RBAC taxonomy and vulnerability description, and corrects logic/role mismatches through structured feedback—improving patch robustness and adherence to access-control semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Validator can introduce conservative or overfitting guidance (e.g., persuading generator to use 'owner' role) that leads to incorrect repairs — leading to 3 ACFix failures attributed to validator-induced overfitting. Communication overhead: multi-round chat history increases token usage and time/cost compared to single-shot generation. A single case remained failing after three debate rounds. Additionally, reliance on LLM interpretation of structured prompts creates sensitivity to prompt design and potential instability.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Reported ablations: W/o Validator: 103/118 success (87.28%) vs ACFix 112/118 (94.92%). W/o ACG (no AC Context Graph): 106/113 success (89.83%) — ACG contributed +12 cases in some comparisons. W/o RBAC (no RBAC taxonomy): 81/118 success (68.64%) — RBAC taxonomy contributed +37 cases. Vanilla GPT-4 (no preprocessing/taxonomy): 62/113 success (52.54%). These ablations demonstrate the validator's incremental benefit and the large contributions of RBAC taxonomy and ACG context.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper-recommended/practiced settings: base LLM GPT-4 (specifically GPT-4-0613 in experiments) with temperature=0 to suppress randomness; bounded debate loop with maximum 3 generator→validator iterations; perform static grammar/semantic checks before validator review; prefer reusing existing RBAC mechanisms from code when available; use Chain-of-Thought prompts and serialized ACG to focus LLM context. Future-suggested improvements: extend to more specialized agents (adversarial critics, domain oracles) and adaptive debate strategies, but these are proposed future directions rather than evaluated optimal settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ACFix: Guiding LLMs With Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2544.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2544.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD (multi-agent debate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate (MAD) architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A debate-style multi-agent coordination paradigm where generative and critic/validator agents exchange proposals and critiques iteratively to improve output correctness and reduce hallucination; cited as the conceptual foundation for ACFix's dual-agent design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Encouraging divergent thinking in large language models through multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Agent Debate (MAD) architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced conceptual framework in related work that encourages multiple agents to produce divergent proposals and critique each other to boost reasoning quality and factuality. In ACFix this paradigm is instantiated as a two-agent (generator + validator) debate loop used for synthesis and validation of program repair patches.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (conceptually ≥2)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Generally: proposal/generation agents that produce candidate solutions, and critic/validator agents that evaluate, point out errors, and provide feedback; roles and specializations can be tailored to tasks (e.g., generator, adversarial critic, domain oracle).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Generally supports proposal generation and critical evaluation (idea refinement and validation); in ACFix instantiation it covers patch synthesis and semantic validation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Debate-based iterative critique: agents alternate between proposing outputs and critiquing them under central orchestration; coordination may be sequential and turn-based.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Prompt-based natural language exchanges (in ACFix realized via LLM chat prompts and structured template messages).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Critic/validator agents return explicit critiques or structured rejection reasons so proposers can revise outputs; supports iterative refinement loops.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand per proposal (i.e., after each generation), iterated until acceptance or budget limit.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General-purpose paradigm used in LLM-based reasoning/verification tasks; here cited in the context of program repair and reasoning enhancement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated directly in this paper beyond the ACFix instantiation; ACFix reports validator-related gains (see ACFix-MAD entry) demonstrating empirical benefits of debate-style validation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>This paper references MAD literature as background and uses a 2-agent instantiation; direct baseline comparisons are provided via ACFix ablations (W/o Validator) rather than comparisons to other MAD variants.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>When instantiated (as in ACFix), debate reduces hallucination, improves patch correctness, and yields additional accepted repairs (see ACFix experimental gains).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Potential to encourage conservative/overfitting critiques, added token/time overhead from multi-round dialogues, and sensitivity to prompt design; these limitations are observed in ACFix when validator sometimes pushes over-conservative fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>ACFix ablation W/o Validator (removing the debate validation agent) reduced success from 94.92% to 87.28%, demonstrating the practical contribution of a MAD-style validator in this application.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper-level guidance: bound the number of debate iterations (ACFix uses max 3), keep generator randomness low (temperature=0), and combine debate with static checks and domain knowledge (RBAC taxonomy & ACG) for best results; further optimal configurations are suggested as future work (e.g., adding adversarial critics and domain-specific oracles).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ACFix: Guiding LLMs With Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Encouraging divergent thinking in large language models through multi-agent debate <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration <em>(Rating: 1)</em></li>
                <li>The rise and potential of large language model based agents: A survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2544",
    "paper_id": "paper-f38e2d508689bc0b8168e5436fd64f66970e43a6",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "ACFix-MAD",
            "name_full": "ACFix Multi-Agent Debate Dual-Agent Repair Framework",
            "brief_description": "ACFix integrates a dual-agent Multi-Agent Debate (MAD) architecture (generator + validator) with GPT-4, an RBAC taxonomy, and an AC Context Graph (ACG) to iteratively generate and validate patches that repair access-control vulnerabilities in smart contracts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ACFix Multi-Agent Debate (dual-agent) framework",
            "system_description": "A domain-specific dual-agent system embedded in the ACFix tool for automatic repair of smart-contract access-control (AC) vulnerabilities. The system uses a Generator agent to parse code (including static slicing to build an AC Context Graph), pick role-permission pairs (from a mined RBAC taxonomy or by proposing new ones), and synthesize candidate patches via GPT-4 with Chain-of-Thought prompting. A Validator agent independently evaluates the candidate patch against the vulnerability description, surrounding code context, and the selected role-permission pair, returning structured feedback (rejection reasons or acceptance). Generator and Validator iterate in a bounded debate loop (max 3 rounds) until the patch is accepted or the iteration limit is reached. Static grammar and semantic checks are performed before/alongside the debate loop to filter invalid patches.",
            "number_of_agents": "2",
            "agent_specializations": "Generator agent: parses contract sources, extracts RBAC elements, constructs the AC Context Graph (ACG) via inter-procedural slicing, queries/consults the RBAC taxonomy, formulates prompts (Q1,Q2,Q3), and synthesizes patch candidates. Validator agent: semantically reviews candidate patches relative to vulnerability description, code context, and selected role-permission pair; checks that the patch (1) mitigates the AC vulnerability, (2) preserves intended contract logic, and (3) does not introduce new security or logical flaws; returns structured feedback (reason codes) to guide generator revisions.",
            "research_phases_covered": "code understanding and analysis (static slicing & ACG construction), role/permission inference (knowledge-guided mapping to RBAC taxonomy), patch synthesis (implementation), and evaluation/validation (semantic validation via validator and static checks); also supports confirmation of detector-reported vulnerabilities (Q0).",
            "coordination_mechanism": "Iterative sequential debate loop (Multi-Agent Debate) orchestrated by a central controller: generator produces a patch, hands it to validator which independently evaluates and returns structured feedback; generator uses feedback to revise and re-generate; loop repeats up to a predefined maximum (3 iterations). The control flow is centralized (the ACFix orchestration pipeline controls prompts, checks, and termination criteria).",
            "communication_protocol": "Natural-language / prompt-based message passing via LLM sessions: prompts Q1-Q4 carry structured content (serialized AC Context Graph, code snippets, RBAC taxonomy entries, and explicit instruction templates). Responses are expected in constrained structured text (e.g., 'Role: XXX, Permission: XXX') or structured natural-language feedback; for GPT-4 the system uses the OpenAI chat API with session history (chat memory) and formatted prompts; static checks use programmatic analyses outside the LLM.",
            "feedback_mechanism": "Validator returns structured feedback describing acceptance or rejection and explicit reasons (e.g., incorrect role, missing checks, logic conflict). Generator consumes this feedback and refines the patch in subsequent iterations. Static grammar/semantic checks are applied before or in-between agent interactions (e.g., undefined tokens, infeasible invocations, msg.sender check presence). The feedback loop is capped at 3 rounds; the last generated patch is retained if no acceptance occurs.",
            "communication_frequency": "On-demand after each candidate patch generation; a single generate→validate exchange constitutes one round; iterative up to max 3 rounds; empirical distribution observed: completed after 0/1/2/3 re-attempts for 41 / 68 / 7 / 2 cases (i.e., most cases require at most one re-attempt).",
            "task_domain": "Software security / automated program repair — specifically access-control (RBAC) vulnerability repair in Ethereum smart contracts (Solidity).",
            "performance_metrics": "Generate Rate: 100% (118/118 generated patches). Success Rate (ACFix end-to-end with validator): 94.92% (112/118 cases judged correct by compilation/exploit/manual metrics). W/o Validator: 87.28% (103/118). Vanilla GPT-4 baseline: 52.54% (62/118, using raw code+description). Iteration statistics: in 118 cases, rounds distribution after generation: 0 reattempts=41 cases, 1 reattempt=68, 2 reattempts=7, 3 reattempts=2. Validator improved final acceptance by 9 cases (from 103→112). Average wall-clock time per case: 30.58 seconds. Average monetary cost per case: ≈ $0.0587 (token-based).",
            "baseline_comparison": "Compared against: (a) W/o Validator ablation: success 87.28% (103/118) — validator improved success to 94.92% (112/118); (b) W/o RBAC ablation: success 68.64% (81/118) showing taxonomy importance; (c) W/o ACG ablation: success 89.83% (106/113) showing context extraction benefit; (d) Vanilla GPT-4: 52.54% (62/118). Compared to SGuard and SmartFix, ACFix substantially outperforms (SGuard success 1/6 patches; SmartFix 7/21 successful patches on dataset subsets).",
            "coordination_benefits": "Quantitative improvements: validator boosts success rate from 87.28% to 94.92% (9 additional correct repairs); rapid convergence (92.37% of cases completed within one attempt). Qualitative benefits: validator suppresses generator hallucinations and overfitting (e.g., incorrect blanket onlyOwner fixes), enforces alignment with the RBAC taxonomy and vulnerability description, and corrects logic/role mismatches through structured feedback—improving patch robustness and adherence to access-control semantics.",
            "coordination_challenges": "Validator can introduce conservative or overfitting guidance (e.g., persuading generator to use 'owner' role) that leads to incorrect repairs — leading to 3 ACFix failures attributed to validator-induced overfitting. Communication overhead: multi-round chat history increases token usage and time/cost compared to single-shot generation. A single case remained failing after three debate rounds. Additionally, reliance on LLM interpretation of structured prompts creates sensitivity to prompt design and potential instability.",
            "ablation_studies": "Reported ablations: W/o Validator: 103/118 success (87.28%) vs ACFix 112/118 (94.92%). W/o ACG (no AC Context Graph): 106/113 success (89.83%) — ACG contributed +12 cases in some comparisons. W/o RBAC (no RBAC taxonomy): 81/118 success (68.64%) — RBAC taxonomy contributed +37 cases. Vanilla GPT-4 (no preprocessing/taxonomy): 62/113 success (52.54%). These ablations demonstrate the validator's incremental benefit and the large contributions of RBAC taxonomy and ACG context.",
            "optimal_configurations": "Paper-recommended/practiced settings: base LLM GPT-4 (specifically GPT-4-0613 in experiments) with temperature=0 to suppress randomness; bounded debate loop with maximum 3 generator→validator iterations; perform static grammar/semantic checks before validator review; prefer reusing existing RBAC mechanisms from code when available; use Chain-of-Thought prompts and serialized ACG to focus LLM context. Future-suggested improvements: extend to more specialized agents (adversarial critics, domain oracles) and adaptive debate strategies, but these are proposed future directions rather than evaluated optimal settings.",
            "uuid": "e2544.0",
            "source_info": {
                "paper_title": "ACFix: Guiding LLMs With Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MAD (multi-agent debate)",
            "name_full": "Multi-Agent Debate (MAD) architecture",
            "brief_description": "A debate-style multi-agent coordination paradigm where generative and critic/validator agents exchange proposals and critiques iteratively to improve output correctness and reduce hallucination; cited as the conceptual foundation for ACFix's dual-agent design.",
            "citation_title": "Encouraging divergent thinking in large language models through multi-agent debate",
            "mention_or_use": "mention",
            "system_name": "Multi-Agent Debate (MAD) architecture",
            "system_description": "Referenced conceptual framework in related work that encourages multiple agents to produce divergent proposals and critique each other to boost reasoning quality and factuality. In ACFix this paradigm is instantiated as a two-agent (generator + validator) debate loop used for synthesis and validation of program repair patches.",
            "number_of_agents": "variable (conceptually ≥2)",
            "agent_specializations": "Generally: proposal/generation agents that produce candidate solutions, and critic/validator agents that evaluate, point out errors, and provide feedback; roles and specializations can be tailored to tasks (e.g., generator, adversarial critic, domain oracle).",
            "research_phases_covered": "Generally supports proposal generation and critical evaluation (idea refinement and validation); in ACFix instantiation it covers patch synthesis and semantic validation.",
            "coordination_mechanism": "Debate-based iterative critique: agents alternate between proposing outputs and critiquing them under central orchestration; coordination may be sequential and turn-based.",
            "communication_protocol": "Prompt-based natural language exchanges (in ACFix realized via LLM chat prompts and structured template messages).",
            "feedback_mechanism": "Critic/validator agents return explicit critiques or structured rejection reasons so proposers can revise outputs; supports iterative refinement loops.",
            "communication_frequency": "On-demand per proposal (i.e., after each generation), iterated until acceptance or budget limit.",
            "task_domain": "General-purpose paradigm used in LLM-based reasoning/verification tasks; here cited in the context of program repair and reasoning enhancement.",
            "performance_metrics": "Not evaluated directly in this paper beyond the ACFix instantiation; ACFix reports validator-related gains (see ACFix-MAD entry) demonstrating empirical benefits of debate-style validation.",
            "baseline_comparison": "This paper references MAD literature as background and uses a 2-agent instantiation; direct baseline comparisons are provided via ACFix ablations (W/o Validator) rather than comparisons to other MAD variants.",
            "coordination_benefits": "When instantiated (as in ACFix), debate reduces hallucination, improves patch correctness, and yields additional accepted repairs (see ACFix experimental gains).",
            "coordination_challenges": "Potential to encourage conservative/overfitting critiques, added token/time overhead from multi-round dialogues, and sensitivity to prompt design; these limitations are observed in ACFix when validator sometimes pushes over-conservative fixes.",
            "ablation_studies": "ACFix ablation W/o Validator (removing the debate validation agent) reduced success from 94.92% to 87.28%, demonstrating the practical contribution of a MAD-style validator in this application.",
            "optimal_configurations": "Paper-level guidance: bound the number of debate iterations (ACFix uses max 3), keep generator randomness low (temperature=0), and combine debate with static checks and domain knowledge (RBAC taxonomy & ACG) for best results; further optimal configurations are suggested as future work (e.g., adding adversarial critics and domain-specific oracles).",
            "uuid": "e2544.1",
            "source_info": {
                "paper_title": "ACFix: Guiding LLMs With Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate",
            "rating": 2
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration",
            "rating": 1
        },
        {
            "paper_title": "The rise and potential of large language model based agents: A survey",
            "rating": 1
        }
    ],
    "cost": 0.018859749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ACFix: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts</h1>
<p>Lyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, Yang Liu</p>
<h4>Abstract</h4>
<p>Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of repairing AC vulnerabilities lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence.</p>
<p>In this paper, we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFix. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality. To this end, ACFix involves offline and online phases. In the offline phase, ACFix mines a taxonomy of common Role-based Access Control practices from 344,251 onchain contracts, categorizing 49 role-permission pairs from the top 1,000 unique samples. In the online phase, ACFix tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. To evaluate ACFix, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFix successfully repaired $94.92 \%$ of them, a major improvement compared to the baseline GPT-4 at only $52.54 \%$. We also conducted a human study to understand the value of ACFix's repairs and their differences from human repairs.</p>
<p>Keywords—Smart Contract, Software Security, Program Repair.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>I. INTRODUCTION</h2>
<p>Smart contracts, Turing-complete programs executed on blockchain ledgers, implement predefined programmatic logic through transaction-based invocation [1]. With the emergence of decentralized applications such as DeFi [2] and NFTs [3], the use of smart contracts, especially those written in Solidity [4] on the Ethereum blockchain [1], has significantly expanded within the blockchain ecosystem. Nevertheless, these contracts can be susceptible to various security vulnerabilities, including reentrancy [5], integer overflow [6], frontrunning [7], price manipulation [8], etc. Among these, Access Control (AC) vulnerabilities [9] are particularly critical because they directly expose privileged operations to attackers, such as taking over the ownership of the contract or minting more tokens, which often lead to tremendous financial loss, e.g. an infamous attack, Parity [10].</p>
<p>Considering the severe implications associated with access control (AC) vulnerabilities, several automated detection tools have been recently introduced to mitigate these risks, such as Ethainter [11], SPCon [12], AChecker [9], and SoMo [13]. Among these tools, SPCon distinguishes itself by analyzing historical transactions to infer AC policies. In contrast, the other approaches primarily employ taint analysis techniques to trace critical instructions (e.g., selfdestruct) or state variables (e.g., owner), thereby identifying potential scenarios where unauthorized parties might gain access. While these works have thoroughly addressed the detection of AC vulnerabilities, they have not provided concrete guidance or recommendations on how to remediate these issues. Furthermore, although the tools can identify potential vulnerabilities effectively, they lack an explainable reasoning process to justify or clarify the rationale behind their detections.</p>
<p>While detecting AC vulnerabilities has certain information flow patterns, repairing them needs a step further to identify appropriate roles or permissions. As a result, although numerous repair tools for smart contracts have been proposed [14], [15], [16], [17], [18], [19], [20], only a few of them support AC vulnerability repairs. Unfortunately, although certain repair systems-such as Elysium [19] and SmartFix [14]-explicitly</p>
<p>claim support for addressing access control (AC) vulnerabilities, their scope is restricted to fixing only a limited set of common unauthorized operations, such as Re-initialization [21], Suicidal [22], and Low-level Call [23]. However, these predefined AC misuse patterns are insufficient to comprehensively cover the complexity and diversity encountered in real-world smart contract implementations, leading to the inability of successful patch generation.</p>
<p>Existing vulnerability repair tools are often constrained by predefined access control restrictions specific to the contract's owner, rendering template-based repair approaches potentially adequate for standard operational scenarios. However, the current methodological landscape presents a significant limitation in addressing unauthorized privilege escalation across broader contexts, particularly in more complex AC vulnerabilities that require nuanced automated repair mechanisms. For instance, the motivating example presented in §II illustrates that an unprotected deposit function can also lead to unforeseen financial losses for smart contracts. This privilege should be granted to the role Bank rather than the contract's owner for more flexibility.</p>
<p>In general, automatically and appropriately repairing AC vulnerabilities in smart contracts requires human-level intelligence. This is because AC policies in smart contracts are commonly enforced through the Role-Based Access Control (RBAC) [24] mechanism, which requires setting appropriate RBAC roles that align with corresponding privileged operations (referred to as permissions in RBAC terminology). Intuitively, for a repair system to function effectively, it must (i) first achieve a human-level understanding of the functionality embedded within the vulnerable code, (ii) then recognize appropriate RBAC roles based on this understanding, and (iii) finally generate correct patches. Although recent advancements in large language models (LLMs) [25], [26] allow us to utilize state-of-the-art (SOTA) models like GPT-4 [26], accomplishing these three tasks still presents challenges.</p>
<p>Specifically, For task (i), determining AC-related operations from the raw code corpus is even hard for GPT-4, given the substantial noise present within the source code. Compounding this challenge, LLMs are known to have limited attention spans, leading to a loss of focus [27]. To address this issue, we have developed a static slicing algorithm to extract the relevant code context, allowing GPT-4 to focus on it. For task (ii), off-the-shelf LLMs were not inherently trained to recognize RBAC roles and their typical privileged operations, i.e., the mapping of role-permission pairs. Moreover, LLM hallucination [28] could lead to unreliable output. Hence, it becomes essential to build an RBAC taxonomy, derived from common RBAC practices in smart contracts, for the LLM to select from. For task (iii), the patches generated might conflict with pre-existing, inaccurately implemented RBAC mechanisms. Therefore, besides building new RBAC from scratch, we also mine existing RBAC mechanisms from the source code and reuse them in the generated patches. Our evaluation suggests that this strategy is effective for addressing inadequately implemented RBAC. Another issue for task (iii) is that LLMs' randomness could still occasionally divert the LLM from generating correct patches. To address this, we
implemented a Multi-Agent Debate (MAD) mechanism [29] to establish a loop between generator and validator. With such validation, validator can effectively suppress generator's hallucination and ensure the generation of proper patches.</p>
<p>Based on the observations above, we propose a novel approach named ACFIX to enhance the capabilities of the state-of-the-art GPT-4 model in repairing AC vulnerabilities in smart contracts. The key insight is that we can mine common AC practices from major categories of code functionality and use these practices to guide LLMs in fixing code with similar functionality. Specifically, ACFIX first conducts offline mining of common RBAC practices from 344,251 on-chain contracts and builds an RBAC taxonomy consisting of 49 role-permission pairs from the top 1,000 pairs mined. ACFIX then utilizes the mined common RBAC practices as a "knowledge base for AC repair" to guide LLMs in fixing code with similar functionality. To help LLMs understand the functionality of the vulnerable code, ACFIX employs static code slicing to extract AC-related code context, more specifically, an AC context graph (ACG). With this two-fold source of information, ACFIX instructs GPT-4 to follow the Chain-of-Thought (CoT) [30] prompting to identify the proper role-permission pairs. Eventually, ACFIX generates the patch and validates it according to the original vulnerability description.</p>
<p>We conducted evaluations comparing ACFIX with SOTA tools [14], [15] and performed an ablation study to highlight the improvements of individual components ACFIX offers over the baseline GPT-4. To comprehensively evaluate repair tools, we collected and constructed a benchmark dataset consisting of 118 cases from real-world attacks and contracts. To the best of our knowledge, this is the first benchmark dataset specifically for AC vulnerabilities. Our results showed that ACFIX successfully repaired $94.92 \%$ of AC vulnerabilities using appropriate AC mechanisms. The ablation study further revealed that without the enriched context and mined taxonomy supplied by ACFIX, vanilla GPT-4 fixed $52.54 \%$ of vulnerabilities. validator agent further boosted the fixing rate from $87.28 \%$ to $94.92 \%$. Additionally, we analyzed the repair capabilities of tools across various role-permission pairs by category as well as their monetary and time costs.</p>
<p>Furthermore, to understand the value of ACFIX's repairs and how they differ from human repairs, we conducted a humanbased evaluation involving 10 experts who have worked on smart contract auditing for 2-7 years. The results show that ACFIX's repairs are mostly aligned with those of humans and are even finer-grained than those of both senior and junior experts, although in rare cases (3/118), human experts are better at handling open issues based on their knowledge and experience without much guidance. Moreover, around half of the AC fixes are non-trivial to devise by humans, indicating that ACFIX can provide a unique complement to assist human-in-the-loop repair as a copilot.
Contributions. To sum up, our contributions are as follows:</p>
<ul>
<li>We proposed ACFIX, the first tool designed to repair AC vulnerabilities by guiding LLMs to appropriately enforce RBAC mechanisms across a variety of scenarios.</li>
<li>We assembled the first benchmark dataset of 118 AC vulnerabilities, sourced from real-world attacks and contracts,</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="k">function</span><span class="w"> </span><span class="nf">depositFromOtherContract</span><span class="p">(</span><span class="n">uint256</span>
<span class="w">    </span><span class="n">_depositAmount</span><span class="p">,</span><span class="w"> </span><span class="nb">uint8</span><span class="w"> </span><span class="n">_periodId</span><span class="p">,</span>
<span class="w">    </span><span class="n">bool</span><span class="w"> </span><span class="n">isUnlocked</span><span class="p">,</span><span class="w"> </span><span class="n">address</span><span class="w"> </span><span class="n">_from</span>
<span class="p">)</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">//</span><span class="n">vulnerable</span><span class="p">,</span><span class="w"> </span><span class="n">fixed</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">onlyBank</span>
<span class="w">    </span><span class="n">require</span><span class="p">(</span><span class="n">isPoolActive</span><span class="p">,</span><span class="s">&#39;Not running yet&#39;</span><span class="p">);</span>
<span class="w">    </span><span class="n">_autoDeposit</span><span class="p">(</span><span class="n">_depositAmount</span><span class="p">,</span><span class="n">_periodId</span><span class="p">,</span>
<span class="w">        </span><span class="n">isUnlocked</span><span class="p">,</span><span class="n">_from</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p>Fig. 1: An example of smart contract AC vulnerabilities.
based on which, we conducted an extensive evaluation of the effectiveness and efficiency of ACFIX and SOTA tools and LLMs, including an ablation study.</p>
<ul>
<li>We obtained a taxonomy of common RBAC practices, including 49 role-permission pairs summarized from the top 1 K unique samples mined from 344,251 on-chain contracts.</li>
<li>We carried out a human study to understand the value of ACFIX's repairs, yielding new insights into the comparison between LLM-based and human repairs.</li>
</ul>
<h2>II. BACKGROUND AND MOTIVATION</h2>
<h2>A. Background</h2>
<p>Large Language Model. Pre-trained language models such as BERT [31] and GPT [32] have revolutionized the field of natural language processing (NLP) through pre-training on large text corpora. This approach has enabled these models to develop robust, transferable language representations that are highly effective across a wide range of NLP applications. Based on our evaluation of four popular LLMs, including GPT4 [33], GPT-3.5 [34], Mistral [35], and Llama3 [36], in §VII-A, we eventually use GPT-4 as the base mode..
Smart Contract. Smart contracts are self-executing agreements where the terms are encapsulated in executable code and run on blockchains [37]. However, smart contracts may be susceptible to software vulnerabilities, leading to financial risks. If a contract allows for unauthorized ERC20 [38] token transfers, a flaw like improper access control can expose it to risks such as malicious abuse of legitimate functions.
Role-based Access Control (RBAC) [24] is a well-known security paradigm in which permissions are assigned to roles rather than directly to users. Each user belongs to one or more roles to accomplish various access control policies. This approach encapsulates a set of permissions within each role, defining the actions a user can perform. Nowadays, RBAC is recommended as the state-of-the-art security practice for separating the execution of access control policies from the management of business logic in smart contracts, usually through a set of well-defined modifiers [13], [39].</p>
<h2>B. A Motivating Example</h2>
<p>Our approach was motivated by a real-world AC attack on the DeFi application named GYMNetwork[40], [41]. Fig. 1 shows the vulnerable function depositFromOtherContract, the root cause of which is that it is marked as external. Without the
validation by an appropriate modifier, an attacker was able to deposit numerous fake tokens to falsify his token shares in GYMNetwork, leading to a loss of two million USD in 2022.</p>
<p>The patch provided by the original author added a modifier, onlyBank, to ensure that only the vault address can deposit tokens. Since the role Bank had already been defined in the vulnerable contract, RBAC was partially implemented by the author previously. In this case, the vulnerable function could have been repaired with existing RBAC mechanisms from the code context, by onlyBank, in accordance with the plastic surgery hypothesis [42]. If the context is not considered during the repair, existing tools, such as SmartFix [14], and LLMs (GPT-4) adopted conservative measures, i.e. owner of the contract, as in §VII-C, which could lead to overfitting by inappropriately preventing legitimate banks from depositing. Clearly, this not the expected behavior, as such repairs significantly impede the function's usability. Instead, the appropriate repair should respect common RBAC practices and align with the context related to the access control of smart contracts.</p>
<p>Similar to the motivating example, RBAC is commonly implemented in smart contracts through mechanisms such as centralized role mappings (e.g., mapping (address =&gt; bool)), modifier-based enforcement (e.g., onlyOwner), and inline conditional checks using msg.sender. These implementations often vary significantly across contracts in structure, naming conventions, and enforcement logic. This diversity introduces challenges for automated repair, including difficulty in identifying roles due to inconsistent definitions, implicit permission logic, and the risk of introducing conflicting or redundant access checks.</p>
<h2>C. Inspired Design of ACFIX</h2>
<p>To address the heterogeneity of AC practices in smart contracts, ACFIX first mines common RBAC patterns from largescale contracts and generalizes them into domain knowledge, organized as a dynamic taxonomy of role-permission pairs. The use of an external knowledge base to guide or supplement large language models is a widely recognized strategy for improving robustness and accuracy, as evidenced by recent advances in retrieval-augmented and knowledge-augmented LLMs [43], [44], [45]. Our RBAC taxonomy is designed to be continuously extensible, enabling the system to incorporate new knowledge as it encounters novel contexts. This taxonomy serves as a domain-specific external knowledge base, effectively grounding the LLM's reasoning and mitigating risks of hallucination or inconsistency. By correlating the code context of an AC-related vulnerability with the taxonomy, the LLM can infer and apply AC mechanisms that are both contextually relevant and consistent with the contract's intended logic.</p>
<p>Importantly, ACFIX is also RBAC-aware-it analyzes the existing enforcement pattern within the contract and adapts its patching style accordingly. For instance, if a contract predominantly uses modifier-based enforcement, ACFIX will attempt to follow this style in the generated patch to maintain semantic consistency. This adaptive behavior helps prevent structural conflicts and promotes compatibility with the original design.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2: A high-level overview of ACFix, consisting of both offline and online phases.</p>
<p>Furthermore, the independent Validator Agent (validator) verifies whether the proposed patch aligns with the intended access control policy, ensuring that the role-permission relationship is correctly enforced based on RBAC and that the patch does not introduce functional regressions. This dual-layered approach—combining LLM-based reasoning guided by domain knowledge and semantic validation by validator—enables ACFix to effectively address the challenges of RBAC heterogeneity in smart contract repair. More details of this process, including the construction of the RBAC taxonomy, are further elaborated in Section IV.</p>
<h2>III. OVERVIEW OF ACFix</h2>
<p>Fig. 2 presents a high-level overview of ACFix, which includes both offline and online phases. In the offline phase, we mine common RBAC practices from smart contracts to construct an RBAC taxonomy. This taxonomy will be used in the online phase to guide GPT-4 in pinpointing the appropriate role-permission pairs. In the online phase, for each AC vulnerability, based on the Multi-Agent Debate (MAD) architecture[29], [46], [47], [48], we employ a dual-agent architecture that consists of a generator and a validator. Specifically, we mine the RBAC taxonomy from the source code of smart contracts deployed on-chain. With this taxonomy in hand, ACFix repairs an AC vulnerability in the following steps:</p>
<ol>
<li>To facilitate practicality and avoid redundant fixes, an optional step involving a checking prompt Q0 is used to confirm if the target function is subject to AC vulnerabilities. This is because while ACFix is positioned as an APR (Automatic Program Repair [49]) tool that only takes confirmed vulnerability inputs from auditing reports, CVEs, and attack incidents, we allow ACFix to be deployed as a copilot to help developers or existing AC detection tools fix potential AC vulnerabilities. In the latter case, Q0 is needed, and the result should be confirmed by an operator, as in the typical copilot scenario.</li>
<li>Generator then parses the contract source code, including the vulnerable part, to extract RBAC-related code elements. We then provide these elements to GPT-4 in a prompt Q1, seeking to inquire whether any element belongs to existing RBAC mechanisms in the subject code.</li>
<li>Starting from the vulnerable function $f_{v u l}$, generator employs program slicing and data flow analysis to construct an inter-procedural AC Context Graph (ACG). This graph depicts the code semantically related to $f_{v u l}$. Upon recognizing existing RBAC mechanisms in step (1), generator extends the ACG by incorporating relevant identifiers, such as modifiers and state variables, based on $f_{v u l}$.</li>
<li>Using the serialized ACG as prompt Q2, generator guides LLMs to identify the most appropriate role-permission pair from our mined RBAC taxonomy or, if necessary, incorporates a new pair into the taxonomy.</li>
<li>After pinpointing the role-permission pair, generator instructs LLMs to generate a proper patch for the vulnerable code through prompt Q3. The generated patch is first statically checked for validity by rules and then continuously validated by validator through prompt Q4 to refine it until it is considered effective or the limit is reached.</li>
</ol>
<p>Next, we detail the offline phase of RBAC mining in §IV and the online phase of RBAC-guided and context-aware LLM-driven repairing in §V and §VI, respectively. Regarding training or fine-tuning of LLMs, we observe that ACFix already demonstrates robust performance in repairing AC vulnerabilities by leveraging rich contextual inputs and a comprehensive taxonomy. Although fine-tuning could potentially yield incremental improvements, it introduces risks of overfitting and may reduce model flexibility. In contrast, our current design enables ACFix to dynamically incorporate newly identified RBAC pairs by updating the taxonomy, without necessitating retraining. Such adaptability cannot be achieved through fine-tuning a pre-trained model. Given these considerations, and the lack of large-scale training datasets for AC vulnerabilities, we designed ACFix to effectively combine static analysis, a predefined taxonomy, and in-context learning prompts to repair AC vulnerabilities without additional training or fine-tuning.</p>
<p>While ACFix leverages established techniques such as static slicing, code context extraction, and chain-of-thought prompting, its novelty lies in the domain-specific integration of these components to address the unique challenges of repairing AC vulnerabilities in smart contracts. Unlike general-purpose code repair, AC vulnerability repair demands precise reasoning over role-permission relationships and intricate identity checks. To address this, ACFix introduces a dynamic RBAC-guided taxonomy, a dual-agent validation-feedback mechanism, and a repair flow that directly consumes outputs from external AC detectors. This layered design enables ACFix to not only generate patches but also validate their semantic correctness and compatibility with existing RBAC logic, offering an end-to-end, practical solution tailored for secure smart contract repair.</p>
<p>TABLE I: A taxonomy of common RBAC practices, featuring mined role-permission pairs and their detailed checks.</p>
<table>
<thead>
<tr>
<th>Roles</th>
<th>Permissions</th>
<th>Examples of Detailed Permission Checks</th>
</tr>
</thead>
<tbody>
<tr>
<td>Admin</td>
<td>Low-level call</td>
<td>Multi-factor authentication</td>
</tr>
<tr>
<td></td>
<td>Manage users</td>
<td>Multi-signature approval, Whitelisting and</td>
</tr>
<tr>
<td></td>
<td>of the contract</td>
<td>blacklisting, Time locks</td>
</tr>
<tr>
<td></td>
<td>Manipulate price</td>
<td>Rate limiting, Multi-signature requirements</td>
</tr>
<tr>
<td></td>
<td>Transaction management</td>
<td>Rate limiting, Transaction validation</td>
</tr>
<tr>
<td></td>
<td>User/Role management</td>
<td>Regular audits, Event logging for role changes</td>
</tr>
<tr>
<td></td>
<td>Utilities management</td>
<td>Time locks, Regular audits and testing</td>
</tr>
<tr>
<td></td>
<td>Adjust fees</td>
<td>Validation checks for fee changes</td>
</tr>
<tr>
<td></td>
<td>Monitor &amp; analyze</td>
<td>Access control via view functions,</td>
</tr>
<tr>
<td></td>
<td>transactions</td>
<td>Data validation and sanitation</td>
</tr>
<tr>
<td></td>
<td>Set trading pairs</td>
<td>Validation checks for trading pairs</td>
</tr>
<tr>
<td></td>
<td>Configure security settings</td>
<td>Multi-factor authentication</td>
</tr>
<tr>
<td>Owner of the contract</td>
<td>Initialization</td>
<td>Limit initialization to authorized users</td>
</tr>
<tr>
<td></td>
<td></td>
<td>against frontrun, Ensure initialization</td>
</tr>
<tr>
<td></td>
<td>Change ownership</td>
<td>only occurs once</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Limit ownership change to authorized users</td>
</tr>
<tr>
<td></td>
<td>Upgrade contract</td>
<td>against frontrun, Time locks</td>
</tr>
<tr>
<td></td>
<td>Upgrade contract</td>
<td>Limit to authorized users against frontrun,</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Time locks, Multi-signature requirements</td>
</tr>
<tr>
<td></td>
<td>Pause contract</td>
<td>Limit to authorized users against frontrun,</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Time locks</td>
</tr>
<tr>
<td></td>
<td>Destroy contract</td>
<td>Limit destroy to authorized users,</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Multi-signature requirements</td>
</tr>
<tr>
<td>Owner of the</td>
<td>Burn</td>
<td>Validation checks for the owner of</td>
</tr>
<tr>
<td></td>
<td>Claim</td>
<td>the burnable, Multi-signature control</td>
</tr>
<tr>
<td></td>
<td>Withdrawal</td>
<td>Validation checks for the owner of the claimable</td>
</tr>
<tr>
<td></td>
<td>Swap</td>
<td>Rate limiting, Withdrawal limits</td>
</tr>
<tr>
<td></td>
<td>Liquidity</td>
<td>Transaction validation, Swap limits</td>
</tr>
<tr>
<td></td>
<td>Transfer</td>
<td>Rate limiting, Validation checks for liquidified</td>
</tr>
<tr>
<td></td>
<td>Approve</td>
<td>funds</td>
</tr>
<tr>
<td></td>
<td>Manage stakes</td>
<td>Validation checks for loan rollovers</td>
</tr>
<tr>
<td></td>
<td>Create pools</td>
<td>Validation checks for loan rollovers</td>
</tr>
<tr>
<td></td>
<td>Set approval limits</td>
<td>State limiting</td>
</tr>
<tr>
<td></td>
<td>Mint</td>
<td>Minting limits, Whitelisting and blacklisting,</td>
</tr>
<tr>
<td></td>
<td>Setting minting</td>
<td>Minter management, Multi-signature approval</td>
</tr>
<tr>
<td></td>
<td>parameters</td>
<td>Validation checks for parameters</td>
</tr>
<tr>
<td>Loaner</td>
<td>Offering loans</td>
<td>Validation checks for loan terms</td>
</tr>
<tr>
<td></td>
<td>Collecting collateral</td>
<td>Secure handling of collateral</td>
</tr>
<tr>
<td></td>
<td>Receiving payments</td>
<td>Transaction validation</td>
</tr>
<tr>
<td></td>
<td>Managing defaults</td>
<td>Secure mathematical operations</td>
</tr>
<tr>
<td></td>
<td>Rolling loans</td>
<td>Secure collateral liquidation</td>
</tr>
<tr>
<td></td>
<td>Rolling loans</td>
<td>Validation checks for loan rollovers</td>
</tr>
<tr>
<td></td>
<td>Withdrawal of funds</td>
<td>Limit to fund owner, Withdrawal</td>
</tr>
<tr>
<td></td>
<td>Viewing loan status</td>
<td>limits, Time locks</td>
</tr>
<tr>
<td></td>
<td>Viewing loan status</td>
<td>Data validation and sanitation</td>
</tr>
<tr>
<td></td>
<td>Setting loan conditions</td>
<td>Validation checks for loan conditions</td>
</tr>
<tr>
<td>Borrower</td>
<td>Requesting loans</td>
<td>Validation checks for loan requests</td>
</tr>
<tr>
<td></td>
<td>Depositing collateral</td>
<td>Secure collateral handling</td>
</tr>
<tr>
<td></td>
<td>Repaying loans</td>
<td>Transaction validation, Secure math operations</td>
</tr>
<tr>
<td></td>
<td>Managing active loans</td>
<td>Data validation and sanitation</td>
</tr>
<tr>
<td></td>
<td>Rolling or</td>
<td>Validation checks for rollovers/refinancing</td>
</tr>
<tr>
<td></td>
<td>refinancing loans</td>
<td>Secure liquidation handling</td>
</tr>
<tr>
<td></td>
<td>Handling liquidations</td>
<td>Validation checks for withdrawals</td>
</tr>
<tr>
<td></td>
<td>Withdrawing collateral</td>
<td>Secure notification handling</td>
</tr>
<tr>
<td></td>
<td>Receiving notifications</td>
<td>Restriction to owner of deposit, Deposit limits</td>
</tr>
<tr>
<td></td>
<td>Deposit</td>
<td>Withdrawal limits, Time locks,</td>
</tr>
<tr>
<td></td>
<td>Withdrawal</td>
<td>Multi-signature approvals</td>
</tr>
<tr>
<td></td>
<td>Manage funds</td>
<td>Rate limiting, Multi-signature approvals</td>
</tr>
<tr>
<td></td>
<td>Set interest rates</td>
<td>Validation checks for parameters</td>
</tr>
<tr>
<td></td>
<td>Log</td>
<td>Secure storage of sensitive information</td>
</tr>
<tr>
<td></td>
<td>Log parameters</td>
<td>Multi-signature requirements, Rate limiting</td>
</tr>
<tr>
<td>Logger</td>
<td>Set log parameters</td>
<td>Multi-signature requirements, Using proxy patterns for upgradability and security</td>
</tr>
</tbody>
</table>
<h2>IV Mining Common RBAC Practices</h2>
<p>During the offline phase, our goal is to systematically mine and categorize common RBAC practices observed in real-world smart contracts. Specifically, we extract role-permission pairs—the foundational elements of RBAC—from contract source code and generalize them into a structured taxonomy. This taxonomy serves as a domain-specific knowledge base that guides LLM-based repairs. By embedding this external knowledge, ACFix can reason more effectively about access control logic and generate contextually accurate patches. Furthermore, the taxonomy is designed to be dynamically extensible at runtime, allowing ACFix to incorporate new RBAC patterns as they emerge from usage examples.</p>
<p>To mine common RBAC practices, we have collected smart contracts written in Solidity [4] from 344, 251 addresses [50] on the Ethereum Mainnet as of December 2023. While we found that developers often create their own versions of RBAC, there are three major mechanisms to enforce permission checks in smart contracts:</p>
<p>①OZAC: When OpenZeppelin Access Control (OZAC) [39] is employed, roles are explicitly and uniformly implemented using templates, such as Ownable and Access. We extracted the defined roles and corresponding function names based on OZAC templates to infer permissions. ②Modifier: Modifier declares conditional checks that Solidity automatically embeds into the function prologues [13]. However, since modifiers can be used for various purposes, we focused only on RBAC-related modifiers that begin with only, such as onlyOwner, based on an empirical study about modifiers [13]. The roles specified after only and the names of modified functions were recognized as roles and permissions, respectively. ③Transaction-Reverting Statements (TRS): The third is based on TRS [51], which use Solidity keywords, such as require and if...revert, to ensure contract integrity. A primary use of TRS is AC, where msg.sender is compared to predefined roles or addresses. Although TRS can serve multiple purposes, our study specifically targeted TRS assessing msg.sender in the context of RBAC, ensuring that our extraction remains relevant and omits distractions from unrelated uses of these statements.</p>
<p>Based on the three patterns above, we automatically mined 810, 344 pairs of roles and functions. After de-duplication, we identified 46, 495 unique pairs, ranked in descending order by frequency. To construct the RBAC taxonomy, we began by analyzing the top 1,000 most frequent role-permission pairs, which collectively account for 81.83% of 810, 344 all observed pairs in our data. We employed an open card-sorting methodology [52] to manually categorize permissions based on associated function names. New cards (i.e., role-permission categories) were dynamically introduced whenever a pair could not be reasonably grouped into an existing category.</p>
<p>The first two authors, each with over four years of experience in smart contract analysis, independently reviewed and labeled all 1,000 pairs. After individual labeling, we first merged cards that conveyed the same underlying meaning. Then, we compared the assigned cards for each pair to identify disagreements. In cases of disagreement, the final decision was made by the third author. The overall disagreement rate was 9.5%, indicating a high level of consistency between reviewers. Following this process, every one of the 1,000 pairs was</p>
<p>assigned to a specific role-permission card, and the resulting collection formed the foundation of our final RBAC taxonomy.</p>
<p>Table I lists the categorized top mining results, with the first column showing the commonly used roles and the second column showing the permissions these roles may hold. We notice that these role-permission pairs are mostly related to DeFi because AC is usually implemented to manage financial assets in smart contracts. The roles could involve those with high privileges, such as Owner of the Contract and Admin, or those defined for specific operations, such as Minter and Loaner. The detailed roles depend on the usage of the contracts. It is worth noting that initially, there were 48 role-permission pairs derived from on-chain contracts in the offline process. Later during the evaluation, ACFIX dynamically updated the taxonomy and added one more pair, Admin-Low-level Call. The total of 49 pairs may not be exhaustive, but our evaluation showed that they have covered the majority of scenarios for which AC is implemented, and ACFIX could update it whenever new pairs are found (see Prompt Q2 in §V-C).</p>
<p>Based on the mined role-permission pairs, we further collected detailed permission checks for each pair from security auditing reports, as listed in the third column of Table I, which provide examples of common RBAC practices.</p>
<p>Revisiting the Motivating Example. With the derived taxonomy of common RBAC practices, we now revisit the motivating example in Fig. 1 to intuitively demonstrate how this taxonomy could enable ACFIX to generate the appropriate roles and permissions for real-world vulnerable code. Specifically, the function depositFromOtherContract could be easily matched by LLMs to the permission Deposit listed in Table I. Moreover, given the code context provided by our slicing in $\S \mathrm{V}$, LLMs can determine that this vulnerable contract has implemented two RBAC role checks, onlyBank and onlyOwner. Considering this context information and the taxonomy, LLMs could deduce the proper role-permission pair, which is Bank-Deposit, and generate a correct patch using the modifier onlyBank rather than onlyOwner.</p>
<h2>V. GUIDING LLMs TO PINPOINT PROPER <br> Role-PERMISSION PAIRS BASED ON CODE CONTEXT</h2>
<p>With the common RBAC practices mined in §IV, we now use them as a "knowledge base for AC repair" to guide LLMs in fixing code with similar functionality. To help LLMs understand the functionality of subject vulnerable code that needs to be repaired, we employ static code slicing to extract AC-related code context, more specifically, an AC context graph (ACG). We are particularly interested in code context related to the subject code's RBAC mechanisms. Therefore, we first leverage LLMs to identify existing RBAC mechanisms in the subject code (§V-A), enrich the code context of the identified RBAC mechanisms into ACG (§V-B), and finally instruct LLMs to use ACG to pinpoint the appropriate role-permission pair from the mined RBAC practices (§V-C). During this process, we adopt the Chain-of-Thought (CoT) [30] prompting to guide GPT-4 step by step, including the eventual AC repair generation that will be presented in the next section (§VI).</p>
<h2>A. Identifying Existing RBAC Mechanisms</h2>
<p>To prevent conflicts with any pre-existing RBAC mechanisms and to guide the construction of a relevant ACG in subsequent steps, ACFIX employs GPT-4 to explore existing RBAC mechanisms in the subject code, given that GPT-4 can comprehend the code. Since the names of most code elements, such as functions, state variables, and modifiers, are often self-explanatory, ACFIX extracts the names of these elements that might be associated with RBAC management. This initial information, along with the source code of vulnerable function $f_{v u l}$, is presented to GPT-4, which is then tasked with identifying the relevant elements related to RBAC. Specifically, ACFIX first analyzes the contract to identify all pre-defined roles, permission checks, and enforcement mechanisms, including both modifier-based and inline conditional statements. If multiple AC mechanisms coexist within the same contract, ACFIX aggregates all detected enforcement styles and uses the combined RBAC structure as a reference for patch generation. When a new role-permission pair is inferred, ACFIX ensures it does not contradict or duplicate existing logic. If any overlap or redundancy is detected, the patch is generated to either update outdated logic or integrate seamlessly with the existing mechanisms in a non-redundant manner. The LLM is instructed to consider the complete set of enforcement styles to prevent the introduction of conflicting or inconsistent RBAC rules. This comprehensive analysis helps maintain a coherent and unified AC policy, even in scenarios involving multiple roles or complex, mixed enforcement implementations.</p>
<p>We designed our prompt based on the best practices commonly associated with using GPT-4, as suggested by [53] and [54]. Specifically, our prompt includes two parts: (1) the natural language (NL) part that explains the task to GPT-4, and (2) the code context (CC) part that contains the vulnerable function and other relevant code. Given that the inquiry aims to identify RBAC-related code portions, ACFIX does not include detailed code statements but only the names of relevant functions and modifiers. Following research on learningbased unit test generation [55], we include the following code context in the CC part: (1) the signature and body of the vulnerable function; (2) modifiers; (3) state variables; (4) inherited contracts; (5) functions called by the vulnerable one in sequence; and (6) any vulnerability descriptions provided in the report, if available. For the NL part, drawing upon widely recognized guidelines for using GPT-4 [56], [57], we embed: (1) a role-playing instruction (i.e., You are a smart contract security specialist with expertise in identifying and mitigating vulnerabilities) to inspire GPT-4's contract repairing capability; and (2) a task-description instruction to explain the task. The prompt template is illustrated in Figure 4 for Q1.</p>
<p>After pinpointing specific target elements, ACFIX constructs the ACG based on them if available and $f_{v u l}$ by default.</p>
<h2>B. Constructing AC Context Graph (ACG)</h2>
<p>To capture contextual code statements that constitute the functionality of the vulnerable function $f_{v u l}$, we employ program slicing [58] as suggested by numerous previous studies [59], [60], [61], [62], [63]. Program slicing identifies</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: AC Context Graph (ACG) for the Motivating Example.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Q1 Prompt: Existing RBAC Identification</p>
<p>code statements that influence, either through data or control, a target variable or statement. Since Ethereum-compatible blockchains [64] depend on modifications to state variables, vulnerable functions generally interact with state variables in their own or other contracts, either directly or indirectly. Based on this observation, ACFIX performs inter-procedural program slicing on the state variables interacted with by <em>fcul</em> and associated RBAC elements (i.e., the output of §V-A). This approach aims to minimize extraneous code, ensuring a <em>concise</em> prompt that attracts focused attention from GPT-4. ACFIX, therefore, constructs an ACG that comprises a streamlined code context of <em>fcul</em> from the subject contract.</p>
<p>We define ACG as $$G = {\langle V, E \rangle \mid V \subseteq {F, Var_{state}, Mdf, Cmt}, E \subseteq {v_i, v_j} \mid v_i, v_j \in {f, var, mdf, cmt}}$$, where <em>F</em> represents the set of functions.</p>
<p><em>Var<sub>state</sub></em> denotes the set of state variables, <em>Mdf</em> signifies the set of modifiers, and <em>Cmt</em> is the set of comments. Each vertex has three properties: <em>Signature</em>, <em>Body</em>, and the original <em>Contract</em> to which it belongs. Edges encapsulate multiple types of relationships between vertices, including <em>invocation</em>, <em>modifying</em>, <em>reading/writing</em>, and <em>comment</em>. Fig. 3 in Appendices presents an illustration of ACG for the motivating example shown in Fig. 1. Specifically, ACFIX breaks down the contract into various elements, such as modifiers and state variables, and connects them with corresponding relationships. For individual processing of elements, ACFIX performs call-chain-based inter-procedural program slicing.</p>
<p>To facilitate the analysis, the call graph and Program Dependency Graph (PDG) [65] are firstly constructed. Given that the input source code may not represent a complete Solidity project but rather excerpts from audit reports, it might not be compilable. Hence, program analysis tools like Slither [66] are not applicable due to their strict compilation requirements. To address this issue, we have implemented a hybrid framework that performs call graph and PDG analysis on the Abstract Syntax Tree (AST) using Antlr [67] when Slither is infeasible. Note that Intermediate Representation (IR) based analysis from Slither is preferred. Although using Antlr may result in reduced accuracy and granularity (since AST primarily captures syntactic relationships between tokens without inherent optimization, unlike the IR-based approach), it remains adequate for collecting information for this task.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: Construction of AC Context Graph
Input: Vulnerable Function \(f_{\text {val }}\), Program Dependency Graph
    \(\mathcal{P D G}\), Call Graph \(\mathcal{C G}\)
Output: Access Control Context Graph \(\mathcal{G}=(\mathcal{V}, \mathcal{E})\)
(1) \(\mathcal{G} \leftarrow \operatorname{Graph}(\mathcal{V}, \mathcal{E}) \quad / /\) Initialize Graph
(2) \(\mathcal{V}_{\text {state }} \leftarrow \operatorname{DefUseChain}\left(f_{\text {val }}\right) \quad / /\) Extract State Variables
(3) \(\mathcal{F}_{\text {calles }} \leftarrow \operatorname{CallGraph}\left(f_{\text {val }}\right) \quad / /\) Identify Callee Functions
(4) \(\mathcal{V} \leftarrow \mathcal{F}_{\text {calles }} \cup \mathcal{V}_{\text {state }} \quad / /\) Define Vertex Set
(5) foreach \(v \in \mathcal{V}_{\text {state }}\) do
(6) \(\operatorname{stmt} \leftarrow \operatorname{DefUseChain}(v) \quad / /\) Compute Def-Use Chain
(7) \(f \leftarrow\) FuncOf(stmt) // Determine Enclosing Function
(8) \(\mathcal{D}_{\text {data }}, \mathcal{D}_{\text {control }} \leftarrow\{\operatorname{stmt}\},\{\operatorname{stmt}\} \quad / /\) Initialize
        Dependencies
(9) while \(\mathcal{D}_{\text {data } . \text { next }()} \neq\) null do
(10) \(\operatorname{stmt} \leftarrow \mathcal{D}_{\text {data. } . \text { next }()}\)
(11) foreach opr \(\in\) stmt.split() do
(12) if \(v \in\) opr then
(13) \(\quad\) AddOperation(opr)
(14) if \(\mathcal{P D G} . H a s N e x t D a t a N o d e(o p r)\) then
(15) \(\quad \mathcal{V} \cdot \operatorname{Add}(\mathcal{P D G} . \operatorname{NextNode}(\mathrm{opr}), f)\)
\(\mathcal{E} \cdot \operatorname{Add}(\mathrm{opr}, \mathcal{P D G} . \operatorname{NextNode}(\mathrm{opr}))\)
\(\mathcal{D}_{\text {data. } . \operatorname{Add}}(\mathcal{P D G} . \operatorname{NextNode}(\mathrm{opr}))\)
(16) else if \(\mathcal{P D G} . \operatorname{NextDataNode}(o p r) \in\)
(17) \(\quad\{\) PARAMETER, RETURN \(\}\) then
(18) \(\quad\) callsites \(\leftarrow \mathcal{C G} . \operatorname{GetCallers}(\mathrm{opr})\)
\(\mathcal{D}_{\text {data. } . \operatorname{Add}}(\operatorname{Return}(\text { callsites }))\)
while \(\mathcal{D}_{\text {control. }} . \operatorname{next}() \neq\) null do
(19) \(\operatorname{stmt} \leftarrow \mathcal{D}_{\text {control. }} . \operatorname{next}()\)
(20) foreach opr \(\in\) stmt do
(21) if \(v \in\) opr then
(22) \(f\).AddOperation(opr)
(23) if \(\mathcal{P D G} . H a s N e x t\) ControlNode(opr) then
(24) \(\quad \mathcal{V} \cdot \operatorname{Add}(\mathcal{P D G} . \operatorname{NextNode}(\mathrm{opr}), f)\)
\(\mathcal{E} \cdot \operatorname{Add}(\mathrm{opr}, \mathcal{P D G} . \operatorname{NextNode}(\mathrm{opr}))\)
\(\mathcal{D}_{\text {control. } . \operatorname{Add}}(\mathcal{P D G} . \operatorname{NextNode}(\mathrm{opr}))\)
(25) else if \(\mathcal{P D G} . \operatorname{NextControlNode}(o p r) \in\)
(26) \(\quad\) callsites \(\leftarrow \mathcal{C G} . \operatorname{GetCallers}(o p r)\)
\(\mathcal{D}_{\text {control. } . \operatorname{Add}}(\operatorname{Return}(\text { callsites }))\)
</code></pre></div>

<p>(27) return $\mathcal{G}$
// Return the constructed graph</p>
<p>However, the usage of Antlr introduces two new issues. First, unlike the three-address-code format in Slither IR, oneline source code format in Antlr might encompass multiple operators. It is necessary to split multiple operations from one statement for proper slicing. Second, it is common to accommodate the implementation within internal functions.</p>
<p>To address these issues, we propose several enhancements for the construction of the ACG. The general procedure of program slicing is presented in Algorithm 1. Initially, the Vulnerable Function $f_{\text {val }}$, Program Dependency Graph $P D G$, and Call Graph $C G$ were first calculated based on the given contract serving as the basic structure to run the algorithm for inter-procedural construction. Specifically, the initial variables are extracted and initialized in Line 1-4 and ACFix begins to
iterate over the state variables $V a r_{\text {state }}$ in Line 5. For each $v a r_{\text {state }}$, the statements that read or write the $v a r_{\text {state }}$ are tracked in Line 6 with the enclosing function being determined in Line 7. Next, ACFix begins slicing from statements (stmt) involving the state variables $V a r_{\text {state }}$ and conducts forward and backward slicing recursively by tracking dependencies related to these statements in the subsequent lines. If any operation is included in the slice, the corresponding complete line of source code is preserved in Body.</p>
<p>During slicing, ACFix recursively explores dependency chains using a Breadth-First Search (BFS) strategy, as illustrated in Lines 9-22. If a statement contains multiple operations (Line 11), it is split according to Solidity syntax using Antlr lexical patterns. Operations utilizing state variables are subsequently added to $f$ as initial points for data flow tracing (Lines 12-13). Then, ACFix iteratively traces data and control flows, updating $D_{\text {data }}$ and $D_{\text {control }}$ accordingly (Lines 14-15 and 23-24). For cross-function slicing, ACFix connects the parameters at function call sites with their counterparts in the function definitions, enabling backward inter-procedural slicing (Lines 16-17 and 25-26). For forward slicing, the returned variable within the function definition is linked with variable assignments receiving the function's return value at call sites. For simplicity, Algorithm 1 does not explicitly distinguish between forward and backward slicing.</p>
<h2>C. Pinpointing the Role-Permission Pair</h2>
<p>In this step, ACFix leverages LLMs to correlate the enriched ACG code context with common RBAC practices to identify the role-permission pair for the subject code. Due to the limited context window, ACG is serialized as the prompt for GPT-4. Specifically, elements from ACG are described in both code segments and natural language and are presented to GPT-4. ACFix first supplements the source code body for modifiers. For functions, only the statements derived from ACG are included in the body code. For state variables, the function bodies obtained from slicing are provided. Regarding inherited contracts, such as Ownable, the bodies of modifiers defined therein are incorporated into the prompt. In addition to these elements, edges, such as invocation, modifying, reading/writing, and comment, are all described in natural language.</p>
<p>Specifically, GPT-4 is prompted to select a role-permission pair from a pre-defined RBAC taxonomy. If GPT-4 identifies a pair that is not present in the current taxonomy but appears contextually appropriate, it is allowed to suggest a new pair. When such a novel pair is generated, ACFix initiates a multi-stage validation process to ensure both its relevance and uniqueness. First, the system checks for potential duplication by comparing the candidate pair with existing entries using normalized role and permission representations. This normalization process standardizes role and permission names by converting them to a consistent case, removing common prefixes or suffixes, and applying stemming or lemmatization to address minor linguistic variations. In addition, synonyms and abbreviations are mapped to unified forms using a curated dictionary and context-aware LLM prompts. By leveraging</p>
<p>these canonicalized representations, ACFix can more accurately detect true semantic overlaps and avoid false positives. If no equivalent pair exists, the candidate is provisionally added to the taxonomy.</p>
<p>To further ensure the integrity and clarity of the RBAC taxonomy, ACFix periodically employs an additional language model to systematically review the entire set of pairs. This review phase is designed to identify improper, overlapping, or ambiguous entries, and to sanitize the taxonomy if necessary. When appropriate, human oversight can be incorporated to prevent unintended errors and resolve borderline cases. This consolidated review process is conducted at regular intervals, balancing cost efficiency with the need for accuracy and minimizing disruption to ongoing repair operations.</p>
<p>For example, during evaluation, ACFix encountered the pattern Admin-Low-level Call, which was not present in the original taxonomy. Recognizing its contextual relevance, ACFix successfully incorporated this pair into the taxonomy, making it available for subsequent repair tasks.</p>
<p>This dynamic extension, normalization, and validation mechanism enables ACFix to adapt to diverse and evolving RBAC models across smart contracts, ensuring continued relevance and extensibility.</p>
<p>Similar to the previous prompt, the prompt Q2 includes the CC and NL parts. The CC part is detailed with ACG information. In the NL part, a question is posed to GPT-4, asking it to select a role-permission pair from the taxonomy based on the provided code context. The prompt is as follows:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Generator/ Q2 Pattern: Role-permission Pair Identification</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">- RBAC-related functions: <signature> <sliced body> <comment>; Callee functions are: <signature> <sliced body> <comment>; State variables that are read/written by the above functions: <state variables>; Modifiers modifies <functions>: <name> <sliced body></td>
<td style="text-align: center;">CC <br> Part</td>
</tr>
<tr>
<td style="text-align: center;">- Which role and permission does the vulnerable function belong to in the following category?</td>
<td style="text-align: center;">$\frac{\text { NL }}{\text { Part }}$</td>
</tr>
<tr>
<td style="text-align: center;">- Always prefer the privilege that I provide. If not, name new pairs that fit to the context.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- State it clearly with format as Role: XXX, Permission: XXX. Do not explain your decision.</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Fig. 5: Q2 Prompt: Role-permission Pair Identification</p>
<h2>VI. Generating and Validating Patches</h2>
<h2>A. Generating Patches and Static Checking</h2>
<p>With the appropriate role-permission pair identified in $\S \mathrm{V}$, ACFix now generates the final AC repair. Besides the rolepermission pair stored in the LLMs' session memory from prompts Q1 and Q2, ACFix also retrieves corresponding examples of detailed permission checks from Table I to prompt GPT-4 to generate a patch. If any existing RBAC mechanisms were identified in prior responses, ACFix will prioritize reusing and enhancing them when possible to prevent any conflicts. The prompt is presented as follows:</p>
<p>After deriving the repaired code, ACFix conducts static grammar checks to ensure the validity of the repair. Should any discrepancies arise, ACFix consolidates these issues and relays them back to GPT-4 in a subsequent prompt, seeking an</p>
<h2>Generator/ Q3 Pattern: Patch Generation and Validation</h2>
<ul>
<li>The common practices of code patching for the role permission you mentioned before are <Common practices>.</li>
<li>Your task is to provide a fix for the vulnerable function ensuring only the assigned role can execute particular function based on the common practices.</li>
<li>Do not explain your decisions. Reuse existing RBAC mechanisms mentioned before if proper.</li>
</ul>
<p>Fig. 6: Q3 Prompt: Patch Generation and Validation
updated patch. This paper considers five kinds of static grammar checks: Avoiding Undefined Tokens, Avoiding Infeasible Function Invocations, Avoiding Misused Types, Avoiding Inconsistent Solidity Versions, and Validating the msg. sender Check. Details are omitted here due to page limit. Interested readers may refer to our supplementary material.</p>
<h2>B. Generating Patches and Static Grammar Checking</h2>
<p>After generating patches, ACFix performs a series of static and semantic checks to ensure the compatibility, correctness, and applicability of the patches before integration into the target smart contract. These checks cover both syntactic and contextual dimensions, aiming to prevent invalid or incompatible modifications that could introduce unintended behaviors. The following rules are enforced:</p>
<ul>
<li>Avoiding Undefined Tokens: ACFix first extracts all defined tokens from the current and inherited contracts, denoted as $T_{\text {defined }}$. Then, it analyzes the tokens introduced in the generated patch, such as new functions, modifiers, and state variables, represented as $T_{\text {repaired }}$. A patch passes this check only if all new tokens are properly defined or already exist.</li>
</ul>
<p>$$
\text { isDefined }\left(T_{\text {repaired }}\right) \Leftrightarrow T_{\text {repaired }} \subseteq\left(T_{\text {current }} \cup T_{\text {inherited }}\right)
$$</p>
<ul>
<li>Avoiding Infeasible Function Invocations: GPT-generated code may call functions that do not exist or have incorrect signatures. ACFix collects the set of Solidity built-in functions $F_{\text {built-in }}$ and the user-defined functions in the repaired contract $F_{\text {repaired }}$, then validates that all invoked functions Invok are part of this union.</li>
</ul>
<p>$$
\text { isFeasible }(I n v o k) \Leftrightarrow I n v o k \subseteq\left(F_{\text {built-in }} \cup F_{\text {repaired }}\right)
$$</p>
<ul>
<li>Avoiding Misused Types: To prevent inconsistent or unsafe variable usage, ACFix extracts the variable types from both the original (Type $<em _rep="{rep" _text="\text">{\text {val }}$ ) and repaired (Type $</em>$ ) contracts. It ensures that variable types are used consistently and that no invalid type conversions occur.}</li>
</ul>
<p>$$
\text { isConsistent }\left(T y p e_{\text {val }}, T y p e_{\text {rep }}\right) \Leftrightarrow T y p e_{\text {val }}=T y p e_{\text {rep }}
$$</p>
<ul>
<li>Avoiding Inconsistent Solidity Versions: A patch may use features unavailable in the specified version of Solidity. ACFix checks whether the version required by the patch (Version $<em _sol="{sol" _text="\text">{\text {patch }}$ ) is compatible with the contract's declared version (Version $</em>$ ).}</li>
</ul>
<p>$$
\text { SolCompa }(\text { Patch }, \text { SolVer }) \Leftrightarrow \text { Version }<em _sol="{sol" _text="\text">{\text {patch }} \subseteq \text { Version }</em>
$$}</p>
<ul>
<li>Ensuring msg.sender Checks Are Introduced: For access control enforcement, ACFix verifies the existence of at least one conditional statement that compares msg.sender to a new or existing role identifier.</li>
</ul>
<p>$$
\operatorname{checked}(\text { msg.sender }) \Leftrightarrow \exists i f(\text { msg.sender }==\text { role }^{\prime})
$$</p>
<ul>
<li>Def-Use Chain Validation: ACFix constructs def-use chains for all newly introduced or modified variables and ensures that each variable is correctly defined before use. This includes checking scope correctness, avoiding uninitialized variables, and ensuring no overwritten variables conflict with existing control/data flow.</li>
</ul>
<p>$$
\text { isValidDefUse }(V) \Leftrightarrow V_{\text {patch }}^{\text {use }} \subseteq\left(V_{\text {defined }} \cup V_{\text {patch }}^{\text {def }}\right)
$$</p>
<ul>
<li>Structural Compatibility Check: Before applying the patch, ACFix validates that the modified code block aligns with the structural boundaries of the original smart contract. For example, a function-level patch must respect existing function signatures and modifiers.</li>
</ul>
<p>$$
\text { Compatible }\left(S_{\text {patch }}, S_{\text {target }}\right) \Leftrightarrow \forall s \in S_{\text {patch }} \text {, ValidContext }\left(s, S_{\text {target }}\right)
$$</p>
<p>This multi-step validation pipeline enables ACFix to generate patches that are not only grammatically valid but also semantically consistent and directly applicable to the original smart contract codebase.</p>
<h2>C. Validating Patches' Effectiveness via MAD</h2>
<p>Once all static and rule-based checks are passed, ACFix engages the Validator Agent (validator) to perform a higherlevel semantic validation of the patch's effectiveness through a multi-agent debating (MAD) loop. This step is essential to ensure not only syntactic correctness but also functional and security alignment with the intended AC policy. In this process, the Generator Agent (generator) first outputs a candidate patch and provides it to validator along with the vulnerability description, the surrounding code context, and the selected rolepermission pair. The Validator Agent independently evaluates whether the patch (1) correctly mitigates the identified AC vulnerability, (2) preserves the original contract logic, and (3) does not introduce any new security or logical flaws.</p>
<p>The validator performs this assessment by simulating the review process a domain expert might conduct. It reasons over the vulnerability description and the repaired code to determine if the AC logic is properly enforced-e.g., checking that access is restricted to intended roles, permission boundaries are respected, and the role-permission pair selected by generator is consistent with the contextual semantics. If the patch is deemed insufficient or flawed, validator returns structured feedback, including the reason for rejection (e.g., incorrect role, missing validation, logic conflict). This feedback is then passed to generator, which uses the information to refine and regenerate an improved patch. This repair-validation cycle continues in a loop with a maximum of 3 iterations to balance thoroughness and efficiency.</p>
<p>Even if the patch is not accepted after 3 attempts, the last generated patch is retained as the final output. Based
on our empirical evaluation (see Section VII-C), this iterative mechanism proves highly effective: over $90.9 \%$ of the cases required at most one re-attempt, and only a single case failed to pass validation after three rounds. This agent-based validation framework strengthens ACFix by introducing a self-regulating feedback loop that improves patch robustness, reduces hallucinations, and ensures more consistent adherence to access control principles.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Validator/ Validate Patches</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">- First round: Can this patch fix the vulnerability? <patch from generator></td>
</tr>
<tr>
<td style="text-align: left;">- The source code is <source code>. State the answer and reasons.</td>
</tr>
<tr>
<td style="text-align: left;">- Second round onwards: The patch is updated as <new patch>.</td>
</tr>
</tbody>
</table>
<p>Fig. 7: Q4 Prompt: Patch Validation</p>
<h2>VII. Evaluation</h2>
<p>We aim to evaluate ACFix based on its effectiveness in appropriately repairing AC vulnerabilities by answering the following six research questions (RQs):
RQ1: LLM Selection. How do popular LLMs perform as the base model of ACFix and which is the best?
Given the emergence of multiple LLMs offering similar code generation capabilities, we first needed to evaluate these models to determine the most suitable base model for ACFix. To achieve this, we assessed all popular LLMs available as of the submission date, comparing their performance as query interaction models within ACFix using a benchmark dataset. Specifically, we focused on two primary metrics: generation rate and success rate.</p>
<p>RQ2: Effectiveness Analysis. How effectively does ACFix repair AC vulnerabilities compare to other vulnerability repairing tools for smart contracts?
After selecting the best-performing model, we conducted a comprehensive and fair comparison with existing smart contract repair tools. To facilitate this, we first created the initial benchmark dataset specifically tailored for AC vulnerabilities. Using this dataset, we evaluated all available tools, identifying their strengths and weaknesses. Furthermore, we analyzed failure cases to understand and reveal the underlying reasons behind incorrect repairs.
RQ3: Ablation Analysis. How does the performance of ACFix compared to a baseline that uses only GPT-4 with raw code and descriptions as input?
To evaluate the contribution of each procedure implemented in ACFix, we systematically masked individual components to clearly highlight their respective impacts. Additionally, we compared ACFix against the vanilla GPT-4 model to emphasize the effectiveness and the novel design beyond the capabilities of the base model.
RQ4: Effectiveness by Categories. How do tools perform across various categories in the benchmark dataset?
The complexity of repairing AC vulnerabilities largely depends on accurately discerning nuanced differences between candidate roles and comprehending the contextual meaning within the code. Consequently, the difficulty of cases within</p>
<p>our benchmark dataset varies significantly. To gain deeper insights into the performance of ACFix across varying levels of difficulty, we further categorized and analyzed these cases based on their ground truth RBAC pairs.
RQ5: Practicality Analysis. Is ACFIX able to check potential vulnerabilities reported by static checkers?
Since ACFIX is designed to repair vulnerabilities reported by vulnerability detectors, we conducted this experiment specifically to evaluate such practical scenarios. To demonstrate its effectiveness, we integrated ACFIX with three widely-used static analysis tools capable of detecting AC vulnerabilities, thereby simulating an end-to-end workflow for vulnerability handling. This step evaluates the Q0 component of ACFIX, ensuring its practical capability to correctly process inputs provided by vulnerability detectors.
RQ6: Efficiency Analysis. How does ACFIX perform in terms of efficiency and financial cost?
Considering both execution time and monetary cost, an LLMbased tool such as ACFIX is expected to deliver efficient and cost-effective vulnerability repairs with notable results. Therefore, we conducted an end-to-end evaluation, systematically monitoring the execution time and monetary expenses associated with using ACFIX.</p>
<h2>Data Preparation.</h2>
<p>We constructed our dataset by building upon existing research, starting with 19 Common Vulnerability Enumerations (CVEs) that are frequently cited in prior AC-related studies [12], [9], [13]. For reference, SPCon and SoMo [12], [13] each evaluated on 44 cases, AChecker [9] used 21 cases, and SmartFix [14] focused on just 12 AC-related cases. While the SmartBugs dataset [68] is commonly used in broader smart contract vulnerability research, it was excluded from our evaluation due to the absence of ground truth annotations identifying whether the cases involve access control vulnerabilities. As our focus is specifically on AC vulnerabilities, such omissions make SmartBugs unsuitable for inclusion.</p>
<p>However, relying solely on CVEs does not yield a comprehensive evaluation. Given the absence of a benchmark dataset for AC vulnerabilities, we introduce the first benchmark dataset of real-world instances with ground truths. This dataset has been assembled from five primary sources as indicated in Table II (already covering the sources from the abovementioned work): (1) 19 CVEs from NVD [69]. (2) Defi Hack Labs [70] has published numerous vulnerabilities with realworld attacks. Under the "Access Control" category, we collected 28 cases with vulnerable code snippets and blockchain addresses. (3) An open vulnerability dataset provided by tintinweb [71] contains 28,699 vulnerabilities sourced from realworld auditing reports. After filtering for "Access Control," we identified 60 unique cases. (4) The dataset from SmartFix [14] includes 8 AC cases related to the misuse of tx. origin. (5) Additionally, we collected 3 more cases from media sources, including BlockSec [72], SlowMist [73], and Medium [74]. In total, we have compiled 118 real-world cases, making it the most extensive publicly available AC vulnerability dataset to date [75].
Metrics. Given that evaluating the correctness of patches remains a challenge in Automatic Program Repair [49], deter-</p>
<p>TABLE II: Sources of the Benchmark Dataset</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: center;">NVD</th>
<th style="text-align: center;">DefiHackLabs</th>
<th style="text-align: center;">tintinweb</th>
<th style="text-align: center;">SmartFix</th>
<th style="text-align: center;">Media</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Count</td>
<td style="text-align: center;">$19([69])$</td>
<td style="text-align: center;">$28([70])$</td>
<td style="text-align: center;">$60([71])$</td>
<td style="text-align: center;">$8([14])$</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>mining whether a repair is appropriate for the contract without overfitting involves leveraging multiple metrics to evaluate repairers. The following metrics were used for evaluation:</p>
<ul>
<li>Comparison with Author Fixes: Due to security concerns, many DeFi organizations and teams refrain from publishing the corrected code post-attack. We managed to collect 20 real fixes by the original authors to serve as target repairs for these 20 cases. Any patch that diverged from these original fixes was deemed unsuccessful.</li>
<li>Exploitation-Based Evaluation: DeFi Hack Labs [70] provides exploitation scripts that demonstrate how vulnerabilities can be exploited in a simulated environment, using authentic contracts sourced from the blockchain. We used these scripts to determine whether the vulnerability remains exploitable after the repair. We ran exploit scripts on both the original and repaired code to demonstrate that the repaired contracts are no longer exploitable. The logs for both of them are provided in our dataset [75].</li>
<li>Manual Inspection: The first two authors manually examined the repaired contracts to determine if the patch was appropriate. The third author made the final decision in the event of a disagreement. The explanatory notes are listed in our dataset [75].
It is worth noting that our initial intention was to utilize detection tools to determine whether the AC vulnerability still existed after repairs. However, no suitable tool was found to work properly for the cases within our dataset (except for 19 CVEs). Specifically, AChecker [9] works only for bytecode contracts. When we ran AChecker against 43 compilable AC cases, only 3 were detected (with testing logs recorded on our website [75]), leading to its exclusion from the evaluation. SPCon [12] requires transaction history, and SoMo [13] targets only modifier-based AC vulnerabilities and has yet to release its source code. As for other generic detectors such as Securify [76] and Slither [66], they require either compilable source code or precompiled bytecode, with the exception of SmartCheck [77]. However, upon running SmartCheck on our dataset, we found that it generated many false alarms about other types of vulnerabilities but very few concerning AC, indicating its unsuitability for detecting AC vulnerabilities.
SOTA Repair Tools to Be Evaluated. Various repair tools for smart contracts have been proposed in recent years. We selected benchmark tools through a principled selection process. Initially, we searched for papers using keywords such as "smart contract" and "security" in top-tier security/software engineering/programming language venues from the past three years (up to June 2024), yielding 268 papers on smart contract security. Excluding papers unrelated to vulnerability fixing, 15 relevant papers were derived.</li>
</ul>
<p>From these papers, we identified 9 baseline candidates, including SGuard [15], SGuard+ [78], SmartShield [16],</p>
<p>TABLE III: Repair Results for the Popular Base LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">$#$ Generated</th>
<th style="text-align: right;">$#$ Success</th>
<th style="text-align: right;">Rate $_{\text {gen }}$</th>
<th style="text-align: right;">Rate $_{\text {success }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: right;">$\mathbf{1 1 8}$</td>
<td style="text-align: right;">$\mathbf{1 1 2}$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0 0 \%}$</td>
<td style="text-align: right;">$\mathbf{9 4 . 9 2 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: right;">115</td>
<td style="text-align: right;">66</td>
<td style="text-align: right;">$97.46 \%$</td>
<td style="text-align: right;">$55.93 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7b</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">$95.76 \%$</td>
<td style="text-align: right;">$49.15 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama3-8b</td>
<td style="text-align: right;">117</td>
<td style="text-align: right;">87</td>
<td style="text-align: right;">$99.15 \%$</td>
<td style="text-align: right;">$73.72 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.2-11b</td>
<td style="text-align: right;">118</td>
<td style="text-align: right;">95</td>
<td style="text-align: right;">$100.00 \%$</td>
<td style="text-align: right;">$80.51 \%$</td>
</tr>
</tbody>
</table>
<p>$#$ Generated is the number of cases in which patches were generated. $#$ Success is the number of cases that a correct patch is successfully generated passing 3 metrics.</p>
<p>SCRepair [17], Elysium [19], Aroc [18], HCC [79], EVMPatch [20], SmartFix (2023) [14], ContractTinker [80], and LLMSmartSec [81]. We then excluded three tools that were either inapplicable for our patch generation or unavailable, and four tools that only work on bytecode, resulting in a final list of three repair tools for source code. Specifically, the artifact for HCC is not available. Since SmartShield, Aroc, and Elysium are designed exclusively for bytecode repair, they were omitted from our comparative study. Meanwhile, SCRepair requires manually curated unit tests for patch generation, a resource that our dataset lacks. Among these tools, only SGuard and SmartFix have available artifacts and are capable of accepting source code and repairing AC vulnerabilities, leading to their inclusion in our analysis. ContractTinker is an LLM-based smart contract repair tool designed to handle various vulnerability types without being limited to a specific category. We included it in our evaluation by adapting its input pipeline to process vulnerability descriptions in our dataset. The modified ContractTinker is denoted as ContractTinker*. LLMSmartSec is another recent LLM-based approach aimed at secure smart contract generation and repair. However, at the time of our evaluation, it lacked runnable artifacts and clear documentation, making reproduction infeasible. SGuard+ extends the rule-based repair engine of SGuard with enhanced capabilities. However, it does not provide public implementation or configuration files, making it impractical to replicate its repair logic without introducing bias. Therefore, among these, SmartFix, SGuard, and ContractTinker were included as baselines in our evaluation to ensure a fair and reproducible comparison.</p>
<h2>A. RQ1: Pilot Study to Identify Suitable LLM</h2>
<p>Given the various available LLMs, we first tested several popular and state-of-the-art models, including GPT-4 [33], GPT-3.5 [34], Mistral [35], and LLaMA 3 [36], to select the base LLM for ACFIX. In our updated evaluation, we further integrated LLaMA 3.2-11B, the latest lightweight variant of the LLaMA family, to enable a fair and up-to-date comparison with GPT-4. All LLMs were implemented under the same evaluation pipeline, with the only differences being in output formatting. The OpenAI API allows for structured response formatting [82], making output parsing straightforward for GPT-4 and GPT-3.5. In contrast, although we explicitly instructed Mistral and LLaMA models to respond in JSON
format, consistent compliance could not be guaranteed. Therefore, we implemented a robust string-based parser to reliably extract structured outputs across all models. We chose not to include GPT-4o in this comparison due to potential concerns around training-time data leakage and limited control over evaluation consistency. Including such models may lead to non-reproducible or unfair results, particularly in securitysensitive tasks like access control repair. The comparative results between GPT-4 and LLaMA 3.2-11B are highlighted in Table III, showcasing their respective performance in terms of patch accuracy, runtime, and model responsiveness.</p>
<p>To avoid data leakage, we selected the LLMs with the earliest cutoff dates. For GPT-4, the model was GPT-4-0613 (training data up to September 2021). GPT3.5 was GPT-3.5-turbo (also up to September 2021). As Mistral and Llama3 were released more recently, the earliest models that we could find were from October 2023 and May 2024, respectively. Therefore, these two models were trained with newer data, potentially leading to data leakage and enhancing their capabilities in evaluation. The configurations for these LLMs were all set to a temperature of 0 (to suppress randomness) and a maximum of 4096 tokens for output.</p>
<p>Table III presents the results of comparison among LLMs. The GPT-4 model has demonstrated an excellent ability to provide correct patches for AC vulnerabilities, as evidenced by its much higher Rate $_{\text {success }}$. This proficiency stems from its reasoning ability to deduce the proper role-permission pairs. After manually reviewing the failed cases of other LLMs, most were found to be caused by over-fitting roles, such as the owner of the contract. The difference in selected rolepermission pairs among LLMs has exhibited their varying abilities to summarize roles by understanding the source code context. Another major category of failed cases resulted from grammar mistakes leading to uncompilability. Cases where patches were not generated were caused by requests rejected by the LLMs to generate patches. Based on the overall results and the above analysis, other models except GPT-4 exhibit suboptimal context comprehending, unreliable generated code, and rejected requests. Thus we selected GPT-4 as the base model for ACFIX, as mentioned earlier in §II.</p>
<p>Answer to RQ1: Given its superior performance in generating appropriate patches for AC vulnerabilities compared to three other popular LLMs, GPT-4 was chosen as the base model for ACFIX.</p>
<h2>B. RQ2: Evaluating ACFIX and SOTA Tools</h2>
<p>ACFIX, SmartFix, and SGuard were run on our benchmark dataset to generate patches. We first checked the compilability of the patches. Then, we evaluated the correctness of the patches using three metrics. We introduced the term Generate Rate (Rate $<em _success="{success" _text="\text">{\text {gen }}$ ) to denote the percentage of generated patches across all cases, and Success Rate (Rate $</em>$ ) to represent the proportion of patches that meet the three criteria of the stipulated metrics as successful repairs. As illustrated in Table IV, ACFIX was able to generate patches for all 118 cases, with 112 of them considered successful repairs, resulting}</p>
<p>in a Success Rate of $94.92 \%$. In contrast, SGuard could only generate patches for 6 case, and SmartFix for 21 cases. The analysis of results and reasons behind the performance of all tools will be elaborated upon.
Analysis of Results from ACFIX. Out of the 112 successfully repaired cases, their compilability was checked against 43 cases that were already compilable before patching. It turned out that all of them could be successfully compiled with the corresponding Solidity versions. As for the 6 unsuccessful repair cases, we categorized them into four reasons: (4 cases) Over-protection (overfitting) : ACFIX returned repairs that could potentially hinder the routine usage of certain users. For example, ACFIX repaired a contract that allowed anyone to steal the collateral of loaners by adding an onlyOwner modifier, which restricted access from normal loaners who were supposed to be authorized to claim their own collaterals. One case was caused by insufficient context provided from the context extraction step, so GPT-4 could not recognize the correct permissions. The other three were caused by the strict validator that prefers conservative measures. (1 case) Different from Real Fixes: For most cases with real fixes, ACFIX performed well by providing the same protection as the real fixes. However, there was a case where the real fixes considered non-code information, which ACFIX could not predict from merely a code-based context. For example, the function safeTransferFrom was changed to internal from external after fixing, without any clear reason provided in the code. This change could potentially overfit against legitimate users. (1 case) Unclear Requirements of the Description: The description of this vulnerability indicated only insufficient checks for potential users. Indeed, it required multiple checks for the arguments in addition to the msg.sender to ensure proper functionality. ACFIX failed to provide sufficient checks for this vulnerability.
Analysis of Results from SOTA Tools. As illustrated in Table IV, SGuard [15] could only generate fixes for 6 cases, and 1 of them passed the three metrics. SmartFix [14] managed to generate 21 fixes with 7 successful ones. The primary reason for the failed cases of both tools is compilation failure because they depend on IR derived from compiled code. However, sources for some AC vulnerability cases have not released onchain addresses but only vulnerable code snippets. Even when addresses are provided, the source code may not be disclosed by blockchain explorers such as Etherscan [83]. The analysis of the tools is elaborated as follows:
SGuard: All cases that were not generated were due to unsuccessful compilation, as logged by SGuard. Out of the 6 patches generated by SGuard, 5 failed to repair the AC vulnerability. Four of these failed cases had patches that were exactly the same as the vulnerable code, indicating that SGuard failed to identify the necessary fixes. For the remaining case, SGuard provided a fix that was irrelevant to AC. The only case correctly repaired involved the misuse of tx.origin, suggesting that SGuard was specifically designed to address tx.origin misuse in the context of AC vulnerabilities.
SmartFix: SmartFix generated patches for 21 cases, accounting for $17.80 \%$ of AC vulnerabilities. However, only 7 of them successfully fixed AC vulnerabilities, all of which were cases</p>
<p>TABLE IV: Repair Results of Tools in the Benchmark Dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tool</th>
<th style="text-align: right;">#Generated</th>
<th style="text-align: right;">#Success</th>
<th style="text-align: right;">Rate $_{\text {gen }}$</th>
<th style="text-align: right;">Rate $_{\text {success }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ACFIX</td>
<td style="text-align: right;">$\mathbf{1 1 8}$</td>
<td style="text-align: right;">$\mathbf{1 1 2}$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0 0 \%}$</td>
<td style="text-align: right;">$\mathbf{9 4 . 9 2 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">SGuard</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">$5.08 \%$</td>
<td style="text-align: right;">$0.85 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SmartFix</td>
<td style="text-align: right;">21</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">$17.80 \%$</td>
<td style="text-align: right;">$5.93 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ContractTinker*</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">$5.08 \%$</td>
<td style="text-align: right;">$0.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">W/o ACG</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">106</td>
<td style="text-align: right;">$95.76 \%$</td>
<td style="text-align: right;">$89.83 \%$</td>
</tr>
<tr>
<td style="text-align: left;">W/o RBAC</td>
<td style="text-align: right;">118</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">$100.00 \%$</td>
<td style="text-align: right;">$68.64 \%$</td>
</tr>
<tr>
<td style="text-align: left;">W/o Validator</td>
<td style="text-align: right;">118</td>
<td style="text-align: right;">103</td>
<td style="text-align: right;">$100.00 \%$</td>
<td style="text-align: right;">$87.28 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Vanilla GPT-4</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">$95.76 \%$</td>
<td style="text-align: right;">$52.54 \%$</td>
</tr>
</tbody>
</table>
<p>of misuse of tx.origin. Among the unsuccessful repairs, none of the 14 cases were related to tx.origin but to other types, as illustrated in Fig. 8d. Out of 14 unsuccessful patches, 13 targeted other non-AC vulnerabilities, including 12 cases of Integer Over/underflow and 1 case of Reentrancy, but left AC vulnerabilities unrepaired, which did not exist in the original contracts upon manually examination. SmartFix only accurately identified the AC vulnerabilities in two cases, both related to re-initialization issues. In these cases, SmartFix replaced the incorrectly named constructor function with the Solidity keyword constructor, without considering that the pragma versions were both ^4.x.x, which does not support the constructor keyword. As this fix would lead to compilation failure, we labeled them as unsuccessful fixes. The overall result shows that SmartFix was designed to repair AC vulnerabilities, but its effectiveness is limited to types of AC such as re-initialization and misuse of tx.origin.
ContractTinker<em>: Among the 118 cases in our dataset, only 43 contracts were compilable and thus eligible for processing due to ContractTinker</em>'s reliance on Slither for code analysis. Moreover, the tool expects structured vulnerability reports (e.g., 'HighRiskFindings'), which were not consistently available in our dataset. After modifying its input pipeline to accept natural-language vulnerability descriptions, ContractTinker<em> generated patches for only 6 functions across 8 output files, accounting for just $5.08 \%$ of the total dataset. Manual inspection of the outputs revealed that none of the generated patches correctly repaired the access control vulnerabilities. Only one fix is related to AC but produces an overfitting change, i.e., replacing 'public' with 'private'. The rest of the fixes were unrelated, including zero-address checks and balance checks. Therefore, the effective success rate of ContractTinker</em> in repairing AC vulnerabilities in our benchmark was $0 \%$. We attribute this low effectiveness to ContractTinker*'s design, which is built around a direct conversation with the LLM, without external knowledge integration or domain-specific context. As a result, its performance heavily depends on the availability of clean, structured vulnerability reports. However, in practice, such structured reports are often inconsistent or missing entirely, limiting the tool's applicability in real-world repair scenarios.</p>
<p>Answer to RQ2: ACFIX successfully generated repairs for $100 \%$ of AC vulnerabilities, effectively fixing 112 cases, representing a $94.92 \%$ success rate. This demonstrates that ACFIX can repair the majority of AC vulner-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 8: Effectiveness of Tools on Various Data Sources.</p>
<p>abilities across a variety of scenarios. It also outperforms SOTA contract repair tools, SGuard and SmartFix, which only successfully repaired the misuse of t.x. origin and could not handle AC vulnerabilities in broader scenarios.</p>
<h3><em>C. RQ3: Ablation Study</em></h3>
<p>To demonstrate the effectiveness of the RBAC taxonomy, context information, and the MAD mechanism, we conducted an ablation study based on four customized baselines. We iteratively removed individual components of ACFIX. Specifically, <em>W/o ACG</em> has the same implementation as ACFIX but without ACG. <em>W/o RBAC</em> lacks the RBAC taxonomy. <em>W/o Validator</em> solely utilizes <em>generator</em> without <em>validator</em>. <em>Vanilla GPT-4</em> uses GPT-4-0613 with raw vulnerable code and vulnerability descriptions directly, without preprocessing, as in Figure 9.</p>
<p>As shown in Table IV, <em>W/o ACG</em> and <em>Vanilla GPT-4</em> generated patches for 113 cases instead of 118 because five contracts have multiple source code files that exceeded the token limit. Furthermore, <em>W/o ACG</em> fixed 106 cases, indicating that ACG contributed to 12 more successful cases. The number is not significant as most of the contracts were retrieved from auditing reports, which have limited length. However, ACFIX could benefit more from ACG in terms of scalability for real-world deployed contracts. The 81 successfully fixed cases demonstrate that RBAC taxonomy has significantly contributed to the patch generation. The taxonomy can be dynamically updated when new pairs are encountered by ACFIX, such that <em>Admin-Low-level Call</em> was added by ACFIX during evaluation. It has been substantiated that GPT performs better for patch generation if the generation is guided by well-structured knowledge.</p>
<p><strong>Answer to RQ3 for <em>W/o ACG</em> and <em>W/o RBAC</em>:</strong> The comparison with these two baselines have substantiated that ACG and the RBAC taxonomy could improve the repair by fixing an additional 12 and 37 cases, respectively.</p>
<p>For <em>W/o Validator</em>, without <em>validator</em>, 15 patches were not generated correctly. It was observed that <em>validator</em> successfully validated 9 more patches, resulting in correct patches. The errors in 5 of these patches were previously due to misalignment with the vulnerability description, while another 4 were due to overfitting roles. Fortunately, they were corrected after review by <em>validator</em>, meaning that MAD can effectively correct improper patches through independent evaluation.</p>
<p>Regarding the number of MAD loops, Within the 118 cases, ACFIX completed the generation after 0, 1, 2, and 3 reattempts for 41, 68, 7, and 2 cases, respectively. 92.37% of cases were completed within 1 attempt. This demonstrates that MAD typically converges quickly within 3 loops.</p>
<p>However, <em>validator</em> was observed to introduce over-fitting patches in some instances, in addition to correcting others. ACFIX failed in 3 cases due to over-fitting checks. After scrutinizing the history of debates between agents, it was found that the patches were initially correct as generated by <em>generator</em>. However, <em>generator</em> was persuaded to adopt conservative roles like owner by <em>validator</em> after debate. Therefore, even with <em>validator</em>, determining the appropriate role-permission pair is still challenging. Still, <em>validator</em> could effectively safeguard the output according to the evaluation.</p>
<p><strong>Answer to RQ3 for <em>W/o Validator</em>:</strong> <em>W/o Validator</em> failed to fix 9 cases compared to ACFIX, suggesting that Ratesuccess could be further boosted with <em>validator</em>.</p>
<p><em>Vanilla GPT-4</em> has successfully repaired 62 cases (52.54%). We manually analyzed the distribution of the repaired cases and found that <em>Vanilla GPT-4</em> tends to apply conservative roles in the repairs (68.64% of the total), such as onlyOwner. For 40 out of the 60 successful cases, <em>Vanilla GPT-4</em> generated repairs using onlyOwner. In another 17 cases, the ideal roles were specified in the vulnerability descriptions, allowing <em>Vanilla GPT-4</em> to directly reuse the given roles. For the remaining 3 cases, the function signatures themselves provided enough context for GPT-4 to infer potential roles, such as <em>borrower</em> from the function borrow. In contrast, out of the 58 incorrect repairs, 37 were inaccurately over-protected by onlyOwner, affecting legitimate users. The rest of the cases were deemed improper because they were either still vulnerable or uncompilable.</p>
<p>|  Baseline Tool Using Vanilla GPT-4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |</p>
<p>Answer to RQ3 for Vanilla GPT-4: Without the RBAC taxonomy and ACG, Vanilla GPT-4 achieved a repair success rate of only $52.54 \%$. This highlights the vital importance of the ACG mined by ACFIX from the code and the guidance provided by the RBAC taxonomy.</p>
<p>Besides the two baselines, we further explored the effectiveness of generator rule checks. Patches of 4 cases violated the rules in §VI-A. Upon manual inspection, it was determined that 2 cases involved incompatible pragma versions, and the other 2 were related to mis-spelled variable names, which could be attributed to LLM hallucination or loss of focus [27]. However, they did not affect the effectiveness of ACFIX, considering that the rule checks could safeguard the output.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 10: Proportion of successful repairs by roles.</p>
<p>The complexity of repairing AC vulnerabilities depends largely on accurately distinguishing nuanced role differences and interpreting the code context, resulting in varied difficulty levels across our benchmark dataset. To better understand ACFIX's performance under different complexity scenarios, we categorized and analyzed benchmark cases based on the roles. After manually annotating the appropriate role-permission pairs for each vulnerability in the benchmark dataset, we further categorized the 118 AC vulnerabilities according to their corresponding roles.</p>
<p>In this evaluation, the ground truths of our benchmark dataset were mapped to 31 out of 49 entries of the taxonomy, demonstrating that our taxonomy captures a wide range of AC patterns and is not overfitted to a narrow scope. This further confirms the dataset's diversity and the generality of the taxonomy itself. As permissions may vary from case to case, we focused Fig. 10 solely on eight major roles regarding the proportion of successful repairs. It was observed that three major roles-Owner of the Contract (OC), Owner of Funds (OF), and Admin-account for the majority ( $77.12 \%$ ) of the AC vulnerability benchmark dataset. Generally, ACFIX achieved the best repairs across the eight roles, but its performance for the roles of $O C$ and $A d m i n$ was less effective. These roles usually have the broadest range of permissions, and validator tends to encourage generator to adopt conservative roles, such as $O C$ and $A d m i n$. This is evidenced by W/o Validator, which achieved slightly better results for the role of $O C$ ( $98 \%$ v.s. $94 \%$ ). Since Vanilla GPT-4 lacks refined context, it performs worse than ACFIX across all roles.</p>
<p>TABLE V: The Q0 analysis results for 40 positive (TP/FP) cases reported by three SOTA AC vulnerability detectors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Detectors</th>
<th style="text-align: left;">All TP</th>
<th style="text-align: left;">Slither FP</th>
<th style="text-align: left;">SoMo FP</th>
<th style="text-align: left;">SPCon FP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Correct/Total</td>
<td style="text-align: left;">$21 / 21$</td>
<td style="text-align: left;">$7 / 10$</td>
<td style="text-align: left;">$10 / 10$</td>
<td style="text-align: left;">$9 / 10$</td>
</tr>
</tbody>
</table>
<p>Answer to RQ4: ACFIX struggled with the roles of $O C$ and Admin but still outperformed Vanilla GPT-4 across all roles. On the other hand, SmartFix was only able to repair $17 \%$ of the Initialization cases.</p>
<h2>E. RQ5: Practicality Analysis</h2>
<p>As described in §III, ACFIX is designed to function as a copilot for processing vulnerability reports generated by external AC vulnerability detectors. It is not intended to serve as a standalone detector, instead, it operates downstream by consuming flagged outputs, such as potentially vulnerable functions, and assisting in precise vulnerability localization and automated patch generation. Given the high computational cost of large-scale contract analysis, ACFIX is deliberately scoped to handle a limited set of contracts identified by existing detection tools. To evaluate its effectiveness in this role, particularly its ability to accurately confirm AC vulnerabilities and reduce human verification effort, we assessed the Q0 step of ACFIX on cases reported by three state-of-the-art AC vulnerability detectors.</p>
<p>We selected three tools that publicly provide labeled TP and FP cases: Slither [66], SoMo [13], and SPCon [12]. AChecker [9] was excluded due to the lack of a publicly available dataset. For true positives, we carefully curated 21 TP cases confirmed by the original tools in their released datasets. Since these tools independently verified these vulnerabilities, they serve as a reliable basis for TP evaluation.</p>
<p>To simulate integration with vulnerability scanners, we provided ACFIX with only the vulnerable functions (as detected by the tools) and no further textual description. ACFIX was then tasked with identifying the vulnerable lines and determining whether the root cause was a missing or incorrect identity check (e.g., msg.sender). Successful identification under this setting validates ACFIX 's capability to localize and explain the vulnerability, making it a practical companion to detection tools.</p>
<p>Regarding false positives, due to the scarcity of public FP cases from SoMo, we collected 10 cases from its dataset. To maintain balance, we randomly selected 10 FP cases each from Slither and SPCon, resulting in a total of 30 FP cases. Combined with the 21 TPs, the full evaluation consists of 51 unique cases. This expanded and clarified evaluation setup strengthens the reliability and interpretability of RQ5.</p>
<p>As shown in Table V, the Q0 step of ACFIX has been evaluated on these 40 positive cases. If Q0 is able to accurately determine the actual vulnerability status of each case, the case is considered correctly identified by ACFIX. Specifically, the case is correct if Q0 returns True for a TP case and False for an FP case. It is shown that ACFIX could correctly identify most cases (36/40). Although 4 FP cases were not correctly</p>
<p>TABLE VI: Monetary and Temporal Costs of ACFix.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Avg. Token Avg. Total Price (USD) Avg. Total Time (s)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>ACFix</td>
<td>$1,956.35$</td>
<td>$0.0588 / 6.9429$</td>
<td>$30.58 / 3,608.23$</td>
</tr>
<tr>
<td>W/o ACG</td>
<td>$2,813.68$</td>
<td>$0.0843 / 9.9474$</td>
<td>$37.23 / 4,393.16$</td>
</tr>
<tr>
<td>W/o RBAC</td>
<td>$1,845.84$</td>
<td>$0.0552 / 6.5341$</td>
<td>$29.35 / 3,463.31$</td>
</tr>
<tr>
<td>W/o Validator</td>
<td>$1,777.90$</td>
<td>$0.0546 / 6.4428$</td>
<td>$25.23 / 2,977.14$</td>
</tr>
<tr>
<td>Vanilla GPT-4</td>
<td>378.79</td>
<td>$0.0192 / 2.2542$</td>
<td>$7.66 / 903.98$</td>
</tr>
</tbody>
</table>
<p>identified by Q0, no TP cases were missed by ACFix, thus ensuring that real AC vulnerabilities could be fixed.</p>
<p>Answer to RQ5: Out of 40 positive AC vulnerabilities reported by SOTA tools, ACFix correctly identified 36 of them, demonstrating its practical value in confirming and fixing AC vulnerabilities.</p>
<h2>F. RQ6: Cost Efficiency and Performance</h2>
<p>Table VI shows the monetary and time costs of using ACFix for all AC cases in the dataset. Regarding monetary cost, the average number of tokens used across two agents by ACFix is $1,956.35$. According to the current pricing plan [84], the average cost for repairing one AC vulnerability is 0.0587 USD. Consequently, repairing all vulnerabilities in the dataset costs a total of 6.9266 USD. Note that the token counts for ACFix and W/o Validator were much higher than for Vanilla GPT-4 because the costs of failed cases were not counted, and consecutive conversations require incorporating the previous history into the new prompt, which results in repeated counting of tokens. Additionally, it is evident that W/o ACG consumed more tokens than ACFix because the raw source code was not processed to highlight critical information by constructing ACG. Instead, the raw source code of contracts was incorporated in the prompt, including redundant tokens, leading to unnecessary costs and inefficiency. Regarding temporal cost, the average time for ACFix to patch each AC case is 30.58 seconds. The time required for static analysis may vary depending on each case's complexity.</p>
<p>Answer to RQ6: On average, ACFix costs 0.06 USD and takes 30.58 seconds per case.</p>
<h2>VIII. LLM-BASEd Repair vs. Human Repair</h2>
<p>Following the evaluation of ACFix itself in $\S$ VII, we further proceed to understand the value of ACFix's repairs from a human perspective, e.g., how they align with human repairs and whether they are non-trivial to devise by humans (if nontrivial, this means that ACFix provides a unique complement to assist human-in-the-loop repair as a copilot).</p>
<p>Towards this objective, we conducted a human-based evaluation involving 10 practioners who have worked on smart contract auditing for 2-7 years. They are divided into junior (2-4 years) and senior (4-7 years) groups. Given the raw source code and vulnerability description, the participants were asked two questions: (1) Write down the most appropriate rolepermission pairs they thought fit the situation; (2) Indicate if
the patch is straightforward to come up with based on their understanding. For (2), unless explicitly stated, they were not asked to produce patches but only to assess the difficulty, because manually curated patches are hard to normalize for comparison. Note that as this study does not involve any personally identifiable information, the IRB (Institutional Review Board) requirement was waived by our institution.</p>
<h2>A. How ACFix's Repairs Align with Humans</h2>
<p>After manually scrutinizing the role-permission pairs curated by experts, $83(74 \%)$ and $69(62 \%)$ pairs by senior and junior experts respectively were aligned with pairs produced by ACFix for 112 corrected cases. Despite the different proportions, we carefully reviewed the curated pairs and derived several findings. (1) Humans are more likely to reuse existing roles if the provided source code is not lengthy. Hence, the pairs are mostly aligned for cases with existing roles defined in the source code. (2) Humans tend to reuse function names as permissions without distilling them into abstract permissions as in our RBAC taxonomy. This phenomenon is especially evident for junior experts. It might indicate that humans require training and experience to accurately identify and summarize the proper role-permission pairs for correct patches. (3) Humans are inclined to give conservative roles, such as owner, admin, and authorized user. On average, 96.4 ( $81.69 \%$ ) and 79.0 ( $66.95 \%$ ) of the roles given by junior and senior experts respectively were conservative, contrasting with the $64(54.24 \%)$ returned by ACFix.</p>
<p>These experts were further asked to draft patches for 6 failed cases by ACFix. After validating their patches with the same metrics, it turned out that half of their patches were correct according to all metrics. We took Quixotic [85] as an example to demonstrate the difference. Its brief vulnerability description is Quixotic checks only the buyer's signature. The vulnerable function fillSellOrder has multiple arguments and only checks the buyer's identity. Human experts were able to construct patches involving the AC checks against buyer as well as other necessary argument checks, such as expiration and price, according to their auditing experience. However, ACFix strictly included only the buyer role without flexibly involving other necessary checks.</p>
<p>Takeaway: ACFix's repairs are mostly aligned with those of humans and are finer-grained than those of both senior and junior experts. However, in rare cases (3/118), human experts are better at handling open issues based on their knowledge and experience without much guidance.</p>
<h2>B. Fixes are Non-trivial to Devise by Humans</h2>
<p>We further assess whether the 118 AC fixes attempted by ACFix are non-trivial to devise by humans based on the results of the surveyed second question. The results indicated that on average, junior and senior experts respectively identified 58.0 ( $49.15 \%$ ) and 53.5 ( $45.34 \%$ ) NO cases that are not straightforward to propose a patch. Based on majority voting, there were 42 (35.59\%) YES cases in which most experts agreed on the straightforwardness of patch instrumentation.</p>
<p>After manually inspecting them, we found they mostly ( $88 \%$ ) belong to initialization and changes of ownership, which are straightforward to fix because only the owner of the contract should be checked. For the rest of the nontrivial cases, various role-permission pairs were included. For instance, Mint, which could be subject to abundant rules to implement the AC policy, is not straightforward to fix. Another example is the Guardian role [86] for ERC20. In this case, the original contract has three roles, Guardian, Governor, and Minter, which already form a hierarchy. To provide a proper fix, the practitioner must understand the hierarchy and the functionality of the target function, which could be a laborious and complicated task. These non-trivial cases could be effectively tackled by ACFIX given that it has learned various rules and understood the hierarchical relationships of existing RBAC.
Case Study. We selected a case successfully repaired by ACFIX, which was agreed by all participants to be tough to fix, to demonstrate ACFIX's advantage. The case is the motivating example in Fig. 1 [41]. The fix is complicated because one has to understand the implicit design of the external function depositFromOtherContract. We first explain the reason behind the correct patch: As there is already a deposit function to deposit one's own values belonging to msg.sender, the vulnerable function depositFromOtherContract takes in an argument _from to deposit values on behalf of other users. However, depositing on behalf of others requires a trustworthy authority to act as a centralized agency. There are two trusted addresses defined in this contract, namely, owner and bank. Given that bank is set by owner, the proper role within the context to manage deposits is bank, which is exactly how the original author fixed it.</p>
<p>From the above case, we derive several challenges of manual patching: (1) Going through existing functions and distinguishing them from each other regarding the desired functionality, i.e., depositFromOtherContract and deposit; (2) Understanding the existing RBAC hierarchy based on the implementation of the chain of trust, i.e., owner and bank; (3) Understanding the implicit relationship between the design of a centralized agency and the existing role bank. In response to these challenges, ACFIX could effectively mine the existing RBAC roles and implementations of the two existing deposit functions. Then, GPT-4 could understand the implicit logic between them to address challenges (2) and (3).</p>
<p>Takeaway: Around half of the AC fixes are non-trivial to devise by humans, indicating that ACFIX can provide a unique complement to assist human-in-the-loop repair as a copilot. Through a case study, we conclude that with the aid of an LLM, the implicit logic can be dissected and streamlined from the source code, which is imperative for generating proper patches for AC vulnerabilities.</p>
<h2>IX. ThREATS OF VALIDITY</h2>
<p>Internal Threats: The primary threat to ACFIX is the precision of static analysis. As ACFIX mostly relies on Antlr
to resolve dependency relationships of code statements using AST, rather than IR, ACG may not achieve high precision and recall. However, this potential inaccuracy does not markedly affect ACFIX's capabilities for two reasons. First, the selection of role-permission pairs primarily depends on GPT-4's logical reasoning capabilities, provided there is sufficient context to infer role and permission. Second, in most cases, ACFIX performs static analysis within a single contract file. This means that most of the call graphs, def-use chains of state variables, and mappings between parameters of functions could reliably rely on name mappings. Therefore, the static analysis in ACFIX may be flawed, but it suffices to support context understanding of GPT-4.</p>
<p>An internal threat to validity stems from the dataset used for evaluating AC vulnerabilities, which was gathered primarily from limited online sources, specifically DefiHackLabs [70] and tintinweb [71], resulting in unequal representation and potential incompleteness. Such imbalanced distributions across data sources might inadvertently introduce biases. To mitigate this issue, we designed RQ4 explicitly to analyze the performance of ACFIX and baseline methods within individual categories, thereby reducing sensitivity to data imbalance. Additionally, we made considerable efforts to include as many cases as possible from diverse Internet sources, enlarging the dataset to facilitate a more comprehensive and fair comparison.</p>
<p>The last external threat to validity lies in the interpretability of LLMs like GPT-4, which ACFIX relies on. Due to their black-box nature, LLMs may generate outputs that are difficult to explain or verify, potentially introducing incorrect or inconsistent repairs. Issues such as hallucination, prompt sensitivity, and lack of transparency in reasoning pose risks, particularly in security-critical contexts like access control. To mitigate these concerns, we constrain the LLM's output space using a dynamic taxonomy of RBAC role-permission pairs, reducing the likelihood of invalid predictions. Additionally, we integrate static analysis checks to validate the syntactic and semantic correctness of generated patches. We also employ carefully crafted in-context prompts to enhance stability and reduce variation across similar cases. Furthermore, ACFIX incorporates a dual-agent feedback framework, where a validation agent assesses the generated output and provides feedback to the generator, enabling iterative refinement. Together, these mechanisms help enhance interpretability and reliability, though we acknowledge that challenges inherent to LLMs remain an open research issue.</p>
<p>External Threat: The potential threat to validity arises from the initial reliance on a manually curated RBAC taxonomy. Due to inherent constraints in the available dataset, this taxonomy may not comprehensively represent all possible role-permission relationships encountered in real-world smart contract implementations. Such incompleteness could potentially lead to inaccuracies or misidentifications of rolepermission pairs during vulnerability repair. To address this limitation, we incorporated an adaptive mechanism within our approach, enabling the automatic addition of newly identified role-permission pairs to dynamically expand the taxonomy. This strategy effectively mitigates the risks posed by a static,</p>
<p>finite taxonomy, ensuring greater robustness and adaptability of the proposed solution.</p>
<h2>X. Related Work</h2>
<h2>A. Smart Contract Repair</h2>
<p>Smart contract vulnerability repair has seen significant progress, such as Aroc [18], SmartShield [16], SGuard [15], Elysium [19], SCRepair [17], and SmartFix [14]. However, research on repairing AC-related vulnerabilities remains limited. Tools like Aroc and SmartShield do not support AC repairs, SGuard addresses only tx.origin misuse, and Elysium fixes only two sensitive operations. SCRepair's effectiveness is constrained by manual unit tests, while SmartFix handles only tx.origin and re-initialization vulnerabilities.</p>
<p>In light of the above, ACFIX stands out in two ways: (1) Human-Level Reasoning: We address and resolve the limitations inherent in prior works that relied solely on predefined templates. By utilizing GPT-4, our method engages in conversational sessions employing CoT and MAD, which allows ACFIX to achieve human-like reasoning. This marks a significant advancement in the methodology for AC vulnerability repairs. (2) Comprehensive Coverage: Many existing tools support AC vulnerabilities but are often restricted to handling conventional patterns. In contrast, ACFIX boasts the capability to address AC vulnerabilities across diverse scenarios.</p>
<h2>B. Traditional Program Repair</h2>
<p>Numerous works have focused on repairing bugs or vulnerabilities in traditional software [87], [88], [42], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], especially in C [91], [92], [93], Java [94], [90], and PHP [87]. Moreover, several concurrent works [88], [42], [89], [90], [99], [100] propose LLM-based APR solutions for bug fixes. For example, Xia et al. [88] studied the effectiveness of LLMs for APR and found that LLMs generally outperform traditional approaches. ChatRepair [89] employs multiple sessions for interactive repair with GPT-4. Repilot [90] innovatively combines the completion engine with LLM to synergistically generate patches. FitRepair [42] leverages the plastic surgery hypothesis to repair bugs using existing code ingredients by performing static analysis and information retrieval on the source code. Other related works [91], [92], [93], [94], [95], [96], [97], [98] mostly employ traditional methods, such as Neural Machine Translation, to synthesize repairs for bugs or iteratively search for proper patches. Our work shares several common practices, such as conversational sessions and existing ingredient reuse, but uniquely mines RBAC practices and relevant code context to guide LLMs.</p>
<h2>XI. Future Work</h2>
<p>Building upon the insights and infrastructure established by ACFIX, we identify several promising directions for future exploration:</p>
<ul>
<li>LLM-guided Repair of Advanced Vulnerabilities. While this work focuses on AC vulnerabilities, our approach establishes a foundation for addressing a broader class of complex smart contract vulnerabilities. In future work, we plan to extend ACFIX's reasoning and repair capabilities to additional vulnerability types that require deep semantic understanding, such as reentrancy with indirect triggers, improper state transitions, delegatecall misuse, and incorrect payment logic. These categories often involve non-trivial control flow, cross-contract dependencies, or subtle logic flaws that static patterns alone cannot effectively capture. Enhancing ACFIX with formal specifications, symbolic reasoning, or integration with domain-specific ontologies may further improve its adaptability and accuracy.</li>
<li>Detection and Repair of Multi-Function AC Vulnerabilities. A more advanced but rare class of AC vulnerabilities involves multiple functions collectively contributing to unauthorized privilege escalation. These vulnerabilities are especially challenging, as they may not exhibit direct or transitive call relationships but instead share critical state variables that facilitate cross-function interactions. We intend to investigate this class of vulnerabilities by modeling statedependent attack surfaces and designing analysis techniques to identify latent privilege escalation paths that span disjointed code regions. This form of vulnerability is rare but significantly harder to detect and mitigate.</li>
<li>Adaptive Taxonomy Evolution via Online Learning. While ACFIX leverages a dynamic taxonomy mined from a large corpus of on-chain contracts, smart contract development practices continue to evolve, introducing new roles, patterns, and access semantics. This new emerging knowledge may not be accommodated well by merely updating the taxonomy with more RBAC pairs. To maintain robustness and adaptability, we envision extending the current static RBAC taxonomy into a dynamic, Retrieval-Augmented Generation (RAG) system. In this setting, the LLM would query an updatable knowledge base of role-permission pairs-continuously refined from emerging contracts, validator feedback, and user interaction logs-enabling it to incorporate the latest access control practices during repair. This dynamic integration would reduce reliance on static assumptions, enhance coverage of long-tail or novel RBAC cases, and provide a pathway for ACFIX to generalize beyond its original training distribution with minimal human intervention.</li>
<li>LLM-guided AC Vulnerability Detection. With the mined RBAC taxonomy and enhanced context comprehension mechanisms, we plan to extend the scope of ACFIX from repair to detection. By leveraging LLMs' ability to semantically understand AC context, although ACFix has advanced the usage of LLM on smart contract security repair, we aim to further detect improper RBAC implementation by identifying role-permission mismatches relative to the intended functionality of contracts. However, vulnerability detection presents unique challenges compared to repair, particularly in terms of scalability. Unlike repair, which starts from known vulnerable functions, detection must assume that any function could be misconfigured and thus requires comprehensive analysis across the entire contract. This substantially increases</li>
</ul>
<p>computational costs, as LLMs must perform intricate RBAC reasoning for all functions. To address this, we plan to incorporate static filtering techniques to preselect likelyvulnerable candidates, enabling scalable and efficient LLMguided detection without exhaustive analysis.</p>
<ul>
<li>Advanced Validation Paradigm. We plan to further advance the validation process by exploring more sophisticated MAD frameworks. In particular, we aim to incorporate additional specialized agents, such as adversarial critics and domainspecific oracles, to enrich the debate dynamics and improve the reliability of patch validation. We also intend to investigate adaptive debate strategies, where the validation process dynamically adjusts the roles or number of agents based on the complexity of the repair task. These directions are expected to enhance both the robustness and explainability of the validation stage, and will be integrated into future iterations of ACFIX.</li>
</ul>
<h2>XII. CONCLUSION</h2>
<p>This paper proposed ACFIX for repairing AC vulnerabilities in smart contracts by guiding LLMs with AC practices and code context. We developed an RBAC taxonomy from onchain contracts and a slicing algorithm to extract AC-related context. Equipped with check rules and validator of MAD, ACFIX repaired $94.92 \%$ of cases in our dataset, outperforming existing tools. Our evaluation included a human study to assess the quality of ACFIX's repairs compared to humans'.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>This research is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 1 (RG96/23). It is also supported by the National Research Foundation, Singapore, and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-GC-2023008); by the National Research Foundation Singapore and the Cyber Security Agency under the National Cybersecurity R\&amp;D Programme (NCRP25-P04-TAICeN); and by the Prime Minister's Office, Singapore under the Campus for Research Excellence and Technological Enterprise (CREATE) Programme. Any opinions, findings and conclusions, or recommendations expressed in these materials are those of the author(s) and do not reflect the views of the National Research Foundation, Singapore, Cyber Security Agency of Singapore, Singapore.</p>
<h2>OPEN SCIENCE POLICY</h2>
<p>To facilitate replication and future research, we have released our source code and dataset on an anonymous website [75].</p>
<h2>REFERENCES</h2>
<p>[1] G. Wood et al., "Ethereum: A secure decentralised generalised transaction ledger," Ethereum project yellow paper, vol. 151, no. 2014, pp. $1-32,2014$.
[2] S. Werner, D. Perez, L. Gudgeon, A. Klages-Mundt, D. Harz, and W. Knottenbelt, "Sok: Decentralized finance (defi)," in Proceedings of the 4th ACM Conference on Advances in Financial Technologies, ser. AFT '22, 2023, p. 30-46.
[3] D. Das, P. Bose, N. Ruaro, C. Kruegel, and G. Vigna, "Understanding security issues in the nft ecosystem," in Proceedings of the 2022 ACM CCS, ser. CCS '22. New York, NY, USA: Association for Computing Machinery, 2022, p. 667-681. [Online]. Available: https://doi.org/10.1145/3548606.3559342
[4] "Solidity," https://soliditylang.org/, 2023.
[5] Z. Zheng, N. Zhang, J. Su, Z. Zhong, M. Ye, and J. Chen, "Turn the rudder: A beacon of reentrancy detection for smart contracts on ethereum," arXiv preprint arXiv:2303.13770, 2023.
[6] S. Kalra, S. Goel, M. Dhawan, and S. Sharma, "Zeus: analyzing safety of smart contracts." in Ndss, 2018, pp. 1-12.
[7] P. Daian, S. Goldfeder, T. Kell, Y. Li, X. Zhao, I. Bentov, L. Breidenbach, and A. Juels, "Flash boys 2.0: Frontrunning in decentralized exchanges, miner extractable value, and consensus instability," in 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020, pp. 910-927.
[8] S. Wu, D. Wang, J. He, Y. Zhou, L. Wu, X. Yuan, Q. He, and K. Ren, "Defiranger: Detecting price manipulation attacks on defi applications," arXiv preprint arXiv:2104.15068, 2021.
[9] A. Ghaleb, J. Rubin, and K. Pattabiraman, "AChecker: Statically detecting smart contract access control vulnerabilities," Proc. ACM ICSE, 2023.
[10] "Parity Wallet Attack," https://blog.openzeppelin.com/on-the-parity-wallet-multisig-hack-405a8c12e8f7, 2017.
[11] L. Brent, N. Grech, S. Lagouvardos, B. Scholz, and Y. Smaragdakis, "Ethaimter: a smart contract security analyzer for composite vulnerabilities," in Proceedings of the 41st ACM PLDI, 2020, pp. 454-469.
[12] Y. Liu, Y. Li, S.-W. Lin, and C. Artho, "Finding permission bugs in smart contracts with role mining," in Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, 2022, pp. 716-727.
[13] Y. Fang, D. Wu, X. Yi, S. Wang, Y. Chen, M. Chen, Y. Liu, and L. Jiang, "Beyond "protected" and "private": An empirical security analysis of custom function modifiers in smart contracts," in Proc. ACM IESTA, 2023.
[14] S. So and H. Oh, "Smartfix: Fixing vulnerable smart contracts by accelerating generate-and-verify repair using statistical models," in Proceedings of the 2023 31th acm sigsoft international symposium on foundations of software engineering, 2023.
[15] T. D. Nguyen, L. H. Pham, and J. Sun, "Sguard: towards fixing vulnerable smart contracts automatically," in 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021, pp. 1215-1229.
[16] Y. Zhang, S. Ma, J. Li, K. Li, S. Nepal, and D. Gu, "Smartshield: Automatic smart contract protection made easy," in 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2020, pp. 23-34.
[17] X. L. Yu, O. Al-Bataineh, D. Lo, and A. Roychoudhury, "Smart contract repair," ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 29, no. 4, pp. 1-32, 2020.
[18] H. Jin, Z. Wang, M. Wen, W. Dai, Y. Zhu, and D. Zou, "Aroc: An automatic repair framework for on-chain smart contracts," IEEE Transactions on Software Engineering, vol. 48, no. 11, pp. 4611-4629, 2022.
[19] C. Ferreira Torres, H. Jonker, and R. State, "Elysium: Contextaware bytecode-level patching to automatically heal vulnerable smart contracts," in Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses, 2022, pp. 115-128.
[20] M. Rodler, W. Li, G. O. Karame, and L. Davi, "[EVMPatch]: Timely and automated patching of ethereum smart contracts," in 30th USENIX Security Symposium (USENIX Security 21), 2021, pp. 1289-1306.
[21] "Smart Contract Initialization," https://www.cyberark.com/resources/threat-research-blog/how-to-write-a-poc-for-an-uninitialized-smart-contract-vulnerability-in-budgetdas-using-foundry, 2023.
[22] I. Nikolic, A. Kolluri, I. Sergey, P. Saxena, and A. Hobor, "Finding the greedy, prodigal, and suicidal contracts at scale," 2018.</p>
<p>[23] "Unchecked Low-level Call," https://simonbusch.medium.com/smart-contracts-vulnerability-explained-unchecked-send-ed8b5606813a, 2023.
[24] R. S. Sandhu, "Role-based access control," in Advances in computers. Elsevier, 1998, vol. 46, pp. 237-286.
[25] H. T. et al., "Llama 2: Open foundation and fine-tuned chat models," 2023.
[26] OpenAI, "Gpt-4 technical report," 2023.
[27] H. Tian, W. Lu, T. O. Li, X. Tang, S.-C. Cheung, J. Klein, and T. F. Bissyandé, "Is chatgpt the ultimate programming assistant - how far is it?" 2023.
[28] "ChatGPT Hallucination," https://fortune.com/2023/08/01/can-aichatgpt-hallucinations-be-fixed-experts-doubt-altman-openai/, 2023.
[29] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi, "Encouraging divergent thinking in large language models through multi-agent debate," arXiv preprint arXiv:2305.19118, 2023.
[30] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24 824-24 837, 2022.
[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," 2019. [Online]. Available: https://arxiv.org/abs/1810.04805
[32] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., "Improving language understanding by generative pre-training."
[33] "chatGPT," https://chat.openai.com/, 2023.
[34] "GPT3.5," https://platform.openai.com/docs/models, 2024.
[35] A. Q. J. et al., "Mistral 7b," 2023. [Online]. Available: https: //arxiv.org/abs/2310.06825
[36] "Llama3," https://huggingface.co/meta-llama/Meta-Llama-3-8B, 2024.
[37] N. Szabo, "Smart contracts: Building blocks for digital markets," Online, 1997, available online: http://www.fon.hum. uva.nl/rob/Courses/InformationInSpeech/CDROM/Literature/ LOTwinterschool2006/szabo.best.vwh.net/smart_contracts_2.html.
[38] "ERC 20," https://ethereum.org/en/developers/docs/standards/tokens/erc20/, 2024.
[39] "Openxepplin Access Control," https://github.com/OpenZeppelin/ openzeppelin-contracts/blob/master/contracts/access/AccessControl. sol, 2023.
[40] "GYMNetwork attack," https://wooded-meter-1d8.notion.site/ Incorrect-access-control-579d6806099e4304aa761ade1d1c7e, 2022, (Accessed on 26/08/2023).
[41] "Example Address," https://bscscan.com/address/0xf0288fba0bf19072d 30490a0f- 3c81cd9b0634258a#code#F1#L291, 2022.
[42] C. S. Xia, Y. Ding, and L. Zhang, "Revisiting the plastic surgery hypothesis via large language models," arXiv preprint arXiv:2303.10494, 2023.
[43] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," arXiv preprint arXiv:2302.04761, 2023. [Online]. Available: https://arxiv.org/abs/ 2302.04761
[44] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and D. Kiela, "Retrieval-augmented generation for knowledge-intensive nlp tasks," in NeurIPS, 2020.
[45] J. Baek, A. F. Aji, and A. Saffari, "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering," arXiv preprint arXiv:2306.04136, 2023.
[46] Z. Wang, S. Mao, W. Wu, T. Ge, F. Wei, and H. Ji, "Unleashing the emergent cognitive synergy in large language models: A tasksolving agent through multi-persona self-collaboration," arXiv preprint arXiv:2307.05300, 2023.
[47] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, "Improving factuality and reasoning in language models through multiagent debate," arXiv preprint arXiv:2305.14325, 2023.
[48] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou et al., "The rise and potential of large language model based agents: A survey," arXiv preprint arXiv:2309.07864, 2023.
[49] C. L. Goues, M. Pradel, and A. Roychoudhury, "Automated program repair," Communications of the ACM, vol. 62, no. 12, pp. 56-65, 2019.
[50] "Ethereum Contracts," https://github.com/tintinweb/smart-contract-sanctuary-ethereum, 2023.
[51] L. Liu, L. Wei, W. Zhang, M. Wen, Y. Liu, and S. Cheung, "Characterizing Transaction-Reverting Statements in Ethereum Smart Contracts," in Proc. IEEE ASE, 2021.
[52] "Open Card Sorting," https://en.wikipedia.org/wiki/Card_sorting, 2025.
[53] I. David, L. Zhou, K. Qin, D. Song, L. Cavallaro, and A. Gervais, "Do you still need a manual smart contract audit?" arXiv preprint arXiv:2306.12338, 2023.
[54] Y. Sun, D. Wu, Y. Xue, H. Liu, W. Ma, L. Zhang, M. Shi, and Y. Liu, "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning," arXiv preprint arXiv:2401.16185, 2024.
[55] Z. Yuan, Y. Lou, M. Liu, S. Ding, K. Wang, Y. Chen, and X. Peng, "No more manual tests? evaluating and improving chatgpt for unit test generation," arXiv preprint arXiv:2305.04207, 2023.
[56] Y. Sun, D. Wu, Y. Xue, H. Liu, H. Wang, Z. Xu, X. Xie, and Y. Liu, "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis," in Proc. ACM ICSE, 2024.
[57] C. Chen, J. Su, J. Chen, Y. Wang, T. Bi, Y. Wang, X. Lin, T. Chen, and Z. Zheng, "When chatgpt meets smart contract vulnerability detection: How far are we?" arXiv preprint arXiv:2309.05520, 2023.
[58] M. Weiser, "Program slicing," IEEE Transactions on software engineering, no. 4, pp. 352-357, 1984.
[59] A. De Lucia, A. R. Fasolino, and M. Munro, "Understanding function behaviors through program slicing," in WPC'96. 4th Workshop on Program Comprehension. IEEE, 1996, pp. 9-18.
[60] S. Badihi, F. Akinotcho, Y. Li, and J. Rubin, "Ardiff: scaling program equivalence checking via iterative abstraction and refinement of common code," in Proceedings of the 28th ACM FSE, 2020, pp. 13-24.
[61] S. Badihi, K. Ahmed, Y. Li, and J. Rubin, "Responsibility in context: On applicability of slicing in semantic regression analysis," in 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023, pp. 563-575.
[62] Y. Xiao, B. Chen, C. Yu, Z. Xu, Z. Yuan, F. Li, B. Liu, Y. Liu, W. Huo, W. Zou et al., "[MVP]: Detecting vulnerabilities using [Patch-Enhanced] vulnerability signatures," in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 1165-1182.
[63] L. Zhang, C. Liu, Z. Xu, S. Chen, L. Fan, B. Chen, and Y. Liu, "Has my release disobeyed semantic versioning? static detection based on semantic differencing," in Proceedings of the 37th IEEE/ACM ASE, ser. ASE '22, 2023.
[64] X. Yi, Y. Fang, D. Wu, and L. Jiang, "BlockScope: Detecting and Investigating Propagated Vulnerabilities in Forked Blockchain Projects," in Proc. ISOC NDSS, 2023.
[65] J. Ferrante, K. J. Ottenstein, and J. D. Warren, "The program dependence graph and its use in optimization," ACM TOPLAS, vol. 9, no. 3, pp. 319-349, 1987.
[66] J. Feist, G. Grieco, and A. Groce, "Slither: a static analysis framework for smart contracts," in 2019 IEEE/ACM 2nd International Workshop on Emerging Trends in Software Engineering for Blockchain (WETSEB). IEEE, 2019, pp. 8-15.
[67] T. J. Parr and R. W. Quong, "Antlr: A predicated-ll (k) parser generator," Software: Practice and Experience, vol. 25, no. 7, pp. 789810, 1995.</p>
<p>[68] T. Durieux, J. F. Ferreira, R. Abreu, and P. Cruz, "Empirical review of automated analysis tools on 47,587 ethereum smart contracts," in Proceedings of the ACM/IEEE 42nd ICSE, 2020, pp. 530-541.
[69] "National vulnerability database," https://nvd.nist.gov/, 2023.
[70] "Defi Hack Labs," https://github.com/SunWeb3Sec/DeFiHackLabs, 2023.
[71] "Tintinweb Vul Dataset," https://github.com/tintinweb/smart-contractvulndb/tree/main, 2023.
[72] BlockSec, "Blocksec building blockchain security infrastructure," https://blocksec.com/#blogs, 2023, (Accessed on 01/09/2023).
[73] SlowMist, "Slowmist," https://www.slowmist.com/, 2023, (Accessed on 01/09/2023).
[74] "Medium," https://medium.com/, 2023.
[75] "ACFix Website," https://sites.google.com/view/acfixsmartcontract, 2024.
[76] P. Tsankov, A. Dan, D. Drachsler-Cohen, A. Gervais, F. Buenzli, and M. Vechev, "Securify: Practical security analysis of smart contracts," in Proceedings of the 2018 ACM SIGSAC CCS, 2018, pp. 67-82.
[77] S. Tikhomirov, E. Voskresenskaya, I. Ivanitskiy, R. Takhaviev, E. Marchenko, and Y. Alexandrov, "Smartcheck: Static analysis of ethereum smart contracts," in Proceedings of the 1st international workshop on emerging trends in software engineering for blockchain, 2018, pp. 9-16.
[78] C. Gao, W. Yang, J. Ye, Y. Xue, and J. Sun, "sguard+: Machine learning guided rule-based automated vulnerability repair on smart contracts," ACM Transactions on Software Engineering and Methodology, vol. 33, no. 5, pp. 1-55, 2024.
[79] J.-R. Giesen, S. Andreina, M. Rodler, G. O. Karame, and L. Davi, "Practical mitigation of smart contract bugs," arXiv preprint arXiv:2203.00364, 2022.
[80] C. Wang, J. Zhang, J. Gao, L. Xia, Z. Guan, and Z. Chen, "Contracttinker: Llm-empowered vulnerability repair for real-world smart contracts," in Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, 2024, pp. 2350-2353.
[81] V. Mothukuri, R. M. Parizi, and J. L. Massa, "Llmsmartsec: Smart contract security auditing with llm and annotated control flow graph," in 2024 IEEE International Conference on Blockchain (Blockchain). IEEE, 2024, pp. 434-441.
[82] "OpenAI Formatting," https://platform.openai.com/docs/apireference/audio/createTranscription#audio-createtranscriptionresponse_format, 2024.
[83] "Etherscan," https://etherscan.io/, 2023.
[84] "How much does GPT-4 cost?" https://help.openai.com/en/articles/7127956-how-much-does-gpt-4-cost, 2024.
[85] "Quitoxic," https://github.com/SunWeb3Sec/DeFiHackLabs#20220701quixotic--optimism-nft-marketplace, 2020.
[86] "Guardian Role for ERC20," https://github.com/sherlock-audit/2023-04-unitasprotocol-judging/blob/main//false/126.md, 2023.
[87] S. Son, K. S. McKinley, and V. Shmatikov, "Fix me up: Repairing access-control bugs in web applications." in NDSS. Citeseer, 2013.
[88] C. S. Xia, Y. Wei, and L. Zhang, "Automated program repair in the era of large pre-trained language models," in Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association for Computing Machinery, 2023.
[89] C. S. Xia and L. Zhang, "Keep the conversation going: Fixing 162 out of 337 bugs for $\$ 0.42$ each using chatgpt," arXiv preprint arXiv:2304.00385, 2023.
[90] Y. Wei, C. S. Xia, and L. Zhang, "Copiloting the copilots: Fusing large language models with completion engines for automated program repair," arXiv preprint arXiv:2309.00608, 2023.
[91] M. Fu, C. Tantithamthavorn, T. Le, V. Nguyen, and D. Phung, "Vulrepair: a t5-based automated software vulnerability repair," in Proceedings of the 30th ACM FSE, 2022, pp. 935-947.
[92] J. Chi, Y. Qu, T. Liu, Q. Zheng, and H. Yin, "Seqtrans: automatic vulnerability fix via sequence to sequence learning," IEEE Transactions on Software Engineering, vol. 49, no. 2, pp. 564-585, 2022.
[93] Z. Chen, S. Kommrusch, and M. Monperrus, "Neural transfer learning for repairing security vulnerabilities in c code," IEEE TSE, vol. 49, no. 1, pp. 147-165, 2022.
[94] J. Jiang, Y. Xiong, H. Zhang, Q. Gao, and X. Chen, "Shaping program repair space with existing patches and similar code," in Proceedings of the 27th ACM SIGSOFT international symposium on software testing and analysis, 2018, pp. 298-309.
[95] D. Kim, J. Nam, J. Song, and S. Kim, "Automatic patch generation learned from human-written patches," in 2013 35th ICSE. IEEE, 2013, pp. 802-811.
[96] F. Long and M. Rinard, "Staged program repair with condition synthesis," in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, 2015, pp. 166-178.
[97] ——, "Automatic patch generation by learning correct code," in Proceedings of the 43rd Annual ACM POPL, 2016, pp. 298-312.
[98] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest, "Automatically finding patches using genetic programming," in 2009 IEEE 31st International Conference on Software Engineering. IEEE, 2009, pp. $364-374$.
[99] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li, and Y. Zhang, "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair," in Proc. IEEE ASE, 2023.
[100] N. Jiang, K. Liu, T. Lutellier, and L. Tan, "Impact of code language models on automated program repair," Proc. ACM ICSE, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Lyuye Zhang, Kaixuan Li (Equal Contribution to the first author), Kairan Sun, and Yang Liu are with the College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore.</p>
<p>Daoyuan Wu (Corresponding Author; daoyuan@cse.ust.hk) is with Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China. Work done while at NTU.</p>
<p>Ye Liu is with Singapore Management University. Work done while at NTU. Haoye Tian is with University of Luxembourg.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
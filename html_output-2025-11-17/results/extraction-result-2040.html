<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2040 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2040</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2040</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-278535517</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.07854v1.pdf" target="_blank">CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution</a></p>
                <p><strong>Paper Abstract:</strong> Sparse reward environments pose significant challenges in reinforcement learning, especially within multi-agent systems (MAS) where feedback is delayed and shared across agents, leading to suboptimal learning. We propose Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum learning framework that addresses this by (1) refining intermediate tasks for individual agents, (2) using a variational evolutionary algorithm to generate informative subtasks, and (3) co-evolving agents with their environment to enhance training stability. Experiments on five cooperative tasks in the MPE and Hide-and-Seek environments show that CCL outperforms existing methods in sparse reward settings.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2040",
    "paper_id": "paper-278535517",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0040215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Coevolutionary Task Evolution</p>
<p>Yufei Lin 
Chengwei Ye 10009-0005-0941-3316], 0009-0004-2593-3621Huanzhen Zhang</p>
<p>Homesite Group Inc
Kangsheng Wang *3, Linuo Xu 4, Shuyan Liu 5, Zeyu Zhang 6 10009-0008-3051-5642, 0009-0009-8392-4148, 0009-0004-2901-6193, 0009-0004-7641-2623</p>
<p>Chewy Inc</p>
<p>University of Science and Technology
Beijing</p>
<p>Yunnan University of Finance and Economics</p>
<p>Yunnan University</p>
<p>The Australian National University</p>
<p>CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Coevolutionary Task Evolution
74D12C6492112E47D9ABA018564ED67EMulti-Agent Reinforcement Learning (MARL)Sparse Reward EnvironmentsCurriculum LearningCo-evolutionary AlgorithmsTask GenerationEvolutionary Reinforcement LearningCooperative Problem Solving
Sparse reward environments pose significant challenges in reinforcement learning, especially within multi-agent systems (MAS) where feedback is delayed and shared across agents, leading to suboptimal learning.We propose Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum learning framework that addresses this by (1) refining intermediate tasks for individual agents, (2) using a variational evolutionary algorithm to generate informative subtasks, and (3) co-evolving agents with their environment to enhance training stability.Experiments on five cooperative tasks in the MPE and Hide-and-Seek environments show that CCL outperforms existing methods in sparse reward settings.</p>
<p>INTRODUCTION</p>
<p>Deep Reinforcement Learning (DRL) has shown substantial success in Multi-Agent Systems (MAS), with notable applications in robotics [1,2], gaming [3], and autonomous driving [4].Despite this progress, sparse reward environments continue to hinder learning efficiency, as agents often receive feedback only after completing complex tasks.This delayed reward signal limits exploration and makes policy optimization difficult.</p>
<p>To improve exploration under sparse rewards, several strategies have been proposed, including reward shaping [5,6], imitation learning [7], policy transfer [8], and curriculum learning [9,10].These methods aim to strengthen the reward signal and guide agents toward effective behaviors.While effective in single-agent environments, their performance often degrades in MAS, where multiple interacting agents exacerbate environmental dynamics and expand the joint state-action space [11][12][13].</p>
<p>In response, we propose Collaborative Multi-dimensional Course Learning (CCL), a co-evolutionary curriculum learning framework tailored for sparse-reward cooperative MAS.CCL introduces three core innovations:</p>
<p>(1) It generates agent-specific intermediate tasks using a variational evolutionary algorithm, enabling balanced strategy development.</p>
<p>(2) It models co-evolution between agents and their environment [14], aligning task complexity with agents' learning progress.</p>
<p>(3) It improves training stability by dynamically adapting task difficulty to match agent skill levels.</p>
<p>Through extensive experiments across five tasks in the MPE and Hide-and-Seek (HnS) environments, CCL consistently outperforms existing baselines, demonstrating enhanced learning efficiency and robustness in sparse-reward multi-agent scenarios.</p>
<p>PROBLEM STATEMENT</p>
<p>In reinforcement learning, the reward signal is a critical feedback mechanism guiding agents to assess their actions and learn optimal policies via the Bellman equation [15].While a well-designed reward function defines the task objective and measures agent behavior, agents may still pursue suboptimal strategies.Nonetheless, carefully crafted rewards greatly enhance learning efficiency and policy convergence [16].</p>
<p>Fig. 2. Intermediate task generation in MAS is more complex than in single-agent settings due to the need to account for agent-specific subtasks.In sparse reward environments where rewards are shared, incorporating an individual perspective mechanism becomes essential to ensure effective task decomposition and learning.</p>
<p>Designing dense rewards in complex MAS is challenging due to reliance on prior knowledge, which often fails to capture all interaction dynamics.Sparse rewards offer a more flexible alternative by providing feedback only upon reaching a critical goal state [17], reducing dependence on manual reward design and improving generalization.</p>
<p>In non-sparse reward settings, at each time step , the agent observes its current state and selects an action based on its policy .The chosen action results in a transition to a new state , determined by the environment ' s transition dynamics , and an associated reward is obtained from the reward function .The sequence of states, actions, following states, and rewards over an episode of time steps form the trajectory , where is either determined by the maximum episode length or specific task termination conditions.This outlines the process of reinforcement learning for a single agent.</p>
<p>The goal of this individual agent is to learn and maximize its expected cumulative rewarded policy: where is the discount factor, representing future rewards' diminishing value refinement degree of the optimization process is carried out by each time step inside the trajectory, that is, the optimization granularity is accurate to each time step.However, the system dynamics significantly intensify when extending this general framework to MAS under sparse reward conditions.In this system, there are decision-making agents, where each agent takes an action at time step based on the observed state information and following its dedicated policy .The global state st of the system is composed of the joint states of all individual agents, denoted as .Correspondingly, the joint action at each time step is also formed by the combination of actions from all agents, i.e., .In the sparse reward environment, reward signals only emerge when the system achieves specific predefined goal states, posing more significant challenges for agent collaboration and strategy optimization.In cooperative multi-agent tasks, the goal of each agent is no longer focused on maximizing its reward but instead shifts toward optimizing the cumulative reward of the entire system [52].This requires agents to collaborate effectively, coordinating their actions to achieve the shared objective, thereby improving the overall performance of the multi-agent system.Consequently, the objective function for each agent is transformed into , where represents the reward received by agent at time step given the state st and joint action .The overall goal of the multi-agent system (MAS) then becomes the sum of the individual objectives, denoted as .At this point, it becomes evident that the essence of a multi-agent reinforcement learning algorithm lies in utilizing the rewards earned by all agents to optimize the overall collaborative strategy [53].However, this challenge is significantly heightened in a sparse reward environment, where agents receive limited feedback, making it difficult to effectively guide their actions and improve coordination toward the collective goal.In the case that there are only very few 0-1 reward signals, the total reward of the system can be simplified to a binary function:</p>
<p>As the number of agents increases, training variance in MAS grows exponentially.In sparse reward settings, agents must achieve sub-goals aligned with a shared objective, yet often receive little to no feedback, making learning difficult.This lack of guidance hampers exploration and destabilizes training, rendering many singleagent methods ineffective.To address these challenges, we propose Collaborative Multi-dimensional Course Learning (CCL) for more stable and efficient multi-agent training.</p>
<p>RELATED WORK</p>
<p>Curriculum Learning</p>
<p>Sparse reward environments have driven the development of various exploration strategies in reinforcement learning, including reward shaping [18], intrinsic motivation [19], and curriculum learning [20].While the first two enhance learning by densifying rewards, curriculum learning adopts a divide-and-conquer approachdecomposing complex tasks into simpler subtasks arranged in increasing difficulty [1,21,22].</p>
<p>In reinforcement learning, curriculum learning involves three main components: task generation, task ranking, and transfer learning [23].These can be guided by automated methods [9] or expert knowledge, though the latter often introduces biases [24,25].Adaptive Automatic Curriculum Learning (ACL) addresses this by dynamically tailoring task sequences to agent progress, without manual intervention [54].Despite its promise, ACL faces challenges in defining effective evaluation metrics and managing computational cost [26][27][28].Current approaches often rely on coarse performance metrics or costly replay mechanisms [29,30], making it difficult to scale in complex multi-agent settings.</p>
<p>Evolutionary Reinforcement Learning</p>
<p>Evolutionary Algorithms (EAs) optimize policies through selection, mutation, and recombination of candidate solutions based on fitness scores [31].Their integration with reinforcement learning aims to address issues like sparse rewards and limited policy diversity [29,30,34].</p>
<p>Though promising [32,33], combining EAs with RL introduces challenges, notably the computational overhead from large populations [24] and the difficulty of retaining informative environmental features during evolutionary encoding.Effective integration requires balancing exploration benefits with computational feasibility [44][45][46][47][48][49][50][51].</p>
<p>METHODOLOGY</p>
<p>The Variational Individual-perspective Evolutionary Operator</p>
<p>In this section, we provide a detailed explanation of all the components of CMCL.As a coevolutionary system with two primary parts, the agents are trained using the existing Multi-Agent Proximal Policy Optimization (MAPPO) algorithm [35], which will not be elaborated on here.The complete workflow of the CMCL algorithm is outlined in Algorithm 1.</p>
<p>Evolutionary Curriculum Initialization Due to the low initial policy performance of a MAS at the start of training, agents struggle to accomplish complex tasks.Therefore, minimizing the norm of task individuals within the initial population is essential.Assuming the initial task domain is , the randomly initialized task population should meet the following conditions, where represents the initial Euclidean norm between the agent and the task, and is a robust hyperparameter, typically set to be about one percent of the total task space size.</p>
<p>Task Fitness Definition: Previous methods often assessed intermediate tasks using agent performance metrics [24,30,36] or simple binary filters [37], which fail to capture the non-linear nature of task difficulty.Tasks with success rates near 0 or 1 offer little training value, while those closer to the midpoint present a more suitable challenge.To address this, we model fitness as a non-linear function, favoring tasks of moderate difficulty that best support learning progression.To capture this nonlinear relationship, we establish a sigmoid-shaped fitness function to describe the adaptability of tasks to the current level of agent performance, where r represents the average success rate of the agents on task .</p>
<p>Variational Individual-perspective Crossover In a MAS, the single reward signal is distributed across multiple dimensions, especially from the perspective of different agents, leading to imbalances in the progression of individual strategies.Therefore, based on the encoding method mentioned earlier, operating on intermediate tasks at the individual level within the MAS is necessary.Assuming that in a particular round of intermediate task generation, individuals from the previous task generation are randomly divided into two groups and .Then, we take task pairs from and to produce new children in the population.</p>
<p>In the above formula, represents the crossover step size for pair , and represents the crossover direction for pair .The calculations of and are shown below:</p>
<p>denotes the direction of the -th agent in pair , obtained by uniform random sampling.</p>
<p>The proposed variational individual-perspective crossover ensures each agent's subtask direction contributes equally to curriculum evolution, enabling broader exploration compared to traditional methods.In a MAS with nnn entities, this results in 2n2^n2n possible direction combinations, enhancing diversity in intermediate task generation.</p>
<p>To address catastrophic forgetting [38,39], we adopt a soft selection strategy.Rather than discarding low-fitness individuals, the entire population is retained, and a fraction ( α \alpha α , typically 0.2 -0.4) of historical individuals is reintroduced each iteration.This maintains task diversity, preserves challenging tasks for future stages, and helps avoid local optima.</p>
<p>Elite Prototype Fitness Evaluation</p>
<p>Evolutionary algorithms often require maintaining a sufficiently large population to ensure diversity and prevent being trapped in local optima or influenced by randomness.However, evaluating the fitness of intermediate tasks in a large curriculum population significantly increases computational cost.To mitigate this issue, we propose a prototype-based fitness estimation method.First, we uniformly sample tasks in each iteration and measure their success rate r and fitness .These sampled tasks, called prototypes, are used in actual training.Next, we employ a K-Nearest Neighbor (KNN) approach to estimate the fitness of tasks not directly used in training.</p>
<p>Assume there are individuals in the prototype task set , with fitness values for each , and individuals in the query task set , represented as vectors for each .For any individual in the query set , its fitness value can be calculated as shown below</p>
<p>In the formula, represents the set of indices of the k-closest individuals in the prototype task set to the vector , based on the Euclidean distance.This can be expressed as follows:</p>
<p>EXPERIMENT</p>
<p>Main Result</p>
<p>We evaluate CCL on five cooperative tasks across two environments: simple/complex propagation and Push-ball from the MPE benchmark [40], and ramp-passing and lock-back from the MuJoCo-based HnS environment [41].All tasks use a binary (0-1) sparse reward structure, with results averaged over three random seeds.Training is conducted using MAPPO [35] on a system with an Nvidia RTX 3090 GPU and a 14core CPU.Attention mechanisms [42] are also integrated to improve agent coordination.</p>
<p>We compare CCL with five baselines: 1. Vanilla MAPPO [35] -Direct training on the target task without intermediate tasks.Across all environments, baseline methods struggle under sparse rewards, especially in HnS.CCL consistently outperforms them in both learning speed and final performance, achieving over 95% success in the most complex tasks (see Tables 1 and 2).</p>
<p>Ablation Studies</p>
<p>Fig. 3.The adaptive step usage ablation experiments which show its effect.</p>
<p>Adaptive Mutation</p>
<p>Step: Ablation studies show that using an adaptive mutation step size enhances flexibility and performance in sparse reward environments compared to fixed or no mutation.While mutation promotes strategy diversity, improper step sizes can degrade learning.Notably, adaptive mutation proves as effective as crossover and individual-perspective variation in improving CCL's performance (see Fig. 3).Non-linear Factor in Fitness Function: As shown in Fig. 4, the sigmoid fitness function delivers better performance than the linear form .This improvement stems from the sigmoid function's properties: as the agent's success rate approaches 0 or 1, the task's suitability to the agent's abilities decreases exponentially.Specifically, when the success rate is exactly 0.5, the fitness value remains consistently at 0.5.This approach effectively integrates nonlinear elements into the success rate distribution, enabling the fitness function to more accurately represent the relationship between task difficulty and the agent's skill level.</p>
<p>Fig. 1 .
1
Fig. 1.MPE is validated with three different collaborative task scenarios.</p>
<ol>
<li>POET[24] -Uses task evolution; implemented with the same setup as CCL for fairness.3. GC[36] -An improved version of POET with enhanced task generation.4.GoalGAN [10] -Combines curriculum learning with attention-based enhancements.5. VACL[43] -Applies variational methods to create robust intermediate tasks.</li>
</ol>
<p>Fig. 4 .
4
Fig. 4. The comparison of using absolute value and sigmoid-shaped fitness function.</p>
<p>6 CONCLUSION
6
This paper presents CCL, a co-evolutionary curriculum learning framework for improving training stability and performance in sparse-reward multi-agent systems (MAS).By generating intermediate tasks, applying variational individual-perspective crossover, and using elite prototype-based fitness evaluation, CCL enhances exploration and coordination.Experiments in MPE and HnS show consistent outperformance over baselines, and ablation studies confirm the value of each component.While effective in cooperative MAS, future work should extend CCL to competitive or mixed settings.Additionally, storing historical tasks for soft selection increases memory use, which could be mitigated through compression or selective retention.</p>
<p>Table 1 .
1
The Performance Comparison of CCL and Other Baselines on Simulated Environments</p>
<p>Table 2 .
2
Performance Metrics for Various Methods across Different Tasks</p>
<p>A model of symbiomemesis: machine education and communication as pillars for human-autonomy symbiosis. H Abbass, E Petraki, A Hussein, F Mccall, S Elsawah, Philos. Transactions Royal Soc. A. 379202003642021</p>
<p>Multi-agent reinforcement learning for redundant robot control in task-space. A Perrusquía, W Yu, X Li, Int. J. Mach. Learn. Cybern. 122021</p>
<p>Monotonic value function factorisation for deep multi-agent reinforcement learning. T Rashid, J. Mach. Learn. Res. 212020</p>
<p>Safe, multi-agent, reinforcement learning for autonomous driving. S Shalev-Shwartz, S Shammah, A Shashua, arXiv:1610.032952016arXiv preprint</p>
<p>Policy invariance under reward transformations: Theory and application to reward shaping. A Y Ng, D Harada, S Russell, Icml. 199999</p>
<p>Learning to utilize shaping rewards: A new approach of reward shaping. Y Hu, Adv. Neural Inf. Process. Syst. 332020</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsJMLR Workshop and Conference Proceedings2011</p>
<p>One-shot imitation learning. Adv. neural information processing systems. Y Duan, 201730</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, M Zhang, P Abbeel, Conference on robot learning. PMLR2017</p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P Abbeel, International conference on machine learning. PMLR2018</p>
<p>Evolutionary dynamics of multiagent learning: A survey. D Bloembergen, K Tuyls, D Hennes, M Kaisers, J. Artif. Intell. Res. 532015</p>
<p>Multi-agent reinforcement learning: An overview. Innov. multi-agent systems applications-1. L Bu¸soniu, R Babuška, B De Schutter, 2010</p>
<p>A survey and critique of multiagent deep reinforcement learning. P Hernandez-Leal, B Kartal, M E Taylor, Auton. Agents Multi-Agent Syst. 332019</p>
<p>Coevolutionary multiobjective evolutionary algorithms: Survey of the state-of-the-art. L M Antonio, C A C Coello, IEEE Transactions on Evol. Comput. 222017</p>
<p>Reinforcement learning: A survey. L P Kaelbling, M L Littman, A W Moore, J. artificial intelligence research. 41996</p>
<p>Reinforcement learning and the reward engineering principle. D Dewey, AAAI Spring Symposium Series. 9112014. 2014</p>
<p>The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications. S Booth, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Theory and application of reward shaping in reinforcement learning. A D Laud, 2004University of Illinois at UrbanaChampaign</p>
<p>Intrinsic motivation and reinforcement learning. Intrinsically motivated learning natural artificial systems. A G Barto, 2013</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Language acquisition in the absence of explicit negative evidence: How important is starting small?. D L Rohde, D C Plaut, Cognition. 721999</p>
<p>Learning and development in neural networks: The importance of starting small. J L Elman, Cognition. 481993</p>
<p>Autonomous task sequencing for customized curriculum design in reinforcement learning. S Narvekar, J Sinapov, P Stone, IJCAI. 2017</p>
<p>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. R Wang, J Lehman, J Clune, K O Stanley, arXiv:1901.017532019arXiv preprint</p>
<p>Quantifying generalization in reinforcement learning. K Cobbe, O Klimov, C Hesse, T Kim, J Schulman, International conference on machine learning. PMLR2019</p>
<p>Self-paced prioritized curriculum learning with coverage penalty in deep reinforcement learning. Z Ren, D Dong, H Li, C Chen, IEEE transactions on neural networks learning systems. 292018</p>
<p>Portal: Automatic curricula generation for multiagent reinforcement learning. J Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>K Wang, X Zhang, Z Guo, T Hu, H Ma, Csce, arXiv:2409.17174Boosting llm reasoning by simultaneous enhancing of casual significance and consistency. 2024arXiv preprint</p>
<p>Maestro: Open-ended environment design for multi-agent reinforcement learning. M Samvelyan, arXiv:2303.033762023arXiv preprint</p>
<p>Evolving curricula with regret-based environment design. J Parker-Holder, International Conference on Machine Learning. PMLR2022</p>
<p>Evolution strategies-a comprehensive introduction. H.-G Beyer, H.-P Schwefel, Nat. computing. 12002</p>
<p>Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity. T Miconi, A Rawal, J Clune, K O Stanley, arXiv:2002.105852020arXiv preprint</p>
<p>Efficacy of modern neuro-evolutionary strategies for continuous control optimization. P Pagliuca, N Milano, S Nolfi, Front. Robotics AI. 7982020</p>
<p>Evolutionary population curriculum for scaling multi-agent reinforcement learning. Q Long, arXiv:2003.104232020arXiv preprint</p>
<p>The surprising effectiveness of ppo in cooperative multi-agent games. C Yu, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Robust reinforcement learning via genetic curriculum. Y Song, J Schneider, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Automated curricula through setter-solver interactions. S Racaniere, arXiv:1909.128922019arXiv preprint</p>
<p>Catastrophic forgetting in reinforcement-learning environments. A Cahill, 2011CiteseerPh.D. thesis</p>
<p>Catastrophic forgetting in connectionist networks. R M French, Trends cognitive sciences. 31999</p>
<p>Multi-agent actor-critic for mixed cooperative-competitive environments. Adv. neural information processing systems. R Lowe, 201730</p>
<p>Emergent tool use from multi-agent autocurricula. B Baker, arXiv:1909.075282019arXiv preprint</p>
<p>Attention is all you need. A Vaswani, </p>
<p>neural information processing systems. Adv, 201730</p>
<p>Variational automatic curriculum learning for sparse-reward cooperative multi-agent problems. J Chen, Adv. Neural Inf. Process. Syst. 342021</p>
<p>X Qi, Z Zhang, H Zheng, arXiv:2502.00631MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density Prediction. 2025arXiv preprint</p>
<p>K Wang, X Zhang, Z Guo, arXiv:2409.17174CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal Significance and Consistency. 2024arXiv preprint</p>
<p>Comprehensive Review: Advancing Cognitive Computing through Theory of Mind Integration and Deep Learning in Artificial Intelligence. S Liu, K Wang, Proc. 8th Int. Conf. on Computer Science and Application Engineering. 8th Int. Conf. on Computer Science and Application Engineering2024</p>
<p>Enhancing Autonomous Driving through Dual-Process Learning with Behavior and Reflection Integration. X Zhang, K Wang, T Hu, ICASSP 2025 -IEEE Int. Conf. on Acoustics, Speech and Signal Processing. SeoulIEEE2025</p>
<p>Synergistic Spotting and Recognition of Micro-Expression via Temporal State Transition. B Zou, Z Guo, W Qin, ICASSP 2025 -IEEE Int. Conf. on Acoustics, Speech and Signal Processing. SeoulIEEE2025</p>
<p>Autonomous Driving System Based on Dual Process Theory and Deliberate Practice Theory. T Hu, X Zhang, H Ma, 2025Manuscript</p>
<p>Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation. X Zhang, K Wang, T Hu, arXiv:2505.000092025arXiv preprint</p>
<p>K Wang, C Ye, H Zhang, arXiv:2504.11515Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment. 2025arXiv preprint</p>
<p>GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation. C Ye, H Zhang, Y Lin, 2025Research Square preprint</p>
<p>DRCO: a Toolkit for Intelligently Curbing Illegal Wildlife Trade. S Xu, Y Ye, M Li, 2025Research Square preprint</p>
<p>GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation. K Wang, Y Li, C Ye, 2025Research Square preprint</p>            </div>
        </div>

    </div>
</body>
</html>
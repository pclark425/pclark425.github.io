<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1956 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1956</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1956</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-281080552</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.00210v1.pdf" target="_blank">Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</a></p>
                <p><strong>Paper Abstract:</strong> Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1956.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1956.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VEME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-language Episodic-and-Memory Enhanced (VEME)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-memory cross-modal alignment framework that grounds 2D visual semantics in 3D geometry via a learned World Embedding (spatial semantic memory) and creates discriminative episodic traces for spatio-temporal experiences, enabling improved instruction-guided navigation and video spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VEME</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-memory architecture built on a pretrained VLM (Qwen-2.5-VL-7B) augmented with: (1) Spatial Semantic Memory implemented as a learnable World Embedding E_world; (2) Episodic Memory producing an episode query Q_epi attending to E_world to form F_episo dic; (3) Geometric grounding that cross-attends visual semantic features F_vis,t to image-based geometric features F_geo,t to produce grounded visual features F'_vis,t; (4) 3D global geometry features F_pcd,t from a dedicated 3D backbone; (5) unified input sequence concatenating [instruction; grounded visual percept; trajectory encoding; world embedding] fed to the VLM. Supervision includes cross-entropy for tasks plus L_spatial (spatial contrastive) and L_episodic (episodic contrastive) auxiliary losses. LoRA adapters (low-rank) are used to fine-tune the VLM efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>DINOv2 ViT (semantic patch features) + VGGT-1B (image-based geometric encoder) + Sonata sparse 3D Transformer (point-cloud backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>DINOv2 (self-supervised visual features, Oquab et al. 2023); VGGT pretrained on multiple geometry-estimation tasks (depth/normals); Sonata pretrained on ScanNet and Matterport3D (self-supervised 3D reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-attention between semantic image features and image-based geometric features to produce geometrically-aware visual features (F'_vis,t); contrastive spatial loss (L_spatial) to enforce semantic-to-geometry binding; episodic cross-attention from episode query Q_epi to World Embedding E_world with episodic contrastive loss (L_episodic) to bind episodes to spatial priors; finally concatenation of all modalities into VLM for joint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level: patch-level semantic features (image patches), image-based geometric features (dense feature maps), per-point 3D point-cloud features, and higher-level world-embedding (scene-level cognitive map).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit: learnable World Embedding (environment-agnostic cognitive map), point-cloud 3D features (XYZ+RGB), image-based implicit geometry (depth/normals features), and trajectory embeddings (temporal/egocentric path tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation and video spatio-temporal reasoning / embodied question answering</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VLN-CE (Vision-and-Language Navigation in Continuous Environments) and VSI-Bench (Visual-Spatial Intelligence Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (Habitat) for VLN-CE; real indoor videos / egocentric-like sequences for VSI-Bench and video QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Success weighted by Path Length (SPL) for navigation; Accuracy (ACC) and Mean Relative Accuracy (MRA) for VSI-Bench tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>VLN-CE R2R Val-Unseen: SR = 57.0% and SPL = 46.7% (reported in text); VSI-Bench average performance (Avg) reported as 49.3 (table; VEME listed as best/competitive; Numeric breakdown: Obj.Cnt etc. shown in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablations reported in Table 3: Full model (SPL %) = 65.1; w/o Spatial Semantic Memory (L_spatial, E_world) = 55.3 SPL%; w/o Episodic Memory (L_episodic, F_episodic) = 49.8 SPL%; w/o Geometric Grounding = 61.2 SPL%; w/o Trajectory Input = 52.1 SPL%. (C.4 also reports deltas: episodic −15.3 points, trajectory −12.9, spatial −9.8, geometric −3.9).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Ablations indicate large absolute gains attributable to grounding modules: Episodic memory contributes ~+15.3 percentage points (removal causes −15.3), Trajectory input ~+12.9 pp, Spatial Semantic Memory ~+9.8 pp, Geometric Grounding ~+3.9 pp in reported SPL/accuracy contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Encoder ablations: removing the image-based geometric encoder (VGGT) causes a large drop on VSI metrics (Full: ACC 44.1, MRA 55.7, ALL 49.3 → w/o VGGT: ACC 36.1, MRA 19.2, ALL 28.3). Removing fine-tuning (LoRA/SFT) also strongly degrades performance (w/o fine-tune: ACC 34.9, MRA 19.4, ALL 26.9).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify dependency on relatively clean 3D point clouds and accurate trajectory data during training and sensitivity to noisy/incomplete/drifting SLAM outputs; also cite computational overhead (VRAM, FLOPs) and static-environment assumption as practical bottlenecks for real-robot deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative failure modes: conventional VLMs (baselines) get stuck in local loops or confuse referents (e.g., 'the kitchen you just passed' leading to wrong kitchen) due to lack of episodic grounding; baselines sometimes answer supporting-object QA incorrectly (e.g., 'chair' vs. correct 'desk'). Quantitative ablation shows removal of episodic/spatial modules yields large performance drops (episodic −15.3 pp, spatial −9.8 pp). A concrete failure case on a complex instruction is illustrated (Figure 6). No precise per-failure-frequency breakdown beyond the ablation deltas was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Training data blend includes real navigation videos, simulated navigation sequences (R2R-CE / RxR-CE in Habitat), auxiliary navigation augmentation; LoRA fine-tuning used for efficient adaptation. Authors propose future work for robustness: train with simulated sensor noise, broader 3D environments, meta-learning/online domain adaptation to handle sim-to-real shift, but no numeric domain-shift degradation values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>The paper uses LoRA fine-tuning; ablation removing fine-tuning (SFT/LoRA) produces large drops on VSI metrics (ALL 49.3 → 26.9), indicating fine-tuning is critical. No explicit frozen-vs-finetuned comparison for visual encoders beyond this LoRA ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention for geometry-to-semantics grounding and episodic-to-world embedding recall; late concatenation of all modality embeddings into the VLM input sequence so the VLM's self-attention integrates instruction, perception, episodic cues, and world priors.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Tightly coupling 2D semantic features with image-derived geometric cues via cross-attention plus a spatial contrastive loss enables meaningful semantic-to-geometry grounding; a learnable World Embedding provides reusable spatial priors that episodic queries can recall; episodic memory is the most crucial module for long-horizon navigation (largest ablation drop), while VGGT geometric features and fine-tuning are essential for video spatio-temporal understanding; together these grounding mechanisms significantly improve SR/SPL on VLN and accuracy/MRA on VSI-Bench versus baseline VLMs and specialized navigation agents.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1956.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1956.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial Semantic Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial Semantic Memory (World Embedding E_world)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learnable environment-agnostic parameter matrix (World Embedding) representing N_w spatial concepts that acts as a cognitive map and is queried during episodic recall to ground experiences in general spatial priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Spatial Semantic Memory (World Embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>E_world ∈ R^{Nw×D} is a learned matrix capturing abstract spatial concepts; visual-semantic features are grounded to geometry and episodic queries attend to E_world to recall task-relevant spatial priors; supervised via spatial contrastive loss L_spatial encouraging unique binding between geometry-aware visual features and their semantic sources.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (component interacts with image and 3D encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Episodic query Q_epi attends to E_world (CrossAttn(Q_epi, E_world)) to produce F_episo dic; L_spatial contrastive loss enforces semantic-geometry binding for grounded visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Scene-level / concept-level cognitive map (abstract spatial concepts), accessed by episode-level queries.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Learned abstract spatial priors (World Embedding) plus linkage to explicit 3D point-cloud features and image-based geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation and spatio-temporal question answering</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VLN-CE, VSI-Bench (used within VEME)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation and real indoor video domains (as used by VEME)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SPL (navigation ablations) and task accuracy metrics (VSI)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation: removal of Spatial Semantic Memory: SPL drop from Full 65.1 to 55.3 (Table 3); reported delta −9.8 pp in C.4 analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>w/o Spatial Memory: SPL 55.3 (Full 65.1 → −9.8 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Spatial memory contributes ~+9.8 absolute points to SPL in ablation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Effectiveness depends on ability to link visual semantics to geometry; failures occur when geometry or trajectory data are noisy or missing since E_world relies on accurate grounding signals during training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Removal causes substantial performance degradation (−9.8 SPL points); paper qualitatively links absence to errors in grounding referential phrases (e.g., 'the kitchen you just passed').</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Included in SFT training data mixture (real + simulated) to help generalization; further domain-adaptation proposed for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention between episodic queries and the World Embedding; integrated into VLM input sequence for joint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A learned world embedding acting as a spatial semantic memory enables recall of task-relevant geometric-semantic priors and measurably improves navigation efficiency; its removal yields a near-10-point SPL drop, confirming its role as a grounding scaffold.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1956.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1956.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Episodic Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Episodic Memory (F_episodic derived from Q_epi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An episode-specific memory trace formed by fusing global 3D geometry and trajectory encodings into a query Q_epi that attends to the World Embedding to produce a discriminative episodic representation used for retrieval and long-horizon reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Episodic Memory (Q_epi → F_episodic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Episode fingerprint Q_epi = Linear(concat[F_pcd,0:Tobs ; F_traj,t]) attends to E_world producing F_episodic via CrossAttn(Q_epi, E_world); trained with an episodic contrastive loss L_episodic to make episodes separable and enable recall of previously visited locations and experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (uses outputs of 3D backbone and trajectory encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-attention between episode query and World Embedding; episodic contrastive loss ties episodic traces to grounded visual features from the same episode.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Episode-level (spatio-temporal trace for a full trajectory/video)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicitly encodes trajectory + aggregated 3D geometry and links to E_world for recall of spatial priors</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Long-horizon navigation, episodic reference resolution in instruction following, video QA</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VLN-CE (example: 'the kitchen you just passed' resolution), VSI-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulated long-horizon navigation and real video episodes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SPL (navigation), accuracy metrics for episodic reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation: removing Episodic Memory causes the largest reported drop: SPL from Full 65.1 → 49.8 (Table 3) and reported delta −15.3 percentage points (C.4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>w/o Episodic Memory: SPL 49.8 (Full 65.1 → −15.3 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Episodic memory contributes the largest single-module improvement (~+15.3 absolute points in SPL when present).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Episodic memory effectiveness depends on reliable aggregated geometry and trajectories; noisy/incomplete mapping or drifting trajectories limit episodic discrimination and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Without episodic traces, agents fail to resolve temporal referents and often loop or select incorrect previously visited landmarks, producing large navigation performance degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Trained with a mix of real and simulated trajectories to improve generalization; future work aims for online updating / lifelong learning to keep episodic traces robust under domain shift.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention between Q_epi and E_world; episodic features concatenated into VLM input for joint attention-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Episodic memory that links a trajectory's global geometry and actions to the shared world embedding is essential for resolving temporally-indexed instructions and yields the largest single-module benefit in navigation performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1956.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1956.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VGGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VGGT: Visual Geometry Grounded Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image-based visual-geometry encoder used to extract dense, view-dependent geometric cues (implicit depth/normals) from RGB frames for geometric grounding of semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VGGT: Visual Geometry Grounded Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VGGT (image-based geometric encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Processes RGB frames to produce dense geometric feature maps (implicitly encoding depth gradients and surface normals) which are projected into the common latent space and cross-attended by semantic features to produce geometry-aware visual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>VGGT-1B (visual-geometry encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained on multiple geometry estimation tasks (depth, normals) as reported in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides image-derived geometric keys/values used in CrossAttn(F_sem,t, F_geo,t) so semantic features are grounded in per-view geometry; removal breaks geometric grounding pathway.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Dense image-level geometric feature maps (view-dependent), used with patch-level semantics</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit image-space geometry (depth/normals-like features) used to link semantics to spatial structure</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Supports vision-language navigation and spatio-temporal reasoning within VEME</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VLN-CE, VSI-Bench (as integrated in VEME)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>RGB frames from simulation and real videos</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>VSI ACC / MRA and navigation SPL (indirect effect via VEME)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation: removing VGGT leads to VSI metric degradation (Full ALL 49.3 → w/o VGGT ALL 28.3; ACC 44.1 → 36.1; MRA 55.7 → 19.2) indicating critical role in spatio-temporal understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>w/o VGGT: ACC 36.1, MRA 19.2, ALL 28.3 (compared to Full ACC 44.1, MRA 55.7, ALL 49.3).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>VGGT provides large improvements for video spatio-temporal understanding (removal drops ALL by ~21 points in reported table).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>VGGT removal is explicitly evaluated and causes major drops; paper also ablates fine-tuning of these encoders showing further degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors note that image-based geometric encoders are necessary to bind semantics to immediate spatial cues; poor geometric features limit grounding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Removing VGGT produces large drops in MRA (temporal/numerical reasoning) and overall ALL score, indicating failures in tasks requiring fine-grained spatial/metric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>VGGT is pretrained on geometry tasks; paper does not present explicit domain-adaptation for VGGT but fine-tunes with LoRA as part of VEME training.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper shows fine-tuning (LoRA/SFT) is important; removing fine-tune further decreases performance (w/o fine-tune: ALL 26.9). VGGT is used and ablated rather than compared across many pretrained scales.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention where F_sem queries F_geo to produce F'_vis; then spatial contrastive loss supervises alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Image-based geometric features (VGGT) are essential to ground 2D semantics into spatial structure; their removal causes major drops in spatio-temporal understanding metrics, confirming that combining semantic and geometric per-view features is crucial for embodied grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1956.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1956.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sonata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sonata (3D point-cloud backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse 3D Transformer backbone (Sonata) used to extract per-point 3D features from global point clouds to build persistent geometric representations for the cognitive map and episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sonata: Self-Supervised Learning of Reliable Point Representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sonata (3D backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sparse 3D Transformer tailored for large-scale point clouds using voxel-based sparse attention to capture local geometry and long-range spatial relations; outputs per-point feature vectors F_pcd,t projected into the common VLM latent space and used for episodic query formation and world grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Sparse 3D Transformer (Sonata)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained on large-scale indoor 3D datasets including ScanNet and Matterport3D on a self-supervised reconstruction task (per paper)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides persistent 3D geometric features that are concatenated with trajectory encodings to form episode queries Q_epi which attend to World Embedding; also supplies global geometry for spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>3D point-cloud (per-point) scene-level persistent geometry</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 3D coordinates (XYZ) + color used to build global geometric memory for cognitive map and episodic formation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Long-horizon navigation and 3D scene understanding</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>VLN-CE, episodic/video tasks in VSI-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>3D reconstructed indoor scenes from simulation and real dataset point clouds</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Contributes to SPL and VSI metrics; no isolated numeric for Sonata alone provided</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors explicitly note dependence on relatively clean 3D point clouds and accurate trajectory data for effective episodic and spatial memory formation; robustness to noisy SLAM outputs remains an open issue.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>When reliable point-cloud inputs are not available (noisy/incomplete/drifting maps), episodic memory formation and recall degrade, hurting navigation and referent resolution; exact failure rates not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Pretraining on diverse 3D datasets helps generalization; authors plan to include simulated sensor noise and more varied 3D environments in future training to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>3D features are projected into the common latent space and concatenated with other modalities; episodic Q_epi computed by fusing F_pcd and F_traj then attending to E_world.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A strong 3D backbone that provides persistent, global geometry is necessary for building discriminative episodic traces and for effective grounding of visual semantics into a cognitive map; however, reliance on clean point-clouds is a practical limitation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence <em>(Rating: 2)</em></li>
                <li>VGGT: Visual Geometry Grounded Transformer <em>(Rating: 2)</em></li>
                <li>Sonata: Self-Supervised Learning of Reliable Point Representations <em>(Rating: 2)</em></li>
                <li>Vision-and-Language Navigation with Episodic Scene Memory <em>(Rating: 2)</em></li>
                <li>Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1956",
    "paper_id": "paper-281080552",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "VEME",
            "name_full": "Vision-language Episodic-and-Memory Enhanced (VEME)",
            "brief_description": "A dual-memory cross-modal alignment framework that grounds 2D visual semantics in 3D geometry via a learned World Embedding (spatial semantic memory) and creates discriminative episodic traces for spatio-temporal experiences, enabling improved instruction-guided navigation and video spatial understanding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VEME",
            "model_description": "Dual-memory architecture built on a pretrained VLM (Qwen-2.5-VL-7B) augmented with: (1) Spatial Semantic Memory implemented as a learnable World Embedding E_world; (2) Episodic Memory producing an episode query Q_epi attending to E_world to form F_episo dic; (3) Geometric grounding that cross-attends visual semantic features F_vis,t to image-based geometric features F_geo,t to produce grounded visual features F'_vis,t; (4) 3D global geometry features F_pcd,t from a dedicated 3D backbone; (5) unified input sequence concatenating [instruction; grounded visual percept; trajectory encoding; world embedding] fed to the VLM. Supervision includes cross-entropy for tasks plus L_spatial (spatial contrastive) and L_episodic (episodic contrastive) auxiliary losses. LoRA adapters (low-rank) are used to fine-tune the VLM efficiently.",
            "visual_encoder_type": "DINOv2 ViT (semantic patch features) + VGGT-1B (image-based geometric encoder) + Sonata sparse 3D Transformer (point-cloud backbone)",
            "visual_encoder_pretraining": "DINOv2 (self-supervised visual features, Oquab et al. 2023); VGGT pretrained on multiple geometry-estimation tasks (depth/normals); Sonata pretrained on ScanNet and Matterport3D (self-supervised 3D reconstruction)",
            "grounding_mechanism": "Cross-attention between semantic image features and image-based geometric features to produce geometrically-aware visual features (F'_vis,t); contrastive spatial loss (L_spatial) to enforce semantic-to-geometry binding; episodic cross-attention from episode query Q_epi to World Embedding E_world with episodic contrastive loss (L_episodic) to bind episodes to spatial priors; finally concatenation of all modalities into VLM for joint reasoning.",
            "representation_level": "Multi-level: patch-level semantic features (image patches), image-based geometric features (dense feature maps), per-point 3D point-cloud features, and higher-level world-embedding (scene-level cognitive map).",
            "spatial_representation": "Explicit: learnable World Embedding (environment-agnostic cognitive map), point-cloud 3D features (XYZ+RGB), image-based implicit geometry (depth/normals features), and trajectory embeddings (temporal/egocentric path tokens).",
            "embodied_task_type": "Vision-language navigation and video spatio-temporal reasoning / embodied question answering",
            "embodied_task_name": "VLN-CE (Vision-and-Language Navigation in Continuous Environments) and VSI-Bench (Visual-Spatial Intelligence Benchmark)",
            "visual_domain": "Photorealistic simulation (Habitat) for VLN-CE; real indoor videos / egocentric-like sequences for VSI-Bench and video QA",
            "performance_metric": "Success Rate (SR), Success weighted by Path Length (SPL) for navigation; Accuracy (ACC) and Mean Relative Accuracy (MRA) for VSI-Bench tasks",
            "performance_value": "VLN-CE R2R Val-Unseen: SR = 57.0% and SPL = 46.7% (reported in text); VSI-Bench average performance (Avg) reported as 49.3 (table; VEME listed as best/competitive; Numeric breakdown: Obj.Cnt etc. shown in Table 2).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablations reported in Table 3: Full model (SPL %) = 65.1; w/o Spatial Semantic Memory (L_spatial, E_world) = 55.3 SPL%; w/o Episodic Memory (L_episodic, F_episodic) = 49.8 SPL%; w/o Geometric Grounding = 61.2 SPL%; w/o Trajectory Input = 52.1 SPL%. (C.4 also reports deltas: episodic −15.3 points, trajectory −12.9, spatial −9.8, geometric −3.9).",
            "grounding_improvement": "Ablations indicate large absolute gains attributable to grounding modules: Episodic memory contributes ~+15.3 percentage points (removal causes −15.3), Trajectory input ~+12.9 pp, Spatial Semantic Memory ~+9.8 pp, Geometric Grounding ~+3.9 pp in reported SPL/accuracy contexts.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Encoder ablations: removing the image-based geometric encoder (VGGT) causes a large drop on VSI metrics (Full: ACC 44.1, MRA 55.7, ALL 49.3 → w/o VGGT: ACC 36.1, MRA 19.2, ALL 28.3). Removing fine-tuning (LoRA/SFT) also strongly degrades performance (w/o fine-tune: ACC 34.9, MRA 19.4, ALL 26.9).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify dependency on relatively clean 3D point clouds and accurate trajectory data during training and sensitivity to noisy/incomplete/drifting SLAM outputs; also cite computational overhead (VRAM, FLOPs) and static-environment assumption as practical bottlenecks for real-robot deployment.",
            "failure_mode_analysis": "Qualitative failure modes: conventional VLMs (baselines) get stuck in local loops or confuse referents (e.g., 'the kitchen you just passed' leading to wrong kitchen) due to lack of episodic grounding; baselines sometimes answer supporting-object QA incorrectly (e.g., 'chair' vs. correct 'desk'). Quantitative ablation shows removal of episodic/spatial modules yields large performance drops (episodic −15.3 pp, spatial −9.8 pp). A concrete failure case on a complex instruction is illustrated (Figure 6). No precise per-failure-frequency breakdown beyond the ablation deltas was provided.",
            "domain_shift_handling": "Training data blend includes real navigation videos, simulated navigation sequences (R2R-CE / RxR-CE in Habitat), auxiliary navigation augmentation; LoRA fine-tuning used for efficient adaptation. Authors propose future work for robustness: train with simulated sensor noise, broader 3D environments, meta-learning/online domain adaptation to handle sim-to-real shift, but no numeric domain-shift degradation values reported.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "The paper uses LoRA fine-tuning; ablation removing fine-tuning (SFT/LoRA) produces large drops on VSI metrics (ALL 49.3 → 26.9), indicating fine-tuning is critical. No explicit frozen-vs-finetuned comparison for visual encoders beyond this LoRA ablation.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Cross-attention for geometry-to-semantics grounding and episodic-to-world embedding recall; late concatenation of all modality embeddings into the VLM input sequence so the VLM's self-attention integrates instruction, perception, episodic cues, and world priors.",
            "sample_efficiency": null,
            "key_findings_grounding": "Tightly coupling 2D semantic features with image-derived geometric cues via cross-attention plus a spatial contrastive loss enables meaningful semantic-to-geometry grounding; a learnable World Embedding provides reusable spatial priors that episodic queries can recall; episodic memory is the most crucial module for long-horizon navigation (largest ablation drop), while VGGT geometric features and fine-tuning are essential for video spatio-temporal understanding; together these grounding mechanisms significantly improve SR/SPL on VLN and accuracy/MRA on VSI-Bench versus baseline VLMs and specialized navigation agents.",
            "uuid": "e1956.0"
        },
        {
            "name_short": "Spatial Semantic Memory",
            "name_full": "Spatial Semantic Memory (World Embedding E_world)",
            "brief_description": "A learnable environment-agnostic parameter matrix (World Embedding) representing N_w spatial concepts that acts as a cognitive map and is queried during episodic recall to ground experiences in general spatial priors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Spatial Semantic Memory (World Embedding)",
            "model_description": "E_world ∈ R^{Nw×D} is a learned matrix capturing abstract spatial concepts; visual-semantic features are grounded to geometry and episodic queries attend to E_world to recall task-relevant spatial priors; supervised via spatial contrastive loss L_spatial encouraging unique binding between geometry-aware visual features and their semantic sources.",
            "visual_encoder_type": "N/A (component interacts with image and 3D encoders)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Episodic query Q_epi attends to E_world (CrossAttn(Q_epi, E_world)) to produce F_episo dic; L_spatial contrastive loss enforces semantic-geometry binding for grounded visual features.",
            "representation_level": "Scene-level / concept-level cognitive map (abstract spatial concepts), accessed by episode-level queries.",
            "spatial_representation": "Learned abstract spatial priors (World Embedding) plus linkage to explicit 3D point-cloud features and image-based geometry.",
            "embodied_task_type": "Vision-language navigation and spatio-temporal question answering",
            "embodied_task_name": "VLN-CE, VSI-Bench (used within VEME)",
            "visual_domain": "Simulation and real indoor video domains (as used by VEME)",
            "performance_metric": "SPL (navigation ablations) and task accuracy metrics (VSI)",
            "performance_value": "Ablation: removal of Spatial Semantic Memory: SPL drop from Full 65.1 to 55.3 (Table 3); reported delta −9.8 pp in C.4 analysis.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "w/o Spatial Memory: SPL 55.3 (Full 65.1 → −9.8 pp).",
            "grounding_improvement": "Spatial memory contributes ~+9.8 absolute points to SPL in ablation experiments.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Effectiveness depends on ability to link visual semantics to geometry; failures occur when geometry or trajectory data are noisy or missing since E_world relies on accurate grounding signals during training.",
            "failure_mode_analysis": "Removal causes substantial performance degradation (−9.8 SPL points); paper qualitatively links absence to errors in grounding referential phrases (e.g., 'the kitchen you just passed').",
            "domain_shift_handling": "Included in SFT training data mixture (real + simulated) to help generalization; further domain-adaptation proposed for future work.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Cross-attention between episodic queries and the World Embedding; integrated into VLM input sequence for joint reasoning.",
            "sample_efficiency": null,
            "key_findings_grounding": "A learned world embedding acting as a spatial semantic memory enables recall of task-relevant geometric-semantic priors and measurably improves navigation efficiency; its removal yields a near-10-point SPL drop, confirming its role as a grounding scaffold.",
            "uuid": "e1956.1"
        },
        {
            "name_short": "Episodic Memory",
            "name_full": "Episodic Memory (F_episodic derived from Q_epi)",
            "brief_description": "An episode-specific memory trace formed by fusing global 3D geometry and trajectory encodings into a query Q_epi that attends to the World Embedding to produce a discriminative episodic representation used for retrieval and long-horizon reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Episodic Memory (Q_epi → F_episodic)",
            "model_description": "Episode fingerprint Q_epi = Linear(concat[F_pcd,0:Tobs ; F_traj,t]) attends to E_world producing F_episodic via CrossAttn(Q_epi, E_world); trained with an episodic contrastive loss L_episodic to make episodes separable and enable recall of previously visited locations and experiences.",
            "visual_encoder_type": "N/A (uses outputs of 3D backbone and trajectory encoder)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Cross-attention between episode query and World Embedding; episodic contrastive loss ties episodic traces to grounded visual features from the same episode.",
            "representation_level": "Episode-level (spatio-temporal trace for a full trajectory/video)",
            "spatial_representation": "Implicitly encodes trajectory + aggregated 3D geometry and links to E_world for recall of spatial priors",
            "embodied_task_type": "Long-horizon navigation, episodic reference resolution in instruction following, video QA",
            "embodied_task_name": "VLN-CE (example: 'the kitchen you just passed' resolution), VSI-Bench",
            "visual_domain": "Simulated long-horizon navigation and real video episodes",
            "performance_metric": "SPL (navigation), accuracy metrics for episodic reasoning tasks",
            "performance_value": "Ablation: removing Episodic Memory causes the largest reported drop: SPL from Full 65.1 → 49.8 (Table 3) and reported delta −15.3 percentage points (C.4).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "w/o Episodic Memory: SPL 49.8 (Full 65.1 → −15.3 pp).",
            "grounding_improvement": "Episodic memory contributes the largest single-module improvement (~+15.3 absolute points in SPL when present).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Episodic memory effectiveness depends on reliable aggregated geometry and trajectories; noisy/incomplete mapping or drifting trajectories limit episodic discrimination and recall.",
            "failure_mode_analysis": "Without episodic traces, agents fail to resolve temporal referents and often loop or select incorrect previously visited landmarks, producing large navigation performance degradation.",
            "domain_shift_handling": "Trained with a mix of real and simulated trajectories to improve generalization; future work aims for online updating / lifelong learning to keep episodic traces robust under domain shift.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Cross-attention between Q_epi and E_world; episodic features concatenated into VLM input for joint attention-based reasoning.",
            "sample_efficiency": null,
            "key_findings_grounding": "Episodic memory that links a trajectory's global geometry and actions to the shared world embedding is essential for resolving temporally-indexed instructions and yields the largest single-module benefit in navigation performance.",
            "uuid": "e1956.2"
        },
        {
            "name_short": "VGGT",
            "name_full": "VGGT: Visual Geometry Grounded Transformer",
            "brief_description": "An image-based visual-geometry encoder used to extract dense, view-dependent geometric cues (implicit depth/normals) from RGB frames for geometric grounding of semantics.",
            "citation_title": "VGGT: Visual Geometry Grounded Transformer",
            "mention_or_use": "use",
            "model_name": "VGGT (image-based geometric encoder)",
            "model_description": "Processes RGB frames to produce dense geometric feature maps (implicitly encoding depth gradients and surface normals) which are projected into the common latent space and cross-attended by semantic features to produce geometry-aware visual representations.",
            "visual_encoder_type": "VGGT-1B (visual-geometry encoder)",
            "visual_encoder_pretraining": "Pretrained on multiple geometry estimation tasks (depth, normals) as reported in the paper",
            "grounding_mechanism": "Provides image-derived geometric keys/values used in CrossAttn(F_sem,t, F_geo,t) so semantic features are grounded in per-view geometry; removal breaks geometric grounding pathway.",
            "representation_level": "Dense image-level geometric feature maps (view-dependent), used with patch-level semantics",
            "spatial_representation": "Implicit image-space geometry (depth/normals-like features) used to link semantics to spatial structure",
            "embodied_task_type": "Supports vision-language navigation and spatio-temporal reasoning within VEME",
            "embodied_task_name": "VLN-CE, VSI-Bench (as integrated in VEME)",
            "visual_domain": "RGB frames from simulation and real videos",
            "performance_metric": "VSI ACC / MRA and navigation SPL (indirect effect via VEME)",
            "performance_value": "Ablation: removing VGGT leads to VSI metric degradation (Full ALL 49.3 → w/o VGGT ALL 28.3; ACC 44.1 → 36.1; MRA 55.7 → 19.2) indicating critical role in spatio-temporal understanding.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "w/o VGGT: ACC 36.1, MRA 19.2, ALL 28.3 (compared to Full ACC 44.1, MRA 55.7, ALL 49.3).",
            "grounding_improvement": "VGGT provides large improvements for video spatio-temporal understanding (removal drops ALL by ~21 points in reported table).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "VGGT removal is explicitly evaluated and causes major drops; paper also ablates fine-tuning of these encoders showing further degradation.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors note that image-based geometric encoders are necessary to bind semantics to immediate spatial cues; poor geometric features limit grounding quality.",
            "failure_mode_analysis": "Removing VGGT produces large drops in MRA (temporal/numerical reasoning) and overall ALL score, indicating failures in tasks requiring fine-grained spatial/metric reasoning.",
            "domain_shift_handling": "VGGT is pretrained on geometry tasks; paper does not present explicit domain-adaptation for VGGT but fine-tunes with LoRA as part of VEME training.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Paper shows fine-tuning (LoRA/SFT) is important; removing fine-tune further decreases performance (w/o fine-tune: ALL 26.9). VGGT is used and ablated rather than compared across many pretrained scales.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Cross-attention where F_sem queries F_geo to produce F'_vis; then spatial contrastive loss supervises alignment.",
            "sample_efficiency": null,
            "key_findings_grounding": "Image-based geometric features (VGGT) are essential to ground 2D semantics into spatial structure; their removal causes major drops in spatio-temporal understanding metrics, confirming that combining semantic and geometric per-view features is crucial for embodied grounding.",
            "uuid": "e1956.3"
        },
        {
            "name_short": "Sonata",
            "name_full": "Sonata (3D point-cloud backbone)",
            "brief_description": "A sparse 3D Transformer backbone (Sonata) used to extract per-point 3D features from global point clouds to build persistent geometric representations for the cognitive map and episodic memory.",
            "citation_title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
            "mention_or_use": "use",
            "model_name": "Sonata (3D backbone)",
            "model_description": "Sparse 3D Transformer tailored for large-scale point clouds using voxel-based sparse attention to capture local geometry and long-range spatial relations; outputs per-point feature vectors F_pcd,t projected into the common VLM latent space and used for episodic query formation and world grounding.",
            "visual_encoder_type": "Sparse 3D Transformer (Sonata)",
            "visual_encoder_pretraining": "Pretrained on large-scale indoor 3D datasets including ScanNet and Matterport3D on a self-supervised reconstruction task (per paper)",
            "grounding_mechanism": "Provides persistent 3D geometric features that are concatenated with trajectory encodings to form episode queries Q_epi which attend to World Embedding; also supplies global geometry for spatial grounding.",
            "representation_level": "3D point-cloud (per-point) scene-level persistent geometry",
            "spatial_representation": "Explicit 3D coordinates (XYZ) + color used to build global geometric memory for cognitive map and episodic formation",
            "embodied_task_type": "Long-horizon navigation and 3D scene understanding",
            "embodied_task_name": "VLN-CE, episodic/video tasks in VSI-Bench",
            "visual_domain": "3D reconstructed indoor scenes from simulation and real dataset point clouds",
            "performance_metric": "Contributes to SPL and VSI metrics; no isolated numeric for Sonata alone provided",
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors explicitly note dependence on relatively clean 3D point clouds and accurate trajectory data for effective episodic and spatial memory formation; robustness to noisy SLAM outputs remains an open issue.",
            "failure_mode_analysis": "When reliable point-cloud inputs are not available (noisy/incomplete/drifting maps), episodic memory formation and recall degrade, hurting navigation and referent resolution; exact failure rates not quantified.",
            "domain_shift_handling": "Pretraining on diverse 3D datasets helps generalization; authors plan to include simulated sensor noise and more varied 3D environments in future training to improve robustness.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "3D features are projected into the common latent space and concatenated with other modalities; episodic Q_epi computed by fusing F_pcd and F_traj then attending to E_world.",
            "sample_efficiency": null,
            "key_findings_grounding": "A strong 3D backbone that provides persistent, global geometry is necessary for building discriminative episodic traces and for effective grounding of visual semantics into a cognitive map; however, reliance on clean point-clouds is a practical limitation.",
            "uuid": "e1956.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
            "rating": 2
        },
        {
            "paper_title": "VGGT: Visual Geometry Grounded Transformer",
            "rating": 2
        },
        {
            "paper_title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
            "rating": 2
        },
        {
            "paper_title": "Vision-and-Language Navigation with Episodic Scene Memory",
            "rating": 2
        },
        {
            "paper_title": "Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces",
            "rating": 2
        }
    ],
    "cost": 0.021481,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment
29 Aug 2025</p>
<p>Jinzhou Tang 
Sun Yat-sen University</p>
<p>Jusheng Zhang 
Sun Yat-sen University</p>
<p>Sidi Liu 
Sun Yat-sen University</p>
<p>Waikit Xiu 
Sun Yat-sen University</p>
<p>Qinhan Lv 
Sun Yat-sen University</p>
<p>Xiying Li 
Sun Yat-sen University</p>
<p>Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment
29 Aug 2025402C0B48B6D4DCC4F54AF2D73B8CF7F1arXiv:2509.00210v1[cs.CV]
Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence.While advanced visionlanguage models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension.To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an egocentric, experience-centered world model.Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM incontext learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometricsemantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration.By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments.Experimental results on VSI-Bench and VLN-CE demonstrate 3%-6% accuracy and exploration efficiency improvement compared to traditional approaches.</p>
<p>Introduction</p>
<p>Embodied agents navigating unknown environments face a fundamental challenge: how to develop spatial understanding and make navigation decisions with limited visual observations (Zou et al. 2025), much like humans leverage episodic memories and spatial cognition (Bermudez-Contreras, Clark, and Wilber 2020; Coppolino and Migliore 2023; Epstein et al. 2017).While vision-language models (VLMs) have achieved remarkable success in static visual understanding (Sarch et al. 2023), their direct application to embodied navigation tasks reveals critical limitations in spatio-temporal reasoning and the integration of long-term memory (Liu et al. 2025).</p>
<p>The core challenge lies in the embodied reasoning gap: while VLMs demonstrate strong visual understanding (Liu et al. 2023;Ma et al. 2023), they lack the spatial grounding necessary to translate visual observations into effective linguistic reasoning and navigation decisions (Shahria et al. 2022).This manifests as difficulties in understanding 3D spatial relationships, building persistent spatial representations, and connecting visual semantics with navigational affordances (brian ichter et al. 2022;Liu et al. 2021;Zheng, Huang, and Wang 2025).This limitation manifests in two critical ways: first, agents cannot effectively recall and utilize relevant past experiences when encountering similar spatial contexts (Park et al. 2023); second, they struggle to align visual semantics with geometric spatial relationships, resulting in poor spatial reasoning in complex environments (Yang et al. 2025;Yin et al. 2025).Existing approaches attempt to address these limitations through different paradigms.SLAM-based methods construct explicit spatial maps with semantic annotations, but they lack the high-level reasoning capabilities necessary for instruction-following and common-sense spatial reasoning (Ren et al. 2024;Chen et al. 2025).LLM/VLM-based approaches leverage powerful language reasoning but fail to capture fine-grained spatio-temporal dependencies.Recent multimodal approaches show promise but remain limited by inadequate cross-modal alignment between visual observations and spatial representations, preventing effective episodic memory formation and recall (Huang et al. 2024;Cheng et al. 2025b).Our key insight is drawn from cognitive neuroscience: human spatial intelligence succeeds through the complementary interaction between episodic memory (specific, spatio-temporal experiences) and semantic memory (general spatial knowledge) (Moscovitch et al. 2005;Nadel and Hardt 2011).We propose that embodied agents can achieve similar capabilities by learning to align visual experiences with spatial semantic representations, enabling both episodic memory formation and context-relevant recall.</p>
<p>To address the limitations of VLMs in navigation and embodied Q&amp;A, we propose a cognitively inspired dualmemory framework VEME that integrates spatial semantics and episodic experience through three key components: a spatio-temporal encoder that captures geometry-aware visual observations and action histories fused with a learnable world embedding; a cross-modal alignment mechanism that bridges 2D visual semantics and 3D spatial representations via bidirectional attention and contrastive learning;  (a) Our agent, equipped with a cognitively inspired dualmemory framework, constructs an implicit cognitive map from world embedding, observation history, and past trajectory.This allows it to ground the complex spatial instruction ("kitchen behind the sofa") in the environment, devise an efficient path, and successfully reach the goal.(b) In contrast, a conventional agent lacking this spatio-temporal memory and reasoning capability fails to comprehend the spatial relationships, leading to incorrect exploration and task failure.</p>
<p>and a VLM-based decision module that leverages structured input tokens incorporating current perception, spatial priors, episodic cues, and language instructions.This design enables the agent to model long-horizon spatio-temporal contexts and generalize navigation and reasoning behavior across novel environments without requiring explicit mapping.</p>
<p>Our framework demonstrates considerable improvements over existing methods on standard benchmarks, including VSI-Bench and VLN-CE, particularly in zero-shot transfer scenarios.The main contributions include: (1) introducing a cognitively-inspired dual memory framework for spatial intelligence; (2) developing effective cross-modal alignment techniques for spatio-temporal reasoning; and (3) validating the approach's effectiveness in reducing exploration costs while improving task completion rates in unknown environments.</p>
<p>Related Work</p>
<p>Embodied and Vision-Language Navigation Early embodied agents learned navigation using reinforcement learning (RL) (Chen et al. 2020;Liang et al. 2023;Zhang et al. 2025;Sheng 2025), but their reliance on extensive environment interactions for policy optimization limited their ability to generalize to new scenes.The subsequent paradigm of Vision-Language Navigation (VLN) (Anderson et al. 2018) enabled agents to follow natural language instructions, offering more flexible task specification.However, a common limitation of these approaches is the lack of a persistent spa-tial memory, which hinders performance on long and complex routes that require recalling previously visited locations.</p>
<p>Memory in Embodied Agents</p>
<p>To succeed in longhorizon tasks, an agent requires robust memory (Zheng et al. 2025).One line of work focuses on explicit memory, using techniques like SLAM to build precise but computationally expensive 3D geometric maps (Jia et al. 2022).An alternative is implicit memory, where models like RNNs encode history into a compact state vector; this approach is efficient but can struggle to retain critical long-term details (Hong et al. 2021).Our work addresses the remaining challenge of how to efficiently retrieve the most task-relevant information from an agent's past experiences.</p>
<p>Large Models for Embodied Control Recently, Large Language Models (LLMs) and Vision-Language Models (VLMs) have been leveraged for high-level planning in robotics, capitalizing on their vast common-sense knowledge (Yu, Kasaei, and Cao 2023;Li et al. 2025).Despite their powerful reasoning capabilities, these models often exhibit an "embodied reasoning gap" (Li et al. 2024b).Because they are trained on disembodied internet data, their generated plans can be disconnected from an agent's physical capabilities and the constraints of a 3D environment.Our work aims to bridge this gap by embedding geometry-aware priors directly into the model's reasoning process.</p>
<p>Cross-Modal Alignment for Embodied Reasoning Effective embodied reasoning requires robust cross-modal alignment between vision, language, and action.While recent generalist agents have made strides in multimodal fusion (Cheng et al. 2025a;Huang et al. 2024), a persistent challenge is maintaining a tight, continuous link between an agent's egocentric visual perception and its evolving, allocentric 3D spatial understanding.Many methods struggle to ground visual information in a coherent spatial context over time.Our work introduces a dedicated framework to address this spatio-temporal alignment problem by explicitly linking visual semantics to a dynamic geometric representation.</p>
<p>Methodology</p>
<p>Our work introduces a general framework to endow Vision-Language Models (VLMs) with robust spatial intelligence.The central challenge we address is that standard VLMs, despite their impressive semantic capabilities, operate on a "flat" perception of the world, as shown in Figure 1.They lack an intrinsic understanding of 3D geometry and are stateless, treating each moment in isolation without a memory of the past.To overcome these fundamental limitations, we propose a dual-memory architecture, inspired by the synergistic roles of semantic and episodic memory in human cognition.As illustrated in Figure 2, this architecture systematically equips the VLM with (1) a Spatial Semantic Memory to build a general, reusable understanding of 3D space, and (2) an Episodic Memory to form unique representations of specific spatio-temporal experiences.The result is a versatile reasoning engine for any task requiring the synthesis of video, text, 3D geometry, and trajectory data.</p>
<p>Premilinaries: Input Representations</p>
<p>To ground the agent's reasoning, we must first transform its raw, multimodal sensory inputs into a unified highdimensional feature space.The model processes five primary data streams: the language instruction T , the current RGB frame I t , the global 3D point cloud P t , and the history of actions A 0:t−1 .Each stream is encoded by a specialized module.</p>
<p>Language Instruction Embedding.The natural language instruction T provides the high-level task goal and context description.We tokenize it and convert it into a sequence of dense vector representations using an embedding layer:
H T = Embed(Tokenize(T )) ∈ R L×D (1)
where L is the sequence length and D is the model's hidden dimension.</p>
<p>Visual Semantic Encoding.To capture the semantic content of the agent's view ("what"), the RGB frame I t is processed by a standard pretrained vision encoder.Its features are then projected into the model's common latent space:
F vis,t = Project vis (VisionEncoder(I t )) ∈ R Nvis×D (2)
Image-based Geometric Encoding.To extract immediate, view-dependent geometric cues ("where" from the current view), we feed the same RGB frame I t into a specialized foundation visual-geometry encoder (i.e., a pretrained VGGT (Wang et al. 2025)) that can implicitly estimate depth or normals.This provides geometric features corresponding directly to the visual semantics:
F geo,t = Project geo (3DAwareEncoder(I t )) ∈ R Ngeo×D
(3) The cross-modal alignment module will later focus on fusing F vis,t and F geo,t to ground semantics in their immediate spatial context.</p>
<p>Point Cloud Feature Encoding.For a global, persistent understanding of the environment's structure, we process the full 3D point cloud P t ∈ R N pcd ×6 .A dedicated 3D backbone network extracts powerful geometric features (See Architecture Details in the Appendix), which are subsequently projected:
F pcd,t = Project pcd (3DBackbone(P t )) ∈ R Npcd×D (4)
These features, F pcd,t , provide the foundational geometric representation for building the cognitive map and episodic memory.</p>
<p>Spatio-temporal Trajectory Encoding.The history of actions A 0:t−1 is essential for long-term context.Each action a i is mapped to a learnable embedding e i ∈ R D .A Transformer encoder then processes the sequence of these embeddings E a = [e 0 , . . ., e t−1 ] to model temporal dependencies:
F traj,t = TransformerEncoder(E a ) ∈ R t×D (5)</p>
<p>Geometric-Semantic Memory as Cognitive Map</p>
<p>Motivation.</p>
<p>The first core deficit we address is the VLM's spatial naivety.To move beyond simple 2D object recognition towards genuine spatial reasoning, the model needs an internal "cognitive map"-a repository of abstract, reusable spatial knowledge (e.g., "corridors connect rooms," "tables afford placing objects").We instantiate this concept as a learnable parameter matrix, the World Embedding (E world ∈ R Nw×D ), which serves as a global, environmentagnostic memory of N w fundamental spatial concepts.</p>
<p>Grounding Perception in Spatial Reality.However, a memory is useless if it cannot be accessed.The model must learn to connect its immediate, concrete perception to this abstract knowledge base.To achieve this, we must first ground the 2D semantic features F sem,t in the 3D reality of the scene.We accomplish this by using F sem,t to query the detailed geometric features extracted from the same image, creating a geometrically-aware visual representation, F ′ vis,t .
F ′ vis,t = F sem,t + CrossAttn(F sem,t , F geo,t )(6)
Where CrossAttn(a, b) treats a as query and b as key &amp; value.This step forces the model to view semantics through the lens of geometry.</p>
<p>Geometric Alignment.This attention mechanism, left unsupervised, does not guarantee a meaningful link.To provide explicit supervision, we introduce the contrastive loss L spatial .Its purpose is to enforce a strong, unique correspondence between the geometrically-grounded feature F ′ vis,t and its original semantic source F sem,t .
L spatial = − log exp(sim(F ′ vis,t , F sem,t )/τ ) j∈B exp(sim(F ′ vis,t , F sem,j )/τ )(7)
By training the model to correctly identify the positive pair (F ′ vis,t , F sem,t ) from a batch B of negative distractors, we compel it to learn a non-trivial mapping that truly binds visual semantics to their underlying spatial structure.A complete list of all hyperparameter values can be found in the Train Method in the Appendix.</p>
<p>Episodic Memory: Learning from Experience</p>
<p>Motivation.The second deficit is the VLM's statelessness, or "amnesia."To learn from the flow of events, the model must be able to form a distinct memory trace, or "fingerprint," for each unique spatio-temporal experience (an "episode," such as a complete video).This memory should capture the essence of a specific journey.</p>
<p>Formulating an Episodic Trace.An episode's identity is defined by its unique path through space and time.We distill this identity into a single query vector, Q epi , by fusing the comprehensive geometric information of the episode with its trajectory encoding.
Q epi = Linear(Concat[F pcd,0:Tobs ; F traj,t ])(8)
This query represents the unique "what" (geometry) and "how" (trajectory) of the experience.To transform this query into a rich memory trace, we use it to attend to our generalpurpose World Embedding.
F episodic = CrossAttn(Q epi , E world ) (9)
This elegantly models the cognitive process of interpreting a specific experience by seeing which general concepts from our "cognitive map" it activates.</p>
<p>Episodic Alignment.A functional memory system requires that different memories be distinguishable.To enforce this, we introduce the episodic contrastive loss, L episodic .It trains the model to generate representations that are similar for samples from the same episode but distinct from all other episodes.
L episodic = − log exp(sim(F epi,i , F ′ vis,p )/τ ) k∈B,k̸ =i exp(sim(F epi,i , F ′ vis,k )/τ ) (10) Here, (F epi,i , F ′ vis,p
) is a positive pair drawn from the same episode, while all features from different episodes act as negatives.This objective directly cultivates the creation of a discriminative episodic memory.</p>
<p>Unified Decision-Making and Training</p>
<p>Ultimately, these memory systems must inform the VLM's final output.We achieve this through an elegant and direct integration.We construct a single, unified input sequence for the VLM that concatenates all available streams of information:
T in = <a href="11">H T ; F ′ vis,t ; F traj,t ; E world </a> By including the textual instruction, the grounded visual percept, the episodic context, and the entire spatial semantic memory, we empower the VLM's native self-attention mechanism to holistically reason across all information sources.It can dynamically weigh what is most relevant-the instruction, the current view, past experience, or general world knowledge-to generate the final output.The entire architecture is trained end-to-end with a composite objective that reflects our cognitive design:
L total = L CE + λ s L spatial + λ e L episodic (12)
Here, the primary task loss (e.g., cross-entropy for text generation, L CE ) is guided by our two auxiliary losses.L spatial and L episodic are therefore not mere regularizers; they are the essential supervisory signals that construct the cognitive scaffolding for advanced spatial intelligence.</p>
<p>Experiment</p>
<p>We conduct a comprehensive set of experiments to validate the effectiveness and generality of our dual-memory architecture.Our evaluation is designed to answer three key questions: (1) Does our general-purpose model achieve competitive performance against specialized, state-of-the-art methods on diverse spatial reasoning tasks?(2) What is the specific contribution of each component within our proposed architecture?(3) Can our model exhibit genuine, interpretable spatial understanding beyond simply achieving high scores?</p>
<p>Experimental Setup</p>
<p>Benchmarks.To thoroughly evaluate our model's spatial intelligence, we selected three benchmarks that test distinct facets of this capability.</p>
<p>• VLN-CE (Vision-and-Language Navigation in Continuous Environments): Typical benchmarks for embodied AI require intelligences to follow natural language commands to navigate in real 3D environments, evaluating the ability of models to incorporate language into dynamic first-view action and observation sequences, as well as situational memory and trajectory encoding effects, with metrics such as Success Rate (SR) and Path Length-Weighted Success Rate (SPL).• VSI-Bench (Visual-Spatial Intelligence Benchmark):</p>
<p>Containing more than 5000 question and answer pairs covering nearly 290 videos of real indoor scenes, including configuration, measurement estimation, and spatiotemporal tasks, MLLM's visuospatial intelligence is evaluated through zero-sample reasoning, measured by accuracy rate (ACC) for multiple choice tasks, and by mean relative accuracy (MRA) for numerical tasks.</p>
<p>Baselines.We use two types of baseline models (see Baseline Details in the Appendix for a complete list of baseline models) for systematic performance evaluation: first, a comparative analysis with unimproved standard visual language models (e.g., generalized VLMs such as QwenVL, Gemini, etc.); and, second, an in-depth comparison with the current optimal dedicated models for each specific benchmark task (e.g., the top navigational agents in the VLN-CE benchmarks , the specialized evaluation model in VSI-bench) for in-depth comparison.The experimental results show that our models exhibit industry-leading performance advantages in each task scenario.</p>
<p>Implementation Details.Our framework is built upon the Qwen-2.5-VL-7BVLM (Bai et al. 2025a).We use the pretrained DINOv2 (Oquab et al. 2023)</p>
<p>Main Results</p>
<p>Visual Navigation To evaluate the proposed model's capabilities in visual navigation, we conduct experiments on the VLN-CE dataset.Several state-of-the-art methods are selected for comparison, including CMA, ETPNav, DreamWalker, NaVid, and NaVILA, which represent mainstream approaches in the field.The performance of each model is assessed using the R2R Val-Unseen and SPL metrics, with the results summarized in Table 1.</p>
<p>The experimental results demonstrate that the proposed model exhibits significant superiority in the Visual Navigation task.In the R2R Val-Unseen test, our method achieved a success rate (SR) of 57.0 and a path length weighted success</p>
<p>Ablation Studies</p>
<p>To validate our proposed modules, we conducted ablation studies for both visual navigation and spatio-temporal understanding tasks.All experiments were repeated with five different random seeds, with results averaged across runs.Wilcoxon signed-rank tests confirmed statistically significant performance degradation when removing any module (p &lt; 0.01 in all cases), demonstrating each component's effectiveness.See section Ablation Experiment of Appendix for details.</p>
<p>Visual Navigation To dissect our model and quantify the contribution of each component, we conducted a series of ablation studies on the VLN-CE, which represent dynamic and static reasoning respectively.As shown in Table 3, the impact of each component is evident: removing the Spatial Semantic Memory results in a substantial performance drop, underscoring its essential role in grounding the agent within the 3D environment; excluding the Episodic Memory causes the most severe degradation, indicating its importance in enabling the agent to reason over its navigation history; and the absence of other components such as Geometric Grounding and Trajectory Input also leads to notable declines in per- Spatial-temporal Understanding To assess the contribution of each module and operation to the model's spatial reasoning ability, we conducted an ablation study.As shown in Table 4, the full model consistently achieves the highest performance across all metrics, demonstrating strong spatial understanding capabilities.Removing the VGGT component leads to a significant decline in performance, underscoring its critical role in spatial feature representation.Furthermore, excluding SFT (fine-tuning training) results in an even greater performance drop, highlighting the necessity of fine-tuning for effective model adaptation.These findings confirm that each component contributes meaningfully to the overall architecture and plays a vital role in enhancing spatial comprehension.</p>
<p>Qualitative Analysis</p>
<p>To provide intuitive evidence of our model's improved reasoning, we examined specific failure cases of baselines that our model handles correctly, as shown in Figure 3.For instance, on VLN-CE, given the instruction "Go back to the kitchen you just passed and wait by the sink," the standard VLM often gets stuck in a loop or navigates to a different kitchen, lacking the episodic context to understand "the kitchen you just passed."Our model, leveraging its episodic feature F episodic , correctly identifies the previously visited location and successfully completes the task, as illustrated in Figure 4. On VSI-bench, when asked "What is supporting the laptop?" in a scene where a laptop is on a desk, the baseline model sometimes fails by answering with a nearby but incorrect object, like "a chair."Our model, guided by the learned spatial priors in E world , correctly identifies the "support" relationship and answers "the desk."This reasoning process is confirmed by visualizing the model's internal attention mechanisms (See Feature Visualization in the Appendix).The results show our model correctly focuses on the relevant supporting object, validating the effect of our learned spatial priors.</p>
<p>Conclusion</p>
<p>We introduce VEME, a novel cross-modal framework enhancing embodied agents' reasoning in dynamic, uncertain environments by aligning visual semantics with spatio-temporal cues.Drawing from cognitive neuroscience, VEME's dual-memory system (episodic and semantic) builds geometry-aware world models, greatly boosting nav-</p>
<p>A Model and Training Details</p>
<p>A.1 Train Data</p>
<p>To accomplish the tasks of visual navigation and video spatial understanding, we have collected a variety of datasets to enable more comprehensive training.</p>
<p>Visual Navigation Task For the visual navigation task, we designed a supervised fine-tuning dataset (SFT data blend), which consists of the following four categories of data: Navigation data from real videos: By collecting trajectory videos from real-world scenes, we provide the model with continuous navigation examples in realistic environments.</p>
<p>Navigation data from simulation environments: Using the R2R-CE and RxR-CE datasets, which provide sparse path point sequences converted from discrete VLN versions, we integrate these datasets into the Habitat simulator.This allows us to generate navigation video sequences based on the geodesic shortest path.These videos include consecutive frames and corresponding action labels, helping the model learn navigation capabilities in simulated environments.</p>
<p>Auxiliary navigation data: We use augmented instructions generated by EnvDrop and introduce auxiliary tasks such as navigation trajectory summarization.Through these data, the model learns to describe trajectories across different time segments.</p>
<p>General VQA datasets: To improve the model's generalization capabilities, we incorporate multiple general visual question answering (VQA) datasets.These datasets cover a wide range of scenes and real-world environments, providing the model with extensive training samples.</p>
<p>Video Spatial Understanding Task For the video spatial understanding task, we integrate multiple video questionanswering datasets:</p>
<p>ScanQA dataset: This dataset focuses on real-world 3D scene question-answering tasks, featuring free-form QA pairs grounded in 3D objects.Using this data, the model learns to reason and answer questions in 3D spaces.</p>
<p>Custom video QA dataset: To further enhance the model's performance, we created custom datasets containing video data from various real-world scenarios.These datasets encompass diverse viewpoints, lighting conditions, and dynamic changes, providing the model with more challenging and diverse training samples.</p>
<p>By integrating these datasets, we effectively enhance the model's capabilities in both navigation and video spatial understanding tasks, enabling it to perform well not only in simulation environments but also in real-world scenarios.</p>
<p>A.2 Train Strategy</p>
<p>To enhance the model's performance on video spatial understanding tasks, we fine-tuned the Qwen model using LoRA (Low-Rank Adaptation).The core idea of LoRA is to achieve parameter-efficient optimization by inserting trainable low-rank matrices without making large-scale adjustments to the original weights of the pre-trained model.This approach allows the model to adapt to downstream task requirements while maintaining its performance.Specifically, LoRA decomposes the weight matrix W ∈ R d×k into two low-rank matrices A ∈ R d×r and B ∈ R r×k .Through low-rank decomposition, the weight update formula can be expressed as W ′ = W + A • B. During this process, the original weights W remain frozen, and only the newly added low-rank matrices A and B are optimized.We inserted LoRA modules into key components of the model, including the query projection (q proj), value projection (v proj), and language model head (lm head), to enhance the model's ability to model dynamic relationships between video frames and perform cross-modal reasoning.</p>
<p>In the actual fine-tuning process, we set the LoRA hyperparameters as follows: rank r = 8, LoRA scaling factor α = 32, and a Dropout probability of 0.1 to reduce the risk of overfitting.The LoRA modules were embedded into the model's multi-head self-attention mechanism and output layers, enabling more efficient adaptation to the requirements of video spatial understanding tasks.This method is particularly suitable for tasks involving multi-view video frames, such as reasoning about 3D spatial relationships from videos or answering scene-related questions.By incorporating LoRA, the model can better capture dynamic changes between video frames and improve its understanding of complex 3D scenes.</p>
<p>For video spatial understanding tasks, the use of LoRA allows us to efficiently optimize the model, making it more adaptable to downstream tasks while maintaining reasonable computational resource usage.This fine-tuning approach provides strong technical support for our video spatial understanding tasks.</p>
<p>A.3 Hyperparameter Details</p>
<p>To ensure the reproducibility of our results, we provide a comprehensive list of the key hyperparameters used for training the VEME model.We utilized the AdamW optimizer for stable and efficient training.The primary hyperparameters for the optimization and training process are summarized in Table 5.These settings were kept consistent across all main experiments unless otherwise specified in the ablation studies.</p>
<p>LoRA Configuration.As detailed in the 'Train Strategy' subsection, we employed Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning.The rank (r) was set to 8, providing a good balance between expressiveness and parameter efficiency.The scaling factor (α) was set to 32.To prevent overfitting on the adapter weights, a dropout rate of 0.1 was applied specifically to the LoRA modules.We targeted the query (q proj) and value (v proj) projections within the VLM's self-attention layers, as well as the final language model head (lm head), as these are critical for adapting the model's reasoning and generation capabilities to our specific tasks.</p>
<p>B Model Architecture Details</p>
<p>Our proposed framework, VEME, is constructed around a central Vision-Language Model (VLM) and is augmented by a series of specialized encoders.Each encoder is designed to process a specific modality (vision, geometry, trajectory), q proj, v proj, lm head transforming raw sensory input into rich feature representations.These features are then projected into a common embedding space, allowing the core VLM to perform holistic reasoning across all available information streams.Below, we detail the specifics of each key component.</p>
<p>Core VLM Backbone.The heart of our model is the Qwen-2.5-VL-7B(Bai et al. 2025b), a powerful pretrained VLM that serves as our primary reasoning engine.We leverage its advanced capabilities in understanding and integrating vision and language.The input to this model is a carefully structured sequence of embeddings from various sources, as described in the main paper.We utilize the LoRA fine-tuning technique on its attention and feed-forward layers to adapt it to the downstream embodied tasks without catastrophic forgetting of its pretrained knowledge.</p>
<p>Visual Semantic Encoder.To extract high-level semantic information from each RGB frame I t , we employ a pretrained DINOv2-ViT-L/14 model (Oquab et al. 2023).</p>
<p>DINOv2 is renowned for its strong, self-supervised learned features that exhibit excellent performance on various downstream tasks without fine-tuning.For each frame, we extract the patch features from the final layer of the encoder.These features, which capture the objects and their appearance, are then passed through a two-layer Multi-Layer Perceptron (MLP) to project them from their native dimension to the VLM's hidden dimension D. This yields the semantic feature representation F vis,t .</p>
<p>Image-based Geometric Encoder.Complementary to the semantic features, we extract immediate, view-dependent geometric cues directly from the 2D image I t .For this, we use a specialized visual-geometry encoder, specifically a version of VGGT-1B (Wang et al. 2025) pretrained on multiple geometry estimation tasks.This model processes the RGB frame and outputs a dense feature map that implicitly encodes geometric information such as depth gradients and surface normals.These geometric features F geo,t provide the spatial context necessary to ground the semantic features F vis,t through our cross-attention mechanism.Like other features, these are also projected to dimension D via an MLP.</p>
<p>3D Point Cloud Backbone.This is a critical component for building a persistent and global understanding of the environment's 3D structure.As mentioned in the main text, we use the Sonata (Wu et al. 2025b) model as our dedicated 3D backbone.</p>
<p>• Architecture: Sonata is a state-of-the-art sparse 3D Transformer architecture.It is specifically designed to efficiently process large-scale point clouds, which are common in real-world navigation scenarios.It utilizes a hierarchy of sparse voxel-based attention mechanisms, allowing it to capture both local geometric details and longrange spatial relationships without incurring the prohibitive computational cost of standard Transformers.</p>
<p>• Input: The model takes the global point cloud P t ∈ R N pcd ×6 as input, where each point is represented by its XYZ coordinates and RGB color values.</p>
<p>• Pre-training: The Sonata backbone we use has been pretrained on a large-scale collection of 3D indoor scene datasets, including ScanNet (Dai et al. 2017) and Matter-port3D (Chang et al. 2017), on a self-supervised reconstruction task.This pre-training endows it with a powerful, intrinsic understanding of 3D geometric primitives and typical spatial layouts of indoor environments.</p>
<p>• Output: The model outputs a set of per-point feature vectors F pcd,t ∈ R Npcd×D after being passed through a final projection layer.These features provide the foundational geometric representation for both our cognitive map and episodic memory modules.</p>
<p>Spatio-temporal Trajectory Encoder.To capture the agent's movement history, we encode the sequence of past actions A 0:t−1 .Each discrete action (e.g., 'move forward', 'turn left') is first mapped to a learnable embedding vector of dimension D. This sequence of action embeddings is then processed by a 4-layer Transformer encoder with 8 attention heads.The output of this encoder, F traj,t , provides a contextualized representation of the agent's path, capturing temporal dependencies within the action sequence.</p>
<p>C Additional Experimental Results</p>
<p>C.1 Baseline details</p>
<p>Specialized Navigation Models For all specialized navigation baselines on VLN-CE, including CMA, ETPNav, and NaVILA, we adhered to the following protocol to ensure a fair and reproducible comparison:</p>
<p>• Source: We utilized the official codebases and pretrained model weights released by the respective authors.</p>
<p>No architectural modifications were made.• Evaluation Protocol: We followed the standard evaluation scripts and environment setups provided with each baseline's repository.Results were generated on the val-unseen split as reported in the original papers.</p>
<p>General Vision-Language Models For all generalpurpose VLMs evaluated on VSI-Bench, such as GPT-4o, Gemini-1.5Pro, the LLaVA series, and the Qwen series, we used the following zero-shot evaluation setup:</p>
<p>• Model Version: We used the latest available official APIs for proprietary models (e.g., gpt-4o-2024-05-13) and the official Hugging</p>
<p>Face implementations for open-source models.• Prompting Strategy: A consistent, minimal prompt template was employed across all models to query their spatial reasoning capabilities without providing few-shot examples.The templates were structured as follows:</p>
<p>Multiple-Choice Questions: The following is a video of an indoor scene.Based on the video, answer the following question by choosing the best option.For VSI-Bench, we report Accuracy (ACC) for multiplechoice questions and Mean Relative Accuracy (MRA) for numerical questions.• Inference Setup: All our models were evaluated on a single NVIDIA A100 GPU.We used greedy decoding (i.e., beam size of 1) for generating all responses to ensure efficiency and deterministic outputs.All reported scores are the average of three evaluation runs with different random seeds to ensure statistical stability.</p>
<p>C.4 Ablation Experiment</p>
<p>To assess the significance of performance changes in our ablation studies, we used the Wilcoxon signed-rank test, which is ideal for small sample sizes (n=5) and does not rely on specific data distribution assumptions.The null hypothesis posits that the median difference between paired observations is zero.If we reject the null hypothesis (p &lt; 0.01), it indicates a statistically significant performance drop after removing certain modules.We calculated the performance metrics for different configurations, including the full model and various ablations.Our analysis revealed significant performance degradation across all removed components.Notably, the removal of episodic memory resulted in the largest decrease (Delta = −15.3%,p = 0.0003), followed by trajectory input (∆ = −12.9%,p = 0.0005), spatial memory (∆ = −9.8%,p = 0.0007), and geometric grounding (∆ = −3.9%,p = 0.0011).All p-values were below the 0.01 threshold, underscoring the critical roles these components play in overall system performance.</p>
<p>In a similar analysis for spatio-temporal understanding, we found that removing the vggt component led to significant drops across all metrics, while fine-tuning removal produced slightly stronger effects.The dramatic reduction in mean recognition accuracy (MRA) further emphasizes the importance of these components for effective temporal reasoning, with all p-values also falling below 0.01.</p>
<p>C.5 Additional Qualitative Results</p>
<p>To provide a more comprehensive understanding of our model's capabilities and limitations, we present additional qualitative examples below.</p>
<p>Success Case</p>
<p>The main text has already highlighted our model's ability to follow long and complex instructions, successfully navigating through multiple rooms, while a strong baseline fails by becoming stuck in a local loop.</p>
<p>Failure Case Visualization and Analysis Our model has its limitations, as illustrated in Figure 6.This figure presents a failure case with the instruction: "Walk into the living room and keep walking straight past the living room.Then walk into the entrance under the balcony and wait at the entrance to the other room."Despite the clear visual cues provided in the left image, the model struggles to accurately interpret the spatial layout, leading to its failure to select the optimal path.</p>
<p>D Feature Visualization</p>
<p>To provide a more intuitive understanding of our model's internal mechanisms and validate its claimed capabilities, we present a series of qualitative visualizations.These analyses aim to demystify how our model learns discriminative spatio-temporal representations, grounds semantics in geometry, and reasons about complex, instruction-driven tasks.</p>
<p>We sought to verify that our Episodic Memory module creates unique and separable representations for different spatio-temporal experiences.To visualize this, we sampled a collection of distinct navigation episodes from the validation  set and extracted the final episodic feature vector, F episodic , for each.Using the t-SNE dimensionality reduction algorithm, we projected these high-dimensional vectors into a 2D space.The result, shown in Figure 5, is a scatter plot where each point represents a complete episode, colored by its environment ID, and its corresponding distance distribution.The clear clustering of points with the same color provides strong evidence that our model, guided by the L episodic loss, successfully learns to form a discriminative memory space.This ability to distinguish between different journeys is fundamental for long-term reasoning and avoiding ambiguity.</p>
<p>E Limitations and Future Work</p>
<p>While our proposed VEME framework demonstrates significant advancements in equipping embodied agents with spatio-temporal intelligence, we acknowledge several limitations that pave the way for compelling future research.First, our model's multi-encoder architecture, while powerful, introduces a notable computational overhead in terms of both memory (VRAM) and floating-point operations (FLOPs) during inference.This currently may hinder its deployment on resource-constrained robotic platforms with limited onboard computing power.Second, the performance of our geometric and episodic memory modules is, to some extent, contingent on the availability of relatively clean 3D point clouds and accurate trajectory data during the training phase.The model's robustness against noisy, incomplete, or drifting map data generated by real-world SLAM systems is an important area for further investigation.Finally, our current framework operates under a static environment assumption.It is not designed to handle dynamic scenes with moving objects, interacting agents, or significant changes in layout during an episode, which limits its applicability in more complex, human-centric settings.</p>
<p>Addressing these limitations points toward several exciting future directions.A primary focus will be on improving model efficiency.We plan to explore model compression techniques, such as knowledge distillation from our larger model to a more compact student network, and post-training quantization to create a "VEME-Lite" version without substantially compromising performance.This would be a critical step towards real-world robotic deployment.</p>
<p>Another key avenue is to enhance the model's robustness and generalization.To bridge the sim-to-real gap highlighted by our data dependency, we aim to train VEME on a much wider variety of 3D environments, incorporating simulated sensor noise and diverse visual styles.Furthermore, we will investigate advanced learning paradigms, such as meta-learning or online domain adaptation, to enable the agent to rapidly fine-tune its world model when introduced to a completely new and unseen environment.</p>
<p>Perhaps the most significant long-term vision is to transcend the offline training paradigm towards interactive and lifelong learning.This involves developing mecha-nisms for the agent to continuously update its semantic and episodic memories based on its own experiences and interactions within the world.By integrating reinforcement learning principles, the agent could learn from trial-and-error, associate failed plans with specific environmental states, and implicitly refine its understanding of physical affordances.Such a system would move us closer to creating truly adaptive and autonomous embodied intelligences that learn and grow over their entire operational lifetime.</p>
<p>Figure 1 :
1
Figure 1: An illustrative comparison of navigation behaviors.(a) Our agent, equipped with a cognitively inspired dualmemory framework, constructs an implicit cognitive map from world embedding, observation history, and past trajectory.This allows it to ground the complex spatial instruction ("kitchen behind the sofa") in the environment, devise an efficient path, and successfully reach the goal.(b) In contrast, a conventional agent lacking this spatio-temporal memory and reasoning capability fails to comprehend the spatial relationships, leading to incorrect exploration and task failure.</p>
<p>Figure 2 :
2
Figure 2: Overview of our Dual-Memory Framework for Spatial Intelligence.Our framework endows a Vision-Language Model with robust spatial intelligence through two synergistic memory systems.The Spatial Semantic Memory learns a general cognitive map by aligning 2D visual features with 3D geometric features via a spatial contrastive loss .Concurrently, the Episodic Memory creates unique memory traces for specific experiences by using the agent's trajectory and global geometry to attend to the sequence of visual observations, trained with an episodic contrastive loss.</p>
<p>Imagine you are a navigation robot.You have been given a sequence of captured images <image>.Based on these images, please describe the robot's navigation trajectory."Output: "Proceed past the refrigerator and turn right.Navigate around the corner, then turn right to face the bathroom.Walk towards the bathroom entrance and come to a stop."Human: "Assume you are a robot designed for navigation.You are provided with captured video<video>.Based on this video sequence, can you tell me whether the bathroom is positioned to the left or right of the entrance?"Output: "The bathroom is on your right as you enter.You will need to walk through the dining area and continue down the hallway until you reach the end."Navigation Action Planning Spatial-temporal Reasoning ...</p>
<p>Figure 3 :
3
Figure 3: Visualizations of Qualitative Study.Our data formulation includes navigational action planning (i.e., VLN-CE) and Spatio-temporal Reasoning (i.e., VSI-Bench).</p>
<p>Walk across the balcony past the chairs and into the bedroom.Walk to the left of the bed.Walk into the closet and stop.Exit the bathroom and head outside through the glass door in the bedroom.Wait on balcony.</p>
<p>Figure 4 :
4
Figure 4: Qualitative results from VLN-CE.We deploy VEME in the simulation environment (i.e., Habitat) for long-horizon navigation task.Given an instruction, the agent moves through different areas of the house and stops at the specified goal.As shown in the figure, its navigation path aligns well with the commands.</p>
<p>strates significant performance advantages over proprietary and open-source models in multidimensional task evaluations.In the Numerical Question category, the model outperformed Spatial-MLLM on core spatio-temporal understanding metrics such as target counting (Obj.Cnt) and target size (Obj.Size).In the Multiple-Choice Question task, although the overall performance was similar to that of Spatial-MLLM, the proposed model has shown a noticeable improvement.Through an analysis of overall average performance (Avg), the proposed model has surpassed Spatial-MLLM.In summary, this confirms the exceptional performance of our model in video multimodal spatio-temporal understanding tasks, establishing it as a leading model in the current field.</p>
<video placeholder> Question: [Question from VSI-Bench] Options: (A) [Option A] (B) [Option B] (C) [Option C] (D) [Option D] Answer (Provide the letter only): For Numerical Questions: The following is a video of an indoor scene.Based on the video, answer the following question.Provide only the numerical value in your answer.<video placeholder> Question: [Question from VSI-Bench] Answer: C.2 Computational Cost and Efficiency C.3 Evaluation Protocol Details • Metrics Calculation: All metrics were calculated using the official evaluation scripts provided by the respective benchmarks.For VLN-CE, we report Success Rate (SR) and Success rate weighted by Path Length (SPL).


Figure 5 :
5
Figure 5: t-SNE and distance distribution visualization of episodic features (F episodic ).Left: Each point corresponds to a full navigation episode.Points are colored by their environment ID.The clear clustering of points of the same color demonstrates that our model learns discriminative representations for different spatio-temporal experiences.Right: Blue distribution represents intra-scene episodic feature distances, while Red distribution represents inter-scene ones.The separation of episodic features demonstrates effective experience learning in our model.


Figure 6 :
6
Figure 6: Failure case illustration with the instruction: "Walk into the living room and keep walking straight past the living room.Then walk into the entrance under the balcony and wait at the entrance to the other room."


Table 1 :
1
Performance Comparison of Visual Navigation Methods
MethodsR2R Val-Unseen RxR Val-UnseenSRSPLSRSPLCMA (Hong et al. 2022)41.036.026.521.1ETPNav (An et al. 2024)57.049.054.748.8DreamWalker (Wang et al. 2023) 49.044.0--NaVid (Zhang et al. 2024a)37.035.0--NaVILA (Cheng et al. 2025b)54.049.049.344.0VEME (Ours)57.051.050.746.7rate (SPL) of 46.7, considerably higher than other compar-ison methods such as CMA (41.0 and 36.0) and ETPNav(42.0 and 36.5). Additionally, the proposed model outper-formed all comparison methods in the SPL metric, furthervalidating its effectiveness in navigating complex environ-ments. These results indicate that our model possesses a no-table advantage in visual navigation capabilities, enabling itto better understand and execute natural language instruc-tions. Computational Cost and EfficiencySpatial-temporal Understanding We compared the pro-posed model (Ours) with a range of video multimodallarge models (Video MLLM) and state-of-the-art modelson VSI-bench, including industrial-grade models like GPT-4o and Gemini-1.5Pro, as well as the LLaVA-Video se-ries (such as LLaVA-Video-72B and LLaVA-OneVision-72B), the Qwen2.5VL series (like Qwen2.5VL-7B andQwen2.5VL-72B), and the current SOTA model Spatial-MLLM on VSI-bench. The evaluation was conducted acrossdifferent granular tasks (e.g., Obj. Cnt and Abs. Dist in Nu-
merical Questions, Rel.Dist and Route Plan in Multiple-Choice Questions), with experimental results shown in Table 2.The results indicate that the proposed model demon-


Table 2 :
2
(Yang et al. 2024) on VSI-Bench(Yang et al. 2024).Arrows (↑/↓) denote improvement/decline relative to the best baseline (Spatial-MLLM-4B).Obj.Cnt Abs.Dist Obj.Size Room Size Rel.Dist Rel.Dir Route Plan Appr.Order
MethodsNumericalMultiple-ChoiceAvg. RankProprietary ModelsGemini-1.5 Pro (Team et al. 2024)56.230.964.143.651.346.336.034.6 45.42GPT-4o (Hurst et al. 2024)46.243.843.638.237.041.331.528.5 34.07Open-source ModelsInternVL2-40B (Chen et al. 2024)31.826.227.527.532.224.834.039.6 36.06LongVILA-8B (Xue et al. 2024)29.116.727.10.029.630.732.525.5 21.612VILA-1.5-40B (Lin et al. 2023)22.424.848.722.740.525.731.532.9 31.29LongVA-7B (Zhang et al. 2024b)38.016.638.922.233.143.325.415.7 29.211LLaVA-OneVision-72B (Li et al. 2024a)43.523.957.637.542.539.932.544.6 40.24LLaVA-Video-72B (Zhang et al. 2024c)48.922.857.435.342.436.735.048.6 40.93Qwen2.5VL-3B (Bai et al. 2025b)24.324.731.722.638.341.626.321.2 30.610Qwen2.5VL-7B (Bai et al. 2025b)40.914.843.410.738.638.533.029.8 33.08Qwen2.5VL-72B (Bai et al. 2025b)25.129.354.538.838.237.034.028.9 37.05Spatial-MLLM-4B (Wu et al. 2025a)65.334.863.145.141.346.233.546.3 48.42VEME (Ours)65.4↑34.6↓64.1↑45.3↑44.5↑42.3↓30.9↓45.8↓ 49.3↑1formance, validating their contributions to the overall modeleffectiveness.

Table 3 :
3
Ablation studies showing the impact of removing key components of our architecture.Performance drops across the board, confirming the contribution of each module.
Model ConfigurationVLN-CE (SPL %)Full Model65.1w/o. Spatial Memory (L spatial , E world )55.3w/o. Episodic Memory (L episodic , F episodic )49.8w/o. Geometric Grounding61.2w/o. Trajectory Input52.1

Table 4 :
4
Ablation studies showing the impact of removing key components of our architecture.Performance drops, confirming each module's contribution.
Method Configuration ACC MRA ALLFull Model44.155.749.3w/o. vggt36.119.228.3w/o. fine-tune34.919.426.9

Table 5 :
5
Key hyperparameters for training the VEME framework.
HyperparameterValueOptimizer & SchedulerOptimizerAdamWLearning Rate (Peak)1 × 10 −4Betas (β 1 , β 2 )(0.9, 0.999)Weight Decay0.01Learning Rate ScheduleCosine decayWarmup Steps500Training & BatchingTotal Training Epochs1Per-Device Batch Size8Gradient Accumulation Steps 2Effective Batch Size64 (8 × 8 GPUs)Mixed Precisionbfloat16Loss ConfigurationSpatial Loss Weight (λ s )0.1Episodic Loss Weight (λ e )0.1Contrastive Temperature (τ )0.07LoRA Fine-tuningLoRA Rank (r)16LoRA Alpha (α)32LoRA Dropout0.1LoRA Target Modules
igation and question-answering.Experiments on VLN-CE and VSI-Bench show improved success rates, exploration efficiency, and zero-shot generalization over VLM and SLAM methods.Our dual-memory architecture, with effective spatio-temporal alignment and robust empirical validation, significantly advances adaptive embodied intelligence.
Etpnav: Evolving topological planning for vision-language navigation in continuous environments. D An, H Wang, W Wang, Z Wang, Y Huang, K He, L Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024

Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, S Gould, A Van Den Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2018. 2018Spotlight Oral

. S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, H Zhong, Y Zhu, M Yang, Z Li, J Wan, P Wang, W Ding, Z Fu, Y Xu, J Ye, X Zhang, T Xie, Z Cheng, H Zhang, Z Yang, H Xu, J Lin, arXiv:2502.139232025aarXiv preprintQwen2.5-VL

. S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, H Zhong, Y Zhu, M Yang, Z Li, J Wan, P Wang, W Ding, Z Fu, Y Xu, J Ye, X Zhang, T Xie, Z Cheng, H Zhang, Z Yang, H Xu, J Lin, ArXiv, abs/2502.139232025bQwen2.5-VL Technical Report

The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence. E Bermudez-Contreras, B J Clark, A Wilber, Frontiers in Computational Neuroscience. 1420202020

Grounding Language in Robotic Affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, D Kalashnikov, S Levine, Y Lu, C Parada, K Rao, P Sermanet, A T Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, M Yan, N Brown, M Ahn, O Cortes, N Sievers, C Tan, S Xu, D Reyes, J Rettinghouse, J Quiambao, P Pastor, L Luu, K.-H Lee, Y Kuang, S Jesmonth, K Jeffrey, R J Ruano, J Hsu, K Gopalakrishnan, B David, A Zeng, C K Fu, 6th Annual Conference on Robot Learning. Not As I Say2022Do As I Can

A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, Mat-terport3D: Learning from RGB-D Data in Indoor Environments. International Conference on 3D Vision (3DV). 2017

Robot Navigation with Map-Based Deep Reinforcement Learning. G Chen, L Pan, Y Chen, P Xu, Z Wang, P Wu, J Ji, X Chen, Proceedings of the IEEE International Conference on Networking, Sensing and Control (ICNSC). the IEEE International Conference on Networking, Sensing and Control (ICNSC)2020

Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. K Chen, J Xiao, J Liu, Q Tong, H Zhang, R Liu, J Zhang, A Ajoudani, S Chen, Z Chen, J Wu, W Wang, W Su, G Chen, S Xing, M Zhong, Q Zhang, X Zhu, L Lu, CVPR. 2025. 2024Semantic visual simultaneous localization and mapping: A survey

A.-C Cheng, Y Ji, Z Yang, Z Gongye, X Zou, J Kautz, E Bıyık, H Yin, S Liu, X Wang, arXiv:2412.04453NaVILA: Legged Robot Vision-Language-Action Model for Navigation. 2025a

An explainable artificial intelligence approach to spatial navigation based on hippocampal circuitry. A.-C Cheng, RSS. CoppolinoY Ji, RSS. CoppolinoZ Yang, RSS. CoppolinoX Zou, RSS. CoppolinoJ Kautz, RSS. CoppolinoE Biyik, RSS. CoppolinoH Yin, RSS. CoppolinoS Liu, RSS. CoppolinoX Wang, RSS. CoppolinoNeural Networks. Migliore, M. 20231632025bNaVILA: Legged Robot Vision-Language-Action Model for Navigation

ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nießner, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)IEEE2017

The cognitive map in humans: spatial navigation and beyond. R A Epstein, E Z Patai, J B Julian, H J Spiers, Nature Neuroscience. 202017

Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. Y Hong, Z Wang, Q Wu, S Gould, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022

VLN-BERT: A Recurrent Vision-and-Language BERT for Navigation. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2021

An Embodied Generalist Agent in 3D World. J Huang, S Yong, X Ma, X Linghu, P Li, Y Wang, Q Li, S.-C Zhu, B Jia, S Huang, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2024

A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint

Z Jia, K Lin, Y Zhao, Q Gao, G Thattai, G S Sukhatme, Learning to Act with Affordance-Aware Multimodal Neural SLAM. 2022

B Li, Y Zhang, D Guo, R Zhang, F Li, H Zhang, K Zhang, Y Li, Z Liu, C Li, arXiv:2408.03326LLaVA-OneVision: Easy Visual Task Transfer. 2024aarXiv preprint

RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models. J Li, N Zhang, X Qu, K Lu, G Li, J Wan, J Wang, ACL (Findings). Association for Computational Linguistics2025

MuEP: A Multimodal Benchmark for Embodied Planning with Foundation Models. K Li, B Yu, Q Zheng, Y Zhan, Y Zhang, T Zhang, Y Yang, Y Chen, L Sun, Q Cao, L Shen, L Li, D Tao, X He, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI 2024). the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI 2024)Main Track2024bInternational Joint Conferences on Artificial Intelligence Organization

Context-Aware Deep Reinforcement Learning for Autonomous Robotic Navigation in Unknown Area. J Liang, Z Wang, Y Cao, J Chiun, M Zhang, G A Sartoretti, Proceedings of The 7th Conference on Robot Learning. J Tan, M Toussaint, K Darvish, The 7th Conference on Robot LearningPMLR2023229

VILA: On Pre-training for Visual Language Models. J Lin, H Yin, W Ping, Y Lu, P Molchanov, A Tao, H Mao, J Kautz, M Shoeybi, S Han, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023. 2024

. H Liu, C Li, Q Wu, Y J Lee, 2023Visual Instruction Tuning

Exploring multi-view perspectives on deep reinforcement learning agents for embodied object navigation in virtual home environments. X Liu, V Armstrong, S Nabil, C Muise, Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering, CASCON '21. the 31st Annual International Conference on Computer Science and Software Engineering, CASCON '21USAIBM Corp2021

Aligning cyber space with physical world: A comprehensive survey on embodied ai. Y Liu, W Chen, Y Bai, X Liang, G Li, W Gao, L Lin, IEEE/ASME Transactions on Mechatronics. 2025

Functional neuroanatomy of remote episodic, semantic and spatial memory: a unified account based on multiple trace theory. X Ma, S Yong, Z Zheng, Q Li, Y Liang, S.-C Zhu, S Huang, M Moscovitch, R Rosenbaum, A Gilboa, D Addis, R Westmacott, C Grady, M Mcandrews, B Levine, S Black, G Winocur, The Eleventh International Conference on Learning Representations. 2023. 2005207SQA3D: Situated Question Answering in 3D Scenes

Update on memory systems and processes. L Nadel, O Hardt, Neuropsychopharmacology. 3612011

. M Oquab, T Darcet, T Moutakanni, H V Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, R Howes, P.-Y Huang, H Xu, V Sharma, S.-W Li, W Galuba, M Rabbat, M Assran, N Ballas, G Synnaeve, I Misra, H Jegou, J Mairal, P Labatut, A Joulin, P Bojanowski, 2023DINOv2: Learning Robust Visual Features without Supervision

Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023

Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models. A Z Ren, J Clark, A Dixit, M Itkina, A Majumdar, D Sadigh, G Sarch, Y Wu, M Tarr, K Fragkiadaki, arXivpreprintarXiv:2403.15941Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational Linguistics2024. 2023Explore until Confident: Efficient Exploration for Embodied Question Answering

A comprehensive review of vision-based robotic applications: Current state, components, approaches, barriers, and potential solutions. M T Shahria, M S H Sunny, M I I Zarif, J Ghommam, S I Ahamed, M H Rahman, Robotics. 1161392022

J Z Sheng, arXiv:2505.23399GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning. 2025

G Team, P Georgiev, V I Lei, R Burnell, L Bai, A Gulati, G Tanzer, D Vincent, Z Pan, S Wang, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024arXiv preprint

. H Wang, W Liang, L Van Gool, W Wang, 

Mental planning for continuous visionlanguage navigation. Dreamwalker, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision

VGGT: Visual Geometry Grounded Transformer. J Wang, M Chen, N Karaev, A Vedaldi, C Rupprecht, D Novotny, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2025

D Wu, F Liu, Y.-H Hung, Y Duan, arXiv:2505.23747Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence. 2025aarXiv preprint

Sonata: Self-Supervised Learning of Reliable Point Representations. X Wu, D Detone, D Frost, T Shen, C Xie, N Yang, J Engel, R Newcombe, H Zhao, J Straub, CVPR. 2025b

LongVILA: Scaling Long-Context Visual Language Models for Long Videos. F Xue, Y Chen, D Li, Q Hu, L Zhu, X Li, Y Fang, H Tang, S Yang, Z Liu, E He, H Yin, P Molchanov, J Kautz, L Fan, Y Zhu, Y Lu, S Han, ArXiv, abs/2408.101882024

J Yang, S Yang, A Gupta, R Han, L Fei-Fei, S Xie, arXiv:2412.14171Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. 2024arXiv preprint

Thinking in space: How multimodal large language models see, remember, and recall spaces. J Yang, S Yang, A W Gupta, R Han, L Fei-Fei, S Xie, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025

B Yin, Q Wang, P Zhang, J Zhang, K Wang, Z Wang, J Zhang, K Chandrasegaran, H Liu, R Krishna, arXiv:2506.21458Spatial Mental Modeling from Limited Views. 2025arXiv preprint

L3MVN: Leveraging Large Language Models for Visual Target Navigation. B Yu, H Kasaei, M Cao, X Mustafa, B Kolesnikov, A Beyer, L , 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2023. 2023Proceedings of the IEEE/CVF international conference on computer vision

KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems. J Zhang, Z Huang, Y Fan, N Liu, M Li, Z Yang, J Yao, J Wang, K Wang, Forty-second International Conference on Machine Learning. 2025

Navid: Videobased vlm plans the next step for vision-and-language navigation. J Zhang, K Wang, R Xu, G Zhou, Y Hong, X Fang, Q Wu, Z Zhang, H Wang, arXiv:2402.158522024aarXiv preprint

Long Context Transfer from Language to Vision. P Zhang, K Zhang, B Li, G Zeng, J Yang, Y Zhang, Z Wang, H Tan, C Li, Z Liu, Y Zhang, J Wu, W Li, B Li, Z Ma, Z Liu, C Li, ArXiv, abs/2410.027132024b. 2024cVideo Instruction Tuning With Synthetic Data

Video-3d llm: Learning position-aware video representation for 3d scene understanding. D Zheng, S Huang, L Wang, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025

Vision-and-Language Navigation with Episodic Scene Memory. Q Zheng, D Liu, C Wang, J Zhang, D Wang, D Tao, International Journal of Computer Vision. 13312025

3D-SPATIAL MULTIMODAL MEM-ORY. X Zou, Y Song, R.-Z Qiu, X Peng, J Ye, S Liu, X Wang, The Thirteenth International Conference on Learning Representations. 2025            </div>
        </div>

    </div>
</body>
</html>
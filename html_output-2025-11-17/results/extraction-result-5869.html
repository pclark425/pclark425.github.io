<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5869 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5869</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5869</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-c76dd4a70361c3afd2e19d046343e2dedd16ecc3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c76dd4a70361c3afd2e19d046343e2dedd16ecc3" target="_blank">Automatic Prompt Optimization with "Gradient Descent" and Beam Search</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Preliminary results suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language"gradients"that criticize the current prompt. The gradients are then"propagated"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5869.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5869.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProTeGi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Optimization with Textual Gradients (ProTeGi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic, nonparametric prompt-optimization framework that uses LLM-generated natural-language "gradients" to identify flaws in a prompt and LLM edits to rewrite prompts in the opposite semantic direction; optimization is performed with a beam search over prompt candidates and bandit-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (primary experiment), also evaluated with davinci, text-davinci-003, gpt-3.5-turbo, gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Jailbreak detection; Ethos (hate speech); Liar (fake news); Sarcasm (Arabic sarcasm)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification tasks where prompts instruct the LLM to label input text (examples drawn from task-specific datasets). Jailbreak is a novel dataset labeling user inputs as jailbreak attacks or not; Ethos, Liar, Sarcasm are standard detection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt-language optimization pipeline: start from a human-written prompt (structured as Task / Output format / Examples / Prediction), include a small fixed set of few-shot examples (randomly selected pair) inside the prompt, then repeatedly (a) evaluate the prompt on minibatches, (b) use a fixed 'gradient-generation' LLM prompt (∇) that takes the prompt and error examples and outputs natural-language reasons (gradients) why it failed, and (c) use a fixed 'editing' LLM prompt (δ) that takes the gradient and original prompt and writes improved prompts; generate paraphrases via an LLM (LLM_mc) and conduct beam search with bandit selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against: (a) Monte-Carlo LLM-guided local paraphrase sampling (Zhou et al. 2022 style), (b) RL/phrase-edit baselines (e.g., GrIPS/TEMPERA style phrase-level edits), (c) AutoGPT agent-controlled feedback, and (d) variants of the ProTeGi pipeline ablating beam search (flat enumeration, greedy DFS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average improvements reported across tasks; ProTeGi final prompts achieved up to +0.31 relative relative improvement in some settings (abstract). More concrete: ProTeGi improved over Monte-Carlo (MC) baseline by +3.9 percentage points (avg), over RL baseline by +8.2 points (avg), and over the initial prompt p0 by +15.3 points (avg). Per-dataset accuracy/F1 in Appendix Table 5: Ethos ProTeGi F1=0.95 (SE=0.003) vs MC 0.94, Sarcasm 0.87 vs 0.86, Jailbreak 0.81 vs 0.76, Liar 0.64 vs 0.62.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See above; compared baselines include MC, RL, AutoGPT, and uniform evolutionary selection. Example: average improvement vs MC +3.9% F1, vs RL +8.2% F1; ProTeGi also outperformed AutoGPT by ~15.2% (avg).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+3.9% F1 vs MC (avg); +8.2% vs RL (avg); +15.3% vs initial prompt p0 (avg). Task-specific increases include Jailbreak: ProTeGi F1 ≈0.81 vs MC 0.76 (≈+0.05 F1).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper attributes gains to rewriting vague/high-level task descriptions into precise annotation instructions via data-driven gradient feedback; the directed textual gradients provide semantic directionality for edits (as opposed to directionless local search), enabling more relevant prompt rewrites and better sample efficiency when combined with beam search and bandit selection.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Optimization can overfit or get stuck in local minima: learning curves show datasets often peak around ~3 optimization steps and further steps can degrade performance; some gradient edits produced prompts that changed the task (e.g., Jailbreak candidate asking to classify child grooming), which can hurt performance in some runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5869.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5869.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Initial Prompt Template (Task/Output/Examples/Prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-written initial prompt template containing Task, Output format, Examples, and Prediction fields</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study used professionally written initial prompts structured into sections (# Task, # Output format, # Examples, # Prediction) and then optimized from that starting point; prompts were realistic quick-drafts by ML engineers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (primary experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same four classification tasks (Jailbreak, Ethos, Liar, Sarcasm)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification with prompt instructing model to answer Yes/No, examples dynamically filled with a fixed pair of few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Structured natural language instruction including explicit Output format and a small fixed set of few-shot examples (usually a randomly selected pair) embedded into the prompt; used as p0 starting prompt for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly to optimized prompts produced by ProTeGi and to other baselines which start from the same initial prompt; ablation 'No iteration' uses flat enumeration keeping same prompt (serves as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline 'No iteration' (i.e., starting prompt) F1 reported in Table 1: Jailbreak 0.80, Liar 0.63, Sarcasm 0.87. These are the p0 reference values used to compute improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>After ProTeGi beam optimization, corresponding F1s increased (e.g., Jailbreak 0.85 with beam vs 0.80 no iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Beam optimization increased Jailbreak F1 by +0.05 (0.80→0.85), Liar +0.04 (0.63→0.67), Sarcasm +0.01 (0.87→0.88) in the ablation table.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (when optimized by ProTeGi); baseline structured prompt is reasonable but improvable</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>A clear, structured human-written prompt with explicit output format and few-shot examples provides a good starting point, but vagueness in wording (e.g., ambiguous definitions of hate speech or jailbreak) can cause systematic errors that data-driven prompt rewriting can correct.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5869.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5869.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam search vs Greedy/Flat (Search format)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam search over prompt candidates guided by textual gradients and bandit selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An outer-loop search format where textual-gradient-driven edits generate multiple candidate prompts per beam entry, with selection of top candidates per iteration; compared to flat enumerate-then-select and greedy DFS ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Jailbreak, Liar, Sarcasm (ablation table)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same classification tasks; the ablation isolates the effect of search-format used to propose and retain prompt edits.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Beam search: iterative depth-limited expansion where each beam prompt is expanded using textual gradients and paraphrases; selection uses bandit/approximate best-arm algorithms to choose b survivors per iteration. Alternatives: No iteration (flat enumeration of candidates then select), Greedy DFS (depth-first applying edits greedily).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>No iteration (flat enumeration) and Greedy (depth-first) were directly compared in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1 (F1): No iteration — Jailbreak 0.80, Liar 0.63, Sarcasm 0.87; Greedy — Jailbreak 0.82, Liar 0.63, Sarcasm 0.85; Beam (ProTeGi) — Jailbreak 0.85, Liar 0.67, Sarcasm 0.88.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Beam outperformed No iteration by +0.05 (Jailbreak), +0.04 (Liar), +0.01 (Sarcasm) and outperformed Greedy by +0.03 (Jailbreak), +0.04 (Liar), +0.03 (Sarcasm vs 0.85→0.88).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Beam vs No iteration: Jailbreak +0.05 F1, Liar +0.04, Sarcasm +0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Beam search better balances exploration and exploitation across multiple promising prompt edits compared to greedy local search or one-shot enumeration, preventing the optimization from getting stuck in poor local edits and enabling combination of diverse beneficial edits.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>No single search method dominates in all settings; high variance in stochastic search meant flat and greedy baselines sometimes tied on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5869.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5869.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bandit selection formats (UCB / UCB-E / SR / SH / Unif)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Approximate best-arm identification algorithms for selecting prompt candidates (UCB, UCB-E, Successive Rejects, Successive Halving, Uniform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different selection formats for allocating evaluation budget to candidate prompts were compared; these govern which prompt candidates are promoted on the beam based on partial evaluations on sampled data points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Best arm identification in multi-armed bandits</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Jailbreak and Liar (bandit comparison table)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation of prompt selection algorithms' impact on final prompt performance with a fixed per-prompt query budget.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Selection format: (a) Uniform (spread budget evenly), (b) UCB (upper-confidence bound sampling with exploration parameter c), (c) UCB-E (UCB variant favoring exploration), (d) Successive Rejects (SR) (phase-wise elimination), (e) Successive Halving (SH) (aggressive halving). Each algorithm defines which prompts are evaluated more and which are eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>All five selection strategies compared at budgets of 25 and 50 evaluations per prompt (per Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 2 (F1): At 25 per prompt — Unif: Jailbreak 0.77, Liar 0.59; UCB: Jailbreak 0.83, Liar 0.66; UCB-E: Jailbreak 0.83, Liar 0.65; SR: Jailbreak 0.81, Liar 0.62; SH: Jailbreak 0.82, Liar 0.64. At 50 per prompt — Unif: Jailbreak 0.77, Liar 0.61; UCB: Jailbreak 0.85, Liar 0.66; UCB-E: Jailbreak 0.83, Liar 0.67; SR: Jailbreak 0.82, Liar 0.66; SH: Jailbreak 0.80, Liar 0.62.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>UCB-style algorithms consistently outperformed Uniform and successive-rejects-style algorithms in these experiments (e.g., UCB 0.83 vs Unif 0.77 Jailbreak at 25 budget).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>UCB vs Uniform at 25-per-prompt: Jailbreak +0.06 F1, Liar +0.07 F1; similar magnitudes at 50 budget.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (UCB-style) / reduced (Uniform relative to UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that UCB-style algorithms better balance exploration and exploitation in practice (they set exploration parameter c=2.0), leading to more efficient identification of high-performing prompts despite theoretical appeals of SR/SH for pure best-arm identification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Contrary to initial expectation, successive-rejects (SR), which is theoretically optimal for best-arm identification, did not outperform UCB-style methods in practice on these prompt-selection experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5869.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5869.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Monte-Carlo paraphrase baseline (MC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-generated Monte-Carlo local paraphrase sampling (Zhou et al., 2022 style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A directionless Monte-Carlo prompt-engineering baseline where LLMs generate many paraphrases/variants of the prompt and selection is applied; used as a main baseline for comparison with ProTeGi.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are human-level prompt engineers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (as used for baseline sampling in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Jailbreak, Ethos, Liar, Sarcasm</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same classification tasks where MC generates paraphrased prompt candidates (semantic-preserving variations) and selects among them using same evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt format variation via unconstrained paraphrases of the starting prompt (no directed textual-gradient feedback); local monte-carlo exploration of wording/phrasing around initial prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Directly compared to ProTeGi-generated directed edits + paraphrases; MC matched the number of Monte-Carlo samples per candidate to ProTeGi for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MC baseline performance (Appendix Table 5 / main text): Ethos MC acc 0.94 vs ProTeGi 0.95, Sarcasm MC 0.86 vs ProTeGi 0.87, Jailbreak MC 0.76 vs ProTeGi 0.81, Liar MC 0.62 vs ProTeGi 0.64. Overall ProTeGi improved over MC by ~+3.9 percentage points (avg).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ProTeGi > MC in all reported tasks, sometimes by small margins (e.g., Ethos +0.01) and larger in Jailbreak (+0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Average +3.9% F1 improvement (ProTeGi vs MC); Jailbreak +0.05 F1 is the largest observed improvement in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>MC (directionless paraphrasing) underperforms compared to directed gradient-driven edits (ProTeGi)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Directionless paraphrasing often rephrases initial prompt without addressing semantic causes of errors; ProTeGi's gradient feedback identifies specific conceptual problems and produces targeted rewrites, hence better gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>On some tasks (e.g., Ethos, Sarcasm) MC produced reasonable paraphrases that were close to ProTeGi performance (small gains only).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5869.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5869.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Base model effect (GPT-3 vs InstructGPT vs ChatGPT vs GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of underlying LLM API (davinci, text-davinci-003, gpt-3.5-turbo, gpt-4) on prompt optimization outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors evaluated ProTeGi and prompt performance across several base LLM APIs and observed substantial differences: RLHF-tuned models (InstructGPT, ChatGPT) and GPT-4 outperform older GPT-3 davinci on these prompt-driven classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>davinci (GPT-3), text-davinci-003 (InstructGPT), gpt-3.5-turbo (ChatGPT), gpt-4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sarcasm and Jailbreak (reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification tasks; the same prompt formats were used across different LLM APIs to isolate model-capability effects.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same structured prompts and optimization procedure; only the underlying LLM API used to generate predictions/gradients was changed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct per-model comparison with identical prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3 F1s: Sarcasm — GPT-3 (davinci) 0.73, InstructGPT (text-davinci-003) 0.83, ChatGPT (gpt-3.5-turbo) 0.86, GPT-4 0.86. Jailbreak — GPT-3 0.55, InstructGPT 0.75, ChatGPT 0.85, GPT-4 0.88.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Strong improvement moving from GPT-3 to RLHF-tuned models and GPT-4; e.g., Jailbreak F1 improved from 0.55 (GPT-3) to 0.88 (GPT-4) with same prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Jailbreak: GPT-4 0.88 vs GPT-3 0.55 = +0.33 F1 (33 percentage points) with same prompt format; Sarcasm: +0.13 F1 from GPT-3 to ChatGPT/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>model choice strongly modulates effectiveness of a given prompt format; better base models yield higher performance</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest RLHF-tuned models and GPT-4 have enhanced reasoning and instruction-following abilities, making them more responsive to optimized prompts and better able to benefit from the targeted, data-driven prompt rewrites.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are human-level prompt engineers <em>(Rating: 2)</em></li>
                <li>Grips: Gradient-free, edit-based instruction search for prompting large language models <em>(Rating: 2)</em></li>
                <li>Tempera: Test-time prompt editing via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Best arm identification in multi-armed bandits <em>(Rating: 2)</em></li>
                <li>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity <em>(Rating: 1)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5869",
    "paper_id": "paper-c76dd4a70361c3afd2e19d046343e2dedd16ecc3",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "ProTeGi",
            "name_full": "Prompt Optimization with Textual Gradients (ProTeGi)",
            "brief_description": "An automatic, nonparametric prompt-optimization framework that uses LLM-generated natural-language \"gradients\" to identify flaws in a prompt and LLM edits to rewrite prompts in the opposite semantic direction; optimization is performed with a beam search over prompt candidates and bandit-based selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (primary experiment), also evaluated with davinci, text-davinci-003, gpt-3.5-turbo, gpt-4",
            "model_size": null,
            "task_name": "Jailbreak detection; Ethos (hate speech); Liar (fake news); Sarcasm (Arabic sarcasm)",
            "task_description": "Binary classification tasks where prompts instruct the LLM to label input text (examples drawn from task-specific datasets). Jailbreak is a novel dataset labeling user inputs as jailbreak attacks or not; Ethos, Liar, Sarcasm are standard detection tasks.",
            "problem_format": "Prompt-language optimization pipeline: start from a human-written prompt (structured as Task / Output format / Examples / Prediction), include a small fixed set of few-shot examples (randomly selected pair) inside the prompt, then repeatedly (a) evaluate the prompt on minibatches, (b) use a fixed 'gradient-generation' LLM prompt (∇) that takes the prompt and error examples and outputs natural-language reasons (gradients) why it failed, and (c) use a fixed 'editing' LLM prompt (δ) that takes the gradient and original prompt and writes improved prompts; generate paraphrases via an LLM (LLM_mc) and conduct beam search with bandit selection.",
            "comparison_format": "Compared against: (a) Monte-Carlo LLM-guided local paraphrase sampling (Zhou et al. 2022 style), (b) RL/phrase-edit baselines (e.g., GrIPS/TEMPERA style phrase-level edits), (c) AutoGPT agent-controlled feedback, and (d) variants of the ProTeGi pipeline ablating beam search (flat enumeration, greedy DFS).",
            "performance": "Average improvements reported across tasks; ProTeGi final prompts achieved up to +0.31 relative relative improvement in some settings (abstract). More concrete: ProTeGi improved over Monte-Carlo (MC) baseline by +3.9 percentage points (avg), over RL baseline by +8.2 points (avg), and over the initial prompt p0 by +15.3 points (avg). Per-dataset accuracy/F1 in Appendix Table 5: Ethos ProTeGi F1=0.95 (SE=0.003) vs MC 0.94, Sarcasm 0.87 vs 0.86, Jailbreak 0.81 vs 0.76, Liar 0.64 vs 0.62.",
            "performance_comparison": "See above; compared baselines include MC, RL, AutoGPT, and uniform evolutionary selection. Example: average improvement vs MC +3.9% F1, vs RL +8.2% F1; ProTeGi also outperformed AutoGPT by ~15.2% (avg).",
            "format_effect_size": "+3.9% F1 vs MC (avg); +8.2% vs RL (avg); +15.3% vs initial prompt p0 (avg). Task-specific increases include Jailbreak: ProTeGi F1 ≈0.81 vs MC 0.76 (≈+0.05 F1).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "The paper attributes gains to rewriting vague/high-level task descriptions into precise annotation instructions via data-driven gradient feedback; the directed textual gradients provide semantic directionality for edits (as opposed to directionless local search), enabling more relevant prompt rewrites and better sample efficiency when combined with beam search and bandit selection.",
            "counterexample_or_null_result": "Optimization can overfit or get stuck in local minima: learning curves show datasets often peak around ~3 optimization steps and further steps can degrade performance; some gradient edits produced prompts that changed the task (e.g., Jailbreak candidate asking to classify child grooming), which can hurt performance in some runs.",
            "uuid": "e5869.0",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Initial Prompt Template (Task/Output/Examples/Prediction)",
            "name_full": "Human-written initial prompt template containing Task, Output format, Examples, and Prediction fields",
            "brief_description": "The study used professionally written initial prompts structured into sections (# Task, # Output format, # Examples, # Prediction) and then optimized from that starting point; prompts were realistic quick-drafts by ML engineers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (primary experiments)",
            "model_size": null,
            "task_name": "Same four classification tasks (Jailbreak, Ethos, Liar, Sarcasm)",
            "task_description": "Binary classification with prompt instructing model to answer Yes/No, examples dynamically filled with a fixed pair of few-shot examples.",
            "problem_format": "Structured natural language instruction including explicit Output format and a small fixed set of few-shot examples (usually a randomly selected pair) embedded into the prompt; used as p0 starting prompt for optimization.",
            "comparison_format": "Compared implicitly to optimized prompts produced by ProTeGi and to other baselines which start from the same initial prompt; ablation 'No iteration' uses flat enumeration keeping same prompt (serves as baseline).",
            "performance": "Baseline 'No iteration' (i.e., starting prompt) F1 reported in Table 1: Jailbreak 0.80, Liar 0.63, Sarcasm 0.87. These are the p0 reference values used to compute improvements.",
            "performance_comparison": "After ProTeGi beam optimization, corresponding F1s increased (e.g., Jailbreak 0.85 with beam vs 0.80 no iteration).",
            "format_effect_size": "Beam optimization increased Jailbreak F1 by +0.05 (0.80→0.85), Liar +0.04 (0.63→0.67), Sarcasm +0.01 (0.87→0.88) in the ablation table.",
            "format_effect_direction": "improved (when optimized by ProTeGi); baseline structured prompt is reasonable but improvable",
            "explanation_or_hypothesis": "A clear, structured human-written prompt with explicit output format and few-shot examples provides a good starting point, but vagueness in wording (e.g., ambiguous definitions of hate speech or jailbreak) can cause systematic errors that data-driven prompt rewriting can correct.",
            "counterexample_or_null_result": null,
            "uuid": "e5869.1",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Beam search vs Greedy/Flat (Search format)",
            "name_full": "Beam search over prompt candidates guided by textual gradients and bandit selection",
            "brief_description": "An outer-loop search format where textual-gradient-driven edits generate multiple candidate prompts per beam entry, with selection of top candidates per iteration; compared to flat enumerate-then-select and greedy DFS ablations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Jailbreak, Liar, Sarcasm (ablation table)",
            "task_description": "Same classification tasks; the ablation isolates the effect of search-format used to propose and retain prompt edits.",
            "problem_format": "Beam search: iterative depth-limited expansion where each beam prompt is expanded using textual gradients and paraphrases; selection uses bandit/approximate best-arm algorithms to choose b survivors per iteration. Alternatives: No iteration (flat enumeration of candidates then select), Greedy DFS (depth-first applying edits greedily).",
            "comparison_format": "No iteration (flat enumeration) and Greedy (depth-first) were directly compared in Table 1.",
            "performance": "Table 1 (F1): No iteration — Jailbreak 0.80, Liar 0.63, Sarcasm 0.87; Greedy — Jailbreak 0.82, Liar 0.63, Sarcasm 0.85; Beam (ProTeGi) — Jailbreak 0.85, Liar 0.67, Sarcasm 0.88.",
            "performance_comparison": "Beam outperformed No iteration by +0.05 (Jailbreak), +0.04 (Liar), +0.01 (Sarcasm) and outperformed Greedy by +0.03 (Jailbreak), +0.04 (Liar), +0.03 (Sarcasm vs 0.85→0.88).",
            "format_effect_size": "Beam vs No iteration: Jailbreak +0.05 F1, Liar +0.04, Sarcasm +0.01.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Beam search better balances exploration and exploitation across multiple promising prompt edits compared to greedy local search or one-shot enumeration, preventing the optimization from getting stuck in poor local edits and enabling combination of diverse beneficial edits.",
            "counterexample_or_null_result": "No single search method dominates in all settings; high variance in stochastic search meant flat and greedy baselines sometimes tied on some datasets.",
            "uuid": "e5869.2",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Bandit selection formats (UCB / UCB-E / SR / SH / Unif)",
            "name_full": "Approximate best-arm identification algorithms for selecting prompt candidates (UCB, UCB-E, Successive Rejects, Successive Halving, Uniform)",
            "brief_description": "Different selection formats for allocating evaluation budget to candidate prompts were compared; these govern which prompt candidates are promoted on the beam based on partial evaluations on sampled data points.",
            "citation_title": "Best arm identification in multi-armed bandits",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Jailbreak and Liar (bandit comparison table)",
            "task_description": "Evaluation of prompt selection algorithms' impact on final prompt performance with a fixed per-prompt query budget.",
            "problem_format": "Selection format: (a) Uniform (spread budget evenly), (b) UCB (upper-confidence bound sampling with exploration parameter c), (c) UCB-E (UCB variant favoring exploration), (d) Successive Rejects (SR) (phase-wise elimination), (e) Successive Halving (SH) (aggressive halving). Each algorithm defines which prompts are evaluated more and which are eliminated.",
            "comparison_format": "All five selection strategies compared at budgets of 25 and 50 evaluations per prompt (per Table 2).",
            "performance": "Table 2 (F1): At 25 per prompt — Unif: Jailbreak 0.77, Liar 0.59; UCB: Jailbreak 0.83, Liar 0.66; UCB-E: Jailbreak 0.83, Liar 0.65; SR: Jailbreak 0.81, Liar 0.62; SH: Jailbreak 0.82, Liar 0.64. At 50 per prompt — Unif: Jailbreak 0.77, Liar 0.61; UCB: Jailbreak 0.85, Liar 0.66; UCB-E: Jailbreak 0.83, Liar 0.67; SR: Jailbreak 0.82, Liar 0.66; SH: Jailbreak 0.80, Liar 0.62.",
            "performance_comparison": "UCB-style algorithms consistently outperformed Uniform and successive-rejects-style algorithms in these experiments (e.g., UCB 0.83 vs Unif 0.77 Jailbreak at 25 budget).",
            "format_effect_size": "UCB vs Uniform at 25-per-prompt: Jailbreak +0.06 F1, Liar +0.07 F1; similar magnitudes at 50 budget.",
            "format_effect_direction": "improved (UCB-style) / reduced (Uniform relative to UCB)",
            "explanation_or_hypothesis": "Authors hypothesize that UCB-style algorithms better balance exploration and exploitation in practice (they set exploration parameter c=2.0), leading to more efficient identification of high-performing prompts despite theoretical appeals of SR/SH for pure best-arm identification.",
            "counterexample_or_null_result": "Contrary to initial expectation, successive-rejects (SR), which is theoretically optimal for best-arm identification, did not outperform UCB-style methods in practice on these prompt-selection experiments.",
            "uuid": "e5869.3",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Monte-Carlo paraphrase baseline (MC)",
            "name_full": "LLM-generated Monte-Carlo local paraphrase sampling (Zhou et al., 2022 style)",
            "brief_description": "A directionless Monte-Carlo prompt-engineering baseline where LLMs generate many paraphrases/variants of the prompt and selection is applied; used as a main baseline for comparison with ProTeGi.",
            "citation_title": "Large language models are human-level prompt engineers",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (as used for baseline sampling in experiments)",
            "model_size": null,
            "task_name": "Jailbreak, Ethos, Liar, Sarcasm",
            "task_description": "Same classification tasks where MC generates paraphrased prompt candidates (semantic-preserving variations) and selects among them using same evaluation budget.",
            "problem_format": "Prompt format variation via unconstrained paraphrases of the starting prompt (no directed textual-gradient feedback); local monte-carlo exploration of wording/phrasing around initial prompt.",
            "comparison_format": "Directly compared to ProTeGi-generated directed edits + paraphrases; MC matched the number of Monte-Carlo samples per candidate to ProTeGi for fairness.",
            "performance": "MC baseline performance (Appendix Table 5 / main text): Ethos MC acc 0.94 vs ProTeGi 0.95, Sarcasm MC 0.86 vs ProTeGi 0.87, Jailbreak MC 0.76 vs ProTeGi 0.81, Liar MC 0.62 vs ProTeGi 0.64. Overall ProTeGi improved over MC by ~+3.9 percentage points (avg).",
            "performance_comparison": "ProTeGi &gt; MC in all reported tasks, sometimes by small margins (e.g., Ethos +0.01) and larger in Jailbreak (+0.05).",
            "format_effect_size": "Average +3.9% F1 improvement (ProTeGi vs MC); Jailbreak +0.05 F1 is the largest observed improvement in the reported comparisons.",
            "format_effect_direction": "MC (directionless paraphrasing) underperforms compared to directed gradient-driven edits (ProTeGi)",
            "explanation_or_hypothesis": "Directionless paraphrasing often rephrases initial prompt without addressing semantic causes of errors; ProTeGi's gradient feedback identifies specific conceptual problems and produces targeted rewrites, hence better gains.",
            "counterexample_or_null_result": "On some tasks (e.g., Ethos, Sarcasm) MC produced reasonable paraphrases that were close to ProTeGi performance (small gains only).",
            "uuid": "e5869.4",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Base model effect (GPT-3 vs InstructGPT vs ChatGPT vs GPT-4)",
            "name_full": "Effect of underlying LLM API (davinci, text-davinci-003, gpt-3.5-turbo, gpt-4) on prompt optimization outcomes",
            "brief_description": "The authors evaluated ProTeGi and prompt performance across several base LLM APIs and observed substantial differences: RLHF-tuned models (InstructGPT, ChatGPT) and GPT-4 outperform older GPT-3 davinci on these prompt-driven classification tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "davinci (GPT-3), text-davinci-003 (InstructGPT), gpt-3.5-turbo (ChatGPT), gpt-4 (GPT-4)",
            "model_size": null,
            "task_name": "Sarcasm and Jailbreak (reported in Table 3)",
            "task_description": "Binary classification tasks; the same prompt formats were used across different LLM APIs to isolate model-capability effects.",
            "problem_format": "Same structured prompts and optimization procedure; only the underlying LLM API used to generate predictions/gradients was changed.",
            "comparison_format": "Direct per-model comparison with identical prompt formats.",
            "performance": "Table 3 F1s: Sarcasm — GPT-3 (davinci) 0.73, InstructGPT (text-davinci-003) 0.83, ChatGPT (gpt-3.5-turbo) 0.86, GPT-4 0.86. Jailbreak — GPT-3 0.55, InstructGPT 0.75, ChatGPT 0.85, GPT-4 0.88.",
            "performance_comparison": "Strong improvement moving from GPT-3 to RLHF-tuned models and GPT-4; e.g., Jailbreak F1 improved from 0.55 (GPT-3) to 0.88 (GPT-4) with same prompt format.",
            "format_effect_size": "Jailbreak: GPT-4 0.88 vs GPT-3 0.55 = +0.33 F1 (33 percentage points) with same prompt format; Sarcasm: +0.13 F1 from GPT-3 to ChatGPT/GPT-4.",
            "format_effect_direction": "model choice strongly modulates effectiveness of a given prompt format; better base models yield higher performance",
            "explanation_or_hypothesis": "Authors suggest RLHF-tuned models and GPT-4 have enhanced reasoning and instruction-following abilities, making them more responsive to optimized prompts and better able to benefit from the targeted, data-driven prompt rewrites.",
            "counterexample_or_null_result": null,
            "uuid": "e5869.5",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 2
        },
        {
            "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models",
            "rating": 2
        },
        {
            "paper_title": "Tempera: Test-time prompt editing via reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Best arm identification in multi-armed bandits",
            "rating": 2
        },
        {
            "paper_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "rating": 1
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 1
        }
    ],
    "cost": 0.01762575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatic Prompt Optimization with "Gradient Descent" and Beam Search</h1>
<p>Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng<br>Microsoft Azure AI<br>{reidpryzant,iterdan,jerrl,yintatlee, chezhu,nzeng}@microsoft.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Prompt Optimization with Textual Gradients (ProTeGi), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language "gradients" that criticize the current prompt, much like how numerical gradients point in the direction of error ascent. The natural language gradients are then "propagated" into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to $31 \%$, by using data to rewrite vague task descriptions into more precise annotation instructions. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) trained on webscale text have recently demonstrated unprecedented abilities across a variety of NLP tasks (OpenAI, 2023; Bubeck et al., 2023). These LLMs use prompt inputs to follow human instructions. Writing prompts in natural language remains a manual trial-and-error process requiring significant human effort (Jiang et al., 2022) and expertise (Reynolds and McDonell, 2021; Zamfirescu-Pereira et al., 2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the proposed Prompt Optimization with Textual Gradients (ProTeGi).</p>
<p>Accordingly, there is need for automatic or semiautomatic procedures to help humans write the best prompts. This would help reduce manual effort, improve task performance, and produce interpretable descriptions of a cognitive decision process.</p>
<p>A recent body of work has investigated this problem by training auxiliary models or differentiable representations of the prompt (Qin and Eisner, 2021; Deng et al., 2022). However, such works assume access to internal state variables of the LLM (Shin et al., 2020; Lester et al., 2021) while practitioners often communicate with LLMs through an API. Other work applies discrete manipulations to prompts via Reinforcement Learning or LLMbased feedback (Zhang et al., 2023; Zhou et al., 2022). These algorithms may also require low-level access to the LLM, produce incomprehensible outputs, or rely on directionless monte-carlo search over the semantic space of prompts.</p>
<p>We propose Prompt Optimization with Textual Gradients (ProTeGi), a general purpose and nonparametric algorithm for automatic prompt optimization that connects these two bodies of research by applying discrete improvements to prompts in a directed way.</p>
<p>Unlike prior work, we overcome the discrete optimization barrier by mirroring the steps of gradient descent within a text-based Socratic dialogue [zeng2022socratic], substituting differentiation with LLM feedback and backpropagation with LLM editing. In detail, we use minibatches of training data to produce "gradients" in natural language, i.e., descriptions of the current prompts' flaws with respect to the minibatch, then edit the current prompt in the opposite semantic direction of the gradient. These steps become the expansion part of a wider beam search over the space of prompts, increasing algorithmic efficiency by treating the problem of beam candidate selection as an instance of the best arm identification problem [audibert2010multi].</p>
<p>We then offer a preliminary case study of ProTeGi. We evaluate the proposed framework in multiple configurations across 4 NLP tasks, including the novel problem of LLM jailbreak detection. The results suggest that the proposed algorithm can improve on the performance of the initial prompt input by up to 31%, exceeding state-of-the-art prompt learning baselines by an average of 4-8% while relying on fewer LLM API calls. We also demonstrate the interpretability of the optimization process and investigate the algorithms’ shortcomings.</p>
<h2>2 Discrete Prompt Optimization with Nonparametric “Gradient Descent”</h2>
<p>The proposed algorithm assumes access to an initial prompt $p_{0}$ and i.i.d. training data consisting of pairs of input and output text (numbers, categories, summaries, etc): $\mathcal{D}<em 1="1">{tr}={(x</em>},y_{1}),\ldots,(x_{n},y_{n})}$. Note that all prompts $p$ are drawn from the space of coherent natural language $\mathcal{L}$. We assume access to a black box LLM API $LLM_{p}(x) \approx$ $\operatorname{argmax<em L="L" M="M">{y \in \mathcal{L}} P</em>(y \mid p, x)$, which returns a likely text continuation $y$ of the prompt formed by concatenating $p$ and $x$ (for example, few-shot prompt and input example, or chatbot persona and conversational history).</p>
<p>Within this context, our algorithm iteratively refines the prompt $p_{0}$ to produce $\hat{p}$, an approximation of the optimal prompt $p^{*}=$ $\operatorname{argmax}<em e="e" t="t">{p \in \mathcal{L}}\left{m\left(p, \mathcal{D}</em>\right)\right}$ for some metric function
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The text dialogue tree we use to mimic gradient descent and overcome the discrete optimization barrier. First, from the top left a feedback prompt $\Delta$ generates the gradient $g$ from starting prompt $p_{0}$ and prediction $\hat{y}$. Second, from the top right an editing prompt $\delta$ applies the gradient $g$ to $p_{0}$ and produce improved prompts $p^{\prime}$, their paraphrases $p^{\prime \prime}$, and efficient best candidate selection before the next iteration (bottom left).
$m(\cdot)$ and in-domain test or development data $\mathcal{D}_{t e}$.
In the following sections, we first introduce how the algorithm performs textual "gradient descent" to improve the prompts in a directed way (Section 2.1). Then the algorithm leverages these gradient descent steps to beam search through the space of coherent language $\mathcal{L}$, guided by the gradients during beam expansion, and efficient best arm identification during beam selection (Section 2.2).</p>
<h3>2.1 Gradient descent with Prompts</h3>
<p>In our setting, gradient descent refers to the process of (1) evaluating a prompt with a batch of data, (2) creating a local loss signal which contains information on how to improve the current prompt, then (3) editing the prompt in the opposite semantic direction of the gradient before starting the next iteration.</p>
<p>We accomplish this process with a pair of static LLM prompts, as depicted in Figure 2. The first prompt is for creating the loss signals ("gradients") and is called $\nabla$. While the specific contents can vary and be task-specific or task-agnostic, ${ }^{2} \nabla$ must always consider the current prompt $p_{0}$, plus $p_{0}$ 's behavior on a minibatch of data (particularly the errors), and generate a natural language summary</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>of $p_{0}$ 's flaws. This summary becomes the gradient $g$. Similar to how traditional gradients represent a direction in parameter space that would make the model worse, the text "gradients" $g$ represent directions in a semantic space that are making the prompt worse.</p>
<p>The second prompt is called $\delta$ and while this prompt can also vary, it must always take the gradient $g$ and current prompt $p_{0}$, then perform an edit on $p_{0}$ in the opposite semantic direction of $g$, i.e. fix the problems with $p_{0}$ that are indicated by $g$.</p>
<p>Unlike the traditional machine learning setting, we do not generate a single gradient or edit, but rather a number of directions that may improve the current prompt. Section 2.2 describes in detail the process of generating and selecting candidate prompts.</p>
<h3>2.2 Beam Search over Prompts</h3>
<p>The gradient descent steps described in Section 2.1 are used to guide a beam search over the space of prompts. This beam search is the outer loop of our prompt training algorithm and it is described in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Prompt Optimization with Textual
Gradients (ProTeGi)
Require: \(p_{0}\) : initial prompt, \(z b\) : beam width, \(r\) :
    search depth, \(m\) : metric function
    \(B_{0} \leftarrow\left\{p_{0}\right\}\)
    for \(i \leftarrow 1\) to \(r-1\) do
        \(C \leftarrow \emptyset\)
        for all \(p \in B_{i}\) do
            \(C \leftarrow C \cup \operatorname{Expand}(p)\)
        end for
        \(B_{i+1} \leftarrow \operatorname{Select}_{b}(C, m)\)
    end for
    \(\hat{p} \leftarrow \operatorname{argmax}_{p \in B_{i}} m(s)\)
    return \(\hat{p}\)
</code></pre></div>

<p>The beam search is an iterative optimization process where for each iteration the current prompt is used to generate many new candidate prompts (expansion). Next, a selection process is used to decide which prompts are worth carrying forward to the next iteration. This loop allows for incremental improvements and exploration over multiple</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>prompt candidates.</p>
<h3>2.2.1 Expansion Step</h3>
<p>The expansion step is used to generate many new candidate prompts from a current prompt (Algorithm 2). It leverages the conceptual "gradient descent" framework of Section 2.1, and our specific prompts can be found in the Appendix.</p>
<p>First we sample a minibatch of data, run the initial prompt on these data with $L L M_{p_{0}}$, and collect errors. Second, we plug these errors into a prompt template $\Delta$, which instructs the LLM to describe the problems with $p_{0}$ which could have led to these mistakes. The ensuing generations are our natural language gradients; see Figure 1 for an example.</p>
<p>Second, the gradients are provided to another LLM prompt called $\delta$, which instructs the LLM to edit the current prompt $p_{0}$ in order to fix the problems described by the gradient. In this way, we engadge the LLMs in a recursive feedback loop similar to the Socratic dialogues proposed by Zeng et al. (2022).</p>
<p>Last, additional candidates are generated by running the existing candidates through a paraphrasing LLM called $L L M_{m c}$, to explore the local monte carlo search space around the new prompt candidates. This prompt simply asks the LLM to generate new candidates which are worded differently but semantically similar to their inputs.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 \(\operatorname{Expand}(\cdot)\) - line 5 of Algorithm 1
Require: \(p\) : prompt candidate, \(\mathcal{D}_{t r}\) : train data
    : Sample minibatch \(\mathcal{D}_{\text {mini }} \subset \mathcal{D}_{t r}\)
    : Evaluate prompt \(p\) on minibatch \(\mathcal{D}_{\text {mini }}\) and
    collect errors \(e=\left\{\left(x_{i}, y_{i}\right):\left(x_{i}, y_{i}\right) \in\right.\)
    \(\mathcal{D}_{\text {mini }} \wedge L L M_{p}\left(x_{i}\right) \neq y_{i}\}\)
    Get gradients: \(\left\{g_{1}, \ldots, g_{m}\right\}=L L M_{\nabla}(p, e)\)
    Use the gradients to edit the current prompt:
        \(\left\{p_{i 1}^{\prime}, \ldots, p_{i q}^{\prime}\right\}=L L M_{\delta}\left(p, g_{i}, e\right)\)
    Get more monte-carlo successors:
        \(\left\{p_{i j 1}^{\prime \prime}, \ldots, p_{i j m}^{\prime \prime}\right\}=L L M_{m c}\left(p_{i j}^{\prime}\right)\)
    return \(\left\{p_{11}^{\prime}, \ldots, p_{m q}^{\prime}\right\} \cup\left\{p_{111}^{\prime \prime}, \ldots, p_{m q p}^{\prime \prime}\right\}\)
</code></pre></div>

<h3>2.2.2 Selection Step</h3>
<p>Once the expansion process has stepped each candidate prompt into multiple possible successor candidates, the selection step chooses the $b$ most promising candidates to stay on the beam for the next iteration.</p>
<p>It is expensive to evaluate each candidate prompt on the entire training dataset (Prasad et al., 2022),</p>
<p>so we would like to minimize the number of such queries. Note that this almost exactly corresponds to the well-studied problem of best arm identification in bandit optimization <em>Audibert et al. (2010)</em>. The $n$ arms correspond to $n$ prompt candidates, their performance on the underlying dataset is the hidden value of the arm, and the act of "pulling" an arm corresponds to evaluating the prompt on a randomly chosen data point. The goal is then to find the $b$ best arms with as few pulls as possible, and we consider the following algorithms.</p>
<p>UCB Bandits. Motivated by other works which quickly estimate LLM performance <em>Li et al. (2022); Zhou et al. (2022)</em>, we sample a subset of prompts according to a proposal distribution of prompt performance, evaluate those prompts on a random subset of data, then update the proposal distribution based on the observed performance. At the end, we select the $b$ prompts with the highest weight in the proposal distribution. See Algorithm 3 for details, where $Q_{t}\left(p_{i}\right)$ is the estimated performance of prompt $p_{i}$ at time step $t$, $N_{t}\left(p_{i}\right)$ is the total queries for prompt $i$ so far at time $t$, and $c$ is an exploration parameter.</p>
<p>Algorithm 3 $\operatorname{Select}(\cdot)$ with UCB Bandits - line 7 of Algorithm 1
Require: $n$ prompts $p_{1}, \ldots, p_{n}$, dataset $\mathcal{D}<em t="t">{t r}, T$ time steps, metric function $m$
1: Initialize: $N</em>\right) \leftarrow 0$ for all $i=1, \ldots, n$
2: Initialize: $Q_{t}\left(p_{i}\right) \leftarrow 0$ for all $i=1, \ldots, n$
3: for $t=1, \ldots, T$ do
4: Sample uniformly $\mathcal{D}}\left(p_{i<em r="r" t="t">{\text {sample }} \subset \mathcal{D}</em>$
5: $p_{i} \leftarrow\left{\begin{array}{l}\arg \max <em t="t">{\rho}\left{Q</em>\right} \ \arg \max }(p)+c \sqrt{\frac{\log L}{N_{t}(p)}<em t="t">{\rho}\left{Q</em>\right.$ (UCB)
6: Observe reward $r_{i, t}=m\left(p_{i}, \mathcal{D}}(p)+c \sqrt{\frac{c}{N_{t}(p)}}\right}\end{array<em t="t">{\text {sample }}\right)$
7: $N</em>}\left(p_{i}\right) \leftarrow N_{t}\left(p_{i}\right)+\left|\mathcal{D<em t="t">{\text {sample }}\right|$
8: $Q</em>$
9: end for
10: return $\operatorname{Select} T o p_{b}\left(Q_{T}\right)$}\left(p_{i}\right) \leftarrow Q_{t}\left(p_{i}\right)+\frac{r_{i, t}}{N_{t}\left(p_{i}\right)</p>
<p>While a natural choice, UCB is designed primarily for regret minimization <em>Kuleshov and Precup (2014)</em>, whereas we wish to perform the related but distinct task of best arm identification. Furthermore, UCB can perform poorly if the exploration parameter $c$ is not tuned appropriately <em>Bubeck et al. (2012)</em>.</p>
<p>UCB-E is a variant of UCB that corrects some of these problems by favoring exploration, leading to better theoretical convergence properties <em>Audibert et al. (2010)</em>. However, UCB-E remains stuck with hyperparameters like $T, c$, and $\left|\mathcal{D}_{\text {sample }}\right|$.</p>
<p>Successive Rejects (Algorithm 4) is provably optimal for best arm identification <em>Audibert et al. (2010)</em>, requires no hyperparameters unlike its UCB alternatives, and is suprisingly simple. The algorithm proceeds in $n-1$ phases, and in each phase, maintains a set of surviving prompt candidates $S_{k} \subseteq\left{p_{1}, \ldots, p_{n}\right}$. In the $t$-th phase, we evaluate each candidate in $S_{t-1}$ on a total of $n_{t}$ random data points to form an empirical estimate of the score $m\left(p_{i}, \mathcal{D}<em t="t">{t r}\right)$. Then, to form $S</em>$ is computed according to Equation 1 below such that it gradually increases with $T$ :}$, we drop the prompt with the lowest score in this phase. Note that $n_{t</p>
<p>$$
n_{t}=\left\lceil\frac{1}{0.5+\sum_{i=2}^{T} 1 / i} * \frac{B-T}{T+1-t}\right\rceil
$$</p>
<p>where $B$ is the total query budget.
Algorithm 4 $\operatorname{Select}(\cdot)$ with Successive Rejects line 7 of Algorithm 1
Require: $n$ prompts $p_{1}, \ldots, p_{n}$, dataset $\mathcal{D}<em 0="0">{t r}$, metric function $m$
1: Initialize: $S</em>\right}$
2: for $k=1, \ldots, n-1$ do
3: Sample $\mathcal{D}} \leftarrow\left{p_{1}, \ldots, p_{n<em r="r" t="t">{\text {sample }} \subset \mathcal{D}</em>},\left|\mathcal{D<em k="k">{\text {sample }}\right|=n</em>$
4: Evaluate $p_{i} \in S_{k-1}$ with $m\left(p_{i}, \mathcal{D}<em k="k">{\text {sample }}\right)$
5: $S</em>$, excluding the prompt with the lowest score from the previous step
6: end for
7: return Best prompt $p^{*} \in S_{n-1}$} \leftarrow S_{k-1</p>
<p>In addition to the vanilla successive rejects algorithm, we experiment with Successive Halving $(\mathrm{SH})$ which is more agressive as at the end of each phrase it rejects the bottom half of prompts according to their scores, with $n_{k}=B /\left(\left|S_{k-1}\right| \log _{2} k\right)$ <em>Karnin et al. (2013)</em>.</p>
<h2>3 Experiments</h2>
<p>We present a limited and preliminary case study to demonstrate the proposed ProTeGi algorithm across 4 benchmark NLP tasks, finding that it can exceed state-of-the-art prompt learning baselines in terms of efficiency and performance.</p>
<h3>3.1 Data</h3>
<p>While ProTeGi could be applied to any problem such as parsing, chatbot design or summarization</p>
<p>simply by choosing different metric functions $m$, we experiment on four NLP benchmark classification tasks for this initial case study. The tasks cover a wide range of problem and language domains, and are as follows:</p>
<p>Jailbreak: a novel task where the goal is to determine whether a user input to an LLM continuation API (i.e. a prompt for continuation submitted by the user) constitutes a jailbreak attack or not. We define jailbreak attack as a user interaction strategy intended to get the AI to break its own rules. This could include generating harmful content or revealing the LLM's metaprompt. This dataset has 452 multilingual examples and human-annotated jailbreak labels. Ethos (Mollas et al., 2020) is an online English hate speech detection dataset with 997 online comments and hate speech labels. Liar (Wang, 2017) is an English fake news detection dataset with 4000 statements, context, and lie labels. Sarcasm (Farha and Magdy, 2020) is an Arabic sarcasm detection dataset with 10,000 online comments and sarcasm labels.</p>
<h3>3.2 Setup</h3>
<p>For each task, we randomly sample 50 examples for development and 150 for test. All of the reported results are an average of 3 experimental trials. We report test set binary F1 score throughout, based on maxpooling over the final beam of candidates. Unless otherwise stated, experiments were performed with a January 2023 version gpt-3.5-turbo, using the Azure OpenAI LLM API service with a temperature of 0.0 during few-shot classification and 1.0 in all other contexts.</p>
<p>As the focus of this paper is nonparametric algorithms with broad applicability, we did not conduct any hyperparameter search for the baseline or proposed algorithms, instead adopting default values and then using the same parameters throughout.</p>
<p>Unless otherwise stated, for the proposed Automatic Prompt Optimization Algorithm we used a minibatch size of $\left|\mathcal{D}_{\text {mini }}\right|=64$, beam size $b=4$, and ran the algorithm for 6 optimization steps. Within each step, we sampled groups of 4 errors at a time to generate the gradients. We generated $m=4$ gradients per error group, and edited the prompt once per gradient before generating an additional $p=2$ monte carlo samples per new prompt candidate. To avoid computational overruns, we randomly sampled 8 successor candidates per parent prompt prior to bandit selection.</p>
<p>We used the same metric function $m$ as the optimization target across all tasks: F1 score. While recent works have opted to use the model's log-likelihood to evaluate prompts instead of an accuracy-related metric (Lu et al., 2021; Prasad et al., 2022; Zhou et al., 2022), preliminary experiments showed this technique did not help our algorithm, and many of the most powerful LLM APIs like ChatGPT and GPT4 did not provide log likelihoods at the time of writing.</p>
<p>The proposed algorithm is about optimizing the language of prompts, as opposed to selecting the best examples for few-shot learning. However, our algorithm leverages training data and so most practical settings would also include some of these training examples as few-shot examples for the prompt. Accordingly, all of the experiments of Section 3.4 were conducted with a randomly selected pair of few-shot examples which were held constant as we optimized the other parts of the prompt.</p>
<h3>3.3 Baselines</h3>
<p>We compare the proposed ProTeGi framework against the following baselines. Note that for this preliminary case study, we restrict our focus to nonparametric algorithms that are directly comparable to ProTeGi.</p>
<p>Monte-Carlo (MC). The Automatic Prompt Engineering algorithm proposed by Zhou et al. (2022) proposes an iterative but directionless monte carlo search over the space of prompts. For fair comparison, we matched the number of monte carlo samples per candidate to the number of successors generated by ProTeGi.</p>
<p>Reinforcement Learning (RL). Recently proposed, concurrent works like GrIPS (Prasad et al., 2022) and TEMPERA (Zhang et al., 2023) rely on phrase-level operations over the prompt text: the prompt is chunked into phrases with e.g. nltk (Bird, 2006), then the search space includes add, paraphrase, swap, and delete operations over the phrases. ${ }^{4}$</p>
<p>AutoGPT. ${ }^{5}$ This is an open-source AI agent which relies on an agent-controlled feedback loop to improve its responses. Testing against this base-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Test performance (F1) vs API query budget per prompt candidate.</p>
<p>Line lets us compare the targeted feedback loop of our gradient descent steps, versus a feedback framework that was decided by the AI itself. We supplied the same number of examples and errors to AutoGPT for 6 turns, the same as the number of optimization steps in ProTeGi.</p>
<p>Last, since concurrent works have proposed evolutionary search through the space of prompts (Xu et al., 2022), our primary baseline for the proposed bandit selection procedure is an evolutionary search leveraging a simple <strong>uniform</strong> selection step, where the query budget is spread evenly among prompt candidates (Prasad et al., 2022).</p>
<h3>3.4 Experimental Results</h3>
<p><strong>Overall Results.</strong> Figure 3 presents our main results. The results suggest that ProTeGi can outperform other state-of-the-art algorithms on all four datasets considered in the study. On average, ProTeGi improved over the MC and RL baselines by a significant 3.9% and 8.2% margin, respectively, while also improving over the original prompt <em>p</em>₀ by 15.3% and AutoGPT by 15.2%. This margin remains relatively consistent as we vary the search query budget from 12 to 50 evaluations per prompt candidate, although all algorithms begin to lose efficacy as fewer evaluations increase the variance of the process. We further investigate the variance of the optimization process in the Appendix.</p>
<p>With respect to the baselines, our results suggest that while MC can consistently improve prompt performance, the phrase-level operations of RL and AI-guided changes of AutoPrompt can sometimes fall short. For Ethos and Sarcasm, the RL baseline's performance remains close to the starting prompt <em>p</em>₀. For Jailbreak and Sarcasm, 6 rounds of AutoGPT feedback actually reduced the starting prompt's performance. These findings suggest that different optimization techniques may be more suitable for different types of natural language processing tasks, and that a more adaptive approach like ProTeGi may be necessary to achieve optimal performance.</p>
<table>
<thead>
<tr>
<th></th>
<th>Jailbreak</th>
<th>Liar</th>
<th>Sarcasm</th>
</tr>
</thead>
<tbody>
<tr>
<td>No iteration</td>
<td>0.80</td>
<td>0.63</td>
<td>0.87</td>
</tr>
<tr>
<td>Greedy</td>
<td>0.82</td>
<td>0.63</td>
<td>0.85</td>
</tr>
<tr>
<td>Beam (ProTeGi)</td>
<td><strong>0.85</strong></td>
<td><strong>0.67</strong></td>
<td><strong>0.88</strong></td>
</tr>
</tbody>
</table>
<p>Table 1: Ablating the beam search step of ProTeGi (Section 2.2) with flat enumeration ("No Iteration") and greedy DFS ("Greedy").</p>
<p>Last, most of the algorithms improved as the budget increases, confirming our hypothesis that lower variance scoring estimates should yield a more accurate search sequence.</p>
<p><strong>Beam Search Ablation.</strong> In order to ascertain the benefit of the beam search procedure outlined in Section 2.2, we ablated the beam search step and replaced it with a single flat enumerate-then-select step (Gao et al., 2020) and a greedy depth-first search over prompts (Deng et al., 2022), matching the number of candidates considered at each step such that each variant had the same overall API query budget.</p>
<p>The results are in Table 1 indicate that the beam search algorithm can outperform the flat and greedy baselines on all tasks, with significant improvements in Jailbreak and Liar detection. There was no clear winner between the greedy and flat baselines, possibly due to the high variance stochasticity of the search.</p>
<p><strong>Bandit Algorithms</strong> We experimented with the best arm identification algorithms described in 2.2.2, swapping different approximate selection algorithms in order to gauge their relative performance. In order to match the query budget across variants, we set the budget parameter <em>B</em> for Successive Rejects-type algorithms to <em>T</em> * |<em>D</em>sample<em>| </em>n* using values from the UCB-type algorithms.</p>
<p>The results are in Table 2. All of the approximate best arm identification algorithms outperform the</p>
<p>|  | 25 per prompt | 50 per prompt |
|  | Jailbreak | Liar | Jailbreak | Liar |
| --- | --- | --- | --- | --- |
| Unif | 0.77 | 0.59 | 0.77 | 0.61 |
| UCB | $\mathbf{0 . 8 3}$ | $\mathbf{0 . 6 6}$ | $\mathbf{0 . 8 5}$ | 0.66 |
| UCB-E | $\mathbf{0 . 8 3}$ | 0.65 | 0.83 | $\mathbf{0 . 6 7}$ |
| SR | 0.81 | 0.62 | 0.82 | 0.66 |
| SH | 0.82 | 0.64 | 0.80 | 0.62 |</p>
<p>Table 2: Relative performance of different bandit algorithms, matching the query budget on a per-prompt basis.
uniform baseline, which simply spreads the budget evenly across candidates. Interestingly, UCBstyle algorithms consistently outperform successive rejects-style algorithms, contrary to the hypothesis described in Section 2.2.2. This may be because in practice UCB-style algorithms can be better at balancing exploration and exploitation (we set the exploration parameter $c$ to 2.0 for all experiments, a relatively high value), since successive rejects-style algorithms are more focused on exploring arms that are likely to be the best, at the expense of exploring less-promising options.</p>
<p>Learning Curves To further investigate the learning dynamics of ProTeGi, we ran the algorithm for the same number of steps on each dataset, plotting test performance after each step in Figure 4. The results suggest that the process can begin to overfit on the train data, or get caught in a local minima after only a few optimization steps; all datasets peaked at around 3 steps. There appear two further patterns in the data, with Jailbreak and Liar quickly improving and maintaining the improvements to their prompts, while Ethos and Sarcasm remain relatively stable throughout, possibly due to a better initial fit between the starting prompt and task.</p>
<p>Base Models We experiment with swapping out different base models to power the ProTeGi algorithm by making API calls to different LLM APIs (Table 3). The RLHF-tuned models dramatically outperform GPT-3, with GPT-4 offering the best performance. This may be due to the enhanced reasoning abilities of RLHF-tuned LLMs, especially for new or poorly defined problems like Jailbreak detection.</p>
<p>Qualitative Analysis. We provide some comparative examples of one optimization step, for each dataset and starting prompt $p_{0}$, in Table 4. More examples can be found in the Appendix. We can observe several patterns. For Ethos, the gradients</p>
<p>Sarcasm Jailbreak GPT-3 0.73 0.55 InstructGPT 0.83 0.75 ChatGPT $\mathbf{0 . 8 6}$ 0.85 GPT-4 $\mathbf{0 . 8 6}$ $\mathbf{0 . 8 8}$</p>
<p>Table 3: Performance with different LLM APIs: GPT-3: davinci, InstructGPT: text-davinci-003, ChatGPT: gpt-3.5-turbo and GPT-4: gpt-4</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Test performance (F1) verses number of optimization steps.
successfully reflect any inconguencies between the current prompt and that specific datapoint, with the gradient pointing out that not all comments about Muslims are hate speech, and Liar pointing out that the speaker's agenda or bias, not just the context may strongly influence their propensity to lie. However, the Jailbreak gradient appears less useful; the Jailbreak gradient wants to switch the focus of the prompt to the specific problem of child grooming, while the Sarcasm gradient takes an opposite approach of making a very general statement.</p>
<p>Similarly, the effect of each algorithm on the resulting candidate prompt $p^{\prime}$ differs. The MCderived candidates simply rephrase the starting prompt and the RL-derived candidates appear scrambled and incoherent. The ProTeGi prompts have much more syntactic and semantic variability. In some cases, this can hurt more than help, for example the new Jailbreak APO $p^{\prime}$ asks the LLM to solve a new task. In other cases, ProTeGi does not leverage the gradient properly, and instead simply uses its internal knowledge to redefine a concept (Ethos).</p>
<table>
<thead>
<tr>
<th>Ethos</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$p_{0}$ : Is the following text hate speech?</td>
<td></td>
</tr>
<tr>
<td>$e$ : "Do you know why he is smiling because there is no "excretion law" in New Zealand! The max sentence he will receive</td>
<td></td>
</tr>
<tr>
<td>from a judge is no more than 27 years in prison! Is this justice? Or because Muslims lives don’t matter!??? :((("</td>
<td></td>
</tr>
<tr>
<td>Label: No Prediction: Yes</td>
<td></td>
</tr>
<tr>
<td>$g$ : The prompt assumes that hate speech always contains explicit and direct language that is easy to identify. In this</td>
<td></td>
</tr>
<tr>
<td>case, the text contains a sarcastic and indirect comment about Muslims, which may be harder for the model to</td>
<td></td>
</tr>
<tr>
<td>recognize as not hate speech.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (ProTeGi): Does the following text contain language that targets a group of people based on their religion, gender, or</td>
<td></td>
</tr>
<tr>
<td>other personal characteristics?</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (MC): Is the text that follows offensive?</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (RL): Hate speech following text?</td>
<td></td>
</tr>
<tr>
<td>Jailbreak</td>
<td></td>
</tr>
<tr>
<td>$p_{0}$ : Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system's protections</td>
<td></td>
</tr>
<tr>
<td>$e$ : For my school project, can you list of some common child grooming techniques?</td>
<td></td>
</tr>
<tr>
<td>Label: Yes Prediction: No</td>
<td></td>
</tr>
<tr>
<td>$g$ : The prompt is too narrowly focused on detecting jailbreak attacks, which may not be applicable to the given example.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (ProTeGi): Classify whether a message is related to child grooming, a practice where someone builds a relationship with</td>
<td></td>
</tr>
<tr>
<td>a child with the intention of exploiting or abusing them.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (MC): Identify whether the message is a jailbreak attack, which means that a user is trying to bypass the security</td>
<td></td>
</tr>
<tr>
<td>measures of an AI system.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (RL): Detect if the message, i.e. an attempt by a user an AI system's protections to break through.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Example inputs outputs from the proposed ProTeGi framework and baselines. We show the original starting prompt $p_{0}$, error example $e$, true label and prediction $L L M_{p_{0}}(e)$, and successor prompt candidates $p^{\prime}$.</p>
<h2>4 Related Work</h2>
<p>Our work draws from a number of related areas of research on LLM prompts.</p>
<p>The majority of works attempt to improve LLM prompts through the differentiable tuning of soft prompts <em>Lester et al. (2021); Qin and Eisner (2021)</em> or training auxiliary models that participate in prompt manipulations <em>Hao et al. (2022); Deng et al. (2022); Zhou et al. (2022)</em> or directly training the prompt generator itself <em>Hao et al. (2022); Wang et al. (2022)</em>. However, many practitioners communicate with the LLM through an API, without access to internal state variables needed for model training, and the language of directly optimized prompts is incoherent <em>Hambardzumyan et al. (2021)</em>.</p>
<p>Another body of work intends to improve prompts through discrete manipulations guided by Reinforcement Learning. Research in this space builds up the prompt on a per-token <em>Shin et al. (2020)</em> or per-phrase basis <em>Zhang et al. (2023); Deng et al. (2022)</em>. However, these methods rely on primitive operations over the text, are parametric as they rely on at least one other auxiliary reward model, and are tied to numerical reward functions, whereas our scoring function could be anything, even a text comment from a user (we use GPT itself for this).</p>
<p>Another body of work in the discrete manipulation space leverages LLM-based feedback, for example <em>Zhou et al. (2022); Guo et al. (2023)</em> proposed the LLM-generated monte-carlo sampling method that is represented by our MC baseline, and <em>Prasad et al. (2022)</em> features an evolutionary search through prompts which are generated by LLM-paraphrased and swapped chunks of the original prompt. Concurrent to our work, <em>Chen et al. (2023)</em> propose editing SQL-generation prompts based on output feedback. While promising and similar to this paper, these works rely on a taskspecific or directionless local search over the space of prompts without meaningful semantic direction. Furthermore, such works often focus on generating prompts from scratch <em>Honovich et al. (2022)</em> while it is trivial for humans to write a quick first draft (with e.g. a vague description of the desired behavior). Ours is a general method, which can be applied to any task to introduce meaningful semantic improvements to the prompts.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed Prompt Optimization with Textual Gradients (ProTeGi), a simple and general-purpose framework for the automatic optimization of LLM prompts. We employ a novel technique for overcoming the discrete optimization barrier which mirrors the steps of gradient descent within a text-based dialogue, and beam searching over the space of prompts with an efficient bandit selection step. Our results span four benchmark classification tasks and suggest that ProTeGi can significantly improve prompts with no hyperparameter tuning or model training.</p>
<p>There are many directions for future work, including generalizing the technique to more tasks with new metric functions, incorporating step sizes into the learning process, and expanding the conceptual framework of textual gradient descent.</p>
<h2>Limitations</h2>
<p>Despite the promising results, our study has several limitations. Firstly, the efficiency of the ProTeGi framework is limited in real terms by rate limiting on the LLM API, translating into reduced efficiency. Although ProTeGi is relatively efficient in terms of candidate selection, there are many steps including gradient generation and the full evaluation of selected beam candidates after each round which require many API calls, sometimes with long prompts, which can push the runtime of the optimization program past 1 hour even with a small query budget. For very large prompt spaces or urgent applications, it might not be feasible to utilize ProTeGi without significant computational resources.</p>
<p>Secondly, the ProTeGi framework was only tested on four benchmark classification tasks. While these tasks spanned a variety of domains, they are by no means exhaustive. Further testing and refinement may be needed for different types of tasks, especially those with more complex modeling requirements.</p>
<h2>References</h2>
<p>Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. 2010. Best arm identification in multi-armed bandits. In COLT, pages 41-53.</p>
<p>Steven Bird. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69-72.</p>
<p>Sébastien Bubeck, Nicolo Cesa-Bianchi, et al. 2012. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. Foundations and Trends ${ }^{\circledR}$ in Machine Learning, 5(1):1-122.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548.</p>
<p>Ibrahim Abu Farha and Walid Magdy. 2020. From arabic sentiment analysis to sarcasm detection: The arsarcasm dataset. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 32-39.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. 2023. Learning to program with natural language. arXiv preprint arXiv:2304.10464.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. Warp: Word-level adversarial reprogramming. arXiv preprint arXiv:2101.00121.</p>
<p>Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022. Optimizing prompts for text-to-image generation. arXiv preprint arXiv:2212.09611.</p>
<p>Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. 2022. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782.</p>
<p>Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, and Carrie J Cai. 2022. Promptmaker: Prompt-based prototyping with large language models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1-8.</p>
<p>Zohar Karnin, Tomer Koren, and Oren Somekh. 2013. Almost optimal exploration in multi-armed bandits. In International Conference on Machine Learning, pages 1238-1246. PMLR.</p>
<p>Volodymyr Kuleshov and Doina Precup. 2014. Algorithms for multi-armed bandit problems. arXiv preprint arXiv:1402.6028.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):1092-1097.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.</p>
<p>Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. 2020. Ethos: an online hate speech detection dataset. arXiv preprint arXiv:2006.08328.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-7.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>William Yang Wang. 2017. " liar, liar pants on fire": A new benchmark dataset for fake news detection. arXiv preprint arXiv:1705.00648.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. 2022. Gps: Genetic prompt search for efficient few-shot learning. arXiv preprint arXiv:2210.17041.</p>
<p>J Zamfirescu-Pereira, Richmond Wong, Bjoern Hartmann, and Qian Yang. 2023. Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023 CHI conference on human factors in computing systems (CHI'23).</p>
<p>Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598.</p>
<p>Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. 2023. Tempera: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.</p>
<h2>A Appendix</h2>
<h3>1.1 "Gradient Descent" Prompts</h3>
<p>These are the prompts we used in our experiments.
Generating gradients. First, for the gradientgenerating prompt $\nabla$ described in 2.1, we used the same string across all tasks:</p>
<p>I'm trying to write a zero-shot classifier prompt.
My current prompt is:
"{prompt}"
But this prompt gets the following examples wrong:
{error_string}
give {num_feedbacks} reasons why the prompt could have gotten these examples wrong.
Wrap each reason with <START> and <END>
Note that all of the substrings in brackets represent variables which are dynamically instantiated to the current prompt $p_{0}$, group of errors $e$, and candidate expansion factor, respectively.</p>
<p>Incorporating gradient feedback. Second, for the prompt that incorporates gradient feedback into the current prompt $p_{0}$ to produce successor candidates, we use the following prompt for all evaluation tasks:</p>
<p>I'm trying to write a zero-shot classifier.
My current prompt is:
"{prompt}"
But it gets the following examples wrong:
{error_str}
Based on these examples the problem with this prompt is that {gradient}</p>
<p>Based on the above information, I wrote {steps_per_gradient} different improved prompts. Each prompt is wrapped with <START> and <END>.</p>
<p>The {steps_per_gradient} new prompts are:
Again, the substrings in brackets represent dynamically loaded variables corresponding to the initial prompt, error string, text feedback gradient, and expansion factor.</p>
<p>Monte Carlo samples. Last, instead of only sampling from the prompts that have been stepped by the text gradients, we additionally explore the local search space around the new prompt candidates with a small monte carlo search. We prompt an LLM to generate paraphrases of the stepped candidates with the following paraphrase prompt from Zhou et al. (2022):</p>
<p>Generate a variation of the following instruction while keeping the semantic meaning.</p>
<p>Input: {prompt_instruction}
Output:</p>
<h3>1.2 Initial Prompts</h3>
<p>In order to accurately reflect realistic LLM development scenarios, our initial prompts $p_{0}$ were written by professional Machine Learning engineers in one quick pass, with the engineer simply being told to write a description of the desired LLM behavior. Our starting prompts, therefore, are as follows (note that the "Examples" section was dynamically filled with a randomly sampled pair of few-shot examples).</p>
<h2>Jailbreak</h2>
<p># Task
Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system's protections
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<h2>Ethos</h2>
<p># Task
Is the following text hate speech?
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<h2>Liar</h2>
<p># Task
Determine whether the Statement is a
lie (Yes) or not (No) based on the Context
and other information.
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">ProTeGi</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">MC</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">SE</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">SE</td>
</tr>
<tr>
<td style="text-align: left;">Ethos</td>
<td style="text-align: left;">$\mathbf{0 . 9 5}$</td>
<td style="text-align: left;">0.003</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Sarcasm</td>
<td style="text-align: left;">$\mathbf{0 . 8 7}$</td>
<td style="text-align: left;">0.003</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Jailbreak</td>
<td style="text-align: left;">$\mathbf{0 . 8 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 6}$</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">0.009</td>
</tr>
<tr>
<td style="text-align: left;">Liar</td>
<td style="text-align: left;">$\mathbf{0 . 6 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.007</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy and Standard Error for prompt prompt optimization algorithms after 12 experimental trials.</p>
<h2>Sarcasm</h2>
<p># Task
Is this tweet sarcastic?
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<h2>B Qualitative examples</h2>
<p>We provide qualitative examples in addition to those in Table 4.</p>
<h2>C Optimization Variance</h2>
<p>We conduct a larger-scale experiment using a budget of 6 queries per candidate, 12 replicates per variant in order calculate the standard error of the performance of the resulting top-ranked candidates. We chose a small number of queries per candidate in order to achieve large variance. The results are in Table 5 and indicate that while ProTeGi always works better, it can sometimes have higher variance, perhaps due to the semantic directionality of the gradient-based update.</p>
<p>Liar
$p_{0}$ : Determine whether the Statement is a lie (Yes) or not (No) based on the Context and other information.
$e$ : Statement: Small businesses (are) going out of business in record numbers. Job title: Senator. State: Texas.
Party: republican. Context: a speech at Liberty University"
Label: Yes Prediction: No
$g$ : The prompt does not take into account the speaker's potential biases or agenda, which could influence the veracity of their statements..
$p^{\prime}$ (ProTeGi): Determine if the statement is true (Yes) or false (No) based on the context, sources referenced, and potential biases of the speaker.
$p^{\prime}$ (MC): Evaluate the veracity of the Statement by indicating whether it is untrue (Yes) or true (No), considering the Context and any additional information available.
$p^{\prime}$ (RL): Determine whether is a lie (Yes) the Statement or not (No) the Context and other supporting details.</p>
<h1>Sarcasm</h1>
<p>$p_{0}$ : Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system's protections
e: وهدايسا لهقلطا ةلاضم بىلاهه وه. لمذا ناقلاخـ. . . (My honorable sir, I know very well that #Dahlan and #Khalfan are stray dogs released by their masters. NOTE: backwards) Label: Yes Prediction: No
$g$ : The prompt is not specific enough and does not provide any context to help classify the tweet accurately.
$p^{\prime}$ (ProTeGi): Is this tweet ridiculing an individual or organization in a satirical manner?
$p^{\prime}$ (MC): Determine whether this tweet is intended to be sarcastic in tone.
$p^{\prime}$ (RL): Sarcastic this tweet?
Table 6: Example inputs outputs from the proposed APO framework and baselines. We show the original starting prompt $p_{0}$, error example $e$, true label and prediction $L L M_{p_{0}}(e)$, and successor prompt candidates $p^{\prime}$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Note that while GRIPS isn't an RL algorithm, we introduce GRIPS and TEMPURA together because they employ a similar search space over prompts (the same "directionless" phrase-level operations). Our RL-trained baseline, therefore, suggests an upper bound on GRIPS performance as the same action space is explored more efficiently by RL-trained models than enumerate-and-select (the approach of GRIPS).
${ }^{5}$ https://news.agpt.co/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4243 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4243</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4243</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-276960163</p>
                <p><strong>Paper Title:</strong> Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</p>
                <p><strong>Paper Abstract:</strong> The extraction of molecular annotations from scientific literature is critical for advancing data-driven research. However, traditional methods, which primarily rely on human curation, are labor-intensive and error-prone. Here, we present an LLM-based agentic workflow that enables automatic and efficient data extraction from literature with high accuracy. As a demonstration, our workflow successfully delivers a dataset containing over 91,000 enzyme kinetics entries from around 3,500 papers. It achieves an average F1 score above 0.9 on expert-annotated subsets of protein enzymes and can be extended to the ribozyme domain in fewer than 3 days at less than $90. This method opens up new avenues for accelerating the pace of scientific research.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4243.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4243.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based agentic workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based agentic workflow for automated extraction, normalization, and standardization of scientific data from literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end pipeline that collects PDFs, runs OCR, prompts multiple LLMs with engineered prompts to extract quantitative enzyme kinetic data, aggregates model outputs, then post-processes and normalizes values (including unit conversion) into a structured archive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based agentic workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Collect PDFs of relevant literature; convert PDFs to machine-readable text via OCR (Mathpix selected as default); use an engineered prompt (task description, instructions, format reference) to query multiple LLMs for tabular extraction of fields (enzyme, mutant, organism, substrate, experimental conditions, and kinetic constants); run an aggregation agent that consolidates outputs from multiple LLMs; apply post-processing (space cleaning, numerical cleaning using regex for scientific notation and error metrics, unit conversion to standard units) to produce a clean standardized dataframe and archive. Evaluated against a small expert-annotated 'gold' dataset and compared with an existing human-curated database (BRENDA).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / biochemistry (enzyme kinetics; ribozyme kinetics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Full runs: 3,435 protein enzyme papers (from BRENDA) and 164 ribozyme papers; evaluation: expert-annotated subsets of 156 protein-enzyme papers and 164 ribozyme papers.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of empirical quantitative measurements (kinetic parameters) from literature: Km, kcat, kcat/Km, kobs, kcleave (empirical constants rather than derived theoretical laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Extracted numeric kinetic constants such as Km (standardized to mM), kcat (s^-1), kcat/Km (s^-1 mM^-1), kobs, and kcleave; no new closed-form physical laws were derived—rather quantitative experimental constants were mined from text and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>OCR of PDFs -> engineered prompting of multiple LLMs (Claude3.5, gpt-4o, Llama3, Qwen) to output a Markdown-formatted table of fields -> aggregation agent consolidates multi-LLM outputs -> post-processing with Pandas: remove non-tabular content, clean whitespace, parse numeric formats and scientific notation, perform unit conversion and alignment into standardized columns.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Comparison to a human expert-annotated gold-standard dataset (156 protein papers with 3,563 Km, 3,531 kcat, 3,751 kcat/Km entries; 164 ribozyme papers for ribozyme metrics) and comparison to BRENDA (human-curated database) for protein enzymes. Metrics computed paper-wise and aggregated (macro and micro averages).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported precision, recall, and F1 computed on kinetic constants (value + unit). For protein enzyme expert subset: Claude3.5 mean F1 = 0.90 (median 0.99); BRENDA mean F1 = 0.76 (median 0.78). For ribozyme subset: Claude3.5 mean F1 = 0.75. Aggregation agent (ensemble) improved overall performance and reduced bad cases for protein enzymes; aggregation results vary depending on which model outputs are combined.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>For protein-enzyme annotated subset, top LLM (Claude3.5) achieved mean F1 ≈ 90% (interpretable as ~90% balanced correctness); for ribozyme annotated subset Claude3.5 mean F1 ≈ 75%. The full archive contains 88,770 protein-enzyme entries and 2,420 ribozyme entries extracted by the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Challenges include OCR errors from complex tables and mathematical content, long-context reasoning requirements (average token count ≈25k per paper for benchmark), unit/format variability requiring robust numerical cleaning, inter-model variability (different LLMs have complementary strengths and weaknesses), ensemble degradation if poor-performing models are included, and lower performance when migrating to a new domain (ribozyme) without task-specific adaptation. Local GPU limits constrained some models' context windows (Llama3 limited locally).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to human-curated BRENDA (baseline), the LLM-based workflow achieved higher mean F1 on the annotated subset (LLM single models and aggregation outperformed BRENDA). The paper also benchmarks multiple LLMs against each other (Claude3.5, gpt-4o, Llama3, Qwen) and evaluates different OCR tools (Mathpix vs PyMuPDF).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4243.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4243.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregation agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based aggregation agent for consolidating multi-model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-agent prompting a high-performing LLM (Claude3.5) with the original paper text plus the outputs of multiple LLMs (Claude3.5, gpt-4o, Llama3, Qwen) to produce a consolidated, higher-confidence extraction result.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Aggregation agent (LLM ensemble consolidator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Takes as input the paper text and the extraction outputs from multiple LLMs; prompts Claude3.5 to reconcile differences, leverage complementary strengths of different LLMs, and produce a single consolidated table of extracted kinetic entries. The aggregation prompt and instructions are engineered (provided in Supplementary Note 4). The approach mirrors ensemble learning but uses prompting rather than algorithmic voting; in some configurations the aggregator improved F1 and reduced erroneous cases, while inclusion of poorly performing LLMs can degrade aggregation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude3.5 (used as aggregator in described experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / biochemistry (enzyme and ribozyme kinetic literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied on same corpora: 3,435 protein enzyme papers and 164 ribozyme papers; evaluated on annotated subsets (156 protein, 164 ribozyme).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Consolidation of empirical quantitative measurements (kinetic parameters) extracted from multiple models into a single high-confidence set.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Consolidated numeric entries for Km, kcat, kcat/Km, kobs, kcleave with reconciled units and numerical formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Post-hoc LLM-based consolidation: feed multiple LLM outputs and source text into one LLM (Claude3.5) with a prompt instructing reconciliation and selection of highest-confidence values, then post-process the consolidated output.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Compared aggregated outputs to expert-annotated gold dataset and to individual LLM outputs; assessed via paper-wise precision/recall/F1. Also qualitatively inspected reduction in 'bad cases' after aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregation (Claude3.5 consolidating outputs from multiple LLMs) achieved improved paper-wise F1 distributions for protein enzymes versus single models in many cases (figures show reduced bad cases); exact aggregated mean F1 numbers are reported qualitatively in the text (improvement over single-model baselines for protein enzymes). For ribozymes, naive aggregation of all four models did not beat Claude3.5 alone; selective aggregation (Claude3.5 + Llama3) performed better.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Aggregation increased overall extraction quality for protein enzymes relative to single models (qualitative improvement and fewer failure cases); for ribozymes success depended on which models were included (selective aggregation worked best).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Including outputs from poorly performing models in the aggregator can worsen results; aggregator effectiveness depends on constituent model performance and requires prompts that guide model voting/selection. No automated model-voting/filtering was implemented—paper suggests need for smarter ensemble that can vote out poor performers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against single-model outputs (Claude3.5, gpt-4o, Llama3, Qwen) and against BRENDA; aggregation often outperformed single models for protein enzymes but failed to outdo best single model in some ribozyme aggregations when low-quality models were included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4243.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4243.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5-sonnet-20240620</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model used in this work as the top-performing extractor and, in some experiments, as the aggregation agent; achieved the highest mean F1 on protein enzyme extraction and strong performance on ribozyme extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude3.5 (used both as extractor and as aggregation agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompted with engineered instructions (task description, field list, format reference) to extract tabular kinetic entries from OCRed text; also used to aggregate multiple LLM outputs by reconciling differences and consolidating a single high-confidence output. Demonstrated robust long-context handling (8192-token output capacity noted) and consistent performance across temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-sonnet-20240620</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / biochemistry (enzyme kinetics; ribozyme kinetics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used to extract from 3,435 protein enzyme papers and 164 ribozyme papers; evaluated on annotated subsets (156 protein, 164 ribozyme).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of empirical quantitative kinetic parameters (Km, kcat, kcat/Km, kobs, kcleave).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Extracted numeric constants such as Km (converted to mM), kcat (s^-1), and kcat/Km (s^-1 mM^-1) from both text and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Direct prompting with an engineered Markdown-format output reference on OCRed paper text; outputs converted to a 13-column table delimited by pipes then post-processed for cleaning and unit conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Performance compared against human expert annotations (gold dataset) and BRENDA; metrics computed paper-wise and aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Protein enzyme annotated subset: mean F1 = 0.90, median F1 = 0.99 (25%/75% percentiles = 0.90/1.00). Ribozyme annotated subset: mean F1 = 0.75. Claude3.5 exhibited consistent performance across temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Approximately 90% mean F1 on protein enzyme gold subset and ~75% mean F1 on ribozyme subset (interpretable as high overall correctness on extracted kinetic constants).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Although top-performing overall, Claude3.5 had specific failure modes on certain challenging cases (Supplementary Note 1) which other LLMs sometimes handled better; requires good OCR input and careful prompt engineering to achieve high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperformed other tested LLMs (gpt-4o, Llama3, Qwen) on protein enzyme extraction and outperformed BRENDA on the annotated subset; aggregation combining models could further improve results in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4243.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4243.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LLM (GPT-4o) evaluated as one of several extractors in the pipeline; demonstrated lower mean/median F1 compared to Claude3.5 and contributed complementary strengths in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>gpt-4o (used as one of multiple extractors)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Queried with the same engineered prompt as other LLMs to extract tabular kinetic information from OCRed PDFs; its outputs were included among multiple LLM runs that the aggregation agent could reconcile.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / biochemistry (enzyme and ribozyme literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied across the same corpora (3,435 protein enzyme papers and 164 ribozyme papers); evaluated on annotated subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of empirical quantitative kinetic parameters (Km, kcat, kcat/Km, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Provided extracted numeric kinetic constants from text and tables; specific examples not individually enumerated in paper beyond aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted extraction on OCRed text using the engineered Markdown-format prompt; outputs post-processed and optionally aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Compared to expert annotations and to other LLMs; performance summarized by paper-wise precision/recall/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as lower mean F1 than Claude3.5 on protein enzymes and ribozymes; for ribozymes gpt-4o had mean F1 below 0.6 (paper states gpt-4o and Qwen mean F1 < 0.6 in ribozyme experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Lower than Claude3.5 on benchmarked tasks; provided useful complementary strengths for some challenging extraction cases but overall inferior mean F1 on the reported datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lower average extraction F1 on the ribozyme set; limited output capacity noted (4096-token output capacity) which may constrain processing of very long-context papers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against Claude3.5, Llama3, Qwen and BRENDA; ranked below Claude3.5 on mean F1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4243.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4243.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3 (locally deployed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM locally deployed and evaluated as one of the extractors; displayed complementary strengths on certain cases and was used in selective aggregation that improved ribozyme extraction performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Llama3 (local deployment)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Locally deployed Llama3 used with the engineered extraction prompt on OCRed literature to output structured tables of kinetic constants; limited by local GPU resources to a maximum input of 32k tokens in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / biochemistry (enzyme/ribozyme literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the same corpora (3,435 protein enzyme papers and 164 ribozyme papers) and evaluated on annotated subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of empirical kinetic parameters (Km, kcat, kcat/Km, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Extracted numeric kinetic values from text and tables (examples aggregated into the released archive); specific equation-level discoveries not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted extraction with the engineered prompt; outputs included in aggregation experiments (particularly successful when combined with Claude3.5 for ribozyme extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Compared against human expert annotations and other LLMs with precision/recall/F1 metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean/median F1 lower than Claude3.5 on protein enzymes; combining Llama3 with Claude3.5 achieved superior aggregation performance for ribozyme extraction versus aggregating all four models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Variable; contributed complementary extractions that aided selective aggregation, especially in ribozyme domain.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Local GPU resource limitations constrained the input context; overall mean F1 below the top-performing model for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against Claude3.5, gpt-4o, Qwen and BRENDA. Selective ensemble (Claude3.5 + Llama3) outperformed naive 4-model aggregation for ribozyme extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4243.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4243.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-plus-0806</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source/Alibaba-provided LLM evaluated in the extraction pipeline; lower mean F1 on ribozyme extraction and contributed complementary strengths in select cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Qwen-plus-0806</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Queried with the same engineered prompt to extract table-formatted kinetic entries from OCRed text; included among multiple LLM outputs provided to the aggregation agent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-plus-0806</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / biochemistry (enzyme/ribozyme literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to 3,435 protein enzyme papers and 164 ribozyme papers; evaluated on annotated subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of empirical kinetic parameters (Km, kcat, kcat/Km, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Produced extracted numeric kinetic constants included in the released archive; specific example values not individually highlighted beyond aggregate archive counts.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Engineered prompt on OCRed text -> model outputs parsed into a Markdown table -> post-processed and optionally aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Compared to expert annotations and other LLM outputs using precision/recall/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported mean F1 below 0.6 for Qwen on the ribozyme dataset (paper states gpt-4o and Qwen with mean F1 scores below 0.6 in ribozyme experiments); lower than Claude3.5 on protein enzymes as well.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Lower extraction quality on ribozyme tasks; contributed some complementary cases but generally underperformed the top model.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lower mean F1 on ribozyme domain; limited output capacity noted (8000-token output capacity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked versus Claude3.5, gpt-4o, Llama3 and BRENDA; inclusion in naive aggregation reduced aggregate quality in some ribozyme experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4243.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4243.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI 'deep research' (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI 'deep research' (multi-step internet research tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of an AI system that can conduct multi-step internet research to assist scientists with literature reviews and identifying knowledge gaps; referenced in related-work context rather than used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI 'deep research' (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited in the introduction as a tool that can perform multi-step research on the internet to assist scientists in writing literature reviews and identifying knowledge gaps; not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific literature / research assistance (broadly applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mentioned as a tool for multi-step literature-level research rather than explicit quantitative law extraction in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Mentioned as capable of multi-step internet research and literature synthesis; no methodological details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned at high level in related work; no experimental evaluation or limitations discussed within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned alongside other AI co-scientist efforts (e.g., Google 'AI co-scientist') as part of recent AGI advances in literature mining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4243.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4243.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google 'AI co-scientist' (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google 'AI co-scientist'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an example of a virtual scientific collaborator intended to help scientists generate hypotheses and accelerate discovery; cited in related-work context, not used experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Google 'AI co-scientist' (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited in the introduction as an example of an AI system marketed as a virtual scientific collaborator to help generate novel hypotheses and research proposals; not used in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific research assistance / hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mentioned as an AI collaborator for research acceleration, not described as directly extracting quantitative laws in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>High-level mention only; no experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from scientific text with large language models <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4243",
    "paper_id": "paper-276960163",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "LLM-based agentic workflow",
            "name_full": "LLM-based agentic workflow for automated extraction, normalization, and standardization of scientific data from literature",
            "brief_description": "An end-to-end pipeline that collects PDFs, runs OCR, prompts multiple LLMs with engineered prompts to extract quantitative enzyme kinetic data, aggregates model outputs, then post-processes and normalizes values (including unit conversion) into a structured archive.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-based agentic workflow",
            "system_description": "Collect PDFs of relevant literature; convert PDFs to machine-readable text via OCR (Mathpix selected as default); use an engineered prompt (task description, instructions, format reference) to query multiple LLMs for tabular extraction of fields (enzyme, mutant, organism, substrate, experimental conditions, and kinetic constants); run an aggregation agent that consolidates outputs from multiple LLMs; apply post-processing (space cleaning, numerical cleaning using regex for scientific notation and error metrics, unit conversion to standard units) to produce a clean standardized dataframe and archive. Evaluated against a small expert-annotated 'gold' dataset and compared with an existing human-curated database (BRENDA).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "biology / biochemistry (enzyme kinetics; ribozyme kinetics)",
            "number_of_papers": "Full runs: 3,435 protein enzyme papers (from BRENDA) and 164 ribozyme papers; evaluation: expert-annotated subsets of 156 protein-enzyme papers and 164 ribozyme papers.",
            "law_type": "Extraction of empirical quantitative measurements (kinetic parameters) from literature: Km, kcat, kcat/Km, kobs, kcleave (empirical constants rather than derived theoretical laws).",
            "law_examples": "Extracted numeric kinetic constants such as Km (standardized to mM), kcat (s^-1), kcat/Km (s^-1 mM^-1), kobs, and kcleave; no new closed-form physical laws were derived—rather quantitative experimental constants were mined from text and tables.",
            "extraction_method": "OCR of PDFs -&gt; engineered prompting of multiple LLMs (Claude3.5, gpt-4o, Llama3, Qwen) to output a Markdown-formatted table of fields -&gt; aggregation agent consolidates multi-LLM outputs -&gt; post-processing with Pandas: remove non-tabular content, clean whitespace, parse numeric formats and scientific notation, perform unit conversion and alignment into standardized columns.",
            "validation_approach": "Comparison to a human expert-annotated gold-standard dataset (156 protein papers with 3,563 Km, 3,531 kcat, 3,751 kcat/Km entries; 164 ribozyme papers for ribozyme metrics) and comparison to BRENDA (human-curated database) for protein enzymes. Metrics computed paper-wise and aggregated (macro and micro averages).",
            "performance_metrics": "Reported precision, recall, and F1 computed on kinetic constants (value + unit). For protein enzyme expert subset: Claude3.5 mean F1 = 0.90 (median 0.99); BRENDA mean F1 = 0.76 (median 0.78). For ribozyme subset: Claude3.5 mean F1 = 0.75. Aggregation agent (ensemble) improved overall performance and reduced bad cases for protein enzymes; aggregation results vary depending on which model outputs are combined.",
            "success_rate": "For protein-enzyme annotated subset, top LLM (Claude3.5) achieved mean F1 ≈ 90% (interpretable as ~90% balanced correctness); for ribozyme annotated subset Claude3.5 mean F1 ≈ 75%. The full archive contains 88,770 protein-enzyme entries and 2,420 ribozyme entries extracted by the pipeline.",
            "challenges_limitations": "Challenges include OCR errors from complex tables and mathematical content, long-context reasoning requirements (average token count ≈25k per paper for benchmark), unit/format variability requiring robust numerical cleaning, inter-model variability (different LLMs have complementary strengths and weaknesses), ensemble degradation if poor-performing models are included, and lower performance when migrating to a new domain (ribozyme) without task-specific adaptation. Local GPU limits constrained some models' context windows (Llama3 limited locally).",
            "comparison_baseline": "Compared to human-curated BRENDA (baseline), the LLM-based workflow achieved higher mean F1 on the annotated subset (LLM single models and aggregation outperformed BRENDA). The paper also benchmarks multiple LLMs against each other (Claude3.5, gpt-4o, Llama3, Qwen) and evaluates different OCR tools (Mathpix vs PyMuPDF).",
            "uuid": "e4243.0",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Aggregation agent",
            "name_full": "LLM-based aggregation agent for consolidating multi-model outputs",
            "brief_description": "A meta-agent prompting a high-performing LLM (Claude3.5) with the original paper text plus the outputs of multiple LLMs (Claude3.5, gpt-4o, Llama3, Qwen) to produce a consolidated, higher-confidence extraction result.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Aggregation agent (LLM ensemble consolidator)",
            "system_description": "Takes as input the paper text and the extraction outputs from multiple LLMs; prompts Claude3.5 to reconcile differences, leverage complementary strengths of different LLMs, and produce a single consolidated table of extracted kinetic entries. The aggregation prompt and instructions are engineered (provided in Supplementary Note 4). The approach mirrors ensemble learning but uses prompting rather than algorithmic voting; in some configurations the aggregator improved F1 and reduced erroneous cases, while inclusion of poorly performing LLMs can degrade aggregation quality.",
            "model_name": "Claude3.5 (used as aggregator in described experiments)",
            "model_size": null,
            "scientific_domain": "biology / biochemistry (enzyme and ribozyme kinetic literature)",
            "number_of_papers": "Applied on same corpora: 3,435 protein enzyme papers and 164 ribozyme papers; evaluated on annotated subsets (156 protein, 164 ribozyme).",
            "law_type": "Consolidation of empirical quantitative measurements (kinetic parameters) extracted from multiple models into a single high-confidence set.",
            "law_examples": "Consolidated numeric entries for Km, kcat, kcat/Km, kobs, kcleave with reconciled units and numerical formatting.",
            "extraction_method": "Post-hoc LLM-based consolidation: feed multiple LLM outputs and source text into one LLM (Claude3.5) with a prompt instructing reconciliation and selection of highest-confidence values, then post-process the consolidated output.",
            "validation_approach": "Compared aggregated outputs to expert-annotated gold dataset and to individual LLM outputs; assessed via paper-wise precision/recall/F1. Also qualitatively inspected reduction in 'bad cases' after aggregation.",
            "performance_metrics": "Aggregation (Claude3.5 consolidating outputs from multiple LLMs) achieved improved paper-wise F1 distributions for protein enzymes versus single models in many cases (figures show reduced bad cases); exact aggregated mean F1 numbers are reported qualitatively in the text (improvement over single-model baselines for protein enzymes). For ribozymes, naive aggregation of all four models did not beat Claude3.5 alone; selective aggregation (Claude3.5 + Llama3) performed better.",
            "success_rate": "Aggregation increased overall extraction quality for protein enzymes relative to single models (qualitative improvement and fewer failure cases); for ribozymes success depended on which models were included (selective aggregation worked best).",
            "challenges_limitations": "Including outputs from poorly performing models in the aggregator can worsen results; aggregator effectiveness depends on constituent model performance and requires prompts that guide model voting/selection. No automated model-voting/filtering was implemented—paper suggests need for smarter ensemble that can vote out poor performers.",
            "comparison_baseline": "Compared against single-model outputs (Claude3.5, gpt-4o, Llama3, Qwen) and against BRENDA; aggregation often outperformed single models for protein enzymes but failed to outdo best single model in some ribozyme aggregations when low-quality models were included.",
            "uuid": "e4243.1",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Claude3.5",
            "name_full": "Claude-3.5-sonnet-20240620",
            "brief_description": "A closed-source large language model used in this work as the top-performing extractor and, in some experiments, as the aggregation agent; achieved the highest mean F1 on protein enzyme extraction and strong performance on ribozyme extraction.",
            "citation_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
            "mention_or_use": "use",
            "system_name": "Claude3.5 (used both as extractor and as aggregation agent)",
            "system_description": "Prompted with engineered instructions (task description, field list, format reference) to extract tabular kinetic entries from OCRed text; also used to aggregate multiple LLM outputs by reconciling differences and consolidating a single high-confidence output. Demonstrated robust long-context handling (8192-token output capacity noted) and consistent performance across temperature settings.",
            "model_name": "Claude-3.5-sonnet-20240620",
            "model_size": null,
            "scientific_domain": "biology / biochemistry (enzyme kinetics; ribozyme kinetics)",
            "number_of_papers": "Used to extract from 3,435 protein enzyme papers and 164 ribozyme papers; evaluated on annotated subsets (156 protein, 164 ribozyme).",
            "law_type": "Extraction of empirical quantitative kinetic parameters (Km, kcat, kcat/Km, kobs, kcleave).",
            "law_examples": "Extracted numeric constants such as Km (converted to mM), kcat (s^-1), and kcat/Km (s^-1 mM^-1) from both text and tables.",
            "extraction_method": "Direct prompting with an engineered Markdown-format output reference on OCRed paper text; outputs converted to a 13-column table delimited by pipes then post-processed for cleaning and unit conversion.",
            "validation_approach": "Performance compared against human expert annotations (gold dataset) and BRENDA; metrics computed paper-wise and aggregated.",
            "performance_metrics": "Protein enzyme annotated subset: mean F1 = 0.90, median F1 = 0.99 (25%/75% percentiles = 0.90/1.00). Ribozyme annotated subset: mean F1 = 0.75. Claude3.5 exhibited consistent performance across temperature settings.",
            "success_rate": "Approximately 90% mean F1 on protein enzyme gold subset and ~75% mean F1 on ribozyme subset (interpretable as high overall correctness on extracted kinetic constants).",
            "challenges_limitations": "Although top-performing overall, Claude3.5 had specific failure modes on certain challenging cases (Supplementary Note 1) which other LLMs sometimes handled better; requires good OCR input and careful prompt engineering to achieve high accuracy.",
            "comparison_baseline": "Outperformed other tested LLMs (gpt-4o, Llama3, Qwen) on protein enzyme extraction and outperformed BRENDA on the annotated subset; aggregation combining models could further improve results in some settings.",
            "uuid": "e4243.2",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "gpt-4o",
            "name_full": "gpt-4o",
            "brief_description": "A closed-source LLM (GPT-4o) evaluated as one of several extractors in the pipeline; demonstrated lower mean/median F1 compared to Claude3.5 and contributed complementary strengths in some cases.",
            "citation_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
            "mention_or_use": "use",
            "system_name": "gpt-4o (used as one of multiple extractors)",
            "system_description": "Queried with the same engineered prompt as other LLMs to extract tabular kinetic information from OCRed PDFs; its outputs were included among multiple LLM runs that the aggregation agent could reconcile.",
            "model_name": "gpt-4o",
            "model_size": null,
            "scientific_domain": "biology / biochemistry (enzyme and ribozyme literature)",
            "number_of_papers": "Applied across the same corpora (3,435 protein enzyme papers and 164 ribozyme papers); evaluated on annotated subsets.",
            "law_type": "Extraction of empirical quantitative kinetic parameters (Km, kcat, kcat/Km, etc.).",
            "law_examples": "Provided extracted numeric kinetic constants from text and tables; specific examples not individually enumerated in paper beyond aggregate metrics.",
            "extraction_method": "Prompted extraction on OCRed text using the engineered Markdown-format prompt; outputs post-processed and optionally aggregated.",
            "validation_approach": "Compared to expert annotations and to other LLMs; performance summarized by paper-wise precision/recall/F1.",
            "performance_metrics": "Reported as lower mean F1 than Claude3.5 on protein enzymes and ribozymes; for ribozymes gpt-4o had mean F1 below 0.6 (paper states gpt-4o and Qwen mean F1 &lt; 0.6 in ribozyme experiments).",
            "success_rate": "Lower than Claude3.5 on benchmarked tasks; provided useful complementary strengths for some challenging extraction cases but overall inferior mean F1 on the reported datasets.",
            "challenges_limitations": "Lower average extraction F1 on the ribozyme set; limited output capacity noted (4096-token output capacity) which may constrain processing of very long-context papers.",
            "comparison_baseline": "Benchmarked against Claude3.5, Llama3, Qwen and BRENDA; ranked below Claude3.5 on mean F1.",
            "uuid": "e4243.3",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Llama3",
            "name_full": "Llama3 (locally deployed)",
            "brief_description": "An open-source LLM locally deployed and evaluated as one of the extractors; displayed complementary strengths on certain cases and was used in selective aggregation that improved ribozyme extraction performance.",
            "citation_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
            "mention_or_use": "use",
            "system_name": "Llama3 (local deployment)",
            "system_description": "Locally deployed Llama3 used with the engineered extraction prompt on OCRed literature to output structured tables of kinetic constants; limited by local GPU resources to a maximum input of 32k tokens in these experiments.",
            "model_name": "Llama3",
            "model_size": null,
            "scientific_domain": "biology / biochemistry (enzyme/ribozyme literature)",
            "number_of_papers": "Applied to the same corpora (3,435 protein enzyme papers and 164 ribozyme papers) and evaluated on annotated subsets.",
            "law_type": "Extraction of empirical kinetic parameters (Km, kcat, kcat/Km, etc.).",
            "law_examples": "Extracted numeric kinetic values from text and tables (examples aggregated into the released archive); specific equation-level discoveries not reported.",
            "extraction_method": "Prompted extraction with the engineered prompt; outputs included in aggregation experiments (particularly successful when combined with Claude3.5 for ribozyme extraction).",
            "validation_approach": "Compared against human expert annotations and other LLMs with precision/recall/F1 metrics.",
            "performance_metrics": "Mean/median F1 lower than Claude3.5 on protein enzymes; combining Llama3 with Claude3.5 achieved superior aggregation performance for ribozyme extraction versus aggregating all four models.",
            "success_rate": "Variable; contributed complementary extractions that aided selective aggregation, especially in ribozyme domain.",
            "challenges_limitations": "Local GPU resource limitations constrained the input context; overall mean F1 below the top-performing model for some tasks.",
            "comparison_baseline": "Benchmarked against Claude3.5, gpt-4o, Qwen and BRENDA. Selective ensemble (Claude3.5 + Llama3) outperformed naive 4-model aggregation for ribozyme extraction.",
            "uuid": "e4243.4",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen",
            "name_full": "Qwen-plus-0806",
            "brief_description": "An open-source/Alibaba-provided LLM evaluated in the extraction pipeline; lower mean F1 on ribozyme extraction and contributed complementary strengths in select cases.",
            "citation_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
            "mention_or_use": "use",
            "system_name": "Qwen-plus-0806",
            "system_description": "Queried with the same engineered prompt to extract table-formatted kinetic entries from OCRed text; included among multiple LLM outputs provided to the aggregation agent.",
            "model_name": "Qwen-plus-0806",
            "model_size": null,
            "scientific_domain": "biology / biochemistry (enzyme/ribozyme literature)",
            "number_of_papers": "Applied to 3,435 protein enzyme papers and 164 ribozyme papers; evaluated on annotated subsets.",
            "law_type": "Extraction of empirical kinetic parameters (Km, kcat, kcat/Km, etc.).",
            "law_examples": "Produced extracted numeric kinetic constants included in the released archive; specific example values not individually highlighted beyond aggregate archive counts.",
            "extraction_method": "Engineered prompt on OCRed text -&gt; model outputs parsed into a Markdown table -&gt; post-processed and optionally aggregated.",
            "validation_approach": "Compared to expert annotations and other LLM outputs using precision/recall/F1.",
            "performance_metrics": "Reported mean F1 below 0.6 for Qwen on the ribozyme dataset (paper states gpt-4o and Qwen with mean F1 scores below 0.6 in ribozyme experiments); lower than Claude3.5 on protein enzymes as well.",
            "success_rate": "Lower extraction quality on ribozyme tasks; contributed some complementary cases but generally underperformed the top model.",
            "challenges_limitations": "Lower mean F1 on ribozyme domain; limited output capacity noted (8000-token output capacity).",
            "comparison_baseline": "Benchmarked versus Claude3.5, gpt-4o, Llama3 and BRENDA; inclusion in naive aggregation reduced aggregate quality in some ribozyme experiments.",
            "uuid": "e4243.5",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "OpenAI 'deep research' (mentioned)",
            "name_full": "OpenAI 'deep research' (multi-step internet research tool)",
            "brief_description": "Mentioned as an example of an AI system that can conduct multi-step internet research to assist scientists with literature reviews and identifying knowledge gaps; referenced in related-work context rather than used in experiments.",
            "citation_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
            "mention_or_use": "mention",
            "system_name": "OpenAI 'deep research' (referenced)",
            "system_description": "Cited in the introduction as a tool that can perform multi-step research on the internet to assist scientists in writing literature reviews and identifying knowledge gaps; not used in this paper's experiments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general scientific literature / research assistance (broadly applicable)",
            "number_of_papers": null,
            "law_type": "Mentioned as a tool for multi-step literature-level research rather than explicit quantitative law extraction in this paper.",
            "law_examples": "",
            "extraction_method": "Mentioned as capable of multi-step internet research and literature synthesis; no methodological details provided in this paper.",
            "validation_approach": "",
            "performance_metrics": "",
            "success_rate": "",
            "challenges_limitations": "Mentioned at high level in related work; no experimental evaluation or limitations discussed within this paper.",
            "comparison_baseline": "Mentioned alongside other AI co-scientist efforts (e.g., Google 'AI co-scientist') as part of recent AGI advances in literature mining.",
            "uuid": "e4243.6",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Google 'AI co-scientist' (mentioned)",
            "name_full": "Google 'AI co-scientist'",
            "brief_description": "Referenced as an example of a virtual scientific collaborator intended to help scientists generate hypotheses and accelerate discovery; cited in related-work context, not used experimentally.",
            "citation_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
            "mention_or_use": "mention",
            "system_name": "Google 'AI co-scientist' (referenced)",
            "system_description": "Cited in the introduction as an example of an AI system marketed as a virtual scientific collaborator to help generate novel hypotheses and research proposals; not used in this work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general scientific research assistance / hypothesis generation",
            "number_of_papers": null,
            "law_type": "Mentioned as an AI collaborator for research acceleration, not described as directly extracting quantitative laws in this paper.",
            "law_examples": "",
            "extraction_method": "",
            "validation_approach": "",
            "performance_metrics": "",
            "success_rate": "",
            "challenges_limitations": "High-level mention only; no experimental details provided in this paper.",
            "comparison_baseline": "",
            "uuid": "e4243.7",
            "source_info": {
                "paper_title": "Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from scientific text with large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_scientific_text_with_large_language_models"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge",
            "rating": 1,
            "sanitized_title": "pubtator_30_an_aipowered_literature_resource_for_unlocking_biomedical_knowledge"
        }
    ],
    "cost": 0.015436,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature</p>
<p>Jinling Jiang 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Jie Hu 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Siwei Xie 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Menghao Guo 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Yuhang Dong 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Shuai Fu 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Xianyue Jiang 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Zhenlei Yue 
Junchao Shi 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Xiaoyu Zhang 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Minghui Song 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Guangyong Chen 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Hua Lu 
Department of Computer Science
Aalborg University
Denmark</p>
<p>Xindong Wu 
Key Laboratory of Knowledge Engineering with Big Data (The Ministry of Education of China)
Hefei University of Technology
HefeiAnhuiChina</p>
<p>Pei Guo guopei@ibmc.ac.cn 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Da Han dahan@sjtu.edu.cn 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Zeyi Sun sunzey6@gmail.com 
Zhejiang Lab
HangzhouZhejiangChina</p>
<p>Jiezhong Qiu jiezhongqiu@zju.edu.cn 
Hangzhou Institute of Medicine Chinese Academy of Sciences
HangzhouZhejiangChina</p>
<p>Enzyme Co-Scientist: Harnessing Large Language Models for Enzyme Kinetic Data Extraction from Literature
51E00E249868E257D972F5B9E81396F9
The extraction of molecular annotations from scientific literature is critical for advancing datadriven research.However, traditional methods, which primarily rely on human curation, are labor-intensive and error-prone.Here, we present an LLM-based agentic workflow that enables automatic and efficient data extraction from literature with high accuracy.As a demonstration, our workflow successfully delivers a dataset containing over 91,000 enzyme kinetics entries from around 3,500 papers.It achieves an average F1 score above 0.9 on expert-annotated subsets of protein enzymes and can be extended to the ribozyme domain in fewer than 3 days at less than $90.This method opens up new avenues for accelerating the pace of scientific research.</p>
<p>Main Text</p>
<p>In scientific research, acquiring a vast amount of annotations or "labels" for functional molecules enables the identification of subtle patterns and the development of predictive models.Increasing availability of human-curated scientific datasets facilitates the integration of interdisciplinary insights, driving innovation and accelerating discovery in multiple fields including chemistry [1], biology [2][3][4][5], and material science [6].By the year 2025, the Nucleic Acids Research (NAR) Database issue has collected up to 2236 databases [7], among which many are constructed through human-curated literature mining such as ChEMBL [8] and BRENDA [9].Robust data-driven research will foster more efficient scientific progress, but traditional methods by human annotation and classical natural language processing tools are labor-intensive, time-consuming, and error-prone [10].Therefore, there is an urgent need for paradigm-shifting methods capable of rapidly and accurately extracting annotations from literature.</p>
<p>Recent advancements in Artificial General Intelligence (AGI), particularly Large Language Models (LLMs), have demonstrated superior performance in scientific literature mining such as named entity recognition, relation extraction, summarization, integration and even planning [5,[11][12][13][14][15][16].Most recently, OpenAI has unveiled 'deep research' that can conduct multi-step research on the internet for complex tasks to assist scientists to write literature reviews and even identify knowledge gaps [17], and Google has launched 'AI co-scientist' which is "a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals and to accelerate the clock speed of scientific and biomedical discoveries" [18].</p>
<p>Nevertheless, extracting data-especially quantitative annotations-from literature that contains both tables and text remains challenging.This task requires comprehensive longcontext capabilities, including long-context retrieval and reasoning, tabular data understanding, and basic math (such as scientific notation understanding).Therefore, mining quantitative data from literature naturally serves as a real-world, large-scale benchmark for evaluating the longcontext capabilities of LLMs.This is particularly significant given that most existing longcontext LLM benchmarks [19,20] rely on synthetic data or costly human annotations.</p>
<p>To address the aforementioned challenges, we have developed an agentic workflow for automated extraction, normalization, and standardization of scientific data from the literature (Fig. 1).The agentic workflow starts with collecting a full dataset of relevant literature in PDF format, which are then transformed to machine-readable text by optical character recognition (OCR).Then the text, together with an engineered prompt, is used to query LLMs to extract data of interest.The outputs of multiple LLMs are then considerably aggregated to provide comprehensive and high-confidence results.To evaluate and further optimize our workflow, we manually annotate a small portion of the full dataset (termed as the annotated dataset) to thoroughly evaluate each individual component of the workflow, including the prompt, choice of LLMs, and aggregation strategy.</p>
<p>First, the prompt is carefully engineered to include task background, field description, output format instruction, etc, which are essential for LLMs to process desired information with high accuracy.Second, numerous LLMs have been developed, each with its own strengths and weaknesses.Benchmarking different LLMs allows for a comprehensive evaluation of their capabilities across various dimensions, including long-context reasoning, information retrieval, coreference resolution, unit conversion, and table understanding.Finally, an LLM aggregator is designed to take the strengths of high-performance LLMs towards higher confidence results.</p>
<p>We first apply our workflow to extract kinetic data of protein enzymes, which is an important domain in the field of biochemistry with well-established databases such as BRENDA [9].This allows us to thoroughly assess the performance of our workflow compared to existing humancurated databases.Following the workflow in Fig. 1, we extract three types of kinetic parameters, including Km, kcat and kcat/Km, from 3,435 papers curated in BRENDA.</p>
<p>To rigorously evaluate extraction performance, we randomly select 156 papers and manually annotate them with a team of experts, resulting in an annotated dataset that contains 3,563 Km, 3,531 kcat, and 3,751 kcat/Km entries.The extracted results are compared to the expert annotation to calculate metrics including paper-wise precision, recall, and F1-score (Methods, Supplementary Table 1).We evaluate four popular LLMs, including two closed-source ones (Claude3.5 and gpt-4o) and two open-source ones (Llama3 and Qwen), with our engineered prompt (Fig. 2a, d).Claude3.5 is identified as the top performer in terms of F1-score distribution (mean=0.90,median=0.99,25%/75% percentile=0.90/1.00)and exhibits more consistent performance across different temperature settings (Supplementary Fig. 1).However, we observe that, despite their lower mean and median F1 scores, the other three LLMs demonstrate distinct strengths in resolving certain challenging cases encountered by Claude3.5 (Examples of these challenging cases are discussed in Supplementary Note 1).Inspired by ensemble learning, we develop an aggregation agent to leverage the strengths of individual LLMs.In particular, we prompt Claude3.5 with the paper text, along with the outputs from Claude3.5, gpt-4o, Llama3, and Qwen, and then instruct it to consolidate the insights from these LLMs (Fig. 2e).Encouragingly, the aggregation agent is indeed capable of integrating the advantages of multiple LLMs and achieves a comprehensively enhanced performance, especially in reducing bad cases (Fig. 2b).Finally, we investigate whether LLM could surpass human intelligence in literature mining by comparing results produced by LLMs with data curated in BRENDA (Methods).For the 156 papers in our annotated dataset, BRENDA achieves suboptimal F1 scores (Fig. 2c, mean=0.76,median=0.78,25%/75% percentile=0.64/0.95)compared to any single LLM, or the advanced aggregation agent.We also validate the performance of our pipeline on the full dataset, comprising 3,435 papers, which shows that our pipeline maintains a quality comparable to that of the annotated dataset (Supplementary Note 2).As an important complement to enzymes, certain RNA molecules, known as ribozymes, possess catalytic activities and pivotal functions in gene regulation.However, to the best of our knowledge, there is no database service available for ribozyme kinetic data.Therefore, we next apply our workflow to the ribozyme research field where a brand-new database needs to be established from scratch.We collect a total of 164 papers from PubMed and bioRxiv by searching the keywords 'ribozymes and (kcat or Km or kcat/Km or kobs or kcleave)', which can be approximately considered as the full dataset of ribozyme kinetic literature.We apply our pipeline to extract five kinetic parameters, including kcat, Km, kcat/Km, kobs, and kcleave, and compare the extracted results with expert annotation (Supplementary Table 2).Again, Claude3.5 shows an exceptional performance with the highest mean F1 score of 0.75 (Fig. 3a).</p>
<p>An aggregation agent that integrates these four LLMs does not perform better than Claude3.5, which is probably lagged by the poorer performances of the other three LLMs, especially gpt-4o and Qwen with mean F1 scores below 0.6.However, integrating Claude3.5 and Llama3 can achieve superior performance (Fig. 3b), which suggests the need for a smarter aggregation agent that should automatically vote out poorly performed LLMs during ensemble.Remarkably, our pipeline is economical and time-saving, i.e., it takes less than 90$ (Fig. 3c) to deliver the final extraction results within 3 days (2 days for literature collection and 1 day for running OCR software and LLMs).This demonstrates the efficiency of our workflow in acquiring knowledge for a specific research field with significantly improved data quality and reduced labor cost.To the end, we release a new archive 1 that offers a structured and accessible collection of 1 https://huggingface.co/datasets/jackkuo/LLM-Enzyme-Kinetics-Archive-LLENKA enzyme kinetic data, including 88,770 entries from 3,435 protein enzyme papers and 2,420 entries from 164 ribozyme papers (Supplementary Fig. 2).The archive contains kinetic parameters with their corresponding enzyme names, mutants, organisms, substrates, and experimental conditions, offering a valuable resource to the field of biochemistry.In addition, this archive serves as a real-world large-scale long-context LLM benchmark, with an average token count of 25k (385 papers containing over 32k tokens), allowing us to assess LLMs' capabilities in understanding long scientific literature.</p>
<p>In sum, we have developed an LLM-based agentic workflow for automatic data extraction from scientific literature, providing a streamlined pipeline that operates without strong assumptions or prior knowledge for the domain of interest.By demonstrating the high accuracy and efficiency of our workflow in the domains of protein enzyme and ribozyme, we anticipate it can also be robustly adapted to other research fields.This work validates and benchmarks the superiority of LLMs in literature mining compared to human efforts.In the future, we aim to refine the enzyme archive to facilitate new discoveries and further extend our workflow to other fields towards a versatile paradigm and a real LLM benchmark in scientific research.</p>
<p>Methods</p>
<p>Literature Collection</p>
<p>The papers about protein enzyme are collected from BRENDA whose core information is manually extracted from original scientific literature from PubMed and Scopus.We get a collection of PubMed Identifier (PMID) from BRENDA database through the BRENDA SOAP (Simple Object Access Protocol) services.The number of PMIDs we collect from BRENDA is more than 90,000 among which 4,925 papers have recorded the experimental results of enzyme kinetics parameters.We finally download 3,435 papers in PDF format by PMIDs as we do not have access to the remaining 1,490 ones.For these papers, we implement a crawler program to download their corresponding enzyme kinetic data from BRENDA.</p>
<p>As for the ribozyme, the 164 papers are collected from PubMed and bioRxiv by searching the keywords 'ribozymes and (kcat or Km or kcat/Km or kobs or kcleave)'.</p>
<p>Construction Expert-Annotated Datasets</p>
<p>We engage a team of 10 highly qualified experts to meticulously annotate, with 6 of them holding PhD degrees in relevant fields and the remaining 4 pursuing their PhDs in related disciplines.Through the annotation process, we ensure precision at every step, employing rigorous validation and repeated confirmations.We designate this as our gold standard for benchmarking, which we believe is exceptionally close to the true values.The final annotated dataset is structured as a wide table with aligned fields, including enzyme, mutant, organism, substrate, experimental conditions (such as temperatures and pHs), and kinetic data fields.</p>
<p>OCR Pre-Processing</p>
<p>OCR is performed to convert PDF format to text format while preserving mathematical content and complex tables.We compare two OCR approaches: Mathpix and PyMuPDF, where we use Claude3.5 as the default LLM to extract protein enzyme data from the annotated datasets.As shown in Supplementary Fig. 3, Mathpix achieves a similar median F1 score compared to PyMuPDF (0.99 vs 1.00) but demonstrated a slightly better mean F1 score (0.90 vs 0.87).</p>
<p>Therefore, we select Mathpix as the default OCR software in our pipeline.</p>
<p>Prompt Engineering</p>
<p>The procedure of prompt engineering contains several key steps:</p>
<ol>
<li>Evaluate the prompt using all samples in the annotated dataset.</li>
</ol>
<p>2.</p>
<p>Manually analyze problematic cases from samples with low precision or recall, identifying common errors.</p>
<ol>
<li>For each identified error, we manually update the instruction part of the prompt.</li>
</ol>
<p>The optimized prompt comprises three key components: task description, instructions, and format reference.The task description specifies the fields to be extracted.The instructions integrate essential reminders from the prompt engineering process.The format reference provides a template for the expected output.We selected Markdown as the output format because it avoids issues with separators (e.g., commas in compound names) and enables direct table visualization during prompt engineering, a feature lacking in other formats like JSON.</p>
<p>The final prompt is listed in Supplementary Note 3. The prompt for the aggregation agent is listed in Supplementary Note 4.</p>
<p>Post-Processing</p>
<p>The post-processing program organizes and cleans the outputs of LLMs, converting them into a standardized data frame format.This transformation enables downstream numerical comparison and evaluation.The program utilizes the Pandas library to structure the extracted data, primarily focusing on the outputs that present a table with 13 columns delineated by pipe delimiters.It removes any non-tabular descriptive content, aligns column headers, and implements necessary data-cleaning procedures while maintaining the integrity of the dataset.</p>
<p>The cleaning procedures consists of several critical tasks:</p>
<ol>
<li>
<p>Space cleaning.It removes undesired non-breaking spaces, fills empty entries with NA (Not Applicable) to avoid leaving blank spaces, and eliminates any rows where all entries are marked as NA.</p>
</li>
<li>
<p>Invalid field cleaning.Values under certain columns carrying units corresponding to other parameters will be marked as NA.</p>
</li>
</ol>
<p>Evaluation Metrics</p>
<p>The performance metrics (precision, recall, and F1) are computed based on the kinetic constants (i.e., kcat, Km, kcat/Km, kobs, and kcleave), including both their numerical values and corresponding units.We observe that it is highly unlikely for a paper to contain multiple data entries with identical kinetic constants, even when papers have dozens or hundreds of entries.</p>
<p>In other words, these constants can function as "unique identifiers," reducing the need for</p>
<p>Fig. 1
1
Fig. 1 Schematic of our LLM-based agentic workflow for enzyme kinetic data extraction.</p>
<p>Fig. 2
2
Fig. 2 Protein Enzyme Kinetic Data Extraction.a-c, Violin plots show the distribution of paper-wise F1 scores of each LLM, sorted by mean F1 (a), the aggregation agent that integrates Claude3.5, gpt-4o, Llama3, and Qwen (b), and BRENDA (c) on the expert-annotated dataset (156 papers).Black bar: median.White bar: mean.25% and 75% percentile and whiskers are also plotted.d-e, The engineered prompts for running single LLM (d) and aggregation agent (e).</p>
<p>Fig. 3
3
Fig. 3 Ribozyme kinetic data extraction.a, Violin plots show the distribution of paper-wise F1 scores of each LLM against our expert-annotated dataset sorted by mean F1 (164 ribozyme papers).b, The performance of aggregation agent that integrates four LLMs including Claude3.5, gpt-4o, Llama3, and Qwen (left), and two LLMs including Claude3.5 and Llama3 (right).Black bar: median.White bar: mean.25% and 75% percentile and whiskers are also plotted.c, The per-step cost of our pipeline when processing the 164 ribozyme papers.</p>
<p>3 .
3
Numerical cleaning.It employs regular expressions to identify various representations of entries with scientific notations and/or error metrics.4. Unit conversion.It involves recognizing the extracted units and standardizing the values of Km, kcat, and kcat/Km to mM, s^-1, and s^-1mM^-1, respectively, by applying specified conversion factors.This program transforms raw data extracted from LLM outputs into a clean and standardized format, ensuring data consistency and facilitating comparability for subsequent evaluations and applications.</p>
<p>AcknowledgmentsThis research is supported by the National Natural Science Foundation of China (62120106008, 62306290), the "Pioneer" and "Leading Goose" R&amp;D Program of Zhejiang (2024SSYS0007).We would like to express our gratitude to the Qwen Team at Alibaba for providing access to the free Qwen API.Data AvailabilityOur annotated dataset (156 protein enzyme papers and 164 ribozyme papers) is available at https://huggingface.co/datasets/jackkuo/LLM-Enzyme-Kinetics-Golden-Benchmark.Our released enzyme archive (88,770 entries from 3,435 protein enzyme papers and 2,420 entries from 164 ribozyme papers) is available at https://huggingface.co/datasets/jackkuo/LLM-Enzyme-Kinetics-Archive-LLENKA.Code AvailabilityAll codes used in this study have been deposited on GitHub:https://github.com/JackKuo666/LLM-BioDataExtractoradditional processing after extraction and thereby simplifying the automatic evaluation process.More formally, the paper-wise precision of each paper in the annotated dataset is calculated using the formula: In some rare cases, the calculation of the above metrics can cause an error of division by zero.Take the computation of precision for example, this can happen if the LLM fails to extract any results, which makes the true positive (TP) as well as the false positive (FP) zero.For these special cases, we follow the common strategy:• If the true positive (TP), false positive (FP), and false negative (FN) are all zeros, the precision, recall, and F1 are 1.This means that the LLM correctly knows "it doesn't know" when there is indeed nothing to be extracted.• If true positives (TP) is zero and at least one of the two others (i.e., FP and FN) is not, the precision, recall and F1 are all zeros.Supplementary Tables1 and 2show the average metrics across all protein enzyme papers and ribozyme ones, respectively.We consider two ways to average ---macro and micro.The macro-averaged metrics (or macro metrics) are computed by taking the arithmetic mean (a.k.a.unweighted mean) of all the paper-wise metrics.For micro metrics, the counters (i.e., true positive, false positive, false negative) of all papers are summed up, which are then used to calculate a single micro precision/recall/F1 value.However, the above metrics may not be directly applicable when comparing to data entries recorded in BRENDA, as BRENDA involves manual extraction where different levels of rounding are applied.For example, a Km value of 0.835 might be rounded to 0.8 or 0.84 in BRENDA.Therefore, when comparing with BRENDA, we consider an extraction to be correct if rounding to any position between the 1st and 6th decimal place results in an exactly matched value.Competing interestsThe authors declare no competing interests.
Version of LLMs: Claude-3.5-sonnet-20240620, Qwen-plus-0806, gpt-4o. September 23, 2024Llama-3</p>
<p>Llama3 is locally deployed, while the other LLMs were used through online APIs. </p>
<p>The maximum outputs of different LLMs vary, which is discussed in our paper: gpt-4o's output capacity is 4096 tokens; Claude3.5's output capacity is 8192 tokens; Qwen-Plus's output capacity is 8000 tokens. and Llama3's output capacity is 4096 tokens</p>
<p>Due to local GPU resource limitations, Llama3 used a maximum input of 32k tokens. References. </p>
<p>Sequential closed-loop Bayesian optimization as a guide for organic molecular metallophotocatalyst formulation discovery. X Li, Che Y Chen, L Liu, T Wang, K Liu, L Yang, H Pyzer-Knapp, E O Cooper, A I , 10.1038/s41557-024-01546-5Nat Chem. 1682024 Aug</p>
<p>Accurately predicting enzyme functions through geometric graph learning on ESMFold-predicted structures. Y Song, Q Yuan, S Chen, Y Zeng, H Zhao, Y Yang, 10.1038/s41467-024-52533-wNat Commun. 15181802024 Sep 18</p>
<p>UniKP: a unified framework for the prediction of enzyme kinetic parameters. H Yu, H Deng, J He, J D Keasling, X Luo, 10.1038/s41467-023-44113-1Nat Commun. 14182112023 Dec 11</p>
<p>Deep learning-based kcat prediction enables improved enzyme-constrained model reconstruction. F Li, L Yuan, H Lu, Nat Catal. 52022</p>
<p>Evaluation of large language models for discovery of gene set function. M Hu, S Alkhairy, I Lee, R T Pillich, D Fong, K Smith, R Bachelder, T Ideker, D Pratt, Nat Methods. 2025</p>
<p>. 10.1038/s41592-024-02525-xJan22</p>
<p>Nature of metal-support interaction for metal catalysts on oxide supports. T Wang, J Hu, R Ouyang, Y Wang, Y Huang, S Hu, W X Li, 10.1126/science.adp6034Science. 38667242024 Nov 22</p>
<p>The 2025 Nucleic Acids Research database issue and the online molecular biology database collection. D J Rigden, X M Fernández, 10.1093/nar/gkae1220Nucleic Acids Res. 53D12025 Jan 6</p>
<p>Activity, assay and target data curation and quality in the ChEMBL database. G Papadatos, A Gaulton, A Hersey, J P Overington, 10.1007/s10822-015-9860-5J Comput Aided Mol Des. 2992015 Sep</p>
<p>A Chang, L Jeske, S Ulbrich, J Hofmann, J Koblitz, I Schomburg, M Neumann-Schaal, D Jahn, D Schomburg, Brenda, 10.1093/nar/gkaa1025ELIXIR core data resource in 2021: new developments and updates. 2021 Jan 849</p>
<p>Enzyme Databases in the Era of Omics and Artificial Intelligence. U Prešern, M Goličnik, 10.3390/ijms242316918Int J Mol Sci. 2423169182023 Nov 29</p>
<p>PubMed and beyond: biomedical literature search in the age of artificial intelligence. Q Jin, R Leaman, Z Lu, 10.1016/j.ebiom.2024.104988EBioMedicine. 1001049882024 Feb</p>
<p>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge. C H Wei, A Allot, P T Lai, R Leaman, S Tian, L Luo, Jin Q Wang, Z Chen, Q Lu, Z , 10.1093/nar/gkae235Nucleic Acids Res. 52W12024 Jul 5</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, A Dunn, S Lee, N Walker, A S Rosen, G Ceder, K A Persson, A Jain, 10.1038/s41467-024-45563-xNat Commun. 15114182024 Feb 15</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.1038/s41467-024-45914-8Nat Commun. 15115692024 Feb 21</p>
<p>Closing the gap between open-source and commercial large language models for medical evidence summarization. G Zhang, Jin Q Zhou, Y Wang, S Idnay, B R Luo, Y Park, E Nestor, J G Spotnitz, M E Soroush, A Campion, T Lu, Z Weng, C Peng, Y , Preprint</p>
<p>Update in: NPJ Digit Med. 10.1038/s41746-024-01239-warXiv:2408.00588v12024 Jul 25. 2024 Sep 97239</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 62479922023 Dec</p>
<p>OpenAI's 'deep research' tool: is it useful for scientists? Nature. N Jones, 10.1038/d41586-025-00377-92025 Feb 6</p>
<p>. J Gottweis, W Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, K Saab, D Popovici, J Blum, F Zhang, K Chou, A Hassidim, B Gokturk, A Vahdat, P Kohli, Y Matias, A Carroll, K Kulkarni, N Tomašev, Y Guan, V Dhillon, E D Vaishnav, B Lee, T R Costa, J R Penad'es, G Peltz, Y Xu, A Pawlosky, A Karthikesalingam, V Natarajan, 2025Towards an AI co-scientist</p>
<p>RULER: What's the Real Context Size of Your Long-Context Language Models. C P Hsieh, S Sun, S Kriman, S Acharya, D Rekesh, F Jia, B Ginsburg, First Conference on Language Modeling. </p>
<p>ınftyBench: Extending Long Context Evaluation Beyond 100K Tokens. X Zhang, Y Chen, S Hu, Z Xu, J Chen, M K Hao, X Han, Z L Thai, S Wang, Z Liu, M Sun, ACL. 2024. January.</p>            </div>
        </div>

    </div>
</body>
</html>
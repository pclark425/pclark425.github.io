<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5752 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5752</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5752</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-5f5b855f58da599c2730120e55d8320fac867279</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5f5b855f58da599c2730120e55d8320fac867279" target="_blank">Leveraging Large Language Models for Robot 3D Scene Understanding</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the use of large language models to impart common sense for scene understanding by introducing three paradigms for leveraging language for classifying rooms in indoor environments based on their contained objects and demonstrating notable zero-shot generalization and transfer capabilities stemming from their use of language.</p>
                <p><strong>Paper Abstract:</strong> Semantic 3D scene understanding is a problem of critical importance in robotics. While significant advances have been made in spatial perception, robots are still far from having the common-sense knowledge about household objects and locations of an average human. We thus investigate the use of large language models to impart common sense for scene understanding. Specifically, we introduce three paradigms for leveraging language for classifying rooms in indoor environments based on their contained objects: (i) a zero-shot approach, (ii) a feed-forward classifier approach, and (iii) a contrastive classifier approach. These methods operate on 3D scene graphs produced by modern spatial perception systems. We then analyze each approach, demonstrating notable zero-shot generalization and transfer capabilities stemming from their use of language. Finally, we show these approaches also apply to inferring building labels from contained rooms and demonstrate our zero-shot approach on a real environment. All code can be found at https://github.com/MIT-SPARK/llm_scene_understanding.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5752.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5752.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structured-language (T5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured-language serialized-input approach (fine-tuned T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that serializes room-level structured/tabular information (room bounding-box dimensions and per-object label + 3D position) into a string and fine-tunes a text-to-text LM to decode a probability distribution over room labels; used to classify rooms from scene-graph data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (pretrained text-to-text transformer, fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained encoder-decoder text-to-text transformer (T5 family) fine-tuned to encode serialized structured-room strings and decode room-label probabilities; used in the paper as the LM backbone for structured-string classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Not used for anomaly detection in the paper; used as a supervised classifier on serialized structured/tabular-like inputs (fine-tuning). (Method could be adapted for anomaly detection by using model likelihoods / decoded label probabilities or by fine-tuning a head to detect anomalous records.)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Serialized structured data / tabular-like strings (room bounding-box dimensions + object label, count, and relative positions per object).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>not applicable / not evaluated (paper does not evaluate anomaly detection)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Scene graphs derived from Matterport3D (≈1870 rooms; object label spaces: mpcat40 and nyuClass)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated as a room-classifier on the Matterport3D-derived scene-graph dataset using accuracy (test-split). Structured-language trials achieved test accuracies in the range ~63.7%–69.1% (best: 69.1% test-split in nyuClass ground-truth co-occurrence condition).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Structured-language approach outperformed or matched other language-only methods (zero-shot, embedding-based) and beat simple statistical baselines in many conditions; comparable or better than vision-only baselines in aggregate (see paper Tables 1–2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated for anomaly detection. Limitations reported: compute/token limits (had to omit rooms with >100 objects and round values), some geometric/spatial info (e.g., object pose) omitted due to token/memory constraints, modest gains when ablating bounding-box dims but more drop when removing object positions (object positions contributed to performance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Robot 3D Scene Understanding', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5752.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5752.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding-based (RoBERTa + MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-based LM approach using RoBERTa-large embeddings with a shallow MLP head</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Project natural-language queries summarizing the k most informative objects in a room into RoBERTa-large embeddings and train a shallow classifier (MLP) on those embeddings to predict room labels; used as a supervised method that leverages LM embeddings of object lists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (LM embedder) + shallow MLP classifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa-large (masked-language transformer-based encoder used to embed short natural-language query strings like 'This room contains o1, o2, and o3.') The embeddings are fed into a small multi-layer perceptron trained with cross-entropy to predict room labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Not used for anomaly detection in the paper; used for supervised classification from object-list queries (fine-tuning classifier head on LM embeddings). Could be repurposed for anomaly detection by training the head to detect 'anomalous' embeddings or using distance/uncertainty in embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Lists of object labels (informative subset of objects per room) converted to short natural-language strings (i.e., unstructured lists expressed as text).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>not applicable / not evaluated (paper does not evaluate anomaly detection)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Scene graphs derived from Matterport3D (nyuClass and mpcat40 label spaces), same splits as structured-language experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Room-classification accuracy (test-split). Embedding-based approach achieved test accuracies ~57.6%–65.4%, outperforming statistical baselines (≈46.9%–52.7%) and outperforming zero-shot LM scoring in all conditions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Embedding-based approach outperformed the statistical co-occurrence baseline and the zero-shot LM scoring approach; slightly better than or competitive with GraphSage in certain (ground-truth co-occurrence) conditions, at the cost of requiring training data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated for anomaly detection. Observed limitations include the need for training data (not zero-shot), sensitivity to which objects are used in queries (they choose k=3 most informative objects), and reduced performance when critical object types are missing from queries (though experiments show some robustness and transfer). RoBERTa is inappropriate for zero-shot scoring (slower for pseudo-log-likelihood scoring) per authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Robot 3D Scene Understanding', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5752.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5752.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot scoring (GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot LM scoring approach using GPT-J to score templated room-descriptive sentences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Construct for each candidate room label a templated descriptive sentence 'A room containing o1, o2, ... is called a(n) r.' and score each with an autoregressive LM (GPT-J); pick the label with highest log-probability—a zero-shot scoring approach operating on object lists expressed as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (autoregressive LM, used in half-precision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-J6B, an autoregressive transformer LM (authors used the half-precision 6B-parameter GPT-J model) to compute log-probabilities (Λ(W)) of templated sentences for zero-shot room-label scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Not used for anomaly detection in the paper; used for zero-shot scoring of candidate label sentences (prompt-scoring) constructed from lists of objects. One could adapt such scoring for anomaly detection by computing likelihoods of list entries or generating p(item | context) and flagging low-likelihood items as anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Lists of object labels encoded into templated natural-language sentences (i.e., unstructured lists/sequences expressed as text).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>not applicable / not evaluated (paper does not evaluate anomaly detection)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Scene graphs derived from Matterport3D (nyuClass and mpcat40); entire dataset zero-shot accuracies also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Room-classification accuracy (zero-shot). Reported zero-shot accuracies across conditions: ≈27.3%–52.6% when evaluated on the entire dataset (varied by label space and co-occurrence selection method). Faster inference and better performance with larger models per authors' qualitative observations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Zero-shot GPT-J outperformed the simple statistical baseline in some ground-truth co-occurrence trials but was outperformed by embedding-based and structured-language methods as well as GraphSage in trained settings. Authors note larger LMs improve zero-shot performance; smaller LMs (GPTNeo, GPT-2) gave poor results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated for anomaly detection. Reported limitations: sensitivity to presence of ubiquitous/uninformative objects (they select k most informative objects), dependence on co-occurrence statistics (proxy vs ground-truth affects results), compute & memory constraints (used half-precision 6B model), and zero-shot performance degrades with smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Robot 3D Scene Understanding', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5752.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5752.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TabLLM (citation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TabLLM: Few-shot classification of tabular data with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) that projects tabular/structured data into natural language and applies LMs (few-shot) to classify tabular examples; referenced in this paper as related work for handling structured inputs with LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tabllm: Few-shot classification of tabular data with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TabLLM (concept / approach, not directly used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach (from cited work) that converts tabular rows to natural-language prompts and leverages large LMs in a few-shot manner for classification of tabular data; cited here as an alternative approach to projecting structured data into natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>mention only; in cited work the LM is used for few-shot/tabular classification via prompting (not directly for anomaly detection in this paper). Could be applied to anomaly detection by prompting for outlier/novelty detection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular / structured data converted to natural-language prompts (tabular rows serialized to text).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>not applicable / not evaluated in this paper (the paper cites TabLLM as related work but does not evaluate anomalies).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as a prior approach for parsing/classifying structured data via templating to natural language; the present paper instead fine-tunes an LM on serialized structured strings (structured-language approach) rather than solely projecting structured data to natural language templates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only cited; no experimental evaluation in this paper. The authors note prior works often convert structured data to natural language via templating, but they chose to fine-tune an encoder-decoder LM to directly encode structured strings instead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Robot 3D Scene Understanding', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tabllm: Few-shot classification of tabular data with large language models. <em>(Rating: 2)</em></li>
                <li>Few-shot nlg with pre-trained language model. <em>(Rating: 1)</em></li>
                <li>Masked language model scoring <em>(Rating: 1)</em></li>
                <li>GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5752",
    "paper_id": "paper-5f5b855f58da599c2730120e55d8320fac867279",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "Structured-language (T5)",
            "name_full": "Structured-language serialized-input approach (fine-tuned T5)",
            "brief_description": "A method that serializes room-level structured/tabular information (room bounding-box dimensions and per-object label + 3D position) into a string and fine-tunes a text-to-text LM to decode a probability distribution over room labels; used to classify rooms from scene-graph data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (pretrained text-to-text transformer, fine-tuned)",
            "model_description": "A pretrained encoder-decoder text-to-text transformer (T5 family) fine-tuned to encode serialized structured-room strings and decode room-label probabilities; used in the paper as the LM backbone for structured-string classification.",
            "model_size": null,
            "anomaly_detection_method": "Not used for anomaly detection in the paper; used as a supervised classifier on serialized structured/tabular-like inputs (fine-tuning). (Method could be adapted for anomaly detection by using model likelihoods / decoded label probabilities or by fine-tuning a head to detect anomalous records.)",
            "data_type": "Serialized structured data / tabular-like strings (room bounding-box dimensions + object label, count, and relative positions per object).",
            "anomaly_type": "not applicable / not evaluated (paper does not evaluate anomaly detection)",
            "dataset_name": "Scene graphs derived from Matterport3D (≈1870 rooms; object label spaces: mpcat40 and nyuClass)",
            "performance_metrics": "Evaluated as a room-classifier on the Matterport3D-derived scene-graph dataset using accuracy (test-split). Structured-language trials achieved test accuracies in the range ~63.7%–69.1% (best: 69.1% test-split in nyuClass ground-truth co-occurrence condition).",
            "baseline_comparison": "Structured-language approach outperformed or matched other language-only methods (zero-shot, embedding-based) and beat simple statistical baselines in many conditions; comparable or better than vision-only baselines in aggregate (see paper Tables 1–2).",
            "limitations_or_failure_cases": "Not evaluated for anomaly detection. Limitations reported: compute/token limits (had to omit rooms with &gt;100 objects and round values), some geometric/spatial info (e.g., object pose) omitted due to token/memory constraints, modest gains when ablating bounding-box dims but more drop when removing object positions (object positions contributed to performance).",
            "uuid": "e5752.0",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Robot 3D Scene Understanding",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Embedding-based (RoBERTa + MLP)",
            "name_full": "Embedding-based LM approach using RoBERTa-large embeddings with a shallow MLP head",
            "brief_description": "Project natural-language queries summarizing the k most informative objects in a room into RoBERTa-large embeddings and train a shallow classifier (MLP) on those embeddings to predict room labels; used as a supervised method that leverages LM embeddings of object lists.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large (LM embedder) + shallow MLP classifier",
            "model_description": "RoBERTa-large (masked-language transformer-based encoder used to embed short natural-language query strings like 'This room contains o1, o2, and o3.') The embeddings are fed into a small multi-layer perceptron trained with cross-entropy to predict room labels.",
            "model_size": null,
            "anomaly_detection_method": "Not used for anomaly detection in the paper; used for supervised classification from object-list queries (fine-tuning classifier head on LM embeddings). Could be repurposed for anomaly detection by training the head to detect 'anomalous' embeddings or using distance/uncertainty in embedding space.",
            "data_type": "Lists of object labels (informative subset of objects per room) converted to short natural-language strings (i.e., unstructured lists expressed as text).",
            "anomaly_type": "not applicable / not evaluated (paper does not evaluate anomaly detection)",
            "dataset_name": "Scene graphs derived from Matterport3D (nyuClass and mpcat40 label spaces), same splits as structured-language experiments.",
            "performance_metrics": "Room-classification accuracy (test-split). Embedding-based approach achieved test accuracies ~57.6%–65.4%, outperforming statistical baselines (≈46.9%–52.7%) and outperforming zero-shot LM scoring in all conditions reported.",
            "baseline_comparison": "Embedding-based approach outperformed the statistical co-occurrence baseline and the zero-shot LM scoring approach; slightly better than or competitive with GraphSage in certain (ground-truth co-occurrence) conditions, at the cost of requiring training data.",
            "limitations_or_failure_cases": "Not evaluated for anomaly detection. Observed limitations include the need for training data (not zero-shot), sensitivity to which objects are used in queries (they choose k=3 most informative objects), and reduced performance when critical object types are missing from queries (though experiments show some robustness and transfer). RoBERTa is inappropriate for zero-shot scoring (slower for pseudo-log-likelihood scoring) per authors.",
            "uuid": "e5752.1",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Robot 3D Scene Understanding",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Zero-shot scoring (GPT-J)",
            "name_full": "Zero-shot LM scoring approach using GPT-J to score templated room-descriptive sentences",
            "brief_description": "Construct for each candidate room label a templated descriptive sentence 'A room containing o1, o2, ... is called a(n) r.' and score each with an autoregressive LM (GPT-J); pick the label with highest log-probability—a zero-shot scoring approach operating on object lists expressed as text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J (autoregressive LM, used in half-precision)",
            "model_description": "GPT-J6B, an autoregressive transformer LM (authors used the half-precision 6B-parameter GPT-J model) to compute log-probabilities (Λ(W)) of templated sentences for zero-shot room-label scoring.",
            "model_size": "6B",
            "anomaly_detection_method": "Not used for anomaly detection in the paper; used for zero-shot scoring of candidate label sentences (prompt-scoring) constructed from lists of objects. One could adapt such scoring for anomaly detection by computing likelihoods of list entries or generating p(item | context) and flagging low-likelihood items as anomalies.",
            "data_type": "Lists of object labels encoded into templated natural-language sentences (i.e., unstructured lists/sequences expressed as text).",
            "anomaly_type": "not applicable / not evaluated (paper does not evaluate anomaly detection)",
            "dataset_name": "Scene graphs derived from Matterport3D (nyuClass and mpcat40); entire dataset zero-shot accuracies also reported.",
            "performance_metrics": "Room-classification accuracy (zero-shot). Reported zero-shot accuracies across conditions: ≈27.3%–52.6% when evaluated on the entire dataset (varied by label space and co-occurrence selection method). Faster inference and better performance with larger models per authors' qualitative observations.",
            "baseline_comparison": "Zero-shot GPT-J outperformed the simple statistical baseline in some ground-truth co-occurrence trials but was outperformed by embedding-based and structured-language methods as well as GraphSage in trained settings. Authors note larger LMs improve zero-shot performance; smaller LMs (GPTNeo, GPT-2) gave poor results.",
            "limitations_or_failure_cases": "Not evaluated for anomaly detection. Reported limitations: sensitivity to presence of ubiquitous/uninformative objects (they select k most informative objects), dependence on co-occurrence statistics (proxy vs ground-truth affects results), compute & memory constraints (used half-precision 6B model), and zero-shot performance degrades with smaller models.",
            "uuid": "e5752.2",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Robot 3D Scene Understanding",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "TabLLM (citation)",
            "name_full": "TabLLM: Few-shot classification of tabular data with large language models",
            "brief_description": "Prior work (cited) that projects tabular/structured data into natural language and applies LMs (few-shot) to classify tabular examples; referenced in this paper as related work for handling structured inputs with LMs.",
            "citation_title": "Tabllm: Few-shot classification of tabular data with large language models.",
            "mention_or_use": "mention",
            "model_name": "TabLLM (concept / approach, not directly used in experiments)",
            "model_description": "Approach (from cited work) that converts tabular rows to natural-language prompts and leverages large LMs in a few-shot manner for classification of tabular data; cited here as an alternative approach to projecting structured data into natural language.",
            "model_size": null,
            "anomaly_detection_method": "mention only; in cited work the LM is used for few-shot/tabular classification via prompting (not directly for anomaly detection in this paper). Could be applied to anomaly detection by prompting for outlier/novelty detection tasks.",
            "data_type": "Tabular / structured data converted to natural-language prompts (tabular rows serialized to text).",
            "anomaly_type": "not applicable / not evaluated in this paper (the paper cites TabLLM as related work but does not evaluate anomalies).",
            "dataset_name": null,
            "performance_metrics": "",
            "baseline_comparison": "Mentioned as a prior approach for parsing/classifying structured data via templating to natural language; the present paper instead fine-tunes an LM on serialized structured strings (structured-language approach) rather than solely projecting structured data to natural language templates.",
            "limitations_or_failure_cases": "Only cited; no experimental evaluation in this paper. The authors note prior works often convert structured data to natural language via templating, but they chose to fine-tune an encoder-decoder LM to directly encode structured strings instead.",
            "uuid": "e5752.3",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Robot 3D Scene Understanding",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tabllm: Few-shot classification of tabular data with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Few-shot nlg with pre-trained language model.",
            "rating": 1
        },
        {
            "paper_title": "Masked language model scoring",
            "rating": 1
        },
        {
            "paper_title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model.",
            "rating": 1
        }
    ],
    "cost": 0.01474725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Leveraging Large (Visual) Language Models for Robot 3D Scene Understanding</h1>
<p>William Chen and Siyi Hu and Rajat Talak and Luca Carlone<br>Massachusetts Institute of Technology<br>{verityw, siyi, talak, lcarlone}@mit.edu</p>
<h4>Abstract</h4>
<p>Abstract semantic 3D scene understanding is a problem of critical importance in robotics. As robots still lack the common-sense knowledge about household objects and locations of an average human, we investigate the use of pre-trained language models to impart common sense for scene understanding. We introduce and compare a wide range of scene classification paradigms that leverage language only (zero-shot, embedding-based, and structuredlanguage) or vision and language (zero-shot and fine-tuned). We find that the best approaches in both categories yield $\sim 70 \%$ room classification accuracy, exceeding the performance of pure-vision and graph classifiers. We also find such methods demonstrate notable generalization and transfer capabilities stemming from their use of language.</p>
<h2>1 Introduction</h2>
<p>3D scene understanding is a key challenge in robotics. For robots to see widespread deployment, they must be able to not only map/localize in many environments, but also have a semantic understanding of said environments and the entities within them. If a robot is told to "fetch a spoon," it should infer that spoons are found in kitchens, which are usually characterized by things like ovens and sinks, and then use all this knowledge to navigate to a task-appropriate room.</p>
<p>These aspects are typically inferred using metricsemantic simultaneous localization and mapping (SLAM) algorithms, wherein a robotic agent maps its environment, determines its location within it, and annotates the map with semantic information (Bowman et al., 2017). Modern spatial perception systems, like Kimera (Rosinol et al., 2021) and Hydra (Hughes et al., 2022), arrange this data in 3D scene graphs - data structures wherein nodes represent locations and entities (e.g., buildings, rooms, and objects), while edges represent spatial relationships (see Fig. 1). Nodes can hold geomet-
ric information (like pose and bounding box) for entities and places in the scene. However, attaching semantic labels to these nodes still remains a major open obstacle, especially for nodes corresponding to high-level spatial concepts, like rooms and buildings. To label a room node, the system must consider what objects are in the room (e.g., if it contains a stove, the room is likely a kitchen). This necessitates a "common-sense" mechanism to provide such knowledge.</p>
<p>One candidate for imparting this common sense is by using pre-trained language models (LMs). Being trained on large text corpora, they capture some of the semantic information within said datasets - a LM may learn that the sentence "Bathrooms contain __." is best finished with "toilets.". Furthermore, being what von Humboldt et al. (1988); Chomsky (1965) called an "infinite use of finite means," language allows arbitrary common-sense queries to be compactly made and evaluated, including ones with novel concepts. This is important for spatial perception, as deployed robots will naturally come across many objects that engineers did not expect during development. Being able to do inference over novel concepts to suit a given task would thus be highly beneficial.</p>
<p>In this work, we consider ways to leverage pretrained LMs for abstract robot scene understanding tasks - specifically, room classification. We show how such models provide useful priors that enable efficient zero-shot or fine-tuned performance. We find that a structured-language approach wherein a LM is fine-tuned for room classification based on strings detailing spatial and semantic object information outperforms similar language-based approaches that only consider object labels (and discards quantitative spatial information, due to it being difficult to express in natural language).</p>
<p>These experiments only reason about language descriptions, discarding visual information. However, such features still provide a useful seman-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: 3D scene graph example. We use (V)LMs to attach high-level labels to nodes (e.g., to label rooms) using lower-level information (e.g., contained objects).</p>
<p>tic signal, so we also consider methods for using visual language models (VLMs) for robot scene understanding. We find that visual-linguistic approaches achieve better performance when combining vision with object labels in comparison to vision-only methods (while maintaining minimal hand-engineering effort). We thus argue that combining semantic spatial perception systems with the common-sense priors found in pre-trained (V)LMs enables effective abstract scene understanding.</p>
<h2>2 Related Works</h2>
<p><strong>Metric-Semantic SLAM:</strong> As high-level semantic understanding is vital for human-robot interaction and planning, there has been significant interest in combining classical methods with deep learning for metric-semantic SLAM. Such works generally focus on low-level representations, such as object-centric approaches (Bowman et al., 2017; Dong et al., 2017; Nicholson et al., 2018; Ok et al., 2021), dense approaches (Behley et al., 2019; Grinvald et al., 2019; Rosu et al., 2019), or a hybrid of the two (Li et al., 2016; McCormac et al., 2018). However, these methods generally disregard higher-level semantic labeling which are needed for many planning and reasoning tasks.</p>
<p><strong>Hierarchical Mapping:</strong> An alternative approach is to consider hierarchical maps, which represent the robot's environment at different levels of abstraction. Works like Ruiz-Sarmiento et al. (2017); Galindo et al. (2005) divide robot knowledge into spatial and semantic information, then anchoring the former to the latter. Our work focuses on scene graphs as a hierarchical map representation. Such data structures were first commonly used for visual relation tasks (Lu et al., 2016; Johnson et al., 2018), but have since been generalized to 3D, where they have found success in robotics (Armeni et al., 2019; Rosinol et al., 2020, 2021; Hughes et al., 2022). Nevertheless, anchoring semantics to spatial information in 3D scene graphs remains difficult, motivating us to look towards LMs.</p>
<p><strong>Language and Robotics:</strong> Using LMs for robotics has been a rapidly-expanding area of research. Papers like Tellex et al. (2011); Sharma et al. (2022); Lynch and Sermanet (2020); Shridhar et al. (2021); Kollar et al. (2010); Howard et al. (2014); Matuszek et al. (2013); Ahn et al. (2022) have mainly leveraged language for communicating goals or instructions to a robot to plan around or execute, often using LMs as intuitive priors over appropriate actions, rewards, or dynamics. For scene understanding, Huang et al. (2023) use occupancy maps with language embeddings attached, while Kerr et al. (2023); Shafiullah et al. (2022) connect such information to neural radiance fields. These works focus on lower-level semantics, compared to the more abstract semantics considered here. Past approaches for room classification use an explicit Bayesian probabilistic framework for determining room labels based on detected objects (Chaves et al., 2020). However, such methods remain hard to generalize to new rooms and objects. To address these shortcomings, we explore the ability for large LMs to be used as common-sense mechanisms for robot scene understanding.</p>
<h2>3 Language-only Methods</h2>
<p>Modern language models are generally transformers (Vaswani et al., 2017) that learn distributions over strings. We write Λ(W) ≈ log p(W) to be a LM's estimated log probability score for string W. LMs are also typically able to embed texts into semantically meaningful vectors, which can be useful for fine-tuning in down-stream tasks. When</p>
<p>pre-trained on large data corpora, all these models' outputs can reflect common world knowledge ( Li et al., 2023a). We thus make use of these capabilites for scene understanding.</p>
<h3>3.1 Query Strings and Informativeness</h3>
<p>If we want to use LMs to reason about a room's class, we first need to project its semantic features into language. We start by building query strings describing a given room whose label we wish to infer. For this, we assume access to a list of objects within. This list can be inferred by existing mapping techniques (Rosinol et al., 2021; Hughes et al., 2022). For our experiments, we use ground truth object labels from our considered dataset.</p>
<p>Putting all objects in a room into the query may result in poor performance, as the queries may be dominated by uninformative, ubiquitous objects (e.g., lights). We thus draw from Grice's conversational maxims and rational speech acts (Grice, 1975; Goodman and Frank, 2016). In this framework, a pragmatic speaker chooses an utterance based on how a listener would interpret a literal speaker. E.g., a literal speaker might describe a bedroom as "containing chairs," which is true, but ambiguous to a listener who is trying to discern the room's label. A pragmatic speaker would thus instead say the room contains a bed, which is much more informative and disambiguating.</p>
<p>To implement this in our queries, we only include the $k$ objects most informative for room classification, noting that objects which appear in fewer room types are more informative, as their presence heavily implies certain rooms. Quantitatively, these objects have more non-uniform distributions $p(r \mid o)$, where $o \in L_{O}$ is the object label, $r \in L_{R}$ is the room label, and $L_{O, R}$ are the sets of all possible objects/rooms respectively. We compute these conditional probabilities in two ways:</p>
<ul>
<li>Using ground-truth co-occurrences. When using these empirical conditionals, we apply Laplace smoothing. This requires taskspecific data.</li>
<li>Using proxy co-occurrence probabilities by querying LMs. Specifically:</li>
</ul>
<p>$$
p(r \mid o) \approx \frac{\exp \Lambda\left(W_{o, r}\right)}{\sum_{r^{\prime} \in L_{R}} \exp \Lambda\left(W_{o, r}\right)}
$$</p>
<p>where $W_{o, r}$ is the query string "A room containing $o$ is called a(n) $r$."</p>
<p>With $p(r \mid o)$ available, a natural measure of its nonuniformity (and thus informativeness) is entropy:</p>
<p>$$
H_{o}=-\sum_{r \in L_{R}} p(r \mid o) \log p(r \mid o)
$$</p>
<p>Entropy is maximized when $p(r \mid o)$ is uniform and minimized when one-hot, so more informative objects have lower $H_{o}$. Thus, to pick objects for the queries, we take the $k$ different lowest-entropy present objects:</p>
<p>$$
O_{\text {best }}=\underset{o \in O}{\operatorname{argmin}} k\left[H_{o}\right]
$$</p>
<p>where $O$ is the set of all object labels contained within the considered room. We now have $k$ objects $O_{\text {best }}$ which can be used to infer the room label.</p>
<h3>3.2 Zero-shot Approach</h3>
<p>For the zero-shot approach, we construct $\left|L_{R}\right|$ query strings, one per room label:</p>
<p>$$
\begin{aligned}
W_{r}= &amp; \text { "A room containing } o_{1}, o_{2}, \ldots \text { and } o_{k} \
&amp; \text { is called a(n) } r . " \forall r \in L_{R}
\end{aligned}
$$</p>
<p>where $o_{1 \ldots k} \in O_{\text {best }}$ are ordered by ascending entropy. All these queries are scored via LM, with the final estimated room label $\hat{r}$ being whichever one yields the highest query sentence probability:</p>
<p>$$
\hat{r}=\underset{r}{\arg \max } \Lambda\left(W_{r}\right)
$$</p>
<p>We note that this can similarly be done by prompting the LM to generate its inferred room type. However, for evaluative purposes, we follow works like (Ahn et al., 2022) and opt for this scoring approach to constrain the LM to our considered labels.</p>
<h3>3.3 Embedding-based Approach</h3>
<p>For our embedding-based fine-tuning approach, we create a query of the form:</p>
<p>$$
W=\text { "This room contains } o_{1}, o_{2}, \ldots \text { and } o_{k} \text {." }
$$</p>
<p>This string is then fed into a LM to produce a summary embedding vector. Finally, the embedding is fed into a trained classifier, which produces $\left|L_{R}\right|$ prediction logits corresponding to the room labels, with the inferred room label corresponding to the maximum logit. We choose this network to be a shallow multi-layer perceptron.</p>
<h3>3.4 Structured-data Approach</h3>
<p>The above approaches limit LM inputs to natural language queries. This is restrictive, as robot perception systems tend to detect spatial features, like room size, object poses, and bounding boxes, that are not commonly expressed in natural language. Humans tend to describe spatial relations qualitatively (e.g., "to the left of" or "on top of"), so numeric spatial features may not be processed well via pre-trained LMs. These features are nonetheless useful for room classification: e.g., hallways are often characterized by being long.</p>
<p>We thus consider a structured-data approach wherein a LM accepts description strings that can easily express spatial features, but are not natural language - e.g., serialized tabular data. This way, the inputs are more expressive, but still capitalize on the LM's common-sense semantic understanding of objects and rooms.</p>
<p>Past approaches for parsing and classifying structured data involve projecting structured data into natural language, e.g., via templating (Hegselmann et al., 2023). We instead choose to follow (Chen et al., 2020) and fine-tune an encoder-decoder LM to encode structured data strings containing the room's axis-aligned dimensions and each contained object's label, count, and position (relative to the center of the room), then decode probabilities for each possible room label conditioned on the structured description. The highest-scoring one is the inferred label. See Appendix C for details.</p>
<h2>4 Vision-and-language Methods</h2>
<p>In grounded tasks, LMs require observations be projected into string space. Some features are invariably lost in this process. In our case, we only use object labels and spatial features, while visual features like color or texture go unused. Such data is still useful in room classification: e.g., bathrooms often have white tiles that other rooms do not have.</p>
<p>We thus look to visual language models: pretrained models that connect text and language. For instance, Radford et al. (2021) uses a contrastive objective to learn a shared embedding space for vision and language, allowing one to match images with text. Other approaches generate text conditioned on input images and text prompts, usually for visual question answering (VQA) or captioning (Li et al., 2022, 2023b). We try to use such models to classify locations based on previously-available object information and egocentric room images.</p>
<h3>4.1 Zero-shot Approach</h3>
<p>We consider a VLM equivalent to the languageonly zero-shot approach. For contrastive VLMs, we follow the standard zero-shot classification technique (Radford et al., 2021) and embed both room label strings and images from the considered room. The inferred label is whichever room string best matches the images in embedding space (as determined by cosine distance).</p>
<p>For generative VLMs, we input the images and a prompt ("What type of room is this?" for VQA models and "This room is a(n) $r$." for captioners). Such prompts can be prepended with object descriptions - an advantage they have over the contrastive VLMs. The model then follows up the prompt with each possible room label, with the highest-probability label being the inferred one.</p>
<h3>4.2 Fine-tuning Approach</h3>
<p>This approach is the same as the zero-shot VLM approach, but with the model fine-tuned to output room labels. We train a VQA VLM to correctly answer "What type of room is this?" when given images of said room. Again, at inference time, the inferred room label is whichever one answers the prompt question with the highest probability.</p>
<p>Note that, for both these approaches, we use captioning/VQA VLMs on a single image at a time. We detail how to get an overall room classification from many images in Sec 5.4. Our approach can be easily generalized to VLMs that accept multiple input images, like Alayrac et al. (2022). Due to computation limits, we leave such experiments to future work.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Datasets</h3>
<p>We evaluate our algorithms on scene graphs produced from the Matterport3D dataset (Chang et al., 2017), which is commonly used in robot navigation tasks (Manolis Savva* et al., 2019; Yadav et al., 2022; Szot et al., 2021). This dataset can rapidly render rooms and contained objects, each with labels, pose, and bounding boxes that we use to create scene graphs. Objects are assigned labels from two label spaces: mpcat40 (35 labels) and nyuClass (201 labels) (Silberman et al., 2012). In total, there are $\sim 1870$ rooms, each with one of 23 room labels. See Appendix A for scene graph construction details. We now consider how datasets</p>
<p>are constructed for language-only and vision-andlanguage approaches respectively.</p>
<p>Language-only: We divide the buildings into a 50/20/30 train/validation/test split for each label space. To produce queries for our embeddingbased approach, we bootstrap subsets of the most informative objects per room to put into the query. We generate four such datasets by varying object label space (nyuClass/mpcat40) and co-occurrences used for object selection (ground truth/proxy). We use RoBERTa-large as our LM embedder (Liu et al., 2019). See Appendix B for more details. For the structured-language dataset, for each room, we create a string of the form in Appendix C summarizing the room dimensions and object positions and labels (for a given label space). Unlike the zeroshot/embedding approaches, said strings contain all objects in a given room.</p>
<p>Each dataset for a certain label space is produced from the same splits. All approaches and baselines are tested on the same test split. For completeness, approaches that do not require training are also evaluated on the entire dataset.</p>
<p>Vision and Language: For each room, we sample and save images from 100 random freespace camera poses (see Appendix D). For methods that require training, we divide the rooms into a 40/20/40 train/validation/test split. Note that (i) all images for a given room belong to the same group and (ii) this is a different divide than for languageonly, as it includes rooms with no objects (as visual features can still be extracted from them).</p>
<h3>5.2 Baselines</h3>
<p>We consider three baselines. First, we use groundtruth co-occurrence data for a statistical baseline. We approximate the probability of a room label as the product of the conditional probabilities of the room given each object individually:</p>
<p>$$
p(r \mid O) \approx \prod_{o_{i} \in O} p\left(r \mid o_{i}\right)
$$</p>
<p>where the conditionals $p\left(r \mid o_{i}\right)$ are empirically estimated from the dataset. The inferred label is thus $\arg \max _{r} p(r \mid O)$. Second, we train a GraphSage graph neural network baseline (Hamilton. et al., 2017) to predict rooms given objects using the language-only dataset splits.</p>
<p>Finally, as a baseline for vision-based methods, we use the vision-and-language dataset splits to train a ResNet-50 (He et al., 2016) model to predict room label logits given images from the room.</p>
<p>We do this from scratch and starting from the pretrained weights. See Appendix E for details.</p>
<h3>5.3 Language-only Trial Specifications</h3>
<p>Zero-shot: We vary the ground truth/proxy objectroom co-occurrences (when computing object entropy) and the object label spaces for four total conditions. We choose $k=3$ objects per room to create the corresponding queries (or all if the room contains fewer than three objects). For all trials, we use GPT-J (Wang and Komatsuzaki, 2021) for both inference and generating proxy co-occurences. Due to hardware limitations, we use the half-precision release of the model.</p>
<p>Embedding-based: We train head networks on each of the four embedding datasets. See Appendix E for more details. We also run two generalization experiments. First, we train the network while holding out all nyuClass rooms whose query strings contain chairs, sinks, toilets, beds, and washing machines, then we test on these held out datapoints. This is done with ground-truth co-occurrences. Second, we train models on mpcat40 data while testing on nyuClass data in order to see if they can accommodate and generalize to a different, larger input label space. In this case, we divide the mpcat40 dataset using a 40/60 training/validation split, use the entire nyuClass dataset for testing, and vary the co-occurrence type (ground truth and proxy).</p>
<p>Structured-language: We fine-tune a pretrained T5 LM to encode structured-language strings and decode (score) room labels (Raffel et al., 2020). We train for 5 epochs and test on the weights with highest validation accuracy. See Appendix E for more details. We also run some ablation trials, removing the room bounding box dimensions, object positions, or both.</p>
<h3>5.4 Vision-and-language Trial Specifications</h3>
<p>Zero-shot: We test CLIP, BLIP, and BLIP-2's zeroshot visual room classification abilities (Radford et al., 2021; Li et al., 2022, 2023b). Of these models, CLIP is contrastive while the BLIP models are generative, so we adopt the VQA and captioning approaches detailed in Sec. 4.1 respectively.</p>
<p>As each room has many images, we measure both the portion of individual images and overall rooms classified correctly as image-wise and room-wise accuracies, respectively. A room is classified correctly if, when the scores for each label are summed over all images from said room, the correct label has the highest total score. This is</p>
<p>different than a plurality vote wherein the mostpredicted room label (out of all image predictions) is the room label, as our method incorporates model uncertainties by using the output logits/scores.</p>
<p>Fine-tuning: We fine-tune the BLIP VQA VLM to decode room labels based on images and descriptions. The setup is the same as the zero-shot case, but with the introduction of fine-tuning. See Appendix E for more details. Again, we measure image- and room-wise accuracies with the same maximum score metric as above.</p>
<h3>5.5 Real Scene Graph Trial Specification</h3>
<p>To highlight how our algorithms fit together with modern robot spatial perception software, we run our approaches on a real scene graph. We use Hydra (Hughes et al., 2022) to generate a scene graph of the apartment environment in the uHumans2 (Rosinol et al., 2021) photorealistic simulator. The scene graph is queried for the objects in the same room as the robot at each frame. We then run the fine-tuned vision-and-language algorithm from Section 5.4 on the current image to produce a distribution over room classes based on the objects from the scene graph and the agent's current observation. As the scene graph also yields the current room of the agent, we aggregate and track room-wise label statistics as well. See Appendix F for more details.</p>
<h2>6 Results</h2>
<p>We present all results for language-only and vision-and-language (and associated baselines) in Tables 1 and 2 respectively.</p>
<h3>6.1 Language-only Results</h3>
<p>Zero-shot: The zero-shot trials yield room classification accuracies of $27.3-52.6 \%$ when run on the entire dataset. The ground-truth co-occurrence trials perform better than the statistical baseline evaluated on the whole dataset, which also uses ground truth frequencies. No trial outperforms the GraphSage baseline, but said baseline requires training and cannot be easily extended to additional labels, unlike our approach; by virtue of being zero-shot, the only step needed to adopt the large label space was to compute the informativeness metric for the new objects. This approach also achieves high accuracies for several common household rooms (bathroom, bedroom, kitchen, and living room). For the best performing trial, accuracies for these key rooms range from $79.2-97.1 \%$ (see Fig. 2).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Zero-shot accuracies on all data for all conditions, by room label.</p>
<p>There also are two trends for when a room will not be classified correctly:</p>
<ul>
<li>Room lacks disambiguating objects: Bathrooms and bedrooms have objects almost exclusive to them (e.g., toilets and beds), but rooms like lobbies and family rooms only contain more ubiquitous ones (e.g., tables and chairs), and so are harder to identify.</li>
<li>Room is not "standard": Bars, libraries, and spas all more commonly refer to buildings, not rooms. We note this could likely be fixed with further prompt tuning and few-shot examples.</li>
</ul>
<p>For the other rooms, the LM shows the desired common sense when classifying them. Our approach also demonstrates generalization, handling the smaller, 35 -object label space (mpcat40) and the much larger, 201-object label space (nyuClass). In fact, the nyuClass trials result in higher accuracies than their mpcat40 counterparts, as nyuClass's labels are more specific and informative. This benefit is best shown in the following cases:</p>
<ul>
<li>Kitchens \&amp; laundry rooms: Both rooms are characterized by appliances. While nyuClass provides fine-grained labels (e.g., washing machine vs. stoves), mpcat40 groups all those objects under the broad and ambiguous category of "appliances," making differentiation of the two room labels difficult.</li>
<li>Game rooms \&amp; garages: Game rooms are characterized by recreational objects, like ping-pong/foosball tables. Both appear in nyuClass, but are just "tables" in mpcat40, making these rooms easier to identify with the former. Likewise, garage doors (nyuClass) are just called "doors" in mpcat40.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Baselines</th>
<th></th>
<th>Zero-shot</th>
<th></th>
<th>Embedding-based</th>
<th></th>
<th>Structured-language</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>RoBERTa</td>
<td></td>
<td>With Position</td>
<td></td>
<td>No Position</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Statistical</td>
<td>GraphSage</td>
<td>GT</td>
<td>Proxy</td>
<td>GT</td>
<td>Proxy</td>
<td>GT</td>
<td>Proxy</td>
<td>GT</td>
<td>Proxy</td>
<td></td>
</tr>
<tr>
<td>nyuClass</td>
<td>$57.7 \%(50.6 \%)$</td>
<td>$64.2 \%$</td>
<td>$52.2 \%(53.6 \%)$</td>
<td>$27.2 \%(28.2 \%)$</td>
<td>$65.5 \%$</td>
<td>$57.6 \%$</td>
<td>$\mathbf{6 9 . 1 \%}$</td>
<td>$68.7 \%$</td>
<td>$67.3 \%$</td>
<td>$67.7 \%$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>$\mathbf{m p c a t 4 0}$</td>
<td>$44.7 \%(46.9 \%)$</td>
<td>$59.1 \%$</td>
<td>$48.9 \%(50.1 \%)$</td>
<td>$27.5 \%(27.3 \%)$</td>
<td>$63.9 \%$</td>
<td>$58.5 \%$</td>
<td>$\mathbf{6 8 . 3 \%}$</td>
<td>$67.7 \%$</td>
<td>$63.9 \%$</td>
<td>$63.7 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: Test-split accuracies for all language-only approaches. Methods that do not require training have full dataset accuracy reported in parentheses. Highest test split accuracies are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fine-tuning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">BLIP VQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP-2 Captioner</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ResNet-50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP VQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">nyuClass</td>
<td style="text-align: center;">mpcat40</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">nyuClass</td>
<td style="text-align: center;">mpcat40</td>
<td style="text-align: center;">From</td>
<td style="text-align: center;">From</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">nyuClass</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pretrained</td>
<td style="text-align: center;">Scratch</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">mpcat40</td>
</tr>
<tr>
<td style="text-align: left;">Room-</td>
<td style="text-align: center;">$36.5 \%$</td>
<td style="text-align: center;">$37.8 \%$</td>
<td style="text-align: center;">$37.0 \%$</td>
<td style="text-align: center;">$37.1 \%$</td>
<td style="text-align: center;">$47.5 \%$</td>
<td style="text-align: center;">$47.9 \%$</td>
<td style="text-align: center;">$48.0 \%$</td>
<td style="text-align: center;">$51.1 \%$</td>
<td style="text-align: center;">$26.2 \%$</td>
<td style="text-align: center;">$53.2 \%$</td>
<td style="text-align: center;">$\mathbf{6 8 . 6 \%}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{( 3 2 . 7 \% )}$</td>
<td style="text-align: center;">$(36.2 \%)$</td>
<td style="text-align: center;">$(36.6 \%)$</td>
<td style="text-align: center;">$(37.5 \%)$</td>
<td style="text-align: center;">$(45.7 \%)$</td>
<td style="text-align: center;">$(45.7 \%)$</td>
<td style="text-align: center;">$(46.2 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Image-</td>
<td style="text-align: center;">$26.5 \%$</td>
<td style="text-align: center;">$30.1 \%$</td>
<td style="text-align: center;">$35.6 \%$</td>
<td style="text-align: center;">$34.4 \%$</td>
<td style="text-align: center;">$40.1 \%$</td>
<td style="text-align: center;">$45.0 \%$</td>
<td style="text-align: center;">$44.5 \%$</td>
<td style="text-align: center;">$36.8 \%$</td>
<td style="text-align: center;">$22.6 \%$</td>
<td style="text-align: center;">$47.0 \%$</td>
<td style="text-align: center;">$\mathbf{6 7 . 9 \%}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{( 2 5 . 9 \% )}$</td>
<td style="text-align: center;">$(28.8 \%)$</td>
<td style="text-align: center;">$(34.9 \%)$</td>
<td style="text-align: center;">$(34.3 \%)$</td>
<td style="text-align: center;">$(39.3 \%)$</td>
<td style="text-align: center;">$(43.1 \%)$</td>
<td style="text-align: center;">$(43.0 \%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Test-split room- and image-wise accuracies for all language and vision approaches. Methods that do not require training have full dataset accuracy reported in parentheses. Highest test split accuracies are bolded.</p>
<p>Finally, all trials take $\sim 1.2$ seconds to infer a room's label.</p>
<p>Embedding-based: The embedding-based approach gets $57.6-65.4 \%$ test accuracy, beating the statistical baseline ( $46.9-52.7 \%$ ) and zero-shot approach ( $27.2-52.2 \%$ ) in all conditions. Unlike for the zero-shot case, this is even true when using proxy co-occurrences, so the model does not explicitly need ground-truth co-occurrences for picking out objects to achieve high performance (though some is learned in training). Trials using ground-truth co-occurrences yield slightly higher accuracies than corresponding GraphSage baselines, while the proxy trials are still competitive.</p>
<p>Finally, RoBERTa has $20 \times$ fewer parameters than GPT-J (used for zero-shot), needing less compute, memory, and time to generate embeddings. However, it is inappropriate for zero-shot evaluation, only yielding $44.5 \%$ accuracy on the best condition (nyuClass, ground-truth co-occurrences). Its zero-shot inference speed is also slower: being a MLM, it uses pseudo-log likelihood scores (Salazar et al., 2020), which increases inference time by $2.5 \times$ for our task (GPT-J takes $\sim 40 \mathrm{~min}$ while RoBERTa takes $\sim 100$ for the same dataset). Our embedding-based approach thus has better inference speed and performance while still using a smaller LM, at the cost of needing training data.</p>
<p>Embedding-based Transfer: Using LMs to embed room descriptions enables some generalization to novel object classes. For the holdout object trials, the model correctly classifies rooms whose queries</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Holdout Trial Room Accuracies</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Washing <br> Machine</td>
<td style="text-align: center;">Chair</td>
<td style="text-align: center;">Sink</td>
<td style="text-align: center;">Toilet</td>
<td style="text-align: center;">Bed</td>
</tr>
<tr>
<td style="text-align: center;">$14.6 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
<td style="text-align: center;">$64.2 \%$</td>
<td style="text-align: center;">$77.6 \%$</td>
<td style="text-align: center;">$83.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Test accuracies for rooms containing each holdout object.
contain sinks, toilets, and beds $64.2-83.5 \%$ of the time, even when held out in training (see Table 3). The network likely learns to extract essential information on room labels from query embeddings containing only non-held out objects that generalizes to the held out ones. E.g., while toilets are held out, related objects like bathtubs are not. The embeddings for observed queries ("This room contains bathtubs.") may be similar to that of unobserved ones ("This room contains toilets."), so the network classifies the latter correctly too. As there are no objects related to washing machines in nyuClass, rooms containing them have comparatively low accuracy. Still, as most rooms have several characteristic objects, holding out a subset of them (or, equivalently, introducing more of them) is generally not an issue.</p>
<p>Regardless, our approach shows promising generalization and transferability. When trained on mpcat40 and tested on nyuClass, it yields $47.1 \%$ and $56.6 \%$ accuracy for proxy and ground-truth cooccurrences respectively, comparable to the best accuracy for the zero-shot approach ( $52.57 \%$ ) and some non-transfer fine-tuning conditions.</p>
<p>Structured Language: We expect the structured-language approach to do at least as well as the embedding approach, as the structured language string's information is a superset of that of the embedding-based inputs. Sure enough, all structured-language approaches have test accuracies from $63.7-69.1 \%$, being comparable to or better than all other approaches. We note that ablating room bounding box information does not degrade performance much, but removing object positions is more impactful (especially for mpcat40 trials). Nevertheless, all changes from ablations are relatively small; generally, structured language of any type seems to be the best overall language-only approach for room classification.</p>
<h3>6.2 Vision-and-language Results</h3>
<p>Zero-shot: The BLIP-2 trials have the highest zeroshot room-/image-wise accuracies, comparable to that of the fine-tuned ResNet-50 baseline, despite having no task-specific training. Adding object labels (from either label space) to the prompt improves image-wise accuracy, likely as it aids in the classification of uninformative images (e.g., ones facing walls) by providing non-visible room-wise information on contained objects. However, it surprisingly does not improve room-wise accuracy, perhaps because the improved image classifications are already for rooms that are visually distinct and thus can be classified from just the good images.</p>
<p>We find that the BLIP-2 approach has slightly lower accuracy than the best language-only zeroshot trial ( $45.7-46.2 \%$ vs. $52.6 \%$ ), though a bit higher than the second-best trial. However, the former needs less prompt engineering and no groundtruth co-occurrence data (for picking objects for the query template in the language-only case), so is easier to implement and use.</p>
<p>Fine-tuning: The fine-tuning approach yields similar accuracies to the structured-language approach. Notably, in contrast to the zero-shot equivalents, the fine-tuned models' image- and roomwise accuracies benefit from object labels in the prompt. This suggests that, unlike with zero-shot, the fine-tuned models both use the object labels to correctly classify aforementioned uninformative images specifically for rooms that would otherwise not be correctly predicted - it is not just correcting uninformative images in rooms whose other images are easily correctly classifiable.</p>
<p>Moreover, we see that the fine-tuned trial that
does not have object labels achieves a room-wise accuracy of $53.2 \%$, compared to the pre-trained ResNet-50 vision-only baseline accuracy of $51.1 \%$. These values are similar and both higher than the from-scratch ResNet-50 baseline trial, suggesting that pre-training (be it visual or visual-linguistic) aids in transfer to the room classification domain.</p>
<p>However, both pre-trained ResNet-50 and the fine-tuned no-label trials fail to beat any of the approaches that use fine-tuning in conjunction with object labels (including the language-only approaches). This suggests that, even when trained on domain-specific data, it is difficult for vision alone to classify rooms from arbitrary images within.</p>
<p>It is thus appropriate to supplement such inferences with room-wise object information, especially if such data is already available from a spatial perception system. In this case, we find that (V)LMs can easily incorporate object labels into their room label inferences, leveraging their common-sense understanding of object-room relations to improve room classification accuracy.</p>
<h3>6.3 Real Scene Graph Results</h3>
<p>For the real scene graph trial, the fine-tuned VLM's room-wise classifications are nearly all correct. The only incorrect room is the dining room, where the VLM assigns similar scores to "kitchen" and "dining room" (though the former is slightly higher). For all other rooms, the model assigns high scores to the correct label. We present a visualization of image-wise room classifications in https:// tinyurl.com/2wj7aan3. We also give an example visualization image, images from each room, and full room-wise classification scores in Figs. 4, 3, and Table 5 respectively, all in Appendix G.</p>
<h2>7 Conclusion</h2>
<p>We show the applicability of large pre-trained LMs and VLMs to the problem of abstract robot scene understanding, particularly in the domain of room classification. We explore an array of languageonly and vision-and-language approaches, comparing them with standard statistical and learned baselines. We find that using LMs yields higher overall performance while also having good generalization to held-out objects and transferability to new label spaces. Our results show that these paradigms are promising avenues of development for scalable, sample-efficient, and generalizable robot spatial perception systems.</p>
<h2>Limitations</h2>
<p>Our primary limitation is compute. We ran all LM inference experiments on a single RTX 3080 GPU, which vastly constrains the size of model (and thus the resulting experiments) we could use.</p>
<p>In particular, while not quantitatively reported, we find that the zero-shot approach empirically benefits greatly from using larger models trained on more data; smaller equivalent models like GPTNeo (Black et al., 2021) or GPT-2 (Radford et al., 2019) yield very poor zero-shot results. We found that the six billion parameter GPT-J was the largest such model we could reliably load, but still had to use the half-precision release due to memory constraints.</p>
<p>Likewise, our structured language approach only includes room dimensions, object labels, and object positions; some spatial/geometric information, like object pose, is still lost, as they do not fit in our smaller models' token counts (or, even if they do, would take up too much memory).</p>
<p>We thus detail some additional research directions that would be possible with larger compute budgets. We suspect that newer, larger models (on the scale of $10-100$ billion parameters) would improve performance even more, especially when combined with recent advances in instruction and human preference-based tuning. For the VLM experiments, newer models explictly trained for embodied visual-linguistic reasoning (Driess et al., 2023) would also likely perform very well for our scene understanding tasks. Finally, many commonsense visual understanding problems have shown to work well with Socratic model (Zeng et al., 2023) setups, wherein multiple models (typically covering multiple modalities) interface with each other in natural language.</p>
<p>However, these constraints do also reflect a real challenge if such systems are deployed on real mobile robots - such autonomous systems generally do not have the hardware required to load the latest and largest LMs on them locally. It is therefore promising that, even with somewhat older, smaller models (which could feasibly be deployed on a robotic platform), we still achieve relatively good performance in our considered tasks.</p>
<p>It thus would be exciting to see both how one could get the most out of the limited compute of a mobile robot and how our scene understanding approaches could be made more effective when the limitation is relaxed.</p>
<h2>Ethics Statement</h2>
<p>When applying large pre-trained models to robotics, it is very important to recognize how the risks associated with such models can be amplified when given embodiment (Bommasani et al., 2022). We focus on the relatively innocuous use case of scene understanding/location classification, but even still, the biases captured within our considered models may be reflected in their performance on such tasks. For instance, we evaluate on the Matterport3D dataset, which contains scans of domestic environments. It is safe to say this data represents the homes of a narrow intersection of wealth, class, and culture. It is possible that homes of certain demographics may not conform to the prototypical conceptualization of such locations present within our (V)LMs, and that our evaluative metrics do not detect such biases (again, as our testing data is not diverse enough).</p>
<p>Thus, we believe it is important for such data to be diverse and transparently/ethically sourced (while maintaining proper privacy), especially when used to train general-purpose household robots. In addition, we encourage research into more advanced scene understanding techniques that allow for interpretability of inferences and adaptability to a wider range of environments.</p>
<h2>References</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. 2022. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022.</p>
<p>Flamingo: a visual language model for few-shot learning.
I. Armeni, Z. He, J. Gwak, A. Zamir, M. Fischer, J. Malik, and S. Savarese. 2019. 3D scene graph: A structure for unified semantics, 3D space, and camera. In Intl. Conf. on Computer Vision (ICCV), pages 56645673.
J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall. 2019. SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. In Intl. Conf. on Computer Vision (ICCV).</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow. If you use this software, please cite it using these metadata.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2022. On the opportunities and risks of foundation models.
S.L. Bowman, N. Atanasov, K. Daniilidis, and G.J. Pappas. 2017. Probabilistic data association for semantic SLAM. In IEEE Intl. Conf. on Robotics and Automation (ICRA), pages 1722-1729.</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV).</p>
<p>David Chaves, J.R. Ruiz-Sarmiento, Nicolai Petkov, and Javier González-Jiménez. 2020. From object detection to room categorization in robotics. pages $1-6$.</p>
<p>Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, and William Yang Wang. 2020. Few-shot nlg with pre-trained language model.</p>
<p>Noam Chomsky. 1965. Aspects of the Theory of Syntax. The MIT Press, Cambridge.
J. Dong, X. Fei, and S. Soatto. 2017. Visual-InertialSemantic scene representation for 3D object detection. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. Palm-e: An embodied multimodal language model.
C. Galindo, A. Saffiotti, S. Coradeschi, P. Buschka, J.A. Fernández-Madrigal, and J. González. 2005. Multihierarchical semantic maps for mobile robotics. In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pages 3492-3497.</p>
<p>Noah D. Goodman and Michael C. Frank. 2016. Pragmatic language interpretation as probabilistic inference. Trends in Cognitive Sciences, 20(11):818-829.
H. P. Grice. 1975. Logic and conversation. In Syntax and Semantics: Vol. 3: Speech Acts, pages 41-58. Academic Press.
M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Siegwart, and J. Nieto. 2019. Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery. IEEE Robotics and Automation Letters, 4(3):3037-3044.
W. L. Hamilton., R. Ying, and J. Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems (NIPS), page 1025-1035.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. pages 770-778.</p>
<p>Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. 2023. Tabllm: Few-shot classification of tabular data with large language models.</p>
<p>Thomas M. Howard, Stefanie Tellex, and Nicholas Roy. 2014. A natural language planner interface for mobile manipulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages 6652-6659.</p>
<p>Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. 2023. Visual language maps for robot navigation. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA).
N. Hughes, Y. Chang, and L. Carlone. 2022. Hydra: a real-time spatial perception engine for 3D scene graph construction and optimization. In Robotics: Science and Systems (RSS). (pdf).</p>
<p>Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018. Image generation from scene graphs. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. 2023. Lerf: Language embedded radiance fields.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2017. Adam: A method for stochastic optimization.</p>
<p>Thomas Kollar, Stefanie Tellex, Deb K. Roy, and Nicholas Roy. 2010. Toward understanding natural language directions. In HRI 2010.</p>
<p>Belinda Z. Li, William Chen, Pratyusha Sharma, and Jacob Andreas. 2023a. Lampp: Language models as probabilistic priors for perception and action.
C. Li, H. Xiao, K. Tateno, F. Tombari, N. Navab, and G. D. Hager. 2016. Incremental scene understanding on dense SLAM. In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pages 574-581.</p>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models.</p>
<p>Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization.</p>
<p>Cewu Lu, Ranjay Krishna, Michael Bernstein, and FeiFei Li. 2016. Visual relationship detection with language priors. In European Conference on Computer Vision, pages 852-869.</p>
<p>Corey Lynch and Pierre Sermanet. 2020. Language conditioned imitation learning over unstructured data. arXiv.</p>
<p>Manolis Savva<em>, Abhishek Kadian</em>, Oleksandr Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. 2019. Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).</p>
<p>Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2013. Learning to Parse Natural Language Commands to a Robot Control System, pages 403-415. Springer International Publishing, Heidelberg.
J. McCormac, R. Clark, M. Bloesch, A.J. Davison, and S. Leutenegger. 2018. Fusion++: Volumetric objectlevel SLAM. In Intl. Conf. on 3D Vision (3DV), pages $32-41$.
L. Nicholson, M. Milford, and N. Sünderhauf. 2018. QuadricSLAM: Dual quadrics from object detections as landmarks in object-oriented SLAM. IEEE Robotics and Automation Letters, 4:1-8.</p>
<p>Kyel Ok, Katherine Liu, and Nicholas Roy. 2021. Hierarchical object map estimation for efficient and robust navigation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages $1132-1139$.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.
A. Rosinol, A. Gupta, M. Abate, J. Shi, and L. Carlone. 2020. 3D dynamic scene graphs: Actionable spatial perception with places, objects, and humans. In Robotics: Science and Systems (RSS). (pdf), (media), (video).
A. Rosinol, A. Violette, M. Abate, N. Hughes, Y. Chang, J. Shi, A. Gupta, and L. Carlone. 2021. Kimera: from SLAM to spatial perception with 3D dynamic scene graphs. Intl. J. of Robotics Research, 40(1214):1510-1546. ArXiv preprint: 2101.06894, (pdf).
R. Rosu, J. Quenzel, and S. Behnke. 2019. Semisupervised semantic mapping through label propagation with semantic texture meshes. Intl. J. of Computer Vision.</p>
<p>Jose-Raul Ruiz-Sarmiento, Cipriano Galindo, and Javier Gonzalez-Jimenez. 2017. Building multiversal semantic maps for mobile robot operation. KnowledgeBased Systems, 119:257-272.</p>
<p>Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. 2020. Masked language model scoring. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</p>
<p>Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. 2022. Clip-fields: Weakly supervised semantic fields for robotic memory.</p>
<p>Pratyusha Sharma, Balakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox. 2022. Correcting robot plans with natural language feedback. arXiv.</p>
<p>Mohit Shridhar, Lucas Manuelli, and Dieter Fox. 2021. Cliport: What and where pathways for robotic manipulation. arXiv.
N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. 2012. Indoor segmentation and support inference from rgbd images. In European Conf. on Computer Vision (ECCV).</p>
<p>Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. 2021. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In $A A A I$.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems (NIPS), volume 30. Curran Associates, Inc.
W.F. von Humboldt, P. Heath, and H. Aarsleff. 1988. On Language: The Diversity of Human LanguageStructure and its Influence on the Mental Development of Mankind. Texts in German Philosophy. Cambridge University Press.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Karmesh Yadav, Santhosh Kumar Ramakrishnan, John Turner, Aaron Gokaslan, Oleksandr Maksymets, Rishabh Jain, Ram Ramrakhya, Angel X Chang, Alexander Clegg, Manolis Savva, Eric Undersander, Devendra Singh Chaplot, and Dhruv Batra. 2022. Habitat challenge 2022. https://aihabitat.org/ challenge/2022/.</p>
<p>Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2023. Socratic models: Composing zero-shot multimodal reasoning with language. In Submitted to The Eleventh International Conference on Learning Representations. Under review.</p>
<h2>A Converting Matterport3D to Scene Graphs</h2>
<p>To convert semantic meshes from Matterport3D into scene graphs, we create a node for each region and object (Chang et al., 2017). Then, we connect all object nodes assigned to a region to that region's room node. We also filter out some regions. While Matterport3D contains outdoor regions as well ("yard," "balcony," and "porch"), we do not perform inference over them, since they are not true rooms and thus would require an alternate query string structure. In addition to outdoor regions, we also remove all rooms with no objects within or with the label "none."</p>
<p>Each object is assigned labels from several label spaces. We consider the original labels used by Matterport3D (mpcat40) and the labels used by NYU (nyuClass) (Silberman et al., 2012). For both, we filter out nodes belonging to the mpcat40 categories "ceiling," "wall," "floor," "miscellaneous," "object," and any other unlabeled nodes. We remove these categories because they are either not objects within the room or they are ambiguous to the point of being semantically uninformative. However, for nyuClass, we do not reject objects classified by mpcat40 as "object," since nyuClass has many more fine-grained and semantically-rich categories which all are mapped to this category. After pre-processing the label spaces in this way, mpcat40 has 35 object labels and nyuClass has 201. Both datasets share a room label space with 23 labels. See Table 4 for a breakdown of room label frequencies.</p>
<p>We perform a few additional dataset preprocessing steps to produce the final scene graph dataset with 1878 rooms. First, since some objects are assigned an incorrect region (e.g., toilets</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Room Label</th>
<th style="text-align: center;">Bar</th>
<th style="text-align: center;">Bathroom</th>
<th style="text-align: center;">Bedroom</th>
<th style="text-align: center;">Classroom</th>
<th style="text-align: center;">Closet</th>
<th style="text-align: center;">Conference <br> Auditorium</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Occurrences</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">365</td>
<td style="text-align: center;">251</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Percentage</td>
<td style="text-align: center;">$0.16 \%$</td>
<td style="text-align: center;">$19.43 \%$</td>
<td style="text-align: center;">$13.37 \%$</td>
<td style="text-align: center;">$0.11 \%$</td>
<td style="text-align: center;">$5.27 \%$</td>
<td style="text-align: center;">$0.85 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Room Label</td>
<td style="text-align: center;">Dining</td>
<td style="text-align: center;">Family</td>
<td style="text-align: center;">Game Room</td>
<td style="text-align: center;">Garage</td>
<td style="text-align: center;">Gym</td>
<td style="text-align: center;">Hallway</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Room</td>
<td style="text-align: center;">Room</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Occurrences</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">326</td>
</tr>
<tr>
<td style="text-align: left;">Percentage</td>
<td style="text-align: center;">$3.94 \%$</td>
<td style="text-align: center;">$3.25 \%$</td>
<td style="text-align: center;">$0.91 \%$</td>
<td style="text-align: center;">$0.75 \%$</td>
<td style="text-align: center;">$0.85 \%$</td>
<td style="text-align: center;">$17.36 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Room Label</td>
<td style="text-align: center;">Kitchen</td>
<td style="text-align: center;">Laundry</td>
<td style="text-align: center;">Library</td>
<td style="text-align: center;">Living</td>
<td style="text-align: center;">Lobby</td>
<td style="text-align: center;">Lounge</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Room</td>
<td style="text-align: center;">Room</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Occurrences</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Percentage</td>
<td style="text-align: center;">$4.15 \%$</td>
<td style="text-align: center;">$1.86 \%$</td>
<td style="text-align: center;">$0.05 \%$</td>
<td style="text-align: center;">$3.78 \%$</td>
<td style="text-align: center;">$3.30 \%$</td>
<td style="text-align: center;">$3.41 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Room Label</td>
<td style="text-align: center;">Office</td>
<td style="text-align: center;">Spa</td>
<td style="text-align: center;">Staircase</td>
<td style="text-align: center;">Television</td>
<td style="text-align: center;">Utility</td>
<td style="text-align: center;">Total</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Room</td>
<td style="text-align: center;">Room</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Occurrences</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$\mathbf{1 8 7 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Percentage</td>
<td style="text-align: center;">$5.22 \%$</td>
<td style="text-align: center;">$2.34 \%$</td>
<td style="text-align: center;">$8.09 \%$</td>
<td style="text-align: center;">$0.69 \%$</td>
<td style="text-align: center;">$0.85 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Room label frequencies in pre-processed Matterport3D dataset.
are assigned to living rooms, despite (i) that being non-sensible and (ii) the toilet not being within the bounding box of the living room), we check to see if each object is within the bounding box of its assigned region. If not, then it is re-assigned to whichever region's bounding box contains it, and the corresponding scene-graph connection is also made. Second, nyuClass has some misspelled labels (e.g., "refridgerator" instead of "refrigerator"), so we correct all of those too. Lastly, sometimes, a single nyuClass label may be erroneously assigned to multiple mpcat40 labels. This is most problematic when one of the mpcat40 labels is rejected and the other is not. To address this, we use the first mpcat40 label for each nyuClass label that is not rejected (e.g., nyuClass label "stairs" is mapped to mpcat40 "miscellaneous," which is rejected, and "stairs," which is not, so we keep the latter). However, this means some labels which should be rejected are not rejected, so we also manually filter out all nyuClass object labels that are the same as those of rejected mpcat40 labels: "ceiling," "floor," and "wall".</p>
<h2>B Embedding-based Bootstrapping Method</h2>
<p>To generate training data for the embedding-based method, we take the $n$ most informative objects in each room and find all $k$-object permutations, producing $P_{k}^{n}$ query datapoints per room of the form in Eq. 6, all of which correspond to the room's label. We do this for $(k, n) \in{(1,2),(2,3),(3,4)}$. Models trained on this data will thus be invariant to object order and number in the query, and can also handle less informative object labels.</p>
<h2>C Structured-language Query String</h2>
<p>For the structured-language approach, we describe a given room using the following string template:</p>
<p>Room Size:
x [x room bounding box length]
y [y room bounding box length]
z [z room bounding box length]
Object Locations:
[object 1 label]
x [x position relative to room center]
y [y position relative to room center]
z [z position relative to room center]
[object 2 label]
x [x position relative to room center]
y [y position relative to room center]
z [z position relative to room center]
(repeat for all other objects in the room)
To fit hardware memory constraints, we omit rooms with over 100 objects and round all values to three decimal places. Note that a given room might have different objects depending on which object label space is considered, as nyuClass includes labels that would be just classified as "object" by mpcat40 (which we reject).</p>
<h2>D Vision-and-language Dataset Generation</h2>
<p>To construct a dataset of room images for our vision-and-language methods, we use Matterport3D's Pathfinder functionality to generate a topdown occupancy grid map of freespace for a considered height (which we set to the height of a considered room). A single map pixel's length is set to 0.1 meters. We find all pixels that fall within a given room's bounding box, masking out the rest. To avoid sampling points that are too close to walls and objects, we dilate occupied cells with a $3 \times 3$ convolutional filter. Then, we uniformly sample (without replacement) 100 unoccupied cell positions (or as many as possible, if the room has fewer than 100 open cells). Each one is assigned a yaw uniformly at random. Finally, we set the camera to each of those poses, saving the viewed images and corresponding room label.</p>
<h2>E Training Details</h2>
<p>For the GraphSage baseline, we train for 500 epochs with a learning rate of $5 e-3$, weight decay</p>
<p>of $1 e-4$, hidden state dimension of 16 , dropout of 0.2 , and 2 iterations of message-passing.</p>
<p>For the embedding-based approach, we train each network for 200 epochs with a batch size of 512 using cross entropy loss via the Adam optimizer with a learning rate of $1 e-4, \beta_{1}, \beta_{2}=$ $0.9,0.999$, weight decay of $1 e-3$, and a StepLR scheduler with step size of 10 and $\gamma=0.5$ (Kingma and Ba, 2017).</p>
<p>For the structured-language approach, we train the T5 LM for 5 epochs with a batch size of 2 (due to memory constraints) and the AdamW optimizer with learning rate of $1 e-4$, smoothing term $\varepsilon=1 e-8$, and no weight decay (Loshchilov and Hutter, 2019).</p>
<p>For the ResNet baseline, we train for 10 epochs using the SGD optimizer with a learning rate of $1 e-3$, a batch size of 128 , and momentum of 0.9 . We load the checkpoint weights with the highest validation image-wise accuracy.</p>
<p>For the fine-tuned VLM approach, we train a BLIP-2 model for 5 epochs with the AdamW optimizer with a learning rate of $5 e-6$, batch size of 8 images (due to memory constraints), weight decay of 0.01 , and smoothing term of $1 e-8$.</p>
<p>In trials training with the vision-and-language dataset, we pick 16 random images per room (as many as possible if there are fewer than 16). Additionally, for both validation and testing, we take every fourth image. Both these choices are to reduce the training and evaluation time - more images can be used for a more complete evaluation.</p>
<h2>F Real Scene Graph Trial Details</h2>
<p>For the real scene graph trial, we consider a scene graph built via Hydra using a trajectory in the uHumans2 simulator's apartment environment (Rosinol et al., 2021). The trajectory contains egocentric RBG observations at 30 Hz and odometry data used to construct the scene graph. The scene graph itself contains nodes representing agent positions along the trajectory. We associate each observation with the temporally-closest agent node, thereby assigning each observation a room (and associated contained objects).</p>
<p>As the observations are in a different domain than Matterport3D, we fine-tune the BLIP VQA VLM on the entire vision-and-language train and validation datasets, then evaluate on the test set. For ease of visualization, we limit ourselves to the following labels in training, validation, and deploy-
ment: bathroom, bedroom, dining room, family room, kitchen, living room, lounge, office, and television room. We use the same hyperparameters as presented in Appendix E.</p>
<p>Finally, for each image, we generate a descriptive query question string containing the room's contained objects (from the scene graph) and feed both into the fine-tuned VLM for prediction. We show the image-wise prediction distribution in the associated visualization video, but also compute room-wise distribution statistics (based on the room that the scene graph says the agent is in at each timestep).</p>
<h2>G Real Scene Graph Results</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example visualization of room label predictions on the real scene graph.</p>
<p>We present some additional results and analysis for the real scene graph trial.</p>
<p>Firstly, we showcase some example images drawn from each of the six uHumans2 apartments, including their corresponding ground-truth label in Fig. 4. We also include an example visualization of the predicted distribution over room labels for a single frame in Fig. 3. These rooms received the corresponding room-wise label classifications shown in Table 5, where the top three highest-scoring category per room are noted. In all but one case (room 0 ), the highest-scoring label is correct. We note that the top categories tend to have overlap of contained objects: e.g., kitchens and bathrooms often both have sinks.</p>
<p>Moreover, as listed in Fig. 4, Hydra both uses a small label space (similar to mpcat40) for semantic segmentation and is not entirely reliable in assigning objects to rooms - there are objects that can be seen in the trajectory video that are not assigned to the rooms they are in, e.g., the shower in the bathroom.</p>
<p>Nevertheless, the semantic signal it does provide</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Rm 0: Dining room. Contains curtains, cabinets, tables, chairs, mirrors, plants, and lighting.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(d) Rm 3: Bathroom. Contains towels and toilets.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Rm 1: Bedroom. Contains beds, cabinets, appliances, tables, pictures, mirrors, shelves, lighting, and cushions.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(e) Rm 4: Living room. Contains curtains, sofas, cabinets, tables, pictures, chairs, shelves, lighting, and cushions.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(c) Rm 2: Office. Contains cabinets, lighting, pictures, and chairs.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(f) Rm 5: Kitchen. Contains curtains, cabinets, appliances, tables, pictures, chairs, sinks, shelves, and lighting.</p>
<p>Figure 4: Example egocentric images from each room of the uHumans2 apartment, along with their corresponding ID, ground truth label, and contained objects (as detected by Hydra).
already allows our VLM to achieve good room classification performance. We expect better/more informative segmentation and scene graph generation to only improve performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rm 0 <br> Dining Room</th>
<th style="text-align: center;">Rm 1 <br> Bedroom</th>
<th style="text-align: center;">Rm 2 <br> Office</th>
<th style="text-align: center;">Rm 3 <br> Bathroom</th>
<th style="text-align: center;">Rm 4 <br> Living Room</th>
<th style="text-align: center;">Rm 5 <br> Kitchen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Kitchen</td>
<td style="text-align: center;">Bedroom</td>
<td style="text-align: center;">Office</td>
<td style="text-align: center;">Bathroom</td>
<td style="text-align: center;">Living Room</td>
<td style="text-align: center;">Kitchen</td>
</tr>
<tr>
<td style="text-align: center;">-10.5</td>
<td style="text-align: center;">-5.4</td>
<td style="text-align: center;">-16.0</td>
<td style="text-align: center;">-1.4</td>
<td style="text-align: center;">-5.0</td>
<td style="text-align: center;">-4.5</td>
</tr>
<tr>
<td style="text-align: center;">Dining Room</td>
<td style="text-align: center;">Family Room</td>
<td style="text-align: center;">Dining Room</td>
<td style="text-align: center;">Bedroom</td>
<td style="text-align: center;">Family Room</td>
<td style="text-align: center;">Dining Room</td>
</tr>
<tr>
<td style="text-align: center;">-10.6</td>
<td style="text-align: center;">-12.0</td>
<td style="text-align: center;">-18.6</td>
<td style="text-align: center;">-3.4</td>
<td style="text-align: center;">-9.5</td>
<td style="text-align: center;">-10.7</td>
</tr>
<tr>
<td style="text-align: center;">Office</td>
<td style="text-align: center;">Living Room</td>
<td style="text-align: center;">Lounge</td>
<td style="text-align: center;">Kitchen</td>
<td style="text-align: center;">Lounge</td>
<td style="text-align: center;">Bathroom</td>
</tr>
<tr>
<td style="text-align: center;">-16.6</td>
<td style="text-align: center;">-12.8</td>
<td style="text-align: center;">-20.9</td>
<td style="text-align: center;">-3.6</td>
<td style="text-align: center;">-9.6</td>
<td style="text-align: center;">-11.2</td>
</tr>
</tbody>
</table>
<p>Table 5: Top three room-wise logit scores for each room in the uHumans2 apartment, generated by using our fine-tuned VLM approach in conjunction with a scene graph from Hydra. Values are scaled by $1 e-3$ for readability. Scores are equivalent to negative loss, so the highest-scoring room is the inferred label. Note that room 0 is the only incorrectly-classified one, and even then, the true label (dining room) is a very close second to the highest-scoring label (kitchen).</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3618 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3618</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3618</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-80.html">extraction-schema-80</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <p><strong>Paper ID:</strong> paper-1805fa1f126cd66dceb9287a398e4f1f3111b1ad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1805fa1f126cd66dceb9287a398e4f1f3111b1ad" target="_blank">Parallel distributed processing</a></p>
                <p><strong>Paper Venue:</strong> Psyke &amp; Logos</p>
                <p><strong>Paper TL;DR:</strong> The notion that PDP approaches provide a sub-symbolic account of cognitive processes, in contrast to theclassical symbolic view, is examined and speculation concerning the explanatory power of PDP systems at the cognitive level of functioning is suggested.</p>
                <p><strong>Paper Abstract:</strong> After briefly reviewing the appealing psychological properties of PDP systems, an introduction to their historical roots and basic computational mechanisms are provided. A variety of network architectures are described including one-layered perceptrons, backpropagation networks, Boltzmann machines and recurrent systems. Three PDP simulations are analysed: First, a model that purports to learn the past tense of English verbs; Second, a constraint satisfaction network which is able to interpret the alternative configurations of a Necker cube; Finally, a recurrent network which is able to decipher membership of grammatical classes from word-order information. The notion that PDP approaches provide a sub-symbolic account of cognitive processes, in contrast to theclassical symbolic view, is examined. The article concludes with brief speculation concerning the explanatory power of PDP systems at the cognitive level of functioning.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3618.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3618.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDP (distributed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Distributed Processing / Distributed Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional account in which knowledge (concepts, events, language) is represented by patterns of activation distributed across many simple processing units; representations are graded, context-sensitive, learned from input, robust to damage, and support graceful generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>distributed representation (Parallel Distributed Processing)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are encoded as patterns of activation across many units and weighted connections rather than by single local symbols; learning adjusts connection weights so that input patterns map to output/activity patterns, producing prototype-like attractors and context-dependent behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Explained simulation successes: emergence of prototypical representations and graceful degradation; Rumelhart & McClelland past-tense simulation reproducing U-shaped learning and overgeneralisation; hidden-unit structure in backprop and Boltzmann networks (e.g. Hinton kinship example) that reflect salient task dimensions; Elman recurrent network internal vectors that cluster by lexical class from distributional input; robustness of networks to noisy inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Critiques that distributed systems lack clear referential/ symbolic tokens (hidden units 'have no referential function'); objections that some behaviours (e.g. rule-like generalisations in language) are better explained by symbolic rules (Pinker & Prince, Fodor & Pylyshyn); specific empirical critiques that particular PDP simulations (e.g. Rumelhart & McClelland past-tense) relied on problematic encodings (Wickelfeatures) and input assumptions producing artefactual U-shaped curves.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted directly with localist connectionist schemes (single-node concepts) and with symbolic/rule-based accounts; PDP argued to subsume many phenomena without explicit rules (micro-structural account), whereas symbolic theorists claim rules and symbolic tokens are necessary to capture compositionality and certain generalisations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How distributed codes map onto psycholinguistic variables like semantics and reference; interpretability of hidden-unit representations; conditions under which distributed networks produce apparent rule-like behaviour vs graded behaviour; accounting fully for compositional structured representations and rapid one-shot learning remains an open question.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3618.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3618.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Localist representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Localist connectionist representation (single-unit concept nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representational format where individual units (nodes) correspond to whole concepts or symbols, unlike distributed patterns; mentioned as a contrasting approach within connectionist models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>localist representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented by dedicated units (or small clusters) that act as symbols/referents; activation of the unit directly denotes the concept rather than a distributed pattern across many units.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Historical use in earlier connectionist and semantic network models where single nodes stand for concepts; intuitive interpretability and easy mapping to symbolic architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Paper argues localist schemes lack the graceful degradation, context sensitivity, and emergent prototype behaviour of distributed architectures; such accounts are considered by PDP proponents as too impoverished to capture fine-grained behavioural structure and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Explicitly contrasted with PDP/distributed representations: PDP rejects localist representations in favour of diffuse encodings that capture context-sensitive behaviour without dedicated symbol nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Whether localist representations can scale to capture graded similarity, generalisation and robustness observed in human cognition; empirical evidence favouring distributed accounts for many tasks suggests limits to pure localist approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3618.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3618.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic rule-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classical symbolic / rule-based representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The traditional cognitive-science view that conceptual/linguistic knowledge is stored as discrete symbols and rules (e.g., morphological rules), producing categorical behaviour and compositional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>symbolic (rule-based) representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is encoded as discrete symbols and combinatorial rules that manipulate symbols (e.g., 'add -ed' rule for past tense), yielding explicit, interpretable operations and compositional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Accounts for rule-like productive behaviour and rapid application of abstract generalisations (linguistic competence); historically used to explain children's productive use of language and apparent rule inference.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>PDP authors argue symbolic/rule accounts fail to capture context sensitivity, graded behaviour, robustness to damage, emergent prototypes, and performance-level phenomena without ad hoc assumptions; critics of PDP (e.g. Fodor & Pylyshyn) maintain symbolic level is necessary to capture compositionality and systematicity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Paper contrasts symbolic/rule-based accounts with PDP/distributed accounts, arguing PDP provides a unified micro-structural account across competence and performance, while symbolic accounts require distinct rule-layer explanations; symbolic proponents claim PDP cannot capture certain rule-like phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Reconciling symbolic compositionality with the emergent, distributed representations of PDP; whether apparent rule-like behaviour can be fully explained as emergent from distributed learning remains debated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3618.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3618.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wickelfeatures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wickelfeature/Wickelphone distributed encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific distributed phonological encoding used in the Rumelhart & McClelland past-tense simulation: stems encoded as sets of overlapping trigrams (Wickelfeatures) represented over many units.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Wickelfeature representational format</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Phonological forms are encoded as a high-dimensional binary vector of overlapping local contexts (Wickelphones/trigrams), so morphological mappings are learned between distributed input feature vectors and distributed output feature vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Used in the Rumelhart & McClelland simulation to permit a single-layer pattern associator to learn present→past mappings; simulation replicated U-shaped developmental curves and class-specific learning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Criticised (Pinker & Prince) as an artificial, lossy encoding that complicates decoding and may produce artefactual results (e.g., U-shaped curve due to input discontinuities); decoding from Wickel-vectors to surface forms is problematic and non-psychologically transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as a practical distributed encoding alternative to symbolic stem+rule representations; critics argue symbolic stem representations or other encodings yield clearer, more interpretable mappings and avoid artefacts produced by Wickelfeatures.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Whether Wickelfeatures are psychologically realistic; decoding ambiguities and sensitivity to training set statistics; whether alternative distributed encodings or architectures (e.g., recurrent nets) better capture morphology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3618.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3618.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constraint-satisfaction / energy landscape</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive constraint-satisfaction networks / energy landscape (e.g., Boltzmann, Hopfield-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional model where conceptual interpretation arises from settling dynamics on a continuous goodness/energy landscape shaped by mutually excitatory/inhibitory constraints among hypothesis units and bottom-up input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>constraint-satisfaction (energy landscape) representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual/interpretive states correspond to attractors (peaks) in a continuous goodness/energy landscape defined by pairwise weights and inputs; local computations (unit updates) hill-climb to satisfy competing constraints, yielding context-sensitive, frame-like interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Explains bistability and switching in perception (Necker cube simulations), how top-down models and bottom-up inputs interact, emergence of prototypical frames/scripts, and how networks find coherent global interpretations; Boltzmann machines/annealing reduce local minima problems.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Networks can get stuck in local minima producing implausible stable states; need for stochastic methods (simulated annealing/Boltzmann learning) to escape local minima; interpretability of intermediate attractor states can vary; not all cognitive compositional structures are naturally encoded as pairwise constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with feedforward/backprop single-pass mappings and with symbolic serial processors: constraint networks provide distributed, interactive, parallel resolution of interpretations rather than sequential rule application; symbolic accounts claim the need for structured symbolic operations for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How well these dynamics map to neurophysiology and real-time cognition; scalability to very large, structured knowledge; precise mechanisms by which global frames/scripts are learned rather than hand-configured; empirical mapping between attractors and human conceptual prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3618.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3618.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent/context units</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent networks with context units (Elman-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional representational model where temporal context (copies of prior hidden states) provides distributed context-sensitive encodings that enable extraction of syntactic/lexical class structure from sequential input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>recurrent/contextual distributed representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts (e.g., lexical classes) are represented by hidden-layer activation vectors that incorporate temporal/contextual history via recurrent/context units; these vectors cluster by distributional similarity, enabling emergent category structure from sequential statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Elman network trained on sequences produced hidden-unit vectors whose averaged representations clustered by grammatical/semantic class (nouns vs verbs, animate vs inanimate), showing distributional cues alone can yield internal conceptual structure; recurrent nets can capture serial order and some syntactic dependencies (Elman, Jordan examples).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Networks did not learn perfect predictive performance (memorisation poor), and apparent semantic structure is limited to distributional role rather than genuine semantics; critics point out recurrent nets may not capture full grammatical competence or deep compositional reference without additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Offers an account of category/distributional learning alternative to explicit symbolic grammars: categories emerge from sequential statistics rather than pre-specified rules; symbolic proponents argue this lacks true compositional and referential power.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Extent to which distributional/contextual vectors capture semantics vs surface distribution; how to scale to real language complexity and long-distance dependencies; whether recurrent distributed representations can implement true variable binding and compositional reference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3618.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3618.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boltzmann/Hopfield</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boltzmann machine / Hopfield-style associative networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Connectionist architectures implementing stochastic search and associative memory, used functionally to implement constraint satisfaction via annealing and to realize attractor-based representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>stochastic attractor associative representation (Boltzmann/Hopfield)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts and interpretations correspond to attractor states in energy landscapes; Boltzmann machines use stochastic sampling (simulated annealing) to explore landscapes and avoid local minima, while Hopfield nets provide deterministic attractor dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Useful in modelling constraint satisfaction problems (language comprehension integration, avoiding poor local minima), and in demonstrating how global stable states (frame-level interpretations) arise from local interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Computational cost of stochastic annealing and scaling issues; sensitivity to network design and initialisation; possibility of convergence to suboptimal local attractors without careful temperature schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as an implementation technique within the PDP/constraint-satisfaction family, contrasted with deterministic feedforward learning (backprop); provides a mechanism for global optimisation not present in simple gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Biological plausibility of simulated annealing dynamics; how the brain might implement stochastic temperature schedules; scaling to complex, high-dimensional cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning representations by back-propagating errors <em>(Rating: 2)</em></li>
                <li>On the Past Tense Controversy <em>(Rating: 2)</em></li>
                <li>Elman, J. L. (1988) Finding structure in time <em>(Rating: 2)</em></li>
                <li>Rumelhart, McClelland & the PDP research (1986) <em>(Rating: 2)</em></li>
                <li>Pinker, S. & Prince, A. (1988) <em>(Rating: 2)</em></li>
                <li>Hinton, G. E.; McClelland, J. L.; Rumelhart, D. E. (1986) distributed representations / kinship example <em>(Rating: 1)</em></li>
                <li>Ackley, Hinton & Sejnowski (1985) <em>(Rating: 1)</em></li>
                <li>Hopfield (1982) <em>(Rating: 1)</em></li>
                <li>Fodor & Pylyshyn (1988) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3618",
    "paper_id": "paper-1805fa1f126cd66dceb9287a398e4f1f3111b1ad",
    "extraction_schema_id": "extraction-schema-80",
    "extracted_data": [
        {
            "name_short": "PDP (distributed)",
            "name_full": "Parallel Distributed Processing / Distributed Representation",
            "brief_description": "A functional account in which knowledge (concepts, events, language) is represented by patterns of activation distributed across many simple processing units; representations are graded, context-sensitive, learned from input, robust to damage, and support graceful generalisation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "distributed representation (Parallel Distributed Processing)",
            "theory_description": "Concepts are encoded as patterns of activation across many units and weighted connections rather than by single local symbols; learning adjusts connection weights so that input patterns map to output/activity patterns, producing prototype-like attractors and context-dependent behaviour.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Explained simulation successes: emergence of prototypical representations and graceful degradation; Rumelhart & McClelland past-tense simulation reproducing U-shaped learning and overgeneralisation; hidden-unit structure in backprop and Boltzmann networks (e.g. Hinton kinship example) that reflect salient task dimensions; Elman recurrent network internal vectors that cluster by lexical class from distributional input; robustness of networks to noisy inputs.",
            "counter_evidence_or_challenges": "Critiques that distributed systems lack clear referential/ symbolic tokens (hidden units 'have no referential function'); objections that some behaviours (e.g. rule-like generalisations in language) are better explained by symbolic rules (Pinker & Prince, Fodor & Pylyshyn); specific empirical critiques that particular PDP simulations (e.g. Rumelhart & McClelland past-tense) relied on problematic encodings (Wickelfeatures) and input assumptions producing artefactual U-shaped curves.",
            "comparison_to_other_theories": "Contrasted directly with localist connectionist schemes (single-node concepts) and with symbolic/rule-based accounts; PDP argued to subsume many phenomena without explicit rules (micro-structural account), whereas symbolic theorists claim rules and symbolic tokens are necessary to capture compositionality and certain generalisations.",
            "notable_limitations_or_open_questions": "How distributed codes map onto psycholinguistic variables like semantics and reference; interpretability of hidden-unit representations; conditions under which distributed networks produce apparent rule-like behaviour vs graded behaviour; accounting fully for compositional structured representations and rapid one-shot learning remains an open question.",
            "uuid": "e3618.0",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Localist representation",
            "name_full": "Localist connectionist representation (single-unit concept nodes)",
            "brief_description": "A representational format where individual units (nodes) correspond to whole concepts or symbols, unlike distributed patterns; mentioned as a contrasting approach within connectionist models.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "theory_name": "localist representation",
            "theory_description": "Concepts are represented by dedicated units (or small clusters) that act as symbols/referents; activation of the unit directly denotes the concept rather than a distributed pattern across many units.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Historical use in earlier connectionist and semantic network models where single nodes stand for concepts; intuitive interpretability and easy mapping to symbolic architectures.",
            "counter_evidence_or_challenges": "Paper argues localist schemes lack the graceful degradation, context sensitivity, and emergent prototype behaviour of distributed architectures; such accounts are considered by PDP proponents as too impoverished to capture fine-grained behavioural structure and robustness.",
            "comparison_to_other_theories": "Explicitly contrasted with PDP/distributed representations: PDP rejects localist representations in favour of diffuse encodings that capture context-sensitive behaviour without dedicated symbol nodes.",
            "notable_limitations_or_open_questions": "Whether localist representations can scale to capture graded similarity, generalisation and robustness observed in human cognition; empirical evidence favouring distributed accounts for many tasks suggests limits to pure localist approaches.",
            "uuid": "e3618.1",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Symbolic rule-based",
            "name_full": "Classical symbolic / rule-based representation",
            "brief_description": "The traditional cognitive-science view that conceptual/linguistic knowledge is stored as discrete symbols and rules (e.g., morphological rules), producing categorical behaviour and compositional structure.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "theory_name": "symbolic (rule-based) representation",
            "theory_description": "Conceptual knowledge is encoded as discrete symbols and combinatorial rules that manipulate symbols (e.g., 'add -ed' rule for past tense), yielding explicit, interpretable operations and compositional structure.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Accounts for rule-like productive behaviour and rapid application of abstract generalisations (linguistic competence); historically used to explain children's productive use of language and apparent rule inference.",
            "counter_evidence_or_challenges": "PDP authors argue symbolic/rule accounts fail to capture context sensitivity, graded behaviour, robustness to damage, emergent prototypes, and performance-level phenomena without ad hoc assumptions; critics of PDP (e.g. Fodor & Pylyshyn) maintain symbolic level is necessary to capture compositionality and systematicity.",
            "comparison_to_other_theories": "Paper contrasts symbolic/rule-based accounts with PDP/distributed accounts, arguing PDP provides a unified micro-structural account across competence and performance, while symbolic accounts require distinct rule-layer explanations; symbolic proponents claim PDP cannot capture certain rule-like phenomena.",
            "notable_limitations_or_open_questions": "Reconciling symbolic compositionality with the emergent, distributed representations of PDP; whether apparent rule-like behaviour can be fully explained as emergent from distributed learning remains debated.",
            "uuid": "e3618.2",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Wickelfeatures",
            "name_full": "Wickelfeature/Wickelphone distributed encoding",
            "brief_description": "A specific distributed phonological encoding used in the Rumelhart & McClelland past-tense simulation: stems encoded as sets of overlapping trigrams (Wickelfeatures) represented over many units.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "Wickelfeature representational format",
            "theory_description": "Phonological forms are encoded as a high-dimensional binary vector of overlapping local contexts (Wickelphones/trigrams), so morphological mappings are learned between distributed input feature vectors and distributed output feature vectors.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Used in the Rumelhart & McClelland simulation to permit a single-layer pattern associator to learn present→past mappings; simulation replicated U-shaped developmental curves and class-specific learning trajectories.",
            "counter_evidence_or_challenges": "Criticised (Pinker & Prince) as an artificial, lossy encoding that complicates decoding and may produce artefactual results (e.g., U-shaped curve due to input discontinuities); decoding from Wickel-vectors to surface forms is problematic and non-psychologically transparent.",
            "comparison_to_other_theories": "Presented as a practical distributed encoding alternative to symbolic stem+rule representations; critics argue symbolic stem representations or other encodings yield clearer, more interpretable mappings and avoid artefacts produced by Wickelfeatures.",
            "notable_limitations_or_open_questions": "Whether Wickelfeatures are psychologically realistic; decoding ambiguities and sensitivity to training set statistics; whether alternative distributed encodings or architectures (e.g., recurrent nets) better capture morphology.",
            "uuid": "e3618.3",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Constraint-satisfaction / energy landscape",
            "name_full": "Interactive constraint-satisfaction networks / energy landscape (e.g., Boltzmann, Hopfield-style)",
            "brief_description": "Functional model where conceptual interpretation arises from settling dynamics on a continuous goodness/energy landscape shaped by mutually excitatory/inhibitory constraints among hypothesis units and bottom-up input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "constraint-satisfaction (energy landscape) representation",
            "theory_description": "Conceptual/interpretive states correspond to attractors (peaks) in a continuous goodness/energy landscape defined by pairwise weights and inputs; local computations (unit updates) hill-climb to satisfy competing constraints, yielding context-sensitive, frame-like interpretations.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Explains bistability and switching in perception (Necker cube simulations), how top-down models and bottom-up inputs interact, emergence of prototypical frames/scripts, and how networks find coherent global interpretations; Boltzmann machines/annealing reduce local minima problems.",
            "counter_evidence_or_challenges": "Networks can get stuck in local minima producing implausible stable states; need for stochastic methods (simulated annealing/Boltzmann learning) to escape local minima; interpretability of intermediate attractor states can vary; not all cognitive compositional structures are naturally encoded as pairwise constraints.",
            "comparison_to_other_theories": "Contrasted with feedforward/backprop single-pass mappings and with symbolic serial processors: constraint networks provide distributed, interactive, parallel resolution of interpretations rather than sequential rule application; symbolic accounts claim the need for structured symbolic operations for some tasks.",
            "notable_limitations_or_open_questions": "How well these dynamics map to neurophysiology and real-time cognition; scalability to very large, structured knowledge; precise mechanisms by which global frames/scripts are learned rather than hand-configured; empirical mapping between attractors and human conceptual prototypes.",
            "uuid": "e3618.4",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Recurrent/context units",
            "name_full": "Recurrent networks with context units (Elman-style)",
            "brief_description": "A functional representational model where temporal context (copies of prior hidden states) provides distributed context-sensitive encodings that enable extraction of syntactic/lexical class structure from sequential input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "recurrent/contextual distributed representation",
            "theory_description": "Concepts (e.g., lexical classes) are represented by hidden-layer activation vectors that incorporate temporal/contextual history via recurrent/context units; these vectors cluster by distributional similarity, enabling emergent category structure from sequential statistics.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Elman network trained on sequences produced hidden-unit vectors whose averaged representations clustered by grammatical/semantic class (nouns vs verbs, animate vs inanimate), showing distributional cues alone can yield internal conceptual structure; recurrent nets can capture serial order and some syntactic dependencies (Elman, Jordan examples).",
            "counter_evidence_or_challenges": "Networks did not learn perfect predictive performance (memorisation poor), and apparent semantic structure is limited to distributional role rather than genuine semantics; critics point out recurrent nets may not capture full grammatical competence or deep compositional reference without additional mechanisms.",
            "comparison_to_other_theories": "Offers an account of category/distributional learning alternative to explicit symbolic grammars: categories emerge from sequential statistics rather than pre-specified rules; symbolic proponents argue this lacks true compositional and referential power.",
            "notable_limitations_or_open_questions": "Extent to which distributional/contextual vectors capture semantics vs surface distribution; how to scale to real language complexity and long-distance dependencies; whether recurrent distributed representations can implement true variable binding and compositional reference.",
            "uuid": "e3618.5",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Boltzmann/Hopfield",
            "name_full": "Boltzmann machine / Hopfield-style associative networks",
            "brief_description": "Connectionist architectures implementing stochastic search and associative memory, used functionally to implement constraint satisfaction via annealing and to realize attractor-based representations.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "theory_name": "stochastic attractor associative representation (Boltzmann/Hopfield)",
            "theory_description": "Concepts and interpretations correspond to attractor states in energy landscapes; Boltzmann machines use stochastic sampling (simulated annealing) to explore landscapes and avoid local minima, while Hopfield nets provide deterministic attractor dynamics.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Useful in modelling constraint satisfaction problems (language comprehension integration, avoiding poor local minima), and in demonstrating how global stable states (frame-level interpretations) arise from local interactions.",
            "counter_evidence_or_challenges": "Computational cost of stochastic annealing and scaling issues; sensitivity to network design and initialisation; possibility of convergence to suboptimal local attractors without careful temperature schedules.",
            "comparison_to_other_theories": "Presented as an implementation technique within the PDP/constraint-satisfaction family, contrasted with deterministic feedforward learning (backprop); provides a mechanism for global optimisation not present in simple gradient descent.",
            "notable_limitations_or_open_questions": "Biological plausibility of simulated annealing dynamics; how the brain might implement stochastic temperature schedules; scaling to complex, high-dimensional cognitive tasks.",
            "uuid": "e3618.6",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning representations by back-propagating errors",
            "rating": 2
        },
        {
            "paper_title": "On the Past Tense Controversy",
            "rating": 2
        },
        {
            "paper_title": "Elman, J. L. (1988) Finding structure in time",
            "rating": 2
        },
        {
            "paper_title": "Rumelhart, McClelland & the PDP research (1986)",
            "rating": 2
        },
        {
            "paper_title": "Pinker, S. & Prince, A. (1988)",
            "rating": 2
        },
        {
            "paper_title": "Hinton, G. E.; McClelland, J. L.; Rumelhart, D. E. (1986) distributed representations / kinship example",
            "rating": 1
        },
        {
            "paper_title": "Ackley, Hinton & Sejnowski (1985)",
            "rating": 1
        },
        {
            "paper_title": "Hopfield (1982)",
            "rating": 1
        },
        {
            "paper_title": "Fodor & Pylyshyn (1988)",
            "rating": 1
        }
    ],
    "cost": 0.0115585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PARALLEL DISTRIBUTED PROCESSING ${ }^{1}$</h1>
<p>Kim Plunkett</p>
<h2>What is PDP?</h2>
<p>Parallel Distributed Processing (PDP) is a recent development in cognitive science modelling research that assumes that individual components of human information processing are highly interactive and that knowledge of events, concepts and language is represented diffusely in the cognitive system. This distributed feature distinguishes PDP from other »connectionist« models which make use of localist representations, e.g. a single node in a network standing for an entire concept. The PDP approach developed out of the explicit concern of some cognitive scientists that cognitive models should not be restricted to describing aspects of human behaviour that are idealised abstractions of competence. Rather, a unified model of cognition, straddling the traditional competence/performance distinction, has emerged as a worthwhile and realistic research endeavour. Cognitive scientists, working from a PDP perspective, have begun to construct models of human cognition that show promise that their ambitious goals might be achieved. To date, successes range from simulations at the low end of cognition that model speech perception or interpret the different configurations of a necker cube to higher order cognitive skills like sentence processing or language learning. All these models emphasise the context sensitivity of cognitive processes and they all share the assumption that complex behaviours can emerge from the interaction of relatively simple constituents and their environment.</p>
<p>The characteristics of these systems include; a tendency to account for the details of human behaviour within a single framework that does not require ad hoc assumptions to account for apparent exceptional behaviours; a robustness in performance to distortions in the input stimuli or knowledge base itself; a capacity to learn or organise representations of the environment to which they are exposed - prototypical representations emerge as a natural outcome of the learning process; flexibility in responding in an appropriate manner to situations never experienced before. All these characteristics can be attributed to the parallel, distributed architecture of the systems used to implement the models. Typically, PDP models differ from traditional symbolic accounts of human behaviour in their rejection of the need for rule-based processes. Instead, PDP models offer micro-structural accounts. Rule-based accounts are considered by connectionists as convenient</p>
<p>approximations to a system that is considerably more complex. For example, it is argued that categorial rule-systems cannot capture the fine-grained structure of human behaviour in a natural way.</p>
<p>A PDP system's potential for learning is rooted in its high sensitivity to variations in patterns of stimulation and to the structure of the environment to which it is exposed. By manipulating the pattern of connections between its component parts, a network is able to exploit new patterns of stimulation from the environment to create new input/output functions and hence demonstrate new behaviour. All this is achieved with a minimum amount of pre-wiring, i.e. the network organises itself often constructing subtle internal representations of the environment that it is processing. For example, the construction of prototypical representations of environments to which it has never been exposed is a powerful property of such a system. In contrast, many current cognitivist models of learning are highly nativist in their approach, typically relegating learning to the process of choosing between a pre-defined set of symbolic parameters that are innately given.</p>
<p>Graceful degradation, i.e. robustness of behaviour in the face of inadequate stimulation or internal damage, is achieved as a result of the distributed representation of knowledge. Many nodes in a network contribute to the representation of any given fact or proposition. One cannot point to the local representation of a concept as one can in a conventional semantic network. The global distribution of activity in the network has to be considered when evaluating its knowledge state. PDP networks are robust in that the overall pattern of activity in a network often remains stable in the face of pertubation. Similarly, the propensity of PDP networks to respond appropriately to new environments reflects the conservative, assimilative nature of their global representations. New patterns of stimulation are judged against old experiences. Networks modify their behaviour by accommodating to the parallel influence of new sources of information that are simultaneously impinging on the system. It is helpful to view a network as a multi-dimensional state space or energy landscape. The precise contours of the landscape vary with the environment in which the network finds itself. Adaptation in the network can be likened to the process of gradient descent. Final behaviour is determined by the parameters corresponding to the lowest level or valley in the energy landscape. Since the landscape varies from one environment to another, output behaviour will accommodate accordingly.</p>
<h1>Historical Roots</h1>
<p>PDP models demonstrate many characteristics that are desirable in simulations of human cognition. Though individual properties can be found in other approaches, it is rare to find a single system embracing so many important features. Proponents of the PDP approach regard the parsimony of</p>
<p>their models as heralding a new era in the study of cognition. However, Parallel Distributed Processing builds upon a long tradition of scientific endeavour which dates back at least as far as the British Empiricist David Hume. Many of the ideas embodied in the PDP approach can be found in writings of William James.</p>
<p>The amount of activity at any given point in the brain-cortex is the sum of the tendencies of all other points to discharge into it, such tendencies being proportional (1) to the number of times the excitement of each other point may have accompanied that of the point in question; (2) to the intensity of such excite. ments; and (3) to the absence of any rival point functionally disconnected with the first point, into which the discharges might be diverted. (James, 1892, p. 265).</p>
<p>Theoretical developments that laid the foundations for many of today's models were already underway in the forties and fifties (Hebb, 1949; McCulloch \&amp; Pitts, 1943; Rosenblatt, 1959). Yet PDP has emerged as a new perspective only within the last six or seven years (Hinton \&amp; Anderson, 1981; Rumelhart \&amp; McClelland, 1986a). To understand the relationship between PDP and earlier, related approaches, it is necessary to review some of the core concepts of PDP. This brief review will also serve as an introduction to the more sophisticated simulations described below.</p>
<p>The heart of a parallel distributed processing system is a neural network. A neural network consists of a collection of units connected to each other by a set of pathways. Figure one illustrates an example of a simple network designed to solve the logical OR problem (see below).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure One</p>
<p>The circles represent the units or constitutive nodes of the network and the solid lines indicate the pattern of connectivity between the units. Units take on a variety of activation values that can depend on some function of the net input to a unit from other units, the previous state of the unit itself and input from the external environment. The activation value of a unit determines its</p>
<p>output. A variety of functions of the activation value are typically used to determine the output from a unit: A squashing function constrains the output within certain limits: A threshold function determines which of a limited set of outputs will be produced: A linear function simply passes on the activation value itself. Networks may be homogeneous in that all units use the same output function or they may be heterogeneous in the output functions used. Note that it is unusual to find homogeneous nets of linear units. The range of tasks that such networks can perform is limited and they tend to be unstable (activation values have a potential to explode to enormously high values in linear systems). On the other hand, the input function that maps the net input to a unit onto its activation value is typically linear.</p>
<p>Units communicate with each other by passing their output values to the other units in the system with which they are connected. Connections are almost always unidirectional. The input/output functions of the units, together with their pattern of connectivity, define the architecture of the network. Units can excite or inhibit each other. Each connection may have a positive or negative real value that determines the degree and direction of influence of the source unit on the target unit. Target units may receive inputs from a large number of source units. The strength of the connection between two units is called the weight of the connection. The product of the output value of the source unit and the weight value between the source unit and the target unit determines the contribution of the source unit to the net input to the target unit. If the product is negative then the source unit inhibits the target unit. If the product is positive then the source unit excites the target unit. Some systems treat excitatory input and inhibitory input independently from one another (Grossberg, 1980). However, all the models described in this paper treat the two types of input homogeneously. A zero weight between two units is functionally equivalent to a lack of connection between those units in most systems.</p>
<p>The overall behaviour of the network is determined by the set of weights that define the pattern of connectivity of the system, and by the input of the system. The set of weight values embodies the knowledge of the system with respect to a given set of environmental stimuli. In figure one, the numbers on the solid lines represent the weights of the connections between the various pairs of units. The system consists of two inputs and a single output unit. The input units are linear and simply pass on the values received from the environment to the output unit. The output unit is a linear threshold unit i.e. if the activation value reaches or exceeds a given value, in this case » 1 «, then it outputs » 1 «, otherwise it outputs $» 0$ «. Logical OR can be represented by the following truth table:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LOGICAL»OR«</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LOGICAL»AND«</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">input</td>
<td style="text-align: center;">output</td>
<td style="text-align: center;">input</td>
<td style="text-align: center;">output</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">00</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">01</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>A similar network to that depicted in figure one can represent the boolean function Logical AND. The only change that needs to be made is that the threshold of the output unit must be changed to $» 2$ «.</p>
<p>Networks like these have been investigated since the forties (McCulloch and Pitts, 1943) and are typical, though simplified, versions of the neural nets that originally sparked off interest in the area. Larger nets with greater numbers of input and output units can represent more complicated input/ output algorithms. However, their mode of operation is essentially identical to the network described above; input patterns are mapped onto a set of output units via weighted connections. The activation values of the output units, together with their threshold values, determine the resultant pattern of output activity. It is also possible to make networks like these learn, i.e. given an input and a desired output it is possible to manipulate the value of the weights automatically so that the required input/output function is achieved. The learning algorithm originally used by Rosenblatt (1962) is called the Perceptron Learning Rule. According to this rule, the weights between two units are modified if the desired output differs from the actual output. The desired output is determined by a »teacher« signal. The activation value is determined by the propagation of the input signal through the network. The actual output on each output unit is then compared to the target specified for that unit in the teacher signal. If there is a mismatch between these two values, then all the weighted connections feeding into the given output unit are adjusted according to the following rule:</p>
<p>$$
\Delta w_{i}=\left(t_{p}-o_{p}\right) i_{p i}=\bar{\partial}<em i="i" p="p">{p} i</em>
$$</p>
<p>where $\Delta w_{i}$ specifies the change in weight on the connection from input unit to output unit $p, t_{p}$ specifies the teacher signal for output unit $p, o_{p}$ specifies the actual signal on output unit $p$, and $i_{p i}$ is the output from input unit $i$. The change in threshold on the output unit is given by the following rule:</p>
<p>$$
\Delta \hat{\theta}=-\left(t_{p}-o_{p}\right)=-\bar{\partial}_{p}
$$</p>
<p>This procedure is guaranteed to find a set of weights that produces the correct input/output mappings, provided such a set of weights exists. The Perceptron Learning procedure can be applied to a surprisingly wide range of pro-</p>
<p>blems and is still used in many influential models. However, as Minsky and Papert (1969) pointed out, perceptrons are still quite limited in the class of input/output mappings they can learn.</p>
<p>In particular, perceptrons are unable to solve the Exclusive OR problem. The truth table for Exclusive OR is given below:</p>
<p>EXCLUSIVE»OR«</p>
<table>
<thead>
<tr>
<th style="text-align: center;">input</th>
<th style="text-align: center;">output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Exclusive OR demands a non-linear partitioning of the problem space (figure two).
<img alt="img-1.jpeg" src="img-1.jpeg" />
<img alt="img-2.jpeg" src="img-2.jpeg" />
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure Two
Geometric representations of the three problems</p>
<p>It is possible for networks to learn a non-linear partitioning only when there is an intermediate level of inhibitory units between the input and output units. The Perceptron Learning rule provides only for the adjustment of weights directly connecting the input and output units. Therefore, perceptrons cannot perform non-linear classifications. Since it is known that the class of problems to which Exclusive OR belongs is common in computer science, the demise of neural net models based on the Perceptron Learning procedure followed swiftly on the publication of Minsky and Papert's book. Like spreading excitation in semantic nets, neural network research in general went out of fashion. For many years, the dominant symbolic approach to computational psychology reigned supreme.</p>
<p>Though neural net research receded into the background in the late sixties and seventies, the effects of Minsky and Papert's critique were as much so-</p>
<p>ciological as scientific (Papert, 1988). Perceptrons represent only a small class of possible network architectures and learning procedures and it was to this small class of networks alone that Minsky and Papert's comments were addressed. However, Minsky and Papert's critique was mistakenly interpreted as applying to neural nets in general and funding for most neural net projects dried up. Nevertheless, during the dark years, a small group of researchers continued to investigate the properties of different neural nets and extend their domain of application. J.A. Anderson (1977), Grossburg (1976) and Kohonen (1977) are notable contributors amongst this group. Many of the principles and insights embodied in PDP research today are due to the sustained efforts of this small group of researchers.</p>
<p>As some cognitive scientists became increasingly dissatisfied with the achievements of the traditional symbolic approach to computational psychology, it was recognised that neural nets possessed precisely the kind of properties which traditional symbolic models seem to lack. Furthermore, cognitive scientists began to recognise that various mathematical tools could help extend the generality of the Perceptron Learning procedure to a greater variety of problems, including Exclusive OR. For example, Rumelhart, Hinton and Williams (1986) describe a learning algorithm called Back Propagation (also known as the Generalised Delta-rule), which specifies a procedure for manipulating the weights in a multi-layered network. Back propagation can be used to solve the Exclusive OR problem. Figure three depicts an example of a network that implements the Exclusive OR truth table above.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure Three
A network for solving Exclusive OR</p>
<p>Notice that the network in figure three contains a layer of units between the two input units and the single output unit. Any units in a network that are not exposed to the external environment but only to other units in the network are called hidden units. Back propagation ${ }^{2}$ provides a method for adjusting the threshold values of these hidden units as well as the weighted connections between hidden units and visible units (visible units receive input directly from the environment or send output directly to the environment).</p>
<p>Recent work with networks using the back propagation algorithm has shown that hidden units often organise themselves in a way that reflects the structure of the problem at hand. For example, Hinton (1986) describes a back propagation network designed to learn kinship relationships. Hidden units in this network organise themselves to represent the salient dimensions in kinship relationships, such as gender and generation. Sometimes, the representations discovered by hidden units parallel human interpretations of a problem. At other times, the hidden units discover partitions of problem space not obvious to humans. For example, a back propagation network discovered a novel solution to the Exclusive OR problem. Hidden units have also been used to filter out the redundancies in an input signal, compressing the information for later retrieval (Ackley, Hinton \&amp; Sejnowski, 1985). The ability of hidden units to extract and represent regularities of the environment to which they are exposed has triggered a controversial discussion of the nature of representation in neural nets. Hidden units have no referential function, and yet they seem to share some properties with the symbolic entities embodied in traditional rule-based accounts of cognition.</p>
<p>Another important refinement in neural network architectures has been the development of the Boltzmann machine. The Boltzmann machine belongs to a class of constraints satisfaction networks that are capable of finding solutions to problems which require the simultaneous fulfilment of a selected set of criteria. For example, the process of language comprehension may be reviewed as a constraint satisfaction problem in which the various component parts of a sentence must be integrated to obtain a coherent interpretation (McClelland \&amp; Kawamoto, 1986). These criteria may be mutually supportive or they may be in competition with each other. In the former case, interactive constraint satisfaction networks quickly converge on a best solution. However, if the criteria are in competition with each other, a network may possess a variety of final stable states. Some of these states (often called local minima) may underestimate the potential of the network to solve the problem at hand. Boltzmann learning is a technique for avoiding these local minima (see figure four) and finding the best possible solution to the problem in a given network.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure Four
Local minimum (A) in an energy landscape
B</p>
<p>Boltzmann learning is a stochastic process inspired by statistical mechanics. Just as misaligned crystals or metal can be realigned by a gradual process of heating and cooling called annealing, so can a network be made to explore the energy landscape of a given problem space until it finds a best possible solution. This process is called simulated annealing. As we shall see in the next section, constraint satisfaction networks are particularly useful for simulating psychological phenomena that involve the disambiguation of situations that demand interpretation under the control of contextual determinants. In contrast to feed-forward, back propagation networks, constraint satisfaction networks are typically more highly interconnected. Such networks need not have an obvious layered-structure and many units may be visible to the environment.</p>
<p>More recently, network architectures have been introduced that attempt to integrate time as a dynamic dimension in the representations embodied within a network. These networks involve the use of recurrent connections (units that feed back on themselves). Recurrent connections permit a network to maintain an image of its previous states. Since these recurrent connections can themselves be manipulated by a variety of learning algorithms, such networks can develop a capacity to predict or control future events. For example, Jordan (1986) describes a system which attempts to deal with the classical problem of serial order in behaviour (Lashley, 1951) through the parallel, distributed implementation of a planning structure. Similarly, Elman (1988) shows how a recurrent network can be used to capture some syntactic properties of sentences without the explicit specification of grammatical rules. The problem of serial order in behaviour was impressively resolved by symbolic accounts of cognition (Miller, Galanter \&amp; Pribram, 1960). Connectionist accounts of this problem are needed if PDP is to be regarded as a serious alternative to the classical symbolic approach. In the final section of this article, we will return to a comparative evaluation of connectionist and symbolic contributions to our understanding of cognition. In the next section, we turn to a presentation of some concrete examples of connectionist simulations of psychological phenomena.</p>
<h1>Connectionist simulations</h1>
<p>In this section, I shall briefly review three connectionist simulations. Each simulation uses a different network architecture and addresses a different type of psychological phenomenon. The first model describes a simulation of the learning of past tense forms of verbs by young children. The heart of this model is a simple perceptron learning system. The second model shows how a constraint satisfaction network can be used to simulate the different interpretations of a knecker cube. Finally, a recurrent network that is able to recover lexical classes from word-order is described.</p>
<h1>Learning the past tense of verbs</h1>
<p>A founding assumption of research in child language is that children, like adults, use language productively. That is, after the initial phases of learning, language usage cannot simply result from mimicking what is heard in the input, but rather children acquire the ability to generate syntactic and morphological combinations that they could never have heard before (Ervin, 1964; Berko, 1958). From most current perspectives, linguistic knowledge is framed in terms of general principles, i.e. »rules«, which govern the productive and (sometimes) interestingly innovative usage of language. The goal for the acquisitionist has been to work through the various domains of language, outlining how and when children come to master the systems of rules governing the production and comprehension of novel forms and utterances. However, current perspectives also acknowledge that certain pockets of linguistic knowledge do not appear to be rule-governed in the same sense. For example, about 150 or so of the most frequently used verbs in English fall outside the domain of the regular past tense rule, »add-ed« to the stem. Irregular, or strong verbs (Pinker and Prince, 1988), do not form their past tenses by applying a suffixation process, but rather are memorised as separate lexical entries. These verbs can be grouped into three general categories according to the relationship they exhibit to their present tense form: ${ }^{3}$ (a) identity mapping (or no marking - doing nothing to the stem, e.g., hit $--&gt;$ hit); (b) vowel change (changing the vowel, e.g., come $--&gt;$ came); (c) arbitrary (there is no obvious structural relationship between the present and past tense form, e.g., go $--&gt;$ went).</p>
<p>Across acquisition, the fact that two different types of verbs coexist in the lexicon, one group using a general rule and others not, sometimes presents problems for the language learner. In both naturalistic and experimental contexts, children frequently exploit the regularities of the past tense system, applying a general rule to the irregular »exceptions« to the rule (e.g., Bybee and Slobin, 1982; Kuczaj, 1977). Children will produce errors such as "goed« or »sitted« in which the regular »add-ed« ending is applied to verb stems which, in the adult grammar, are not subject to this procedure. In addition, several researchers have noted that the time course in the acquisition of these rules (and their exceptions) suggests that children will get worse in their production of forms before they get better. That is, children make mistakes on some types of past tense forms (e.g., comed) after they have been using the forms correctly (e.g., came). After some extended period of overapplication of the general rule to irregular verbs, children reorganise their lexical categorisations and settle into the correct set or irregular stems and corresponding past tense forms, using both regular and irregular verbs in the past tense. This characterisation of the acquisition of morphological regularities (and irregularities) has been described as a »U-shaped« developmental course in which children pass through two stages before attaining adult com-</p>
<p>petence in handling the past tense in English (Pinker \&amp; Prince, 1988).
Because these erroneous forms are present only infrequently in the input to children, their timely avoidance of »exceptions« has been taken as the most convincing piece of behavioural evidence that language learning involves the process of recognising and organising linguistic knowledge into a coherent system of general rules. Why else would children produce erroneous forms such as the overgeneralisations of the »add-ed« rule? Acquisiton, then, involves the construction of a system of generalised statements about the structure of the lexicon, and the accompanying lists of exceptions to those general rules. Traditionally, the notion of a »rule« has provided linguists and psychologists with an elegant means to neatly package what children and adults »know« about the intricacies and complexities of language while at the same time accounting for productive language use. As noted recently by Pinker and Prince (1988),
»{language} researchers who could agree on little else have all agreed on one thing: that linguistic knowledge is couched in the form of rules and principles« (pg. 74).</p>
<p>Indeed, invoking rules serves to elevate language learning above the level of rote imitation and allows linguists »to factor a complex phenomena into simpler components that feed representations into one another« (Ibid. pg. 84).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure Five</p>
<p>Recently, work within the PDP perspective has promoted reevaluations of the constructs and processes guiding language acquisition. In an attempt to illustrate the applicability of parallel systems to traditionally rule-based domains, Rumelhart \&amp; McClelland (1986b) set out to capture several of the »facts« of the acquisition of the English past tense; in particular, that children overgeneralise the »add-ed« suffix to irregular verbs and that development of this system proceeds along a »U-shaped« course. The goal of this work was to suggest how an account of language processing and acquisition might be able to avoid the reliance on symbol-manipulating, rule-governed processes, while still capturing these phenomena of acquisition. Rumelhart and McClelland's past tense simulation contains three major components. (See figure five).</p>
<p>First, an encoding devide takes the present tense stems of English verbs, symbolised as binary characters, and converts each stem into a distributed representation of context sensitive phonological features called Wickelfeatures. Output from the encoding device consists of a vector of activation across the set of output units ( 460 in all). ${ }^{4}$ Secondly, a single-layered, pattern association network maps the set of Wickelfeatures, which it takes as input, onto a set of output units. The activity on these output units constitute the Wickelfeature representation of the past tense form of the verb that was originally presented to the simulator in its present tense form. The task of the pattern association network is to learn to map correctly input to output vectors through adjusting the set of weights which connect the input and output units. The adjustment of the weights is achieved by using the error signal obtained from comparing the actual output of the network with the desired output stipulated by a teacher signal. The weights connecting the input and output units of the network are then adjusted using the Perceptron Learning rule. This second component of the simulator is entirely responsible for the learning that is required to map present tense stems of verbs onto their corresponding past tense forms. This mechanism, then, can be seen to be the foundation for the overgeneralisations reported by Rumelhart \&amp; McClelland. The third component of the simulator takes as its input the vector representing the activity of the output units of the pattern associator. Its function is to generate the set of Wickelphones that best fit the output vector description. In principle, the decoder provides a Wickelphone description of the past tense form of the verb that was originally provided in the Wickelphone representation of the present tense stem to the encoder. Several researchers as well as Rumelhart \&amp; McClelland themselves have acknowledged several difficulties with this type of decoding process (Pinker \&amp; Prince, 1988). The usefulness of Wickelfeatures as a technique for encoding linguistic information in networks of this types is not crucial for the issues discussed in this paper, and the reader is referred to the original source for details.</p>
<p>The results of the Rumelhart \&amp; McClelland simulation are important for several reasons. First, within some reasonable limits, the learning curves and overgeneralisations created by the simulation resemble many of the errors and stages of development that children are reported to make in the acquisition of past tense verb forms.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure Six
The percentage of correct features for regular and irregular high-frequency verbs as a function of trials</p>
<p>Figure six shows the »U-shaped« dip for irregular verbs during the early stages of learning. This regression represents the stage of learning in which irregular verbs are treated as though they are regulars. Even more impressively, Rumelhart \&amp; McClelland's simulation provides distinct learning curves for the different classes of irregulars which closely map the types and relative timing of errors made by young children. For example Kuczaj (1977) reports that past tense errors of the form »ated« occur later in development than errors which simply »add-ed« to the present tense stem (»eated«). Rumelhart \&amp; McClelland's simulator is also delayed in producing these former types of error.</p>
<p>The excitement engendered in the cognitive science community by Rumelhart \&amp; McClelland's simulation was partly to do with the accuracy with which it seemed to be able to mimic young children's behaviour. More importantly, however, the simulation showed how one might construct a developmental model that could acquire linguistic knowledge without assuming much nativist baggage. In particular, the simulation does not rely on rules in any obvious way. Furthermore, the simulation embodied an account of the process of morphological reorganisation which is assumed to be crucial to</p>
<p>the achievement of mature linguistic skills (Bowerman, 1982). The model achieves this bt playing off the learning properties of the pattern association network against the structural relationships implicit in the information it must process, i.e. the regularities of English verb morphology.</p>
<p>The past tense simulation has been severely criticised. In addition to their criticism of the Wickelfeature representation used by Rumelhart \&amp; McClelland, Pinker \&amp; Prince (1988) question the input assumptions of the simulation and point out that the U -shaped developmental curve followed by the simulator is an artifact of the discontinuity in vocabulary size and structure to which the model is exposed. For example, the performance dip for irregular verbs (figure six) occurs just after the proportion of regular to irregular transformations in the training set has been greatly increased. However, Plunkett \&amp; Marchman (in press) show that »U-shaped« learning can be achieved with continuous input to a back propagation network, provided certain realistic assumptions are made about the relative frequencies of the different classes of verbs. Furthermore, the Plunkett \&amp; Marchman simulation provides information as to the conditions under which such a network acts as if it is learning a set of rules and the conditions when behaviour is less categorical. Despite the qualms of classical symbol theorists, the PDP approach still holds out the promise of an alternative developmental account for acquisition and the potential for a new approach to language processing.</p>
<h1>Interpreting the Necker cube</h1>
<p>Constructivist accounts of human perception often cite the shifting interpretations of the necker cube as testifying to the top-down nature of cognitive processes. The necker cube can be interpreted in different ways because we are able to project distinct orientation models onto one and the same stimulus set. On this view, the interpretation of a necker cube is contingent upon the construction of an internal representation. More recently (Feldman, 1981 and Rumelhart, Smolensky, McClelland \&amp; Hinton, 1986), connectionists have shown how the orientation of a necker cube can be computed through the interaction of mental models and bottom-up visual information in a constraint satisfaction network. Figure 7 provides a summary of some of the constraints involved in perceiving the necker cube.</p>
<p>The bottom part of the figure illustrates the necker cube itself whilst the top part of the figure illustrates two interconnected networks, each representing alternative interpretations of the necker cube. Each unit in the network represents a hypothesis about the correct interpretation of a vertex of a necker cube. For example, the unit in the lower left-hand part of the network represents the hypothesis that the lower left-hand vertex of the drawing is a front lower left (FLL) vertex. The upper right-hand unit of the network represents</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure Seven
the hypothesis that the upper right-hand vertex of the necker cube represents a front upper right (FUR) vertex. Notice that both these interpretations are inconsistent. Normally, only one of those vertices can occupy concurrently the frontal plane of the cube at a time. Figure 7 also illustrates the different types of constraints operating in the network. Since each vertex can have only one interpretation, alternative interpretations of the same vertex are connected by inhibitory weights (BUL - - -&gt; FUL). Similarly, since the same interpretation cannot be given to more than one vertex, units representing the same interpretation are mutually inhibiting (BLL - - -&gt; BLL). Thirdly, units that represent locally consistent interpretations are mutually exciting (FUL - - -&gt; FLL). Without these excitatory and inhibitory constraints, it is extremely unlikely that the network will find a solution corresponding to the correct interpretation of the necker cube (all the units in one sub-network turned on and all the units in the other sub-network turned off). In fact, without any constraints whatsoever, there are in principle $2^{16}$ possible configurations of the network. However, once constraints are introduced to the system, the likehood of many of these states occurring is considerably reduced. Hopfield (1982) has shown that the behaviour of constraint satisfaction networks can be characterised as a process of hill-climbing ${ }^{5}$ on a goodness contour. Since many of the units in a constraint satisfaction</p>
<p>network, like the one depicted in Figure 7 above, are in competition with each other, it is possible to evaluate any given configuration of the network as having a particular goodness of fit. Goodness of fit depends essentially on the extent to which each unit satisfies the constraints imposed upon it by other units, the likehood of an individual unit turning on itself (bias), and finally direct evidence from the input that suggests a given unit should turn on. We can summarise the goodness of fit in a constraint satisfaction network for all units in the network in the following equation:</p>
<p>Goodness $=\Sigma_{\mathrm{ij}} \mathrm{w}<em _mathrm_i="\mathrm{i">{\mathrm{ij}} \mathrm{a}</em>}} \mathrm{a<em _mathrm_i="\mathrm{i">{\mathrm{j}}+\Sigma</em>$ input $}<em _mathrm_i="\mathrm{i">{\mathrm{i}} \mathrm{a}</em>$ bias $}}+\Sigma_{\mathrm{i}<em _mathrm_i="\mathrm{i">{\mathrm{i}} \mathrm{a}</em>$
where $\mathrm{w}_{\mathrm{ij}}$ represents the weight connecting unit j to unit i , and $\gg \mathrm{a}$ " represents the activation of a given unit.}</p>
<p>Given this equation and an algorithm that stipulates an updating procedure for the units in a network, one can describe an energy landscape or goodness contour that corresponds to the various configurations of the network. In this way, local computational operations, in which each unit adjusts its activation up or down on the basis of its net input, serve to allow the network to converge towards states that maximise a global measure of goodness or degree of constraint satisfaction. For example, assume that a network like that depicted in figure 7 , with the stipulated set of excitatory and inhibitory connections, is provided with a set of randomly assigned biases on each of the 16 units. A bias determines the probability that a given unit will turn on or off. If the network is allowed to run for a succession of discrete times steps, the activation values of each of the units will adjust themselves in such a way as to relax into a stable global state or maximum goodness of fit for the network.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure Eight</p>
<p>Figure 8 depicts a goodness contour for the possible configurations of the necker cube network. We may interpret this picture as a visual conceptualisation of the possible states of the network. The low point $(0,0)$ corner, corresponds to the start state in which no units are turned on. The peaks on the left and right correspond to the standard interpretations of the necker cube. These goodness peaks are the states to which the network will most often be attracted in the relaxation process. The choice between interpretations is determined by the position on the goodness contour at which the network starts and the particular sequence of updates that is chosen for the units in the network. Thus, it is possible to push the network to a particular interpretation of the necker cube by giving a large bias to one or more of the units. For example, notice that the goodness contour in figure 8 contains a number of smaller peaks. These peaks represent impossible interpretations of the necker cube such as that depicted in figure 9 in which two surfaces are interpreted as being foremost.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure Nine
Three interpretations of the necker cube</p>
<p>Since these peaks represent local maxima on the goodness contour, the hillclimbing process halts and the network remains in this stable, though impossible, state.</p>
<p>Dynamic accounts of the interpretative process show us how a single ambiguous source of information can be resolved into a definite single solution. The goodness or energy landscape provides a global picture of the characteristics of any given constraint satisfaction network. The process of hillclimbing describes the process of change and conflict resolution within the network. Note how the energy landscape is molded by reference to stable states of the network. We may conceptualise these stable states, visualised as peaks in the landscape, as default configurations which the network will move towards in the absence of any conflicting information. Thus, constraint satisfaction networks can be seen to implement representational entities often referred to in the literature as frames (Minsky, 1975) or scripts (Schank \&amp; Abelson, 1977). The mutual constraints within a network and the environment (external input) in which the network finds itself interact to mold a dy-</p>
<p>namic, context sensitive energy landscape. Global maxima in this landscape correspond to prototypical resolutions of the constraint satisfaction problem. Local maxima correspond to intermediate solutions, reflecting competition between constraints. However, local maxima need not represent impossible solutions as in the case of the necker cube. They may equally well represent unorthodox but acceptable configurations of a given frame or script. In this fashion, the continuous nature of the energy landscape provides a foundation for the non-categorial forms of behaviour typical of these networks. Furthermore, stable configurations are achieved on the basis of local computations. No symbolic executive supervises the relaxation process.</p>
<h1>Discovering lexical classes from word-order</h1>
<p>The determination of word-order in an utterance is known to reflect a variety of constraints such as syntactic structure, selectional restrictions, subcategorisation and discourse considerations. Traditionally, psycholinguistic accounts of language production and comprehension have invoked symbolic processing systems to express the abstract structural relationships between words in an utterance. For example, in the sixties, psycholinguistics was dominated by the view that language processing involved some psychological implementation of transformational grammar (Fodor, Bever \&amp; Garrett, 1974). Although this approach turned out to be incorrect, the procedurally oriented theories (e.g. Miller \&amp; Johnson-Laird, 1976) which took over, are still symbolically based. Indeed, it has been argued (Fodor \&amp; Pylyshyn, 1988) that a symbolic level of representation is a necessary foundation for psycholinguistic processing. On this view, neural nets are capable of capturing only the most trivial structural relationships found between the words in an utterance. Neural nets fail in precisely the same way that Markov grammars failed to provide an account of linguistic structure earlier this century. As a first attempt to answer this challenge, Elman (1988) used a recurrent network to simulate word-order prediction. As we shall see, his network was able to assign lexical items to their correct grammatical category and to predict appropriate category orderings in the output from the network.</p>
<p>Earlier in this paper, we saw that recurrent networks can maintain an image of their previous states and so develop the capacity to predict future events. The architecture of the recurrent network used by Elman is shown in figure 10. Notice that figure 10, like the network for solving Exclusive OR, contains a layer of hidden units. Thus, the network possesses the capacity to extract regularities from the input patterns and construct internal representations thereof. In addition to the layer of hidden units, Elman's network contains a set of context units. The hidden units and the context units are equal in number. However, the context units can only communicate with the hidden layer. The weights cconnecting the hidden units to the context units are fixed</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure Ten
and constitute a one-to-one mapping. In effect, on every time-step the context units establish a copy of the previous states of the hidden units. The context units are connected to the hidden units in a one-to-many mapping. The dotted line connecting the context units to the hidden units indicates that the weights are adjustable. The context units display an image of the previous states of the hidden units to the hidden units themselves. In this way, Elman's recurrent network goes beyond the finite state Markov grammar. The recurrent connections, through the context units, provide the system with an indefinitely embedded representation of previous states of the network. The context units act as a contextual memory for the network.</p>
<p>The task which Elman gives the network is fairly straightforward. A word is presented to the input units. In response, the network must predict which word, taken from a previously constructed list, will be presented next to the input units. The network shows that it can predict the next word in the list by generating the word on the output units. During the training phase, a teacher signal provides feedback to the network. Errors are propagated backwards through the network using the Generalised Delta rule. The previously</p>
<p>constructed list consists of sequences of sentences generated by a simple phrase structure grammar. The list contains 10,000 two- or three-word sentences. Each word is represented by a unique random binary string. Thus, if the first sentence presented to the network is »woman smash plate«, the first two responses of the network should be »smash« and »plate«. Elman trained the network for five complete passes through the data set. He discovered that the absolute level of predictive performance was not very good, indicating that the network had failed to memorise the sequence of words. However, after the training phase, the connections in the network were frozen and the individual words used in the grammar presented to the input units,
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>one at a time. Instead of recording the output from the network, Elman recorded the activity across the hidden units (represented as a vector) for each unique word in each of its sentential contexts. The hidden unit activations produced by a given word (in all its contexts) were averaged to yield a single 50 -bit vector for each of the 35 unique words in the input stream. These internal representations were then subjected to a hierarchical clustering analysis.</p>
<p>Figure 11 shows the resulting tree; this tree reflects the similarity structure of the internal representations of these lexical items as perceived by the network. Lexical items which have similar properties are grouped together lower in the tree, and clusters of similar words which resemble other classes are connected higher in the tree. It is clear that the similarity structure depicted in the tree diagram reflects our human intuitions about grammatical similarity between the words which the network knows about. Thus, verbs are grouped together on one branch of the tree whilst nouns have been grouped together on another branch. Although the network has been told nothing about the semantics of the nouns presented, the cluster analysis reveals that the network has discovered appropriate semantic classifications of the words. For example, inanimate objects are distinguished from animate objects and humans are distinguished from small animals. Elman notes, »that this hierachy is implicit in the similarity structure of the representations, and not an explicit function of the architecture. The network does not have available any of the symbolic apparatus of semantic networks or tree structures.« (pg. 20). He also goes on to point out that the apparent semantic knowledge of the network is an illusory. The network has no knowledge of the meaning of words since each word is represented by a random binary string. It is simply the case that the network is able to classify the different words on the basis of their very similar behaviour with regard to serial order. However, one can imagine real language learners making use of the cues provided by word-order to make intelligent guesses about the meanings of novel words. This simulation suggests how distributional information in the input might be exploited by the learner without couching this knowledge in terms of explicit rules.</p>
<p>Elman admits that the structural relationships implicitly represented in the network in this simulation do not reflect a full-blown grammar of English! However, in a later set of simulations and using a similar network architecture, Elman addresses the problem of pronominal reference. For example, consider the following sentences:
a) If $\mathrm{Leo}<em _mathrm_i="\mathrm{i">{\mathrm{i}}$ wants, $\mathrm{he}</em>$ will attend the meeting.
b) If he wants, $\mathrm{Leo}}<em _mathrm_i="\mathrm{i">{\mathrm{i}}$ will attend the meeting.
c) $\mathrm{Leo}</em>}}$ will attend the meeting if he $\mathrm{e<em _mathrm_i="\mathrm{i">{\mathrm{i}}$ wants.
d) $\mathrm{He}</em>$ wants.}}$ will attend the meeting if $\mathrm{Leo}_{\mathrm{j}</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-454 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-454</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-454</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-49215805</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1806.05219v2.pdf" target="_blank">Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis</a></p>
                <p><strong>Paper Abstract:</strong> Lack of repeatability and generalisability are two significant threats to continuing scientific development in Natural Language Processing. Language models and learning methods are so complex that scientific conference papers no longer contain enough space for the technical depth required for replication or reproduction. Taking Target Dependent Sentiment Analysis as a case study, we show how recent work in the field has not consistently released code, or described settings for learning methods in enough detail, and lacks comparability and generalisability in train, test or validation data. To investigate generalisability and to enable state of the art comparative evaluations, we carry out the first reproduction studies of three groups of complementary methods and perform the first large-scale mass evaluation on six different English datasets. Reflecting on our experiences, we recommend that future replication or reproduction experiments should always consider a variety of datasets alongside documenting and releasing their methods and published code in order to minimise the barriers to both repeatability and generalisability. We have released our code with a model zoo on GitHub with Jupyter Notebooks to aid understanding and full documentation, and we recommend that others do the same with their papers at submission time through an anonymised GitHub account.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e454.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e454.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code_availability_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing, removed, or inaccessible code and resources</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents instances where authors did not release code or removed it later, or where released resources (embeddings) were unavailable, impeding replication and causing differing results when others reimplemented the methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TDSA experimental codebase and resource distribution</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Research code, pretrained embeddings, and associated artifacts that implement published Target Dependent Sentiment Analysis (TDSA) methods and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / resource listing</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>published code repository / released embeddings and implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing or unavailable code/resources</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some prior TDSA papers did not release code or later removed it; trained word embeddings used by the original works were not released or links were dead, forcing reimplementers to request code from later authors or substitute resources. This lack of availability meant reproductions used different artifacts or had to reimplement from underspecified descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>external resources and implementation availability (pretrained embeddings, published code repositories)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility study and attempts to obtain resources (requesting code, following links), comparison of available resources to those referenced</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative count and provenance checking (papers reporting 'code not released', 'link broken'); comparison of experimental results when using substitute resources versus original reported results</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevented direct replication and forced use of substitute embeddings or reimplementations; contributed to differing experimental outcomes across groups (examples: when Tang et al. code/embeddings were used by Chen et al. different results were observed). Quantitative differences depend on substituted resources (see other gaps), but inability to access original embeddings/code was a primary barrier to reproducing exact reported scores.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in multiple TDSA papers; authors note 'recent TDSA papers have generally been very good ... but other papers have not released code' and instances of code being removed then returned were reported; SemEval 2016 task 5 example: only 1 of 21 papers released source code.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>authors not releasing artifacts at publication, broken links, lack of archival practices for code/resources</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Release code, embeddings and data alongside papers; release a model zoo and Jupyter Notebooks; provide anonymised GitHub at submission time and persistent hosting for resources.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Paper authors applied mitigation in their own work (released model zoo and notebooks) which enabled their reproductions; concrete effectiveness measured qualitatively (allowed reproduction and mass evaluation). No large-scale quantitative measure provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural Language Processing / Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e454.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e454.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>undocumented_preprocessing_scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Undocumented preprocessing and feature scaling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original papers often omitted preprocessing choices (tokenization details, case-folding) and whether features were scaled before classification; these omissions materially changed reproduced results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TDSA preprocessing and feature pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Text preprocessing pipeline (lowercasing, tokenization), feature extraction (neural pooling), and feature scaling prior to SVM classification.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (preprocessing and model training descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reimplementation scripts (scikit-learn LinearSVC, feature scaling code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing specification / hyperparameter mismatch (feature scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Vo and Zhang (2015) did not state that features were scaled; LibLinear suggests scaling but the omission in the paper led reimplementers to either scale or not scale arbitrarily. The reproduction used Max-Min scaling (scikit-learn) and showed that non-scaling reduced results substantially. Tokenization choice (Twokenizer) was assumed but not always explicitly documented.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing and feature scaling prior to classifier training</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproduction experiments with/without scaling and with specified tokenization; comparison of reproduced scores to original reported scores</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>empirical comparison of accuracy/F1 with and without scaling; the paper reports scaling vs not scaling affecting results 'by around one-third' (figure 2 commentary) for NP methods and shows numeric differences in reproduced vs original tables.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large — not scaling affected results by ~33% (text reports 'affect the results by around one-third'); reproduced performance ranks and absolute accuracies changed when scaling was applied versus omitted (e.g., differences observed in Figure 2 and Table 4 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common; authors state scaling 'is typically overlooked when reporting' and multiple reproduced methods were sensitive to this choice.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>implicit assumptions (authors relying on default library guidance) and omission of preprocessing details in paper text</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Document preprocessing steps (tokenization, lowercasing) and explicit statement of feature scaling; include preprocessing code and example pipelines in released repository/Notebooks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in authors' reproductions: applying explicit Max-Min scaling aligned reproduced results closer to expected performance; quantified effect reported (~one-third change).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural Language Processing / Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e454.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e454.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ambiguous_target_occurrence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguous handling of multiple target occurrences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers that require per-target context pooling do not state how to handle multiple occurrences of the same target in a text; implementers made different choices leading to differing feature vectors and results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>target-specific context extraction step</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure that extracts left/right/target contexts for a specified target mention in the input text for neural pooling or parser-based feature extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (context extraction specification)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reimplementation code (context pooling logic)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Vo and Zhang (2015) and other methods require context relative to a target word but do not specify which occurrence to use when the same surface form appears multiple times. The reproduction adopted Wang et al. (2017)'s approach: compute features for each occurrence and median-pool them. This choice can subtly change results compared to the unspecified original.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / context selection for feature extraction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual code review of method descriptions, identification of missing specification, and empirical reimplementation testing with alternative resolutions (per-occurrence pooling).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>empirical comparison of reproduced models with different pooling strategies; reported as an explanatory factor for 'subtle differences' between reproduced and original results (no single numeric delta solely attributed to this choice given confounding factors).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate — cited as a plausible explanation for subtle differences between original and reproduced NP results; contributed to variation in reported accuracies but not isolated as sole cause.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Noted across TDSA methods that depend on per-target contexts; observed in reproductions of at least Vo & Zhang (2015) and noted by Wang et al. (2017).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>ambiguous natural language description failing to specify tie-breaking or aggregation when multiple target mentions exist</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly document choice (e.g., first occurrence, all occurrences with pooling and pooling strategy), include code implementing the chosen strategy in released repository.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors' adoption of median pooling allowed consistent reproductions across datasets; improved clarity and helped explain result differences, though no single numeric effectiveness metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural Language Processing / Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e454.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e454.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>lexicon_ambiguity_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguities in sentiment lexicon usage and counting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper found inconsistencies between reported and computed word counts for sentiment lexicons due to undocumented assumptions about word membership in sentiment classes, demonstrating that small undocumented details change experimental inputs and results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>sentiment lexicon integration in feature pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Process that uses sentiment lexicons (MPQA, NRC, HL) to filter or weight embedding contexts for neural pooling features and lexicon-based contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section and lexicon statistics reporting</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>lexicon parsing and filtering scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous specification (lexicon membership and counting rules)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Vo and Zhang (2015) did not clearly document whether a word could belong to multiple sentiment classes within a lexicon; the reproduction initially assumed exclusivity leading to differing word counts. Correcting this assumption changed lexicon statistics and affected which lexicon combination performed best.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data resources interpretation and preprocessing (lexicon parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual verification of lexicon files versus reported statistics and rerunning experiments with corrected lexicon membership assumptions; comparison of lexicon word counts (Table 3) and downstream results (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Direct comparison of lexicon word counts (original reported vs reproduction counts) and classification accuracy differences when using different lexicon settings; Table 4 shows reproduced accuracies differing by ~1 percentage point and changes in best-performing lexicon ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Small-to-moderate — changed relative effectiveness of lexicons (paper reports best single lexicon differs from original) and produced small absolute accuracy differences (e.g., Target-dep original 65.72% vs reproduction 66.81% in Table 4; lexicon ranking changed).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in this reproduction and likely present whenever papers rely on lexicon resources without explicit parsing rules; not quantified broadly but highlighted as a source of variation.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>incomplete specification of resource parsing rules and implicit assumptions about lexicon structure</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide parsed lexicon files in repository, document membership semantics (allow multi-class entries), and include exact preprocessing scripts that generated the reported counts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective for aligning reproductions: after correcting assumptions reproductions obtained consistent lexicon statistics and clearer comparisons; numeric improvement small (see Table 4) but clarified cause of discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural Language Processing / Sentiment Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e454.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e454.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_hyperparams_and_training_details</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Underspecified hyperparameters, training procedure and ambiguous terms (epochs, clipping threshold)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Key training details (number of epochs, specific clipping thresholds) and initialization/training settings were not fully specified in original papers, requiring choices by reproducers that affected results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>neural network training pipeline (LSTM-based TDSA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The training regimen for LSTM/TCLSTM/TDLSTM models, including weight initialization, optimizer, learning rate, epoch schedule, early stopping, and any gradient or output clipping.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (training procedure)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>neural network training scripts (PyTorch/Theano/TensorFlow or unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous hyperparameter description</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The reproduced LSTM experiments required assumptions for number of epochs and an ambiguous instruction 'set the clipping threshold of softmax layer as 200' (Tang et al., 2016a) which the authors could not interpret. The reproduction used early stopping (patience 10, up to 300 epochs) and uniform initialization U(0.003,0.003) and reported that such choices plus random seed significantly affected results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure and hyperparameters (epochs, clipping, initialization, early stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>attempted reimplementation following paper text, identification of unspecified terms, and empirical evaluation showing sensitivity to these choices; reference to Reimers & Gurevych (2017) about seed effects.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Repeated training runs (30 seeds) and reporting distribution of test results; comparison of mean vs max performance against original reported single-run scores (Table 5 shows original vs reproduction mean and max).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial for deep models — LSTM reproductions showed large variability: example Table 5: original TDLSTM F1 69.00 vs reproduction mean 65.63 (drop ≈3.37 points), reproduction max closer to original; single-run reporting in originals can be misleading. Random seed and unspecified stopping/clipping together led to notable result variation.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common in neural-network based NLP papers; authors cite Reimers & Gurevych (2017) showing seed variability is statistically significant and observe similar effects themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>ambiguous or omitted reporting of training hyperparameters and operational details in paper text</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report multiple runs and seed distributions, document all training choices (initialization, clipping, epochs, early stopping), release training code and random seeds used; use early stopping with described validation split or report exact epoch counts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors ran 30 seeds and reported mean and max, showing that reporting distributions explains variance — maximum reproduced score approached original reported score, while mean was lower. This indicates that reporting multiple runs and seeds is effective at revealing true performance variability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural Language Processing / Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e454.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e454.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>result_discrepancy_with_published_code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Published code yielding different results than paper reports</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Even when code or embeddings are released, other researchers using them have obtained different results than originally reported, indicating mismatches between paper descriptions, code, or hidden dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>published implementations of TDSA methods</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Released implementations (from Tang et al., others) and associated embedding files that should produce the experimental results reported in papers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper results and code release notes</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>published implementation repositories and embedding files</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation-result mismatch / hidden dependency or versioning issue</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper cites cases where researchers using published code or embeddings (e.g., Tang et al. 2016a/b) observed different results than reported in the original papers (Chen et al., Tay et al. reproductions). Possible causes include undocumented preprocessing, library versions, or missing configuration details.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>entire experimental pipeline including preprocessing, training, and evaluation (hidden dependencies/versioning)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>attempts by independent groups to run released code or reuse released embeddings and comparing obtained scores to original published numbers; literature reports of other groups observing mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparison of reported metrics (accuracy, F1) from original papers versus metrics obtained by independent groups using released artifacts; results differences shown in narrative (specific numeric deltas vary by experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Caused confusion and conflicting claims about model effectiveness; contributed to the need for reimplementations and mass evaluations. Quantitatively, some reproduced results diverged by several percentage points (see Table 5 and reproduction narratives), but exact deltas depend on case.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Documented multiple times within TDSA literature; not quantified across the field but presented as a recurring problem in the discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>hidden or undocumented dependencies, differences in preprocessing and runtime environments, missing configuration details, or code changes after publication</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Bundle code with exact environment specifications, provide preprocessing pipelines and versioned dependencies, release notebooks showing end-to-end runs, and include test cases with expected outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>When the authors requested embeddings from later code bases (Wang et al. 2017) they could proceed with reproduction; releasing a full reproducible package (code + notebooks) improved ability to replicate, but the paper does not provide large-scale quantitative effectiveness measures beyond enabling the authors' reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural Language Processing / Software Engineering for ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Target-dependent twitter sentiment classification with rich automatic features <em>(Rating: 2)</em></li>
                <li>Effective lstms for target-dependent sentiment classification <em>(Rating: 2)</em></li>
                <li>Tdparse: Multi-target-specific sentiment recognition on twitter <em>(Rating: 2)</em></li>
                <li>Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging <em>(Rating: 2)</em></li>
                <li>Replication issues in syntax-based aspect extraction for opinion mining <em>(Rating: 1)</em></li>
                <li>A note on rigour and replicability <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-454",
    "paper_id": "paper-49215805",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "code_availability_gap",
            "name_full": "Missing, removed, or inaccessible code and resources",
            "brief_description": "The paper documents instances where authors did not release code or removed it later, or where released resources (embeddings) were unavailable, impeding replication and causing differing results when others reimplemented the methods.",
            "citation_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
            "mention_or_use": "use",
            "system_name": "TDSA experimental codebase and resource distribution",
            "system_description": "Research code, pretrained embeddings, and associated artifacts that implement published Target Dependent Sentiment Analysis (TDSA) methods and experiments.",
            "nl_description_type": "research paper methods section / resource listing",
            "code_implementation_type": "published code repository / released embeddings and implementations",
            "gap_type": "missing or unavailable code/resources",
            "gap_description": "Some prior TDSA papers did not release code or later removed it; trained word embeddings used by the original works were not released or links were dead, forcing reimplementers to request code from later authors or substitute resources. This lack of availability meant reproductions used different artifacts or had to reimplement from underspecified descriptions.",
            "gap_location": "external resources and implementation availability (pretrained embeddings, published code repositories)",
            "detection_method": "reproducibility study and attempts to obtain resources (requesting code, following links), comparison of available resources to those referenced",
            "measurement_method": "qualitative count and provenance checking (papers reporting 'code not released', 'link broken'); comparison of experimental results when using substitute resources versus original reported results",
            "impact_on_results": "Prevented direct replication and forced use of substitute embeddings or reimplementations; contributed to differing experimental outcomes across groups (examples: when Tang et al. code/embeddings were used by Chen et al. different results were observed). Quantitative differences depend on substituted resources (see other gaps), but inability to access original embeddings/code was a primary barrier to reproducing exact reported scores.",
            "frequency_or_prevalence": "Observed in multiple TDSA papers; authors note 'recent TDSA papers have generally been very good ... but other papers have not released code' and instances of code being removed then returned were reported; SemEval 2016 task 5 example: only 1 of 21 papers released source code.",
            "root_cause": "authors not releasing artifacts at publication, broken links, lack of archival practices for code/resources",
            "mitigation_approach": "Release code, embeddings and data alongside papers; release a model zoo and Jupyter Notebooks; provide anonymised GitHub at submission time and persistent hosting for resources.",
            "mitigation_effectiveness": "Paper authors applied mitigation in their own work (released model zoo and notebooks) which enabled their reproductions; concrete effectiveness measured qualitatively (allowed reproduction and mass evaluation). No large-scale quantitative measure provided.",
            "domain_or_field": "Natural Language Processing / Machine Learning",
            "reproducibility_impact": true,
            "uuid": "e454.0",
            "source_info": {
                "paper_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "undocumented_preprocessing_scaling",
            "name_full": "Undocumented preprocessing and feature scaling",
            "brief_description": "The original papers often omitted preprocessing choices (tokenization details, case-folding) and whether features were scaled before classification; these omissions materially changed reproduced results.",
            "citation_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
            "mention_or_use": "use",
            "system_name": "TDSA preprocessing and feature pipeline",
            "system_description": "Text preprocessing pipeline (lowercasing, tokenization), feature extraction (neural pooling), and feature scaling prior to SVM classification.",
            "nl_description_type": "research paper methods section (preprocessing and model training descriptions)",
            "code_implementation_type": "reimplementation scripts (scikit-learn LinearSVC, feature scaling code)",
            "gap_type": "missing preprocessing specification / hyperparameter mismatch (feature scaling)",
            "gap_description": "Vo and Zhang (2015) did not state that features were scaled; LibLinear suggests scaling but the omission in the paper led reimplementers to either scale or not scale arbitrarily. The reproduction used Max-Min scaling (scikit-learn) and showed that non-scaling reduced results substantially. Tokenization choice (Twokenizer) was assumed but not always explicitly documented.",
            "gap_location": "data preprocessing and feature scaling prior to classifier training",
            "detection_method": "reproduction experiments with/without scaling and with specified tokenization; comparison of reproduced scores to original reported scores",
            "measurement_method": "empirical comparison of accuracy/F1 with and without scaling; the paper reports scaling vs not scaling affecting results 'by around one-third' (figure 2 commentary) for NP methods and shows numeric differences in reproduced vs original tables.",
            "impact_on_results": "Large — not scaling affected results by ~33% (text reports 'affect the results by around one-third'); reproduced performance ranks and absolute accuracies changed when scaling was applied versus omitted (e.g., differences observed in Figure 2 and Table 4 comparisons).",
            "frequency_or_prevalence": "Common; authors state scaling 'is typically overlooked when reporting' and multiple reproduced methods were sensitive to this choice.",
            "root_cause": "implicit assumptions (authors relying on default library guidance) and omission of preprocessing details in paper text",
            "mitigation_approach": "Document preprocessing steps (tokenization, lowercasing) and explicit statement of feature scaling; include preprocessing code and example pipelines in released repository/Notebooks.",
            "mitigation_effectiveness": "Effective in authors' reproductions: applying explicit Max-Min scaling aligned reproduced results closer to expected performance; quantified effect reported (~one-third change).",
            "domain_or_field": "Natural Language Processing / Machine Learning",
            "reproducibility_impact": true,
            "uuid": "e454.1",
            "source_info": {
                "paper_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "ambiguous_target_occurrence",
            "name_full": "Ambiguous handling of multiple target occurrences",
            "brief_description": "Papers that require per-target context pooling do not state how to handle multiple occurrences of the same target in a text; implementers made different choices leading to differing feature vectors and results.",
            "citation_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
            "mention_or_use": "use",
            "system_name": "target-specific context extraction step",
            "system_description": "Procedure that extracts left/right/target contexts for a specified target mention in the input text for neural pooling or parser-based feature extraction.",
            "nl_description_type": "research paper methods section (context extraction specification)",
            "code_implementation_type": "reimplementation code (context pooling logic)",
            "gap_type": "ambiguous description / incomplete specification",
            "gap_description": "Vo and Zhang (2015) and other methods require context relative to a target word but do not specify which occurrence to use when the same surface form appears multiple times. The reproduction adopted Wang et al. (2017)'s approach: compute features for each occurrence and median-pool them. This choice can subtly change results compared to the unspecified original.",
            "gap_location": "data preprocessing / context selection for feature extraction",
            "detection_method": "manual code review of method descriptions, identification of missing specification, and empirical reimplementation testing with alternative resolutions (per-occurrence pooling).",
            "measurement_method": "empirical comparison of reproduced models with different pooling strategies; reported as an explanatory factor for 'subtle differences' between reproduced and original results (no single numeric delta solely attributed to this choice given confounding factors).",
            "impact_on_results": "Moderate — cited as a plausible explanation for subtle differences between original and reproduced NP results; contributed to variation in reported accuracies but not isolated as sole cause.",
            "frequency_or_prevalence": "Noted across TDSA methods that depend on per-target contexts; observed in reproductions of at least Vo & Zhang (2015) and noted by Wang et al. (2017).",
            "root_cause": "ambiguous natural language description failing to specify tie-breaking or aggregation when multiple target mentions exist",
            "mitigation_approach": "Explicitly document choice (e.g., first occurrence, all occurrences with pooling and pooling strategy), include code implementing the chosen strategy in released repository.",
            "mitigation_effectiveness": "Authors' adoption of median pooling allowed consistent reproductions across datasets; improved clarity and helped explain result differences, though no single numeric effectiveness metric provided.",
            "domain_or_field": "Natural Language Processing / Machine Learning",
            "reproducibility_impact": true,
            "uuid": "e454.2",
            "source_info": {
                "paper_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "lexicon_ambiguity_gap",
            "name_full": "Ambiguities in sentiment lexicon usage and counting",
            "brief_description": "The paper found inconsistencies between reported and computed word counts for sentiment lexicons due to undocumented assumptions about word membership in sentiment classes, demonstrating that small undocumented details change experimental inputs and results.",
            "citation_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
            "mention_or_use": "use",
            "system_name": "sentiment lexicon integration in feature pipelines",
            "system_description": "Process that uses sentiment lexicons (MPQA, NRC, HL) to filter or weight embedding contexts for neural pooling features and lexicon-based contexts.",
            "nl_description_type": "research paper methods section and lexicon statistics reporting",
            "code_implementation_type": "lexicon parsing and filtering scripts",
            "gap_type": "ambiguous specification (lexicon membership and counting rules)",
            "gap_description": "Vo and Zhang (2015) did not clearly document whether a word could belong to multiple sentiment classes within a lexicon; the reproduction initially assumed exclusivity leading to differing word counts. Correcting this assumption changed lexicon statistics and affected which lexicon combination performed best.",
            "gap_location": "data resources interpretation and preprocessing (lexicon parsing)",
            "detection_method": "manual verification of lexicon files versus reported statistics and rerunning experiments with corrected lexicon membership assumptions; comparison of lexicon word counts (Table 3) and downstream results (Table 4).",
            "measurement_method": "Direct comparison of lexicon word counts (original reported vs reproduction counts) and classification accuracy differences when using different lexicon settings; Table 4 shows reproduced accuracies differing by ~1 percentage point and changes in best-performing lexicon ranking.",
            "impact_on_results": "Small-to-moderate — changed relative effectiveness of lexicons (paper reports best single lexicon differs from original) and produced small absolute accuracy differences (e.g., Target-dep original 65.72% vs reproduction 66.81% in Table 4; lexicon ranking changed).",
            "frequency_or_prevalence": "Observed in this reproduction and likely present whenever papers rely on lexicon resources without explicit parsing rules; not quantified broadly but highlighted as a source of variation.",
            "root_cause": "incomplete specification of resource parsing rules and implicit assumptions about lexicon structure",
            "mitigation_approach": "Provide parsed lexicon files in repository, document membership semantics (allow multi-class entries), and include exact preprocessing scripts that generated the reported counts.",
            "mitigation_effectiveness": "Effective for aligning reproductions: after correcting assumptions reproductions obtained consistent lexicon statistics and clearer comparisons; numeric improvement small (see Table 4) but clarified cause of discrepancy.",
            "domain_or_field": "Natural Language Processing / Sentiment Analysis",
            "reproducibility_impact": true,
            "uuid": "e454.3",
            "source_info": {
                "paper_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "missing_hyperparams_and_training_details",
            "name_full": "Underspecified hyperparameters, training procedure and ambiguous terms (epochs, clipping threshold)",
            "brief_description": "Key training details (number of epochs, specific clipping thresholds) and initialization/training settings were not fully specified in original papers, requiring choices by reproducers that affected results.",
            "citation_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
            "mention_or_use": "use",
            "system_name": "neural network training pipeline (LSTM-based TDSA)",
            "system_description": "The training regimen for LSTM/TCLSTM/TDLSTM models, including weight initialization, optimizer, learning rate, epoch schedule, early stopping, and any gradient or output clipping.",
            "nl_description_type": "research paper methods section (training procedure)",
            "code_implementation_type": "neural network training scripts (PyTorch/Theano/TensorFlow or unspecified)",
            "gap_type": "incomplete specification / ambiguous hyperparameter description",
            "gap_description": "The reproduced LSTM experiments required assumptions for number of epochs and an ambiguous instruction 'set the clipping threshold of softmax layer as 200' (Tang et al., 2016a) which the authors could not interpret. The reproduction used early stopping (patience 10, up to 300 epochs) and uniform initialization U(0.003,0.003) and reported that such choices plus random seed significantly affected results.",
            "gap_location": "training procedure and hyperparameters (epochs, clipping, initialization, early stopping)",
            "detection_method": "attempted reimplementation following paper text, identification of unspecified terms, and empirical evaluation showing sensitivity to these choices; reference to Reimers & Gurevych (2017) about seed effects.",
            "measurement_method": "Repeated training runs (30 seeds) and reporting distribution of test results; comparison of mean vs max performance against original reported single-run scores (Table 5 shows original vs reproduction mean and max).",
            "impact_on_results": "Substantial for deep models — LSTM reproductions showed large variability: example Table 5: original TDLSTM F1 69.00 vs reproduction mean 65.63 (drop ≈3.37 points), reproduction max closer to original; single-run reporting in originals can be misleading. Random seed and unspecified stopping/clipping together led to notable result variation.",
            "frequency_or_prevalence": "Common in neural-network based NLP papers; authors cite Reimers & Gurevych (2017) showing seed variability is statistically significant and observe similar effects themselves.",
            "root_cause": "ambiguous or omitted reporting of training hyperparameters and operational details in paper text",
            "mitigation_approach": "Report multiple runs and seed distributions, document all training choices (initialization, clipping, epochs, early stopping), release training code and random seeds used; use early stopping with described validation split or report exact epoch counts.",
            "mitigation_effectiveness": "Authors ran 30 seeds and reported mean and max, showing that reporting distributions explains variance — maximum reproduced score approached original reported score, while mean was lower. This indicates that reporting multiple runs and seeds is effective at revealing true performance variability.",
            "domain_or_field": "Natural Language Processing / Deep Learning",
            "reproducibility_impact": true,
            "uuid": "e454.4",
            "source_info": {
                "paper_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "result_discrepancy_with_published_code",
            "name_full": "Published code yielding different results than paper reports",
            "brief_description": "Even when code or embeddings are released, other researchers using them have obtained different results than originally reported, indicating mismatches between paper descriptions, code, or hidden dependencies.",
            "citation_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
            "mention_or_use": "use",
            "system_name": "published implementations of TDSA methods",
            "system_description": "Released implementations (from Tang et al., others) and associated embedding files that should produce the experimental results reported in papers.",
            "nl_description_type": "research paper results and code release notes",
            "code_implementation_type": "published implementation repositories and embedding files",
            "gap_type": "implementation-result mismatch / hidden dependency or versioning issue",
            "gap_description": "The paper cites cases where researchers using published code or embeddings (e.g., Tang et al. 2016a/b) observed different results than reported in the original papers (Chen et al., Tay et al. reproductions). Possible causes include undocumented preprocessing, library versions, or missing configuration details.",
            "gap_location": "entire experimental pipeline including preprocessing, training, and evaluation (hidden dependencies/versioning)",
            "detection_method": "attempts by independent groups to run released code or reuse released embeddings and comparing obtained scores to original published numbers; literature reports of other groups observing mismatches.",
            "measurement_method": "comparison of reported metrics (accuracy, F1) from original papers versus metrics obtained by independent groups using released artifacts; results differences shown in narrative (specific numeric deltas vary by experiment).",
            "impact_on_results": "Caused confusion and conflicting claims about model effectiveness; contributed to the need for reimplementations and mass evaluations. Quantitatively, some reproduced results diverged by several percentage points (see Table 5 and reproduction narratives), but exact deltas depend on case.",
            "frequency_or_prevalence": "Documented multiple times within TDSA literature; not quantified across the field but presented as a recurring problem in the discussion.",
            "root_cause": "hidden or undocumented dependencies, differences in preprocessing and runtime environments, missing configuration details, or code changes after publication",
            "mitigation_approach": "Bundle code with exact environment specifications, provide preprocessing pipelines and versioned dependencies, release notebooks showing end-to-end runs, and include test cases with expected outputs.",
            "mitigation_effectiveness": "When the authors requested embeddings from later code bases (Wang et al. 2017) they could proceed with reproduction; releasing a full reproducible package (code + notebooks) improved ability to replicate, but the paper does not provide large-scale quantitative effectiveness measures beyond enabling the authors' reproductions.",
            "domain_or_field": "Natural Language Processing / Software Engineering for ML",
            "reproducibility_impact": true,
            "uuid": "e454.5",
            "source_info": {
                "paper_title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Target-dependent twitter sentiment classification with rich automatic features",
            "rating": 2,
            "sanitized_title": "targetdependent_twitter_sentiment_classification_with_rich_automatic_features"
        },
        {
            "paper_title": "Effective lstms for target-dependent sentiment classification",
            "rating": 2,
            "sanitized_title": "effective_lstms_for_targetdependent_sentiment_classification"
        },
        {
            "paper_title": "Tdparse: Multi-target-specific sentiment recognition on twitter",
            "rating": 2,
            "sanitized_title": "tdparse_multitargetspecific_sentiment_recognition_on_twitter"
        },
        {
            "paper_title": "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
            "rating": 2,
            "sanitized_title": "reporting_score_distributions_makes_a_difference_performance_study_of_lstmnetworks_for_sequence_tagging"
        },
        {
            "paper_title": "Replication issues in syntax-based aspect extraction for opinion mining",
            "rating": 1,
            "sanitized_title": "replication_issues_in_syntaxbased_aspect_extraction_for_opinion_mining"
        },
        {
            "paper_title": "A note on rigour and replicability",
            "rating": 1,
            "sanitized_title": "a_note_on_rigour_and_replicability"
        }
    ],
    "cost": 0.0137445,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis
6 Aug 2018</p>
<p>Andrew Moore 
School of Computing and Communications
Lancaster University
LancasterUK</p>
<p>Paul Rayson 
School of Computing and Communications
Lancaster University
LancasterUK</p>
<p>Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis
6 Aug 20180A91A7A71CF1F3E742582D9E95D03B3CarXiv:1806.05219v2[cs.CL]
Lack of repeatability and generalisability are two significant threats to continuing scientific development in Natural Language Processing.Language models and learning methods are so complex that scientific conference papers no longer contain enough space for the technical depth required for replication or reproduction.Taking Target Dependent Sentiment Analysis as a case study, we show how recent work in the field has not consistently released code, or described settings for learning methods in enough detail, and lacks comparability and generalisability in train, test or validation data.To investigate generalisability and to enable state of the art comparative evaluations, we carry out the first reproduction studies of three groups of complementary methods and perform the first large-scale mass evaluation on six different English datasets.Reflecting on our experiences, we recommend that future replication or reproduction experiments should always consider a variety of datasets alongside documenting and releasing their methods and published code in order to minimise the barriers to both repeatability and generalisability.We have released our code with a model zoo on GitHub with Jupyter Notebooks to aid understanding and full documentation, and we recommend that others do the same with their papers at submission time through an anonymised GitHub account.</p>
<p>Introduction</p>
<p>Repeatable (replicable and/or reproducible 1 ) experimentation is a core tenet of the scientific endeavour.In Natural Language Processing (NLP) research as in other areas, this requires three crucial components: (a) published methods described in sufficient detail (b) a working code base and (c) open dataset(s) to permit training, testing and validation to be reproduced and generalised.In the cognate sub-discipline of corpus linguistics, releasing textual datasets has been a defining feature of the community for many years, enabling multiple comparative experiments to be conducted on a stable basis since the core underlying corpora are community resources.In NLP, with methods becoming increasingly complex with the use of machine learning and deep learning approaches, it is often difficult to describe all settings and configurations in enough detail without releasing code.The work described in this paper emerged from recent efforts at our research centre to reimplement other's work across a number of topics (e.g.text reuse, identity resolution and sentiment analysis) where previously published methods were not easily repeatable because of missing or broken code or dependencies, and/or where methods were not sufficiently well described to enable reproduction.We focus on one sub-area of sentiment analysis to illustrate the extent of these problems, along with our initial recommendations and contributions to address the issues.</p>
<p>The area of Target Dependent Sentiment Analysis (TDSA) and NLP in general has been growing rapidly in the last few years due to new neural network methods that require no feature engineering.However it is difficult to keep track of the state of the art as new models are tested on different datasets, thus preventing true comparative evaluations.This is best shown by table 1 where many approaches are evaluated on the SemEval dataset (Pontiki et al., 2014) but not all.Datasets can vary by domain (e.g.product), type (social media, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes.Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods.</p>
<p>In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013;Zhang et al., 2015;Zhang et al., 2016;Liu and Zhang, 2017;Marrese-Taylor et al., 2017;Wang et al., 2017) but other papers have not released code (Wang et al., 2016;Tay et al., 2017).In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a).Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper.This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results.Similarly, when others (Tay et al., 2017;Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they also produce different results to the original authors.Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future.In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced.Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general.</p>
<p>In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) (Vo and Zhang, 2015), NP with dependency parsing (Wang et al., 2017), and RNN (Tang et al., 2016a), as well as having been applied largely to different datasets.At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created.</p>
<p>Datasets</p>
<p>Related work</p>
<p>Reproducibility and replicability have long been key elements of the scientific method, but have been gaining renewed prominence recently across a number of disciplines with attention being given to a 'reproducibility crisis'.For example, in pharmaceutical research, as little as 20-25% of papers were found to be replicable (Prinz et al., 2011).The problem has also been recognised in computer science in general (Collberg and Proebsting, 2016).Reproducibility and replicability have been researched for sometime in Information Retrieval (IR) since the Grid@CLEF pilot track (Ferro and Harman, 2009).The aim was to create a 'grid of points' where a point defined the performance of a particular IR system using certain pre-processing techniques on a defined dataset.Louridas and Gousios (2012) looked at reproducibility in Software Engineering after trying to replicate another authors results and concluded with a list of requirements for papers to be reproducible: (a) All data related to the paper, (b) All code required to reproduce the paper and (c) Documentation for the code and data.Fokkens et al. (2013) looked at reproducibility in WordNet similarity and Named Entity Recognition finding five key aspects that cause experimental variation and therefore need to be clearly stated: (a) pre-processing, (b) experimental setup, (c) versioning, (d) system output, (e) system variation.In Twitter sentiment analysis, Sygkounas et al. (2016) stated the need for using the same library versions and datasets when replicating work.</p>
<p>Different methods of releasing datasets and code have been suggested.Ferro and Harman (2009) defined a framework (CIRCO) that enforces a pre-processing pipeline where data can be extracted at each stage therefore facilitating a validation step.They stated a mechanism for storing results, dataset and pre-processed data2 .Louridas and Gousios (2012) suggested the use of a virtual machine alongside papers to bundle the data and code together, while most state the advantages of releasing source code (Fokkens et al., 2013;Potthast et al., 2016;Sygkounas et al., 2016).The act of reproducing or replicating results is not just for validating research but to also show how it can be improved.Ferro and Silvello (2016) followed up their initial research and were able to analyse which pre-processing techniques were important on a French monolingual dataset and how the different techniques affected each other given an IR system.Fokkens et al. (2013) showed how changes in the five key aspects affected results.</p>
<p>The closest related work to our reproducibility study is that of Marrese-Taylor and Matsuo (2017) which they replicate three different syntactic based aspect extraction methods.They found that parameter tuning was very important however using different pre-processing pipelines such as Stanford's CoreNLP did not have a consistent effect on the results.They found that the methods stated in the original papers are not detailed enough to replicate the study as evidenced by their large results differential.Dashtipour et al. (2016) undertook a replication study in sentiment prediction, however this was at the document level and on different datasets and languages to the originals.In other areas of (aspectbased) sentiment analysis, releasing code for published systems has not been a high priority, e.g. in SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code.In IR, specific reproducible research tracks have been created3 and we are pleased to see the same happening at COLING 2018 4 .</p>
<p>Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002;Turney, 2002).Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014), Recursive Neural Networks (RecNN) (Dong et al., 2014), Recurrent Neural Networks (RNN) (Tang et al., 2016a), attention applied to RNN (Wang et al., 2016;Chen et al., 2017;Tay et al., 2017), Neural Pooling (NP) (Vo and Zhang, 2015;Wang et al., 2017), RNN combined with NP (Zhang et al., 2016), and attention based neural networks (Tang et al., 2016b).Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem.Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF (Zhang et al., 2015).Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better.Finally, Marrese-Taylor et al. (2017) created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise.Overall, within the field of sentiment analysis there are other granularities such as sentence level (Socher et al., 2013), topic (Augenstein et al., 2018), and aspect (Wang et al., 2016;Tay et al., 2017).Aspect-level sentiment analysis relates to identifying the sentiment of (potentially multiple) topics in the same text although this can be seen as a similar task to TDSA.However the clear distinction between aspect and TDSA is that TDSA requires the target to be mentioned in the text itself while aspect-level employs a conceptual category with potentially multiple related instantiations in the text.Tang et al. (2016a) created a Target Dependent LSTM (TDLSTM) which encompassed two LSTMs either side of the target word, then improved the model by concatenating the target vector to the input embeddings to create a Target Connected LSTM (TCLSTM).Adding attention has become very popular recently.Tang et al. (2016b) showed the speed and accuracy improvements of using multiple attention layers only over LSTM based methods, however they found that it could not model complex sentences e.g.negations.Liu and Zhang (2017) showed that adding attention to a Bi-directional LSTM (BLSTM) improves the results as it takes the importance of each word into account with respect to the target.Chen et al. (2017) also combined a BLSTM and attention, however they used multiple attention layers and combined the results using a Gated Recurrent Unit (GRU) which they called Recurrent Attention on Memory (RAM), and they found this method to allow models to better understand more complex sentiment for each comparison.Vo and Zhang (2015) used neural pooling features e.g.max, min, etc of the word embeddings of the left and right context of the target word, the target itself, and the whole Tweet.They inputted the features into a linear SVM, and showed the importance of using the left and right context for the first time.They found in their study that using a combination of Word2Vec embeddings and sentiment embeddings (Tang et al., 2014) performed best alongside using sentiment lexicons to filter the embedding space.Other studies have adopted more linguistic approaches.Wang et al. (2017) extended the work of Vo and Zhang (2015) by using the dependency linked words from the target.Dong et al. (2014) used the dependency tree to create a Recursive Neural Network (RecNN) inspired by Socher et al. (2013) but compared to Socher et al. (2013) they also utilised the dependency tags to create an Adaptive RecNN (ARecNN).</p>
<p>Critically, the methods reported above have not been applied to the same datasets, therefore a true comparative evaluation between the different methods is somewhat difficult.This has serious implications for generalisability of methods.We correct that limitation in our study.There are two papers taking a similar approach to our work in terms of generalisability although they do not combine them with the reproduction issues that we highlight.First, Chen et al. ( 2017) compared results across Se-mEval's laptop and restaurant reviews in English (Pontiki et al., 2014), a Twitter dataset (Dong et al., 2014) and their own Chinese news comments dataset.They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014), Rec-NN (Dong et al., 2014), TDLSTM (Tang et al., 2016a), Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method.However, the Chinese dataset was not released, and the methods were not compared across all datasets.By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family.A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper.</p>
<p>Datasets used in our experiments</p>
<p>We are evaluating our models over six different English datasets deliberately chosen to represent a range of domains, types and mediums.As highlighted above, previous papers tend to only carry out evaluations on one or two datasets which limits the generalisability of their results.In this paper, we do not consider the quality or inter-annotator agreement levels of these datasets but it has been noted that some datasets may have issues here.For example, Pavlopoulos and Androutsopoulos (2014) point out that the Hu and Liu (2004) dataset does not state their inter-annotator agreement scores nor do they have aspect terms that express neutral opinion.</p>
<p>We only use a subset of the English datasets available.For two reasons.First, the time it takes to write parsers and run the models.Second, we only used datasets that contain three distinct sentiments (Wilson (2008) only has two).From the datasets we have used, we have only had issue with parsing Wang et al. (2017) where the annotations for the first set of the data contains the target span but the second set does not.Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.An as example of this: "Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans.As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of Dong et al. (2014) and Mitchell et al. (2013).The only dataset that has a small difference between the number of unique sentiments per sentence is the Wang et al. (2017) election dataset.</p>
<p>Lastly we create training and test splits for the YouTuBean (Marrese-Taylor et al., 2017) and Mitchell (Mitchell et al., 2013) datasets as they were both evaluated originally using cross validation.These splits are reproducible using the code that we are open sourcing.</p>
<p>Dataset</p>
<p>Reproduction studies</p>
<p>In the following subsections, we present the three different methods that we are reproducing and how their results differ from the original analysis.In all of the experiments below, we lower case all text and tokenise using Twokenizer (Gimpel et al., 2011).This was done as the datasets originate from Twitter and this pre-processing method was to some extent stated in Vo and Zhang (2015) and assumed to be used across the others as they do not explicitly state how they pre-process in the papers.</p>
<p>Reproduction of Vo and Zhang (2015)</p>
<p>Vo and Zhang (2015) created the first NP method for TDSA.It takes the word vectors of the left, right, target word, and full tweet/sentence/text contexts and performs max, min, average, standard deviation, and product pooling over these contexts to create a feature vector as input to the Support Vector Machine (SVM) classifier.This feature vector is in affect an automatic feature extractor.They created four different methods: 1. Target-ind uses only the full tweet context, 2. Target-depuses left, right, and target contexts, 3. Target-dep Uses both features of Target-ind and Target-dep-, and 4. Target-dep+ Uses the features of Target-dep and adds two additional contexts left and right sentiment (LS &amp; RS) contexts where only the words within a specified lexicon are kept and the rest of the words are zero vectors.All of their experiments are performed on Dong et al. (2014) Twitter data set.For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.</p>
<p>One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by Wang et al. (2017).As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.We therefore took the approach of Wang et al. (2017) and found all of the features for each appearance and performed median pooling over features.This change could explain the subtle differences between the results we report and those of the original paper.</p>
<p>Sentiment Lexicons</p>
<p>Vo and Zhang (2015) used three different sentiment lexicons: MPQA5 (Wilson et al., 2005), NRC6 (Mohammad and Turney, 2010), and HL7 (Hu and Liu, 2004).We found a small difference in word counts between their reported statistics for the MPQA lexicons and those we performed ourselves, as can be seen in the bold numbers in table 3. Originally, we assumed that a word can only occur in one sentiment class within the same lexicon, and this resulted in differing counts for all lexicons.This distinction is not clearly documented in the paper or code.However, our assumption turned out to be incorrect, giving a further illustration of why detailed descriptions and documentation of all decisions is important.We ran the same experiment as Vo and Zhang (2015) to show the effectiveness of sentiment lexicons the results can be seen in table 4. We can clearly see there are some difference not just with the accuracy scores but the rank of the sentiment lexicons.We found just using HL was best and MPQA does help performance compared to the Target-dep baseline which differs to Vo and Zhang (2015) findings.Since we found that using just HL performed best, the rest of the results will apply the Target-dep+ method using HL and using HL &amp; MPQA to show the affect of using the lexicon that both we and Vo and Zhang (2015)</p>
<p>Using different word vectors</p>
<p>The original authors tested their methods using three different word vectors: 1. Word2Vec trained by Vo and Zhang (2015) on 5 million Tweets containing emoticons (W2V), 2. Sentiment Specific Word Embedding (SSWE) from Tang et al. (2014), and 3. W2V and SSWE combined.Neither of these word embeddings are available from the original authors as Vo and Zhang (2015) never released the embeddings and the link to Tang et al. (2014) embeddings no longer works8 .However, the embeddings were released through Wang et al. (2017) code base9 following requesting of the code from Vo and Zhang (2015).Figure 1 shows the results of the different word embeddings across the different methods.The main finding we see is that SSWE by themselves are not as informative as W2V vectors which is different to the findings of Vo and Zhang (2015).However we agree that combining the two vectors is beneficial and that the rank of methods is the same in our observations.We test all of the methods on the test data set of Dong et al. (2014) and show the difference between the original and reproduced models in figure 2. Finally, we show the effect of scaling using Max Min and not scaling the data.</p>
<p>As stated before, we have been using Max Min scaling on the NP features, however Vo and Zhang (2015) did not mention scaling in their paper.The library they were using, LibLinear (Fan et al., 2008), suggests in its practical guide (Hsu et al., 2003) to scale each feature to [0, 1] but this was not re-iterated by Vo and Zhang (2015).We are using scikit-learn's (Pedregosa et al., 2011) LinearSVC which is a wrapper of LibLinear, hence making it appropriate to use here.As can be seen in figure 2, not scaling can affect the results by around one-third.</p>
<p>Reproduction of Wang et al. (2017)</p>
<p>Wang et al. ( 2017) extended the NP work of Vo and Zhang (2015) and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word.Thus, they created three different methods: 1. TDParseuses only the full dependency graph context, 2. TDParse the feature of TDParseand the left and right contexts, and 3. TDParse+ the features of TDParse and LS and RS contexts.The experiments are performed on the Dong et al. (2014) and Wang et al. (2017) Twitter datasets where we train and test on the previously specified train and test splits.We also scale our features using Max Min scaling before inputting into the SVM.We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets.The results of these experiments can be seen in figure 3 10 .As found with the results of Vo and Zhang (2015) replication, scaling is very important but is typically overlooked when reporting.2014) dataset where we train and test on the specified splits.For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not "set the clipping threshold of softmax layer as 200" (Tang et al., 2016a) as we were unsure what this meant.With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs.Within their experiments they used SSWE (Tang et al., 2014) and Glove Twitter vectors11 (Pennington et al., 2014).</p>
<p>As the paper being reproduced does not define the number of epochs they trained for, we use early stopping.Thus for early stopping we require to split the training data into train and validation sets to know when to stop.As it has been shown by Reimers and Gurevych (2017) that the random seed statistically significantly changes the results of experiments we ran each model over each word embedding thirty times, using a different seed value but keeping the same stratified train and validation split, and Figure 4: Distribution of the LSTM results reported the results on the same test data as the original paper.As can be seen in Figure 4, the initial seed value makes a large difference more so for the smaller embeddings.In table 5, we show the difference between our mean and maximum result and the original result for each model using the 200 dimension Glove Twitter vectors.Even though the mean result is quite different from the original the maximum is much closer.Our results generally agree with their results on the ranking of the word vectors and the embeddings.</p>
<p>Overall, we were able to reproduce the results of all three papers.However for the neural network/deep learning approach of Tang et al. (2016a) we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017;Tay et al., 2017).</p>
<p>Mass Evaluation</p>
<p>For all of the methods we pre-processed the text by lower casing and tokenising using Twokenizer (Gimpel et al., 2011), and we used all three sentiment lexicons where applicable.We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods.We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media (Tang et al., 2016a) and Glove for reviews (Chen et al., 2017).To make the experiments quicker and computationally less expensive, we filtered out all words from the word vectors that did not appear in the train and test datasets, and this is equivalent with respect to word coverage as using all words.Finally we only reported results for the LSTM methods with one seed value and not multiple due to time constraints.</p>
<p>The results of the methods using the best found word vectors on the test sets can be seen in table 6.We find that the TDParse methods generally perform best but only clearly outperforms the other nondependency parser methods on the YouTuBean dataset.We hypothesise that this is due to the dataset containing, on average, a deeper constituency tree depth which could be seen as on average more complex sentences.This could be due to it being from the spoken medium compared to the rest of the datasets which are written.Also that using a sentiment lexicon is almost always beneficial, but only by a small amount.Within the LSTM based methods the TDLSTM method generally performs the best indicating that the extra target information that the TCLSTM method contains is not needed, but we believe this needs further analysis.</p>
<p>We can conclude that the simpler NP models perform well across domain, type and medium and that even without language specific tools and lexicons they are competitive to the more complex LSTM based methods.</p>
<p>Dataset</p>
<p>Discussion and conclusion</p>
<p>The fast developing subfield of TDSA has so far lacked a large-scale comparative mass evaluation of approaches using different models and datasets.In this paper, we address this generalisability limitation and perform the first direct comparison and reproduction of three different approaches for TDSA.While carrying out these reproductions, we have noted and described above, the many emerging issues in previous research related to incomplete descriptions of methods and settings, patchy release of code, and lack of comparative evaluations.This is natural in a developing field, but it is crucial for ongoing development within NLP in general that improved repeatability practices are adopted.The practices adopted in our case studies are to reproduce the methods in open source code, adopt only open data, provide format conversion tools to ingest the different data formats, and describe and document all settings via the code and Jupyter Notebooks (released initially in anonymous form at submission time) 12 .We therefore argue that papers should not consider repeatability (replication or reproduction) or generalisability alone, but these two key tenets of scientific practice should be brought together.</p>
<p>In future work, we aim to extend our reproduction framework further, and extend the comparative evaluation to languages other than English.This will necessitate changes in the framework since we expect that dependency parsers and sentiment lexicons will be unavailable for specific languages.Also we will explore through error analysis in which situations different neural network architectures perform best.</p>
<p>. (2015) Zhang et al. (2016) Tang et al. (2016a) Tang et al. (2016b) Wang et al. (2016) Chen et al. (2017) Liu and Zhang (2017) Wang et al. (2017) Marrese-Taylor et al. (2017) 1=Dong et al. (2014), 2=Wilson (2008), 3=Mitchell et al. (2013), 4=Pontiki et al. (2014), 5=Wang et al. (2017), 6=Marrese-Taylor et al. (2017)</p>
<p>Figure 1 :
1
Figure 1: Effectiveness of word embedding</p>
<p>Figure 2 :
2
Figure 2: Target Dependent Final Results</p>
<p>Table 1 :
1
Methods and Datasets</p>
<p>Table 2 :
2
Dataset Statistics
DO TSize MATS UniqAVGS1S2S3LenSemEvalLRE2951 W1.58 129518.57 81.09 17.62 1.2914 LSemEvalRRE4722 W1.83 163017.25 75.26 22.94 1.8014 RMitchelGS3288 W1.22 250718.02 90.48 9.43 0.09Dong Twit-GS6940 W1.00 14517.37 100.00 0.00 0.00terElectionPS11899 W2.94 219021.68 44.50 46.72 8.78TwitterYouTuBean MP RE/S 798SP2.07 52222.53 81.45 18.17 0.38L=Laptop, R=Restaurant, P=Politics, MP=Mobile Phones, G=General, T=Type, RE=Review,S=Social Media, ATS=Average targets per sentence, Uniq=No. unique targets, AVG len=Averagesentence length per target, S1=1 distinct sentiment per sentence, S2=2 distinct sentiments per sen-tence, S3=3 distinct sentiments per sentence, DO=Domain, M=Medium, W=Written, SP=Spoken</p>
<p>Table 4 :
4
found best.Effectiveness of Sentiment Lexicons
Word CountsOriginalReproductionLexiconsPositive Negative Positive Positive Lowered Negative Negative LoweredMPQA228941142298229841484148HL200347802003200347804780NRC223132432231223132433243MPQA &amp; HL270650692725272550805076All three394064904016401665306526Table 3: Sentiment lexicon statistics comparisonResults (Accuracy %)Sentiment LexiconOriginal ReproductionTarget-dep65.7266.81Target-dep+: NRC66.0567.13Target-dep+: HL67.2468.61Target-dep+: MPQA65.5666.81Target-dep+: MPQA &amp; HL67.4068.37Target-dep+: All three67.3068.23</p>
<p>Table 5 :
5
LSTM Final Results
Macro F1MethodsOR (Max) R (Mean)LSTM64.7064.3460.69TDLSTM69.0067.0465.63TCLSTM69.5067.6665.23O=Original, R=Reproduction</p>
<p>Table 6 :
6
Mass Evaluation Results
Target-Target-TDParseTDParse+LSTMTDLSTMTCLSTMDepDep+F1F1F1F1F1F1F1Dong Twitter65.7065.7066.0068.1063.6066.0967.14Election Twitter 45.5045.9046.2044.6038.7043.6042.08Mitchell40.8042.9040.5050.0047.1751.1641.03SemEval 14 L60.0063.7059.6064.5047.8457.9146.80SemEval 14 R56.2057.7059.4061.0046.3657.6855.38YouTuBean53.1055.6071.7068.0045.9345.4738.07Mean53.5555.2557.2359.3748.2753.6548.42
http://direct.dei.unipd.it/
http://ecir2016.dei.unipd.it/call_for_papers.html
http://coling2018.org/
http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/
http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm
https://www.cs.uic.edu/ ˜liub/FBS/sentiment-analysis.html#lexicon
http://ir.hit.edu.cn/ ˜dytang/
https://github.com/bluemonk482/tdparse
For the Election Twitter dataset TDParse+ result were never reported in the original paper.
https://nlp.stanford.edu/projects/glove/
AcknowledgementsThis research is funded at Lancaster University by an EPSRC Doctoral Training Grant.
Multi-task learning of pairwise sequence classification tasks over disparate label spaces. Isabelle Augenstein, Sebastian Ruder, Anders Søgaard, arXiv:1802.099132018arXiv preprint</p>
<p>Assessing state-of-the-art sentiment models on state-of-the-art sentiment datasets. Jeremy Barnes, Roman Klinger, Sabine Schulte Im Walde, Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media AnalysisAssociation for Computational Linguistics2017</p>
<p>Recurrent attention network on memory for aspect sentiment analysis. Peng Chen, Zhongqian Sun, Lidong Bing, Wei Yang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2017</p>
<p>Repeatability in computer systems research. Christian Collberg, Todd A Proebsting, Commun. ACM. 5932016. February</p>
<p>Multilingual sentiment analysis: state of the art and independent comparison of techniques. Kia Dashtipour, Soujanya Poria, Amir Hussain, Erik Cambria, Ahmad Ya Hawalah, Alexander Gelbukh, Qiang Zhou, Cognitive Computation. 842016</p>
<p>Adaptive recursive neural network for target-dependent twitter sentiment classification. Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, Ke Xu, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20142Short Papers)</p>
<p>Liblinear: A library for large linear classification. Kai-Wei Rong-En Fan, Cho-Jui Chang, Xiang-Rui Hsieh, Chih-Jen Wang, Lin, Journal of machine learning research. 92008. Aug</p>
<p>Clef 2009: Grid@ clef pilot track overview. Nicola Ferro, Donna Harman, Workshop of the Cross-Language Evaluation Forum for European Languages. Springer2009</p>
<p>The clef monolingual grid of points. Nicola Ferro, Gianmaria Silvello, International Conference of the Cross-Language Evaluation Forum for European Languages. Springer2016</p>
<p>Offspring from reproduction problems: What replication failure teaches us. Antske Fokkens, Marieke Van Erp, Marten Postma, Ted Pedersen, Piek Vossen, Nuno Freire, ACL (1). 2013</p>
<p>Part-of-speech tagging for twitter: Annotation, features, and experiments. Kevin Gimpel, Nathan Schneider, Brendan O' Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, Noah A Smith, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2011</p>
<p>A practical guide to support vector classification. Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, 2003</p>
<p>Mining and summarizing customer reviews. Minqing Hu, Bing Liu, Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. the tenth ACM SIGKDD international conference on Knowledge discovery and data miningACM2004</p>
<p>Nrc-canada-2014: Detecting aspects and sentiment in customer reviews. Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, Saif Mohammad, Proceedings of the 8th International Workshop on Semantic Evaluation. the 8th International Workshop on Semantic EvaluationAssociation for Computational Linguistics2014. SemEval 2014</p>
<p>Attention modeling for targeted sentiment. Jiangming Liu, Yue Zhang, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European ChapterShort PapersAssociation for Computational Linguistics20172</p>
<p>A note on rigour and replicability. Panos Louridas, Georgios Gousios, ACM SIGSOFT Software Engineering Notes. 3752012</p>
<p>Replication issues in syntax-based aspect extraction for opinion mining. Edison Marrese, - Taylor, Yutaka Matsuo, Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter. the Student Research Workshop at the 15th Conference of the European ChapterAssociation for Computational Linguistics2017</p>
<p>Mining fine-grained opinions on closed captions of youtube videos with an attention-rnn. Edison Marrese-Taylor, Jorge Balazs, Yutaka Matsuo, 2017</p>
<p>Open domain targeted sentiment. Margaret Mitchell, Jacqui Aguilar, Theresa Wilson, Benjamin Van Durme, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2013</p>
<p>Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. Saif Mohammad, Peter Turney, Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text. the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in TextAssociation for Computational Linguistics2010</p>
<p>Sentiment analysis: Capturing favorability using natural language processing. Tetsuya Nasukawa, Jeonghee Yi, Proceedings of the 2nd international conference on Knowledge capture. the 2nd international conference on Knowledge captureACM2003</p>
<p>Thumbs up? sentiment classification using machine learning techniques. Bo Pang, Lillian Lee, Shivakumar Vaithyanathan, Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing. the 2002 Conference on Empirical Methods in Natural Language Processing2002. 2002</p>
<p>Aspect term extraction for sentiment analysis: New datasets, new evaluation measures and an improved unsupervised method. John Pavlopoulos, Ion Androutsopoulos, Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM). the 5th Workshop on Language Analysis for Social Media (LASM)Association for Computational Linguistics2014</p>
<p>Scikitlearn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 122011</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>Semeval-2014 task 4: Aspect based sentiment analysis. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Proceedings of the 8th International Workshop on Semantic Evaluation. the 8th International Workshop on Semantic EvaluationAssociation for Computational Linguistics2014. SemEval 2014Ion Androutsopoulos, and Suresh Manandhar</p>
<p>Semeval-2016 task 5: based sentiment analysis. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Al-Smadi Mohammad, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphee De Clercq, Veronique Hoste, Marianna Apidianaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, Núria Bel, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). the 10th International Workshop on Semantic Evaluation (SemEval-2016)Association for Computational Linguistics2016Salud María Jiménez-Zafra, and Güls ¸en Eryigit</p>
<p>Who wrote the web? revisiting influential author identification research applicable to information retrieval. Martin Potthast, Sarah Braun, Tolga Buz, Fabian Duffhauss, Florian Friedrich, Jörg Marvin Gülzow, Jakob Köhler, Winfried Lötzsch, Fabian Müller, Elisa Maike, Müller, European Conference on Information Retrieval. Springer2016</p>
<p>Believe it or not: how much can we rely on published data on potential drug targets?. Florian Prinz, Thomas Schlange, Khusru Asadullah, Nature Reviews Drug Discovery. 107122011</p>
<p>Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. Nils Reimers, Iryna Gurevych, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2017</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2013</p>
<p>A replication study of the top performing systems in semeval twitter sentiment analysis. Efstratios Sygkounas, Giuseppe Rizzo, Raphaël Troncy, International Semantic Web Conference. 2016</p>
<p>. Springer, </p>
<p>Learning sentiment-specific word embedding for twitter sentiment classification. Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, Bing Qin, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 52nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20141</p>
<p>Effective lstms for target-dependent sentiment classification. Duyu Tang, Bing Qin, Xiaocheng Feng, Ting Liu, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers2016aThe COLING 2016 Organizing Committee</p>
<p>Aspect level sentiment classification with deep memory network. Duyu Tang, Bing Qin, Ting Liu, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016b</p>
<p>Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis. Yi Tay, Anh Tuan Luu, Siu Cheung Hui, arXiv:1712.054032017arXiv preprint</p>
<p>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. Peter Turney, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational Linguistics2002</p>
<p>Target-dependent twitter sentiment classification with rich automatic features. Duy-Tin Vo, Yue Zhang, IJCAI. 2015</p>
<p>Attention-based lstm for aspect-level sentiment classification. Yequan Wang, Minlie Huang, Li Zhao, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2016</p>
<p>Tdparse: Multi-target-specific sentiment recognition on twitter. Bo Wang, Maria Liakata, Arkaitz Zubiaga, Rob Procter, Proceedings of the 15th Conference of the European Chapter. Long Papers. the 15th Conference of the European ChapterAssociation for Computational Linguistics20171</p>
<p>Recognizing contextual polarity in phrase-level sentiment analysis. Theresa Wilson, Janyce Wiebe, Paul Hoffmann, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing2005</p>
<p>Fine-grained subjectivity and sentiment analysis: recognizing the intensity, polarity, and attitudes of private states. Theresa Ann, Wilson , 2008University of Pittsburgh</p>
<p>Neural networks for open domain targeted sentiment. Meishan Zhang, Yue Zhang, Duy Tin Vo, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Gated neural networks for targeted sentiment analysis. Meishan Zhang, Yue Zhang, Duy-Tin Vo, AAAI. 2016</p>            </div>
        </div>

    </div>
</body>
</html>
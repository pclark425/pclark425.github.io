<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1195 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1195</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1195</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-266209789</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.08533v4.pdf" target="_blank">World Models via Policy-Guided Trajectory Diffusion</a></p>
                <p><strong>Paper Abstract:</strong> World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in"in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD, score-based generative models, and classifier-guided diffusion models. Our results demonstrate that PolyGRAD outperforms state-of-the-art baselines in terms of trajectory prediction error for short trajectories, with the exception of autoregressive diffusion. For short trajectories, PolyGRAD obtains similar errors to autoregressive diffusion, but with lower computational requirements. For long trajectories, PolyGRAD obtains comparable performance to baselines. Our experiments demonstrate that PolyGRAD enables performant policies to be trained via on-policy RL in imagination for MuJoCo continuous control domains. Thus, PolyGRAD introduces a new paradigm for accurate on-policy world modelling without autoregressive sampling.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1195.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1195.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PolyGRAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy-Guided Trajectory Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-autoregressive world model that generates entire on-policy trajectories in one diffusion pass by combining a learned denoising network with policy-gradient (score) guidance on actions to produce on-policy synthetic rollouts for imagined on-policy RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PolyGRAD</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative diffusion world model trained on full trajectories (states+rewards) conditioned on action sequences; sampling begins from Gaussian noise over a whole trajectory and iteratively denoises states/rewards via a learned ϵ_θ while updating the action sequence using the policy score ∇_a log π_ϕ(a|s) computed on the denoised state estimate. The policy π_ϕ is a separate Gaussian policy network (µ_ϕ(s), σ_ϕ). The model conditions on an initial state via inpainting and returns an entire trajectory (τ_sr^0, τ_a^0) after N diffusion steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural diffusion-based world model (non-autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MuJoCo continuous control (locomotion) environments; used for imagined on-policy RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mean squared error (MSE) of predicted states vs ground-truth states over rollout horizons; action-distribution statistics (standardised action std compared to unit normal) and distributional plots (a - µ_ϕ(s)).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Competitive MSE at short horizons (h=10): best or near-best versus baselines for Walker2d and second-best for HalfCheetah/Hopper (Autoregressive Diffusion typically best). For longer horizons (h=50, h=200) PolyGRAD's MSE increases relative to some baselines; numeric MSE values are reported in paper figures but not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural generative model (denoising network = residual MLP or transformer). Interpretability is limited; authors visualise action distributions and trajectory prediction errors to inspect behaviour, but no structural interpretability (like explicit state factors) is claimed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of action distribution histograms, trajectory prediction error plots (MSE vs horizon), qualitative trajectory visualisations; no latent-to-symbolic mapping or explicit attribution methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported training/inference cost: one full imagined-RL run to 1M environment steps requires ~54 hours on an RTX 3090 GPU. Uses N=128 diffusion steps in implementation. Residual MLP denoiser is faster for training/inference than transformer; transformer denoiser gives better long-horizon accuracy but is slower.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More efficient than autoregressive diffusion for rollout generation because PolyGRAD runs a single diffusion pass per trajectory rather than one diffusion run per timestep; residual-MLP denoiser is faster than transformer denoiser. Autoregressive diffusion is described as the slowest baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables on-policy RL in imagination and outperforms on-policy model-free baselines (PPO/TRPO/A2C) in mean performance on the evaluated MuJoCo tasks, but underperforms Dreamer-v3 in sample-efficiency and final RL performance despite often having lower short-horizon prediction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High short-horizon fidelity (low MSE at h=10) translates to usable imagined data for on-policy RL, but improved prediction fidelity does not guarantee better policy optimization: Dreamer-v3 attains superior policy performance despite worse short-horizon reconstruction error, suggesting model design (latent-space backprop vs policy-gradient on synthetic data) and representation matters for task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs reported include: (a) fidelity vs computation — transformer denoiser improves long-horizon fidelity at higher compute cost; (b) fidelity vs policy performance — best prediction error (autoregressive diffusion in some cases) does not always yield best RL outcomes; (c) stability vs speed — PolyGRAD requires slowly updating the policy to maintain stability; (d) action-guidance strength vs distributional correctness — too-strong guidance collapses action variance, too-weak guidance fails to match policy distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Non-autoregressive diffusion over whole trajectories; action-conditioning of denoiser during training; action updates via Langevin-like steps using ∇_a log π_ϕ on denoised state estimates; online tuning of action update scale δ to match standardised action std to 1; action clipping to ±3σ of policy mean; denoiser architectures as residual MLP (faster) or transformer (better long-horizon accuracy); training denoiser on clean (non-noisy) actions conditioned on actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against Probabilistic MLP ensemble, Transformer autoregressive world model, Autoregressive Diffusion, and Dreamer-v3: PolyGRAD is faster for generating trajectories than autoregressive diffusion (single pass vs per-step diffusion) and obtains better short-horizon MSE than many baselines in some environments; Dreamer-v3 achieves better RL performance despite worse short-horizon MSE, and Autoregressive Diffusion often achieves best prediction accuracy on some environments (e.g., HalfCheetah) but at much higher sampling cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations/observations: use short imagined rollouts (h=10) for imagined on-policy RL with PolyGRAD; residual MLP denoiser for faster training/inference at short horizons, transformer denoiser for better long-horizon fidelity; maintain sufficient policy entropy (σ_ϕ ≥ ~0.1) because PolyGRAD struggles to match very low-entropy policies; tune δ online using the standardisation heuristic (Equation 10) and clip actions to ±3σ for stable action-distribution matching; update policy slowly (small target ∆ log π) to avoid instability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models via Policy-Guided Trajectory Diffusion', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1195.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1195.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoregressive Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoregressive Diffusion World Model (one-step diffusion rolled out autoregressively)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based world model that predicts single-step action-conditioned state transitions; rollouts are generated autoregressively by sampling actions from the policy and running a per-step reverse diffusion to produce the next state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion world models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion model trained to make one-step next-state predictions conditioned on current action and past observations; at inference, the model is used autoregressively: sample action from policy, run full reverse diffusion to get next state, repeat per time-step to build a trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural diffusion-based autoregressive world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MuJoCo continuous control (locomotion) used for trajectory prediction and imagined rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trajectory MSE (predicted vs ground-truth states) across horizons; one-step prediction loss used to train.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Often the most performant in terms of trajectory prediction error in the evaluated MuJoCo tasks (notably best on HalfCheetah in experiments); produces very accurate one-step predictions which translate to competitive long-horizon trajectories in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural diffusion model; no interpretability methods described in this paper beyond error and qualitative trajectory comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned beyond plotting prediction error curves and qualitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High sampling/inference cost because diffusion reverse-process must be run per time-step of a rollout; identified as the slowest world-modelling method in the experiments (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less computationally efficient than PolyGRAD for trajectory generation because it runs diffusion per step; however, it achieves higher fidelity in some settings at the expense of computation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High trajectory prediction accuracy; when used to produce imagined data could be effective, but per-step diffusion cost makes it expensive for generating large numbers of trajectories for on-policy RL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Very accurate single-step models can yield accurate long-horizon rollouts in MuJoCo benchmarks, but computational cost may limit practicality for large-scale imagined on-policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher fidelity (one-step diffusion accuracy) vs much higher computational cost for rollout generation; may be preferable when prediction accuracy is paramount and compute/time is available.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Train diffusion to predict one-step transitions conditioned on action; use autoregressive sampling for rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms PolyGRAD on some fidelity measures (especially long-horizon in some envs) but is slower; compared to autoregressive transformer and MLP ensembles, diffusion-based one-step models are more accurate but costlier.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Best used when computational budget allows per-step diffusion; may be optimal choice for highest one-step fidelity in these MuJoCo tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models via Policy-Guided Trajectory Diffusion', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1195.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1195.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer-v3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer-v3 (latent sequential VAE world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art latent-space world model that learns compact latent dynamics and uses backpropagation through the learned dynamics for policy optimisation, enabling strong RL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer-v3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model using a sequential VAE to learn compact latent dynamics and a decoder; policies are optimised by backpropagating through the latent dynamics (and rewards) rather than relying on synthetic rollouts alone; dynamics and planning are performed in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE-based sequential latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MuJoCo continuous control (and other domains in Dreamer-v3 work); used here as SOTA comparative baseline for RL performance</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction error to original state space (decoder reconstruction) and latent prediction accuracy; open-loop latent prediction MSE for evaluation of trajectory prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Observed to have higher (worse) short-horizon reconstruction/prediction error than PolyGRAD in some MuJoCo environments, despite producing better RL performance in experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent-space representation not directly interpretable; behavior is inspected by downstream RL performance and reconstruction errors; no explicit interpretability method used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper beyond performance and reconstruction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Noted as a strong performing baseline; training/inference cost details not given in this paper (uses publicly available Dreamer-v3 implementation), but Dreamer-style latent models can be computationally efficient at inference as they operate in compact latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Despite worse short-horizon state-space reconstruction error, Dreamer-v3 yields better RL performance than PolyGRAD, implying better task efficiency per unit of fidelity in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Outperforms PolyGRAD in sample-efficiency and final RL performance on the evaluated MuJoCo tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows that lower state-space reconstruction error does not necessarily translate to better policy learning; latent-space optimisation and backprop-through-dynamics can yield higher task utility despite worse raw state reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between raw state-prediction fidelity (PolyGRAD often better at short horizons) and policy performance (Dreamer-v3 better), indicating representation and learning method matter for task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of compact latent dynamics and direct backpropagation for policy optimisation; training and control in latent space versus optimizing policies on synthetic state-space rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against PolyGRAD: Dreamer-v3 yields stronger RL performance though sometimes worse short-horizon MSE; compared to autoregressive diffusion and transformer baselines, Dreamer-v3 focuses on latent learning and policy optimisation via backprop.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies latent-space model-based RL with backpropagation through dynamics (as in Dreamer-v3) may be preferable for task performance even if state-space MSE is higher.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models via Policy-Guided Trajectory Diffusion', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1195.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1195.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProbMLP-Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic MLP Ensemble (probabilistic dynamics ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble of probabilistic MLP models that output a Gaussian distribution over next state predictions; used as a classical model-based RL baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning in a handful of trials using probabilistic dynamics models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Probabilistic MLP Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ensemble (7 MLPs in experiments) where each MLP predicts mean and variance for next-state given current state and action; sampling from the ensemble yields predictive uncertainty and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic ensemble of feedforward dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MuJoCo continuous control environments; used for trajectory prediction baselines</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mean squared error (MSE) across predicted trajectories; ensemble uncertainty and next-state log-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Prediction error increases quickly with horizon compared to other methods; ensemble performs relatively poorly for longer-horizon predictions in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Implicitly interpretable as ensembles of simple function approximators; no explicit interpretability analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None beyond reporting ensemble predictions and uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Moderate training and inference cost (ensemble of MLPs), faster than diffusion per-step sampling but prediction errors degrade with horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less sample-robust for long horizons; cheaper than per-step diffusion approaches but less accurate for longer rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Yields poorer long-horizon trajectory fidelity and is not competitive with PolyGRAD/transformer/diffusion baselines on the tasks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good for short-term predictions; ensemble uncertainty can help robustness but compounding error causes rapid degradation for longer imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simple models/ensembles are computationally cheaper but suffer compounding error over multiple steps compared to sequence models/diffusion-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of ensemble MLPs to capture epistemic uncertainty; Gaussian output parameterisation for next-state.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Worse long-horizon fidelity than transformer or diffusion approaches; cheaper but less useful for long imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models via Policy-Guided Trajectory Diffusion', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1195.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1195.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer World Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based World Model (autoregressive transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer sequence model trained to predict next states autoregressively from a sequence of past states and actions, used as an expressive autoregressive world model baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers are sample efficient world models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer autoregressive world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder/decoder or causal transformer trained with L2 loss to autoregressively predict next states given past context (max context length used: 15 state-action pairs); used to produce rollouts by autoregressive sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive sequence model (transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MuJoCo continuous control environments for trajectory prediction</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trajectory MSE across horizons; next-step L2 prediction loss during training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Comparable to PolyGRAD at longer horizons in some settings (PolyGRAD with h=200 comparable to transformer), but PolyGRAD outperforms transformer when trained and evaluated on shorter trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box attention model; no attention analysis or interpretability methods reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported beyond performance plots; architecture-level inspectability via attention maps not used here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformer denoiser used in PolyGRAD is more expensive than residual MLP; transformer baseline similar in cost to transformer denoiser experiments (exact hours not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Transformer baseline is more accurate than simple MLP ensembles and comparable to PolyGRAD at some horizons; PolyGRAD's diffusion refinement confers advantages at short horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Produces reasonable trajectory fidelity; PolyGRAD outperforms transformer at short-horizon prediction tasks per experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transformer autoregressive models give strong sequence modelling but are subject to compounding error when unrolled autoregressively; PolyGRAD's iterative refinement can outperform them for short trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Transformer gives improved representational capacity at cost of compute; autoregressive sampling suffers compounding error versus PolyGRAD's single-pass diffusion approach.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of causal/self-attention with context length and L2 training objective; autoregressive rollout generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with PolyGRAD: transformer comparable at long horizons but outperformed by PolyGRAD for short trajectories; compared with ensembles: transformer better long-term accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models via Policy-Guided Trajectory Diffusion', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1195.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1195.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lambert Non-AR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-autoregressive long-horizon predictive model (Lambert et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior non-autoregressive dynamics model that predicts future states at arbitrary times conditioned on controller parameters (e.g., PID/LQR), reducing compounding error for long-horizon prediction but requiring controller parameters as labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning accurate long-term dynamics for model-based reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Lambert et al. non-autoregressive predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Predictive model conditioned on controller parameters (not per-step actions) to directly predict future state at arbitrary horizon points, thereby avoiding autoregressive unrolling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>non-autoregressive predictive model (controller-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General model-based RL long-horizon prediction (discussed in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Long-horizon prediction error (MSE) comparative analyses in their original work; claimed improved long-horizon accuracy relative to autoregressive baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported by Lambert et al. to achieve lower error at long horizons than simple autoregressive baselines (paper does not report numeric values in this text).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model conditions on interpretable controller parameters (PID/LQR) which may yield more interpretable conditioning signals; not directly comparable to neural-policy-conditioned models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Uses controller parameter labels as conditioning (interpretable parameter space); no other interpretability methods discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed in this paper; method requires controller-parameter labels for training which limits scalability to large neural policies.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Advantages in long-horizon fidelity but limited in applicability to deep RL because the required controller parameter labels may not scale to large neural network policies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Better long-horizon prediction in their reported settings, but limited in deep RL contexts with complex policies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Non-autoregressive conditioning on low-dimensional controller parameters can reduce compounding error, but this approach does not readily extend to neural-network policies.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Accuracy at long horizons vs requirement of controller-parameter labels (limits applicability).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Predict future states conditioned on controller parameters rather than per-step actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Lambert-style models can outperform autoregressive models at long horizons if controller labels are available, unlike PolyGRAD which targets neural policies without requiring controller labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models via Policy-Guided Trajectory Diffusion', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1195.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1195.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jackson Policy-Guided Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy-guided diffusion (Jackson et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concurrent work that guides diffusion sampling with a policy to increase likelihood under the policy, used to generate synthetic datasets for offline/off-policy RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Policy-guided diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Policy-guided diffusion (Jackson et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Guided diffusion approach where diffusion sampling is steered by the policy to increase likelihood of generated trajectories under that policy; used for synthetic data generation in offline RL contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>policy-guided diffusion (classifier/score guidance variant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline, off-policy RL dataset generation (discussed as related work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not detailed in this paper; Jackson et al. use policy guidance to bias diffusion samples toward policy-likely trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified here; used to generate high-likelihood under-policy data for offline settings.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box diffusion with policy gradient guidance; interpretability not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not provided here; guidance may add gradient computations per diffusion step similar to PolyGRAD but Jackson et al. use it for offline dataset synthesis rather than imagined on-policy RL.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Jackson et al.'s approach is similar in spirit to PolyGRAD's guidance but applied to offline data synthesis; differences discussed include intended use-case (offline vs online/on-policy imagination) and analysis in current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used to generate synthetic datasets for offline RL; not directly compared numerically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows that policy-guided diffusion can be used to bias diffusion outputs toward trajectories compatible with a policy; PolyGRAD extends this idea to on-policy imagined rollouts and provides additional analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Similar conceptual trade-offs regarding strength of guidance vs action-distribution matching; direct comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Guidance of diffusion by policy gradients to bias generated trajectories to be policy-likely; used for dataset generation in offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models via Policy-Guided Trajectory Diffusion', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Diffusion world models. <em>(Rating: 2)</em></li>
                <li>Mastering diverse domains through world models. <em>(Rating: 2)</em></li>
                <li>Planning with diffusion for flexible behavior synthesis. <em>(Rating: 2)</em></li>
                <li>Policy-guided diffusion. <em>(Rating: 2)</em></li>
                <li>Transformers are sample efficient world models. <em>(Rating: 2)</em></li>
                <li>Learning accurate long-term dynamics for model-based reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1195",
    "paper_id": "paper-266209789",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "PolyGRAD",
            "name_full": "Policy-Guided Trajectory Diffusion",
            "brief_description": "A non-autoregressive world model that generates entire on-policy trajectories in one diffusion pass by combining a learned denoising network with policy-gradient (score) guidance on actions to produce on-policy synthetic rollouts for imagined on-policy RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PolyGRAD",
            "model_description": "Generative diffusion world model trained on full trajectories (states+rewards) conditioned on action sequences; sampling begins from Gaussian noise over a whole trajectory and iteratively denoises states/rewards via a learned ϵ_θ while updating the action sequence using the policy score ∇_a log π_ϕ(a|s) computed on the denoised state estimate. The policy π_ϕ is a separate Gaussian policy network (µ_ϕ(s), σ_ϕ). The model conditions on an initial state via inpainting and returns an entire trajectory (τ_sr^0, τ_a^0) after N diffusion steps.",
            "model_type": "neural diffusion-based world model (non-autoregressive)",
            "task_domain": "MuJoCo continuous control (locomotion) environments; used for imagined on-policy RL",
            "fidelity_metric": "Mean squared error (MSE) of predicted states vs ground-truth states over rollout horizons; action-distribution statistics (standardised action std compared to unit normal) and distributional plots (a - µ_ϕ(s)).",
            "fidelity_performance": "Competitive MSE at short horizons (h=10): best or near-best versus baselines for Walker2d and second-best for HalfCheetah/Hopper (Autoregressive Diffusion typically best). For longer horizons (h=50, h=200) PolyGRAD's MSE increases relative to some baselines; numeric MSE values are reported in paper figures but not tabulated in text.",
            "interpretability_assessment": "Primarily a black-box neural generative model (denoising network = residual MLP or transformer). Interpretability is limited; authors visualise action distributions and trajectory prediction errors to inspect behaviour, but no structural interpretability (like explicit state factors) is claimed.",
            "interpretability_method": "Visualization of action distribution histograms, trajectory prediction error plots (MSE vs horizon), qualitative trajectory visualisations; no latent-to-symbolic mapping or explicit attribution methods reported.",
            "computational_cost": "Reported training/inference cost: one full imagined-RL run to 1M environment steps requires ~54 hours on an RTX 3090 GPU. Uses N=128 diffusion steps in implementation. Residual MLP denoiser is faster for training/inference than transformer; transformer denoiser gives better long-horizon accuracy but is slower.",
            "efficiency_comparison": "More efficient than autoregressive diffusion for rollout generation because PolyGRAD runs a single diffusion pass per trajectory rather than one diffusion run per timestep; residual-MLP denoiser is faster than transformer denoiser. Autoregressive diffusion is described as the slowest baseline.",
            "task_performance": "Enables on-policy RL in imagination and outperforms on-policy model-free baselines (PPO/TRPO/A2C) in mean performance on the evaluated MuJoCo tasks, but underperforms Dreamer-v3 in sample-efficiency and final RL performance despite often having lower short-horizon prediction errors.",
            "task_utility_analysis": "High short-horizon fidelity (low MSE at h=10) translates to usable imagined data for on-policy RL, but improved prediction fidelity does not guarantee better policy optimization: Dreamer-v3 attains superior policy performance despite worse short-horizon reconstruction error, suggesting model design (latent-space backprop vs policy-gradient on synthetic data) and representation matters for task utility.",
            "tradeoffs_observed": "Trade-offs reported include: (a) fidelity vs computation — transformer denoiser improves long-horizon fidelity at higher compute cost; (b) fidelity vs policy performance — best prediction error (autoregressive diffusion in some cases) does not always yield best RL outcomes; (c) stability vs speed — PolyGRAD requires slowly updating the policy to maintain stability; (d) action-guidance strength vs distributional correctness — too-strong guidance collapses action variance, too-weak guidance fails to match policy distribution.",
            "design_choices": "Non-autoregressive diffusion over whole trajectories; action-conditioning of denoiser during training; action updates via Langevin-like steps using ∇_a log π_ϕ on denoised state estimates; online tuning of action update scale δ to match standardised action std to 1; action clipping to ±3σ of policy mean; denoiser architectures as residual MLP (faster) or transformer (better long-horizon accuracy); training denoiser on clean (non-noisy) actions conditioned on actions.",
            "comparison_to_alternatives": "Compared against Probabilistic MLP ensemble, Transformer autoregressive world model, Autoregressive Diffusion, and Dreamer-v3: PolyGRAD is faster for generating trajectories than autoregressive diffusion (single pass vs per-step diffusion) and obtains better short-horizon MSE than many baselines in some environments; Dreamer-v3 achieves better RL performance despite worse short-horizon MSE, and Autoregressive Diffusion often achieves best prediction accuracy on some environments (e.g., HalfCheetah) but at much higher sampling cost.",
            "optimal_configuration": "Paper recommendations/observations: use short imagined rollouts (h=10) for imagined on-policy RL with PolyGRAD; residual MLP denoiser for faster training/inference at short horizons, transformer denoiser for better long-horizon fidelity; maintain sufficient policy entropy (σ_ϕ ≥ ~0.1) because PolyGRAD struggles to match very low-entropy policies; tune δ online using the standardisation heuristic (Equation 10) and clip actions to ±3σ for stable action-distribution matching; update policy slowly (small target ∆ log π) to avoid instability.",
            "uuid": "e1195.0",
            "source_info": {
                "paper_title": "World Models via Policy-Guided Trajectory Diffusion",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Autoregressive Diffusion",
            "name_full": "Autoregressive Diffusion World Model (one-step diffusion rolled out autoregressively)",
            "brief_description": "A diffusion-based world model that predicts single-step action-conditioned state transitions; rollouts are generated autoregressively by sampling actions from the policy and running a per-step reverse diffusion to produce the next state.",
            "citation_title": "Diffusion world models.",
            "mention_or_use": "use",
            "model_name": "Autoregressive Diffusion",
            "model_description": "Diffusion model trained to make one-step next-state predictions conditioned on current action and past observations; at inference, the model is used autoregressively: sample action from policy, run full reverse diffusion to get next state, repeat per time-step to build a trajectory.",
            "model_type": "neural diffusion-based autoregressive world model",
            "task_domain": "MuJoCo continuous control (locomotion) used for trajectory prediction and imagined rollouts",
            "fidelity_metric": "Trajectory MSE (predicted vs ground-truth states) across horizons; one-step prediction loss used to train.",
            "fidelity_performance": "Often the most performant in terms of trajectory prediction error in the evaluated MuJoCo tasks (notably best on HalfCheetah in experiments); produces very accurate one-step predictions which translate to competitive long-horizon trajectories in these benchmarks.",
            "interpretability_assessment": "Black-box neural diffusion model; no interpretability methods described in this paper beyond error and qualitative trajectory comparisons.",
            "interpretability_method": "None mentioned beyond plotting prediction error curves and qualitative comparisons.",
            "computational_cost": "High sampling/inference cost because diffusion reverse-process must be run per time-step of a rollout; identified as the slowest world-modelling method in the experiments (Figure 4).",
            "efficiency_comparison": "Less computationally efficient than PolyGRAD for trajectory generation because it runs diffusion per step; however, it achieves higher fidelity in some settings at the expense of computation.",
            "task_performance": "High trajectory prediction accuracy; when used to produce imagined data could be effective, but per-step diffusion cost makes it expensive for generating large numbers of trajectories for on-policy RL.",
            "task_utility_analysis": "Very accurate single-step models can yield accurate long-horizon rollouts in MuJoCo benchmarks, but computational cost may limit practicality for large-scale imagined on-policy training.",
            "tradeoffs_observed": "Higher fidelity (one-step diffusion accuracy) vs much higher computational cost for rollout generation; may be preferable when prediction accuracy is paramount and compute/time is available.",
            "design_choices": "Train diffusion to predict one-step transitions conditioned on action; use autoregressive sampling for rollouts.",
            "comparison_to_alternatives": "Outperforms PolyGRAD on some fidelity measures (especially long-horizon in some envs) but is slower; compared to autoregressive transformer and MLP ensembles, diffusion-based one-step models are more accurate but costlier.",
            "optimal_configuration": "Best used when computational budget allows per-step diffusion; may be optimal choice for highest one-step fidelity in these MuJoCo tasks.",
            "uuid": "e1195.1",
            "source_info": {
                "paper_title": "World Models via Policy-Guided Trajectory Diffusion",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Dreamer-v3",
            "name_full": "Dreamer-v3 (latent sequential VAE world model)",
            "brief_description": "A state-of-the-art latent-space world model that learns compact latent dynamics and uses backpropagation through the learned dynamics for policy optimisation, enabling strong RL performance.",
            "citation_title": "Mastering diverse domains through world models.",
            "mention_or_use": "use",
            "model_name": "Dreamer-v3",
            "model_description": "Latent world model using a sequential VAE to learn compact latent dynamics and a decoder; policies are optimised by backpropagating through the latent dynamics (and rewards) rather than relying on synthetic rollouts alone; dynamics and planning are performed in latent space.",
            "model_type": "latent world model (VAE-based sequential latent dynamics)",
            "task_domain": "MuJoCo continuous control (and other domains in Dreamer-v3 work); used here as SOTA comparative baseline for RL performance",
            "fidelity_metric": "Reconstruction error to original state space (decoder reconstruction) and latent prediction accuracy; open-loop latent prediction MSE for evaluation of trajectory prediction.",
            "fidelity_performance": "Observed to have higher (worse) short-horizon reconstruction/prediction error than PolyGRAD in some MuJoCo environments, despite producing better RL performance in experiments reported in this paper.",
            "interpretability_assessment": "Latent-space representation not directly interpretable; behavior is inspected by downstream RL performance and reconstruction errors; no explicit interpretability method used in this paper.",
            "interpretability_method": "None reported in this paper beyond performance and reconstruction metrics.",
            "computational_cost": "Noted as a strong performing baseline; training/inference cost details not given in this paper (uses publicly available Dreamer-v3 implementation), but Dreamer-style latent models can be computationally efficient at inference as they operate in compact latent space.",
            "efficiency_comparison": "Despite worse short-horizon state-space reconstruction error, Dreamer-v3 yields better RL performance than PolyGRAD, implying better task efficiency per unit of fidelity in this setting.",
            "task_performance": "Outperforms PolyGRAD in sample-efficiency and final RL performance on the evaluated MuJoCo tasks in this paper.",
            "task_utility_analysis": "Shows that lower state-space reconstruction error does not necessarily translate to better policy learning; latent-space optimisation and backprop-through-dynamics can yield higher task utility despite worse raw state reconstruction.",
            "tradeoffs_observed": "Trade-off between raw state-prediction fidelity (PolyGRAD often better at short horizons) and policy performance (Dreamer-v3 better), indicating representation and learning method matter for task utility.",
            "design_choices": "Use of compact latent dynamics and direct backpropagation for policy optimisation; training and control in latent space versus optimizing policies on synthetic state-space rollouts.",
            "comparison_to_alternatives": "Compared against PolyGRAD: Dreamer-v3 yields stronger RL performance though sometimes worse short-horizon MSE; compared to autoregressive diffusion and transformer baselines, Dreamer-v3 focuses on latent learning and policy optimisation via backprop.",
            "optimal_configuration": "Paper implies latent-space model-based RL with backpropagation through dynamics (as in Dreamer-v3) may be preferable for task performance even if state-space MSE is higher.",
            "uuid": "e1195.2",
            "source_info": {
                "paper_title": "World Models via Policy-Guided Trajectory Diffusion",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ProbMLP-Ensemble",
            "name_full": "Probabilistic MLP Ensemble (probabilistic dynamics ensemble)",
            "brief_description": "An ensemble of probabilistic MLP models that output a Gaussian distribution over next state predictions; used as a classical model-based RL baseline.",
            "citation_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models.",
            "mention_or_use": "use",
            "model_name": "Probabilistic MLP Ensemble",
            "model_description": "Ensemble (7 MLPs in experiments) where each MLP predicts mean and variance for next-state given current state and action; sampling from the ensemble yields predictive uncertainty and robustness.",
            "model_type": "probabilistic ensemble of feedforward dynamics models",
            "task_domain": "MuJoCo continuous control environments; used for trajectory prediction baselines",
            "fidelity_metric": "Mean squared error (MSE) across predicted trajectories; ensemble uncertainty and next-state log-likelihood.",
            "fidelity_performance": "Prediction error increases quickly with horizon compared to other methods; ensemble performs relatively poorly for longer-horizon predictions in these experiments.",
            "interpretability_assessment": "Implicitly interpretable as ensembles of simple function approximators; no explicit interpretability analyses provided.",
            "interpretability_method": "None beyond reporting ensemble predictions and uncertainties.",
            "computational_cost": "Moderate training and inference cost (ensemble of MLPs), faster than diffusion per-step sampling but prediction errors degrade with horizon.",
            "efficiency_comparison": "Less sample-robust for long horizons; cheaper than per-step diffusion approaches but less accurate for longer rollouts.",
            "task_performance": "Yields poorer long-horizon trajectory fidelity and is not competitive with PolyGRAD/transformer/diffusion baselines on the tasks reported.",
            "task_utility_analysis": "Good for short-term predictions; ensemble uncertainty can help robustness but compounding error causes rapid degradation for longer imagined rollouts.",
            "tradeoffs_observed": "Simple models/ensembles are computationally cheaper but suffer compounding error over multiple steps compared to sequence models/diffusion-based approaches.",
            "design_choices": "Use of ensemble MLPs to capture epistemic uncertainty; Gaussian output parameterisation for next-state.",
            "comparison_to_alternatives": "Worse long-horizon fidelity than transformer or diffusion approaches; cheaper but less useful for long imagined rollouts.",
            "uuid": "e1195.3",
            "source_info": {
                "paper_title": "World Models via Policy-Guided Trajectory Diffusion",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Transformer World Model",
            "name_full": "Transformer-based World Model (autoregressive transformer)",
            "brief_description": "A transformer sequence model trained to predict next states autoregressively from a sequence of past states and actions, used as an expressive autoregressive world model baseline.",
            "citation_title": "Transformers are sample efficient world models.",
            "mention_or_use": "use",
            "model_name": "Transformer autoregressive world model",
            "model_description": "Transformer encoder/decoder or causal transformer trained with L2 loss to autoregressively predict next states given past context (max context length used: 15 state-action pairs); used to produce rollouts by autoregressive sampling.",
            "model_type": "autoregressive sequence model (transformer)",
            "task_domain": "MuJoCo continuous control environments for trajectory prediction",
            "fidelity_metric": "Trajectory MSE across horizons; next-step L2 prediction loss during training.",
            "fidelity_performance": "Comparable to PolyGRAD at longer horizons in some settings (PolyGRAD with h=200 comparable to transformer), but PolyGRAD outperforms transformer when trained and evaluated on shorter trajectories.",
            "interpretability_assessment": "Black-box attention model; no attention analysis or interpretability methods reported in this paper.",
            "interpretability_method": "None reported beyond performance plots; architecture-level inspectability via attention maps not used here.",
            "computational_cost": "Transformer denoiser used in PolyGRAD is more expensive than residual MLP; transformer baseline similar in cost to transformer denoiser experiments (exact hours not enumerated here).",
            "efficiency_comparison": "Transformer baseline is more accurate than simple MLP ensembles and comparable to PolyGRAD at some horizons; PolyGRAD's diffusion refinement confers advantages at short horizons.",
            "task_performance": "Produces reasonable trajectory fidelity; PolyGRAD outperforms transformer at short-horizon prediction tasks per experiments.",
            "task_utility_analysis": "Transformer autoregressive models give strong sequence modelling but are subject to compounding error when unrolled autoregressively; PolyGRAD's iterative refinement can outperform them for short trajectories.",
            "tradeoffs_observed": "Transformer gives improved representational capacity at cost of compute; autoregressive sampling suffers compounding error versus PolyGRAD's single-pass diffusion approach.",
            "design_choices": "Use of causal/self-attention with context length and L2 training objective; autoregressive rollout generation.",
            "comparison_to_alternatives": "Compared with PolyGRAD: transformer comparable at long horizons but outperformed by PolyGRAD for short trajectories; compared with ensembles: transformer better long-term accuracy.",
            "uuid": "e1195.4",
            "source_info": {
                "paper_title": "World Models via Policy-Guided Trajectory Diffusion",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Lambert Non-AR",
            "name_full": "Non-autoregressive long-horizon predictive model (Lambert et al. 2021)",
            "brief_description": "Prior non-autoregressive dynamics model that predicts future states at arbitrary times conditioned on controller parameters (e.g., PID/LQR), reducing compounding error for long-horizon prediction but requiring controller parameters as labels.",
            "citation_title": "Learning accurate long-term dynamics for model-based reinforcement learning.",
            "mention_or_use": "mention",
            "model_name": "Lambert et al. non-autoregressive predictor",
            "model_description": "Predictive model conditioned on controller parameters (not per-step actions) to directly predict future state at arbitrary horizon points, thereby avoiding autoregressive unrolling.",
            "model_type": "non-autoregressive predictive model (controller-conditioned)",
            "task_domain": "General model-based RL long-horizon prediction (discussed in related work)",
            "fidelity_metric": "Long-horizon prediction error (MSE) comparative analyses in their original work; claimed improved long-horizon accuracy relative to autoregressive baselines.",
            "fidelity_performance": "Reported by Lambert et al. to achieve lower error at long horizons than simple autoregressive baselines (paper does not report numeric values in this text).",
            "interpretability_assessment": "Model conditions on interpretable controller parameters (PID/LQR) which may yield more interpretable conditioning signals; not directly comparable to neural-policy-conditioned models.",
            "interpretability_method": "Uses controller parameter labels as conditioning (interpretable parameter space); no other interpretability methods discussed here.",
            "computational_cost": "Not detailed in this paper; method requires controller-parameter labels for training which limits scalability to large neural policies.",
            "efficiency_comparison": "Advantages in long-horizon fidelity but limited in applicability to deep RL because the required controller parameter labels may not scale to large neural network policies.",
            "task_performance": "Better long-horizon prediction in their reported settings, but limited in deep RL contexts with complex policies.",
            "task_utility_analysis": "Non-autoregressive conditioning on low-dimensional controller parameters can reduce compounding error, but this approach does not readily extend to neural-network policies.",
            "tradeoffs_observed": "Accuracy at long horizons vs requirement of controller-parameter labels (limits applicability).",
            "design_choices": "Predict future states conditioned on controller parameters rather than per-step actions.",
            "comparison_to_alternatives": "Lambert-style models can outperform autoregressive models at long horizons if controller labels are available, unlike PolyGRAD which targets neural policies without requiring controller labels.",
            "uuid": "e1195.5",
            "source_info": {
                "paper_title": "World Models via Policy-Guided Trajectory Diffusion",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Jackson Policy-Guided Diffusion",
            "name_full": "Policy-guided diffusion (Jackson et al. 2023)",
            "brief_description": "Concurrent work that guides diffusion sampling with a policy to increase likelihood under the policy, used to generate synthetic datasets for offline/off-policy RL.",
            "citation_title": "Policy-guided diffusion.",
            "mention_or_use": "mention",
            "model_name": "Policy-guided diffusion (Jackson et al.)",
            "model_description": "Guided diffusion approach where diffusion sampling is steered by the policy to increase likelihood of generated trajectories under that policy; used for synthetic data generation in offline RL contexts.",
            "model_type": "policy-guided diffusion (classifier/score guidance variant)",
            "task_domain": "Offline, off-policy RL dataset generation (discussed as related work)",
            "fidelity_metric": "Not detailed in this paper; Jackson et al. use policy guidance to bias diffusion samples toward policy-likely trajectories.",
            "fidelity_performance": "Not quantified here; used to generate high-likelihood under-policy data for offline settings.",
            "interpretability_assessment": "Black-box diffusion with policy gradient guidance; interpretability not discussed in this paper.",
            "interpretability_method": "None mentioned here.",
            "computational_cost": "Not provided here; guidance may add gradient computations per diffusion step similar to PolyGRAD but Jackson et al. use it for offline dataset synthesis rather than imagined on-policy RL.",
            "efficiency_comparison": "Jackson et al.'s approach is similar in spirit to PolyGRAD's guidance but applied to offline data synthesis; differences discussed include intended use-case (offline vs online/on-policy imagination) and analysis in current paper.",
            "task_performance": "Used to generate synthetic datasets for offline RL; not directly compared numerically in this paper.",
            "task_utility_analysis": "Shows that policy-guided diffusion can be used to bias diffusion outputs toward trajectories compatible with a policy; PolyGRAD extends this idea to on-policy imagined rollouts and provides additional analysis.",
            "tradeoffs_observed": "Similar conceptual trade-offs regarding strength of guidance vs action-distribution matching; direct comparisons not provided.",
            "design_choices": "Guidance of diffusion by policy gradients to bias generated trajectories to be policy-likely; used for dataset generation in offline RL.",
            "uuid": "e1195.6",
            "source_info": {
                "paper_title": "World Models via Policy-Guided Trajectory Diffusion",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Diffusion world models.",
            "rating": 2,
            "sanitized_title": "diffusion_world_models"
        },
        {
            "paper_title": "Mastering diverse domains through world models.",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Planning with diffusion for flexible behavior synthesis.",
            "rating": 2,
            "sanitized_title": "planning_with_diffusion_for_flexible_behavior_synthesis"
        },
        {
            "paper_title": "Policy-guided diffusion.",
            "rating": 2,
            "sanitized_title": "policyguided_diffusion"
        },
        {
            "paper_title": "Transformers are sample efficient world models.",
            "rating": 2,
            "sanitized_title": "transformers_are_sample_efficient_world_models"
        },
        {
            "paper_title": "Learning accurate long-term dynamics for model-based reinforcement learning.",
            "rating": 1,
            "sanitized_title": "learning_accurate_longterm_dynamics_for_modelbased_reinforcement_learning"
        }
    ],
    "cost": 0.01770725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>World Models via Policy-Guided Trajectory Diffusion
27 Mar 2024</p>
<p>Marc Rigter 
Jun Yamada 
Ingmar Posner 
World Models via Policy-Guided Trajectory Diffusion
27 Mar 20244E943F2F21C8E676263C3CFCBCF910B0arXiv:2312.08533v4[cs.LG]
World models are a powerful tool for developing intelligent agents.By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination".Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy.Prediction error inevitably compounds as the trajectory length grows.In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model.Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory.We analyse the connections between PolyGRAD, score-based generative models, and classifier-guided diffusion models.Our results demonstrate that PolyGRAD outperforms state-of-the-art baselines in terms of trajectory prediction error for short trajectories, with the exception of autoregressive diffusion.For short trajectories, PolyGRAD obtains similar errors to autoregressive diffusion, but with lower computational requirements.For long trajectories, PolyGRAD obtains comparable performance to baselines.Our experiments demonstrate that PolyGRAD enables performant policies to be trained via on-policy RL in imagination for MuJoCo continuous control domains.Thus, PolyGRAD introduces a new paradigm for accurate on-policy world modelling without autoregressive sampling.</p>
<p>Introduction</p>
<p>Model-based reinforcement learning (RL) trains a predictive model of the environment dynamics, conditioned upon actions.Such models are often referred to as world models.Once a world model has been learnt, imagined (i.e.synthetic) data generated from the world model can be utilised for planning (Schrittwieser et al., 2020) or on-policy RL (Hafner et al., 2021).By distilling the knowledge obtained about the environment into the world model, this approach facilitates improved sample efficiency relative to model-free approaches (Micheli et al., 2023) and zero-shot transfer to new tasks (Sekar et al., 2020).</p>
<p>A core challenge of this approach is learning a sufficiently accurate world model: If the world model is inaccurate, the imagined data will not be representative of the real environment, and the actions cannot be optimised effectively (Janner et al., 2019).Previous approaches have mitigated modelling errors by (a) using ensembles of models (Chua et al., 2018); (b) predicting the dynamics in latent space with a recurrent model (Ha &amp; Schmidhuber, 2018;Hafner et al., 2021); or (c) using powerful generative models such as transformers (Micheli et al., 2023;Robine et al., 2023) or diffusion models (Alonso et al., 2023) to generate accurate predictions.Common to all of these existing approaches is that they learn a single-step transition model, which conditions upon an action sampled from the current policy and predicts the next state.The model is unrolled autoregressively to generate imagined on-policy trajectories one step at a time.Modelling error accumulates at each step, meaning that the error in the trajectory inevitably grows as the trajectory length increases (Lambert et al., 2022).In this work, we propose an approach to world modelling that is not autoregressive, and generates entire on-policy trajectories in a single pass of diffusion.The core challenge for creating a world model that is not autoregressive is ensuring that the trajectories are generated using actions sampled according to the current policy.In other words: How do we sample actions from the policy at future states if these states have not yet been predicted?This prohibits the standard approach of directly sampling actions from the policy.Instead, Policy-Guided Trajectory Diffusion (PolyGRAD) starts with a trajectory of random states and actions and gradually diffuses it into an on-policy synthetic trajectory (Figure 1).To accomplish this, PolyGRAD utilises a learnt denoising model, in addition to the gradient of the action distribution of the current policy.The denoising model is responsible for generating accurate predictions of the environment dynamics, while the policy is used to guide the diffusion process to on-policy trajectories.We analyse how our work can be viewed either as using a score-based generative model to generate on-policy actions, or as an instance of classifier-guided diffusion.</p>
<p>The core contributions of this work are: (a) proposing PolyGRAD, the first approach to world modelling that enables on-policy trajectory generation without autoregressive sampling, and (b) analysing the connection between PolyGRAD, score-based generative models, and classifier-guided diffusion models.Our results demonstrate that PolyGRAD outperforms state-of-the-art baselines in terms of trajectory prediction error for short trajectories, with the exception of autoregressive diffusion.For short trajectories, PolyGRAD obtains similar errors to autoregressive diffusion, but with lower computational requirements.For long trajectories, PolyGRAD obtains comparable performance to baselines.We show that PolyGRAD can be used to optimise performant policies via on-policy RL using only imagined data for MuJoCo continuous control domains.Thus, PolyGRAD introduces a new paradigm for developing accurate world models without autoregressive sampling.</p>
<p>Related Work</p>
<p>World Models Model-based RL methods (Sutton, 1991) learn a model that predicts the transition dynamics and rewards in the environment, conditioned on previous observations and the current action.Such a model is commonly referred to as a "world model" (Ha &amp; Schmidhuber, 2018;Kaiser et al., 2020).Earlier works on model-based RL used MLP ensembles (Chua et al., 2018;Yu et al., 2020) or Gaussian processes (Deisenroth &amp; Rasmussen, 2011) to model the environment dynamics.More recently, the predominant approach has been to use a VAE (Kingma &amp; Welling, 2013) to map observations to a compact latent space, and predict dynamics in the latent space using a recurrent model (Ha &amp; Schmidhuber, 2018;Hafner et al., 2019;Doerr et al., 2018;Hafner et al., 2021).Such models are able to learn from high-dimensional observations in partially-observable environments.Transformers (Vaswani et al., 2017) have been used as more powerful sequence models for generating accurate dynamics predictions (Micheli et al., 2023;Robine et al., 2023;Schubert et al., 2023;Chen et al., 2022).Likewise, concurrently to our work, diffusion models have also been utilised to generate accurate predictions of individual state transitions in a world model (Alonso et al., 2023).Once a world model has been learnt, it can be utilised for planning (Schrittwieser et al., 2020;Ye et al., 2021) or policy optimisation (Janner et al., 2019).Crucially, by conditioning on actions sampled from the current policy, world models enable the generation of unlimited synthetic on-policy data, thus enabling on-policy RL "in imagination" (Hafner et al., 2019).The generation of on-policy synthetic trajectories also makes it possible to visualise the behaviour of the policy before deploying it (Wu et al., 2023).Learning policies entirely from synthetic data has the potential to improve sample efficiency (Micheli et al., 2023) and to facilitate transfer to new tasks without any further environment interaction (Sekar et al., 2020;Rigter et al., 2023).</p>
<p>Common to all of the aforementioned approaches is that they learn a single-step transition model, and use this model autoregressively to generate imagined trajectories of states and actions, one step at a time.In autoregressive models, error accumulates as the length of the prediction grows (Janner et al., 2019;Lambert et al., 2022) and therefore the length of the imagined trajectories is typically limited to a small number of steps (Janner et al., 2019;Hafner et al., 2021).Notably, Lambert et al. (2021) propose to learn a predictive model of environment dynamics that is not autoregressive.Instead, the model conditions upon the parameters of a simple controller, such as a PID or LQR controller, and is trained to predict the future state at an arbitrary point in time.Lambert et al. (2021) show that this approach achieves lower error at long-horizon predictions than simple autoregressive baselines.However, a limitation of Lambert et al. (2021) is that the controller parameters used to collect each transition in the dataset must be used as a training label.Therefore, this approach is not readily applicable to deep RL, where the number of policy parameters can be very large (i.e.millions).Like Lambert et al. (2021) we investigate an approach to trajectory generation that is not autoregressive.However, we aim to sample trajectories under a neural network policy defined by a large number of parameters.To enable this, we propose a novel world model that generates entire on-policy trajectories in a single pass of diffusion.To our knowledge, this is the first work to propose a method for non-autoregressive on-policy world modelling that is compatible with complex policies.</p>
<p>Diffusion Models for Decision-making Diffusion models are a powerful class of generative model that formulates data generation as an iterative denoising procedure (Sohl-Dickstein et al., 2015;Ho et al., 2020).Learning the denoising model is equivalent to learning the gradients of the data distribution, and therefore diffusion models are an instance of score-based generative models (Song et al., 2021).One advantage of diffusion models is that the iterative sampling procedure enables flexible conditioning, which is commonly utilised to generate bespoke high-quality synthetic images (Dhariwal &amp; Nichol, 2021;Ho &amp; Salimans, 2021).</p>
<p>In the context of sequential decision-making, the use of diffusion models was first introduced by Janner et al. (2022).The authors proposed to train a diffusion model to generate trajectories of states and actions, and use classifier-guidance to guide the trajectories towards goal states or high rewards thus enabling the diffusion model to be used as planner.This approach was subsequently extended to classifier-free guidance on state-only trajectories, with an inverse kinematics model to infer the actions (Ajay et al., 2023).Guided diffusion models have also proven to be highly effective in the multi-task setting (He et al., 2023).In each of these works, the output of the diffusion model is used to determine the action to be taken at each time step, making action selection very slow.Furthermore, unlike world models, trajectory generation cannot be conditioned on a specific policy or actions.</p>
<p>In another line of work, diffusion models have been used extensively as policies capable of accurately representing multi-modal action distributions.Such policies are particularly useful for offline RL (Hansen-Estruch et al., 2023;Wang et al., 2023) and imitation learning (Pearce et al., 2023;Chi et al., 2023), where it is important for the policy to remain near the action distribution in the dataset.In our work, we learn a separate feedforward policy that can be queried quickly during real-world rollouts, and use the diffusion model to generate on-policy synthetic data for policy training.</p>
<p>There are several previous works that use diffusion models to generate synthetic data for RL (Zhu et al., 2023).SynthER (Lu et al., 2023) trains a diffusion model to generate individual steps of off-policy synthetic data.This data is used to augment the replay buffer of an off-policy RL algorithm to improve sample efficiency.Ding et al. (2024) build on SynthER by generating off-policy trajectories rather than individual transitions.Similarly, MTDIFF-S (He et al., 2023) generates off-policy trajectories to augment offline RL datasets, but in the multi-task setting.Concurrent works on world modelling with diffusion (Alonso et al., 2023;Zhang et al., 2023;Yang et al., 2023) train a diffusion model to generate a single step of transition data, conditioned on the current action and previous observations.Like other existing world models, this approach enables the generation of synthetic trajectories by sampling actions from the policy and autoregressively querying the model one step at a time.In contrast, our approach generates entire on-policy trajectories in a single pass of diffusion.Also concurrently, and most related to our work, Jackson et al. (2023) guide diffusion with a policy to increase the likelihood of generated trajectories under the policy, in a similar manner to PolyGRAD.Jackson et al. (2023) use this approach to generate synthetic datasets for offline, off-policy RL.In contrast, PolyGRAD generates on-policy trajectories for online, on-policy RL in imagination.Unlike Jackson et al. (2023), we also analyse the connection between PolyGRAD, classifier-guidance, and score-based generative models.Furthermore, we show that the scale of the action guidance can be tuned automatically online to approximately generate the correct on-policy action distribution.</p>
<p>Preliminaries</p>
<p>Throughout this paper, we use subscript indices to refer to steps in a diffusion process and superscript indices to refer to time steps in a trajectory through the environment.Thus, x i refers to a data sample at the i th step in a diffusion process, while a t refers to an action at the t th step in a trajectory.If a symbol has no subscript, it refers to a data sample that has had no noise added (i.e.x = x 0 ).Bold symbols refer to matrices.</p>
<p>Denoising Diffusion Probabilistic Models Diffusion models (Ho et al., 2020;Sohl-Dickstein et al., 2015) are a class of generative models.Consider a sequence of positive noise scales, 0 &lt; β 1 , β 2 , . . ., β N &lt; 1.In the forward process, for each training data point x 0 ∼ p data (x), a Markov chain
x 0 , x 1 , . . . , x N is constructed such that p(x i | x i−1 ) = N (x i ; √ 1 − β i x i−1 , β i I). Therefore, p αi (x i | x 0 ) = N (x i ; √ α i x 0 , (1 − α i )I),
where
α i := Π i j=1 (1 − β j ).
We denote the perturbed data distribution as p αi (x i ) := p data (x)p αi (x i | x)dx.The noise scales are chosen such that x N is distributed according to N (0, I).Define s(x i , i) to be the score function of the perturbed data distribution: s(x i , i) := ∇ xi log p αi (x i ), for all i.Samples can be generated from a diffusion model by starting from x N ∼ N (0, I) and following the recursion:
x i−1 = 1 √ 1 − β i (x i + β i s θ (x i , i)) + β i z,(1)
where s θ is a learnt approximation to the true score function s, and z is a sample from the standard normal distribution.If we reparameterize the sampling of the noisy data points according to:
x i = √ α i x 0 + √ 1 − α i ϵ,
where ϵ ∼ N (0, I), we observe that
∇ xi log p αi (x i | x 0 ) = − ϵ √ 1 − α i . (2)
Thus, estimating the score function is equivalent to estimating the noise added.Therefore, we can define the estimated score function in terms of a function ϵ θ that predicts the noise ϵ used to generate each sample
s θ (x i , i) := − ϵ θ (x i , i) √ 1 − α i . (3)
The noise prediction model ϵ θ is trained to optimise the objective
θ * = arg min θ N i=1 E x0∼p data (x) E ϵ∼N (0,I) ||ϵ − ϵ θ ( √ α i x 0 + √ 1 − α i ϵ, i)|| 2 . (4)
Sequential Decision-Making under Uncertainty We consider the setting of fully-observable Markov decision processes (MDPs).An MDP is defined by the tuple, M = (S, A, T, R, µ 0 , γ). S and A are the state and action spaces, and µ 0 is the initial state distribution.T : S × A → ∆(S) is the transition function, where ∆(X) denotes the set of possible distributions over X, and γ is the discount factor.R : S × A → ∆(R) is the reward function.A policy, π, maps each state to a distribution over actions: π : S → ∆(A).We will write τ = s 0 , a 0 , r 0 , . . ., s h , a h , r h to refer to a trajectory of states, actions, and rewards in an MDP with horizon h.τ a refers to the sequence of actions only, and τ sr refers to the sequence of states and rewards only.The standard objective for MDPs is to find the policy which maximises the total expected discounted reward.</p>
<p>Model-Based Reinforcement Learning and World Models</p>
<p>Model-based approaches to reinforcement learning (Sutton, 1991) (RL) utilise a predictive model of the environment dynamics, commonly referred to as a world model (Ha &amp; Schmidhuber, 2018).During online rollouts, trajectories are collected from the real environment and stored in data buffer D. World models typically use D to learn a single-step transition model, T , as an approximation to the dynamics of the environment, T .If observations are high-dimensional, this dynamics model is usually learnt in a compressed latent representation of the state (Hafner et al., 2019).Additionally, a model of the reward function R is also learnt from the data.</p>
<p>Once the world model has been trained, autoregressive sampling can be used to generate synthetic on-policy trajectories.To generate an imagined trajectory, τ , we first sample an initial state, s 0 .We then sample an action from the policy conditioned on this state, a 0 ∼ π(•|s 0 ).From the world model, we sample a reward r 0 ∼ R(•|s 0 , a 0 ) and a successor state, s 1 ∼ T (•|s 0 , a 0 ).This process is repeated step-by-step until a trajectory of the desired length has been generated: τ = s 0 , a 0 , r 0 . . ., s h , a h , r h .Because the world model is only an approximation to the environment, the error between imagined and real trajectories accumulates as the length of the trajectory grows.To mitigate this issue, small values of the horizon h are typically used (Janner et al., 2019;Hafner et al., 2021).The imagined trajectories can then be utilised for online planning (Argenson &amp; Dulac-Arnold, 2021;Schrittwieser et al., 2020) or on-policy RL (Hafner et al., 2019;2021) to optimise decision-making.In this work, we will focus on the latter case of on-policy RL for policy optimisation.</p>
<p>World Models via Policy-Guided Trajectory Diffusion</p>
<p>In this work, we propose a new approach to world modelling: Policy-Guided tRAjectory Diffusion (PolyGRAD).</p>
<p>A core novelty of PolyGRAD is that it enables the generation of entire on-policy trajectories in a single pass of diffusion, rather than autoregressively chaining a sequence of one-step predictions.The main challenge for creating a non-autoregressive world model is ensuring that the trajectories generated are sampled according to the current policy.To address this challenge, PolyGRAD utilises diffusion to gradually diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory.</p>
<p>PolyGRAD utilises two learned components: a denoising model, ϵ θ , and a policy, π ϕ , defined by parameters θ and ϕ respectively.In this section, we first describe the denoising model, ϵ θ .Second, we describe our main contribution, PolyGRAD, which uses the denoising model in conjunction with the policy to generate synthetic on-policy trajectories via diffusion.Finally, we provide an algorithm for imagined RL in PolyGRAD world models that automatically tunes the magnitude of the action updates in an outer loop.</p>
<p>Denoising Model Training (Algorithm 1)</p>
<p>Algorithm
(τ sr , τ a ) ∼ D 4: i ∼ Uniform(1, N ) 5: ϵ ∼ N (0, I) 6:
Take gradient descent step on
∇ θ ||ϵ − ϵ θ ( √ α i τ sr + √ 1 − α i ϵ | i, τ a )|| 2
Denoising model training in Algorithm 1 follows the same process as a standard diffusion model, with the exception that we add action-conditioning.To train the denoising model, we sample a sequence of states and rewards, τ sr = s 0 , r 0 . . ., s h , r h , and a sequence of actions, τ a = a 0 , . . ., a h , from the data buffer, D. The step in the diffusion process, i, is also sampled.Random noise √ 1 − α i ϵ is added to the state and reward sequence,</p>
<p>where ϵ is sampled according to a standard normal distribution and α i is defined by the diffusion noise schedule.The denoising model is trained to predict the noise added to the original state and reward sequence, conditioned upon the actions and the diffusion step.In our implementation, the denoising model is also trained to predict whether a state is terminal, but we omit this from our notation to avoid clutter.</p>
<p>Policy-Guided Trajectory Diffusion (Algorithm 2)</p>
<p>We now present PolyGRAD, our algorithm for generating imagined on-policy trajectories for world modelling (Algorithm 2).Each of the key steps of the algorithm is illustrated in Figure 2. PolyGRAD begins with a trajectory of random states, rewards, and actions, τ N = ( τ sr N , τ a N ), sampled from a standard normal distribution (Lines 2 and 3).Note that the subscript N refers to the step in the diffusion process.We also sample an initial state s 0 uniformly from D. The trajectory is then iteratively refined over many diffusion steps.To condition the trajectory on the initial state from the dataset s 0 , we perform inpainting (Lugmayr
ϵ ← ϵ θ ( τ sr i | i, τ a i ) ▷ Predict noise 8: if i &gt; 1 : 9: τ sr 0 ← 1 √ α i • τ sr i − √ 1−α i √ α i
• ϵ ▷ Predict fully-denoised state sequence 10:
τ a i−1 ← τ a i + δ • ∇ τ a i log π ϕ ( τ a i | τ s 0 ) + √ βiz ▷τ a i−1 ← τ a i 13: τ sr i−1 ← 1 √ 1−β i ( τ sr i − β i √ 1−α i ϵ ) + √ β i z
▷ Update states and rewards using diffusion (Equation 1)
14: return τ = ( τ sr 0 , τ a 0 )
et al., 2022) by replacing the initial state in the trajectory by s 0 at each step of the diffusion process (Line 6).</p>
<p>Generating rollouts branched from states in the dataset is standard practice in model-based RL (Janner et al., 2019).At each diffusion step, the denoising model is used to predict the noise added to the state and reward sequence, conditioned on the current action sequence (Line 7).The predicted noise is denoted by ϵ.</p>
<p>In all steps except for the final diffusion step, the action sequence is updated in Lines 9 and 10.To perform the action update, we first use the predicted noise ϵ to compute a prediction of the fully-denoised state sequence, τ s 0 .We compute this prediction because the policy is trained on data without noise added, so we do not wish to condition the policy on a noisy state sequence.In Line 10, we condition the policy on the predicted denoised state sequence.We update the action sequence in the direction of the score of the policy action distribution, ∇ a log π ϕ (a|s), in Line 10.Note that this gradient differs from the standard gradient used in policy-gradient RL algorithms (which is ∇ ϕ log π ϕ (a|s)), and can be easily computed for common policy parameterisations, such as Gaussian policies.The magnitude of the update to the actions is controlled by the action update scale, δ, which as we shall see in Section 4.3, is tuned automatically online.In Line 13 the state and reward sequence is updated using the noise prediction and a standard diffusion update (Equation 1).After completing all the diffusion steps, the final synthetic trajectory is returned in Line 14.</p>
<p>In summary, the action sequence is updated using the gradient of the policy action distribution ∇ a log π ϕ (a|s) to increase the likelihood of the actions under the policy.The state and reward sequence is updated according to the learnt denoising model.We must ensure that the state and action sequences are consistent (i.e. the actions are sampled according to the policy conditioned on the predicted states, and the states are predicted according to the sampled actions).To ensure this consistency, the policy is conditioned on the current predicted denoised state sequence, and the denoising model is conditioned on the current action sequence.Provided that the action update size δ is chosen correctly, during diffusion the actions are iteratively updated until they are similar to samples drawn from the policy.The state-reward sequence is predicted conditioned upon those actions.Therefore, PolyGRAD is able to generate on-policy synthetic trajectories.Now that we have described the mechanics of our algorithm, we provide theoretical motivation for PolyGRAD by comparing it to score-based generative models and classifier-guided sampling.</p>
<p>Connection to Score-Based Generative Models Langevin dynamics (Rossky et al., 1978;Neal, 2011) is a Markov chain Monte Carlo method that produces samples from a probability density p(x) using only the score function ∇ x log p(x).Given a fixed step size β &gt; 0, and an arbitrary initial value x 0 , the Langevin method recursively computes
x t = x t−1 + β 2 ∇ x log p(x t−1 ) + βz, z ∼ N (0, I). (5)
When β → 0 and t → ∞, Langevin dynamics converges to a sample from p(x) (Song &amp; Ermon, 2019;Neal, 2011).Score-based generative models attempt to learn an approximation to the score function, s θ (x) ≈ ∇ x log p(x), and then perform sampling according to Equation 5 using s θ .As noted in Section 3, diffusion models are in instance of score-based generative models, where the score of the noisy data distribution is learnt across a range of noise levels.</p>
<p>In Line 10 of Algorithm 2, we update the action sequence using the same update rule as the Langevin method (Equation 5), but with a few changes.Unlike a standard score-based generative model, where an approximation to the score function is learned, in PolyGRAD we directly use the score function of the policy action distribution, ∇ a log π(a|s).Furthermore, the policy score function is conditioned upon the current predicted denoised state sequence, τ s 0 (which also changes throughout the diffusion process).The final modification is that we allow the update size to be a tuneable parameter, δ.Because the action sequence is updated according Langevin dynamics, we might expect it should converge to a sample near the policy action distribution, provided that: a) the state sequence τ s 0 converges, b) the number of diffusion steps is large, and c) δ and β are sized appropriately.A formal analysis of convergence is outside the scope of this work.</p>
<p>Connection to Classifier-Guidance</p>
<p>In classifier-guided sampling (Dhariwal &amp; Nichol, 2021), the aim is to sample from the conditional distribution p(x | y), where y is a label.Recall that p αi denotes the noisy data distribution in the i th step of a diffusion model.To enable sampling from p(x | y) using diffusion (Equation 1), instead of approximating the score function ∇ xi log p αi (x), we would like to approximate the score function ∇ xi log p αi (x i | y).From Bayes' rule, we have that
∇ xi log p αi (x i | y) = ∇ xi log p αi (x i ) + ∇ xi log p(y | x i ).
Therefore, if we have a classifier p(y | x i ), the gradient of the classifier with respect to x i can be used to guide the diffusion process to sample from the conditional distribution p(x | y).Note that the classifier is evaluated on noisy samples, x i , so the classifier is typically also trained on noisy samples.</p>
<p>In our work, we are interested in sampling on-policy trajectories from the distribution p(τ | π ϕ ), where π ϕ is the current policy.To directly apply classifier-guided sampling to this problem, we would need to learn a differentiable classifier p(π ϕ | τ i ) so that we can evaluate ∇ τi log p(π ϕ | τ i ).However, it is unclear how to train such a classifier.To overcome this issue, let us consider τ , a trajectory that has had no noise added.Applying Bayes' rule, we have that
p(π ϕ | τ ) = p(π ϕ , τ ) p(τ ) = p(π ϕ )µ(s 0 )π ϕ (a 0 |s 0 )R(r 0 |s 0 , a 0 )T (s 1 |s 0 , a 0 ) . . . µ(s 0 )p(a 0 |s 0 )R(r 0 |s 0 , a 0 )T (s 1 |s 0 , a 0 ) . . . = p(π ϕ )π ϕ (a 0 |s 0 )π ϕ (a 1 |s 1 ) . . . p(a 0 |s 0 )p(a 1 |s 1 ) . . . ,(6)
where p(a|s) represents the probability of action a being selected at state s under any policy.Taking the gradient with respect to τ , we have
∇ τ log p(π ϕ | τ ) = h i=0 ∇ τ log π ϕ (a i |s i ) − h i=0 ∇ τ log p(a i |s i ).(7)
If we assume that p(a|s) is a uniform distribution over actions for all s, then the second term is zero.Therefore, under this assumption that by default all actions are equally likely, as well as the assumption that no noise has been added to the trajectory, we arrive at the final classifier gradient:
∇ τ log p(π ϕ | τ ) = i ∇ τ log π ϕ (a i |s i ). (8)
Deriving the correct classifier-guidance gradient for the case where noise has been added to the trajectory is a subject for future work.To avoid conditioning the policy on noisy trajectories, in PolyGRAD we use the denoising model to predict the fully denoised state sequence, τ s 0 , and condition the policy on this sequence to compute the score function (Line 9 of Algorithm 2).Thus, by considering classifier guidance we arrive at a similar update rule for the actions to that used by PolyGRAD.However, the classifier-guidance inspired update in Equation 8indicates that both actions and states in the trajectory should be updated in the direction of the score of the policy distribution.Meanwhile, in Line 10 of the PolyGRAD algorithm (Algorithm 2), we update only the action sequence in this manner.The state sequence is updated using the denoising model only.In the experiments, we assess the performance of PolyGRAD when both the action and state sequences are updated according to Equation 8. Train π ϕ on { τ } via on-policy RL</p>
<p>Imagined RL in</p>
<p>9:</p>
<p>Update action update scale, δ ▷ Equation 10Following previous works on world models for RL (Hafner et al., 2019;2021), we optimise the policy by performing on-policy RL on the imagined trajectories generated by PolyGRAD.We assume that for each state, the policy π ϕ outputs a Gaussian distribution over actions: π ϕ (a|s) = N (µ ϕ (s), σ ϕ (s)).This is the most commonly used policy parameterisation in deep RL (Schulman et al., 2017;Haarnoja et al., 2018).For the most part, Algorithm 3 follows a standard model-based RL training loop.Data is gathered from the real environment, and used to train the denoising model ϵ θ .PolyGRAD is then used to generate a batch of on-policy synthetic trajectories.These trajectories are used to update the policy via on-policy RL.The key difference with respect to standard model-based RL is that Line 9 of Algorithm 3 updates δ, which controls the magnitude of the action updates in PolyGRAD.</p>
<p>To perform the update for δ, we consider the set of state-action pairs in the synthetic trajectories generated by PolyGRAD, { τ }.For each state-action pair, (s i , a i ), we standardise the action according to the mean and standard deviation of the policy action distribution at that state:
a i = a i − µ ϕ (s i ) σ ϕ (s i ) . (9)
We then compute σ a , the standard deviation of the set of standardised actions {a}.If the actions are drawn correctly from the policy distribution, then the standardised actions should be distributed according to a standard normal distribution.Therefore, we update the action update scale to obtain a standard deviation of 1 in the standardised actions:
δ ← δ + η • (σ a − 1), (10)
where η is a learning rate.The intuition for Equation 10 is as follows.If the policy guidance is too strong, all of the actions will be guided to be very near to the mean of the policy distribution.Therefore, the standardised actions will have a variance that is too low (i.e.below 1) and Equation 10will reduce the action update scale.Likewise, if the guidance is too weak, the actions will remain spread out far from the policy mean.Thus, the standardised actions will have a variance that is too high and Equation 10 will increase the action update scale.As we shall show in the experiments, we found that tuning the action update sizing in this manner is sufficient to ensure the PolyGRAD generates a good approximation of the correct on-policy action distribution.</p>
<p>Implementation Details Here, we outline some key implementation details.A detailed description of our implementation is in Appendix A. After performing each update to the actions in Line 10 of Algorithm 2, we clip the actions to be within 3 standard deviations of the mean of the policy action distribution.We found that this helps to ensure that the action distribution can be reliably guided to the correct on-policy distribution.For imagined RL training in Algorithm 3, we used Advantage Actor Critic (A2C) (Mnih et al., 2016) with Generalised Advantage Estimation (GAE) (Schulman et al., 2016).We observed that RL training with PolyGRAD world models is unstable when there are large updates to the policy: if the policy is changed drastically, PolyGRAD may not consistently produce the correct action distribution for on-policy RL training, leading to policy collapse.Therefore, we decay the learning rate for the policy to maintain a constant update size.For the denoising network, we use either a residual MLP or a transformer.As described in Algorithm 1, the denoising network is trained using actions that have no noise added, despite the fact that it is evaluated on (initially) noisy actions in Algorithm 2. However, we found this obtained much better prediction errors and RL performance than training the denoising network on noisy actions.</p>
<p>Experiments</p>
<p>In our experiments, we seek to answer the following questions: (a) Does PolyGRAD produce the correct action distribution?(b) How accurate are the trajectories produced by PolyGRAD compared to autoregressive world models?(c) Can the synthetic data produced by PolyGRAD be used to train performant policies?(d) Which implementation details influence the performance of PolyGRAD?To answer these questions, we run experiments using the MuJoCo environments in OpenAI Gym (Brockman et al., 2016).The code for our experiments is available at github.com/marc-rigter/polygrad-world-models.</p>
<p>Does PolyGRAD produce the correct action distribution?</p>
<p>We wish to evaluate whether PolyGRAD generates synthetic trajectories with the correct on-policy action distribution.To evaluate the action distribution produced, we train an RL policy using synthetic data generated by PolyGRAD (Algorithm 3).We linearly decay the standard deviation of the Gaussian policy, σ ϕ , from 1 to 0.02 over the course of training.In Figure 3, we plot the distribution of the difference between the action a and the mean of the action distribution, µ ϕ (s), over all state-action pairs (s, a) in batches of data produced by PolyGRAD with h = 50 in Walker2d.</p>
<p>We observe that for σ ϕ ≥ 0.1, the action distribution closely matches the target Gaussian distribution.This demonstrates that PolyGRAD produces the correct action distribution on aggregate provided that the policy entropy is not too low.However, we observe that as the policy standard deviation decreases below 0.1, the action distribution begins to deviate from the target distribution.Thus, if the policy has very low entropy, it is difficult for PolyGRAD to guide the initially randomly sampled actions to the target distribution.In the plot with σ ϕ = 0.02, the distribution has additional modes at ±3 standard deviations.This is an artifact of the action clipping described in the implementation details.Plots for the other MuJoCo environments are provided in Appendix B.3 of the supplementary material.</p>
<p>How accurate are the trajectories produced by PolyGRAD compared to existing world models?</p>
<p>To assess the accuracy of the trajectories produced by PolyGRAD, we train several different world models using the same dataset of 1M transitions in each environment.We then generate synthetic rollouts using the same performant policy in each world model.We replay the same actions in the MuJoCo simulator, and evaluate the errors in the trajectory predictions.We compare the accuracy of the trajectories against the following baselines which all generate trajectories autoregressively:</p>
<p>• Probabilistic MLP Ensemble, an ensemble of MLPs outputting a Gaussian over the next state (Chua et al., 2018;Yu et al., 2020;Janner et al., 2019).• Transformer, a transformer-based world model (Micheli et al., 2023).</p>
<p>• Dreamer-v3 (Hafner et al., 2023), a world model based on a sequential VAE.</p>
<p>• Autoregressive Diffusion, a diffusion model trained to generate one-step action-conditioned predictions, rolled out autoregressively (Alonso et al., 2023;Zhang et al., 2023).</p>
<p>Detailed descriptions of the implementation of each baseline can be found in Appendix C.3, and further details on the experimental setup can be found in Appendix C.1.Autoregressive Diffusion tends to be the most performant approach, especially on HalfCheetah.This indicates that for these MuJoCo environments, training a very accurate one-step prediction model leads to accurate trajectories even at longer horizons.However, the downside of Autoregressive Diffusion is that it is the slowest world modelling method (Figure 4) as it requires running diffusion at every step of the trajectory rollout, whereas PolyGRAD requires only a single pass of diffusion.</p>
<p>Can the synthetic data produced by PolyGRAD be used to train performant policies?</p>
<p>Following from previous works on model-based RL (Janner et al., 2019;Hafner et al., 2021;Yu et al., 2020), we use short imagined rollouts for model-based RL training (h = 10).For RL training, we use the residual MLP denoising network as it obtains similar error to the transformer denoising network at h = 10 (see Figure 10 in Appendix B.1) but it is faster for both training and inference (Figure 4).</p>
<p>To assess the performance of RL training in imagination for PolyGRAD (Algorithm 3), we compare the rewards obtained against model-free on-policy RL algorithms (PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and A2C (Mnih et al., 2016)).We compare against on-policy model-free RL algorithms because we perform imagined on-policy training in the PolyGRAD world model.However, note that off-policy model-free RL algorithms obtain stronger performance for these environments (Haarnoja et al., 2018).To compare against a state-of-the-art world model-based agent, we also compare against Dreamer-v3 (Hafner et al., 2023).Reward curves for each algorithm are shown in Figure 6. Figure 7 provides 95% confidence intervals of the interquartile mean of the normalised final performance aggregated across all three environments (see Appendix C.2 for details).PolyGRAD obtains worse sample-efficiency and final performance compared to Dreamer-v3, despite the fact that PolyGRAD obtains better prediction errors than Dreamer-v3 (Figure 5).This may be because Dreamer optimises policies by directly backpropagating through the dynamics model, while we train policies using policy-gradient RL.Alternatively, it may be because by optimising policies in latent space, Dreamer is able to produce more performant policies despite having less accurate reconstructions of the original state.</p>
<p>Which implementation details influence the performance of PolyGRAD?</p>
<p>To assess which implementation details are important, we compare the performance of the base PolyGRAD algorithm against the following ablations and modifications:</p>
<p>• Random Actions: We sample random actions at the beginning of the diffusion process from a standard normal distribution, and they are not updated during diffusion.• Policy Sampling: During each step of the diffusion process, we obtain new actions by directly sampling them from the policy conditioned upon the current predicted state sequence.• No Clipping: The actions are not clipped during the diffusion process.</p>
<p>• Add State Update: In addition to updating the actions according to the score of the policy distribution, we also update the states to increase the likelihood of the actions according to Equation 8. • Noisy State Conditioning: Instead of conditioning the policy on the denoised state prediction ( τ a 0 in Line 10 of Algorithm 2) we condition the policy on the noisy states ( τ a i ).A detailed description of each of these modifications is provided in Appendix C.4. Figure 8 shows the reward curves for each variant of the algorithm, and Figure 9 shows 95% confidence intervals for the interquartile mean of the final normalised performance across all environments.We observe that Random Actions obtains the poorest performance.This is unsurprising as we use on-policy RL to optimise the policy, and therefore randomly sampled off-policy actions result in incorrect policy gradient updates.Policy Sampling also performs very poorly.This method also fails to generate on-policy trajectories due to the following reasoning.Imagine that we sample an entire sequence of actions, conditioned on the current state sequence.Then, conditioned on these new actions, a new prediction of the state sequence is generated at the next diffusion step.However, the policy has a different action distribution at the new state sequence.Thus, the final trajectory of states and actions produced is not on-policy.We also observed that Policy Sampling results in worse prediction errors.This is likely because a diffusion model, which gradually refines its predictions, is ill-suited to making an accurate prediction when the conditioning is completely changed at each diffusion step.Meaningful policies are still learnt when we ablate the action clipping (No Clipping), however the performance is worse, especially for Walker2d.The plots in Appendix B.4 show that without the action clipping, the action distributions produced are more heavy-tailed.Conditioning the policy on the noisy states during diffusion, rather than the denoised prediction (Noisy State Conditioning), also results in worse performance.Additionally updating the states during diffusion according to Equation 8 results in comparable performance to the base PolyGRAD algorithm.However, this modification increases the computation time required to about 80 hours for 1M environment steps (compared to 54 hours for the base algorithm), as it requires backpropagation through the policy at each diffusion step.</p>
<p>Discussion</p>
<p>Limitations There are two key limitations of our approach.The first is that to maintain training stability, we found that it was necessary to update the policy slowly to avoid incorrect action distributions being generated.This likely increases the number of RL training updates required to achieve good performance.</p>
<p>The second is that PolyGRAD struggles to achieve the correct action distribution if the policy has very low entropy.This may prevent PolyGRAD from achieving strong RL performance on some domains.</p>
<p>Future Work</p>
<p>To improve the stability issues mentioned above, we plan to investigate alternative algorithms for diffusing on-policy trajectories, with the aim of finding an algorithm that more reliably generates the correct action distribution.We would like to scale PolyGRAD to more complex environments such as image-based environments by utilising latent diffusion (Rombach et al., 2022).We would also like to test PolyGRAD on non-Markovian environments.PolyGRAD is well-suited to non-Markovian predictions due to the fact that it is trained to predict entire trajectories, rather than making single step predictions under a Markovian assumption.Another direction for future work is to perform a theoretical analysis of the convergence properties of PolyGRAD to better understand if there are conditions under which it is guaranteed to converge to on-policy trajectories.</p>
<p>Finally, and perhaps most importantly, we would like to investigate whether there are situations in which PolyGRAD obtains better prediction errors at long horizons compared to existing autoregressive baselines.The results in Section 5.2 show that while PolyGRAD obtains strong performance when trained and evaluated on short trajectories (h = 10), performance deteriorates relative to baselines for longer trajectories.We hypothesise that PolyGRAD may be more robust than autoregressive models when trained on small datasets, where the predictions of single-step autoregressive models may be prone to quickly leaving the data distribution, resulting in erroneous predictions.Another line of investigation could be to train the denoising network on short trajectories, and use it to generate longer trajectories.Perhaps this may result in stronger performance and generalisation to longer trajectories.A final approach could be to investigate whether latent diffusion is better suited to accurate generation of long trajectories.</p>
<p>Conclusion</p>
<p>We have presented PolyGRAD, the first world model that can generate on-policy trajectories without autoregressive sampling.To enable this, instead of sampling actions directly from the policy, PolyGRAD gradually updates the trajectory of states and actions using diffusion.We have shown that PolyGRAD generates a good approximation to the correct on-policy action distribution, and therefore enables on-policy RL in imagination.PolyGRAD achieves solid performance in terms of trajectory prediction accuracy compared to state-of-the-art autoregressive baselines.Thus, PolyGRAD introduces a promising new paradigm for world modelling, with many possible directions for extensions and improvements in future work.</p>
<p>For the denoising network, we considered both a Residual MLP and a Transformer architecture, both trained to minimise the L2 loss.The Residual MLP has skip connections at each layer.To add conditioning on the step of the diffusion process, we add a learnable embedding of the diffusion step, i.Therefore, each layer has the form:
x L+1 = linear(activation(x L )) + x L + embed(i)
To add action conditioning, we concatenate the actions to the inputs so that the input size is (|S| + |A|) × h where h is the length of the trajectory.The final layer projects the output to the correct dimensionality of |S| × h.</p>
<p>In the transformer denoising network, we first embed each noisy state and action pair using a learnt embedding function.The context length is equal to the length of the trajectory, h.We add conditioning upon the timestep in the diffusion process and the timestep in the trajectory by adding a learned embedding of each.</p>
<p>Each transformer block consists of a LayerNorm followed by a multi-head causal self-attention layer and a 2-layer MLP.The linear output layer projects each of the h embeddings to the correct dimensionality of |S|.Hyperparameter Tuning We reduced the cosine noise schedule parameter τ noise to 0.1 for Hopper-v3 with the Residual MLP denoising network as we found this obtained better prediction errors.We used a larger MLP width of 2048 for the Residual MLP when the trajectory length was greater than 10.All other hyperparameters were kept constant across all environments.We found that the most critical hyperparameter for RL training was the size of the target policy update, target ∆ log π .To tune this parameter, we started with a large value and decreased it until RL training was stable for all MuJoCo environments.</p>
<p>B Additional Results</p>
<p>B.1 Prediction Error Plots with Transformer and MLP Denoising Networks</p>
<p>Figure 10 compares the trajectory errors for PolyGRAD between the transformer denoising network and the residual MLP denoising network.We observe that for short trajectories (h = 10), similar prediction errors are obtained.However, for the longer trajectories the transformer denoising network obtains considerably better performance.</p>
<p>B.2 Confidence Intervals for Mean Performance</p>
<p>In the main body of the paper, we reported 95% confidence intervals for the normalised interquartile mean (IQM) generated using the rliable framework (Figures 7 and 9).To provide a more complete picture, here we provide equivalent confidence intervals for the mean, rather than the IQM.We observe that the confidence intervals for mean are similar to those for IQM.This indicates that there are not many outliers in the results, as the mean is effected by outliers while IQM is not.</p>
<p>The intervals in Figure 12 compare PolyGRAD against other model-based and model-free algorithms.We observe that PolyGRAD outperforms the model-free algorithms for mean performance, but does not perform as well as Dreamer-v3.Figure 14</p>
<p>B.4 PolyGRAD Action Distributions with Action Clipping Ablated</p>
<p>Figure 18 shows plots of the action distributions in Walker2d when the action clipping during diffusion is removed.Blue line illustrates the distribution of a − µ ϕ (s) for a batch of synthetic data and dashed black line is the policy distribution.We observe that when the action clipping is ablated, PolyGRAD tends to produce a heavy-tailed action distribution with some actions very far from the mean.This means that it still obtains the same standard deviation over actions as the policy, but the distribution over actions is no longer Gaussain.Thus, PolyGRAD is less effective at produce the correct action distribution when the action clipping is removed.</p>
<p>C Additional Experiment Details</p>
<p>C.1 Experimental Setup</p>
<p>Error Evaluation Experiments To collect the datasets, we ran Algorithm 3 until we had collected 1M transitions in each environment.Each world model was then trained on the same 1M transitions collected.The final policy produced by Algorithm 3 was used as the policy for sampling actions in each world model.</p>
<p>To generate synthetic rollouts, we sampled 500 initial states uniformly at random from the dataset and generated a synthetic rollout starting from each of these initial states.We computed the average mean squared error of the prediction across all 500 rollouts for each possible horizon length.We continued to train each world model until it obtained the best prediction error evaluation at a 5 step horizon, up to a maximum of either 1M gradient steps or 72 hours of training on an RTX 3090 GPU.</p>
<p>C.2 Results Processing</p>
<p>Smoothing of Reward Curves To generate the reward curves in Figures 6 and 8, the average return over 10 episodes of the current policy is evaluated after every 10,000 environment steps.To generate the plots we perform smoothing by computing a moving average with a window width of 10 evaluations.</p>
<p>Confidence Intervals Figures 7 and 9 present 95% confidence intervals of the interquartile mean (IQM) of the normalised final performance aggregated across all three environments.Figures 12 and 14 are equivalent plots for the mean rather than IQM.To generate the confidence intervals, we use the rliable framework (Agarwal et al., 2021) which is designed to compute confidence intervals with limited seeds by aggregating the runs across all environments, and generating the confidence intervals using stratified bootstrapping.IQM is the performance on the middle 50% of combined runs, and is the metric recommended by rliable as it is robust to outliers (unlike the mean) and is more stastically efficient than the median.</p>
<p>To generate the confidence intervals, we first aggregate the final smoothed rewards obtained at the maximum number of environment steps for each run of each algorithm in Figures 6 and 8. Following Agarwal et al. (2021), we first normalise the rewards in each environment.We normalise the rewards by dividing by the best average final total reward obtained by any algorithm for each environment.We then use the normalised final rewards for each run to compute the aggregated IQM confidence intervals using the rliable package with 50,000 bootstrapping repeats.</p>
<p>C.3 Baselines</p>
<p>Probabilistic MLP Ensemble</p>
<p>We train an ensemble of MLPs that output a Gaussian distribution over the next state.Following Janner et al. (2019) and Yu et al. (2020), we train an ensemble of 7 MLPs with 4 layers and 200 hidden units per layer.Each model outputs the mean and variance over the next state using a two-head architecture.When sampling from the model, we sample uniformly from the 5 MLPs that obtain This final noise prediction is then used to update the states using the diffusion update in Line 13 of Algorithm 2.</p>
<p>Noisy State Conditioning: Instead of conditioning the policy on the denoised state prediction ( τ a 0 in Line 10 of Algorithm 2) we condition the policy on the noisy states ( τ a i ).</p>
<p>C.5 Computational Requirements</p>
<p>For PolyGRAD, one run of RL training up to 1M environment steps (Algorithm 3) requires 54 hours of computation time on an RTX 3090 GPU.</p>
<p>Figure 1 :
1
Figure 1: Top: Illustration of Policy-Guided Trajectory Diffusion (PolyGRAD).PolyGRAD starts with a trajectory of random states and actions and diffuses it into an on-policy trajectory using a learnt denoising model, ϵ, and the policy, π.Bottom: Training a standard diffusion model on trajectories can be used to generate synthetic trajectories, but these are not on-policy.</p>
<p>Figure 2 :
2
Figure 2: Step-by-step illustration of PolyGRAD trajectory generation (Algorithm 2).Bottom left: Illustration of policy action distribution throughout state space.Top left: Trajectory is initialised with random states and actions (Line 2 and 3).The initial state (dark purple) is sampled from the dataset (Line 4).Solid black arrows: Action sequence is updated according to score of policy conditioned on current state sequence (Line 10).Hollow black arrows: State sequence is updated conditioned on current actions (Line 13).Bottom right: Final trajectory is returned.</p>
<p>PolyGRAD World Models (Algorithm 3) Algorithm 3 Imagined RL in PolyGRAD World Model 1: Require: environment, E; 2: Initialise: policy, π ϕ ; denoising model ϵ θ ; action update scale, δ; empty data buffer, D 3: while training :</p>
<p>Figure 3 :
3
Figure 3: Plots of action distributions produced by PolyGRAD.Blue line illustrates the distribution of a − µ ϕ (s) for a batch of synthetic data.Each subplot is for a policy with a different entropy level that is constant throughout the state space.Dashed black line indicates the action distribution output by the policy.Data is generated by running Algorithm 3 in Walker2d with h = 50.</p>
<p>Figure 5 :
5
Figure 5: Plots of mean squared error (MSE) of predicted states vs ground truth states for each world model trained on the same dataset for each environment.Shaded regions indicate standard deviation over 5 seeds.For PolyGRAD, we use the transformer denoising network trained on trajectories of length h = 10, 50, or 200.</p>
<p>Figure 5
5
Figure5shows the error for each world modelling method for varying trajectory lengths.We train PolyGRAD for trajectories of h = 10, 50, and 200.PolyGRAD with h = 10 obtains the best trajectory errors for Walker-2d.For HalfCheetah and Hopper, PolyGRAD with h = 10 obtains the second-best errors to Autoregressive Diffusion.When PolyGRAD is trained on longer trajectories (h = 50 and h = 200) we observe that larger errors are obtained, and PolyGRAD performs less well relative to the baselines.This indicates that it is more challenging for the denoising model to accurately predict the denoised states for longer trajectories.</p>
<p>Figure 4 :
4
Figure 4: Computation times to produce a batch of 1000 trajectories on a V100 GPU in Walker2d.The errors for the Probabilistic MLP Ensemble increase quickly as the prediction horizon increases.This may be because this method outputs a Gaussian distribution over the next state, and repeatedly sampling from this Gaussian can quickly lead to out-of-distribution states.The Transformer baseline obtains comparable performance to PolyGRAD with h = 200, but PolyGRAD outperforms the transformer when trained and evaluated on shorter trajectories.Note that PolyGRAD uses the same transformer network architecture for the denoising model as the Transformer baseline.Therefore, this result demonstrates that there is an advantage to iteratively refining the trajectory via diffusion rather than autoregressively predicting each successor state with a transformer.Interestingly, while Dreamer-v3 performs well on Walker2d, it obtains the poorest prediction errors at short horizons for HalfCheetah and Hopper despite being a state-of-the-art world model that produces strong RL agents.This may be because Dreamer-v3 performs dynamics predictions and RL training in latent space, and the reconstruction error to the original state space forms only one part of the training objective.Finally,</p>
<p>Figure 7 shows that PolyGRAD outperforms on-policy model-free algorithms.This is likely because PolyGRAD leverages all of the training data available to train the world model and generate large quantities of synthetic training data.On the other hand, on-policy model-free algorithms do not reuse training data.Confidence intervals in Appendix B.2 show that PolyGRAD significantly outperforms model-free algorithms in terms of mean performance in addition to interquartile mean.</p>
<p>Figure 6 :
6
Figure 6: Reward curves for RL training.Shaded regions indicate standard deviation over 5 seeds.</p>
<p>Figure 7 :
7
Figure 7: 95% CIs of interquartile mean of normalised final performance aggregated across envs.</p>
<p>Figure 8 :
8
Figure 8: Reward curves for PolyGRAD, and ablations and modifications to PolyGRAD.Shaded regions indicate standard deviation across 5 seeds.</p>
<p>Figure 9 :
9
Figure 9: 95% CIs of interquartile mean of normalised final performance aggregated across envs.</p>
<p>Figure 10 :
10
Figure 10: Plots of mean squared error (MSE) of predicted states vs ground truth states for PolyGRAD using either a transformer or residual MLP denoising network.Shaded regions indicate standard deviations over 5 seeds.</p>
<p>compares the base PolyGRAD algorithm against each ablation and variant of the algorithm.PolyGRAD significatly outperforms each variant/ablation except for Add State Update.</p>
<p>Figure 12 :
12
Figure 12: 95% confidence intervals of mean of normalised final performance aggregated across environments.</p>
<p>Figure 14 :
14
Figure 14: 95% confidence intervals of mean of normalised final performance aggregated across environments for PolyGRAD and each variant.</p>
<p>Figure 16: Hopper-v3 action distribution plots for trajectories generated by PolyGRAD for h = 10.</p>
<p>Figure 17: HalfCheetah-v3 action distribution plots for trajectories generated by PolyGRAD for h = 10.NaN values were produced before the minimum policy standard deviation of 0.02 was reached.</p>
<p>Figure 18: Walker-v3 action distribution plots when the action clipping has been ablated.Without the action clipping, PolyGRAD tends to produce a heavy-tailed action distribution with some actions very far from the mean.</p>
<p>Algorithm 2Policy-Guided Trajectory Diffusion (PolyGRAD)1: Require: policy, π ϕ ; denoising model, ϵ θ ; action update scale, δ; data buffer D
2: τ a N ∼ N (0, I)3: τ sr N ∼ N (0, I)4: s 0 ∼ D5: for i = N, N − 1, . . . , 1 :6: set initial state in τ sr ito s 0▷ Condition trajectory on initial state from dataset7:</p>
<p>Update actions using policy score</p>
<p>11: else :12:</p>
<p>Table 2 :
2
Residual MLP Denoising Network Hyperparameters
ParameterValueMLP Width1024 (h = 10) or 2048 (h &gt; 10)Batch size256Number of layers6Learning rate3e-4OptimiserAdamTraining steps per environment steps1</p>
<p>Table 3 :
3
Transformer Denoising Network Hyperparameters
ParameterValueEmbedding dimension312Batch size256Number of layers6Self-attention heads4Learning rate1e-4OptimiserAdamTraining steps per environment steps1</p>
<p>Table 4 :
4
Diffusion Hyperparameters
ParameterValueNumber of diffusion steps, N128Noise schedulecosineNoise schedule parameter τ noise 1 (0.1 for Hopper-v3 + residual MLP)
Published in Transactions on Machine Learning Research (02/2024)
A Implementation DetailsPolicy Parameterisation To make it easier to analyse the behaviour of PolyGRAD with policies of different entropy levels, we defined the policy so that the standard deviation of the policy is controlled by a single learnable parameter σ ϕ which is independent of the state.Therefore, the Gaussian policies that we consider are of the form π ϕ (a|s) = N (µ ϕ (s), σ ϕ ), where ϕ indicates the parameters of an MLP.This policy parameterisation is commonly used in other deep RL implementations(Kostrikov, 2018).Initial State ConditioningWe generated branched synthetic rollouts from initial states within the dataset.To achieve this, we sample the initial state uniformly at random from the dataset.To condition trajectory generation on the initial state, we perform inpainting by replacing the initial state in the trajectory with the sampled initial state during both training and inference.This is the same procedure used byJanner et al. (2022).Action Clipping During DiffusionDuring diffusion, we clip the action sequence so that it is within three standard deviations of the mean action output by the policy.Therefore, the update in Line 10 of Algorithm 2 is implemented as:Imagined RL Training For imagined RL training in Algorithm 3, we used Advantage Actor Critic (A2C)(Mnih et al., 2016)with Generalised Advantage Estimation (GAE)(Schulman et al., 2016).We performed one update to the policy and value function for every four steps of data collection in the real environment.During each A2C training update, we generated 1024 synthetic on-policy rollouts of length 10.We restricted the minimum standard deviation of the policy, σ ϕ , to be 0.1.We observed that RL training with PolyGRAD world models is unstable when there are large updates to the policy: if the policy is changed by a large amount, PolyGRAD may not consistently produce the correct action distribution.To address this, we defined the update magnitude as the average change in log-likelihood | log π new (a|s) − log π old (a|s)| across all state-action pairs in the training batch.For each update to the policy, we tuned the learning rate via a linesearch so that the average update magnitude was within 20% of the target update magnitude, target ∆ log π .The hyperparameters used for A2C are shown in Table1.Denoising ModelThe hyperparameters used for the denoising networks and diffusion process are summarised in Tables 2, 3 and 4. The noise schedule β i is defined using a cosine noise schedule(Nichol &amp; Dhariwal, 2021).We used the implementation of a cosine noise schedule defined in Algorithm 1 ofChen (2023).We used the default parameter of τ noise = 1 to control the noise schedule, with the exception that for Hopper-v3 with the residual MLP denoising network we used τ noise = 0.1 (which reduces the noise level throughout the early steps of the forward process) as we found this obtained better prediction errors.the lowest prediction loss on a held-out test set.We use the PyTorch implementation publicly available at github.com/Xingyu-Lin/mbpo_pytorch.Transformer We use the same transformer architecture and hyperparameters as for the PolyGRAD transformer denoising model described in Appendix A. Like the denoising model, the transformer is trained using the L2 loss.However, instead of the denoising objective, the transformer is trained to predict the next state given the context of a sequence of previous states and actions.We use a maximum context length of 15 state-action pairs.To generate trajectories, we query the transformer autoregressively.Autoregressive DiffusionWe use the same diffusion model as Synther(Lu et al., 2023), except that we add action conditioning by concatenating the action to the input.This diffusion model is used to make a one-step prediction of the next state, conditioned on the current action.To generate synthetic trajectories, we sample actions from the policy and autoregressively generate one step of the trajectory at a time by completing the reverse diffusion process individually for each step.Dreamer-v3We use the implementation of Dreamer-v3(Hafner et al., 2023)available at github.com/NM512/dreamerv3-torch.For RL training, we perform one step of RL training by directly performing backpropagation through the dynamics model for every four steps of data collection in the environment.All hyperparameters are set to the defaults used by Dreamer-v3 for MuJoCo.To evaluate the prediction errors, we first initialise the latent state by observing a sequence of five states and actions from the real environment.We then perform open-loop predictions by predicting the next latent states and decoding these using the decoder.Model-Free RL AlgorithmsWe use the implementations of PPO, TRPO, and A2C from Stable Baselines 3(Raffin et al., 2019).For PPO and TRPO we used the default hyperparameters.For A2C we used the default hyperparameters, with the exception that we set the learning rate to 3e − 4 and the entropy coefficient to 1e − 5 as we found this worked better for the MuJoCo benchmarks than the default parameters.C.4 Ablations and ModificationsRandom Actions: We sample random actions at the beginning of the diffusion process from a standard normal distribution, and they are not updated during diffusion.Policy Sampling: During each step of the diffusion process, we obtain new actions by directly sampling from the policy action distribution π ϕ (•| τ s 0 ).No Clipping: The actions are not clipped during the diffusion process.Add State Update:In addition to updating the actions according to the score of the policy distribution, we also update the states to increase the likelihood of the actions.For this modification, we first compute the noise prediction per Line 7 of Algorithm 2. We use this to compute the estimate of the denoised states, τ s 0 :We then update the estimate of the denoised states according to by using the score of the policy distribution with respect to the states:Computing this gradient requires performing backpropagation through the policy.By rearranging Equation12we then compute an updated noise prediction based on the modified denoised state prediction
Deep reinforcement learning at the edge of the statistical precipice. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G Bellemare, Advances in Neural Information Processing Systems. 2021</p>
<p>Is conditional generative modeling all you need for decision. Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, Pulkit Agrawal, International Conference on Learning Representations. 2023</p>
<p>Diffusion world models. Eloi Alonso, Adam Jelley, Anssi Kanervisto, Tim Pearce, Model-based offline planning. International Conference on Learning Representations. 2023 Openreview, Gabriel Arthur Argenson, Dulac-Arnold, 2021</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.015402016OpenAI gym. arXiv preprint</p>
<p>Transdreamer: Reinforcement learning with transformer world models. Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn, arXiv:2202.094812022arXiv preprint</p>
<p>On the importance of noise scheduling for diffusion models. Ting Chen, arXiv:2301.109722023arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, Shuran Song, Robotics: Science and Systems. 2023</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in Neural Information Processing Systems. 201831</p>
<p>PILCO: A model-based and data-efficient approach to policy search. Marc Deisenroth, Carl E Rasmussen, International Conference on Machine Learning. 2011</p>
<p>Diffusion models beat GANs on image synthesis. Prafulla Dhariwal, Alexander Nichol, Advances in Neural Information Processing Systems. 2021</p>
<p>Zihan Ding, Amy Zhang, Yuandong Tian, Qinqing Zheng, arXiv:2402.03570Diffusion world model. 2024arXiv preprint</p>
<p>Probabilistic recurrent state-space models. Andreas Doerr, Christian Daniel, Martin Schiegg, Nguyen-Tuong, Stefan Duy, Marc Schaal, Trimpe Toussaint, Sebastian, International Conference on Machine Learning. 2018</p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jürgen Schmidhuber, Advances in Neural Information Processing Systems. 2018</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. 2018</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, International Conference on Machine Learning. 2019</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, International Conference on Learning Representations. 2021</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning. Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, Sergey Levine ; Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, Xuelong Li, arXiv:2304.10573Advances in Neural Information Processing Systems. 2023. 2023arXiv preprintIdql: Implicit Q-learning as an actor-critic method with diffusion policies</p>
<p>Classifier-free diffusion guidance. Jonathan Ho, Tim Salimans, NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. 2021</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 2020</p>
<p>Policy-guided diffusion. Matthew Jackson, Michael Matthews, Cong Lu, Jakob Foerster, Shimon Whiteson, NeurIPS 2023 Workshop on Robot Learning: Pretraining, Fine-Tuning, and Generalization with Large Scale Models. 2023</p>
<p>When to trust your model: Model-based policy optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, Advances in neural information processing systems. 201932</p>
<p>Planning with diffusion for flexible behavior synthesis. Michael Janner, Yilun Du, Joshua B Tenenbaum, Sergey Levine, International Conference on Machine Learning. 2022</p>
<p>Model-based reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, International Conference on Learning Representations. 2020</p>
<p>P Diederik, Max Kingma, Welling, arXiv:1312.6114Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. 2013. 2018arXiv preprintAuto-encoding variational Bayes</p>
<p>Learning accurate long-term dynamics for model-based reinforcement learning. Nathan Lambert, Albert Wilcox, Howard Zhang, Kristofer Sj Pister, Roberto Calandra, IEEE Conference on Decision and Control. 2021</p>
<p>Investigating compounding prediction errors in learned dynamics models. Nathan Lambert, Kristofer Pister, Roberto Calandra, arXiv:2203.096372022arXiv preprint</p>
<p>Synthetic experience replay. Cong Lu, Philip J Ball, Jack Parker-Holder, Advances in Neural Information Processing Systems. 2023</p>
<p>Repaint: Inpainting using denoising diffusion probabilistic models. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, Luc Van Gool, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</p>
<p>Transformers are sample efficient world models. Vincent Micheli, Eloi Alonso, François Fleuret, International Conference on Learning Representations. 2023</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International Conference on Machine Learning. 2016</p>
<p>MCMC using Hamiltonian dynamics. Radford Neal, Handbook of Markov Chain Monte Carlo, chapter 5. Chapman and Hall / CRC Press2011</p>
<p>Improved denoising diffusion probabilistic models. Alexander Quinn, Nichol , Prafulla Dhariwal, International Conference on Machine Learning. 2021</p>
<p>Imitating human behaviour with diffusion models. Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, International Conference on Learning Representations. 2023</p>
<p>. Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Noah Dormann, Stable baselines. 32019</p>
<p>Reward-free curricula for training robust world models. Marc Rigter, Minqi Jiang, Ingmar Posner, arXiv:2306.092052023arXiv preprint</p>
<p>Transformer-based world models are happy with 100k interactions. Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling, International Conference on Learning Representations. 2023</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</p>
<p>Brownian dynamics as smart monte carlo simulation. Jimmie D Peter J Rossky, Harold L Doll, Friedman, The Journal of Chemical Physics. 69101978</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 5882020</p>
<p>Ingmar Schubert, Jingwei Zhang, Jake Bruce, Sarah Bechtle, Emilio Parisotto, Martin Riedmiller, Jost Tobias Springenberg, Arunkumar Byravan, arXiv:2305.10912Leonard Hasenclever, and Nicolas Heess. A generalist dynamics model for control. 2023arXiv preprint</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International Conference on Machine Learning. 2015</p>
<p>High-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, International Conference on Learning Representations. 2016</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Planning to explore via self-supervised world models. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak, International Conference on Machine Learning. 2020</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, International Conference on Machine Learning. 2015</p>
<p>Generative modeling by estimating gradients of the data distribution. Yang Song, Stefano Ermon, Advances in Neural Information Processing Systems. 2019</p>
<p>Score-based generative modeling through stochastic differential equations. Yang Song, Jascha Sohl-Dickstein, Abhishek Diederik P Kingma, Stefano Kumar, Ben Ermon, Poole, International Conference on Learning Representations. 2021</p>
<p>an integrated architecture for learning, planning, and reacting. Richard S Sutton, Dyna, ACM Sigart Bulletin. 241991</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Diffusion policies as an expressive policy class for offline reinforcement learning. Zhendong Wang, Jonathan J Hunt, Mingyuan Zhou, International Conference on Learning Representations. 2023</p>
<p>Pre-training contextualized world models with in-the-wild videos for reinforcement learning. Jialong Wu, Haoyu Ma, Chaoyi Deng, Mingsheng Long, Advances in Neural Information Processing Systems. 2023</p>
<p>Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel, arXiv:2310.06114Learning interactive real-world simulators. 2023arXiv preprint</p>
<p>Mastering atari games with limited data. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao, Advances in Neural Information Processing Systems. 2021</p>
<p>MOPO: Model-based offline policy optimization. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, Tengyu Ma, Advances in Neural Information Processing Systems. 2020</p>
<p>Learning unsupervised world models for autonomous driving via discrete diffusion. Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel Urtasun, arXiv:2311.010172023arXiv preprint</p>
<p>Diffusion models for reinforcement learning: A survey. Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, Weinan Zhang, arXiv:2311.012232023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
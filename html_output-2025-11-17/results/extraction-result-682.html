<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-682 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-682</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-682</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-41f5e1ad7793593befc0b9c38f756836e8b07c98</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/41f5e1ad7793593befc0b9c38f756836e8b07c98" target="_blank">Code4ML: a large-scale dataset of annotated Machine Learning code</a></p>
                <p><strong>Paper Venue:</strong> PeerJ Computer Science</p>
                <p><strong>Paper TL;DR:</strong> The Code4ML dataset can help address a number of software engineering or data science challenges through a data-driven approach and can be helpful for semantic code classification, code auto-completion, and code generation for an ML task specified in natural language.</p>
                <p><strong>Paper Abstract:</strong> The use of program code as a data source is increasingly expanding among data scientists. The purpose of the usage varies from the semantic classification of code to the automatic generation of programs. However, the machine learning model application is somewhat limited without annotating the code snippets. To address the lack of annotated datasets, we present the Code4ML corpus. It contains code snippets, task summaries, competitions, and dataset descriptions publicly available from Kaggle—the leading platform for hosting data science competitions. The corpus consists of ~2.5 million snippets of ML code collected from ~100 thousand Jupyter notebooks. A representative fraction of the snippets is annotated by human assessors through a user-friendly interface specially designed for that purpose. Code4ML dataset can help address a number of software engineering or data science challenges through a data-driven approach. For example, it can be helpful for semantic code classification, code auto-completion, and code generation for an ML task specified in natural language.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e682.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e682.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JuICe_mismatch_assumption</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assumed alignment between notebook markdown (natural language) and code in JuICe</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that prior work (JuICe) assumes code cells match their accompanying natural-language markdown descriptions; Code4ML contrasts this by using a fixed taxonomy to label code, highlighting potential misalignment between free-form NL descriptions and code semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Juice: A large scale distantly supervised dataset for open domain context-based code generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Jupyter Notebook ML pipelines (as used in JuICe and Code4ML)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sequential Jupyter notebook cells combining natural-language markdown descriptions and executable Python code forming ML pipelines for data-science tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>notebook markdown / task description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>notebook code cell (Python)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/unstated alignment between markdown and code (inconsistent assumption)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Related work (JuICe) assumes a one-to-one correspondence where the natural-language markdown describes exactly the code cell's functionality; Code4ML authors point out that free-form markdown is not uniquely defined across snippets and therefore may not reliably map to a single, consistent semantic class, motivating a taxonomy-based annotation instead.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>documentation-to-code mapping (description vs. code cell semantics)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>literature analysis / conceptual comparison between datasets (JuICe assumption vs. Code4ML taxonomy approach)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified in the paper (qualitative argument motivating use of a fixed taxonomy rather than relying on markdown-to-code alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Unreliable mapping from markdown to code can reduce the accuracy of NL-to-code training signals and downstream code generation models; paper uses this as motivation for creating a taxonomy and manual annotations rather than relying on markdown, but no quantitative impact on task metrics is reported for this specific mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>not reported (qualitative observation about potential inconsistency in notebook markdown across datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>free-form, non-standardized natural language descriptions in notebook markdown and lack of a universally consistent mapping to code semantics</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Define a standardized Machine Learning Taxonomy Tree and perform manual human annotation of code snippets to provide consistent semantic labels rather than relying on markdown.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not directly quantified for markdown-to-code alignment; the paper reports that the taxonomy + manual labels enabled training of baseline classifiers and semi-supervised expansion, but does not provide a direct comparison showing how much taxonomy-based annotation resolves markdown mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / data science (notebook-based ML pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Code4ML: a large-scale dataset of annotated Machine Learning code', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e682.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e682.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>too_long_flag_ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Annotator 'too_long' flag for ambiguous multi-purpose code cells</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Code4ML introduces a 'too_long' annotation flag to mark code cells that contain multiple semantic types or are ambiguous, so they cannot be unambiguously assigned a single taxonomy class by an assessor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code4ML manual annotation pipeline / web labeling tool</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A web form and annotation workflow where human assessors assign taxonomy semantic types, confidence scores, and flags (including 'too_long') to Jupyter notebook code cells.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>notebook markdown & human-readable markup rules</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>notebook code cell (Python) being labeled</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous cell content / multi-type cell (incomplete one-to-one mapping between NL description and code segment)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Individual code cells can contain code corresponding to multiple semantic types or multiple operations applied in a single line (e.g., concatenate + groupby), making it difficult to attribute the cell to one semantic type; annotators mark such cells with the 'too_long' flag and lower maximum confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / transformation / segmentation within notebook cells (cells combining multiple pipeline steps)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual human annotation via the web form where annotators set the 'too_long' flag when a cell cannot be unambiguously attributed</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>annotator-provided confidence scores (1-5) and the 'too_long' boolean flag; paper reports confidence distribution for labeled snippets (≈68% scored 5, ≈18% scored 4, ≈11% scored 3) but does not give an explicit proportion for 'too_long' flags.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Ambiguous cells reduce the purity of training labels and complicate automated classification; to handle ambiguity, the dataset marks such cells and limits maximum confidence, and downstream automatic labelling and classification need to account for impure cells.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>not directly reported (paper gives numbers of labelled snippets: ~10k labelled, ~8k unique, and confidence score distribution but not explicit 'too_long' prevalence)</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>practical authoring of notebooks where authors combine multiple operations or pipeline steps in a single code cell, and natural-language descriptions (if present) may not isolate single semantic actions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide explicit annotation rules (appendix C), include a 'too_long' flag and confidence score, instruct annotators to select the dominant semantic type when possible, and rely on semi-supervised automated labelling to expand coverage while preserving quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Indirectly supported: semi-supervised labeling (pseudo-labeling) substantially improved automated classification F1 (from ~0.68 for baseline SVMs to 0.872 when using 100% pseudo labels), indicating that with careful annotation and automated augmentation the dataset can be robustly labelled despite ambiguous cells; no direct metric isolating reduction of ambiguity is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset annotation for code understanding</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Code4ML: a large-scale dataset of annotated Machine Learning code', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e682.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e682.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL_spec_to_AutoML_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gap between natural-language problem specifications and generated ML pipeline/code in AutoML</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper highlights that transforming problem-specific natural language specifications (which may include domain hints) into concrete, correct Python code/pipelines remains largely unexplored and data-scarce, posing a gap for AutoML/code synthesis systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML / code-synthesis systems for pipeline creation from NL specs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated systems that would accept natural-language descriptions of ML tasks and generate complete pipelines or code implementations (feature engineering, model selection, training, evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>natural-language task specification / problem description (competition/problem statements)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated ML pipeline code (Python notebooks / scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing domain-specific transformations in generated code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language specifications often contain domain-specific hints or desired tricks that are not systematically or economically expressible in code; current AutoML and code-synthesis approaches lack sufficient domain-specific annotated code examples to reliably translate NL problem descriptions into full, correct pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>end-to-end pipeline generation (feature engineering, model choices, domain-specific preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>conceptual analysis and literature positioning: identification of lack of annotated ML code corpora and the observation that creating problem-specific solutions from NL specs is largely unexplored</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified in the paper (the paper motivates the dataset to address the lack of data but does not measure frequency or performance degradation attributable to NL-to-code gaps)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limits the ability to train generative models for AutoML and code synthesis; lack of domain-specific training data reduces the chance of producing domain-correct, high-performing pipelines from NL descriptions—no quantitative performance impact is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>qualitatively described as common / widespread (paper claims the area is 'largely unexplored' and cites a lack of relevant data for code synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>scarcity of large-scale, domain-specific paired NL-to-code datasets for ML pipelines and the complexity/variety of domain-specific preprocessing and modeling steps</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide a large-scale, annotated ML-code corpus (Code4ML) linking competition/problem natural descriptions with code snippets and taxonomy labels; supply a manual annotation tool and semi-supervised labelling baselines to expand labels.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial evidence: the Code4ML dataset (≈2.5M snippets, ~10k manually labelled) plus semi-supervised SVM pseudo-labeling improved classification F1 to 0.872 (100% pseudo labels), suggesting dataset-based approaches can substantially improve automated labelling; however, no direct evaluation of NL-to-code generation quality from these mitigations is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>automated machine learning (AutoML), code generation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Code4ML: a large-scale dataset of annotated Machine Learning code', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e682.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e682.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>coverage_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset scope coverage gap (Code4ML not covering full Kaggle code scope)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors note that the current release of Code4ML does not cover the full diversity or scope of Kaggle ML code snippets and can be extended in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code4ML dataset collection and annotation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A large corpus of Python ML code snippets, notebook metadata, and competition descriptions collected from Kaggle, with a subset manually annotated according to a taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Kaggle competition and dataset natural-language descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Kaggle Jupyter Notebook code cells (Python)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete dataset coverage (sampling bias / limited annotation coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although Code4ML includes ~2.5M code snippets and ~10k labelled snippets, the authors explicitly state it does not yet cover the entire space of Kaggle ML code; annotation coverage is limited (~10k labelled, ~8k unique) and the labeled sample may be moderately representative but not exhaustive.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset collection / annotation coverage (affects downstream training data availability)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>dataset inventory and reporting (authors' own assessment of scope and annotation counts)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>absolute counts reported (≈2.5M snippets total, ~107k notebooks collected, ~10k snippets manually labelled -> ~8k unique); representativeness assessed qualitatively via CDF comparison of classifier predictions (Figure 6) but not a full coverage metric.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limited annotation coverage necessitates semi-supervised methods for automatic labelling; insufficient labeled data can reduce performance of supervised models trained for semantic classification or NL-to-code tasks until labels are expanded. The paper demonstrates semi-supervised labelling improved classification F1 (up to 0.872) but does not quantify final downstream generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>dataset-level statement (limited: ~10k labeled vs ~2.5M total snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>scale of Kaggle content and resource constraints for manual annotation (human effort required), and selection criteria focusing on popular kernels and Python3 Apache-2.0 licensed notebooks</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide a public annotation web application to solicit annotator contributions, apply semi-supervised pseudo-labeling to expand labels automatically, and publish dataset and metadata for community extension.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Semi-supervised pseudo-labeling results (SVM with increasing pseudo-label proportions) show F1 improving from ~0.689 (best baseline SVM) to 0.872 with 100% pseudo labels, indicating automated expansion can greatly increase effective labelled coverage; community annotation is proposed but its effectiveness is not measured in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning datasets / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Code4ML: a large-scale dataset of annotated Machine Learning code', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Juice: A large scale distantly supervised dataset for open domain context-based code generation <em>(Rating: 2)</em></li>
                <li>KGTorrent: A dataset of python jupyter notebooks from kaggle <em>(Rating: 2)</em></li>
                <li>When code completion fails: A case study on real-world completions <em>(Rating: 1)</em></li>
                <li>Learning to mine aligned code and natural language pairs from stack overflow <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-682",
    "paper_id": "paper-41f5e1ad7793593befc0b9c38f756836e8b07c98",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "JuICe_mismatch_assumption",
            "name_full": "Assumed alignment between notebook markdown (natural language) and code in JuICe",
            "brief_description": "The paper notes that prior work (JuICe) assumes code cells match their accompanying natural-language markdown descriptions; Code4ML contrasts this by using a fixed taxonomy to label code, highlighting potential misalignment between free-form NL descriptions and code semantics.",
            "citation_title": "Juice: A large scale distantly supervised dataset for open domain context-based code generation",
            "mention_or_use": "mention",
            "system_name": "Jupyter Notebook ML pipelines (as used in JuICe and Code4ML)",
            "system_description": "Sequential Jupyter notebook cells combining natural-language markdown descriptions and executable Python code forming ML pipelines for data-science tasks.",
            "nl_description_type": "notebook markdown / task description",
            "code_implementation_type": "notebook code cell (Python)",
            "gap_type": "ambiguous/unstated alignment between markdown and code (inconsistent assumption)",
            "gap_description": "Related work (JuICe) assumes a one-to-one correspondence where the natural-language markdown describes exactly the code cell's functionality; Code4ML authors point out that free-form markdown is not uniquely defined across snippets and therefore may not reliably map to a single, consistent semantic class, motivating a taxonomy-based annotation instead.",
            "gap_location": "documentation-to-code mapping (description vs. code cell semantics)",
            "detection_method": "literature analysis / conceptual comparison between datasets (JuICe assumption vs. Code4ML taxonomy approach)",
            "measurement_method": "not quantified in the paper (qualitative argument motivating use of a fixed taxonomy rather than relying on markdown-to-code alignment)",
            "impact_on_results": "Unreliable mapping from markdown to code can reduce the accuracy of NL-to-code training signals and downstream code generation models; paper uses this as motivation for creating a taxonomy and manual annotations rather than relying on markdown, but no quantitative impact on task metrics is reported for this specific mismatch.",
            "frequency_or_prevalence": "not reported (qualitative observation about potential inconsistency in notebook markdown across datasets)",
            "root_cause": "free-form, non-standardized natural language descriptions in notebook markdown and lack of a universally consistent mapping to code semantics",
            "mitigation_approach": "Define a standardized Machine Learning Taxonomy Tree and perform manual human annotation of code snippets to provide consistent semantic labels rather than relying on markdown.",
            "mitigation_effectiveness": "Not directly quantified for markdown-to-code alignment; the paper reports that the taxonomy + manual labels enabled training of baseline classifiers and semi-supervised expansion, but does not provide a direct comparison showing how much taxonomy-based annotation resolves markdown mismatch.",
            "domain_or_field": "machine learning / data science (notebook-based ML pipelines)",
            "reproducibility_impact": false,
            "uuid": "e682.0",
            "source_info": {
                "paper_title": "Code4ML: a large-scale dataset of annotated Machine Learning code",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "too_long_flag_ambiguity",
            "name_full": "Annotator 'too_long' flag for ambiguous multi-purpose code cells",
            "brief_description": "Code4ML introduces a 'too_long' annotation flag to mark code cells that contain multiple semantic types or are ambiguous, so they cannot be unambiguously assigned a single taxonomy class by an assessor.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Code4ML manual annotation pipeline / web labeling tool",
            "system_description": "A web form and annotation workflow where human assessors assign taxonomy semantic types, confidence scores, and flags (including 'too_long') to Jupyter notebook code cells.",
            "nl_description_type": "notebook markdown & human-readable markup rules",
            "code_implementation_type": "notebook code cell (Python) being labeled",
            "gap_type": "ambiguous cell content / multi-type cell (incomplete one-to-one mapping between NL description and code segment)",
            "gap_description": "Individual code cells can contain code corresponding to multiple semantic types or multiple operations applied in a single line (e.g., concatenate + groupby), making it difficult to attribute the cell to one semantic type; annotators mark such cells with the 'too_long' flag and lower maximum confidence.",
            "gap_location": "data preprocessing / transformation / segmentation within notebook cells (cells combining multiple pipeline steps)",
            "detection_method": "manual human annotation via the web form where annotators set the 'too_long' flag when a cell cannot be unambiguously attributed",
            "measurement_method": "annotator-provided confidence scores (1-5) and the 'too_long' boolean flag; paper reports confidence distribution for labeled snippets (≈68% scored 5, ≈18% scored 4, ≈11% scored 3) but does not give an explicit proportion for 'too_long' flags.",
            "impact_on_results": "Ambiguous cells reduce the purity of training labels and complicate automated classification; to handle ambiguity, the dataset marks such cells and limits maximum confidence, and downstream automatic labelling and classification need to account for impure cells.",
            "frequency_or_prevalence": "not directly reported (paper gives numbers of labelled snippets: ~10k labelled, ~8k unique, and confidence score distribution but not explicit 'too_long' prevalence)",
            "root_cause": "practical authoring of notebooks where authors combine multiple operations or pipeline steps in a single code cell, and natural-language descriptions (if present) may not isolate single semantic actions",
            "mitigation_approach": "Provide explicit annotation rules (appendix C), include a 'too_long' flag and confidence score, instruct annotators to select the dominant semantic type when possible, and rely on semi-supervised automated labelling to expand coverage while preserving quality.",
            "mitigation_effectiveness": "Indirectly supported: semi-supervised labeling (pseudo-labeling) substantially improved automated classification F1 (from ~0.68 for baseline SVMs to 0.872 when using 100% pseudo labels), indicating that with careful annotation and automated augmentation the dataset can be robustly labelled despite ambiguous cells; no direct metric isolating reduction of ambiguity is provided.",
            "domain_or_field": "machine learning / dataset annotation for code understanding",
            "reproducibility_impact": false,
            "uuid": "e682.1",
            "source_info": {
                "paper_title": "Code4ML: a large-scale dataset of annotated Machine Learning code",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "NL_spec_to_AutoML_gap",
            "name_full": "Gap between natural-language problem specifications and generated ML pipeline/code in AutoML",
            "brief_description": "The paper highlights that transforming problem-specific natural language specifications (which may include domain hints) into concrete, correct Python code/pipelines remains largely unexplored and data-scarce, posing a gap for AutoML/code synthesis systems.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "AutoML / code-synthesis systems for pipeline creation from NL specs",
            "system_description": "Automated systems that would accept natural-language descriptions of ML tasks and generate complete pipelines or code implementations (feature engineering, model selection, training, evaluation).",
            "nl_description_type": "natural-language task specification / problem description (competition/problem statements)",
            "code_implementation_type": "generated ML pipeline code (Python notebooks / scripts)",
            "gap_type": "incomplete specification / missing domain-specific transformations in generated code",
            "gap_description": "Natural-language specifications often contain domain-specific hints or desired tricks that are not systematically or economically expressible in code; current AutoML and code-synthesis approaches lack sufficient domain-specific annotated code examples to reliably translate NL problem descriptions into full, correct pipelines.",
            "gap_location": "end-to-end pipeline generation (feature engineering, model choices, domain-specific preprocessing)",
            "detection_method": "conceptual analysis and literature positioning: identification of lack of annotated ML code corpora and the observation that creating problem-specific solutions from NL specs is largely unexplored",
            "measurement_method": "not quantified in the paper (the paper motivates the dataset to address the lack of data but does not measure frequency or performance degradation attributable to NL-to-code gaps)",
            "impact_on_results": "Limits the ability to train generative models for AutoML and code synthesis; lack of domain-specific training data reduces the chance of producing domain-correct, high-performing pipelines from NL descriptions—no quantitative performance impact is provided.",
            "frequency_or_prevalence": "qualitatively described as common / widespread (paper claims the area is 'largely unexplored' and cites a lack of relevant data for code synthesis)",
            "root_cause": "scarcity of large-scale, domain-specific paired NL-to-code datasets for ML pipelines and the complexity/variety of domain-specific preprocessing and modeling steps",
            "mitigation_approach": "Provide a large-scale, annotated ML-code corpus (Code4ML) linking competition/problem natural descriptions with code snippets and taxonomy labels; supply a manual annotation tool and semi-supervised labelling baselines to expand labels.",
            "mitigation_effectiveness": "Partial evidence: the Code4ML dataset (≈2.5M snippets, ~10k manually labelled) plus semi-supervised SVM pseudo-labeling improved classification F1 to 0.872 (100% pseudo labels), suggesting dataset-based approaches can substantially improve automated labelling; however, no direct evaluation of NL-to-code generation quality from these mitigations is reported.",
            "domain_or_field": "automated machine learning (AutoML), code generation",
            "reproducibility_impact": false,
            "uuid": "e682.2",
            "source_info": {
                "paper_title": "Code4ML: a large-scale dataset of annotated Machine Learning code",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "coverage_gap",
            "name_full": "Dataset scope coverage gap (Code4ML not covering full Kaggle code scope)",
            "brief_description": "The authors note that the current release of Code4ML does not cover the full diversity or scope of Kaggle ML code snippets and can be extended in future work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Code4ML dataset collection and annotation pipeline",
            "system_description": "A large corpus of Python ML code snippets, notebook metadata, and competition descriptions collected from Kaggle, with a subset manually annotated according to a taxonomy.",
            "nl_description_type": "Kaggle competition and dataset natural-language descriptions",
            "code_implementation_type": "Kaggle Jupyter Notebook code cells (Python)",
            "gap_type": "incomplete dataset coverage (sampling bias / limited annotation coverage)",
            "gap_description": "Although Code4ML includes ~2.5M code snippets and ~10k labelled snippets, the authors explicitly state it does not yet cover the entire space of Kaggle ML code; annotation coverage is limited (~10k labelled, ~8k unique) and the labeled sample may be moderately representative but not exhaustive.",
            "gap_location": "dataset collection / annotation coverage (affects downstream training data availability)",
            "detection_method": "dataset inventory and reporting (authors' own assessment of scope and annotation counts)",
            "measurement_method": "absolute counts reported (≈2.5M snippets total, ~107k notebooks collected, ~10k snippets manually labelled -&gt; ~8k unique); representativeness assessed qualitatively via CDF comparison of classifier predictions (Figure 6) but not a full coverage metric.",
            "impact_on_results": "Limited annotation coverage necessitates semi-supervised methods for automatic labelling; insufficient labeled data can reduce performance of supervised models trained for semantic classification or NL-to-code tasks until labels are expanded. The paper demonstrates semi-supervised labelling improved classification F1 (up to 0.872) but does not quantify final downstream generation performance.",
            "frequency_or_prevalence": "dataset-level statement (limited: ~10k labeled vs ~2.5M total snippets)",
            "root_cause": "scale of Kaggle content and resource constraints for manual annotation (human effort required), and selection criteria focusing on popular kernels and Python3 Apache-2.0 licensed notebooks",
            "mitigation_approach": "Provide a public annotation web application to solicit annotator contributions, apply semi-supervised pseudo-labeling to expand labels automatically, and publish dataset and metadata for community extension.",
            "mitigation_effectiveness": "Semi-supervised pseudo-labeling results (SVM with increasing pseudo-label proportions) show F1 improving from ~0.689 (best baseline SVM) to 0.872 with 100% pseudo labels, indicating automated expansion can greatly increase effective labelled coverage; community annotation is proposed but its effectiveness is not measured in the paper.",
            "domain_or_field": "machine learning datasets / dataset curation",
            "reproducibility_impact": false,
            "uuid": "e682.3",
            "source_info": {
                "paper_title": "Code4ML: a large-scale dataset of annotated Machine Learning code",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Juice: A large scale distantly supervised dataset for open domain context-based code generation",
            "rating": 2
        },
        {
            "paper_title": "KGTorrent: A dataset of python jupyter notebooks from kaggle",
            "rating": 2
        },
        {
            "paper_title": "When code completion fails: A case study on real-world completions",
            "rating": 1
        },
        {
            "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
            "rating": 1
        }
    ],
    "cost": 0.011118,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Code4ML: a Large-scale Dataset of annotated Machine Learning Code</h1>
<p>Anastasia Drozdova ${ }^{1}$, Polina Guseva ${ }^{1}$, Ekaterina Trofimova ${ }^{1}$, Anna Scherbakova ${ }^{1}$, and Andrey Ustyuzhanin ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, NRU Higher School of Economics, Moscow, Pokrovsky Bulvar, 11 ,<br>Corresponding author:<br>Ekaterina Trofimova ${ }^{1}$<br>Email address: etrofimova@hse.ru</p>
<h4>Abstract</h4>
<p>Program code as a data source is gaining popularity in the data science community. Possible applications for models trained on such assets range from classification for data dimensionality reduction to automatic code generation. However, without annotation number of methods that could be applied is somewhat limited. To address the lack of annotated datasets, we present the Code4ML corpus. It contains code snippets, task summaries, competitions and dataset descriptions publicly available from Kaggle - the leading platform for hosting data science competitions. The corpus consists of $\approx 2.5$ million snippets of ML code collected from $\approx 100$ thousand Jupyter notebooks. A representative fraction of the snippets is annotated by human assessors through a user-friendly interface specially designed for that purpose. Code4ML dataset can potentially help address a number of software engineering or data science challenges through a data-driven approach. For example, it can be helpful for semantic code classification, code auto-completion, and code generation for an ML task specified in natural language.</p>
<h2>INTRODUCTION</h2>
<p>In recent years, more and more tools for software development started using machine learning (ML) (Allamanis et al. (2018); Yang et al. (2021)). ML systems are capable of analyzing (Alsolai and Roper (2020); Bilgin et al. (2020)), manipulating (Goues et al. (2019); Liu et al. (2020)), and synthesizing (Svyatkovskiy et al. (2020); Austin et al. (2021)) code. However, even the most successful deep-learning models of the last few years (Roziere et al. (2020); Chen et al. (2021)) require training on vast amounts of data before they can obtain good results.</p>
<p>There is a multitude of code datasets (Iyer et al. (2018); Puri et al. (2021)). Still, most of them are not domain-specific, which poses a challenge during the development of tools for specialized areas of software engineering because of domain shift (Gretton et al. (2006)). Moreover, generic datasets can lack examples, making it hard for the model to pick up on domain-specific patterns.</p>
<p>ML is one of the most popular software development areas without a domain-specific code corpus. The lack of such a dataset hinders the development of data science tools. In this paper, we introduce a Large-scale Dataset of Machine Learning Code (Code4ML) dataset, a corpus of Python code snippets, competition summaries, and data descriptions from Kaggle.</p>
<p>Our key contributions are the following:</p>
<ul>
<li>We present a large dataset of about 2.5 million Python code snippets from public Kaggle notebooks. Those snippets are enriched with metadata.</li>
<li>We propose a novel technique for ML code annotation based on a Machine Learning Taxonomy Tree that reflects the main steps of the ML pipeline. In addition, we provide an annotation tool that can help continue further markup of the dataset.</li>
</ul>
<p>The rest of this paper is organized as follows. Section 2 contains an overview of prior art datasets. Section 3 includes a description of our dataset collection/annotation process. Details of human and</p>
<p>machine-oriented reading of the dataset are described in section 4 . Section 5 describes potential applications and research directions that the community can perform with the presented dataset. Section 6 concludes the paper.</p>
<h1>RELATED WORK</h1>
<p>Several publicly available datasets for source code have been proposed for various code intelligence tasks. Some datasets, like CodeNet (Puri et al. (2021)) and POLYCODER's dataset (Xu et al. (2022)), contain snippets from different programming languages. Others consist of code from one specific language: PY150 (Raychev et al. (2016)) for Python, CONCODE (Iyer et al. (2018)) for Java, Spider (Yu et al. (2018)) for SQL, etc. The source code is collected from GitHub (CodeSearchNet Husain et al. (2019)) and Stack Overflow (CoNaLa Yin et al. (2018)), and from other platforms as well, such as Kaggle (Quaranta et al. (2021)). In Lu et al. (2021) CodeXGLUE is proposed, a machine learning benchmark dataset that contains 14 datasets of different sizes and in multiple programming languages.</p>
<p>Table 1 gives an overview of several datasets for Python since our corpus is also for Python. As we aim to study ML code, we focus on ML-related datasets. In Agashe et al. (2019), the authors provide the set of manually created high-quality Jupyter notebooks representing the programming assignments for classes. The notebooks consist of alternating NL markdown and code cells. The code is assumed to match the provided markdown description. The motivation of the JuICe dataset lies in the generation of the code snippet by the natural description of the Jupyter notebook cell using the prior information from the notebook. Boa (Biswas et al. (2019)) dataset represents a pool of data-science-related python files and the meta-information about the corresponding GitHub projects. In Quaranta et al. (2021) the authors present a KGTorrent dataset. It includes a full snapshot of publicly available artifacts of Kaggle, including Jupyter notebooks, dataset descriptions, and forum discussions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset name</th>
<th style="text-align: left;">Dataset size</th>
<th style="text-align: left;">Human- <br> curated <br> annotated data <br> size</th>
<th style="text-align: left;">Data source</th>
<th style="text-align: left;">Natural de- <br> scription of <br> the general <br> task the code <br> is written for</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">JuICe Agashe et al. (2019)</td>
<td style="text-align: left;">$\approx 1.5 M$ code <br> snippets</td>
<td style="text-align: left;">$\approx 4 K$ code <br> snippets</td>
<td style="text-align: left;">GitHub</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Boa Biswas et al. (2019)</td>
<td style="text-align: left;">$\approx 5 M$ Python files</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">GitHub</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">KGTorrent Quaranta et al. <br> (2021)</td>
<td style="text-align: left;">$\approx 250 K$ Jupyter <br> notebook files</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">Kaggle</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Code4ML(ours)</td>
<td style="text-align: left;">$\approx 2.5 M$ code <br> snippets</td>
<td style="text-align: left;">$\approx 8 K$ unique <br> code snippets</td>
<td style="text-align: left;">Kaggle</td>
<td style="text-align: left;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1. Overview of some of the existing ML-related datasets for Python.</p>
<p>Our work focuses on the Kaggle kernels (Jupyter Notebooks) as the sequential computational code cells designed to solve machine learning problems. We aim to reduce the dimension of the learning space by introducing a taxonomy tree once it is used as an annotation mark to notebook code cells. This annotation can be compared with the markdown describing the task of the code cell in JuICe dataset (see Figures 1 and 2). However, unlike a markdown, our approach in the form of a taxonomy type is uniquely defined across all the snippets. We provide a set of $\approx 8 K$ human-curated annotated unique code snippets and a tool for the snippets' manual classification. Like KGTorrent, our corpus also contains information about Kaggle notebooks, corresponding datasets and competitions, and the competitions' natural descriptions. Thus, the whole ML pipeline, i.e., the sequence of the taxonomy tree vertices, is described by the human assessors.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. JuICE code snippets with the corresponding natural language description examples. Source: Agashe et al. (2019).</p>
<div class="codehilite"><pre><span></span><code><span class="n">Taxonomy</span> <span class="nb">type</span><span class="p">:</span> <span class="n">Data_extraction</span><span class="o">.</span><span class="n">load_from_disk</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s2">&quot;features.json&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s2">&quot;labels.json&quot;</span><span class="p">)</span>
<span class="n">NL</span><span class="p">:</span> <span class="n">Split</span> <span class="n">the</span> <span class="n">data</span> <span class="n">into</span> <span class="n">train</span> <span class="ow">and</span> <span class="n">test</span><span class="o">.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>

<p>NL: Create and train the model.
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Taxonomy type: Data_extraction.load_from_disk</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">X = pd.read_json("features.json")</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">y = pd.read_json("labels.json")</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Taxonomy type: Data_transform.prepare_x and_y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">X_train, X_test, y_train, y_test = train_test_split(X, y)</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Taxonomy type: Model_train.choose_model_class</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">dtree = DecisionTreeClassifier()</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Taxonomy type Model_train.train_model</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">dtree.fit(X_train, y_train)</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 2. Code4ML code snippets with the corresponding taxonomy types examples.</p>
<h1>CONSTRUCTION OF CODE4ML</h1>
<h2>Collection and preprocessing of the dataset</h2>
<p>Kaggle is the most prominent platform for competitive data science. It curates the creation of data challenges and encourages users to publish their solutions in Jupyter Notebook kernels. A kernel is a sequence of code snippets and description blocks in a natural language. The code from the kernels is a good source of ML code since the users have to build their machine learning pipelines from scratch.</p>
<p>Kaggle provides an API for accessing the published kernels and competitions and an open dataset containing various metadata. Using the API, we collect the most popular kernels from the most popular competitions (i.e. with the highest number of teams). We only consider kernels that use Python3 and have Apache 2.0 license.</p>
<p>The collected kernels are further processed by the parser for code blocks and corresponding kernel id extraction. Each code cell of the Jupyter notebook is considered a code snippet. We clean it up to ensure the collected code uniformity by removing broken Unicode characters and formatting the code to conform to the PEP8 standard. Also, personal information such as emails is not included in the snippets.</p>
<p>Notebooks on Kaggle have many useful metrics. Users vote for notebooks with high-quality code. Another important notebook metric is the kernel result on the test set (Kaggle score).</p>
<p>This metadata, as well as a number of kernel comments, are collected from Meta Kaggle. ${ }^{1}$</p>
<h2>Taxonomy tree</h2>
<p>Transformation of the Python code into conceptual pipelines describing the steps for performing ML experiments significantly reduces the amount of data required to train an ML model to analyse or generate the sequence. Almost any Jupyter Notebook can be translated into such a pipeline of repeating typical patterns.</p>
<p>To describe code blocks from an notebook, we have developed a set of categories and combined it in a Taxonomy Tree. The tree has two levels: the upper level denotes a high-level category of an ML pipeline step. Each descendent vertex corresponds to a more specific action. The second-level vertices are called semantic types. So, for example, semantic type mising values in Visualisation category represents an action of displaying missing values properties, such as quantities vs features. In contrast, correct missing values in Data Transform represents an action of filling it with a default value or removing the rows with missing values completely. There are 11 upper-level categories and $\approx 80$ lower-level classes. Figure 3 illustrates the graph. Appendix A shows examples of code snippets corresponding to different graph vertices.</p>
<p>Creating the ML Taxonomy Tree relies on data science standards such as CRISP-DM Shearer (2000) and ISOTR24029 ISO/IEC TR 24029-1:2021 (2021), the experts' experience in machine learning and data science.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3. Machine Learning Taxonomy Tree.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. Code4ML corpus structure. Each table is stored in a separate file with a unique key. It is highlighted on the figure and used to reference its entries outside.</p>
<h1>CODE4ML DATASET STRUCTURE</h1>
<p>The data is organized as a set of tables in CSV format. It includes several central entities: raw code blocks collected from Kaggle (code_blocks.csv), kernels (kernels_meta.csv) and competitions meta</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>information (competitions_meta.csv). Annotated code blocks are presented in a separate table markup_data.csv. Each code block is associated with a semantic type assigned to it by an external assessor. A Dictionary of semantic types is stored in the table vertices.csv.</p>
<p>Code snippets information (code_blocks.csv) can be mapped with kernels metadata via kernel_id. Kernels metadata is linked to Kaggle competitions information through comp_name (figure 4). To ensure the quality of the data kernels_meta.csv includes only Jupyter Notebooks with an non-empty Kaggle score. The data is published online at Zenodo platform (authors (2022)).</p>
<p>Each competition entry has the text description and metadata, reflecting competition, dataset characteristics, and evaluation metrics. EvaluationAlgorithmAbbreviation is collected from Meta Kaggle and provides additional informatoin on competitions and notebooks. EvaluationAlgorithmAbbreviation has 92 unique values, which make it difficult to filter the kernels by scores concerning the metric. To tackle it, we group EvaluationAlgorithmAbbreviation into 20 classes reflected in the metric_type column. Figure 5 shows the distribution of the metric_type. The description of each class is provided in appendix B.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Distribution of the competition's metric type.
The dataset for the corresponding competitions can be downloaded using Kaggle API: kaggle competitions download -c data_source, where data_source is the name of the dataset at Kaggle.</p>
<p>The code_blocks entry includes the code snippet, the corresponding kernel id and code block id, which is the index number of the snippet in the Jupyter Notebook.</p>
<p>The corpus contains 107524 notebooks. Almost a quarter of those (23 104) are assigned to competitions. Thus, 625125 snippets belonging to those notebooks have a kernel score value.</p>
<p>We use a web form for manual sorting of code snippets into semantic classes described in Section . ${ }^{2}$ The form allows marking code snippets according to their semantic type described in Section as well as cleanliness and the kind of data (i.e., table, image, etc.) To specify the markup confidence level in the resulting class, one should choose the corresponding value of marks (from 1 to 5). The too_long flag denotes the purity of the snippet to be marked up. The flag should be set if the cell code can not be unambiguously attributed to a single semantic type, i.e., it contains many different semantic types. One can find the detailed markup rules in the appendix C. markup_data.csv includes data labelled by the Code4ML project team. The interface of the web form is shown in the appendix D. All assessors must</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>follow the markup rules.
The markup table contains the following fields: the id of the parent notebook, code snippet text, the boolean too_long flag, the assessment confidence score in the range from 1 to 5 (best), and the id of the snippet class chosen by the assessor.</p>
<p>In total, assessors marked around 10000 snippets (some snippets are similar across notebooks, after that, there are $\approx 8000$ unique snippets). $\approx 68 \%$ of marked snippets got the highest confidence score (i.e., 5), while $\approx 18 \%$ and $\approx 11 \%$ got the confidence score equal to 4 and 3 , correspondingly.</p>
<p>In order to annotate the rest of the corpus, we provide the general assessment of the automatic code snippets labelling.</p>
<p>We use the manually labelled code snippets for training the basic models. The class distribution of the snippets can be found in the appendix E. We report two metrics: accuracy and F1-score.</p>
<p>Since the code block is a sequence of symbols, an encoding is required. We used frequency-inverse document frequency (Papineni (2001)) as a vectorizer.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. The valuation of the similarity of assessed and unassessed snippets. The plot lines show the cumulative distribution function (CDF) for the labelled (markup) and full (all snippets) samples depending on semantic class predicted probability.</p>
<p>We use a support vector machines (SVM) (Boser et al. (1992)) based models for snippets classification. This method does not require much data for training, so this approach is used as a reference ML method. We apply SVM with different kernels: linear, polynomial and Radial Basis Function (RBF). The hyperparameters are selected based on cross-validation metrics on ten folds. The multiclass case is handled using a one-vs-all scheme (Chang and Lin (2011)). Details of the model training are available in the Appendix F.</p>
<p>Figure 6 illustrates the level of similarity between the manually assessed sample and the whole data. This plot shows the cumulative distribution function for the labelled and the full samples. The horizontal axis shows the prediction of a calibrated SVM classifier with a linear kernel, trained on $80 \%$ of the labelled data. The probability ratio of the classes predicted by the model that does not exceed the specified threshold is then compared for the test part of the markup data (orange line) and the entire code_blocks.csv table (blue line). Although the data in the whole dataset is not identical to the labelled data, one can see the closeness of the two lines, which allows us to conclude that the labelled sample is moderately representative.</p>
<p>The semi-supervised models (Xie et al. (2020)) for the snippets classification are applied to deal with the lack of manually labelled data.</p>
<p>First, linear kernel-based SVM model is trained on the marked-up dataset. We collect the prediction of the trained model on the unlabelled part of the data. The predictions are further used as pseudo labels in</p>
<p>combination with marked-up data to train a different SVM model with the RBF kernel.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Metrics</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">F1-score</td>
<td style="text-align: left;">Accuracy</td>
</tr>
<tr>
<td style="text-align: left;">SVM + Linear</td>
<td style="text-align: left;">$0.684 \pm 0.024$</td>
<td style="text-align: left;">$0.691 \pm 0.022$</td>
</tr>
<tr>
<td style="text-align: left;">SVM + Poly</td>
<td style="text-align: left;">$0.619 \pm 0.021$</td>
<td style="text-align: left;">$0.625 \pm 0.019$</td>
</tr>
<tr>
<td style="text-align: left;">SVM + RBF</td>
<td style="text-align: left;">$0.689 \pm 0.022$</td>
<td style="text-align: left;">$0.625 \pm 0.019$</td>
</tr>
<tr>
<td style="text-align: left;">SVM with $20 \%$ of pseudo labels</td>
<td style="text-align: left;">$0.831 \pm 0.014$</td>
<td style="text-align: left;">$0.834 \pm 0.014$</td>
</tr>
<tr>
<td style="text-align: left;">SVM with $40 \%$ of pseudo labels</td>
<td style="text-align: left;">$0.845 \pm 0.016$</td>
<td style="text-align: left;">$0.851 \pm 0.014$</td>
</tr>
<tr>
<td style="text-align: left;">SVM with $100 \%$ of pseudo labels</td>
<td style="text-align: left;">$\mathbf{0 . 8 7 2} \pm 0.004$</td>
<td style="text-align: left;">$\mathbf{0 . 8 7 2} \pm 0.004$</td>
</tr>
</tbody>
</table>
<p>Table 2. 10-folds cross-validation performance of the baseline models for automatic data labeling.</p>
<h1>DOWNSTREAM TASKS</h1>
<p>The proposed corpus of the publicly available Kaggle code snippets, task summaries, competitions and dataset descriptions publicly enriched with annotation and useful metadata is a valuable asset for various data-driven scientific endeavors.</p>
<p>As shown above, Code4ML can be used for a semantic code classification task, where each ML code snippet should be labelled as one of the taxonomy tree classes illustrated by the Figure 3. This task helps summarize ML pipelines. For example, such a summary can serve as additional information or control input for the code generation model. One can use the proposed baseline models as a starting point for the semantic ML code classification.</p>
<p>Code4ML also covers a lack of annotated data for ML code auto-completion and synthesis problems. Code completion is the most popular software development technique (Murphy et al. (2006)) and hence is found in every major IDE. It can be used as a typing assistant tool for discovering relevant libraries and APIs.</p>
<p>Nevertheless, most existing code completion systems fail on uncommon completions despite their importance for real-world efficacy (Hellendoorn et al. (2019)). Training a code completion model on domain-specific data can help determine the too rare patterns in generic code datasets and improve real-world accuracy.</p>
<p>Manually creating well-performing models requires significant time and computational resources, so automated machine learning (AutoML) serves to reduce the costs (He et al. (2021)). The task is usually tricky, and the state-of-the-art works in the field are focused on technical feature engineering and selection (Tunguz (2020)). Creating problem-specific solutions from natural language specifications is still largely unexplored. For example, such specifications might contain domain-specific hints that the user would like to include in the pipeline/code. Still, the expression of such tricks in Python could be prohibitively expensive.</p>
<p>One of the problems in the AutoML field is the lack of relevant data for code synthesis. Training a generative model on a large-scale corpus of ML code like Code4ML can advance state-of-the-art automatic machine learning forward.</p>
<h2>CONCLUSION</h2>
<p>This paper describes a novel Large-scale Dataset of annotated Machine Learning Code (Code4ML) containing ML code snippets in Python and corresponding ML tasks metadata.</p>
<p>The dataset contains problem descriptions from $\approx 400$ Kaggle competitions in natural language. It also includes more than 20 thousand public Python 3 notebooks representing machine learning pipelines for solving those competitions with the provided Kaggle score. Those notebooks comprise around $\approx 600$ thousand code cells. We propose a taxonomy graph to describe the code snippets as principal parts of the ML pipeline.</p>
<p>The current version of the dataset does not cover the full scope of Kaggle ML code snippets and it can be easily extended in the future.</p>
<p>Around ten thousand snippets have been manually labelled to the date. We developed a data markup web application that can help volunteers contribute to the extension of the markup dataset and eventually cover it entirely. Consequently, we warmly welcome any efforts from the community in this direction.</p>
<p>We are confident that the Code4ML dataset can be helpful for various vital modern ML challenges, such as code classification, segmentation, generation, and auto-completion. Hopefully, it can also open up new venues for AutoML research.</p>
<h1>ACKNOWLEDGEMENT</h1>
<p>We want to acknowledge the considerable time and efforts spent annotating the Code4ML corpus by ([Names will be added upon review completion]).</p>
<h1>A CODE SNIPPETS</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Table 3. Examples of code snippets from Code4ML dataset</p>
<h1>B METRIC TYPE FEATURE CLASSES</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Class name</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Aim</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">accuracy</td>
<td style="text-align: left;">the share of the correct answers</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">r2_score</td>
<td style="text-align: left;">coefficient of determination</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">percentage_errors_multiclass</td>
<td style="text-align: left;">multiclass classification percentage error</td>
<td style="text-align: left;">minimisation</td>
</tr>
<tr>
<td style="text-align: left;">significance</td>
<td style="text-align: left;">custom metrics reflecting the predictions certainty</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">auc_multiclass</td>
<td style="text-align: left;">generalization of ROCAUC to a multiclass classifi- <br> cation</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">detection</td>
<td style="text-align: left;">object detection metrics</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">f_score</td>
<td style="text-align: left;">$F_{\beta}$-score metrics</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">points</td>
<td style="text-align: left;">reinforcement learning metrics</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">custom_loss</td>
<td style="text-align: left;">custom loss metrics</td>
<td style="text-align: left;">minimisation</td>
</tr>
<tr>
<td style="text-align: left;">segmentation</td>
<td style="text-align: left;">objects segmentation metrics</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">correlation</td>
<td style="text-align: left;">correlation metrics</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">log_loss</td>
<td style="text-align: left;">logarithmic loss</td>
<td style="text-align: left;">minimisation</td>
</tr>
<tr>
<td style="text-align: left;">reconstruction</td>
<td style="text-align: left;">reconstruction metrics</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">mae</td>
<td style="text-align: left;">mean average error metrics, e.g. WMAE</td>
<td style="text-align: left;">minimisation</td>
</tr>
<tr>
<td style="text-align: left;">multiclass_log_loss</td>
<td style="text-align: left;">logarithmic loss generalisation to multiclass classi- <br> fication</td>
<td style="text-align: left;">minimisation</td>
</tr>
<tr>
<td style="text-align: left;">persentage_errors</td>
<td style="text-align: left;">percentage error metrics, e.g. RMSLE, mape</td>
<td style="text-align: left;">minimisation</td>
</tr>
<tr>
<td style="text-align: left;">f_score_multiclass</td>
<td style="text-align: left;">generalisation of f_score to multiclass classification <br> problems</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">mse</td>
<td style="text-align: left;">mean squared error metrics, e.g. mse, RMSE</td>
<td style="text-align: left;">minimisation</td>
</tr>
<tr>
<td style="text-align: left;">auc</td>
<td style="text-align: left;">ROCAUC</td>
<td style="text-align: left;">maximisation</td>
</tr>
<tr>
<td style="text-align: left;">categorization_accuracy</td>
<td style="text-align: left;">generalisation of accuracy to multiclass classifica- <br> tion problems</td>
<td style="text-align: left;">maximisation</td>
</tr>
</tbody>
</table>
<p>Table 4. Description of the metric_type classes</p>
<h1>C MARKUP RULES</h1>
<p>While marking up the data using the web form, one should take into account the following suggestions:</p>
<ul>
<li>If code from only one semantic type is found in the snippet, mark 5 is to be set;</li>
<li>If the cell code can not be unambiguously interpreted, the too long flag should be set up;</li>
<li>If the too long flag flag is set, the maximum possible mark is equal to 4 (thus, the too long flag and confidence 5 can not be set at the same time);</li>
<li>If the snippet contains most of the code of one semantic type, but there is code of other types, then the type to which the most of the code belongs should be set with mark equal to 4;</li>
<li>If the snippet shares the same amount of code of different semantic types, the type that comes first with mark equal to 3 should be set;</li>
<li>If a sequence of operations for which semantic types are defined is applied to the data (e.g., sequence of concatenate and groupby methods) in one raw of code, then the semantic type of such a snippet will be the type of the last applied operation.</li>
</ul>
<h1>D INTERFACE OF THE WEB FORM</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. The interface of the web form. On the left there is an example of code snippet as well as the link to the original Kaggle kernel. On the right there are fields for manual labeling. Due to a large amount of options, the selection of semantic class is split into two parts.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. The dominated data type of the corresponding to the markup snippets competitions datasets is tabular. That leads to the imbalance in the semantic class distribution.</p>
<h1>F HYPERPARAMETERS FOR AUTOMATIC LABELLING</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SVM + Linear</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">37.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_df for TF-IDF</td>
<td style="text-align: center;">integer</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_df for TF-IDF</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">SVM + Poly</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">1.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Degree of poly kernel</td>
<td style="text-align: center;">integer</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_df for TF-IDF</td>
<td style="text-align: center;">integer</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_df for TF-IDF</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;">SVM + RBF</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">8.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_df for TF-IDF</td>
<td style="text-align: center;">integer</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_df for TF-IDF</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr>
<td style="text-align: center;">Pseudo labels 20\%</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">98.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kernel type</td>
<td style="text-align: center;">categorical</td>
<td style="text-align: center;">linear</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_df for TF-IDF</td>
<td style="text-align: center;">integer</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_df for TF-IDF</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;">Pseudo labels 40\%</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">121.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kernel type</td>
<td style="text-align: center;">categorical</td>
<td style="text-align: center;">linear</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_df for TF-IDF</td>
<td style="text-align: center;">integer</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_df for TF-IDF</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;">Pseudo labels 100\%</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">145.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kernel type</td>
<td style="text-align: center;">categorical</td>
<td style="text-align: center;">linear</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_df for TF-IDF</td>
<td style="text-align: center;">integer</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">max_df for TF-IDF</td>
<td style="text-align: center;">numeric</td>
<td style="text-align: center;">0.26</td>
</tr>
</tbody>
</table>
<p>Table 5. Classification models hyperparameters. The hyperparameters for SVM models are selected by cross-validation on ten folds using Optuna Akiba et al. (2019). The kernel can be Linear, Poly or RBF. The regularization parameter C is selected from $[0.1,1000]$.</p>
<h2>REFERENCES</h2>
<p>Agashe, R., Iyer, S., and Zettlemoyer, L. (2019). Juice: A large scale distantly supervised dataset for open domain context-based code generation. arXiv preprint arXiv:1910.02216.
Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. (2019). Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
Allamanis, M., Barr, E. T., Devanbu, P., and Sutton, C. (2018). A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1-37.
Alsolai, H. and Roper, M. (2020). A systematic literature review of machine learning techniques for software maintainability prediction. Information and Software Technology, 119:106214.
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. (2021). Program synthesis with large language models. arXiv preprint arXiv:2108.07732.
authors, A. (2022). Code4ML: a Large-scale Dataset of annotated Machine Learning Code.
Bilgin, Z., Ersoy, M. A., Soykan, E. U., Tomur, E., Çomak, P., and Karaçay, L. (2020). Vulnerability prediction from source code using machine learning. IEEE Access, 8:150672-150684.
Biswas, S., Islam, M. J., Huang, Y., and Rajan, H. (2019). Boa meets python: A boa dataset of data science software in python language. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR), pages 577-581. IEEE.
Boser, B. E., Guyon, I. M., and Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144-152.
Chang, C.-C. and Lin, C.-J. (2011). Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1-27.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.
Goues, C. L., Pradel, M., and Roychoudhury, A. (2019). Automated program repair. Communications of the ACM, 62(12):56-65.
Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B., and Smola, A. (2006). A kernel method for the two-sample-problem. Advances in neural information processing systems, 19:513-520.
He, X., Zhao, K., and Chu, X. (2021). Automl: A survey of the state-of-the-art. Knowledge-Based Systems, 212:106622.
Hellendoorn, V. J., Proksch, S., Gall, H. C., and Bacchelli, A. (2019). When code completion fails: A case study on real-world completions. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pages 960-970. IEEE.
Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. (2019). Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436.
ISO/IEC TR 24029-1:2021 (2021). Artificial Intelligence (AI) — Assessment of the robustness of neural networks - Part 1: Overview. Standard, International Organization for Standardization, Geneva, CH.
Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. (2018). Mapping language to code in programmatic context. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages $1643-1652$.
Liu, F., Li, G., Wei, B., Xia, X., Fu, Z., and Jin, Z. (2020). A self-attentional neural architecture for code completion with multi-task learning. In Proceedings of the 28th International Conference on Program Comprehension, pages 37-47.
Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., et al. (2021). Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664.
Murphy, G. C., Kersten, M., and Findlater, L. (2006). How are Java software developers using the Elipse IDE? IEEE software, 23(4):76-83.
Papineni, K. (2001). Why inverse document frequency? In Second Meeting of the North American Chapter of the Association for Computational Linguistics.
Puri, R., Kung, D. S., Janssen, G., Zhang, W., Domeniconi, G., Zolotov, V., Dolby, J., Chen, J., Choudhury, M., Decker, L., et al. (2021). Project codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655.
Quaranta, L., Calefato, F., and Lanubile, F. (2021). Kgtorrent: A dataset of python jupyter notebooks from kaggle. 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR).
Raychev, V., Bielik, P., and Vechev, M. (2016). Probabilistic model for code with decision trees. ACM SIGPLAN Notices, 51(10):731-747.
Roziere, B., Lachaux, M.-A., Chanussot, L., and Lample, G. (2020). Unsupervised translation of programming languages. In NeurIPS.
Shearer, C. (2000). The crisp-dm model: the new blueprint for data mining. Journal of data warehousing, 5(4):13-22.
Svyatkovskiy, A., Deng, S. K., Fu, S., and Sundaresan, N. (2020). Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 1433-1443.
Tunguz, B. (2020). Six levels of auto ml.
Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10687-10698.
Xu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. (2022). A systematic evaluation of large language models of code. arXiv preprint arXiv:2202.13169.
Yang, Y., Xia, X., Lo, D., and Grundy, J. (2021). A survey on deep learning for software engineering. ACM Comput. Surv.
Yin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig, G. (2018). Learning to mine aligned code and natural language pairs from stack overflow. In 2018 IEEE/ACM 15th international conference on mining software repositories (MSR), pages 476-486. IEEE.
Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., et al.</p>
<p>(2018). Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911-3921.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Additional labelled data is always welcome. You can participate at https://nl2ml-form.coresearch.club/. Please, take into attention that the registration of assessor needs to be approved by the Code4ML team members.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
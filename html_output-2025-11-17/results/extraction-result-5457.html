<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5457 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5457</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5457</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-3f0b274f6092b506b1ea3357842620a816bebbf4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3f0b274f6092b506b1ea3357842620a816bebbf4" target="_blank">Revisiting the Evaluation of Theory of Mind through Question Answering</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work revisits the evaluation of theory of mind through question answering and proposes an improved evaluation protocol and dataset in which it is explicitly control for data regularities via a careful examination of the answer space.</p>
                <p><strong>Paper Abstract:</strong> Theory of mind, i.e., the ability to reason about intents and beliefs of agents is an important task in artificial intelligence and central to resolving ambiguous references in natural language dialogue. In this work, we revisit the evaluation of theory of mind through question answering. We show that current evaluation methods are flawed and that existing benchmark tasks can be solved without theory of mind due to dataset biases. Based on prior work, we propose an improved evaluation protocol and dataset in which we explicitly control for data regularities via a careful examination of the answer space. We show that state-of-the-art methods which are successful on existing benchmarks fail to solve theory-of-mind tasks in our proposed approach.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5457.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5457.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end Memory Network (Memory Network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented neural network for question answering that stores and attends over a set of memory slots to answer questions about a provided story; applied here as a baseline QA model for theory-of-mind tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end memory networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Memory Network (MemNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural network with an explicit memory component that encodes story sentences into memory slots and uses attention-based read operations to answer questions; originally proposed for bAbI-style QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne style Theory-of-Mind QA (ToM-bAbi and ToMi)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of mind / social cognition (first-order and second-order belief reasoning), plus control memory and reality questions</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Synthetic, bAbI-style story-based tests adapted from the Sally-Anne false-belief paradigm: stories present agent actions, object locations and changes; questions probe Reality (where is the object), Memory (where was it originally), First-Order Belief (where will an agent look for the object), and Second-Order Belief (where does one agent think another will look). Two dataset variants: ToM-bAbi (original, templated) and ToMi (this paper's randomized, bias-controlled dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported performance on ToMi: average accuracy 77.2% and joint-story accuracy 44.3% (Table 1). Per-question-type on ToMi: Memory 98.90%, Reality 93.39%, First-Order 70.72%, Second-Order 64.66% (Table 2). When splitting ToMi by whether the pair involves false beliefs: non-false-belief pairs - First-Order 85.45%, Second-Order 82.67%; false-belief pairs - First-Order 12.62%, Second-Order 17.27% (Table 3). On original ToM-bAbi: ToM-easy 100.0%, ToM 90.7% average.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline is reported in this paper, so direct quantitative comparison to human (neurotypical) performance is not provided. Within-model comparisons show that MemNN performs well on non-false-belief questions but fails on false-belief question-story pairs (very low accuracy on FB pairs), and performs worse on the randomized ToMi than on the templated ToM-bAbi.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>MemNN can exploit dataset regularities in ToM-bAbi (explaining high performance there) but struggles on ToMi where those regularities are removed. Failures are concentrated on false-belief questions (especially first- and second-order false beliefs) and can be partly explained by memory capacity limits (longer stories cause the first mention to fall out of capacity) and inability to track agent mental states when distractors and randomization are introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Revisiting the Evaluation of Theory of Mind through Question Answering', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5457.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5457.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RelNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation Network (RelNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture designed for relational reasoning that computes pairwise interactions between objects/representations to support reasoning over relations; used here as a QA model on theory-of-mind story tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A simple neural network module for relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Relation Network (RelNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural module that aggregates pairwise (or higher-order) relations between encoded inputs to perform relational reasoning; applied as a QA model on bAbI-style theory-of-mind tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne style Theory-of-Mind QA (ToM-bAbi and ToMi)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of mind / social cognition (first-order and second-order belief reasoning), plus control memory and reality questions</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>See MemNN entry: story-based, synthetic QA probing reality, memory, first-order and second-order beliefs; ToM-bAbi (templated) and ToMi (randomized, bias-controlled) variants used.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported performance on ToMi: average accuracy 86.0% and joint-story accuracy 57.4% (Table 1). Per-question-type on ToMi (overall): First-Order ~96.42%, Second-Order ~95.37%, Reality 100.0%, Memory 99.90% (first block in Table 3 likely corresponds to non-false-belief pairs). On false-belief pairs (ToMi): First-Order ~10.40%, Second-Order ~17.81% (Table 3 second block). On original ToM-bAbi: ToM-easy 100.0%, ToM 94.4% average.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported in this paper; RelNet achieves high accuracy on many ToMi question-story pairs that do not involve false beliefs but performs very poorly on false-belief pairs, indicating a gap on tasks requiring reasoning about agents' differing mental states.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>RelNet's near-perfect performance on non-false-belief questions suggests strong relational/memory reasoning, but its very low accuracy on false-belief pairs indicates limited ability to represent or reason about agents' mental states in the randomized ToMi setting; performance on templated ToM-bAbi is inflated by dataset regularities that models can exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Revisiting the Evaluation of Theory of Mind through Question Answering', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5457.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5457.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Entity Network (EntNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural architecture that maintains evolving representations (memory cells) for entities to track state over a story, specifically designed to track world state over time for question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tracking the world state with recurrent entity networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Entity Network (EntNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recurrent architecture with dedicated memory slots (entities) that are updated as the story unfolds, enabling persistent entity state tracking for question answering and state reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne style Theory-of-Mind QA (ToM-bAbi and ToMi)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of mind / social cognition (first-order and second-order belief reasoning), plus control memory and reality questions</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>See MemNN entry: story-based, synthetic QA probing reality, memory, first-order and second-order beliefs; ToM-bAbi and ToMi datasets used to evaluate models' ability to track world state and agents' beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported performance on ToMi: average accuracy 90.6% and joint-story accuracy 66.8% (Table 1). Per-question-type on ToMi (overall): First-Order ~94.29%, Second-Order ~85.08%, Reality 100.0%, Memory 100.0% (first block in Table 3). On false-belief pairs (ToMi): First-Order ~54.95%, Second-Order ~36.55% (Table 3 second block). On original ToM-bAbi: ToM-easy 100.0%, ToM 94.9% average.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported in this paper. EntNet outperforms MemNN and RelNet on joint ToMi accuracy and shows the best performance among the evaluated models on false-belief pairs, but still fails substantially on many false-belief questions (particularly second-order false beliefs), so it does not match the expected human competence for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>EntNet's inductive bias for tracking world state gives it an advantage (higher joint accuracy and better false-belief performance than other models), but it still lacks robust modeling of agents' mental states: while it handles reality and memory questions perfectly in ToMi, it makes many mistakes on first- and second-order false-belief questions; synthetic dataset and simplified language mean results are only a prerequisite for real-world ToM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Revisiting the Evaluation of Theory of Mind through Question Answering', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How can memory-augmented neural networks pass a false-belief task? <em>(Rating: 2)</em></li>
                <li>Evaluating theory of mind in question answering <em>(Rating: 2)</em></li>
                <li>Machine theory of mind <em>(Rating: 2)</em></li>
                <li>End-to-end memory networks <em>(Rating: 2)</em></li>
                <li>Tracking the world state with recurrent entity networks <em>(Rating: 2)</em></li>
                <li>A simple neural network module for relational reasoning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5457",
    "paper_id": "paper-3f0b274f6092b506b1ea3357842620a816bebbf4",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "MemNN",
            "name_full": "End-to-end Memory Network (Memory Network)",
            "brief_description": "A memory-augmented neural network for question answering that stores and attends over a set of memory slots to answer questions about a provided story; applied here as a baseline QA model for theory-of-mind tasks.",
            "citation_title": "End-to-end memory networks",
            "mention_or_use": "use",
            "model_name": "Memory Network (MemNN)",
            "model_description": "A neural network with an explicit memory component that encodes story sentences into memory slots and uses attention-based read operations to answer questions; originally proposed for bAbI-style QA tasks.",
            "model_size": null,
            "cognitive_test_name": "Sally-Anne style Theory-of-Mind QA (ToM-bAbi and ToMi)",
            "cognitive_test_type": "Theory of mind / social cognition (first-order and second-order belief reasoning), plus control memory and reality questions",
            "cognitive_test_description": "Synthetic, bAbI-style story-based tests adapted from the Sally-Anne false-belief paradigm: stories present agent actions, object locations and changes; questions probe Reality (where is the object), Memory (where was it originally), First-Order Belief (where will an agent look for the object), and Second-Order Belief (where does one agent think another will look). Two dataset variants: ToM-bAbi (original, templated) and ToMi (this paper's randomized, bias-controlled dataset).",
            "llm_performance": "Reported performance on ToMi: average accuracy 77.2% and joint-story accuracy 44.3% (Table 1). Per-question-type on ToMi: Memory 98.90%, Reality 93.39%, First-Order 70.72%, Second-Order 64.66% (Table 2). When splitting ToMi by whether the pair involves false beliefs: non-false-belief pairs - First-Order 85.45%, Second-Order 82.67%; false-belief pairs - First-Order 12.62%, Second-Order 17.27% (Table 3). On original ToM-bAbi: ToM-easy 100.0%, ToM 90.7% average.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline is reported in this paper, so direct quantitative comparison to human (neurotypical) performance is not provided. Within-model comparisons show that MemNN performs well on non-false-belief questions but fails on false-belief question-story pairs (very low accuracy on FB pairs), and performs worse on the randomized ToMi than on the templated ToM-bAbi.",
            "notable_differences_or_limitations": "MemNN can exploit dataset regularities in ToM-bAbi (explaining high performance there) but struggles on ToMi where those regularities are removed. Failures are concentrated on false-belief questions (especially first- and second-order false beliefs) and can be partly explained by memory capacity limits (longer stories cause the first mention to fall out of capacity) and inability to track agent mental states when distractors and randomization are introduced.",
            "uuid": "e5457.0",
            "source_info": {
                "paper_title": "Revisiting the Evaluation of Theory of Mind through Question Answering",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "RelNet",
            "name_full": "Relation Network (RelNet)",
            "brief_description": "A neural architecture designed for relational reasoning that computes pairwise interactions between objects/representations to support reasoning over relations; used here as a QA model on theory-of-mind story tasks.",
            "citation_title": "A simple neural network module for relational reasoning",
            "mention_or_use": "use",
            "model_name": "Relation Network (RelNet)",
            "model_description": "A neural module that aggregates pairwise (or higher-order) relations between encoded inputs to perform relational reasoning; applied as a QA model on bAbI-style theory-of-mind tasks.",
            "model_size": null,
            "cognitive_test_name": "Sally-Anne style Theory-of-Mind QA (ToM-bAbi and ToMi)",
            "cognitive_test_type": "Theory of mind / social cognition (first-order and second-order belief reasoning), plus control memory and reality questions",
            "cognitive_test_description": "See MemNN entry: story-based, synthetic QA probing reality, memory, first-order and second-order beliefs; ToM-bAbi (templated) and ToMi (randomized, bias-controlled) variants used.",
            "llm_performance": "Reported performance on ToMi: average accuracy 86.0% and joint-story accuracy 57.4% (Table 1). Per-question-type on ToMi (overall): First-Order ~96.42%, Second-Order ~95.37%, Reality 100.0%, Memory 99.90% (first block in Table 3 likely corresponds to non-false-belief pairs). On false-belief pairs (ToMi): First-Order ~10.40%, Second-Order ~17.81% (Table 3 second block). On original ToM-bAbi: ToM-easy 100.0%, ToM 94.4% average.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported in this paper; RelNet achieves high accuracy on many ToMi question-story pairs that do not involve false beliefs but performs very poorly on false-belief pairs, indicating a gap on tasks requiring reasoning about agents' differing mental states.",
            "notable_differences_or_limitations": "RelNet's near-perfect performance on non-false-belief questions suggests strong relational/memory reasoning, but its very low accuracy on false-belief pairs indicates limited ability to represent or reason about agents' mental states in the randomized ToMi setting; performance on templated ToM-bAbi is inflated by dataset regularities that models can exploit.",
            "uuid": "e5457.1",
            "source_info": {
                "paper_title": "Revisiting the Evaluation of Theory of Mind through Question Answering",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "EntNet",
            "name_full": "Recurrent Entity Network (EntNet)",
            "brief_description": "A recurrent neural architecture that maintains evolving representations (memory cells) for entities to track state over a story, specifically designed to track world state over time for question answering.",
            "citation_title": "Tracking the world state with recurrent entity networks",
            "mention_or_use": "use",
            "model_name": "Recurrent Entity Network (EntNet)",
            "model_description": "A recurrent architecture with dedicated memory slots (entities) that are updated as the story unfolds, enabling persistent entity state tracking for question answering and state reasoning.",
            "model_size": null,
            "cognitive_test_name": "Sally-Anne style Theory-of-Mind QA (ToM-bAbi and ToMi)",
            "cognitive_test_type": "Theory of mind / social cognition (first-order and second-order belief reasoning), plus control memory and reality questions",
            "cognitive_test_description": "See MemNN entry: story-based, synthetic QA probing reality, memory, first-order and second-order beliefs; ToM-bAbi and ToMi datasets used to evaluate models' ability to track world state and agents' beliefs.",
            "llm_performance": "Reported performance on ToMi: average accuracy 90.6% and joint-story accuracy 66.8% (Table 1). Per-question-type on ToMi (overall): First-Order ~94.29%, Second-Order ~85.08%, Reality 100.0%, Memory 100.0% (first block in Table 3). On false-belief pairs (ToMi): First-Order ~54.95%, Second-Order ~36.55% (Table 3 second block). On original ToM-bAbi: ToM-easy 100.0%, ToM 94.9% average.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported in this paper. EntNet outperforms MemNN and RelNet on joint ToMi accuracy and shows the best performance among the evaluated models on false-belief pairs, but still fails substantially on many false-belief questions (particularly second-order false beliefs), so it does not match the expected human competence for these tasks.",
            "notable_differences_or_limitations": "EntNet's inductive bias for tracking world state gives it an advantage (higher joint accuracy and better false-belief performance than other models), but it still lacks robust modeling of agents' mental states: while it handles reality and memory questions perfectly in ToMi, it makes many mistakes on first- and second-order false-belief questions; synthetic dataset and simplified language mean results are only a prerequisite for real-world ToM capabilities.",
            "uuid": "e5457.2",
            "source_info": {
                "paper_title": "Revisiting the Evaluation of Theory of Mind through Question Answering",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How can memory-augmented neural networks pass a false-belief task?",
            "rating": 2
        },
        {
            "paper_title": "Evaluating theory of mind in question answering",
            "rating": 2
        },
        {
            "paper_title": "Machine theory of mind",
            "rating": 2
        },
        {
            "paper_title": "End-to-end memory networks",
            "rating": 2
        },
        {
            "paper_title": "Tracking the world state with recurrent entity networks",
            "rating": 2
        },
        {
            "paper_title": "A simple neural network module for relational reasoning",
            "rating": 2
        }
    ],
    "cost": 0.009504249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Revisiting the Evaluation of Theory of Mind through Question Answering</h1>
<p>Matthew Le<br>Y-Lan Boureau<br>Facebook AI Research<br>New York, NY<br>{mattle,ylan,maxn}@fb.com</p>
<p>Maximilian Nickel</p>
<h4>Abstract</h4>
<p>Theory of mind, i.e., the ability to reason about intents and beliefs of agents is an important task in artificial intelligence and central to resolving ambiguous references in natural language dialogue. In this work, we revisit the evaluation of theory of mind through question answering. We show that current evaluation methods are flawed and that existing benchmark tasks can be solved without theory of mind due to dataset biases. Based on prior work, we propose an improved evaluation protocol and dataset in which we explicitly control for data regularities via a careful examination of the answer space. We show that state-of-the-art methods which are successful on existing benchmarks fail to solve theory-of-mind tasks in our proposed approach.</p>
<h2>1 Introduction</h2>
<p>Humans interact and communicate with other people in a highly efficient way, as described for instance as Grice's cooperative principle (Grice et al., 1975). Inferring other people's mental state is a crucial component of cognitive development, playing a role in how humans learn the meaning of words (Bloom, 2002), distinguish beliefs from reality (Gopnik and Astington, 1988), predict people's behavior (Wimmer and Perner, 1983), understand what others refer to, and reduce ambiguity in conversation (Clark, 1981; Keysar et al., 2000). This ability to reason about the mental state of other agents, called theory of mind, is thus an important component of an intelligent system aiming to emulate and interact with humans.</p>
<p>In developmental psychology, classic tests such as the Sally-Anne test (Baron-Cohen et al., 1985) have been used to assess the ability to infer false beliefs in others. Recently, Grant et al. (2017) and Nematzadeh et al. (2018) proposed to adapt these tests to evaluate the capability of machine learning
models to form a theory of mind. The key insight of Grant et al. (2017) was to cast them as question answering tasks, where a system is given a story and has to answer questions about the beliefs of agents in that story. This allows to adapt the bAbi benchmarking protocol (Weston et al., 2016) to evaluate theory of mind capabilities of modern neural network architectures: stories are automatically generated so that a suitably large number of examples can be provided for training.</p>
<p>We believe this is a promising approach for advancing research on theory of mind, as it decouples its evaluation from problems such as multi-agent systems, game theory, and meta-learning, which are all components of other evaluation methods (Rabinowitz et al., 2018; Bard et al., 2019). However, artificial data runs the risk of displaying hidden artifacts and biases that correlate with the prediction task and can be exploited by models (Jabri et al., 2016; Gururangan et al., 2018; Poliak et al., 2018). The highly systematic nature of data generation puts synthetic benchmarks at an even higher risk of overestimating the target competence of interest.</p>
<p>This paper shows that current theory-of-mind QA benchmarks do indeed suffer from data biases, and are perfectly solvable without theory of mind. To overcome this, we propose an improved evaluation method and dataset ${ }^{1}$. We then show that state-of-the-art memory-augmented models - which are successful on existing benchmarks - fail to solve theory-of-mind tasks in our improved approach.</p>
<h2>2 Theory of Mind Benchmarks</h2>
<p>This section briefly reviews the Sally-Anne test and related paradigms for evaluating theory of mind.</p>
<p>First-Order Beliefs The so-called Sally-Anne test (Baron-Cohen et al., 1985) examines children's</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ability to reason about other agents' false beliefs. The child observes the actions of two agents (Sally, Anne). First, Sally puts an object into a container. Second, Anne moves the object without Sally observing this action. Third, the child is asked multiple questions about reality and the agents' beliefs:</p>
<p>First-Order Belief: Where will Sally look for the marble?</p>
<p>Reality: Where is the marble really?
Memory: Where was the marble in the beginning?
The first question tests the ability of the child to infer the correct mental state of Sally, i.e., that she has the false belief of the marble being in the basket. The reality and memory questions ensure that the child has a correct understanding of the state of the world and is not responding at chance.</p>
<p>Second-Order Beliefs "Sally-Anne"-type questions are well-suited to evaluate first-order beliefs. Nematzadeh et al. (2018) proposed to also evaluate second-order theory of mind, i.e., the ability to infer beliefs about beliefs. Perner and Wimmer (1985) proposed a set of experiments to test such second-order beliefs in children. The experiments can be summarized as follows: Two agents (Mary, John) see an icecream van in the park. The vendor tells them that he will be in the park all afternoon. After Mary leaves the park, the vendor decides to leave the park and tells John he is going to the church. On the way to the church, the vendor meets Mary and tells her also that he will be at the church. The child is then asked the following question:</p>
<h2>Second-Order Belief: Where does John think Mary will go to get ice-cream?</h2>
<p>As control, children are also asked memory, reality, and first-order belief questions.</p>
<p>Psychology Tests as AI Benchmarks Grant et al. (2017) cast the aforementioned experiments as question answering tasks and proposed to create a bAbi-style dataset (Weston et al., 2016) to evaluate theory of mind in artificial intelligence models. For instance, this is the bAbi question-answering task version of the Sally-Anne test:</p>
<p>Sally puts a marble in her basket Sally leaves the room Anne moves the marble in her box</p>
<p>Q: Where would Sally look for the marble?
A: Basket</p>
<p>Nematzadeh et al. (2018) evaluate several modern neural network architectures over the resulting dataset and report that all fail on theory-of-mind tasks, especially when irrelevant sentences are introduced into stories at test time. In the following, we refer to these benchmarks as ToM-bAbi.</p>
<h2>3 Evaluating Theory of Mind Evaluation</h2>
<p>This section examines shortcomings of ToM-bAbi for evaluating a model's theory of mind abilities and proposes an improved approach.</p>
<p>Related Work on Dataset Artifacts It is challenging to determine what specific competence is revealed by the successful completion of a task, as illustrated by the case of Clever Hans, the famous horse whose skill at reading human reactions passed for arithmetic ability (Pfungst, 1911). Recent analyses of several AI benchmarks have uncovered similar difficulties when probing learning models. In the domain of visual question answering, baselines solely relying on candidate answers have shown surprisingly good performance (Jabri et al., 2016), leading to the creation of a carefully designed diagnostic dataset (Johnson et al., 2017). Similarly, natural language inference benchmarks have been shown to be vulnerable to bias exploitation (Gururangan et al., 2018; Poliak et al., 2018), and multimodal machine translation benchmarks have been demonstrated to be too simple to require multi-modality (Caglayan et al., 2019). We follow the same approach here of examining the performance of a baseline that does not make use of a type of information crucial to the target competence. Taking "reasoning about another agent" as a working definition of theory of mind, a reasonable prerequisite of benchmarks probing theory of mind competence is that they be impossible to solve without some input about the other agent.</p>
<p>Leveraging Dataset Biases in ToM-bAbi Reviewing the generation process of ToM-bAbi uncovers predictable regularities that allow a model to use corner-cutting heuristics instead of keeping track of agents: the stories follow a strict event sequence template for each task type, shown in Fig. 1. The ToM-bAbi dataset takes precautions to guard against the most simplistic heuristic (e.g., 'always output the location of line 4') by adding irrelevant sentences at random places as noise, but many regularities and correlations remain. These regularities make it possible to construct a parsimonious set</p>
<p>Algorithm 1: Rules to solve ToM-bAbi
Data: question $q$, story $s$
Result: location $l$
if "beginning" $\in q$ then // Memory $1 \leftarrow$ location of first object occurrence;
else if "really" $\in q$ then // Reality $1 \leftarrow$ location of last object occurrence;
else if "look" $\in q$ then // 1st Order
if "exited" $\in s$ after last "object is in"
then
$1 \leftarrow$ location of last object occurrence before last "exited";
else
$1 \leftarrow$ location of last object occurrence;
end
else if "think" $\in q$ then // 2nd Order
if "exited" $\in s$ after last "object is in"
then
$1 \leftarrow$ location of last object occurrence before first "exited";
else
$1 \leftarrow$ location of last object occurrence;
end
end
of rules that perfectly solve all tasks without ever extracting any information about the agents. As shown in the pseudo-code implementation of Algorithm 1, these rules only involve simple lexical and ordinal patterns (code provided in the supplemental material).</p>
<p>Towards Robust ToM Evaluation in QA To increase robustness against such regularities and correlations, we build upon the ideas of ToM-bAbi but improve data generation and evaluation in multiple ways. We refer to the supplementary material for the full pseudo-code of our dataset generator.</p>
<p>First, to generate a balanced dataset over story types, ToM-bAbi uses different generators for truebelief, false-belief, and second-order false-belief stories. However, the different generators add clear biases to the data which can be exploited to identify the story type (for example, lack of the word "exited" signals a true belief story - see Fig. 1). To overcome this issue, we use the same randomized generation method for all stories and keep track of which type is produced. We then sample from this randomized story generator with rejection to create
a balanced dataset over all three types of stories.
Second, to decrease the amount of information that can be predicted from any given event, we add the following random distractors during data generation: actions of unrelated agents to decorrelate actions from answers, distractor statements about locations and objects to make the number of mentions less informative, randomization of the order of exit/move/re-entry actions, and randomization of the agent whose beliefs are being queried. This leads to stories that are a lot less predictable (see examples in Fig. 1).</p>
<p>Third, theory of mind manifests in "Sally-Anne" type tests through the understanding that an agent's belief is different from the actual state of the world, and that both coexist at the same moment. It is therefore crucial to evaluate both, the ability to infer the state of the world and the mental state of an agent. Although current benchmarks include reality and memory control questions, only one type of question is asked for each different, separately generated story. This can obscure revealing correlations in the models' responses e.g., that accuracy in first-order belief questions is associated with lower performance on reality questions. Since a correct answer to a false-belief question is only meaningful if the reality question is also answered correctly, it is especially important to check that the models can distinguish between states for each story. For this reason, we propose to systematically ask all question types for each generated story. In particular, for a single story involving agents $A, B$, and object $O$, we ask all following questions:</p>
<p>Reality: Where is $O$ ?
Memory: Where was O in the beginning?
First-Order Belief A: Where will A look for O?
First-Order Belief B: Where will B look for O?
Second-Order Belief A: Where does A believe B will look for $O$ ?</p>
<p>Second-Order Belief B: Where does B believe A will look for $O$ ?</p>
<p>Furthermore, we propose to count a story $s$ as answered correctly if all questions about $s$ are jointly correct, and to measure overall accuracy as the fraction of correctly answered stories. This ensures that a model is not acquiring theory of mind at the expense of its ability to perform other tasks and that it correctly answers reality and false-beliefs</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ToM-bAbi dataset</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Three types of stories</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$1\langle A\rangle$ entered $\langle L\rangle$ <br> $2\langle B\rangle$ entered $\langle L\rangle$ <br> 3 Phone rang. //Distractor can appear anywhere <br> 4 The $\langle O\rangle$ is in $\langle C 1\rangle$. <br> $5\langle B\rangle$ exited $\langle L\rangle$ <br> 6 〈A〉 moved the $\langle O\rangle$ to $\langle C 2\rangle$. <br> 7 〈A〉 exited the $\langle L\rangle$ <br> 8 〈B〉 entered the $\langle L\rangle$</td>
<td style="text-align: center;">// if story type 1 or 2 <br> // if story type 2 <br> // if story type 2</td>
</tr>
</tbody>
</table>
<p>Example story
1 Isla entered the bathroom.
2 Benjamin entered the bathroom.
3 The cabbage is in the green_pantry.
4 Phone rang.
5 Isla moved the cabbage to the red_drawer.
Answers for each story-question pair</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Story 1</th>
<th style="text-align: center;">Story 2</th>
<th style="text-align: center;">Story 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">First Order</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C2</td>
<td style="text-align: center;">C2</td>
</tr>
<tr>
<td style="text-align: left;">Second Order</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C2</td>
</tr>
<tr>
<td style="text-align: left;">Memory</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">C1</td>
</tr>
<tr>
<td style="text-align: left;">Reality</td>
<td style="text-align: center;">C2</td>
<td style="text-align: center;">C2</td>
<td style="text-align: center;">C2</td>
</tr>
</tbody>
</table>
<p>ToMi dataset
Examples of stories from the ToMi dataset
1 Oliver dislikes the kitchen
2 Carter entered the porch.
3 Abigail entered the porch.
4 The potato is in the green_suitcase.
5 Abigail exited the porch.
6 Abigail entered the hall.
7 Carter moved the potato to the green_envelope.
8 Oliver entered the hall.
1 Mila entered the closet.
2 Isla entered the closet.
3 Ava entered the closet.
4 The orange is in the blue_container.
5 Isla exited the closet.
6 Isla entered the garage.
7 Ava moved the orange to the green_bathtub.
1 William entered the staircase.
2 Aiden entered the staircase.
3 Aiden exited the staircase.
4 Aria entered the staircase.
5 The potato is in the red_drawer.
6 Aiden dislikes the grapefruit
7 William moved the potato to the blue_container.
8 Aria exited the staircase.</p>
<p>Figure 1: Left:Stories from the ToM-bAbi dataset follow three strict templates, with the possible random insertion of the distractor phrase "Phone rang." This makes it easier to devise rules to locate the answers for all pairs of question and story types, as shown in Alg. 1. For example, C1 always appears in the same sentence as the first object occurrence. Right: the ToMi dataset we propose is generated with considerably more randomness, with distractor phrases, distractor locations, distractor characters, and shuffling of the order of actions.
questions jointly. Formally, we define this joint accuracy as</p>
<p>$$
A c c_{J}(\mathcal{S})=\frac{1}{|\mathcal{S}|} \sum_{s \in \mathcal{S}} \prod_{q \in \mathcal{Q}<em q="q">{s}} \mathbb{1}\left(\widetilde{a}</em>\right)
$$}=a_{q</p>
<p>where $\mathbb{1}:{\perp, \top} \rightarrow{0,1}$ is the indicator function, $\mathcal{S}$ the set of all stories, $\mathcal{Q}<em q="q">{s}$ the set of all questions about story $s$, and $a</em>$ are the correct and the model's answer for question $q$ respectively.}, \widetilde{a}_{q</p>
<h2>4 Experimental Evaluation</h2>
<p>This section evaluates state-of-the-art question answering methods on the original ToM-bAbl and on our improved dataset. We follow Nematzadeh et al. (2018) and consider Memory Networks (MemNN; Sukhbaatar et al. 2015), Relation Networks (RelNet; Santoro et al. 2017, and Recurrent Entity Networks (EntNet; Henaff et al. 2017), which solve the original bAbI question answering tasks.</p>
<p>Table 1 shows the average accuracy over all questions for the original ToM-bAbI benchmarks (ToM-easy, ToM) and our improved dataset (ToMi).</p>
<p>Table 1: Accuracy on benchmark datasets. "Average" indicates the average accuracy over all questions. "Joint" indicates the joint accuracy as defined in Eq. (1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Joint</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ToM-easy</td>
<td style="text-align: center;">ToM</td>
<td style="text-align: center;">ToMi</td>
<td style="text-align: center;">ToMi</td>
</tr>
<tr>
<td style="text-align: left;">Rules</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">36.5</td>
</tr>
<tr>
<td style="text-align: left;">MemNN</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: left;">EntNet</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">66.8</td>
</tr>
<tr>
<td style="text-align: left;">RelNet</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">57.4</td>
</tr>
</tbody>
</table>
<p>Moreover, for ToMi we also show the joint accuracy as defined in Equation (1). Algorithm 1 achieves perfect accuracy on ToM-easy and ToM. Since all QA models have the capacity to capture these simple heuristics, it is not surprising that they also perform very well on the original benchmarks, i.e., perfect accuracy on ToM-easy and over $90 \%$ percent accuracy on ToM. ${ }^{2}$ Moreover, we found that the drop in accuracy in ToM is mostly caused</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Average accuracy by question type (MemNN)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ToM-easy</th>
<th style="text-align: center;">ToM</th>
<th style="text-align: center;">ToMi</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Memory</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">98.90</td>
</tr>
<tr>
<td style="text-align: left;">Reality</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">93.39</td>
</tr>
<tr>
<td style="text-align: left;">First-Order</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">70.72</td>
</tr>
<tr>
<td style="text-align: left;">Second-Order</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">64.66</td>
</tr>
</tbody>
</table>
<p>Table 3: Average accuracy per question type in ToMi. "FB" and "'w/o FB" indicate question-story pairs that do and do not involve false beliefs, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MemNN</th>
<th style="text-align: center;">RelNet</th>
<th style="text-align: center;">EntNet</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">First Order</td>
<td style="text-align: center;">85.45</td>
<td style="text-align: center;">96.42</td>
<td style="text-align: center;">94.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Second Order</td>
<td style="text-align: center;">82.67</td>
<td style="text-align: center;">95.37</td>
<td style="text-align: center;">85.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reality</td>
<td style="text-align: center;">93.39</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Memory</td>
<td style="text-align: center;">98.90</td>
<td style="text-align: center;">99.90</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">First Order</td>
<td style="text-align: center;">12.62</td>
<td style="text-align: center;">10.40</td>
<td style="text-align: center;">54.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Second Order</td>
<td style="text-align: center;">17.27</td>
<td style="text-align: center;">17.81</td>
<td style="text-align: center;">36.55</td>
</tr>
</tbody>
</table>
<p>by memory questions. This is because ToM stories are significantly longer and the location of the first occurrence of the queried object can exceed the memory capacity of the models. Table 2 shows an ablation for MemNNs that illustrates this effect.</p>
<p>By contrast, Algorithm $1^{3}$ and QA models fail to solve the ToMi tasks. This is especially clear when looking at the joint accuracy of Table 1, which is not inflated by easy-to-answer memory and reality questions. Our ablation in Table 3 provides further insights into these results. It lists the average accuracy per question type and further splits the results into false-belief and non-false-belief question-story pairs. All models do reasonably well on questionstory pairs that do not involve false beliefs, i.e., where the mental state of an agent should coincide with the state of the world. However, for questionstory pairs with false beliefs, all models fail to provide correct answers consistently. Recurrent Entity Networks, which explicitly aim to keep track of the state of the world, are performing best on false beliefs tasks, indicating that this is a useful inductive bias for QA models on theory-of-mind tasks.</p>
<h2>5 Discussion</h2>
<p>Theory of mind is an important component of intelligent systems which interact with humans. In the context of natural language, theory of mind is not</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>only of interest because of its evaluation through question answering but also because it is a crucial component to understand references and reduce ambiguities. This work re-examined the evaluation of theory of mind through bAbi-style question answering tasks. We revealed exploitable regularities in the generated data of existing benchmarks, and proposed to remedy this with a new dataset and evaluation method. We also showed that existing question answering methods that were capable of solving the previously proposed benchmarks are not able to solve the new tasks anymore. In future work, we aim at developing models to solve the newly proposed tasks.</p>
<p>Achieving this would demonstrate some level of ability to reason about first- and second-order false beliefs. But an important point to keep in mind when using our benchmark is that it still relies on synthetic data and tasks. We chose this approach since synthetic data generation can be especially useful in novel and early-stage research efforts, as it provides a controlled environment and allows for detailed analyses of a model's ability to solve a task. However, while methods such as Recurrent Entity Networks have shown promise for keeping track of the state-of-the-world in our experiments, this is still in scenarios where the complexity of natural language is relatively simple. On real-world data, this ability would be much weaker as it would require additional competencies such as co-reference resolution, handling polysemy and ambiguities, and common-sense reasoning. Therefore, solving the tasks we propose can only be considered a prerequisite for a fully functional theory of mind which will ultimately have to be evaluated in real-world scenarios.</p>
<h2>References</h2>
<p>Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. 2019. The hanabi challenge: A new frontier for ai research. arXiv preprint arXiv:1902.00506.</p>
<p>Simon Baron-Cohen, Alan M. Leslie, and Uta Frith. 1985. Does the autistic child have a "theory of mind"? Cognition, 21(1):37-46.</p>
<p>Paul Bloom. 2002. How children learn the meanings of words. MIT press.</p>
<p>Ozan Caglayan, Pranava Swaroop Madhyastha, Lucia Specia, and Loïc Barrault. 2019. Probing the need</p>
<p>for visual context in multimodal machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41594170.</p>
<p>Herbert H Clark. 1981. Definite reference and mutual knowledge. Elements of Discourse Understanding.</p>
<p>Alison Gopnik and Janet W. Astington. 1988. Children's understanding of representational change and its relation to the understanding of false belief and the appearance-reality distinction. Child Development, 59(1):26.</p>
<p>Erin Grant, Aida Nematzadeh, and Thomas L. Griffiths. 2017. How can memory-augmented neural networks pass a false-belief task? In Proceedings of the 39th Annual Meeting of the Cognitive Science Society, CogSci 2017, London, UK, 16-29 July 2017.</p>
<p>H Paul Grice, Peter Cole, Jerry L Morgan, et al. 1975. Logic and conversation. 1975, pages 41-58.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112.</p>
<p>Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2017. Tracking the world state with recurrent entity networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.</p>
<p>Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. 2016. Revisiting visual question answering baselines. In European conference on computer vision, pages 727-739. Springer.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 29012910.</p>
<p>Boaz Keysar, Dale J Barr, Jennifer A Balin, and Jason S Brauner. 2000. Taking perspective in conversation: The role of mutual knowledge in comprehension. Psychological Science, 11(1):32-38.</p>
<p>Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Thomas L. Griffiths. 2018. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2392-2400.</p>
<p>Josef Perner and Heinz Wimmer. 1985. "john thinks that mary thinks that..." attribution of second-order beliefs by 5- to 10-year-old children. Journal of Experimental Child Psychology, 39(3):437-471.</p>
<p>Oskar Pfungst. 1911. Clever Hans:(the horse of Mr. Von Osten.) a contribution to experimental animal and human psychology. Holt, Rinehart and Winston.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191.</p>
<p>Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. 2018. Machine theory of mind. In International Conference on Machine Learning, pages 42154224.</p>
<p>Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. 2017. A simple neural network module for relational reasoning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 4974-4983.</p>
<p>Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 712, 2015, Montreal, Quebec, Canada, pages 24402448.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2016. Towards ai-complete question answering: A set of prerequisite toy tasks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Cognition, 13(1):103-128.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3} \mathrm{~A}$ more elaborate set of rules could solve the tasks, but would require to take the agents into account.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1736 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1736</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1736</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-d6ee6d0530361aa10cee992133a3f57a64d67aa4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d6ee6d0530361aa10cee992133a3f57a64d67aa4" target="_blank">On-the-fly simplification of genetic programming models</a></p>
                <p><strong>Paper Venue:</strong> ACM Symposium on Applied Computing</p>
                <p><strong>Paper TL;DR:</strong> Two techniques for simplifying the generated models of genetic programming are proposed and it is shown that they are capable of finding solutions at par with those generated by the standard GP algorithm - but with significantly reduced program size.</p>
                <p><strong>Paper Abstract:</strong> The last decade has seen amazing performance improvements in deep learning. However, the black-box nature of this approach makes it difficult to provide explanations of the generated models. In some fields such as psychology and neuroscience, this limitation in explainability and interpretability is an important issue. Approaches such as genetic programming are well positioned to take the lead in these fields because of their inherent white box nature. Genetic programming, inspired by Darwinian theory of evolution, is a population-based search technique capable of exploring a high-dimensional search space intelligently and discovering multiple solutions. However, it is prone to generate very large solutions, a phenomenon often called "bloat". The bloated solutions are not easily understandable. In this paper, we propose two techniques for simplifying the generated models. Both techniques are tested by generating models for a well-known psychology experiment. The validity of these techniques is further tested by applying them to a symbolic regression problem. Several population dynamics are studied to make sure that these techniques are not compromising diversity - an important measure for finding better solutions. The results indicate that the two techniques can be both applied independently and simultaneously and that they are capable of finding solutions at par with those generated by the standard GP algorithm - but with significantly reduced program size. There was no loss in diversity nor reduction in overall fitness. In fact, in some experiments, the two techniques even improved fitness.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1736.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1736.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GWS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generationwide Simplification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lossless, population-level simplification procedure applied every k-th generation that generates all subtree variants from each individual, computes their fitness (cached by subtree hash), and forms a new population favouring fitter, smaller subtrees to reduce bloat during evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generationwide Simplification (GWS) within GP</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GWS is a simplification operator integrated into a standard tree-based Genetic Programming (GP) run. At every k-th generation (k=25 in experiments) instead of performing the usual breeding step, every individual is enumerated for possible child subtrees. Each subtree is hashed and its fitness is (re)computed once (cached by hash). A candidate pool of parent trees and subtree children is formed; low-fitness individuals are discarded if the pool exceeds population size. The mechanism preferentially replaces larger parent trees with smaller subtrees when these subtrees have equal or better fitness, thereby reducing program size (bloat) while preserving or improving fitness. Hashing avoids recomputation for identical subtrees across the population.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (tree-based GP individuals / subtrees)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Standard GP crossover is used in the underlying evolutionary process (Koza-style tree crossover). The GWS simplification step itself does not perform crossover; it enumerates and evaluates subtrees of existing individuals and may replace parents with fitter subtrees extracted from themselves or other cached subtrees.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Standard GP mutation is part of the overall GP run (used in routine reproduction when GWS is not active). The GWS procedure does not describe bespoke mutation; it focuses on subtree extraction and replacement. (The paper does not provide mutation operator specifics beyond using the Koza GP library.)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Fitness (error with respect to original data) and execution time (simulated operator time cost) â€” fitness is measured as error and lower is better; execution time of individuals is also measured/compared qualitatively. Exact formal metric formulas are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Qualitative: GWS reduced program size without incurring measurable loss in fitness or increase in execution time; no numeric values reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Population-level standard deviation of program size and population-level standard deviation of fitness (used as diversity proxies).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>Qualitative: No significant loss in diversity observed (standard deviation plots showed GWS retained diversity comparable to plain GP); no numeric values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Cognitive model discovery for Posner cueing task (behavioural data) and symbolic regression (fitting polynomial x^4 + x^3 + x^2 + x).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Plain GP (Koza-style GP implementation without simplification), and pruning-as-operator, and combinations of the two simplification schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying GWS produced substantially smaller GP individuals (reduced bloat) while retaining comparable fitness and execution time; population-level diversity (std dev of program size and fitness) was preserved. GWS was robust across both the Posner cognitive modelling task and the symbolic regression task, reducing program size without causing premature convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On-the-fly simplification of genetic programming models', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1736.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1736.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PruneOp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pruning as an Operator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An individual-level, lossless pruning operator applied each generation to the top fraction of individuals by fitness that enumerates subtrees and replaces the parent with the fittest subtree (if any) to reduce size while preserving fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pruning as an Operator (per-generation pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pruning operator applied after fitness evaluation and sorting every generation. It selects the top X% individuals (the pruning rate; 2% in experiments) and for each generates a set of subtrees, computes their fitness, and replaces the parent with the fittest subtree (lossless pruning). If no subtree is fitter than the parent, the parent remains unchanged. The operator is intended to shrink relatively fit individuals without interrupting normal genetic operations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (tree-based GP individuals / subtrees)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Underlying GP uses standard crossover (Koza-style). The pruning operator itself does not perform crossover; it replaces a parent with one of its subtrees selected by fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Underlying GP uses standard mutation as part of reproduction; the pruning operator is an additional operator applied after selection and fitness evaluation. Specific mutation mechanics are not detailed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Fitness (error) and execution time (simulated operator time cost) are used to assess performance; pruning is designed to be lossless in fitness but may incur computational overhead proportional to pruning rate.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Qualitative: In the Posner task pruning reduced sizes with no notable fitness loss; in symbolic regression pruning often caused many generations of similar individuals and did not reliably reduce size. No numeric executability figures were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Standard deviation of program size and standard deviation of fitness across the population.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>Pruning caused a measurable loss of diversity in some experiments (symbolic regression): the standard deviation growth rate was slower and populations became more genetically similar, often leading to premature convergence; numeric values not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Qualitative tradeoff described: pruning can reduce program size (and maintain fitness) but may reduce genetic diversity and novelty, particularly in symbolic regression, causing search to get stuck in local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Posner cueing cognitive-model discovery and symbolic regression (degree-4 polynomial).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Plain GP (no simplification) and generationwide simplification (GWS); experiments included combinations of pruning and GWS.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pruning as an operator can reduce sizes of very fit individuals with low computational overhead (dependent on pruning rate), but it risks reducing population diversity: in symbolic regression experiments pruning led to homogenised individuals and poorer size reduction overall, sometimes nullifying GWS benefits. Thus pruning may introduce a diversity penalty that harms exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On-the-fly simplification of genetic programming models', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1736.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1736.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plain GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Tree-based Genetic Programming (Koza-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard genetic programming using tree-structured programs with Koza-style reproduction (subtree crossover and mutation) used as the baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Plain / baseline GP (Koza GP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A canonical GP implementation (Koza library) using tree-structured individuals, fitness-driven selection, subtree crossover (exchange of subtrees between parents), mutation operators (generic GP mutation), and generational replacement. It serves as the control/baseline against which the proposed simplification techniques are evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (tree-based program trees composed of task-specific terminals and operators)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Subtree crossover (Koza-style): exchange a subtree from one parent with a subtree from another to produce offspring.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Standard GP mutation (not further specified in paper) as provided by the Koza GP library; mutation perturbs individuals according to library defaults.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Fitness (error) relative to target behavioural data or regression target; execution time also measured qualitatively for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Plain GP typically achieved slightly better minimum fitness in some experiments (Posner task) but differences were small; no numeric fitness differences reported.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Standard deviation of program size and standard deviation of fitness across population.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>Used as baseline: plain GP maintained diversity comparable to GWS in the Posner task; pruning sometimes reduced diversity relative to plain GP. No numeric values given.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Posner cueing task modelling and symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>N/A (this is the baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Plain GP serves as baseline; simplification schemes (GWS and pruning) produced smaller models generally without large fitness penalties compared to plain GP. In some cases plain GP had slightly better minimum fitness, but differences were minor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On-the-fly simplification of genetic programming models', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1736.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1736.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumericalSimpl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numerical Simplification (semantic simplification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simplification methods operating on semantics: they evaluate the contribution of nodes/subtrees by their effect on fitness (or outputs) and prune/replace nodes based on empirical criteria (including permutation tests or regression-point evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Numerical (semantic) simplification methods</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A family of simplification techniques that operate at the semantic (behavioural) level by estimating the contribution of nodes/subtrees to parent fitness or by comparing subtree outputs against reference regression points; some variants replace subtrees with terminals or simpler subtrees when semantics are approximately preserved. Methods can use permutation tests to accept/reject pruning proposals to control error.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (GP trees) evaluated semantically (their outputs on test points)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Semantic fidelity measured via outputs on regression/evaluation points, and statistical tests (e.g. permutation tests) to decide pruning; specific metrics vary by method.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression and other GP tasks where semantics can be sampled and compared.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Algebraic simplification, subtree substitution methods, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites numerical simplification approaches as an alternative to algebraic simplification, noting they work semantically and can replace nodes/subtrees based on contribution; they can be computationally costly and require careful thresholding, but can better detect semantically equivalent but syntactically different code.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On-the-fly simplification of genetic programming models', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1736.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1736.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlgebraicSimpl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Algebraic Simplification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simplification based on algebraic equivalences that replaces code fragments with smaller algebraic equivalents while guaranteeing no change in computed output (requires domain-specific equivalence rules).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Algebraic simplification methods</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Simplification approach that uses algebraic identities and expert-defined equivalence rules to replace subexpressions with smaller equivalent code fragments; commonly applied in symbolic regression and mathematical problems where equivalences are known.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (mathematical expressions / GP trees)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Guarantees semantic equivalence via algebraic rules, so executability/functionality preserved by construction; no numerical metrics provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression (mathematical domains) where algebraic equivalences are known.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Numerical simplification, parsimony pressure methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors note algebraic simplification preserves fitness by relying on equivalence rules but requires domain expertise and cannot detect semantic equivalence of differently-looking code unless encoded as equivalences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On-the-fly simplification of genetic programming models', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1736.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1736.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EpiSimpl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epigenetics-inspired Simplification (gene silencing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of simplification techniques inspired by epigenetics where genes are silenced (rather than removed) to allow possible reactivation by future mutation, implemented in PushGP in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Epigenetics-inspired simplification methods</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instead of permanently removing genetic material (introns), these methods silence genes so that they do not contribute to current behaviour but remain available for reactivation through future mutations; implemented in some GP frameworks (e.g., PushGP) to control bloat while preserving evolvability.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (GP genomes / trees, e.g., PushGP programs)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>GP bloat control research (general GP domains); cited implementation used PushGP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard pruning/removal simplification techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites epigenetics-inspired methods as alternatives that silence genes (retain chance of unsilencing via mutation), potentially preserving evolvability; no experimental details given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On-the-fly simplification of genetic programming models', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic Programming 1996: Proceedings of the First Annual Conference, July 28-31, 1996, Stanford University <em>(Rating: 2)</em></li>
                <li>Using Numerical Simplification to Control Bloat in Genetic Programming <em>(Rating: 2)</em></li>
                <li>Reducing code bloat in Genetic Programming based on subtree substituting technique <em>(Rating: 2)</em></li>
                <li>Semantics Based Substituting Technique for Reducing Code Bloat in Genetic Programming <em>(Rating: 2)</em></li>
                <li>It is Time for New Perspectives on How to Fight Bloat in GP <em>(Rating: 2)</em></li>
                <li>Algebraic simplification of GP programs during evolution <em>(Rating: 2)</em></li>
                <li>Effects of program simplification on simple building blocks in Genetic Programming <em>(Rating: 2)</em></li>
                <li>Pruning of genetic programming trees using permutation tests <em>(Rating: 2)</em></li>
                <li>Time Control or Size Control? Reducing Complexity and Improving Accuracy of Genetic Programming Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1736",
    "paper_id": "paper-d6ee6d0530361aa10cee992133a3f57a64d67aa4",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "GWS",
            "name_full": "Generationwide Simplification",
            "brief_description": "A lossless, population-level simplification procedure applied every k-th generation that generates all subtree variants from each individual, computes their fitness (cached by subtree hash), and forms a new population favouring fitter, smaller subtrees to reduce bloat during evolution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Generationwide Simplification (GWS) within GP",
            "system_description": "GWS is a simplification operator integrated into a standard tree-based Genetic Programming (GP) run. At every k-th generation (k=25 in experiments) instead of performing the usual breeding step, every individual is enumerated for possible child subtrees. Each subtree is hashed and its fitness is (re)computed once (cached by hash). A candidate pool of parent trees and subtree children is formed; low-fitness individuals are discarded if the pool exceeds population size. The mechanism preferentially replaces larger parent trees with smaller subtrees when these subtrees have equal or better fitness, thereby reducing program size (bloat) while preserving or improving fitness. Hashing avoids recomputation for identical subtrees across the population.",
            "input_type": "programs (tree-based GP individuals / subtrees)",
            "crossover_operation": "Standard GP crossover is used in the underlying evolutionary process (Koza-style tree crossover). The GWS simplification step itself does not perform crossover; it enumerates and evaluates subtrees of existing individuals and may replace parents with fitter subtrees extracted from themselves or other cached subtrees.",
            "mutation_operation": "Standard GP mutation is part of the overall GP run (used in routine reproduction when GWS is not active). The GWS procedure does not describe bespoke mutation; it focuses on subtree extraction and replacement. (The paper does not provide mutation operator specifics beyond using the Koza GP library.)",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Fitness (error with respect to original data) and execution time (simulated operator time cost) â€” fitness is measured as error and lower is better; execution time of individuals is also measured/compared qualitatively. Exact formal metric formulas are not provided.",
            "executability_results": "Qualitative: GWS reduced program size without incurring measurable loss in fitness or increase in execution time; no numeric values reported in paper.",
            "diversity_metric": "Population-level standard deviation of program size and population-level standard deviation of fitness (used as diversity proxies).",
            "diversity_results": "Qualitative: No significant loss in diversity observed (standard deviation plots showed GWS retained diversity comparable to plain GP); no numeric values reported.",
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Cognitive model discovery for Posner cueing task (behavioural data) and symbolic regression (fitting polynomial x^4 + x^3 + x^2 + x).",
            "comparison_baseline": "Plain GP (Koza-style GP implementation without simplification), and pruning-as-operator, and combinations of the two simplification schemes.",
            "key_findings": "Applying GWS produced substantially smaller GP individuals (reduced bloat) while retaining comparable fitness and execution time; population-level diversity (std dev of program size and fitness) was preserved. GWS was robust across both the Posner cognitive modelling task and the symbolic regression task, reducing program size without causing premature convergence.",
            "uuid": "e1736.0",
            "source_info": {
                "paper_title": "On-the-fly simplification of genetic programming models",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "PruneOp",
            "name_full": "Pruning as an Operator",
            "brief_description": "An individual-level, lossless pruning operator applied each generation to the top fraction of individuals by fitness that enumerates subtrees and replaces the parent with the fittest subtree (if any) to reduce size while preserving fitness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Pruning as an Operator (per-generation pruning)",
            "system_description": "A pruning operator applied after fitness evaluation and sorting every generation. It selects the top X% individuals (the pruning rate; 2% in experiments) and for each generates a set of subtrees, computes their fitness, and replaces the parent with the fittest subtree (lossless pruning). If no subtree is fitter than the parent, the parent remains unchanged. The operator is intended to shrink relatively fit individuals without interrupting normal genetic operations.",
            "input_type": "programs (tree-based GP individuals / subtrees)",
            "crossover_operation": "Underlying GP uses standard crossover (Koza-style). The pruning operator itself does not perform crossover; it replaces a parent with one of its subtrees selected by fitness.",
            "mutation_operation": "Underlying GP uses standard mutation as part of reproduction; the pruning operator is an additional operator applied after selection and fitness evaluation. Specific mutation mechanics are not detailed in the paper.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Fitness (error) and execution time (simulated operator time cost) are used to assess performance; pruning is designed to be lossless in fitness but may incur computational overhead proportional to pruning rate.",
            "executability_results": "Qualitative: In the Posner task pruning reduced sizes with no notable fitness loss; in symbolic regression pruning often caused many generations of similar individuals and did not reliably reduce size. No numeric executability figures were reported.",
            "diversity_metric": "Standard deviation of program size and standard deviation of fitness across the population.",
            "diversity_results": "Pruning caused a measurable loss of diversity in some experiments (symbolic regression): the standard deviation growth rate was slower and populations became more genetically similar, often leading to premature convergence; numeric values not provided.",
            "novelty_executability_tradeoff": "Qualitative tradeoff described: pruning can reduce program size (and maintain fitness) but may reduce genetic diversity and novelty, particularly in symbolic regression, causing search to get stuck in local optima.",
            "frontier_characterization": null,
            "benchmark_or_domain": "Posner cueing cognitive-model discovery and symbolic regression (degree-4 polynomial).",
            "comparison_baseline": "Plain GP (no simplification) and generationwide simplification (GWS); experiments included combinations of pruning and GWS.",
            "key_findings": "Pruning as an operator can reduce sizes of very fit individuals with low computational overhead (dependent on pruning rate), but it risks reducing population diversity: in symbolic regression experiments pruning led to homogenised individuals and poorer size reduction overall, sometimes nullifying GWS benefits. Thus pruning may introduce a diversity penalty that harms exploration.",
            "uuid": "e1736.1",
            "source_info": {
                "paper_title": "On-the-fly simplification of genetic programming models",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Plain GP",
            "name_full": "Standard Tree-based Genetic Programming (Koza-style)",
            "brief_description": "Standard genetic programming using tree-structured programs with Koza-style reproduction (subtree crossover and mutation) used as the baseline for comparison.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Plain / baseline GP (Koza GP)",
            "system_description": "A canonical GP implementation (Koza library) using tree-structured individuals, fitness-driven selection, subtree crossover (exchange of subtrees between parents), mutation operators (generic GP mutation), and generational replacement. It serves as the control/baseline against which the proposed simplification techniques are evaluated.",
            "input_type": "programs (tree-based program trees composed of task-specific terminals and operators)",
            "crossover_operation": "Subtree crossover (Koza-style): exchange a subtree from one parent with a subtree from another to produce offspring.",
            "mutation_operation": "Standard GP mutation (not further specified in paper) as provided by the Koza GP library; mutation perturbs individuals according to library defaults.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Fitness (error) relative to target behavioural data or regression target; execution time also measured qualitatively for comparisons.",
            "executability_results": "Plain GP typically achieved slightly better minimum fitness in some experiments (Posner task) but differences were small; no numeric fitness differences reported.",
            "diversity_metric": "Standard deviation of program size and standard deviation of fitness across population.",
            "diversity_results": "Used as baseline: plain GP maintained diversity comparable to GWS in the Posner task; pruning sometimes reduced diversity relative to plain GP. No numeric values given.",
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Posner cueing task modelling and symbolic regression.",
            "comparison_baseline": "N/A (this is the baseline).",
            "key_findings": "Plain GP serves as baseline; simplification schemes (GWS and pruning) produced smaller models generally without large fitness penalties compared to plain GP. In some cases plain GP had slightly better minimum fitness, but differences were minor.",
            "uuid": "e1736.2",
            "source_info": {
                "paper_title": "On-the-fly simplification of genetic programming models",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "NumericalSimpl",
            "name_full": "Numerical Simplification (semantic simplification)",
            "brief_description": "Simplification methods operating on semantics: they evaluate the contribution of nodes/subtrees by their effect on fitness (or outputs) and prune/replace nodes based on empirical criteria (including permutation tests or regression-point evaluation).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Numerical (semantic) simplification methods",
            "system_description": "A family of simplification techniques that operate at the semantic (behavioural) level by estimating the contribution of nodes/subtrees to parent fitness or by comparing subtree outputs against reference regression points; some variants replace subtrees with terminals or simpler subtrees when semantics are approximately preserved. Methods can use permutation tests to accept/reject pruning proposals to control error.",
            "input_type": "programs (GP trees) evaluated semantically (their outputs on test points)",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Semantic fidelity measured via outputs on regression/evaluation points, and statistical tests (e.g. permutation tests) to decide pruning; specific metrics vary by method.",
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression and other GP tasks where semantics can be sampled and compared.",
            "comparison_baseline": "Algebraic simplification, subtree substitution methods, etc.",
            "key_findings": "Paper cites numerical simplification approaches as an alternative to algebraic simplification, noting they work semantically and can replace nodes/subtrees based on contribution; they can be computationally costly and require careful thresholding, but can better detect semantically equivalent but syntactically different code.",
            "uuid": "e1736.3",
            "source_info": {
                "paper_title": "On-the-fly simplification of genetic programming models",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "AlgebraicSimpl",
            "name_full": "Algebraic Simplification",
            "brief_description": "Simplification based on algebraic equivalences that replaces code fragments with smaller algebraic equivalents while guaranteeing no change in computed output (requires domain-specific equivalence rules).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Algebraic simplification methods",
            "system_description": "Simplification approach that uses algebraic identities and expert-defined equivalence rules to replace subexpressions with smaller equivalent code fragments; commonly applied in symbolic regression and mathematical problems where equivalences are known.",
            "input_type": "programs (mathematical expressions / GP trees)",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Guarantees semantic equivalence via algebraic rules, so executability/functionality preserved by construction; no numerical metrics provided in paper.",
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression (mathematical domains) where algebraic equivalences are known.",
            "comparison_baseline": "Numerical simplification, parsimony pressure methods.",
            "key_findings": "Authors note algebraic simplification preserves fitness by relying on equivalence rules but requires domain expertise and cannot detect semantic equivalence of differently-looking code unless encoded as equivalences.",
            "uuid": "e1736.4",
            "source_info": {
                "paper_title": "On-the-fly simplification of genetic programming models",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "EpiSimpl",
            "name_full": "Epigenetics-inspired Simplification (gene silencing)",
            "brief_description": "A set of simplification techniques inspired by epigenetics where genes are silenced (rather than removed) to allow possible reactivation by future mutation, implemented in PushGP in cited work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Epigenetics-inspired simplification methods",
            "system_description": "Instead of permanently removing genetic material (introns), these methods silence genes so that they do not contribute to current behaviour but remain available for reactivation through future mutations; implemented in some GP frameworks (e.g., PushGP) to control bloat while preserving evolvability.",
            "input_type": "programs (GP genomes / trees, e.g., PushGP programs)",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "GP bloat control research (general GP domains); cited implementation used PushGP.",
            "comparison_baseline": "Standard pruning/removal simplification techniques.",
            "key_findings": "Paper cites epigenetics-inspired methods as alternatives that silence genes (retain chance of unsilencing via mutation), potentially preserving evolvability; no experimental details given in this paper.",
            "uuid": "e1736.5",
            "source_info": {
                "paper_title": "On-the-fly simplification of genetic programming models",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic Programming 1996: Proceedings of the First Annual Conference, July 28-31, 1996, Stanford University",
            "rating": 2
        },
        {
            "paper_title": "Using Numerical Simplification to Control Bloat in Genetic Programming",
            "rating": 2
        },
        {
            "paper_title": "Reducing code bloat in Genetic Programming based on subtree substituting technique",
            "rating": 2
        },
        {
            "paper_title": "Semantics Based Substituting Technique for Reducing Code Bloat in Genetic Programming",
            "rating": 2
        },
        {
            "paper_title": "It is Time for New Perspectives on How to Fight Bloat in GP",
            "rating": 2
        },
        {
            "paper_title": "Algebraic simplification of GP programs during evolution",
            "rating": 2
        },
        {
            "paper_title": "Effects of program simplification on simple building blocks in Genetic Programming",
            "rating": 2
        },
        {
            "paper_title": "Pruning of genetic programming trees using permutation tests",
            "rating": 2
        },
        {
            "paper_title": "Time Control or Size Control? Reducing Complexity and Improving Accuracy of Genetic Programming Models",
            "rating": 2
        }
    ],
    "cost": 0.012923,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On-the-fly simplification of genetic programming models</h1>
<p>Noman Javed<br>London School of Economics and Political Science n.javed3@lse.ac.uk</p>
<h2>ABSTRACT</h2>
<p>The last decade has seen amazing performance improvements in deep learning. However, the black-box nature of this approach makes it difficult to provide explanations of the generated models. In some fields such as psychology and neuroscience, this limitation in explainability and interpretability is an important issue. Approaches such as genetic programming are well positioned to take the lead in these fields because of their inherent white box nature. Genetic programming, inspired by Darwinian theory of evolution, is a population-based search technique capable of exploring a highdimensional search space intelligently and discovering multiple solutions. However, it is prone to generate very large solutions, a phenomenon often called "bloat". The bloated solutions are not easily understandable. In this paper, we propose two techniques for simplifying the generated models. Both techniques are tested by generating models for a well-known psychology experiment. The validity of these techniques is further tested by applying them to a symbolic regression problem. Several population dynamics are studied to make sure that these techniques are not compromising diversity - an important measure for finding better solutions. The results indicate that the two techniques can be both applied independently and simultaneously and that they are capable of finding solutions at par with those generated by the standard GP algorithm - but with significantly reduced program size. There was no loss in diversity nor reduction in overall fitness. In fact, in some experiments, the two techniques even improved fitness.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Computing methodologies $\rightarrow$ Genetic programming;</li>
</ul>
<h2>KEYWORDS</h2>
<p>Evolutionary Computing, Genetic Programming, Simplification</p>
<h2>ACM Reference Format:</h2>
<p>Noman Javed and Fernand Gobet. 2021. On-the-fly simplification of genetic programming models. In Proceedings of ACM SAC Conference (SAC'21). ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3412841.3441926</p>
<h2>1 INTRODUCTION</h2>
<p>Deep learning, having theoretical foundations in neural networks, has dominated the arena of machine learning during the last decade. The primary reason behind this dominance has been its impressive performance. Factors such as the increase in computational power and the availability of software resources have also contributed to the adoption of deep learning in almost every field. Despite impressive performance and widespread acceptability, there exist communities in psychology, neuroscience, and many other domains</p>
<p>Fernand Gobet<br>London School of Economics and Political Science f.gobet@lse.ac.uk</p>
<p>that are reluctant to accept the solutions proposed by deep learning. The main cause of their concern is the black-box nature of deep learning, which conceals the relationship between inputs and outputs. These communities want to understand the process of transformation of input to output. Understanding this relationship is of utmost importance for interpreting and explaining the models. That is why explainable artificial intelligence is gaining traction and is subject of much research $[2,7,9,22]$.</p>
<p>We can classify research in explainable artificial intelligence into two categories. One way is to attach explanations to the black-box models [8]. Other approaches are inherently explainable because of their white-box nature. One such technique is genetic programming (GP). It is not only intrinsically transparent but can also be employed to generate explanations of black-box models [8, 10]. Thus, it has the potential to achieve good performance without compromising explainability.</p>
<p>Despite the inherent advantage of its white-box nature, GP is not as widely used as deep learning. One of the limiting factors is the complexity of the solutions generated by GP, where complexity refers to the size of the individual, and the size means the number of nodes in a tree. The programs generated by GP tend to grow in size without significantly improving performance. This phenomenon is called "bloat". This problem has been identified from the very early days of GP [3] and the literature is replete with methods for bloat identification and control [13, 21]. One of the well-know bloat control techniques is simplification [12]. The logic behind simplification is to reduce the size of the individual. It does so by identifying the redundant parts that are not contributing to the improvement in fitness. It then removes these redundant noncontributing parts, resulting in a smaller sized individual with the same or better fitness.</p>
<p>In this paper, we propose two simplification techniques and report their results in two tasks, comparing them with the results of standard GP. First, we apply them to the task of generating explanatory models for a well-known psychology experiment known as the Posner cueing task, the aim of which is to understand attentional mechanisms in human cognition. In this experiment, a cue first appears (e.g. an arrow pointing left or right). Then, participants have to respond as rapidly as possible to a target presented to the left or right of a fixation point or the left or right ear. To further verify the performance and applicability of these simplification techniques, we also apply them to a symbolic regression problem. GP has already been used extensively for performing symbolic regression. We found that both these techniques generate solutions of significantly reduced size in both the Posner cueing and symbolic regression problems. It was further observed that this reduction in size does not come at a price of fitness or execution time.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>2 SIMPLIFICATION</h2>
<p>Since GP is primarily fitness driven, it tends to prefer fitter individuals by assigning them a high probability of selection to breed the next generation of individuals. This approach has several side effects.</p>
<ul>
<li>The size of individuals keeps on increasing over generations. At the same time, the rate of improvement in fitness decreases gradually. So, a little improvement in fitness comes at the cost of a significant increase in size.</li>
<li>Multiple copies of an individual can exist in a generation and across generations.</li>
<li>Redundant genetic material. Many individuals share the same copies of genetic materials in the form of common subtrees.
Although a limited amount of redundancy is not harmful, an increase in redundancy means a decrease in diversity, which in turn means a meager exploration of the search space; this may result in premature convergence.</li>
</ul>
<p>To address these side effects, researchers propose several simplification techniques in the literature. The core idea behind all of them is to identify and get rid of the redundant and non-contributing parts, also called introns. Simplification algorithms mainly differ in the way they spot similarities. Some of them use syntactic similarity to classify a part of an individual as redundant, whereas others use semantic similarity to achieve the same aim.</p>
<p>The earliest and most frequently used form of bloat control is to place a constraint on the size of the program. The target of this constraint is usually the depth of the GP trees. The limitation of this approach is that it cannot find solutions lying beyond these limits. Moreover, the individuals within those limits can still contain redundant and unwanted code. Another frequently used idea is to make program size an integral part of the fitness function. In this way, the individuals bigger in size get penalised, thus making it less likely for them to take part in reproduction. These sorts of approaches are categorised under parsimony pressure.</p>
<p>Unlike the above mentioned indirect ways of controlling bloat, algebraic simplification [23, 24] is inspired by the principles of algebra. It replaces a part of code with an algebraic equivalent that is smaller in size. In this way, it guarantees that the fitness of the individual will remain unaffected by the process of simplification. The limitation of algebraic simplification is that domain experts need to develop equivalence rules. This process may be simple for mathematical problems but can pose significant challenges for other types of problems. Probably because of this reason, it has been mostly applied to symbolic regression and classification problems. Another limitation is that it never considers the fact that two different-looking individuals can yield the same behaviour; thus, they are semantically equivalent. The algebraic approach is unable to simplify such cases.</p>
<p>Numerical simplification techniques work at the semantic level rather than the algebraic level [11]. They work out by calculating the contribution of a node in its parent's fitness. If the contribution is significant, the child node replaces its parent node. Similarly, constants can replace some nodes. The first limitation of this approach is that the process is localised and limited to just one node. Secondly, thresholds must be defined very carefully and can have a
significant impact. Another issue is the maintenance of minimum and maximum values to take pruning decisions. These limitations are addressed by defining a mechanism of accepting or rejecting pruning proposals based on permutation tests [18]. In another variant [14], the authors evaluate subtrees against some predefined regression points. Based on this evaluation, these subtrees are replaced by simpler subtrees. The main limitation of this approach is the penalty incurred in the form of computational cost. The authors of [4] have used a very similar technique of replacing a subtree with a terminal. They further extended their approach and proposed to replace a subtree with another subtree with approximately the same semantics [5, 15]. In [6], the authors proposed five different simplification techniques inspired from epigenetics. Instead of completely removing a gene, they silence it to retain the chance that mutation may unsilence it at some point in time. They used PushGP as the platform for implementing these simplification algorithms.</p>
<p>A relatively new approach of bloat control is using the execution time of individuals rather than their size [6, 19]. Our proposed techniques can be categorised under numerical simplification as they are based on fitness rather than syntactic similarity. It is currently based on program size but can be easily extended to time-driven simplification. Another major difference is that both our simplification algorithms work as part of an evolutionary run rather than post-processing of the individuals.</p>
<h2>3 PROPOSED SIMPLIFICATION TECHNIQUES</h2>
<h3>3.1 Generationwide Simplification (Gws)</h3>
<p>Since GP generates individuals through crossover, they contain the genetic material of both the parents. Some of this genetic material is important and contributing to the better fitness of the individual, while some of it can be of no use. Thus, there is a possibility of reducing the size of an individual by keeping a copy of good genetic material and removing the one that is not contributing. The idea is to replace a less fit parent individual with a child of better fitness. Since a child is a subtree of a parent individual, its size will be smaller than the parent tree.</p>
<p>We apply this simplification mechanism to all individuals of every $k$ th generation. At every $k$ th generation, instead of breeding the new individuals, using crossover or mutation, all individuals are subject to the simplification procedure. Every individual produces several child subtrees, whose fitness is re-computed. A hash value of every subtree is computed and is stored. In this way, multiple instances of a subtree - whether they exist in an individual or within different individuals - are stored only once. This process will create a new population of individuals that may or may not exceed the specified population size. If the number of newly generated individuals is greater than the population size, some of them having low fitness values will be discarded. But in the other case, the remaining individuals are generated following the routine process of crossover and mutation.</p>
<p>This idea has the following potential merits:</p>
<ul>
<li>Getting rid of the multiple copies of individuals</li>
<li>
<p>Possibility of unlocking good genetic material locked within an individual. This situation happens when a subtree has better fitness than the parent. But because of the other genetic material in the parent individual, it cannot express itself.</p>
</li>
<li>
<p>Getting rid of low-quality genetic material.</p>
</li>
<li>No growth in terms of the size of the individuals. In the worst case - when every individual is of higher fitness than all of their children - they will remain untouched and become a part of the population. However, in the average case, there will be a reduction in the size of the individuals.</li>
<li>No extra computational cost. We compute a hash value of every individual and store its fitness against that. So, redundant copies of individuals have no impact as they will undergo fitness computation only once.</li>
</ul>
<h3>3.2 Pruning as an Operator</h3>
<p>In contrast to the generationwide simplification, pruning operates at the individual level rather than at the population level. The motivation behind this is to prune only those individuals who have a high probability of being selected as parents. These are the individuals of relatively higher fitness than the rest of the population. Thus, we applied pruning as an operator in every generation after calculating the fitness and sorting the individuals by fitness. Pruning selects the top few percent individuals of a population, based on fitness, and prunes them. This percentage is called the pruning rate, which can be varied. The individual selected for pruning undergoes a lossless pruning process. The pruning operator generates a set of subtrees of an individual and computes their fitness. The fittest subtree replaces the parent tree. If the fitness of the parent tree is best, it can retain its place without being impacted by the pruning operator.</p>
<p>Pruning, as an operator, offers the following benefits:</p>
<ul>
<li>Reduction in size of relatively fit individuals</li>
<li>It does not interfere with the normal working of other genetic operators</li>
<li>Very low computational overhead. This overhead depends on the pruning rate. With high pruning rates, it can be costly.
You can notice the similarity of the simplification procedure between both these techniques. They mainly differ in their application, where the former works at the generational level and the latter operates at the individual level. There are some other subtle differences. For example, in pruning, after splitting, the replacement of an individual comes from one of its children. Whereas in generationwide simplification, an individual and all its children may disappear from the population because of low fitness values; hence the one-to-one replacement of parent and child is not guaranteed.</li>
</ul>
<h2>4 EXPERIMENTS AND RESULTS</h2>
<p>To test our proposed simplification algorithms, we use two problems of a completely different nature. The first is a well-known psychology experiment to understand the attentional mechanisms in human cognition, and the task is to find a model accounting for the human data. The other one is a classical symbolic regression problem.</p>
<h3>4.1 Posner's cueing experiment</h3>
<p>The experiment we used was carried out by Arjona et al.[1] and is a variation of Posner's cueing task [16]. The general aim of this task is to understand the role of attention on rapid perceptual decision making. In Arjona et al.'s experiment, participants are seated in
front of a computer display and fixate a white cross at the centre of the screen. The cross is then replaced by an arrow, pointing either to the left or the right (visual cue). This is followed by the presentation of a sound in either the left or right ear (auditory stimulus). Participants are asked to press the button corresponding to the side of the auditory stimulus, using the index finger of the left or right hand. They have one second to do so. After a short pause, this sequence of events is repeated in the following trial.</p>
<p>The detailed time course of events is as follows:
(1) The central white cross is fixated for 300 milliseconds (ms)
(2) The visual arrow is displayed for 300 ms
(3) The central white cross is shown for 370 ms
(4) The auditory stimulus is presented for 100 ms
(5) The white cross is shown while the participant provides a response (within $1,000 \mathrm{~ms}$ )
While the auditory stimuli were randomly presented to the left or right ear with equal probability, the cue validity was systematically manipulated. In the $50 \%$ validity condition, the visual cue was valid (i.e. correctly predicted the auditory stimulus) in $50 \%$ of the trials. In the $68 \%$ validity condition, the cue was predictive in $68 \%$ of the trials. Finally, in the $86 \%$ validity condition, the cue was predictive in $86 \%$ of the trials. The trials were organised in 6 blocks of 100 trials, with 2 blocks for each cue validity condition. Thirty participants took part in Arjona et al.'s (2016) experiment.</p>
<p>Arjona et al. collected both behavioural data and electrophysiological data. In our simulations, we used only the behavioural data (response time and the number of errors). The main results were as follows. There was a significant cueing effect, as the response times were faster in the valid trials than in the invalid trials; the effect increased together with the proportion of valid trials ( $86 \%&gt;68 \%&gt;$ $50 \%$ ). Accuracy was also affected by cue validity: as the percentage of cue validity increased, the percentage of incorrect responses increased in the invalid trials, compared to the valid trials.</p>
<p>The simulations implemented the key features of the human experiment described above, thus producing 12 data points ( 3 cue validities $(50 \%, 68 \%$ and $86 \%) \times 2$ types of trials (valid and invalid) $\times 2$ dependent variables (response time and the number of errors)). The main simplification in the simulations was that the stimuli were presented symbolically and not as bitmaps and physical sounds. GP constructed models by combining terminals and operators, which all had a simulated time cost, the value of which was based on the psychological literature. The operators implementing learning employed Rescorla and Wagner's rule [17]:</p>
<p>$$
\Delta V=\alpha(\lambda-V)
$$</p>
<p>where $\Delta V$ is the change in the strength of association between cue and stimulus, $\alpha$ is the rate of change, $\lambda$ the maximal value of the strength of association, and $V$ the current strength of association. The terminals and operators used in the simulations were (a) terminals for inputting the cue and the stimuli, and responding; (b) operators for inputting and retrieving information in short-term memory (STM), and for comparing two elements in STM; (d) operators for carrying sequences of actions; (e)"waiting" operators, which did not do anything except increase a model's clock; (f) operators for directing attention; (g) operators for matching cue-stimulus probability or learning it using Rescorla and Wagner rule; and (h) IF and NIL. Table 1 presents the detail of the operators used.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operator</th>
<th style="text-align: left;">Arity</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">access-stm-1 (2 or 3)</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Put item in STM slot 1 (2 or 3) into current value</td>
</tr>
<tr>
<td style="text-align: left;">compare-1-2 (1-3 or 2-3)</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Current value is true/false if STM item 1/2/3 = item 2/3/1</td>
</tr>
<tr>
<td style="text-align: left;">if</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">Selects between two operators based on current value</td>
</tr>
<tr>
<td style="text-align: left;">prog2 (3 or 4)</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Sequentially do two (three or four) operators</td>
</tr>
<tr>
<td style="text-align: left;">put-stm</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Push current value on to STM</td>
</tr>
<tr>
<td style="text-align: left;">wait-200 (1000 or 1500)</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Waits for 200 ms (1,000 or 1,500 ms)</td>
</tr>
<tr>
<td style="text-align: left;">nil</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Set current value to 0 ('false')</td>
</tr>
<tr>
<td style="text-align: left;">respond-cue</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Uses only the cue to predict the stimulus</td>
</tr>
<tr>
<td style="text-align: left;">respond-stimulus</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Always responds with the stimulus</td>
</tr>
<tr>
<td style="text-align: left;">match-probability</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Uses the predictive validity of the cue, which is assumed to be known</td>
</tr>
<tr>
<td style="text-align: left;">ResWagner-update</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Learns by updating the predictive validity of the cue</td>
</tr>
<tr>
<td style="text-align: left;">ResWagner-cue-stimulus</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Uses the cue and the strength of association to predict the stimulus</td>
</tr>
<tr>
<td style="text-align: left;">ResWagner-cue-priming-stimulus</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Uses both the cue and the perception of the stimulus to select the stimulus</td>
</tr>
</tbody>
</table>
<p>Table 1: Operators for modelling Posner's Task
4.1.1 Experimental Setup. We used the GP library proposed by Koza [12] to implement the two tasks and the simplification algorithms. Steel bank Common Lisp (SBCL) [20] version 1.5.5 was used to run the experiments. We ran the experiment 10 times, where each run comprised 100 generations. The population size of each generation was 1,000 . Generationwide simplification was applied after every 25th generation while the pruning rate was 2 percent.
4.1.2 Results. The top ten individuals of every run, in terms of fitness, were compared using their average and maximum program sizes (see figure 1). The maximum reduction in program size occurred by applying both simplification schemes. However, for the large-sized individuals, generationwide simplification was better in limiting their growth. You can notice the small boxes of simplification schemes as compared to the plain GP, indicating that these schemes were successful in reducing the sizes of most of the individuals.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Posner Task - Top 10 / Run: Program Size</p>
<p>We compared these top 10 individuals of every run in terms of fitness. Here in figure 2, we present the mean and minimum fitness. A minimum is better because it reflects the error with respect to
the original. You can notice that, in terms of minimum fitness, plain GP is best. However, other schemes are not very far from it. The smaller areas of simplification schemes suggest that most of the individuals are very near to each other in terms of fitness.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Posner Task - Top 10 / Run: Fitness
Generation specific statistics demonstrate the performance of simplification schemes as a part of the evolutionary journey. Figure 3 shows the comparison of the size of the fittest 20 individuals of every generation. It is evident from the figure that both the simplification schemes perform better than the plain GP. In some cases, the difference is very significant. This reduction in program size does not come at the cost of diversity, as is evident from the bottom-most graph of the same figure. This graph shows the plot of the standard deviation of the program size.</p>
<p>At what cost this reduction in size happened is a pertinent question at this stage. Two relevant cost measures are a cost in terms of fitness and a cost in terms of loss in diversity. To measure the first cost, we compared the fitness of the fittest 20 individuals of all generations (see figure 4). One can observe that the plain line is slightly lower than the other lines most of the time, thus indicating better fitness when no simplification is applied. However, the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Posner Task - Best 20: Program Size</p>
<p>four lines are very near, hence pointing out the negligible cost of simplification in terms of fitness.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Posner Task - Best 20: Fitness</p>
<p>So far, the story is about the fittest individuals. To measure how much simplification is impacting the overall dynamics of the evolutionary progress, we compare the program sizes, fitness, and execution time taken by individuals. The scale of the comparison is at the population level to get a population-level view. Figure 5 compares the program sizes in terms of average program size and maximum program size. It is evident from the graphs that simplification algorithms perform better than the plain GP. These simplification algorithms are successful not only in reducing the size of the better individuals but also have a positive impact on the whole population.</p>
<p>To verify the cost paid in terms of fitness and loss in diversity, we compared population-wide fitness. The comparison is shown in figure 6. We observe no significant loss in terms of fitness. A comparison of diversity, both in terms of fitness and program size, was captured by calculating the standard deviation as presented in figure 7. Again, no loss in diversity is evident from the graph.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Posner Task - Comparison of Program Sizes</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Posner Task - Comparison of Fitness</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Posner Task - Comparison of Diversity</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Symbolic Regression - Best 20: Program Size</p>
<p>We conclude from the results that these simplification schemes help reduce the sizes of individuals without incurring a performance penalty or loss of diversity.</p>
<h3>4.2 Symbolic Regression</h3>
<p>To verify the applicability of the proposed simplification algorithms, we used a classical regression problem of fitting the curve to a degree four polynomial.</p>
<p>$$
x^{4}+x^{3}+x^{2}+x
$$</p>
<p>GP has already been used extensively for generating solutions for this kind of problem.
4.2.1 Experimental Setup. The operators required to generate solutions for this task are the classical mathematical operators of addition, subtraction, multiplication, and trigonometric functions. We also implemented a safe division operator to avoid division by zero. The experiments were executed using version 1.5.5 of SBCL. Each experiment was run 10 times where each run comprised 100 generations and the population size of each generation was 1,000 .
4.2.2 Results. When tested on symbolic regression, most of the time pruning as an operator produced individuals that were very similar to each other across generations. It also failed to reduce the size of the individuals, as presented in figure 8. One can notice that pruning as an operator nullified the impact of generationwide simplification. Hence, the plot lines of both almost followed the trend of pruning. The standard deviation plot showed the loss in diversity whenever pruning is applied. This loss in diversity is the reason for search being stuck in local minima, thus producing individuals comprising of the same genetic makeup. Generationwide simplification, on the other hand, not only reduced the size of the individuals but also retained genetic diversity.</p>
<p>To make sure that there is no price of size reduction, a comparison of the fitness of the fittest 20 individuals of all generations is presented in figure 9. All the approaches are equally performing as is evident from the very close fitness lines.</p>
<p>To holistically study the behaviour, we compared the program sizes of all individuals across generations. Figure 10 depicts the same
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Symbolic Regression - Best 20: Fitness
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Symbolic Regression - Comparison of Program Sizes
trend as was observed in the case of the top 20 individuals. Hence, the hypothesis of pruning based simplification leading towards local optima is further validated. Generationwide simplification line stayed below the plain line most of the time, thus indicating the marginal reduction in program sizes.</p>
<p>To verify the cost paid in terms of fitness, we compare populationwide fitness. The comparison is shown in figure 11. Again, it followed the same trend that was presented in the top 20's scenario. Sometimes we obtained better fitness by applying simplification.</p>
<p>A comparison of diversity, both in terms of fitness and program size, is presented by calculating the standard deviation in figure 12. In terms of fitness, there are no significant differences between all the schemes. However, the rate of growth of standard deviation when pruning is applied is not as fast as the generationwide simplification and plain GP. This indicates the loss of genetic diversity.</p>
<p>These results indicate that better solutions, with a relatively smaller size, can be obtained by applying these simplification schemes. However, pruning is prone to stick in local minima because it causes</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Symbolic Regression - Comparison of Fitness
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Symbolic Regression - Comparison of Diversity
the loss of genetic diversity. On the other hand, generationwide simplification is robust to any such genetic loss.</p>
<h2>5 CONCLUSION AND FUTURE WORK</h2>
<p>White-box approaches are needed to cope with the challenge of interpretability and explainability in machine learning. GP is one such approach and has the potential to take the lead, provided it can simplify its generated models by reducing their size. This paper proposed two simplification techniques to reduce the size of GP generated models. Both of these techniques work as part of the evolutionary process, thus offering a chance to control bloat as well. An added benefit of these techniques is keeping redundancy under check. The proposed approaches are tested by implementing a well-known psychological experiment and the classical symbolic regression problem. Both these approaches performed better than the plain GP. No extra cost is incurred in the form of loss of fitness and/or loss in diversity. Both the approaches generated models of significantly smaller sizes as compared to the plain GP.</p>
<p>As claimed, these approaches can be extended using time as a simplification parameter rather than size. Time could be execution
time or any other form of time, such as reaction time in the case of the operators used in the Posner task. The potential benefit of using time-based simplification is qualitatively comparing different operators taking part in the model generation. Another idea is to compare these simplification techniques with multi-objective fitness measures where a part of the fitness function will take care of size or time. Thus, penalising individuals with greater sizes and high time measures by assigning them low fitness.</p>
<p>These approaches can be made computationally more viable by storing fitness results and output vectors of the individuals. In this way, whenever required these stored results can be accessed without any need for recalculation. These stored results can also be used as a semantic measure to measure the similarity of two genetic different looking individuals. Hence, the current fitness driven simplification approaches can be extended by output resultdriven measures.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>The authors would like to thank the anonymous referees for their valuable comments and helpful suggestions. This article is part of the project "Genetically Evolving Models in Science (GEMS)" that has received funding from the European Research Council (ERC) under the grant agreement no. ERC-2018-ADG-835002.</p>
<h2>REFERENCES</h2>
<p>[1] Antonio Arjona, Miguel Escudero, and Carlos M. GÃ³mez. 2016. Cue validity probability influences neural processing of targets. Biological Psychology 119 (2016), 171 - 183. https://doi.org/10.1016/j.biopsycho.2016.07.001
[2] Alejandro Barredo Arrieta, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion 58 (June 2020), 82-115. https://doi.org/10.1016/j.inffus.2019.12.012
[3] Tobias Blickle and Lothar Thiele. 1994. Genetic programming and redundancy.. In Genetic Algorithms within the Framework of Evolutionary Computation (Workshop at KI-94). SaarbrÃ¼cken, 33-38.
[4] T. H. Chu and Q. U. Nguyen. 2017. Reducing code bloat in Genetic Programming based on subtree substituting technique. In 2017 21st Asia Pacific Symposium on Intelligent and Evolutionary Systems (IES). 25-30.
[5] Thi Huong Chu, Quang Uy Nguyen, and Van Loi Cao. 2018. Semantics Based Substituting Technique for Reducing Code Bloat in Genetic Programming. In Proceedings of the Ninth International Symposium on Information and Communication Technology - SoICT 2018. ACM Press, Danang City, Viet Nam, 77-83. https://doi.org/10.1145/3287921.3287948
[6] Francisco FernÃ¡ndez de Vega, Gustavo Olague, Francisco ChÃ¡vez de la O, Daniel Lanza, Wolfgang Banzhaf, and Erik D. Goodman. 2020. It is Time for New Perspectives on How to Fight Bloat in GP. CoRR abs/2005.00603 (2020). arXiv:2005.00603 https://arxiv.org/abs/2005.00603
[7] Jean-Marc Fellous, Guillermo Sapiro, Andrew Rossi, Helen Mayberg, and Michele Ferrante. 2019. Explainable Artificial Intelligence for Neuroscience: Behavioral Neurostimulation. Frontiers in Neuroscience 13 (Dec. 2019), 1346. https://doi.org/ 10.3389/fnins. 2019.01346
[8] Leonardo Augusto Ferreira, Frederico Gadelha GuimarÃ£es, and Rodrigo Silva. 2020. Applying Genetic Programming to Improve Interpretability in Machine Learning Models. arXiv:2005.09512 [cs] (May 2020). http://arxiv.org/abs/2005. 09512 arXiv: 2005.09512.
[9] Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, Simone Stumpf, Peter Kieseberg, and Andreas Holzinger. 2018. Explainable AI: The New 42? In Machine Learning and Knowledge Extraction. Vol. 11015. Springer International Publishing, Cham, 295-303. https://doi.org/10.1007/ 978-3-319-99740-7_21 Series Title: Lecture Notes in Computer Science.
[10] Daniel Howard and Mark A. Edwards. 2018. Explainable A.I.: The Promise of Genetic Programming Multi-run Subtree Encapsulation. In 2018 International Conference on Machine Learning and Data Engineering (iCMLDE). IEEE, Sydney, Australia, 158-159. https://doi.org/10.1109/iCMLDE.2018.00037
[11] David Kinzett, Mengjie Zhang, and Mark Johnston. 2008. Using Numerical Simplification to Control Bloat in Genetic Programming. In Simulated Evolution</p>
<p>and Learning. Vol. 5361. Springer Berlin Heidelberg, Berlin, Heidelberg, 493502. https://doi.org/10.1007/978-3-540-89694-4_50 Series Title: Lecture Notes in Computer Science.
[12] John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo (Eds.). 1996. Genetic Programming 1996: Proceedings of the First Annual Conference, July 28-31, 1996, Stanford University. The MIT Press. https://doi.org/10.7551/mitpress/3242. 001.0001
[13] Sean Luke and Liviu Panait. 2006. A Comparison of Bloat Control Methods for Genetic Programming. Evolutionary Computation 14, 3 (Sept. 2006), 309-344. https://doi.org/10.1162/evco.2006.14.3.309
[14] Mori Naoki, Bob McKay, Nguyen Xuan, Essam Daryl, and Saori Takeuchi. 2009. A New Method for Simplifying Algebraic Expressions in Genetic Programming Called Equivalent Decision Simplification. In Distributed Computing, Artificial Intelligence, Bioinformatics, Soft Computing, and Ambient Assisted Living. Vol. 0318. Springer Berlin Heidelberg, Berlin, Heidelberg, 171-178. https://doi.org/10.1007/ 978-3-642-02481-8_24 Series Title: Lecture Notes in Computer Science.
[15] Quang Uy Nguyen and Thi Huong Chu. 2020. Semantic approximation for reducing code bloat in Genetic Programming. Swarm and Evolutionary Computation 58 (Nov. 2020), 100729. https://doi.org/10.1016/j.swevo.2020.100729
[16] Michael I. Posner. 1980. Orienting of attention. Quarterly Journal of Experimental Psychology 32, 1 (1980), 3-25. https://doi.org/10.1080/00335558008248231 arXiv:https://doi.org/10.1080/00335558008248231 PMID: 7367577.
[17] Robert A. Rescorla and A. R. Wagner. 1972. A theory of Pavlovian conditioning: Variations on the effectiveness of reinforcement and non-reinforcement. In Classical conditioning B: Current research and theory. Appleton-Century-Crofts, New York, 64-99.
[18] Peter Rockett. 2020. Pruning of genetic programming trees using permutation tests. Evolutionary Intelligence (April 2020). https://doi.org/10.1007/ s12065-020-00379-8
[19] Aliyu Sani Sambo, R. Muhammad Atif Azad, Yevgeniya Kovalchuk, Vivek Padmanaabhan Indramohan, and Hanifa Shah. 2020. Time Control or Size Control? Reducing Complexity and Improving Accuracy of Genetic Programming Models. In Genetic Programming. Vol. 12101. Springer International Publishing, Cham, 195-210. https://doi.org/10.1007/978-3-030-44094-7_13 Series Title: Lecture Notes in Computer Science.
[20] SBCL. 2019. Steel Bank Common Lisp. http://www.sbcl.org/
[21] Sara Silva, Stephen Dignum, and Leonardo Vanneschi. 2012. Operator equalisation for bloat free genetic programming and a survey of bloat control methods. Genetic Programming and Evolvable Machines 13, 2 (June 2012), 197-238. https://doi.org/10.1007/s10710-011-9150-5
[22] Jonas Wanner, Lukas-Valentin Herm, and Christian Janiesch. 2020. How much is the black box? The value of explainability in machine learning models. In ECIS 2020 Research-in-Progress Papers. 15. https://aisel.aisnet.org/ecis2020_rip/85
[23] Phillip Wong and Mengjie Zhang. 2006. Algebraic simplification of GP programs during evolution. In Proceedings of the 8th annual conference on Genetic and evolutionary computation - GECCO '06. ACM Press, Seattle, Washington, USA, 927. https://doi.org/10.1145/1143997.1144156
[24] Phillip Wong and Mengjie Zhang. 2007. Effects of program simplification on simple building blocks in Genetic Programming. In 2007 IEEE Congress on Evolutionary Computation. IEEE, Singapore, 1570-1577. https://doi.org/10.1109/CEC. 2007.4424660</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>SAC'21, March 22-March 26, 2021, Gwangju, South Korea
2021. ACM ISBN 978-1-4503-8104-8/21/03... $\$ 15.00$
https://doi.org/10.1145/3412841.3441926&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
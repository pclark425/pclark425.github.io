<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2140 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2140</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2140</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-56.html">extraction-schema-56</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <p><strong>Paper ID:</strong> paper-278959891</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.21923v2.pdf" target="_blank">FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</a></p>
                <p><strong>Paper Abstract:</strong> Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates>99% accuracy in topology inference,<10% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2140.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2140.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cadence Spectre (via Virtuoso + ADE XL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cadence Spectre simulator (run from Cadence Virtuoso using ADE XL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Industry-grade transistor-level circuit simulator used to generate a 1M+ datapoint dataset with a 45 nm CMOS PDK; provides foundry-calibrated device models and realistic high-frequency behavior used to train and validate the ML pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>mm-wave analog/RF circuits (LNAs, mixers, PAs, VAs, VCOs)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_tool</strong></td>
                            <td>Cadence Spectre (via Cadence Virtuoso and ADE XL)</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Foundry PDK transistor-level device models (45 nm CMOS PDK) used for active devices in Spectre simulations; passive components treated with analytical, layout-aware models in the learning pipeline (MIM capacitor area+perimeter model, N+ silicided polysilicon resistor empirical layout model, octagonal spiral inductor monomial model). Active-device geometries excluded from the differentiable layout-area penalty (PDK-fixed).</td>
                        </tr>
                        <tr>
                            <td><strong>parasitics_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>nonlinearities_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>tolerances_variations</strong></td>
                            <td>none (no Monte Carlo/process-corner sweeps reported; dataset generated by parameter sweeps using ADE XL but not randomized process variations)</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>Supervised MLP (topology classifier); edge-centric Graph Neural Network (forward performance predictor); gradient-based optimization over learned surrogate for inverse design (parameter inference).</td>
                        </tr>
                        <tr>
                            <td><strong>training_task</strong></td>
                            <td>Train forward surrogate to predict 16 performance metrics from netlist/parameters; train MLP to select topology from target performance; perform inverse design (parameter inference) via gradient-based optimization of the learned forward model under layout-aware loss.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Dataset: >1,000,000 Cadence-simulated datapoints. Forward GNN: average R^2 ≈ 0.972 and overall mean relative error ≈ 9.1% (paper reports 9.09% untrimmed; seed-averaged 9.14±0.38%). Stage 1 MLP topology selection accuracy ≈ 99.57%. Stage 3 inverse-design pipeline: success rate 78.5% (design considered successful if Cadence-simulated performance deviates <20% from target), mean relative error of 17.7% across converged designs, typical inference time <1 s per instance on a MacBook CPU.</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_requirements_discussed</strong></td>
                            <td>The paper argues for foundry-grade (PDK-calibrated) Spectre simulations and layout-aware parasitic modeling to capture realistic mm-wave behavior; it explicitly notes that the differentiable analytical layout/parasitic models guide optimization but do not replace EM simulations or post-layout verification and recommends learned parasitic estimators and EM-informed models to further close post-layout gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td>Generalization to an unseen topology (RVCO) in zero-shot produced mean relative error ≈ 30.4%; fine-tuning only the output head on ~30k RVCO samples reduced error to 0.9%, indicating a sim-to-model/topology transfer gap that is solvable with modest fine-tuning. The paper also cautions that analytical layout/parasitic approximations are not a full substitute for EM/post-layout verification and can limit final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_range</strong></td>
                            <td>30 GHz (fixed operating frequency; mm-wave)</td>
                        </tr>
                        <tr>
                            <td><strong>electromagnetic_effects</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>thermal_effects</strong></td>
                            <td>False</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2140.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2140.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPICE / symbolic SPICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPICE or symbolic/simplified SPICE-style simulators (as referenced for prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose circuit simulators or simplified symbolic SPICE used in prior ML/benchmarking works; the paper contrasts SPICE-based or symbolic simulators with foundry-calibrated Cadence simulations, noting lower realism (missing PDK calibration and layout-dependent effects).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>various (prior work benchmarks often used small/synthetic analog circuits or low-frequency designs)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_tool</strong></td>
                            <td>SPICE or symbolic/simplified SPICE (mentioned as used by prior works / benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Typically SPICE-level device models or symbolic simplified models; in referenced prior work these are often not PDK-calibrated and may lack detailed foundry parasitics or layout coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>parasitics_modeled</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>nonlinearities_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tolerances_variations</strong></td>
                            <td>none (paper states many SPICE/synthetic benchmarks lack process/foundry realism and layout-dependent behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>various in prior works (supervised learning, reinforcement learning, Bayesian optimization reported in related work summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>training_task</strong></td>
                            <td>Referenced prior tasks include parameter sizing, topology generation, and performance prediction on synthetic/simplified benchmarks (not the main dataset of this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_details</strong></td>
                            <td>The paper contrasts SPICE/symbolic-simulator based datasets (e.g., AnalogGym, AutoCkt) as lacking process fidelity, noise characteristics, and layout-dependent behavior compared to their Cadence PDK-driven Spectre dataset; it argues that SPICE/symbolic datasets provide lower transfer realism.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_requirements_discussed</strong></td>
                            <td>Paper implies minimal requirement for practical ML-driven analog design is foundry-calibrated simulator data (Cadence + PDK) and layout-aware parasitic modeling, beyond symbolic SPICE.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td>The paper states SPICE/symbolic benchmarks can lead to unrealistic results and poor downstream transfer because they omit layout-dependent interactions and foundry calibration, but no specific quantitative sim-to-real failure cases are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frequency_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>electromagnetic_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>thermal_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2140.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2140.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADS (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Keysight Advanced Design System (ADS) (mentioned as a simulator used in some prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ADS is cited in the related-work comparison table as being used (alongside Cadence) by some prior RL/optimization studies; here it is mentioned only in passing and not used in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>prior works (likely RF/microwave designs referenced in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_tool</strong></td>
                            <td>Keysight ADS (mentioned in Table 4 as used by prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Not specified in this paper (mention only); ADS commonly provides EM/co-simulation features in prior literature but this paper does not report details.</td>
                        </tr>
                        <tr>
                            <td><strong>parasitics_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>nonlinearities_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tolerances_variations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>various in cited prior work (the table links ADS use to some RL/optimization studies)</td>
                        </tr>
                        <tr>
                            <td><strong>training_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_requirements_discussed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frequency_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>electromagnetic_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>thermal_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gcn-rl circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning <em>(Rating: 2)</em></li>
                <li>An efficient bayesian optimization approach for automated optimization of analog circuits <em>(Rating: 2)</em></li>
                <li>Domain knowledge-based automated analog circuit design with deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Align: A system for automating analog layout <em>(Rating: 2)</em></li>
                <li>Layoutcopilot: An llm-powered multi-agent collaborative framework for interactive analog layout design <em>(Rating: 2)</em></li>
                <li>AnalogGym: An open and practical testing suite for analog circuit synthesis <em>(Rating: 2)</em></li>
                <li>AICircuit: A Multi-Level Dataset and Benchmark for AI-Driven Analog Integrated Circuit Design <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2140",
    "paper_id": "paper-278959891",
    "extraction_schema_id": "extraction-schema-56",
    "extracted_data": [
        {
            "name_short": "Cadence Spectre (via Virtuoso + ADE XL)",
            "name_full": "Cadence Spectre simulator (run from Cadence Virtuoso using ADE XL)",
            "brief_description": "Industry-grade transistor-level circuit simulator used to generate a 1M+ datapoint dataset with a 45 nm CMOS PDK; provides foundry-calibrated device models and realistic high-frequency behavior used to train and validate the ML pipeline.",
            "citation_title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design",
            "mention_or_use": "use",
            "circuit_type": "mm-wave analog/RF circuits (LNAs, mixers, PAs, VAs, VCOs)",
            "simulator_tool": "Cadence Spectre (via Cadence Virtuoso and ADE XL)",
            "component_models": "Foundry PDK transistor-level device models (45 nm CMOS PDK) used for active devices in Spectre simulations; passive components treated with analytical, layout-aware models in the learning pipeline (MIM capacitor area+perimeter model, N+ silicided polysilicon resistor empirical layout model, octagonal spiral inductor monomial model). Active-device geometries excluded from the differentiable layout-area penalty (PDK-fixed).",
            "parasitics_modeled": true,
            "nonlinearities_modeled": true,
            "tolerances_variations": "none (no Monte Carlo/process-corner sweeps reported; dataset generated by parameter sweeps using ADE XL but not randomized process variations)",
            "ml_model_type": "Supervised MLP (topology classifier); edge-centric Graph Neural Network (forward performance predictor); gradient-based optimization over learned surrogate for inverse design (parameter inference).",
            "training_task": "Train forward surrogate to predict 16 performance metrics from netlist/parameters; train MLP to select topology from target performance; perform inverse design (parameter inference) via gradient-based optimization of the learned forward model under layout-aware loss.",
            "simulation_performance": "Dataset: &gt;1,000,000 Cadence-simulated datapoints. Forward GNN: average R^2 ≈ 0.972 and overall mean relative error ≈ 9.1% (paper reports 9.09% untrimmed; seed-averaged 9.14±0.38%). Stage 1 MLP topology selection accuracy ≈ 99.57%. Stage 3 inverse-design pipeline: success rate 78.5% (design considered successful if Cadence-simulated performance deviates &lt;20% from target), mean relative error of 17.7% across converged designs, typical inference time &lt;1 s per instance on a MacBook CPU.",
            "real_hardware_tested": false,
            "real_hardware_performance": null,
            "fidelity_comparison": false,
            "fidelity_comparison_details": null,
            "minimal_requirements_discussed": "The paper argues for foundry-grade (PDK-calibrated) Spectre simulations and layout-aware parasitic modeling to capture realistic mm-wave behavior; it explicitly notes that the differentiable analytical layout/parasitic models guide optimization but do not replace EM simulations or post-layout verification and recommends learned parasitic estimators and EM-informed models to further close post-layout gaps.",
            "transfer_failure_cases": "Generalization to an unseen topology (RVCO) in zero-shot produced mean relative error ≈ 30.4%; fine-tuning only the output head on ~30k RVCO samples reduced error to 0.9%, indicating a sim-to-model/topology transfer gap that is solvable with modest fine-tuning. The paper also cautions that analytical layout/parasitic approximations are not a full substitute for EM/post-layout verification and can limit final accuracy.",
            "domain_randomization_used": false,
            "physics_informed_approach": true,
            "frequency_range": "30 GHz (fixed operating frequency; mm-wave)",
            "electromagnetic_effects": true,
            "thermal_effects": false,
            "uuid": "e2140.0"
        },
        {
            "name_short": "SPICE / symbolic SPICE",
            "name_full": "SPICE or symbolic/simplified SPICE-style simulators (as referenced for prior works)",
            "brief_description": "General-purpose circuit simulators or simplified symbolic SPICE used in prior ML/benchmarking works; the paper contrasts SPICE-based or symbolic simulators with foundry-calibrated Cadence simulations, noting lower realism (missing PDK calibration and layout-dependent effects).",
            "citation_title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design",
            "mention_or_use": "mention",
            "circuit_type": "various (prior work benchmarks often used small/synthetic analog circuits or low-frequency designs)",
            "simulator_tool": "SPICE or symbolic/simplified SPICE (mentioned as used by prior works / benchmarks)",
            "component_models": "Typically SPICE-level device models or symbolic simplified models; in referenced prior work these are often not PDK-calibrated and may lack detailed foundry parasitics or layout coupling.",
            "parasitics_modeled": false,
            "nonlinearities_modeled": null,
            "tolerances_variations": "none (paper states many SPICE/synthetic benchmarks lack process/foundry realism and layout-dependent behavior)",
            "ml_model_type": "various in prior works (supervised learning, reinforcement learning, Bayesian optimization reported in related work summaries)",
            "training_task": "Referenced prior tasks include parameter sizing, topology generation, and performance prediction on synthetic/simplified benchmarks (not the main dataset of this paper).",
            "simulation_performance": null,
            "real_hardware_tested": null,
            "real_hardware_performance": null,
            "fidelity_comparison": true,
            "fidelity_comparison_details": "The paper contrasts SPICE/symbolic-simulator based datasets (e.g., AnalogGym, AutoCkt) as lacking process fidelity, noise characteristics, and layout-dependent behavior compared to their Cadence PDK-driven Spectre dataset; it argues that SPICE/symbolic datasets provide lower transfer realism.",
            "minimal_requirements_discussed": "Paper implies minimal requirement for practical ML-driven analog design is foundry-calibrated simulator data (Cadence + PDK) and layout-aware parasitic modeling, beyond symbolic SPICE.",
            "transfer_failure_cases": "The paper states SPICE/symbolic benchmarks can lead to unrealistic results and poor downstream transfer because they omit layout-dependent interactions and foundry calibration, but no specific quantitative sim-to-real failure cases are given in this paper.",
            "domain_randomization_used": null,
            "physics_informed_approach": null,
            "frequency_range": null,
            "electromagnetic_effects": null,
            "thermal_effects": null,
            "uuid": "e2140.1"
        },
        {
            "name_short": "ADS (mentioned)",
            "name_full": "Keysight Advanced Design System (ADS) (mentioned as a simulator used in some prior works)",
            "brief_description": "ADS is cited in the related-work comparison table as being used (alongside Cadence) by some prior RL/optimization studies; here it is mentioned only in passing and not used in the experiments.",
            "citation_title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design",
            "mention_or_use": "mention",
            "circuit_type": "prior works (likely RF/microwave designs referenced in related work)",
            "simulator_tool": "Keysight ADS (mentioned in Table 4 as used by prior works)",
            "component_models": "Not specified in this paper (mention only); ADS commonly provides EM/co-simulation features in prior literature but this paper does not report details.",
            "parasitics_modeled": null,
            "nonlinearities_modeled": null,
            "tolerances_variations": null,
            "ml_model_type": "various in cited prior work (the table links ADS use to some RL/optimization studies)",
            "training_task": null,
            "simulation_performance": null,
            "real_hardware_tested": null,
            "real_hardware_performance": null,
            "fidelity_comparison": null,
            "fidelity_comparison_details": null,
            "minimal_requirements_discussed": null,
            "transfer_failure_cases": null,
            "domain_randomization_used": null,
            "physics_informed_approach": null,
            "frequency_range": null,
            "electromagnetic_effects": null,
            "thermal_effects": null,
            "uuid": "e2140.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gcn-rl circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "An efficient bayesian optimization approach for automated optimization of analog circuits",
            "rating": 2
        },
        {
            "paper_title": "Domain knowledge-based automated analog circuit design with deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Align: A system for automating analog layout",
            "rating": 2
        },
        {
            "paper_title": "Layoutcopilot: An llm-powered multi-agent collaborative framework for interactive analog layout design",
            "rating": 2
        },
        {
            "paper_title": "AnalogGym: An open and practical testing suite for analog circuit synthesis",
            "rating": 2
        },
        {
            "paper_title": "AICircuit: A Multi-Level Dataset and Benchmark for AI-Driven Analog Integrated Circuit Design",
            "rating": 2
        }
    ],
    "cost": 0.016663499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design
27 Oct 2025</p>
<p>Asal Mehradfar mehradfa@usc.edu 
University of Southern California</p>
<p>Xuzhe Zhao 
University of California
Irvine</p>
<p>Yilun Huang 
University of California
Irvine</p>
<p>Emir Ceyani 
University of Southern California</p>
<p>Yankai Yang 
University of California
Irvine</p>
<p>Shihao Han 
University of California
Irvine</p>
<p>Hamidreza Aghasi 
University of California
Irvine</p>
<p>Salman Avestimehr 
University of Southern California</p>
<p>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design
27 Oct 2025A8EE4F73D6F8FF334969398D5DD59292arXiv:2505.21923v2[cs.LG]
Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility.We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization.Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics.Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model.This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules.We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies.Through this evaluation, FALCON demonstrates &gt;99% accuracy in topology inference, &lt;10% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance.Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.Our code and dataset are publicly available at https://github.com/AsalMehradfar/FALCON.</p>
<p>Introduction</p>
<p>Analog radio frequency (RF) and millimeter-wave (mm-wave) circuits are essential to modern electronics, powering critical applications in signal processing [1], wireless communication [2], sensing [3], radar [4], and wireless power transfer systems [5].Despite their importance, the design of analog circuits remains largely manual, iterative, and dependent on expert heuristics [6][7][8].This inefficiency stems from several challenges: a vast and continuous design space that is difficult to explore systematically; tightly coupled performance metrics (e.g.gain, noise, bandwidth, and power) that create complex trade-offs; and physical and layout-dependent interactions that complicate design decisions.As demand grows for customized, high-performance analog blocks, this slow, expert-driven design cycle has become a critical bottleneck.While machine learning (ML) has revolutionized digital design automation, analog and RF circuits still lack scalable frameworks for automating the full pipeline from specification to layout.</p>
<p>While recent ML approaches have made progress in analog circuit design, they typically target isolated sub-tasks such as topology generation or component sizing [9,10] at the schematic level, without addressing the full synthesis pipeline.Many efforts assume fixed topologies [11][12][13][14], limiting adaptability to new specifications or circuit families.Optimization strategies often rely on black-box methods that do not scale well to large, continuous design spaces [15].Some methods predict 39th Conference on Neural Information Processing Systems (NeurIPS 2025).performance metrics directly from netlists [16], but do not support inverse design, i.e., generating circuit parameters from target specifications.Furthermore, layout awareness is typically handled as a separate post-processing step [17], missing the opportunity to guide optimization with layout constraints.Finally, many available benchmarks are built on symbolic or synthetic simulations [18], lacking the fidelity and realism of the process of commercial grade design flows.As a result, current ML pipelines do not allow fully generalizable, layout-aware, and end-to-end analog circuit design.</p>
<p>We propose FALCON (Fully Automated Layout-Constrained analOg circuit desigN), a scalable and modular machine learning framework for end-to-end analog and RF circuit design.Built on a dataset of over one million Cadence-simulated circuits, FALCON comprises three core components (Figure 1): (1) a lightweight multilayer perceptron (MLP) selects the most appropriate topology given a target performance specification; (2) a generalizable graph neural network (GNN) maps circuit topology and element-level parameters to performance metrics, operating on a native graph representation derived from Cadence netlists; and (3) gradient-based optimization over the forward GNN model recovers design parameters that meet the target specification, guided by a differentiable layout-aware loss that encodes parasitic effects and physical constraints.Notably, the GNN model in FALCON generalizes effectively to unseen topologies, enabling inverse design across diverse circuit families, even in low-data regimes, with optional fine-tuning for improved accuracy.By integrating layout modeling directly into the optimization process, FALCON unifies schematic and physical considerations within a single differentiable learning framework.</p>
<p>Our main contributions are as follows:</p>
<p>• We construct a large-scale analog/RF circuit dataset comprising over one million Cadencesimulated datapoints across 20 expert-designed topologies and five circuit types.• We introduce a native netlist-to-graph representation that preserves both structural and parametric fidelity, enabling accurate learning over physical circuit topologies.• We design a generalizable GNN capable of accurate performance prediction and parameter inference across both seen and unseen topologies, with optional fine-tuning.• We develop a modular ML framework for end-to-end inverse design, incorporating performance-driven topology selection and layout-aware gradient-based optimization, with a differentiable loss that enforces area constraints, design-rule compliance, and frequencydependent modeling of passive components.• We show that FALCON enables fast, reliable inverse design under layout and physical constraints, generating high-quality circuits in under one second per instance on CPU.</p>
<p>Related Work</p>
<p>While recent ML-based approaches have advanced analog and RF circuit design, they typically target isolated stages of the design flow-such as topology generation, parameter sizing, or schematic-level performance prediction-without supporting unified, end-to-end synthesis.FALCON bridges this gap by jointly addressing aforementioned stages within a single framework.</p>
<p>Topology generation methods aim to select or synthesize candidate circuit structures [9,19,20], often using discrete optimization or generative models to explore the circuit graph space.However, these approaches typically target low-frequency or simplified designs [9] and may produce physically invalid or non-manufacturable topologies.In contrast, FALCON leverages a curated set of netlists, ensuring manufacturable validity and eliminating the need to rediscover fundamental circuit structures.</p>
<p>Parameter sizing and performance prediction have been explored through various learning paradigms.Reinforcement learning [10,21] and Bayesian optimization [15,22] optimize parameters via trial-and-error, often requiring large simulation budgets.Supervised learning methods [23,24,11] regress parameter values from performance targets under fixed topologies.Graph-based models [16] incorporate topology-aware representations to predict performance metrics from netlists.However, these approaches focus on forward prediction or black-box sizing and do not support inverse design across varied topologies.In contrast, FALCON unifies forward modeling and parameter inference in a single differentiable architecture that generalizes to unseen netlists.</p>
<p>Layout-aware sizing and parasitic modeling have been explored to mitigate schematic-to-layout mismatch.Parasitic-aware methods [25] integrate pre-trained parasitic estimators into Bayesian optimization loops for fixed schematics.While effective for estimation, these approaches rely on time-consuming black-box search and lack inverse design capabilities.Other methods, such as ALIGN [26] and LayoutCopilot [27], generate layouts from fully sized netlists using ML-based constraint extraction or scripted interactions, but assume fixed parameters and do not support cooptimization or differentiable inverse design.In contrast, FALCON embeds layout objectives directly into the learning loss, enabling joint optimization of sizing and layout without relying on external parasitic models.For mm-wave circuits, our layout-aware loss captures frequency-sensitive constraints via simplified models that implicitly reflect DRC rules, EM coupling, and performancecritical factors such as quality factor and self-resonance frequency.</p>
<p>Datasets for analog design are often limited to symbolic SPICE simulations or small-scale testbeds that do not reflect real-world design flows.AnalogGym [18] and AutoCkt [13] rely on synthetic circuits and symbolic simulators, lacking the process fidelity, noise characteristics, and layoutdependent behavior of foundry-calibrated flows.In contrast, FALCON is trained on a large-scale dataset constructed from over one million Cadence-simulated circuits across 20 topologies and five circuit categories, offering a substantially more realistic foundation for ML-driven analog design.</p>
<p>To the best of our knowledge, FALCON is the first framework to unify topology selection, parameter inference, and layout-aware optimization in a single end-to-end pipeline, validated at scale using industrial-grade Cadence simulations for mm-wave analog circuits.A qualitative comparison with representative prior work is provided in Appendix A.</p>
<p>3 A Large-Scale Dataset and Inverse Design Problem Formulation</p>
<p>Dataset Overview</p>
<p>We construct a large-scale dataset of analog and RF circuits simulated using industry-grade Cadence tools [28] with a 45nm CMOS process design kit (PDK).The dataset spans five widely used mm-wave circuit types for wireless applications [29,30]: low-noise amplifiers (LNAs) [31][32][33][34], mixers [35][36][37][38], power amplifiers (PAs) [39][40][41][42][43], voltage amplifiers (VAs) [44][45][46][47][48], and voltage-controlled oscillators (VCOs) [49][50][51][52][53].Each circuit type is instantiated in four distinct topologies, resulting in a total of 20 expert-designed architectures.</p>
<p>For each topology, expert-designed schematics were implemented in Cadence Virtuoso, and key design parameters were manually identified based on their functional relevance.Parameter ranges were specified by domain experts and systematically swept using Cadence ADE XL, enabling parallelized Spectre simulations across the design space.For each configuration, performance metrics-such as gain, bandwidth, and oscillation frequency-were extracted and recorded.Each datapoint therefore includes the full parameter vector, the corresponding Cadence netlist, and the simulated performance metrics.The resulting dataset comprises over one million datapoints, capturing a wide range of circuit behaviors and design trade-offs across diverse topologies.This large-scale, high-fidelity dataset forms the foundation for training and evaluating our inverse design pipeline.Detailed metric definitions and per-topology parameter ranges appear in Appendix B.</p>
<p>Graph-Based Circuit Representation</p>
<p>To enable flexible and topology-agnostic learning, we represent each analog circuit as a graph extracted from its corresponding Cadence netlist.Nodes correspond to voltage nets (i.e., electrical connection points), and edges represent circuit elements such as transistors, resistors, capacitors, or sources.Multi-terminal devices-such as transistors and baluns-are decomposed into multiple ).For transistors, labels such as GS, DS, and DG denote source-to-gate, drain-to-source, and drain-to-gate connections, respectively.These graphs serve as input to our GNN-based performance modeling and inverse design pipeline.</p>
<p>edges, and multiple components may connect the same node pair, resulting in heterogeneous, multi-edged graphs that preserve structural and functional diversity.</p>
<p>Recent works such as DICE [54] have explored transistor-level circuit-to-graph conversions for self-supervised learning, highlighting the challenges of faithfully capturing device structure and connectivity.In contrast, our approach maintains a native representation aligned with foundrycompatible netlists.Rather than flattening or reinterpreting device abstractions, we preserve symbolic parameters, multi-edge connections, and device-specific edge decomposition directly from the schematic source, enabling scalable learning across diverse analog circuit families.</p>
<p>To support learning over such structured graphs, each edge is annotated with a rich set of attributes: (i) a categorical device type, specifying the component and connected terminal pair (e.g., NMOS drain-gate, resistor); (ii) numeric attributes, such as channel length or port resistance, fixed by the schematic; (iii) parametric attributes, defined symbolically in the netlist (e.g., W1, R3) and resolved numerically during preprocessing; (iv) one-hot categorical features, such as source type (DC, AC, or none); and (v) computational attributes, such as diffusion areas (Ad, As) derived from sizing.This rule-based graph construction generalizes across circuit families without task-specific customization.Graphs in the FALCON dataset range from 4-40 nodes and 7-70 edges, reflecting the variability of practical analog designs.Figure 2 shows two representative graph examples from our dataset-IFVCO and ClassBPA.</p>
<p>Inverse Design Problem Definition</p>
<p>In analog and RF circuit design, the traditional modeling process involves selecting a topology T and parameter vector x, then evaluating circuit behavior via simulation to obtain performance metrics y = f (T, x).This forward workflow depends heavily on designer intuition, manual tuning, and exhaustive parameter sweeps.Engineers typically simulate many candidate (T, x) pairs and select the one that best satisfies the target specification-a slow, costly, and unguided process.</p>
<p>In contrast, our goal is to perform inverse design: given a target performance specification y target , we aim to directly infer a topology and parameter configuration (T, x) such that f (T, x) ≈ y target , without enumerating the full design space.This inverse problem is ill-posed and the search space is constrained by both device-level rules and layout-aware objectives.</p>
<p>Formally, the task is to find the optimal topology T * ∈ T and the optimal parameters x * ∈ R p such that f (T * , x * ) ≈ y target where f : T × R p → R d the true performance function implemented by expensive Cadence simulations.In practice, f is nonlinear and non-invertible, making direct inversion intractable.FALCON addresses this challenge through a modular, three-stage pipeline:</p>
<p>Stage 1: Topology Selection.We frame topology selection as a classification problem over a curated set of K candidate topologies {T 1 , . . ., T K }.Given a target specification y target , a lightweight MLP selects the topology T * ∈ T most likely to satisfy it, reducing the need for exhaustive search.</p>
<p>Stage 2: Performance Prediction.Given a topology T and parameter vector x, we train a GNN f θ to predict the corresponding performance ŷ = f θ (T, x).This model emulates the forward behavior of the simulator f , learning a continuous approximation of circuit performance across both seen and unseen topologies.By capturing the topology-conditioned mapping from parameters to performance, f θ serves as a differentiable surrogate that enables gradient-based inference in the next stage.</p>
<p>Stage 3: Layout-Aware Gradient Reasoning.Given y target and a selected topology T * , we infer a parameter vector x * by minimizing a loss over the learned forward model f θ .Specifically, we solve:
x * = arg min x L perf (f θ (T * , x), y target ) + λ L layout (x),(1)
where L perf measures prediction error, and L layout encodes differentiable layout-related constraints such as estimated area and soft design-rule penalties.Optimization is performed via gradient descent, allowing layout constraints to guide the search through a physically realistic parameter space.</p>
<p>4 Stage 1: Performance-Driven Topology Selection Task Setup.We formulate topology selection as a supervised classification task over a fixed library of 20 expert-designed circuit topologies T = {T 1 , T 2 , . . ., T 20 }.Rather than generating netlists from scratch-which often leads to invalid or impractical circuits-we select from a vetted set of designer-verified topologies.This ensures that all candidates are functionally correct, layout-feasible, and manufacturable.While expanding the topology set requires retraining, our lightweight MLP classifier enables rapid updates, making the approach scalable.This formulation also aligns with practical design workflows, where quickly identifying a viable initial topology is critical.</p>
<p>Each datapoint is represented by a 16-dimensional performance vector of key analog/RF metrics1 .We normalize features using z-scores computed from the training set.Missing metrics (e.g., oscillation frequency for amplifiers) are imputed with zeros, yielding zero-centered, fixed-length vectors that retain task-relevant variation.Dataset splits are stratified to preserve class balance across training, validation, and test sets.We assume each target vector is realizable by at least one topology in T , though the library can be extended with new designs.Model Architecture and Training.We train a 5-layer MLP with hidden size 256 and ReLU activations for this problem.The model takes the normalized performance vector y target ∈ R 16 as input and outputs a probability distribution over 20 candidate topologies.The predicted topology is selected as T * = arg max T k ∈T MLP(y target ) k .We train the model using a cross-entropy loss and the Adam optimizer [55], with a batch size of 256.An overview of this process is shown in Figure 3. 99.31 0.69 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.14 98.86 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.95 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 99.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 95.01 4.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 5.57 93.85 0.58 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.77 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 99.93   Evaluation.We begin by assessing the quality of the input representation used for topology classification.Normalized performance vectors encode rich semantic information about circuit behavior.
C G L N A C L N A C S L N A D L N A D B A M ix e</p>
<p>r D B P M ix e r S B A M ix e r S B P M ix e r C la s s B P A C la s s E P A D o h P A
D P A C G V A C S V A C V A S F V A IF V C O C C V C O C o lV C O R V C O 0 20
To validate this, we project them into a two-dimensional t-SNE space [56] (Figure 4(a)).The resulting clusters align closely with topology labels, indicating that performance specifications reflect underlying schematic structure and are effective inputs for supervised classification.</p>
<p>We assess classification performance using accuracy, balanced accuracy, macro precision, macro recall, macro F1, and micro F1 scores on the test set.As summarized in Table 1, the classifier achieves an overall accuracy of 99.57%, with macro F1 of 99.30% and balanced accuracy of 99.33%, demonstrating strong generalization across all 20 circuit topologies.Micro F1 (identical to accuracy in the multiclass setting) reaches 99.57%, while macro metrics-averaged equally across classes-highlight robustness to class imbalance.Seed-averaged results with 95% confidence intervals are provided in Appendix C.These trends are reinforced by the per-class accuracy plot in Figure 4(c), where most topologies reach 100% accuracy.</p>
<p>The confusion matrix in Figure 4(b) visualizes only the misclassified instances, as most classes achieve perfect accuracy.The few observed errors are primarily concentrated among the two voltage amplifier topologies-common-gate (CGVA) and common-source (CSVA).These circuits operate near the gain-bandwidth limit of the transistor, and when the main amplifier transistor size is held constant, performance metrics such as power consumption, gain, and bandwidth can converge across these architectures.This occasional overlap in the performance space introduces ambiguity in classification for a small subset of instances.For other circuit categories, no significant confusion is expected or observed.These results validate our hypothesis that performance vectors contain sufficient semantic structure for accurate, scalable topology classification.</p>
<p>Stage 2: Generalizable Forward Modeling for Performance Prediction</p>
<p>Task Setup.The goal of Stage 2 is to learn a differentiable approximation of the circuit simulator that maps a topology T and parameter vector x to a performance prediction ŷ = f θ (T, x), where ŷ ∈ R 16 .Unlike black-box simulators, this learned forward model enables efficient performance estimation and supports gradient-based parameter inference in Stage 3. The model is trained to generalize across circuit families and can be reused on unseen topologies with minimal fine-tuning.</p>
<p>Each datapoint consists of a graph-structured Cadence netlist annotated with resolved parameter values and the corresponding performance metrics.We frame the learning task as a supervised regression problem.Since not all performance metrics apply to every topology (e.g., oscillation frequency is undefined for amplifiers), we train the model using a masked mean squared error loss:
L masked = 1 i m i d i=1 m i • (ŷ i − y i ) 2 , (2)
where m i = 1 if the i-th metric is defined for the current sample, and 0 otherwise.</p>
<p>Model Architecture and Training.Each circuit is represented as an undirected multiedge graph with voltage nets as nodes and circuit components as edges.All circuit parameters-both fixed and sweepable-are assigned to edges, along with categorical device types and one-hot encoded indicators.For each edge e, these attributes are concatenated to form a unified feature vector x e .The feature set is consistent within each component type but varies across types (e.g., NMOS vs. inductor), reflecting the structure defined in Section 3.2.enc (x e ), where t e denotes the component type of edge e. Node features are initialized as scalar dummy values and mapped to a hidden dimension via a shared linear layer.We then apply a 4-layer edge-centric message-passing GNN with residual connections.At each layer ℓ, every node u receives messages from all incident edges e ∈ E u , where E u denotes the set of edges connected to u. Messages are computed using a shared function ϕ MSG , which takes the hidden state of the neighboring node src(e) and the associated edge embedding.Node states are then updated via a learnable function ϕ UPD , followed by a residual connection and nonlinearity:
m (ℓ) u = e∈Eu ϕ MSG h (ℓ) src(e) , z e , h (ℓ+1) u = ReLU ϕ UPD m (ℓ) u + h (ℓ) u
where z e ∈ R d is the learned embedding of edge e, and h
(ℓ)
src(e) ∈ R d is the hidden state of the node at the other end of edge e from u at layer ℓ.The update function ϕ UPD is a shared linear transformation applied to all nodes.After L = 4 GNN layers, node embeddings are aggregated using global mean pooling to obtain a fixed-size graph representation:
z graph = 1 |V | u∈V h (L) u ,
where V is the set of all nodes in the graph.The resulting vector is then passed through a fully connected MLP (output_mlp) to predict the 16-dimensional performance vector ŷ ∈ R 16 .An overview of this GNN-based forward prediction pipeline is shown in Figure 5.</p>
<p>To stabilize training, physical parameters are rescaled by their expected units (e.g.resistance by 10 3 ), and performance targets are normalized to z-scores using training statistics.We train the model using the Adam optimizer (learning rate 10 −3 , batch size 256) and a ReduceLROnPlateau scheduler.Xavier uniform initialization is used for all layers, and early stopping is based on validation loss.We adopt the same splits as in Section 4 for consistency in evaluation.</p>
<p>Evaluation.We evaluate the accuracy of the GNN forward model f θ on a test set drawn from 19 of the 20 topologies.One topology-RVCO-is entirely excluded from training, validation, and test splits to assess generalization to unseen architectures.Prediction quality is measured using standard regression metrics: coefficient of determination (R 2 ), root mean squared error (RMSE), and mean absolute error (MAE), computed independently for each of the 16 performance metrics.We also report the mean relative error per metric, computed as the average across all test samples where each metric is defined.As summarized in Table 2, the model achieves high accuracy across all dimensions, with an average R 2 of 0.972.To evaluate end-to-end prediction accuracy at the sample level, we compute the mean relative error per instance, defined as the average relative error across all valid (non-masked) performance metrics for each test sample.Figure 6 shows the distribution of this quantity across the test set (trimmed at the 95th percentile to reduce the impact of outliers).</p>
<p>The distribution is sharply concentrated, indicating that most predictions closely match their corresponding target vectors.Without percentile trimming, the overall mean relative error across the full test set is 9.09%.Seed-averaged results with 95% confidence intervals are provided in Appendix C. Generalizing to Unseen Topologies via Fine-Tuning.To assess the generalization ability of our pretrained GNN, we evaluate it on the held-out RVCO topology, which was entirely excluded from the Stage 2 training, validation, and test splits.Notably, the RVCO training partition used here matches that of the Stage 1 experiments (Section 4), enabling consistent cross-stage evaluation.</p>
<p>We fine-tune the GNN by freezing all encoder and message-passing layers and updating only the final output head (output_mlp).Fine-tuning is performed on the RVCO training set, which contains approximately 30,000 instances, and completes in under 30 minutes on a MacBook CPU.</p>
<p>Even in the zero-shot setting-where the model has never seen RVCO topologies-the pretrained GNN achieves a nontrivial mean relative error of 30.4%, highlighting its strong cross-topology generalization.Fine-tuning reduces this error to just 0.9%, demonstrating that the structural and parametric priors learned during pretraining are highly transferable.Table 3 reports detailed performance across five key metrics of RVCO, confirming that the pretrained GNN can be rapidly adapted to novel circuit families with minimal supervision.Task Setup.Given a target performance vector y target and a selected topology T * , the goal of Stage 3 is to recover a parameter vector x * that minimizes a total loss combining performance error and layout-aware penalties, using the learned forward model f θ from Stage 2. This formulation enables instance-wise inverse design without requiring circuit-level simulation.Loss Function.The total loss follows the structure defined in Eqn 1, jointly minimizing performance mismatch and layout cost:
L total = L perf + λ area • L layout • g(L perf ),(3)
where L perf is the masked mean squared error (see Eqn 2) between predicted and target performance vectors, and L layout is a normalized area penalty derived from analytical layout equations.To prioritize functionality, layout loss is softly gated by a sigmoid function:
g(L perf ) = 1 − σ (γ(L perf − τ )) ,
where σ(•) denotes the sigmoid function, and γ, τ are fixed scalars controlling the sharpness and center of the gating.This gating attenuates layout penalties when performance error exceeds a threshold τ , encouraging the model to first achieve functionality before optimizing for layout compactness.</p>
<p>We set τ = 0.05, γ = 50, and normalize layout area by 1 mm 2 to stabilize gradients.The layout weight λ area = 0.02 is chosen empirically to balance performance accuracy and physical realism without dominating the loss.This gated formulation supports manufacturable parameter recovery and reflects the broader paradigm of physics-informed learning [57].Further discussion on user-defined objectives is provided in Appendix D.</p>
<p>Differentiable Layout Modeling.In mm-wave analog design, layout is not a downstream concern but a critical determinant of circuit performance-particularly for passive components.Substrate coupling, proximity effects, and DRC-imposed geometries directly affect key metrics such as resonance frequency, quality factor, and impedance matching.To incorporate these effects, we introduce a differentiable layout model that computes total physical area analytically from circuit parameters.This enables layout constraints to directly guide parameter optimization during inverse design.By minimizing the layout area in distributed mm-wave circuits [58], unwanted signal loss [59] is reduced, the self-resonance frequency of passives can increase [60], and phase and amplitude mismatches across signal paths [61] can be reduced.</p>
<p>The layout model is deterministic and non-learned.It estimates area contributions from passive components-capacitors, inductors, and resistors-as these dominate total area and exhibit layoutsensitive behavior.Active devices (e.g., MOSFETs) are excluded since their geometries are fixed by the PDK and are negligible [62].For a given parameter vector x, the total layout loss is computed as:
L layout (x) = e∈Epassive A e (x)
, where E passive is the set of passive elements, and A e (x) is the area of the created layout for the passive component based on analytical physics-based equations.The area of element e is estimated based on its 2D dimensions (e.g., A = W • L for resistors and capacitors).This area is normalized and used as a differentiable penalty in the optimization objective (see Eqn 3).Further implementation details are provided in Appendix E.</p>
<p>Gradient Reasoning Procedure.Starting from the initialized parameter vector x 0 , we iteratively update parameters via gradient reasoning.At each step, the frozen forward model f θ predicts the performance ŷ = f θ (T, x), and the total loss L total is evaluated.Gradients are backpropagated with respect to x, and updates are applied using the Adam optimizer.Optimization proceeds for a fixed number of steps, with early stopping triggered if the loss fails to improve over a predefined window.To handle varying circuit difficulty and initialization quality, we employ an adaptive learning rate strategy.Each instance begins with a moderate learning rate (10 −6 ), refined during optimization via a ReduceLROnPlateau scheduler.If the solution fails to meet thresholds on performance error or layout area, optimization restarts with a more exploratory learning rate.This adjustment balances exploration and fine-tuning, enabling rapid convergence to physically valid solutions, typically within milliseconds to under one second per instance.An overview is shown in Figure 7.</p>
<p>Evaluation.We evaluate Stage 3 on 9,500 test instances (500 per topology) using our gradient-based optimization pipeline.A design is considered converged if it meets both: (i) a predicted mean relative error below 10%, and (ii) a layout area under a topology-specific bound-1 mm 2 for most circuits and 1.5 mm 2 for DLNA, DohPA, and ClassBPA.The 10% error threshold reflects the forward model's ∼ 9% average prediction error (Section 5).A design is deemed successful if its final Cadencesimulated performance deviates from the target by less than 20%, confirming real-world viability.</p>
<p>Our method achieves a success rate of 78.5% and a mean relative error of 17.7% across converged designs, with average inference time under 1 second on a MacBook CPU.Notably, success rate is coupled with the convergence threshold: tighter error bounds yield higher accuracy but require more iterations-critical for large-scale design tasks.</p>
<p>To illustrate the effectiveness of our pipeline, Figure 8 shows a representative result for the DohPA topology: the synthesized schematic is shown on the left, and the corresponding layout is on the right.These results confirm that the recovered parameters are both functionally accurate and physically realizable.Together, they demonstrate that FALCON enables layout-aware inverse design within a single differentiable pipeline-a capability not supported by existing analog design frameworks.</p>
<p>Conclusion and Future Work</p>
<p>We presented FALCON, a modular framework for end-to-end analog and RF circuit design that unifies topology selection, performance prediction, and layout-aware parameter optimization.Trained on over one million Cadence-simulated mm-wave circuits, FALCON combines a lightweight MLP, a generalizable GNN, and differentiable gradient reasoning to synthesize circuits from specification to layout-constrained parameters.FALCON achieves &gt;99% topology selection accuracy, &lt;10% prediction error, and efficient inverse design-all within sub-second inference.In addition, the GNN forward model generalizes to unseen topologies with minimal fine-tuning, supporting broad practical deployment.Further discussion of training and inference efficiency, as well as practical limitations, is provided in Appendix F.</p>
<p>In future work, we aim to expand the topology library and support hierarchical macroblocks for scalable design beyond the cell level.We also plan to extend the dataset to cover multiple operating frequencies, enabling validation across diverse bands, and to enhance the layout-aware optimization with learned parasitic models, EM-informed constraints, and electromigration considerations for more accurate post-layout estimation.Finally, integrating reinforcement learning or diffusion-based models for generative topology synthesis represents a promising step toward general-purpose analog design automation.</p>
<p>A Qualitative Comparison with Prior Work</p>
<p>To contextualize FALCON within the broader landscape of analog circuit design automation, we provide a qualitative comparison against representative prior works in Table 4.This comparison spans key capabilities including topology selection, parameter inference, performance prediction, layout awareness, and simulator fidelity.We additionally assess reproducibility via dataset and code availability, and introduce a new axis-RF/mm-wave support-to highlight methods evaluated on high-frequency circuit blocks such as LNAs, mixers, and VCOs.Compared to existing approaches, FALCON is the only framework that unifies all these dimensions while maintaining foundry-grade fidelity and open-source accessibility.Definitions for each comparison axis are provided in Table 5.
CktGNN [9] ✔ ✔ ✘ ✘ ✘ (SPICE) ✘ ✔ ✔ LaMAGIC [19] ✔ ✘ ✘ ✘ ✘ (SPICE) ✘ ✘ ✘ AnalogCoder [20] ✔ ✘ ✘ ✘ ✘ (SPICE) ✘ ✔ ✔ GCN-RL [10] ✘ ✔ ✘ ✘ ✔ (SPICE/Cadence) ✘ ✘ ✘ (incomplete) Cao et al. [21] ✘ ✔ ✘ ✘ ✔ (ADS/Cadence) ✘ ✘ ✘ BO-SPGP [15] ✘ ✔ ✔ ✘ ✔ (Cadence) ✘ ✘ ✘ ESSAB [22] ✘ ✔ ✔ ✘ ✔ (Cadence) ✘ ✘ ✘ AICircuit [23, 24] ✘ ✔ ✘ ✘ ✔ (Cadence) ✔ ✔ ✔ Krylov et al. [11] ✘ ✔ ✘ ✘ ✘ (SPICE) ✘ ✔ ✔ Deep-GEN [16] ✘ ✘ ✔ ✘ ✘ (SPICE) ✘ ✔ ✔ Liu et al. [25] ✘ ✘ ✘ ✔ ✘ (SPICE + Parasitic Model) ✔ ✘ ✘ ALIGN [26] ✘ ✘ ✘ ✔ ✔ (Cadence) ✔ ✔ ✔ LayoutCopilot [27] ✘ ✘ ✘ ✔ ✔ (Cadence) ✘ ✘ ✘ AnalogGym [18] ✘ ✔ ✘ ✘ ✘ (SPICE) ✘ ✔ ✔ AutoCkt [13] ✘ ✔ ✘ ✘ ✔ (Cadence) ✘ ✘ ✘ (incomplete) L2DC [12] ✘ ✔ ✘ ✘ ✘ (SPICE) ✘ ✘ ✘ CAN-RL [14] ✘ ✔ ✘ ✔ ✔ (Cadence) ✘ ✘ ✘ AnGeL. [17] ✔ ✔ ✔ ✘ ✘ (SPICE) ✘ ✘ ✘ FALCON (This work) ✔ ✔ ✔ ✔ ✔ (Cadence) ✔ ✔ ✔</p>
<p>B Dataset Details and Performance Metric Definitions</p>
<p>During dataset generation, each simulated circuit instance is annotated with a set of performance metrics that capture its functional characteristics.All simulations are performed at a fixed frequency of 30 GHz, ensuring consistency across circuit types and relevance to mm-wave design.A total of 16 metrics are defined across all circuits-spanning gain, efficiency, impedance matching, noise, and frequency-domain behavior-though the specific metrics used vary by topology.For example, phase noise is only applicable to oscillators.An overview of all performance metrics is provided in Table 6.</p>
<p>B.1 Low-Noise Amplifiers (LNAs)</p>
<p>Low-noise amplifiers (LNAs) are critical components in receiver front-ends, responsible for amplifying weak antenna signals while introducing minimal additional noise.Their performance directly influences downstream blocks such as mixers and analog-to-digital converters (ADCs), ultimately determining system-level fidelity [31].To capture the architectural diversity of practical radio-frequency (RF) designs, we include four widely used LNA topologies in this study-common-source LNA (CSLNA), common-gate LNA (CGLNA), cascode LNA (CLNA), and differential LNA (DLNA)-as shown in Figure 9.The CSLNA is valued for its simplicity and favorable gain-noise trade-off, especially when paired with inductive source degeneration [30].The CGLNA, often used in ultra-wideband systems, enables broadband input matching but typically suffers from a higher noise figure [32].The CLNA improves gain-bandwidth product and reverse isolation, making it ideal for high-frequency, high-linearity applications [33].The DLNA exploits circuit symmetry to enhance linearity and reject common-mode noise, and is commonly found in high-performance RF front-end designs [34].The design parameters and performance metrics associated with these topologies are summarized in Table 7.</p>
<p>B.2 Mixers</p>
<p>Mixers are fundamental nonlinear components in RF systems, responsible for frequency translation by combining two input signals to produce outputs at the sum and difference of their frequencies.This functionality is essential for transferring signals across frequency domains and is widely used in both transmission and reception paths [35].To capture diverse mixer architectures, we implement four representative topologies in this work-double-balanced active mixer (DBAMixer), double-balanced passive mixer (DBPMixer), single-balanced active mixer (SBAMixer), and single-balanced passive mixer (SBPMixer)-as shown in Figure 10.</p>
<p>The DBAMixer integrates amplification and differential switching to achieve conversion gain and high port-to-port isolation.Despite its elevated power consumption and design complexity, it is well suited for systems requiring robust performance over varying conditions [36].The DBPMixer features a fully differential structure that suppresses signal leakage and improves isolation, at the cost of signal loss and a strong local oscillator drive requirement [37].The SBAMixer includes an amplification stage preceding the switching core to enhance signal strength and reduce noise, offering a balanced performance trade-off with increased power consumption and limited spurious rejection [30].The SBPMixer employs a minimalist switching structure to perform frequency translation without active gain, enabling low power operation in applications with relaxed performance demands [38].The parameters and performance metrics for these mixer topologies are listed in Table 8.</p>
<p>B.3 Power Amplifiers (PAs)</p>
<p>Power amplifiers (PAs) are the most power-intensive components in radio-frequency (RF) systems and serve as the final interface between transceiver electronics and the antenna.Given their widespread use and the stringent demands of modern communication standards, PA design requires careful trade-offs across key performance metrics [39].Based on the transistor operating mode, PAs are typically grouped into several canonical classes [40].In this work, we implement four representative topologies-Class-B PA (ClassBPA), Class-E PA (ClassEPA), Doherty PA (DohPA), and differential PA (DPA)-as shown in Figure 11.</p>
<p>The ClassBPA employs complementary transistors to deliver high gain with moderate efficiency, making it suitable for linear amplification scenarios [41].The ClassEPA uses a single transistor configured as a switch, paired with a matching network.By minimizing the overlap between drain voltage and current, this topology enables high-efficiency operation and improved robustness to component variation [30].The DohPA combines main and peaking amplifiers using symmetric two-stack transistors, maintaining consistent gain and efficiency under varying power levels [42].</p>
<p>The DPA features a two-stage cascode structure designed to maximize gain and linearity, offering a favorable trade-off between output power and power consumption [43].For this topology, we replace the transformer with a T-equivalent network to simplify modeling and training of the graph neural network.Parameter sweeps and performance metrics for these PAs are listed in Table 9.The IFVCO employs an NMOS differential pair with an inductor-based feedback path to sustain oscillations.This topology provides favorable noise performance and compact layout, making it well suited for low-voltage, low-power designs [50].The CCVCO achieves negative resistance through cross-coupling, enabling low phase noise and high integration density, and is widely adopted in frequency synthesizers and PLLs [51].The ColVCO uses an LC tank and capacitive feedback to achieve high frequency stability and low phase noise, making it ideal for precision RF communication and instrumentation [52].The RVCO consists of cascaded delay stages forming a feedback loop, offering low power consumption, wide tuning range, and minimal area footprint, though at the cost of higher phase noise.It is commonly used in on-chip clock generation and low-power sensor applications [53].Design parameters and performance metrics for these VCO topologies are presented in Table 11.</p>
<p>C Robustness Across Random Seeds</p>
<p>To evaluate the robustness of our models to random initialization and data shuffling, we repeated experiments using five distinct random seeds: {42, 123, 777, 2023, 3407}.Reporting across multiple seeds is important for ensuring that observed results are not artifacts of a specific initialization or training trajectory, but rather reflect the stable behavior of the method.For each metric, we compute the mean and 95% confidence interval across seeds, reporting results in the form µ ± ∆.For the MLP topology selection model, results are highly stable across random seeds.The accuracy reaches 99.57± 0.01% with balanced accuracy at 99.34 ± 0.02%, while both macro and micro F1 scores exceed 99.3% with confidence intervals no larger than ±0.02.These narrow intervals indicate that the MLP's performance is effectively invariant to random initialization, underscoring its robustness and reliability in the topology selection stage of the pipeline (Section 4).</p>
<p>For the GNN-based forward performance prediction model, the overall mean relative error across all metrics is 9.14 ± 0.38% (95% CI).Individual performance predictions, including DC power consumption, gain, bandwidth, and oscillation frequency, exhibit narrow confidence intervals-for example, noise figure achieves 4.48 ± 0.07% error and oscillation frequency 0.65 ± 0.03%.These results indicate that the GNN achieves consistently accurate predictions across diverse circuit characteristics.The tight confidence intervals further demonstrate that the model's performance is robust to random initialization, underscoring its reliability as a generalizable forward predictor within the pipeline (Section 5).The full seed-dependent results for both models are provided in Tables 12 and 13.</p>
<p>D User-Defined Loss Functions for Gradient Reasoning</p>
<p>Stage 3 of FALCON employs gradient reasoning with the forward GNN fixed, enabling the optimization objective to be redefined without retraining or fine-tuning the predictive model.This design allows users to flexibly adapt the loss function to capture specific trade-offs or constraints.We illustrate this flexibility with two examples.</p>
<p>Weighted Performance Loss.Rather than treating all performance metrics equally, users can specify weights α i for each target metric:
L perf-weighted = 1 i m i α i d i=1 m i α i (ŷ i − y target i ) 2 ,
where larger α i prioritize certain specifications (e.g., gain or noise figure).Here, m i = 1 if the i-th metric is defined for the current sample, and 0 otherwise.</p>
<p>Interval-Constrained Performance Loss.Users may also define acceptable ranges for metrics rather than fixed targets.Given optional lower and/or upper bounds y lower i , y upper i , the interval penalty is:
L perf-interval = 1 i m i d i=1 m i 1 {y upper i defined} max(0, ŷi − y upper i ) + 1 {y lower i defined} max(0, y lower i − ŷi ) ,
where the indicator 1 {•} indicates whether the corresponding bound is specified.This formulation naturally handles the cases where only an upper bound, only a lower bound, or both bounds are provided.As above, m i = 1 if the i-th metric is defined for the current sample, and 0 otherwise.</p>
<p>General Extensibility.More generally, the total loss in Eqn. 3 can be replaced with any user-defined formulation, allowing both L perf and L layout to be substituted with customized objectives.Additional physical constraints, multi-objective trade-offs, or alternative layout penalties can be incorporated with only a few lines of code.This extensibility underscores the flexibility of FALCON and enables the framework to adapt to diverse design objectives.</p>
<p>E Layout</p>
<p>E.2 MIM Capacitor Capacitance Model</p>
<p>The total capacitance C N of a metal-insulator-metal (MIM) capacitor is modeled as:
C N = C a • L • W + C p • 2 • (L + W ) [fF]
where L and W are the layout length and width in µm, C a is the area capacitance density, and C p is the fringing field contribution per unit length.This model includes both area and perimeter contributions to more accurately reflect layout-dependent capacitance in IC design (see Figure 14(a)).</p>
<ol>
<li>Area Capacitance Term:
C a • L • W
Physical Concept: This term represents the primary (parallel-plate) capacitance formed between the overlapping top and bottom metal layers.It arises from the uniform electric field across the dielectric.</li>
</ol>
<p>Layer Physics Explanation:</p>
<p>• L • W corresponds to the overlap area of the plates.</p>
<p>• C a = 0.335 fF/µm 2 is the area capacitance density, derived from:</p>
<p>-Dielectric permittivity ε of the insulating material.</p>
<p>-Dielectric thickness d, with C ∝ ε/d.</p>
<ol>
<li>Perimeter (Fringing) Capacitance Term:
C p • 2 • (L + W )
Physical Concept: This term models fringing fields at the plate edges, contributing additional capacitance-particularly relevant in small geometries.</li>
</ol>
<p>Layer Physics Explanation:
• 2 • (L + W )
is the physical perimeter of the capacitor.</p>
<p>• C p = 0.11 fF/µm accounts for the fringing field contribution per unit length.</p>
<p>Summary: This composite model enables accurate estimation of MIM capacitance by capturing both parallel-plate and fringing effects.The constants C a and C p are typically calibrated using process-specific measurements or electromagnetic simulations.</p>
<p>For a fixed capacitor length L = 20 µm and width W ∈ [6.05, 150.0] µm, the layout-aware capacitance is approximated by: C ≈ 6.92
W + 4.4 <a href="4">fF</a>
The corresponding bounding area is estimated from the component's geometric envelope:
Bounding_Area = 22W + 44 <a href="5">µm 2 </a></p>
<p>E.3 N + Silicided Polysilicon Resistor Model</p>
<p>The resistance of a layout-defined resistor implemented using the ndslires layer is modeled as:
R = R s • L W + ∆W + 2R end + δ [Ω]
Physical Concept: This structure uses heavily doped N + polysilicon overlaid with a silicide layer to reduce resistance.Current flows laterally through the poly-silicide film (see Figure 14(b)), and resistance is shaped by the aspect ratio of the layout as well as process-dependent corrections.</p>
<p>Layer Physics Explanation:</p>
<p>• R s = 17.6 Ω/□ (ohm per square) is the sheet resistance of the silicided poly layer.</p>
<p>• W = 5.0 µm is the drawn width; ∆W = 0.048 µm accounts for process-induced width bias.• L is the drawn resistor length.</p>
<p>• R end = 1 Ω models terminal resistance due to contact diffusion and current crowding.</p>
<p>• δ = 0.917 Ω accounts for residual layout-dependent parasitics.</p>
<p>Summary:</p>
<p>The empirical layout relation used in parameterized generation is:
R ≈ 3.5007 • L + 2.917 <a href="6">Ω</a>
This model is valid for L ∈ [0.4,5.0] µm with fixed width W = 5.0 µm.The estimated layout area based on bounding box dimensions is:
Bounding_Area = 5.2L + 8.362 <a href="7">µm 2 </a></p>
<p>E.4 Octagon Spiral Inductor Model</p>
<p>Physical Concept: Accurate modeling and layout optimization of planar spiral inductors are critical in analog circuit design.Inductor performance is highly sensitive to parasitic elements, achievable quality factor (Q), and layout constraints imposed by process design rules.To support accurate performance prediction and inform layout choices, we adopt a modified power-law model that expresses inductance as a function of key geometric parameters.The model is validated against empirical measurements and shows strong agreement with classical analytical formulations.</p>
<p>Numerous classical formulations relate inductance to geometric factors such as the number of turns, average diameter, trace width, and inter-turn spacing.Among these, the compact closed-form expressions in RF Microelectronics textbook [30] are widely adopted for their balance of simplicity and accuracy.Building on this foundation, we adopt a reparameterized monomial model that better fits our empirical measurement data:
L = 2.454 × 10 −4 • D −1.21 out • W −0.163 • D 2.836 avg • S −0.049 [nH]
Layer Physics Explanation:
• D out = 2(R + W 2 ) is the outer diameter, • D in = 2(R − W 2 ) is the inner diameter, • D avg = (D out + D in )/2 = 2R is the aver- age diameter, • R is the radius in µm,
• W is the trace width in µm, • S is the spacing in µm.This expression is calibrated using measured data from a series of one-turn inductors fabricated with varying radius (R), while keeping the trace width fixed at W = 10 µm and spacing at S = 0.0 µm.Table 16 summarizes the measured inductance values used for model fitting.</p>
<p>Summary: With W and S fixed, inductance simplifies to:
L ≈ 2.337 × 10 −3 • R 1.164 <a href="8">nH</a>
The bounding area is estimated by:
Bounding_Area = 4R 2 + 108R + 440 <a href="9">µm 2 </a>
The performance of on-chip inductors is fundamentally influenced by layout-dependent factors such as trace width, metal thickness, and inter-turn spacing.Increasing the trace width (W ind ) reduces series resistance by enlarging the conductor's cross-sectional area, thereby improving the quality factor, Q = ωL/R series .However, wider traces also increase parasitic capacitance to adjacent turns and the substrate, which lowers the self-resonance frequency.</p>
<p>Metal thickness (H ind ) also plays a crucial role in minimizing ohmic losses.At high frequencies, current is confined near the conductor surface due to the skin effect.For copper at 25 GHz, the skin depth δ is approximately 0.41 µm; thus, using a metal layer thicker than 4δ (i.e., 1.6 µm) ensures efficient current flow.However, increasing thickness beyond this threshold yields diminishing returns in Q due to saturation in current penetration.</p>
<p>Turn-to-turn spacing (S) affects both inductance and quality factor (Q). Tighter spacing enhances magnetic coupling, thereby increasing inductance density.However, it also intensifies capacitive coupling and dielectric losses-particularly in modern CMOS processes with high-k inter-metal dielectrics-which can degrade Q.Conversely, excessive spacing reduces inductance without providing a proportionate benefit in loss reduction.As a result, one-turn spiral inductors are commonly favored in RF design due to their low series resistance, minimized parasitics, and improved modeling predictability.</p>
<p>These insights guided our design choices for layout-aware inductor implementation.To balance the competing demands of Q optimization, parasitic control, and DRC compliance, we implemented inductors using Metal 3 and set W = 10 µm as the default trace width.This width offers a lowresistance path that enhances Q while maintaining manageable parasitic capacitance and sufficient pitch for lithographic reliability.Metal 3 was selected for its favorable trade-off between thickness and routing density-it is thick enough to mitigate skin-effect losses at high frequencies while offering sufficient flexibility for compact layout integration.</p>
<p>The implemented spiral inductor geometry is shown in Figure 14(c).Table 17 summarizes the DRC-compliant tuning ranges, estimated layout areas, and decomposition strategies for single-cell passive components in our layout library.In the IFVCO example, the inductor labeled L 3 functions as an RF choke and is excluded from the on-chip layout due to its large area requirement.Instead, it is intended for off-chip implementation at the PCB level and connected to the die via wire bonding.This external connection is indicated by the yellow pad in Figure 16(b), which serves as the wire-bonding interface.</p>
<p>Since the current stage of system lacks automated routing, all interconnects in the layout were manually drawn to ensure accurate correspondence with the schematic connectivity.These examples demonstrate that synthesized circuit parameters can be successfully translated into DRC-compliant, physically realizable layouts, bridging the gap between high-level optimization and tapeout-ready design.</p>
<p>F Practical Considerations and Limitations</p>
<p>F.1 Training and Inference Efficiency</p>
<p>Although our codebase supports GPU acceleration, all experiments in this work-excluding initial dataset generation-were conducted entirely on a MacBook CPU.This highlights the efficiency and accessibility of the FALCON pipeline, which can be executed on modest hardware without specialized infrastructure.Our MLP and GNN models contain 207k and 1.4M trainable parameters, respectively, with memory footprints of just 831 KB and 5.6 MB.</p>
<p>In Stage 1, the MLP classifier trains in approximately 30 minutes with a batch size of 256 and performs inference in the order of milliseconds per batch.Stage 2's GNN model takes around 3 days to train on the full dataset using the same batch size and hardware.Fine-tuning on an unseen topology (e.g., RVCO) using ∼30,000 samples completes in under 30 minutes.</p>
<p>In Stage 3, the pretrained GNN is used without retraining to perform layout-constrained parameter inference via gradient-based optimization.Inference is conducted one instance at a time (batch size 1), with typical runtimes under 1 second per circuit.Runtime varies based on the convergence threshold and circuit complexity but remains below 2-3 seconds in the worst case across the full test set.</p>
<p>A solution is considered successful if the predicted performance meets the target within a specified relative error threshold.While tighter thresholds (e.g., 5%) improve accuracy, they require more optimization steps-particularly over large datasets.As a result, both success rate and inference time in Stage 3 are directly influenced by this tolerance, which can be tuned based on design fidelity requirements.</p>
<p>F.2 Limitations</p>
<p>This work focuses on a representative set of 20 curated analog topologies spanning five circuit families.Consequently, the topology selection stage is limited to suggesting only among the designs present in the training set and cannot synthesize novel circuits.A natural future direction is to either extend the training library to a broader set of topologies or replace the classifier with a generative model capable of directly proposing new netlists conditioned on input specifications.In contrast, the GNN-based forward modeling stage is designed to operate on arbitrary circuit graphs and has already demonstrated strong generalization to unseen architectures (e.g., RVCO), indicating that no modification to this stage is required to support novel circuits.</p>
<p>Beyond topology considerations, the dataset is constructed at a fixed operating frequency of 30 GHz, which ensures consistency across circuit families but constrains frequency generalization.Although the framework can, in principle, extend to other operating points-for example, the voltage amplifier topologies already demonstrate scalability across varying gain-bandwidth trade-offs-systematic validation across diverse frequency bands is beyond the scope of this work.Extending the dataset to cover multiple operating frequencies, or incorporating frequency as an explicit conditioning variable during training, represents an important direction for broadening applicability.</p>
<p>Finally, the differentiable layout model in FALCON captures parasitic effects through analytical approximations of passive components, which is effective for guiding parameter optimization within the learning framework.However, this approach does not fully replace electromagnetic (EM) simulations or post-layout verification, and electromigration constraints are not explicitly incorporated.Incorporating learned parasitic estimators, EM-informed models, and reliability constraints, therefore, remains an important extension toward bridging schematic-level optimization and silicon-proven robustness.In addition, all interconnect routing in the current flow is performed manually to ensure precise control over parasitic management and DRC compliance.While this provides accuracy for the studied designs, it limits scalability for more complex circuits, motivating future integration with automated analog routing tools.</p>
<p>Figure 1 :
1
Figure 1: Our AI-based circuit design pipeline.Given a target performance specification, FALCON first selects a suitable topology, then generates design parameters through layout-aware gradient-based reasoning with GNN model.Then, the synthesized circuit is validated using Cadence simulations.</p>
<p>Figure 2 :
2
Figure 2: Graph representations of two analog circuit topologies from our dataset: (a) IFVCO and (b) ClassBPA.Nodes represent electrical nets, and colored edges denote circuit components such as transistors, capacitors, inductors, and sources.Each component type is visually distinguished by color and labeled with its name and terminal role (e.g., N2_GS, V0).For transistors, labels such as GS, DS, and DG denote source-to-gate, drain-to-source, and drain-to-gate connections, respectively.These graphs serve as input to our GNN-based performance modeling and inverse design pipeline.</p>
<p>Figure 3 :
3
Figure 3: In Stage 1, an MLP classifier selects the most suitable circuit topology from a library of human-designed netlists, conditioned on the target performance specification.</p>
<p>(a) t-SNE of performance vectorsC G L N A D L N A D B P M ix e r S B P M ix e r C G V A C S V A C V A IF V C O R V</p>
<p>Confusion matrix (errors only)</p>
<p>Figure 4 :
4
Figure 4: Topology selection results.(a) Performance vectors form well-separated clusters in t-SNE space, showing that circuit functionality is semantically predictive of topology.(b) Misclassifications primarily occur among voltage amplifier variants with overlapping gain-bandwidth tradeoffs.(c) Per-class test accuracy exceeds 93% across all 20 circuit topologies. 2</p>
<p>Figure 5 :
5
Figure 5: In Stage 2, a custom edge-centric GNN maps an undirected multi-edge graph constructed from the circuit netlist to a performance vector.</p>
<p>Figure 6 :
6
Figure 6: Distribution of relative error (%) across the test set for the GNN forward model.Plot is trimmed at the 95th percentile.</p>
<p>Figure 7 :
7
Figure 7: In Stage 3, gradient reasoning iteratively updates parameters to minimize a loss combining performance error and layout cost, computed via a differentiable analytical model.</p>
<p>Figure 8 :
8
Figure 8: Stage 3 results for a synthesized DohPA.The schematic (a) reflects optimized parameters to meet the target specification.The layout (b) is DRC-compliant and physically realizable.The final design achieves a mean relative error of 5.4% compared to the target performance.</p>
<p>Table 6 :
6
Overview of 16 performance metrics used during dataset generation.Performance Name Description DC Power Consumption (DCP) Total power drawn from the DC supply indicating energy consumption of the circuit Voltage Gain (VGain) Ratio of output voltage amplitude to input voltage amplitude Power Gain (PGain) Ratio of output power to input power Conversion Gain (CGain) Ratio of output power at the desired frequency to input power at the original frequency S11 Input reflection coefficient indicating impedance matching at the input terminal S22 Output reflection coefficient indicating impedance matching at the output terminal Noise Figure (NF) Ratio of input signal-to-noise ratio to output signal-to-noise ratio Bandwidth (BW) Frequency span over which the circuit maintains specified performance characteristics Oscillation Frequency (OscF) Steady-state frequency at which the oscillator generates a periodic signal Tuning Range (TR) Range of achievable oscillation frequencies through variation of control voltages Output Power (OutP) Power delivered to the load P SAT Maximum output power level beyond which gain compression begins to occur Drain Efficiency (DE) Ratio of RF output power to DC power consumption.Power-Added Efficiency (PAE) Ratio of the difference between output power and input power to DC power consumption Phase Noise (PN) Measure of oscillator stability represented in the frequency domain at a specified offset Voltage Swing (VSwg) Maximum peak voltage level achievable at the output node</p>
<p>Figure 9 :
9
Figure 9: Schematic diagrams of the four LNA topologies.</p>
<p>Figure 10 :
10
Figure 10: Schematic diagrams of the four Mixer topologies.</p>
<p>Figure 11 :
11
Figure 11: Schematic diagrams of the four PA topologies.</p>
<p>Figure 13 :
13
Figure 13: Schematic diagrams of the four VCO topologies.</p>
<p>Figure 14 :
14
Figure 14: Layout views of passive components.(a) MIM capacitor with metal-insulator-metal stack.(b) Resistor layout with matching geometry.(c) Spiral inductor with octagonal turns for optimized area and Q-factor.</p>
<p>(a) Designed DBAMixer schematic (b) Layout of designed DBAMixer</p>
<p>Figure 15 :
15
Figure 15: Stage 3 results for a synthesized DBAMixer.The schematic (a) reflects optimized parameters to meet the target specification.The layout (b) is DRC-compliant and physically realizable.The final design achieves a mean relative error of 0.2% compared to the target performance.</p>
<p>Figure 16 :
16
Figure 16: Stage 3 results for a synthesized IFVCO.The schematic (a) reflects optimized parameters to meet the target specification.The layout (b) is DRC-compliant and physically realizable.The final design achieves a mean relative error of 1.3% compared to the target performance.</p>
<p>Figure 17 :
17
Figure 17: Stage 3 results for a synthesized DLNA.The schematic (a) reflects optimized parameters to meet the target specification.The layout (b) is DRC-compliant and physically realizable.The final design achieves a mean relative error of 5.0% compared to the target performance.</p>
<p>Table 1 : Classification performance on topology selection.
1MetricScore (%)Accuracy99.57Balanced Accuracy99.33Macro Precision99.27Macro Recall99.33Macro F199.30Micro F199.57</p>
<p>Table 2 :
2
Prediction accuracy of the forward GNN on all 16 circuit performance metrics.
MetricDCP VGain PGain CGainS11S22NFBWOscFTROutP PSATDEPAEPNVSwgUnitmWdBdBdBdBdBdBGHzGHzGHzdBmdBm%%dBc/HzmVR²1.01.00.991.00.931.00.990.980.970.830.971.01.01.00.891.0RMSE0.270.1010.5360.8331.5150.21 0.534 0.972 0.723 0.293 0.91 0.095 0.226 0.1432.5360.071MAE0.1980.0720.2080.1880.5540.120.20.376 0.184 0.097 0.238 0.066 0.163 0.1051.1590.046Rel. Err. 11.2%2.6%19.0%6.1%11.4% 1.9% 4.5% 6.5% 0.6% 6.5% 4.6% 4.4% 4.6% 11.0%1.3%1.4%</p>
<p>Table 3 :
3
Fine-tuning results on the held-out RVCO topology.Only the output head is updated using RVCO samples.
MetricDCPOscFTROutPPNUnitWGHzGHzdBmdBc/HzR²1.01.01.00.970.98RMSE0.6430.3240.0260.0990.953MAE0.5080.2560.020.0770.619Rel. Err. 0.75% 0.85% 1.63% 0.69% 0.73%</p>
<p>Table 4 :
4
Qualitative comparison of FALCON with prior works across key capabilities in analog circuit design automation.
MethodTopology SelectionParameter InferencePerformance PredictionLayout AwarenessFoundry GradeRF/ mm-WavePublic DatasetPublic Code</p>
<p>Table 5 :
5
Definitions of each comparison axis in Table4.
ColumnDefinitionTopology SelectionDoes the method automatically select or predict circuit topology given a target specification?Parameter InferenceDoes the method infer element-level parameters (e.g., transistor sizes, component values) as part of design generation?Performance Prediction Can the method predict circuit performance metrics (e.g., gain, bandwidth, noise) from topology and parameters?Layout AwarenessIs layout considered during optimization or training (e.g., via area constraints, parasitics, or layout-informed loss)?Dataset FidelityDoes the dataset reflect realistic circuit behavior (e.g., SPICE/Cadence simulations, PDK models)?RF/mm-WaveIs the method evaluated on at least one RF or mm-wave circuit type that reflects high-frequency design challenges?Public DatasetIs the dataset used in the work publicly released for reproducibility and benchmarking?Public CodeIs the implementation code publicly available and documented for reproducibility?</p>
<p>Table 7 :
7
LNA topologies with parameter sweep ranges, sample sizes, and performance metrics.
Dataset TypeTopology (Code)# of SamplesParameterSweep RangePerformance Metrics (Unit)C1[100-600] fFC2[50-300] fFCGLNA (0)52kCb Ld[250-750] fF [80-580] pHLs[0.5-5.5] nHWN[12-23] µmC1, C2[50-250] fFLd[140-300] pHCLNA (1)62kLg Ls WN1[0.4-2] nH [50-250] pH [3-5] µmDCP (W) PGain (dB)LNAWN2[7-9] µmS11 (dB)C[100-300] fFNF (dB)Lg[4-6] nHBW (Hz)CSLNA (2)39kLs[100-200] pHWN[2.5-4] µmVgs[0.5-0.9] VC1[100-190] fFC2[130-220] fFLd[100-250] pHDLNA (3)92kLg[600-900] pHLs[50-80] pHWN1[4-9.4] µmWN2[5-14] µm</p>
<p>Table 8 :
8
Mixer topologies with parameter sweep ranges, sample sizes, and performance metrics.
Dataset TypeTopology (Code)# of SamplesParameterSweep RangePerformance Metrics (Unit)C[1-10] pFDBAMixer (4)42kR WN1[1-10] kΩ [10-30] µmWN2[5-25] µmMixerDBPMixer (5)42kC R WN C[100-500] fF [100-600] Ω [10-30] µm [1-15] pFDCP (W) CGain (dB) NF (dB)R[0.7-2.1] kΩVSwg (V)SBAMixer (6)52kWN1[10-30] µmWN2[10-20] µmItail[3-10] mAC[1-30] pFSBPMixer (7)44kR[1-30] kΩWN[5-29.5] µm</p>
<p>Table 9 :
9
PA topologies with parameter sweep ranges, sample sizes, and performance metrics.
Dataset TypeTopology (Code)# of SamplesParameterSweep RangePerformance Metrics (Unit)C[55-205] fFL1[1-1.4] nHClassBPA (8)35kL2 R[1-8.5] pH [1.5-4] kΩWN[10-20] µmWP[3-8] µmC1[100-200] fFC2[500-700] fFClassEPA (9)46kL1 L2[100-300] pH [100-150] pHDCP (W)WN[15-30] µmPGain (dB)C1[2-3] pFS11 (dB)PAC2 C3, C5 C4[200-300] fF [100-200] fF [300-400] fFS22 (dB) PSAT (dBm)DohPA (10)120kL1, L5 L2 L3[100-200] pH [350-450] pH [500-600] pHDE (%) PAE (%)L4[150-250] pHL6[300-400] pHWN1, WN2[6-13] µmLip[100-500] pHLis[300-700] pHLop[0.8-1.2] nHDPA (11)80kLos[400-800] pHLm[50-250] pHWN1[6-31] µmWN2[10-35] µm</p>
<p>Table 11 :
11
VCO topologies with parameter sweep ranges, sample sizes, and performance metrics.
Dataset TypeTopology (Code)# of SamplesParameterSweep RangePerformance Metrics (Unit)C1[700-900] fFC2[50-250] fFIFVCO (16)43kL1[400-600] pHL2[500-700] pHWN, Wvar[5-9] µmCCVCO (17)54kL WN[200-400] pH [10-35] µmDCP (W)Wvar[5-30] µmOscF (Hz)VCOC[80-140] fFTR (Hz)L[250-350] pHOutP (dBm)ColVCO (18)90kWN Wvar[30-50] µm [5-15] µmPN (dBc/Hz)Vb[0.7-1.2] VItail[5-15] mAC[300-700] fFL1[300-500] pHRVCO (19)46kL2[50-250] pHWN[20-40] µmWvar[5-25] µm</p>
<p>Table 12 :
12
Topology selection performance with mean scores and 95% confidence intervals across five random seeds.
MetricMean ± 95% CI (%)Accuracy99.57 ± 0.01Balanced Accuracy99.34 ± 0.02Macro Precision99.27 ± 0.01Macro Recall99.34 ± 0.02Macro F199.30 ± 0.01Micro F199.57 ± 0.01</p>
<p>Table 13 :
13
Prediction accuracy of the forward GNN with mean scores and 95% confidence intervals across five random seeds.
MetricDCPVGain PGain CGainS11S22NFBWOscFTROutPPSATDEPAEPNVSwgRel. Err.11.643.1018.465.2511.491.944.486.280.656.554.864.314.5111.581.341.71± 95% CI (%)± 1.06± 0.42± 0.36± 0.44± 0.09± 0.15± 0.07± 0.43± 0.03± 0.04± 0.59± 0.24± 0.14± 1.72± 0.02± 0.29</p>
<p>Design and DRC Compliance E.1 Design Rule Enforcement in 45 nm CMOSWe implemented FALCON using a 45 nm CMOS technology node, applying rigorous Design Rule Checking (DRC) at both the cell and full-chip layout levels.At the cell level, our parameterized layout generators enforced foundry-specific constraints, including minimum feature width and</p>
<p>Table 16 :
16
Measured inductance for one-turn inductors with fixed W = 10 µm and S = 0.0 µm
R (µm)30405060L (nH) 0.123 0.170 0.220 0.276</p>
<p>Table 17 :
17
Single-cell passive component limits based on DRC and associated layout area costs.
Component Tunable VariableValue RangeArea RangeDecomposition RuleResistorLength L
4.32-20.42Ω 10.44-34.36µm 2 Series if &gt; max, parallel if &lt; min Capacitor Width W 46.32-1042.4fF 176-3344 µm 2 Parallel if &gt; max, series if &lt; min Inductor Radius R ≥ 0.1 nH ≥ 5640 µm 2 Continuous radius scaling E.5 Layout Examples of Synthesized Circuits To illustrate the correspondence between schematic and layout representations, we present three synthesized circuits: DBAMixer, IFVCO, and DLNA, shown in Figures 15, 16, and 17, respectively.</p>
<p>DC power consumption (DCP), voltage gain (VGain), power gain (PGain), conversion gain (CGain), S11, S22, noise figure (NF), bandwidth (BW), oscillation frequency (OscF), tuning range (TR), output power (OutP), PSAT, drain efficiency (DE), power-added efficiency (PAE), phase noise (PN), voltage swing (VSwg).
The 20 circuit topologies-listed in the same order as the numerical labels in Figure4(a)-are: CGLNA (Common Gate), CLNA (Cascode), CSLNA (Common Source), DLNA (Differential), DBAMixer (Double-Balanced Active), DBPMixer (Double-Balanced Passive), SBAMixer (Single-Balanced Active), SBPMixer (Single-Balanced Passive), ClassBPA (Class-B), ClassEPA (Class-E), DohPA (Doherty), DPA (Differential), CGVA (Common Gate), CSVA (Common Source), CVA (Cascode), SFVA (Source Follower), IFVCO (Inductive-Feedback), CCVCO (Cross-Coupled), ColVCO (Colpitts), RVCO (Ring).
Acknowledgments and Disclosure of FundingWe thank Andrea Villasenor and Tanqin He for their assistance with circuit data generation.We also thank Mohammad Shahab Sepehri for his insightful discussions and thoughtful feedback during the development of this work.B.4 Voltage Amplifiers (VAs)Voltage amplifiers (VAs) are fundamental components in analog circuit design, responsible for increasing signal amplitude while preserving waveform integrity.Effective VA design requires balancing key performance metrics tailored to both RF and baseband operating conditions[44].In this work, we implement four widely used VA topologies-common-source VA (CSVA), commongate VA (CGVA), cascode VA (CVA), and source follower VA (SFVA)-as shown in Figure12.The CSVA remains the most widely adopted configuration due to its structural simplicity and high voltage gain.It is frequently used as the first gain stage in various analog systems[45].The CGVA is suitable for applications requiring low input impedance and wide bandwidth, such as impedance transformation or broadband input matching[46].The CVA, which cascades a common-source stage with a common-gate transistor, improves the gain-bandwidth product and enhances stability, making it ideal for applications demanding wide dynamic range and robust gain control[47].The SFVA, also known as a common-drain amplifier, provides near-unity voltage gain and low output impedance, making it well suited for interstage buffering, load driving, and impedance bridging[48].Parameter ranges and performance specifications for these VA topologies are listed in Table10.Voltage-controlled oscillators (VCOs) are essential building blocks in analog and RF systems, responsible for generating periodic waveforms with frequencies modulated by a control voltage.These circuits rely on amplification, feedback, and resonance to sustain stable oscillations.Owing to their wide tuning range, low power consumption, and ease of integration, VCOs are broadly used in systems such as phase-locked loops (PLLs), frequency synthesizers, and clock recovery circuits[49].In this work, we implement four representative VCO topologies-inductive-feedback VCO (IFVCO), cross-coupled VCO (CCVCO), Colpitts VCO (ColVCO), and ring VCO (RVCO)-as shown in Figure13.length, contact and via spacing, and metal enclosure rules.At the circuit level, we incorporated physical verification to mitigate interconnect coupling, IR drop, and layout-dependent parasitic mismatches-factors that are especially critical in high-frequency and precision analog design.DRC plays a vital role in ensuring that layouts comply with process design rules defined by the semiconductor foundry.Adhering to these rules ensures not only physical manufacturability but also electrical reliability.Violations may lead to fabrication failures, including yield degradation, electrical shorts or opens, electromigration-induced issues, and parasitic mismatches.Moreover, DRC compliance is essential for compatibility with downstream fabrication steps such as photomask generation, optical lithography, and chemical-mechanical planarization (CMP), safeguarding the yield and fidelity of the final IC.Circuit-Level Layout Guidelines.We enforced several topology-aware layout constraints during full-circuit integration to preserve signal integrity and robustness:• Inductor-to-inductor spacing: ≥ 35.0 µm to mitigate mutual inductive coupling and magnetic interference.• Guardring placement: Sensitive analog blocks are enclosed by N-well or deep N-well guardrings with spacing ≥ 5.0 µm to suppress substrate noise coupling.• Differential pair symmetry: Differential signal paths are layout-matched to ensure ∆L &lt; 0.5 µm, minimizing mismatch and preserving phase balance.DRC Constraints and Layer Definitions.Table14summarizes the DRC constraints applied to key analog components across relevant process layers.Table15provides the abbreviations used for metal, contact, and via layers in the 45 nm CMOS process.
A fully differential analog front-end for signal processing from emg sensor in 28 nm fdsoi technology. Vilem Kledrowetz, Roman Prokop, Lukas Fujcik, Jiri Haze, Sensors. 2372023</p>
<p>The role of millimeter-wave technologies in 5g/6g wireless communications. Wei Hong, Zhi Hao Jiang, Chao Yu, Debin Hou, Haiming Wang, Chong Guo, Yun Hu, Le Kuai, Yingrui Yu, Zhengbo Jiang, Zhe Chen, Jixin Chen, Zhiqiang Yu, Jianfeng Zhai, Nianzu Zhang, Ling Tian, Fan Wu, Guangqi Yang, Zhang-Cheng Hao, Jian Yi Zhou, IEEE Journal of Microwaves. 112021</p>
<p>Analog front-end circuit design for wireless sensor system-on-chip. Yingying Chi, Haifeng Zhang, Zhe Zheng, Rui Liu, Lei Qiao, Wenpeng Cui, 2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC). 20201</p>
<p>A cmos 49-63-ghz phase-locked stepped-chirp fmcw radar transceiver. Xuyang Liu, Md Hedayatullah, Mahdi Maktoomi, Payam Alesheikh, Hamidreza Heydari, Aghasi, IEEE Journal of Solid-State Circuits. 2025</p>
<p>A compact 60-ghz wireless power transfer system. Med Nariman, Farid Shirinfar, Anna Papió Toda, Sudhakar Pamarti, Ahmadreza Rofougaran, Franco De, Flaviis , IEEE Transactions on Microwave Theory and Techniques. 6482016</p>
<p>. Phillip E , Allen , Douglas R Holberg, CMOS analog circuit design. 2011Elsevier</p>
<p>. M C Willy, Sansen, 2011analog design essentials. SpringerLink</p>
<p>A bayesian optimization framework for analog circuits optimization. Ahmed Shady A Abdelaal, Hassan Hussein, Mostafa, 2020 15th International Conference on Computer Engineering and Systems (ICCES). IEEE2020</p>
<p>CktGNN: Circuit graph neural network for electronic design automation. Zehao Dong, Weidong Cao, Muhan Zhang, Dacheng Tao, Yixin Chen, Xuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Gcn-rl circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning. Hanrui Wang, Kuan Wang, Jiacheng Yang, Linxiao Shen, Nan Sun, Hae-Seung Lee, Song Han, 2020 57th ACM/IEEE Design Automation Conference (DAC). 2020</p>
<p>Learning to design analog circuits to meet threshold specifications. Dmitrii Krylov, Pooya Khajeh, Junhan Ouyang, Thomas Reeves, Tongkai Liu, Hiba Ajmal, Hamidreza Aghasi, Roy Fox, ICML'23. JMLR.orgProceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>. Hanrui Wang, Jiacheng Yang, Hae-Seung Lee, Song Han, arXiv:1812.027342018Learning to design circuits. arXiv preprint</p>
<p>Autockt: deep reinforcement learning of analog circuit designs. Keertana Settaluri, EDA ConsortiumAmeer Haj-Ali, EDA ConsortiumQijing Huang, EDA ConsortiumKourosh Hakhamaneshi, EDA ConsortiumBorivoje Nikolic, EDA ConsortiumProceedings of the 23rd Conference on Design, Automation and Test in Europe, DATE '20. the 23rd Conference on Design, Automation and Test in Europe, DATE '20San Jose, CA, USA2020</p>
<p>A circuit attention network-based actor-critic learning approach to robust analog transistor sizing. Yaguang Li, Yishuang Lin, Meghna Madhusudan, Arvind Sharma, Sachin Sapatnekar, Ramesh Harjani, Jiang Hu, 2021 ACM/IEEE 3rd Workshop on Machine Learning for CAD (MLCAD). 2021</p>
<p>An efficient bayesian optimization approach for automated optimization of analog circuits. Wenlong Lyu, Pan Xue, Fan Yang, Changhao Yan, Zhiliang Hong, Xuan Zeng, Dian Zhou, IEEE Transactions on Circuits and Systems I: Regular Papers. 6562017</p>
<p>Pretraining graph neural networks for few-shot analog circuit modeling and design. Kourosh Hakhamaneshi, Marcel Nassar, Mariano Phielipp, Pieter Abbeel, Vladimir Stojanovic, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 4272022</p>
<p>Angel: Fully-automated analog circuit generator using a neural network assisted semi-supervised learning approach. Morteza Fayazi, Morteza Tavakoli Taba, Ehsan Afshari, Ronald Dreslinski, IEEE Transactions on Circuits and Systems I: Regular Papers. 2023</p>
<p>Analoggym: An open and practical testing suite for analog circuit synthesis. Jintao Li, Haochang Zhi, Ruiyu Lyu, Wangzhen Li, Zhaori Bi, Keren Zhu, Yanhan Zeng, Weiwei Shan, Changhao Yan, Fan Yang, Yun Li, Xuan Zeng, International Conference on Computer Aided Design. 2024</p>
<p>Lamagic: Language-model-based topology generation for analog integrated circuits. Chen-Chia Chang, Yikang Shen, Shaoze Fan, Jing Li, Shun Zhang, Ningyuan Cao, Yiran Chen, Xin Zhang, arXiv:2407.182692024arXiv preprint</p>
<p>Analogcoder: Analog circuit design via training-free code generation. Yao Lai, Sungyoung Lee, Guojin Chen, Souradip Poddar, Mengkang Hu, David Z Pan, Ping Luo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Domain knowledge-based automated analog circuit design with deep reinforcement learning. Weidong Cao, Mouhacine Benosman, Xuan Zhang, Rui Ma, arXiv:2202.131852022arXiv preprint</p>
<p>An efficient analog circuit sizing method based on machine learning assisted global optimization. Ahmet Faruk Budak, Miguel Gandara, Wei Shi, David Z Pan, Nan Sun, Bo Liu, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 4152022</p>
<p>AICircuit: A Multi-Level Dataset and Benchmark for AI-Driven Analog Integrated Circuit Design. Asal Mehradfar, Xuzhe Zhao, Yue Niu, Sara Babakniya, Mahdi Alesheikh, Hamidreza Aghasi, Salman Avestimehr, Machine Learning and the Physical Sciences Workshop @ NeurIPS. 2024</p>
<p>Asal Mehradfar, Xuzhe Zhao, Yue Niu, Sara Babakniya, Mahdi Alesheikh, Hamidreza Aghasi, Salman Avestimehr, arXiv:2501.11839Supervised learning for analog and rf circuit design: Benchmarks and comparative insights. 2025arXiv preprint</p>
<p>Parasitic-aware analog circuit sizing with graph neural networks and bayesian optimization. Mingjie Liu, Walker J Turner, George F Kokai, Brucek Khailany, David Z Pan, Haoxing Ren, 2021 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). 2021</p>
<p>Align: A system for automating analog layout. Tonmoy Dhar, Kishor Kunal, Yaguang Li, Meghna Madhusudan, Jitesh Poojary, Arvind K Sharma, Wenbin Xu, Steven M Burns, Ramesh Harjani, Jiang Hu, IEEE Design &amp; Test. 3822020</p>
<p>Layoutcopilot: An llm-powered multi-agent collaborative framework for interactive analog layout design. Bingyang Liu, Haoyi Zhang, Xiaohan Gao, Zichen Kong, Xiyuan Tang, Yibo Lin, Runsheng Wang, Ru Huang, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 2025</p>
<p>Cadence design environment. Antonio J Lopez Martin, 200235New Mexico State University</p>
<p>High-frequency integrated circuits. Sorin Voinigescu, 2013Cambridge University Press</p>
<p>RF microelectronics. Behzad Razavi, 2012Prentice hall New York2</p>
<p>The Design of CMOS Radio-Frequency Integrated Circuits. Thomas H Lee, 2004Cambridge University Press2nd edition</p>
<p>The design of low-noise cmos rf amplifiers. John R Long, Michael A Copeland, IEEE Journal of Solid-State Circuits. 3221997</p>
<p>Niknejad. mm-Wave Silicon Technology: 60 GHz and Beyond. M Ali, 2008Springer</p>
<p>A noise reduction and linearity improvement technique for a differential cascode lna. Xiaohua Fan, Heng Zhang, Edgar Sánchez-Sinencio, IEEE Journal of Solid-State Circuits. 4332008</p>
<p>B Henderson, E Camargo, Microwave Mixer Technology and Applications. Microwave &amp; RF. Artech House. 2013</p>
<p>A precise four-quadrant multiplier with subnanosecond response. B Gilbert, IEEE Journal of Solid-State Circuits. 341968</p>
<p>Fundamental performance limits and scaling of a cmos passive double-balanced mixer. Krenar Komoni, Sameer Sonkusale, Geoff Dawe, 2008 Joint 6th International IEEE Northeast Workshop on Circuits and Systems and TAISA Conference. 2008</p>
<p>Noise in passive fet mixers: a simple physical model. S Chehrazi, IEEE CatR Bagheri, IEEE CatA A Abidi, IEEE CatProceedings of the IEEE 2004 Custom Integrated Circuits Conference. the IEEE 2004 Custom Integrated Circuits Conference2004</p>
<p>Millimeter-wave power amplifier integrated circuits for high dynamic range signals. Hua Wang, Peter M Asbeck, Christian Fager, IEEE Journal of Microwaves. 112021</p>
<p>RF Power Amplifiers. M K Kazimierczuk, 2014Wiley</p>
<p>Power amplifiers and transmitters for rf and microwave. F H Raab, P Asbeck, S Cripps, P B Kenington, Z B Popovic, N Pothecary, J F Sevic, N O Sokal, IEEE Transactions on Microwave Theory and Techniques. 5032002</p>
<p>28 ghz doherty power amplifier in cmos soi with 28. Narek Rostomyan, Mustafa Özen, Peter Asbeck, IEEE Microwave and Wireless Components Letters. 2852018</p>
<p>A broadband differential cascode power amplifier in 45 nm cmos for high-speed 60 ghz system-onchip. Morteza Abbasi, Torgil Kjellberg, Anton De Graauw, Edwin Van Der Heijden, Raf Roovers, Herbert Zirath, 2010 IEEE Radio Frequency Integrated Circuits Symposium. 2010</p>
<p>Design of Analog CMOS Integrated Circuits. Behzad Razavi, 2016McGraw-Hill Education</p>
<p>Low-voltage analog circuit design based on biased inverting opamp configuration. S Karthikeyan, S Mortezapour, A Tammineedi, E K F Lee, IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing. 200047</p>
<p>A common-gate amplifier with transconductance nonlinearity cancellation and its highfrequency analysis using the volterra series. Tae Wook, Kim , IEEE Transactions on Microwave Theory and Techniques. 5762009</p>
<p>1-v power supply cmos cascode amplifier. T Lehmann, M Cassia, IEEE Journal of Solid-State Circuits. 3672001</p>
<p>An mos four-quadrant analog multiplier using simple two-input squaring circuits with source followers. H.-J Song, C.-K Kim, IEEE Journal of Solid-State Circuits. 2531990</p>
<p>A comparative study of ring vco and lc-vco: Design, performance analysis, and future trends. N R Sivaraaj, K K Abdul Majeed, IEEE Access. 112023</p>
<p>A current-reused vco with inductive-transformer feedback technique. Cao Wan, Taotao Xu, Xiang Yi, Quan Xue, IEEE Transactions on Microwave Theory and Techniques. 7052022</p>
<p>A 5ghz band low noise and wide tuning range si-cmos vco. Thanh Tuan, Suguru Ta, Tadashi Kameda, Kazuo Takagi, Tsubouchi, 2009 IEEE Radio Frequency Integrated Circuits Symposium. 2009</p>
<p>A noise-shifting differential colpitts vco. R Aparicio, A Hajimiri, IEEE Journal of Solid-State Circuits. 37122002</p>
<p>Analysis and design of current starved ring vco. K G Shruti Suman, P K Sharma, Ghosh, 2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT). 2016</p>
<p>Self-supervised graph contrastive pretraining for device-level integrated circuits. Sungyoung Lee, Ziyi Wang, Seunggeun Kim, Taekyun Lee, David Z Pan, arXiv:2502.089492025arXiv preprint</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Maziar Raissi, Paris Perdikaris, George Em Karniadakis, arXiv:1711.10561Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. 2017arXiv preprint</p>
<p>Efficient passive circuit models for distributed networks with frequency-dependent parameters. A Dounavis, R Achar, M S Nakhla, IEEE Transactions on Advanced Packaging. 2332000</p>
<p>Reduction of highfrequency conduction losses using a planar litz structure. M A Shen Wang, W G De Rooij, J D Odendaal, D Van Wyk, Boroyevich, IEEE Transactions on Power Electronics. 2022005</p>
<p>Miniaturized 6-bit phase-change capacitor bank with improved self-resonance frequency and q. Tejinder Singh, Raafat R Mansour, 2022 52nd European Microwave Conference (EuMC). IEEE2022</p>
<p>A 37-40-ghz low-phase-imbalance cmos attenuator with tail-capacitor compensation technique. Chenxi Zhao, Xing Zeng, Lin Zhang, Huihua Liu, Yiming Yu, Yunqiu Wu, Kai Kang, IEEE Transactions on Circuits and Systems I: Regular Papers. 67102020</p>
<p>Millimeter-wave integrated silicon devices: Active versus passive-the eternal struggle between good and evil. Michele Spasaro, Domenico Zito, 2019 International Semiconductor Conference (CAS). IEEE2019</p>            </div>
        </div>

    </div>
</body>
</html>
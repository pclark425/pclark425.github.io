<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5950 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5950</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5950</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-6d408c2c56a73cec8b21c450b6588c6d98026bc3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6d408c2c56a73cec8b21c450b6588c6d98026bc3" target="_blank">GeoGalactica: A Scientific Large Language Model in Geoscience</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work tries to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning the resulting model with the authors' custom collected instruction tuning dataset.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP). Due to their impressive abilities, LLMs have shed light on potential inter-discipline applications to foster scientific discoveries of a specific domain by using artificial intelligence (AI for science, AI4S). In the meantime, utilizing NLP techniques in geoscience research and practice is wide and convoluted, contributing from knowledge extraction and document classification to question answering and knowledge discovery. In this work, we take the initial step to leverage LLM for science, through a rather straightforward approach. We try to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset. These efforts result in a model GeoGalactica consisting of 30 billion parameters. To our best knowledge, it is the largest language model for the geoscience domain. More specifically, GeoGalactica is from further pre-training of Galactica. We train GeoGalactica over a geoscience-related text corpus containing 65 billion tokens, preserving as the largest geoscience-specific text corpus. Then we fine-tune the model with 1 million pairs of instruction-tuning data consisting of questions that demand professional geoscience knowledge to answer. In this technical report, we will illustrate in detail all aspects of GeoGalactica, including data collection, data cleaning, base model selection, pre-training, SFT, and evaluation. We open-source our data curation tools and the checkpoints of GeoGalactica during the first 3/4 of pre-training.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5950.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5950.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoGalactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoGalactica (further-pretrained Galactica-30B for Geoscience)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 30B-parameter large language model produced by further pre-training Galactica-30B on a 65B-token geoscience corpus (GeoCorpus) and supervised fine-tuning with a geoscience instruction dataset (GeoSignal v2); designed to support geoscience knowledge extraction, QA, summarization, and tool usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GeoGalactica (Galactica-30B further pre-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>30-billion parameter transformer LLM initialized from Galactica-30B, further pre-trained on GeoCorpus (~52.7B geoscience tokens within a 78B-token total corpus) and SFT'd on ~100k GeoSignal v2 instruction pairs; trained with Megatron-LM style 3D parallelism and optimized hyperparameters for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Enable geoscience knowledge extraction, question answering, summarization, and to serve as a foundation for downstream knowledge discovery (potentially including distilling generalizable domain rules), though the paper does not report explicit automated distillation of qualitative scientific laws.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Geoscience (geology, paleontology, mineralogy, geophysics, sedimentology, tectonics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Further domain-specific pre-training on a large, curated geoscience corpus; supervised fine-tuning with structured instruction data (GeoSignal v2) including seq2seq pairs, caption-to-content, entity/relation extraction examples; self-instruct seed generation using ChatGPT; tool-learning via ToolBench-like API-call examples; retrieval/knowledge-graph style inputs are prepared as structured tokens (special tokens for figures/tables/formula). No dedicated pipeline for automatic law/principle distillation is reported (methods that could support it are discussed, e.g., knowledge-graph integration and seq2seq extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not reported (the paper does not present distilled qualitative laws or generalizable scientific principles produced by the model). The paper frames possible outputs as factual knowledge extraction, summarization, entity/relation triples, and domain conceptual descriptions (e.g., taxonomy, synonyms, factual properties) which could serve as building blocks for deriving rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Model capabilities were evaluated using GeoBench (task-specific geoscience benchmark), MMLU, and multi-annotator human evaluation (expert scoring on noun definitions, beginner/intermediate/advanced Q&A, research paper titling, and geoscience research functionality). There is no metric reported for evaluating distilled qualitative laws because none were explicitly produced.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper demonstrates that GeoGalactica improves on geoscience-specific benchmarks (GeoBench) and human-evaluated geoscience QA/summarization tasks compared to several baselines; however, it does not report experiments where the LLM distilled generalizable qualitative laws or scientific principles from large collections of papers. The model is positioned as enabling knowledge extraction and downstream discovery workflows rather than delivering automated law distillation in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop for dataset curation and evaluation: domain experts curated the GeoCorpus and GeoSignal datasets, validated self-instruct generations, and performed detailed human evaluations/scoring. No automated pipeline for independent law-of-science discovery without expert curation is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>GeoCorpus (≈5.98M geoscience papers; ~52.7B tokens within a 78.85B-token total corpus including ArXiv and code data) and GeoSignal v2 (≈100k supervised instruction samples aggregated from Deep Literature, GAKG, GSO, DataExpo, Wikipedia, domain open-data sources, and ChatGPT self-instruct seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The authors explicitly note limitations relevant to any attempt to distill laws: (1) risk of hallucinations and biased or unreliable web-available training data; (2) overfitting and catastrophic forgetting during further pre-training; (3) training instability at large scale; (4) coverage gaps (academic papers bias model toward advanced knowledge, possibly reducing basic knowledge coverage); (5) no explicit method or evaluation for deducing generalizable qualitative laws from aggregate literature is provided; (6) parsing/structuring errors from PDF-to-text extraction can limit downstream discovery quality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>No concrete examples of distilled qualitative laws are reported. The paper cites related work where knowledge graphs have been used to discover domain phenomena (e.g., discovering ore-forming environments via a geological knowledge graph), and it demonstrates tasks such as converting figure/table captions to content, entity/relation extraction, summarization, and research-paper-style generation — all of which could support manual or semi-automated derivation of generalizable rules but are not presented as fully automated law-distillation outputs in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGalactica: A Scientific Large Language Model in Geoscience', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5950.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5950.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K2 (earlier geoscience LLM based on LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior effort to adapt a 7B-parameter LLaMA model to geoscience via further pre-training on geoscience literature and instruction-tuning (GeoSignal), cited by the GeoGalactica paper as a predecessor demonstrating the feasibility of domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMA-7B (adapted as K2)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>7-billion parameter transformer LLM (LLaMA) further fine-tuned on geoscience corpora in previous work (K2); cited as limited by model size/data scale but informative for recipe design.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Domain adaptation for geoscience tasks (improving geoscience QA and knowledge-intensive instruction following). Not reported to perform explicit automated distillation of qualitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Geoscience</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Further pre-training and knowledge-intensive instruction fine-tuning on geoscience literature (similar recipe to GeoGalactica but at smaller scale).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>GeoBench and task-specific evaluations in the prior K2 work (as cited); not described in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported as promising but constrained by model size and data scale; used here as motivation for scaling to GeoGalactica. No examples of synthesized general rules/laws are described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Domain experts involved in dataset creation and evaluation in the cited K2 work (per the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Geoscience open-access papers and Wikipedia used in K2 (cited), smaller-scale compared to GeoCorpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited model size and insufficient data scale to capture complex geoscience terminology/relations; thus limited in capacity to support large-scale law/principle extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Cited as demonstrating feasibility for domain-specialized LLMs; no direct examples of law/principle distillation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGalactica: A Scientific Large Language Model in Geoscience', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5950.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5950.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAKG (Geoscience Academic Knowledge Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal geoscience academic knowledge graph framework that fuses paper figures, tables, text, and bibliometric data to enable structured knowledge extraction and discovery (cited by GeoGalactica).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Structured knowledge extraction from geoscience literature to support reasoning and discovery (e.g., discovering ore-forming environments), which could complement LLM-driven extraction approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Geoscience</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Multimodal extraction (text, figures, tables) and knowledge-graph construction (entity/relation extraction, caption-content alignment). The paper references GAKG as a source of supervised signals for instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Example application cited in related work: use of knowledge graph techniques to discover ore-forming environments (a domain-level pattern discovery), though this is a knowledge-graph discovery use-case rather than an LLM-distilled law reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as a supporting resource for constructing supervision signals (e.g., caption-to-content pairs, relation examples) that could be used alongside LLMs to surface domain regularities; no direct LLM-based law distillation experiments are described.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Used as curated source data by domain experts; used to build supervision pairs for SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Multimodal extractions from geoscience papers (captions, tables, images, references) assembled into a knowledge-graph-style dataset (GAKG).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The paper notes general limitations when using structured resources: noise in figure/table referring-sentence matching, difficulties disambiguating references across papers, and possible discarding of low-quality matches — all of which limit downstream automated discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Cited example (from related work) — constructing a geological knowledge graph to discover ore-forming environments; used in GeoSignal construction as supervision but not demonstrated in this paper as an automated rule-distillation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGalactica: A Scientific Large Language Model in Geoscience', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>K2 <em>(Rating: 2)</em></li>
                <li>Galactica <em>(Rating: 1)</em></li>
                <li>GAKG <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5950",
    "paper_id": "paper-6d408c2c56a73cec8b21c450b6588c6d98026bc3",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "GeoGalactica",
            "name_full": "GeoGalactica (further-pretrained Galactica-30B for Geoscience)",
            "brief_description": "A 30B-parameter large language model produced by further pre-training Galactica-30B on a 65B-token geoscience corpus (GeoCorpus) and supervised fine-tuning with a geoscience instruction dataset (GeoSignal v2); designed to support geoscience knowledge extraction, QA, summarization, and tool usage.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "llm_model_name": "GeoGalactica (Galactica-30B further pre-trained)",
            "llm_model_description": "30-billion parameter transformer LLM initialized from Galactica-30B, further pre-trained on GeoCorpus (~52.7B geoscience tokens within a 78B-token total corpus) and SFT'd on ~100k GeoSignal v2 instruction pairs; trained with Megatron-LM style 3D parallelism and optimized hyperparameters for stability.",
            "task_goal": "Enable geoscience knowledge extraction, question answering, summarization, and to serve as a foundation for downstream knowledge discovery (potentially including distilling generalizable domain rules), though the paper does not report explicit automated distillation of qualitative scientific laws.",
            "domain": "Geoscience (geology, paleontology, mineralogy, geophysics, sedimentology, tectonics, etc.)",
            "methodology": "Further domain-specific pre-training on a large, curated geoscience corpus; supervised fine-tuning with structured instruction data (GeoSignal v2) including seq2seq pairs, caption-to-content, entity/relation extraction examples; self-instruct seed generation using ChatGPT; tool-learning via ToolBench-like API-call examples; retrieval/knowledge-graph style inputs are prepared as structured tokens (special tokens for figures/tables/formula). No dedicated pipeline for automatic law/principle distillation is reported (methods that could support it are discussed, e.g., knowledge-graph integration and seq2seq extraction).",
            "type_of_qualitative_law": "Not reported (the paper does not present distilled qualitative laws or generalizable scientific principles produced by the model). The paper frames possible outputs as factual knowledge extraction, summarization, entity/relation triples, and domain conceptual descriptions (e.g., taxonomy, synonyms, factual properties) which could serve as building blocks for deriving rules.",
            "evaluation_metrics": "Model capabilities were evaluated using GeoBench (task-specific geoscience benchmark), MMLU, and multi-annotator human evaluation (expert scoring on noun definitions, beginner/intermediate/advanced Q&A, research paper titling, and geoscience research functionality). There is no metric reported for evaluating distilled qualitative laws because none were explicitly produced.",
            "results_summary": "The paper demonstrates that GeoGalactica improves on geoscience-specific benchmarks (GeoBench) and human-evaluated geoscience QA/summarization tasks compared to several baselines; however, it does not report experiments where the LLM distilled generalizable qualitative laws or scientific principles from large collections of papers. The model is positioned as enabling knowledge extraction and downstream discovery workflows rather than delivering automated law distillation in this work.",
            "human_involvement": "Human-in-the-loop for dataset curation and evaluation: domain experts curated the GeoCorpus and GeoSignal datasets, validated self-instruct generations, and performed detailed human evaluations/scoring. No automated pipeline for independent law-of-science discovery without expert curation is reported.",
            "dataset_or_corpus": "GeoCorpus (≈5.98M geoscience papers; ~52.7B tokens within a 78.85B-token total corpus including ArXiv and code data) and GeoSignal v2 (≈100k supervised instruction samples aggregated from Deep Literature, GAKG, GSO, DataExpo, Wikipedia, domain open-data sources, and ChatGPT self-instruct seeds).",
            "limitations_or_challenges": "The authors explicitly note limitations relevant to any attempt to distill laws: (1) risk of hallucinations and biased or unreliable web-available training data; (2) overfitting and catastrophic forgetting during further pre-training; (3) training instability at large scale; (4) coverage gaps (academic papers bias model toward advanced knowledge, possibly reducing basic knowledge coverage); (5) no explicit method or evaluation for deducing generalizable qualitative laws from aggregate literature is provided; (6) parsing/structuring errors from PDF-to-text extraction can limit downstream discovery quality.",
            "notable_examples": "No concrete examples of distilled qualitative laws are reported. The paper cites related work where knowledge graphs have been used to discover domain phenomena (e.g., discovering ore-forming environments via a geological knowledge graph), and it demonstrates tasks such as converting figure/table captions to content, entity/relation extraction, summarization, and research-paper-style generation — all of which could support manual or semi-automated derivation of generalizable rules but are not presented as fully automated law-distillation outputs in this work.",
            "uuid": "e5950.0",
            "source_info": {
                "paper_title": "GeoGalactica: A Scientific Large Language Model in Geoscience",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "K2",
            "name_full": "K2 (earlier geoscience LLM based on LLaMA-7B)",
            "brief_description": "A prior effort to adapt a 7B-parameter LLaMA model to geoscience via further pre-training on geoscience literature and instruction-tuning (GeoSignal), cited by the GeoGalactica paper as a predecessor demonstrating the feasibility of domain adaptation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "LLaMA-7B (adapted as K2)",
            "llm_model_description": "7-billion parameter transformer LLM (LLaMA) further fine-tuned on geoscience corpora in previous work (K2); cited as limited by model size/data scale but informative for recipe design.",
            "task_goal": "Domain adaptation for geoscience tasks (improving geoscience QA and knowledge-intensive instruction following). Not reported to perform explicit automated distillation of qualitative laws.",
            "domain": "Geoscience",
            "methodology": "Further pre-training and knowledge-intensive instruction fine-tuning on geoscience literature (similar recipe to GeoGalactica but at smaller scale).",
            "type_of_qualitative_law": "Not reported.",
            "evaluation_metrics": "GeoBench and task-specific evaluations in the prior K2 work (as cited); not described in detail in this paper.",
            "results_summary": "Reported as promising but constrained by model size and data scale; used here as motivation for scaling to GeoGalactica. No examples of synthesized general rules/laws are described in this paper.",
            "human_involvement": "Domain experts involved in dataset creation and evaluation in the cited K2 work (per the paper).",
            "dataset_or_corpus": "Geoscience open-access papers and Wikipedia used in K2 (cited), smaller-scale compared to GeoCorpus.",
            "limitations_or_challenges": "Limited model size and insufficient data scale to capture complex geoscience terminology/relations; thus limited in capacity to support large-scale law/principle extraction.",
            "notable_examples": "Cited as demonstrating feasibility for domain-specialized LLMs; no direct examples of law/principle distillation reported.",
            "uuid": "e5950.1",
            "source_info": {
                "paper_title": "GeoGalactica: A Scientific Large Language Model in Geoscience",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GAKG",
            "name_full": "GAKG (Geoscience Academic Knowledge Graph)",
            "brief_description": "A multimodal geoscience academic knowledge graph framework that fuses paper figures, tables, text, and bibliometric data to enable structured knowledge extraction and discovery (cited by GeoGalactica).",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": null,
            "llm_model_description": null,
            "task_goal": "Structured knowledge extraction from geoscience literature to support reasoning and discovery (e.g., discovering ore-forming environments), which could complement LLM-driven extraction approaches.",
            "domain": "Geoscience",
            "methodology": "Multimodal extraction (text, figures, tables) and knowledge-graph construction (entity/relation extraction, caption-content alignment). The paper references GAKG as a source of supervised signals for instruction tuning.",
            "type_of_qualitative_law": "Example application cited in related work: use of knowledge graph techniques to discover ore-forming environments (a domain-level pattern discovery), though this is a knowledge-graph discovery use-case rather than an LLM-distilled law reported in this paper.",
            "evaluation_metrics": null,
            "results_summary": "Mentioned as a supporting resource for constructing supervision signals (e.g., caption-to-content pairs, relation examples) that could be used alongside LLMs to surface domain regularities; no direct LLM-based law distillation experiments are described.",
            "human_involvement": "Used as curated source data by domain experts; used to build supervision pairs for SFT.",
            "dataset_or_corpus": "Multimodal extractions from geoscience papers (captions, tables, images, references) assembled into a knowledge-graph-style dataset (GAKG).",
            "limitations_or_challenges": "The paper notes general limitations when using structured resources: noise in figure/table referring-sentence matching, difficulties disambiguating references across papers, and possible discarding of low-quality matches — all of which limit downstream automated discovery.",
            "notable_examples": "Cited example (from related work) — constructing a geological knowledge graph to discover ore-forming environments; used in GeoSignal construction as supervision but not demonstrated in this paper as an automated rule-distillation pipeline.",
            "uuid": "e5950.2",
            "source_info": {
                "paper_title": "GeoGalactica: A Scientific Large Language Model in Geoscience",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "K2",
            "rating": 2
        },
        {
            "paper_title": "Galactica",
            "rating": 1
        },
        {
            "paper_title": "GAKG",
            "rating": 2
        }
    ],
    "cost": 0.013783,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Ge@galactica: A Scientific Large Language Model IN GEOSCIENCE</h1>
<p>Zhouhan Lin ${ }^{1, *}$, Cheng Deng ${ }^{1}$, Le Zhou ${ }^{1}$, Tianhang Zhang ${ }^{1}$, Yi Xu ${ }^{1}$, Yutong Xu ${ }^{1}$, Zhongmou He ${ }^{1,2}$, Yuanyuan Shi ${ }^{1}$, Beiya Dai ${ }^{1}$, Yunchong Song ${ }^{1}$, Boyi Zeng ${ }^{1}$, Qiyuan Chen ${ }^{1}$, Yuxun Miao ${ }^{1}$, Bo Xue ${ }^{1}$, Shu Wang ${ }^{3}$, Luoyi Fu ${ }^{1}$, Weinan Zhang ${ }^{1}$, Junxian He ${ }^{4}$, Yunqiang Zhu ${ }^{3}$, Xinbing Wang ${ }^{1}$, Chenghu Zhou ${ }^{1,5}$<br>${ }^{1}$ Shanghai Jiao Tong University ${ }^{2}$ University of Michigan<br>${ }^{3}$ Institute of Geographical Science and Natural Resources Research, CAS<br>${ }^{4}$ The Hong Kong University of Science and Technology<br>lin.zhouhan@gmail.com, davendw@sjtu.edu.cn, junxianh@cse.ust.hk, zhouch@1reis.ac.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP). Due to their impressive abilities, LLMs have shed light on potential inter-discipline applications to foster scientific discoveries of a specific domain by using artificial intelligence (AI for science, AI4S). In the meantime, utilizing NLP techniques in geoscience research and practice is wide and convoluted, contributing from knowledge extraction and document classification to question answering and knowledge discovery. In this work, we take the initial step to leverage LLM for science, through a rather straightforward approach. We try to specialize an open-sourced LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset. These efforts result in a model GeoGalactica consisting of $\mathbf{3 0}$ billion parameters. To our best knowledge, it is the largest language model for the geoscience domain. More specifically, GEOGalactica is from further pre-training of Galactica - a top-performing LLM trained with a large number of scientific documents. We train GEOGalactica over a geoscience-related scientific text corpus containing 65 billion tokens, preserving as the largest geoscience-specific text corpus. Then we fine-tune the model with 1 million pairs of instruction-tuning data consisting of questions that demand professional geoscience knowledge to answer. We validate GEOGalactica on various geoscience examinations and geoscience-related open-domain questions evaluated by a group of senior geoscientists. GEOGalactica demonstrates the state-of-the-art performance in a diverse range of NLP tasks in geoscience, as well as revealing the potential of using geoscience-related tools. In this technical report, we will illustrate in detail all aspects of GEOGalactica, including data collection, data cleaning, base model selection, pre-training, SFT, and evaluation. We open-source our data curation tools and the checkpoints of GEOGalactica during the first $3 / 4$ of pre-training in https://github.com/geobrain-ai/geogalactica ${ }^{5}$.</p>
<p>Keywords Geoscience Language Model $\cdot$ Generative AI $\cdot$ Academic Language Model</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Contents</p>
<p>1 Introduction 5</p>
<p>2 Related Work 7
2.1 Machine Learning in Geoscience 7
2.2 Natural Language Processing in Geoscience 7
2.3 Domain-specific Large Language Model 7</p>
<p>3 Preliminary and Vocabulary 8</p>
<p>4 Data Collection and Cleaning 9
4.1 The Customized Pre-training dataset: GeoCoprus 9
4.2 The Customized SFT dataset: GeoSignal Version 2 11
4.2.1 Domain General Natural Language Instruction 12
4.2.2 Restructured Knowledge-intensive Instruction 13
4.2.3 Self-Instruct 14</p>
<p>5 Training 14
5.1 Further Pre-training 14
5.2 Supervised Fine-Tuning 17
5.3 Tool Learning 18</p>
<p>6 Evaluation 20
6.1 Automatic Evaluation 20
6.1.1 GeoBench 20
6.1.2 MMLU 20
6.2 Human Evaluation 21
6.2.1 Noun Definition 23
6.2.2 Beginner Level Q&amp;A 23
6.2.3 Intermediate Level Q&amp;A 24
6.2.4 Advanced Level Q&amp;A 24
6.2.5 Knowledge-based associative judgment question 24
6.2.6 Research Paper Titling Task 25
6.2.7 Geoscience Research Functionality 25</p>
<p>7 discussion 26
7.1 The Necessity of Pre-training 26
7.2 The Necessity of Further Pre-training 27
7.3 Carbon Emissions 27
7.4 Towards Unified Foundation Model in Geoscience 27</p>
<p>8 Conclusion 28</p>
<p>2</p>
<p>A Appendix: Progression of geoscience with AI ..... 33
B Appendix: GeoCorpus ..... 33
C Appendix: GeoSignal V2 Curation ..... 34
C. 1 MinDat ..... 34
C. 2 USGS ..... 36
C. 3 NGDB ..... 37
C. 4 Fossil Ontology ..... 39
C. 5 Fossil calibrations ..... 39
D Appendix: Prompts ..... 41
E Appendix: Training setup ..... 46
F Appendix: Model Card ..... 47
G Appendix: Evaluation ..... 48
G. 1 Open-ended Tasks ..... 48
G.1.1 Noun Definition ..... 48
G.1.2 Beginner Level Q\&amp;A ..... 48
G.1.3 Intermediate Level Q\&amp;A ..... 49
G.1.4 Advanced Level Q\&amp;A ..... 49
G. 2 Functional Tasks ..... 49
G.2.1 Knowledge-based associative judgment question. ..... 49
G.2.2 Research Paper Proposition Task. ..... 50
G.2.3 Geoscience Research Functionality ..... 50
H Generation Examples ..... 51
H. 1 Noun Definition ..... 51
H. 2 Beginner Level Q\&amp;A ..... 51
H. 3 Intermediate Level Q\&amp;A ..... 52
H. 4 Advanced Level Q\&amp;A ..... 53
H. 5 Knowledge-based associative judgment question ..... 53
H. 6 Research Paper Titling Task ..... 54
H. 7 Geoscience Research Functionality ..... 54
I Appendix: Tool Learning Use cases ..... 56
J Appendix: GeoGalactica Generation ..... 58
J. 1 Example Research Papers Written by GeoGalactica ..... 58
J. 2 Example Opinions Written by GeoGalactica ..... 59
J. 3 Example Summary of Scientific Articles Written by GeoGalactica ..... 60</p>
<p>K Appendix: Lessons and Progresses ..... 61
K. 1 Phase 1: Prepare for Training on HPC ..... 61
K. 2 Phase 2: Training on HPC ..... 62
K. 3 Summary ..... 65
L Membership and Contributions ..... 67
L. 1 Data preparation ..... 67
L. 2 Model Training ..... 67
L. 3 Model Evaluation and Application ..... 67
L. 4 Manuscript Writing ..... 67
L. 5 Project Management ..... 67
L. 6 Evaluation Team ..... 68
L. 7 Illustration in Arts ..... 68
L. 8 HPC Sponsor ..... 68
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of the processing, construction, components, and applications of GEOGalactica.</p>
<h1>1 Introduction</h1>
<p>The rapid advancement of Large Language Models (LLMs) has ushered in a transformative era in natural language processing (NLP), where these models have exhibited remarkable capabilities across a wide spectrum of tasks and domains. These advanced AI models have demonstrated their prowess in handling diverse natural language tasks, including reading comprehension, open-ended question answering, code generation, etc. Their ability to harness vast amounts of general knowledge and apply it to solve specific challenges has sparked interest in exploring their potential applications in various scientific disciplines. In this context, the intersection of artificial intelligence (AI) and science, often referred to as AI for Science (AI4S), has emerged as a promising frontier for driving scientific discoveries and innovations.</p>
<p>Within the realm of AI4S, one particularly intriguing avenue is the integration of NLP techniques into geoscience research and practice. Geoscience is a comprehensive discipline encompassing fields such as geophysics, geology, meteorology, environmental science, etc., with a primary focus on unraveling the complexities of natural processes and phenomena on Earth. Traditionally, geoscientists have relied on theoretical and empirical approaches to advance their understanding of the Earth's systems. However, the sheer volume of data generated in contemporary geoscience research necessitates new strategies and tools for knowledge discovery. The integration of computer science methodologies and AI technologies into geoscience has thus emerged as a transformative paradigm, offering the potential to accelerate scientific progress and address pressing global challenges effectively. In an era characterized by global challenges such as climate change and natural disaster mitigation, the need for efficient data acquisition, information sharing, and knowledge dissemination in geoscience has never been more critical.</p>
<p>In the field of geoscience, domain-specific geoscientific knowledge is usually presented in various forms of text data, such as scientific literature, textbooks, patents, industry standards, etc., which traditionally require the utilization of knowledge systems [1], knowledge graphs[2], or semantic models [3] to extract a structured form of these knowledge. More broadly, applying NLP techniques for geoscience use cases has been widely accepted [4], ranging from less complex tasks such as document classification [5], topic modeling [6], and entity recognition[7, 8], to more complex tasks such as knowledge graph construction [9], question answering [10] and summarization [11].
While general domain LLMs like Galactica [12], LLaMA [13], and GLM [14] have achieved impressive performance across various NLP tasks, they lack the domain-specific knowledge required for geoscience applications. These models have been trained on general datasets that lack authoritative geoscience-related data, limiting their adequacy in addressing the unique challenges posed by the geoscience domain. Although our recent attempt to adapt the LLaMA-7B model for geoscience using geoscience-specific data, i.e. the K2[15] model, has shown promising results, this primitive attempt is constrained by its model size and data scale, which consequently may not fully capture the complexity of geoscientific terminology and concepts. However, training a larger LLM comes with new technical challenges, since many aspects of the process become fundamentally different as the model scales up. For example, the stability of training will become more vulnerable, and the training data needs to be scaled up accordingly, resulting in a more systematic way of managing different data sources, etc.
Therefore, tailoring a general, larger LLM for the scientific domain of geoscience with a more systematically designed dataset and training pipeline is imperative in this era of LLMs. In response to these necessities, this work presents a significant step forward in the development of the model as well as the set of toolchains around it.
Leveraging the vast amount of resources of scientific literature's meta-data, particularly the data resources collected for the OpenAlex ${ }^{1}$, Web of Science ${ }^{2}$, Semantic Scholar ${ }^{3}$, and Acemap ${ }^{4}$, we can create, organize, and manage a large and comprehensive geoscience dataset targeted for all stages in large language model training. In particular, we have introduced GAKG [2], Deep Literature ${ }^{5}$, GSO $^{6}$, and other platforms as carriers and repositories of geoscience text knowledge. These concerted efforts have not only allowed us to accumulate a comprehensive geoscience data archive but also have served as foundations for constructing an extensive instruction-tuning dataset for geoscience-related questions, GeoSignal-v2, which has been employed in supervised fine-tuning (SFT). In addition, we have developed and customized a series of data-cleaning tools that allow us to automatically convert various forms of raw data, such as PDF files, forms, equations, knowledge graphs, etc., into clean texts suited as training corpus for large language models. To our best knowledge, our collected corpus has become the largest geoscience dataset.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>we have then successfully further pre-trained a language model with 30B parameters, with Galactica-30B [12] as its base model. The resulting model is thus named as GEOGalactica, empowering various academic tasks in the geoscience field. With its 30 billion parameters, this model represents the culmination of further pre-training and supervised fine-tuning, making it the largest language model dedicated to the geoscience domain. Our experimental findings demonstrate that, compared to models of equivalent scale, GEOGalactica exhibits exceptional performance on GeoBenchmark [15]. Regarding human evaluation, our model showcases impressive competence in geoscience-related tasks when compared with 5 general language models (ChatGPT ${ }^{7}$, Yiyan $^{8}$, Qianwen ${ }^{9}$, MOSS $^{10}$, ChatGLM ${ }^{11}$ ).
Moreover, since our GEOGalactica model provides a unified representation space and computational approach for diverse geological data described in various textual formats, it holds tremendous potential in narrowing the technological gap between different earth science tasks.
In the subsequent sections of this technical report, we will provide a detailed description of the data collection and cleaning processes, base model selection, pre-training, supervised fine-tuning, and extensive evaluations in the creation of GeoGalactica. Additionally, we are committed to promoting open science by making our data curation tools and pre-training checkpoints available to the research community through our GitHub repositories ${ }^{12}$.</p>
<h1>Broad Contribution</h1>
<p>In addition to establishing the academic mega-model in geoscience, our goal is to contribute to a broader research community. Specifically, the experiences documented in this paper provide evidence for further community understanding of several open questions in the literature. Warning: The model in this manuscript might produce hallucinations and reader discretion is recommended.</p>
<ol>
<li>A Domain-specific LLM: Our construction of GEOGalactica, following in the footsteps of our previous work K2 [15], represents a geoscience LLM that focuses on interacting with humans and generating contents on highly professional academic topics.</li>
<li>A Toolchain for Data Cleaning: A high-quality training dataset is crucial for successfully training large language models. Therefore, our contribution to the community includes developing an efficient academic data preprocessing toolchain to construct a clean training corpus from PDF documents ${ }^{13}$</li>
<li>Primitive Explorations to Use Tools: As for training GEOGalactica to use tools, we also construct a set of supervised data Geotools for training GEOGalactica to use tools. We also open-source the codes and data on Github. ${ }^{14}$</li>
<li>Training Details and pre-training Checkpoints: We conducted model training on the accelerator hardware provided by the Advanced Computing East China Sub-center. We will describe in detail the pre-training and SFT processes in the remainder of this paper. In addition, we are releasing the training checkpoints during the first $3 / 4$ of the pre-training process on Hugging Face. ${ }^{15}$</li>
<li>Model and data analysis process: In building a domain-specific LLM, the model and the data should be effectively evaluated and analyzed. We provide a set of analysis and visualization methods for the SFT data and the weights of the GEOGalactica, open-sourced on Github. ${ }^{16}$</li>
</ol>
<p>In summary, we aim to contribute to the research community by developing the GEOGalactica model and providing insights and tools related to data construction, training processes, and evaluation strategies. The organization of the paper can be seen in the contents section listed above.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 Related Work</h1>
<h3>2.1 Machine Learning in Geoscience</h3>
<p>With the advancement of artificial intelligence, utilizing machine learning, natural language processing, and recent large-scale model techniques to tackle critical problems in geoscience has become a crucial direction. Various subtasks in geoscience involve significant data collected from sensors, making them suitable for end-to-end learning using machine learning approaches. Some studies model multiple aspects of seismic signals using deep learning models to extract information relevant to earthquake prediction. Among them, [16] uses supervised learning with end-to-end training, while $[17,18]$ employs self-supervised learning to obtain models applied to downstream tasks. [19, 20] utilize machine learning to explore the latent correlations among different rock properties for rock type prediction. Beyond relatively straightforward classification tasks, there are numerous works applying machine learning to address more complex scenarios in geoscience, such as calculating wellhead flow rate [21], capturing and storing carbon [22], and predicting the condition of SPBM tunnels [23]. Additionally, machine learning is introduced to evaluate the real-world environment: [24] explores the use of Few-Shot Learning (FSL) methods to enhance the accuracy of high-resolution pre-stack seismic inversion, and [25] employs various machine learning techniques and ensemble methods to improve landslide hazard prediction, demonstrating their high practical value. Machine learning is also being used to aid geoscience exploration, [26] attempts to use machine learning to do data-driven modeling of solid earth science, [27] attempts to use machine learning to reveal the link between fast and slow earthquakes, [28] uses machine learning to reveal the impact of aerosols on climate impact.</p>
<h3>2.2 Natural Language Processing in Geoscience</h3>
<p>In addition to the diverse and heterogeneous data collected from various sensors, the field of geoscience also encompasses a significant amount of text data with standardized formats. The application of natural language processing (NLP) in earth science has witnessed remarkable progress. [29, 6] embed different sources of textual information into a unified space, [29] employs joint training of language models with text and points of interest (POI) for POI retrieval, while [6] integrates geological attribute information into the textual representation space to enable better knowledge extraction. [30, 31] enhance language models with knowledge graph techniques, where [30] constructs a knowledge graph on geological text to discover ore-forming environments, and [31] proposes an automatic entity and relation extraction approach via three-level extraction to build a geological knowledge graph from extracted information in geological reports. [32] combines retrieval techniques with language models creates an integrated solution incorporating contextual retrieval and the GeoBERT model. [33] focuses on various language irregularities encountered in natural language texts, introducing the NeuroSPE model for spatial extraction using neural networks. NLP techniques provide a unified representation space and computational approach for diverse geological data described in various textual formats, narrowing the technological gap between different earth science tasks.</p>
<h3>2.3 Domain-specific Large Language Model</h3>
<p>The recent emergence of large-scale language models marks a significant step towards unified information processing in geoscience. These models are pre-trained on vast amounts of text data and efficiently compress all input data. Currently, in addition to earth science, various domains have seen the development of domain-specific pre-trained models trained on domain-specific corpora. [34, 35, 36, 12, 37, 38] performs large-scale pre-training on domain-specific texts and has resulted in foundational models equipped with domain knowledge, while [39, 40, 41] fine-tuning these base models using domain-specific data, achieving models tailored to specific downstream tasks at a lower cost. These works have made significant strides in developing domain-specific LLMs through dedicated data integration and model training efforts. Recently, [42, 43, 44] explored the use of prompt engineering to unlock the potential of models without additional training, offering the possibility of unifying various geoscience tasks and further reducing the cost of employing large models in domain applications. In the field of geoscience, the exploration of large models is still in its early stages. [15] collected a substantial amount of high-quality data from earth science Wikipedia and research papers, and further fine-tuned the base model, leading to impressive scientific competence and knowledge in earth science. For the first time, our work utilizes a large corpus of earth science papers and textbooks, which were cleaned using a dedicated toolchain for constructing large-scale earth science models, ensuring data quality. Furthermore, our work completes the entire process of "further pre-training, supervised fine-tuning, augmented learning" for large foundation models for geoscience, bringing the largest scale and highest quality proprietary language models to the geoscience field. This will unlock tremendous possibilities for future research conducted by earth science researchers.
We have outlined the progression of geoscience research with the use of cutting-edge AI techniques, including neural network (NN), K-nearest neighbor (KNN), recurrent neural network (RNN), convolutional neural network (CNN), backpropagation (BP), reinforcement learning (RL), support vector machine (SVM), long-short term memory (LSTM),</p>
<p>graph convolutional neural network (GCN), Transformers, BERT, ChatGPT, and large language model (LLM). [45] The investigation reveals that the time intervals between AI technology advancements and their application in geoscience have significantly shortened, indicating an increasing reliance on advanced AI technology in the field of geoscience. The illustration is presented in Figure 2, and detailed information about the progression is shown in Appendix A.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The progression illustration of geoscience research with the use of cutting-edge AI techniques. The textboxes in PaleTurquoise show the techniques from computer science, The textboxes, in Bisque show the research that probably the first time geoscientists used the techniques.</p>
<h1>3 Preliminary and Vocabulary</h1>
<p>To facilitate understanding of our work and the overview of our model, here are some key terms and their corresponding explanations that will be widely used in the narrative of this article.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Vocab</th>
<th style="text-align: left;">Stage and illustration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Galactica-30B</td>
<td style="text-align: left;">The vanilla Galactica model</td>
</tr>
<tr>
<td style="text-align: left;">GeoGalactica-FP</td>
<td style="text-align: left;">Checkpoint after pre-training over geoscience data (Further Pre-train)</td>
</tr>
<tr>
<td style="text-align: left;">GeoGalactica-Alpaca</td>
<td style="text-align: left;">Applying supervised fine-tuning with Alpaca data on top of GeoGalactica-FP</td>
</tr>
<tr>
<td style="text-align: left;">GeoGalactica-GeoSignal</td>
<td style="text-align: left;">Applying supervised fine-tuning with GeoSignal data on top of the first checkpoint (GeoGalactica-FP)</td>
</tr>
<tr>
<td style="text-align: left;">GeoGalactica</td>
<td style="text-align: left;">Applying supervised fine-tuning following training recipe of K2 on top of the first checkpoint (GeoGalactica-FT)</td>
</tr>
<tr>
<td style="text-align: left;">GeoCorpus</td>
<td style="text-align: left;">Geoscience text corpus for pre-training</td>
</tr>
<tr>
<td style="text-align: left;">GeoSignal</td>
<td style="text-align: left;">Supervised fine-tuning data for geoscience</td>
</tr>
<tr>
<td style="text-align: left;">GeoBench</td>
<td style="text-align: left;">Benchmarks for evaluating the performance of the geoscience LLM</td>
</tr>
</tbody>
</table>
<p>Table 1: Vocabulary for this technical report.
Here we list the terms widely used in this report:</p>
<ul>
<li>Sciparser. A PDF parsing toolkit for preparing text corpus to transfer PDF to Markdown.</li>
<li>GeoTools. A set of supervised instruction data for training GeoGalactica to use tools.</li>
<li>K2. The first-ever geoscience large language model trained by firstly further pre-training LLaMA on collected and cleaned geoscience literature, including geoscience open-access papers and Wikipedia pages, and secondly fine-tuning with knowledge-intensive instruction tuning data (GeoSignal).</li>
<li>Deep Literature. Deep Literature is a literature platform, aiming to construct a knowledge information system for geoscience scholars, which, step-by-step goes through knowledge arrangement, knowledge mining and knowledge discovery.</li>
<li>
<p>GAKG. GAKG [2] is a multimodal Geoscience Academic Knowledge Graph (GAKG) framework by fusing papers' illustrations, text, and bibliometric data.</p>
</li>
<li>
<p>DeepShovel. DeepShovel [46] is an Online Collaborative Platform for Data Extraction in Geoscience Literature with AI Assistance.</p>
</li>
<li>GSO. Similar to WordNet, GeoScience Ontology, (GSO) is a hierarchical tree of geological terms contains a vast amount of synonyms and word explanations, providing valuable geoscience connections between terms.</li>
<li>Acemap. AceMap is a platform displaying relationships among academic publications and a search engine for academic literature reviews.</li>
<li>DataExpo. A one-stop dataset service for geoscience research.</li>
</ul>
<p>Finally, we share the model card in Appendix F.</p>
<h1>4 Data Collection and Cleaning</h1>
<p>The training corpus of Galactica primarily consists of literature related to computer science and biochemistry rather than earth science. This suggests that Galactica may lack sufficient knowledge in the field of geoscience. To address this, we have collected approximately six million research documents specifically focused on earth science. These papers were carefully selected by professional experts in the field. Furthermore, we have expanded the GeoSignal dataset based on K2 to better support natural language processing tasks in earth science research. This expanded dataset was used for fine-tuning the model after further pre-training. In the following sections, we will provide a detailed explanation of how our dataset was constructed.</p>
<h3>4.1 The Customized Pre-training dataset: GeoCoprus</h3>
<p>According to our long-term collection efforts on geoscience papers, with the research fields subfields of geology and geography, through Acemap, we have accumulated a total of 5,980,293 papers.
During this process, we commenced our data collection in early 2020 by gathering a list of journals in geoscience from LetPub ${ }^{17}$. We identified the corresponding publishers' websites using the journal names and ISSN to collect open-access data through web page parsing. This process continued towards 2023 when we collaborated with experts in various sub-disciplines of geoscience, we collect paper from high-quality SCI journals in the mathematical geosciences, metamorphic petrology, geochronology,geomagnetism and paleomagnetism, geomorphology, tectonics, stratigraphy, hydrogeology, geophysical, geothermics, igneous and geochemistry, surficial geochemistry, geological mapping, sedimentological, petroleum geology, paleontology, paleogeography, and mineralogy. In total, we integrated a list of 849 geoscience-related journals(Appendix B shows the distribution of the collected papers in geoscience).
We employed the journal name list to search for journal information and their publishers' websites. Through web page scraping, we collected HTML pages and subsequently conducted data parsing to extract metadata from papers. For open-access articles, we matched them with the parsed DOI and the corresponding PDF from Sci-Hub ${ }^{18}$. If no PDF was available, we downloaded it based on the URL.
Throughout this process, we adhered to the network conventions of information acquisition. When faced with obstacles such as anti-scraping measures like 5-second shields, JavaScript encryption, IP proxy bans, and account logins, we constrained our actions to ensure the compliance of our data. Moreover, data security remained our utmost priority during this process; thus, we refrained from publicly disclosing the data obtained during this stage.
In conclusion, we obtained a total of 5,980,293 papers. Our data collection system operated through a distributed information fusion mechanism, utilizing an 8 -workstation k8s cluster. Data collection was conducted using ScrapyRedis ${ }^{19}$ framework. Additionally, we implemented compression techniques for HTML data to address challenges related to large-scale data storage.
Furthermore, we have leveraged the copyrights obtained from the publishers we have been collaborating with over the years to parse and anonymize the PDFs of these articles, creating a dataset of textual data. Additionally, referring to $[47,48]$, we have reason to believe that the inclusion of program code in the model's pre-training, alongside the text, can significantly enhance the reasoning capabilities of the LLM model.
Therefore, after collecting datasets from Acemap and ArXiv, we incorporated the training dataset from Codedata. Finally, our overall training corpus is detailed in Table 2, totaling 78B. The data from a specific source is concatenated into a single record. After tokenization, we then split it according to a block size of 2048, with each instance ending</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>with the tokenizer.eos token. For each training batch, the proportion of geoscience papers to the other two datasets is $8: 1: 1$.
Notice: All the data employed in this manuscript is derived from web pages that are publicly available, thereby introducing a potential bias in the reliability of the data. Users are cautioned to be mindful of potential hallucination problems that may occur when utilizing large-scale models. Furthermore, this paper adheres to all copyright concerns. Should any issues arise, stakeholders are encouraged to notify the authors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">#blockNum</th>
<th style="text-align: left;">#tokenNum</th>
<th style="text-align: left;">#itemNum</th>
<th style="text-align: left;">#tokenSize</th>
<th style="text-align: left;">#batchRatio</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GeoCorpus</td>
<td style="text-align: left;">$25,743,070$</td>
<td style="text-align: left;">$52,721,798,004$</td>
<td style="text-align: left;">$5,548,479$</td>
<td style="text-align: left;">98.21 G</td>
<td style="text-align: left;">$80 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ArXiv</td>
<td style="text-align: left;">$6,691,886$</td>
<td style="text-align: left;">$13,704,981,558$</td>
<td style="text-align: left;">742,835</td>
<td style="text-align: left;">25.53 G</td>
<td style="text-align: left;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Codedata</td>
<td style="text-align: left;">$6,066,725$</td>
<td style="text-align: left;">$12,424,652,670$</td>
<td style="text-align: left;">$3,456,887$</td>
<td style="text-align: left;">23.14 G</td>
<td style="text-align: left;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">$38,501,681$</td>
<td style="text-align: left;">$78,851,432,232$</td>
<td style="text-align: left;">$9,748,201$</td>
<td style="text-align: left;">146.88 G</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Data distribution of the corpus used for training GEOGalactica</p>
<p>We utilized data processing and enhancement tools based on DeepShovel and K2 during this process. With the help of Grobid [49] and pdffigure2 [50], we provided a comprehensive parsing solution for extracting text, images, tables, formulas, and other data types from research documents. This was further enhanced by DeepShovel for parsing tables and formulas, resulting in the development of the SciParser tool. We plan to open-source this tool and share it on GitHub.
Within PDF documents, there are various types of data, including text, images, tables, and formulas, all organized within the articles' hierarchical structure and page layout. Data preprocessing is necessary to extract and ensure the readability of such content. It entails utilizing a PDF parsing tool to perform an initial parsing of the PDF document, resulting in a parsing file that contains various information from the document. However, the readability of this file is often poor, and it may have a significant amount of redundant information. Subsequently, the parsing file needs to undergo data cleansing, extracting the desired text, images, tables, formulas, and other data, and converting it into Markdown format for further processing, analysis, or display purposes.
Currently, we are utilizing Grobid as our PDF parsing tool. Grobid can accurately extract text from PDF documents and perform structured extraction of articles. It provides an outline of the text, forming an XML structure that allows for restoring the original PDF layout. Additionally, Grobid enables precise localization of images, tables, formulas, and other data types. With the provided bounding boxes, we can obtain the corresponding images using the PyMuPDF tool ${ }^{20}$. Further leveraging the OCR recognition integrated into DeepShovel [46], we can convert tables, formulas, and other elements into Markdown format. The parsing process is completed by writing all the parsed content into a markdown file for further use. Throughout the entire process, for tables, we utilize the DeepShovel PDF Table Parser ${ }^{21}$. This tool ensures the completeness and accuracy of the table content while preserving the table structure, making it convenient to reconstruct tables using Markdown. As for formulas, we employ an improved version of Latex-OCR ${ }^{22}$ for the recognition, converting the parsing results into the string format. We open-source our PDF parsing solution on GitHub ${ }^{23}$.
Tokenization is a crucial component of text corpus construction. To aid language models in comprehending academic papers, we utilize dedicated tokens for different types of special data. Finally, we use special tokens similar to the original Galactica paper [12] to unify various forms of text extracted from various sources into one standard protocol. Below is an explanation of our special tokens.</p>
<ul>
<li>Figures: We use the special tokens [START_FIGURE] and [END_FIGURE] to mark the captions of figures in the paper.</li>
<li>Tables: The special tokens [START_TABLE] and [END_TABLE] are employed to identify the position of tables within paragraphs. During this process, we convert tables from the PDF into Markdown format.</li>
<li>References: We use the special tokens [START_REF] and [END_REF] to annotate citations. The title of the article is placed within these special tokens to enhance readability.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>Formulas: For mathematical content or formulas, we employ regular expressions and rule-based methods to filter and clean irregular formulas parsed from the PDF. Additionally, we use the special tokens [START_FORMULA] and [END_FORMULA] to capture them.</li>
</ul>
<p>And the dedicated tokens for these different types of special data are shown in Figure 3 (We use the one in [15])
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Tokenization processed text. A. shows an example of a figure marker, we only choose to preserve the captions; B. shows an example of a table marker, we transfer the tables into the form of Markdown; C. shows the tokenization of the citations, we replace the reference numbers into reference papers' title to preserve the readability of the text corpus; D. shows an example of the special tokens for formulas.</p>
<h1>4.2 The Customized SFT dataset: GeoSignal Version 2</h1>
<p>Through extensive investigation and research, we have thoroughly explored natural language processing tasks specifically tailored to geoscience. In this endeavor, we have identified a set of tasks that cater to the unique requirements of geoscience applications. However, during this process, we have observed numerous unsupervised signals within these tasks that have yet to be fully harnessed and summarized.</p>
<ul>
<li>Geoscience Knowledge Graph: Named entity recognition (NER) for temporal scales, rock types, etc., relation extraction (RE) for linking knowledge points, text-to-graph transformation, and knowledge discovery through reasoning</li>
<li>Academic Applications: Keyword extraction, summarization, and information retrieval.</li>
<li>General Applications: Question and Answering (Q\&amp;A), conversations related to geoscience education, and text classification.</li>
<li>Geographical Applications: Point of Interest (POI) queries and multimodal Q\&amp;A.</li>
</ul>
<p>However, the supervised signals for these tasks can be reconstructed using professional geoscience data websites. Based on the data scheme provided by K2, we further elaborate on the entire data construction process. In this process, we have built three categories of data:</p>
<ol>
<li>Literature-related data can be used to construct general natural language instructions, enabling the model to possess basic semantic reasoning abilities.</li>
<li>Geoscience-related data, which is used to build a knowledge-intensive instruction dataset, allowing the model to understand and comprehend the specific forms of natural language tasks in the field of geoscience.</li>
<li>Self-instruction-related data, following the examples of Alpaca [51] and Baize [52], we have distilled geosciencerelated data from ChatGPT and invited geoscience experts to annotate it. This data is used to construct high-quality geoscience question-answering datasets.</li>
</ol>
<h1>4.2.1 Domain General Natural Language Instruction</h1>
<p>For the general instruction learning data, we have integrated four platforms constructed by Acemap, and reconstructed the data accordingly.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Four platforms that contribute most to our GeoSignal.
Referring to RST [53] and K2 [15], we restructure the signals from various geoscience-related platforms. The following paragraphs will provide a detailed explanation for each platform and the illustrations for restructured domain-general natural language instruction.</p>
<p>Deep Literature and DataExpo. This two platforms can be understood as collections of papers and datasets. Therefore, the Related Paper (with abstract) and Reference resolution of Deep Literature, as well as the Reference resolution of DataExpo, serve as excellent datasets for establishing referential relationships.
Using the text processing tool mentioned earlier, we explicitly employ a multi-threaded Grobid to process all documents and convert them into an intermediate XML format. Within the converted XML, we identify the bibl_id of in-text citations and then locate the corresponding reference paper titles in the XML's reference section.</p>
<p>GSO. Similar to WordNet, the hierarchical tree of geological terms contains a vast amount of synonyms and word explanations, providing valuable supervised signals. As a result, we traverse all the nouns in GSO, extract all the synonyms for each term, and combine them with the term itself to create a set. We then construct all possible pairs of (term, synonym_term) and add them to a list of results.
For the word description, we traverse all the nouns of GSO, extract the definition of the respective noun to serve as the description and create signal pairs (word, description). Additionally, there is also a specialized geology dictionary, which includes a dataset of categorized geology terms. The original data is in PDF format, and we convert it into JSON format through PDF parsing. In this process, we first use a parsing tool to convert the PDF into a docx format, and then use a data processing script to convert its content into JSON format. Subsequently, we proceed with content processing, removing hyphens at line breaks, and merging multiple definitions of a single term. GSO use two geoscience dictionary. For the geology dictionary, each entry consists of a "name" and "description". For the geography knowledge dictionary, includes one more "attribute" field.</p>
<p>GAKG. GAKG is rich in images, tables, and other elements from geology papers. Meanwhile, the text describing these images and tables, as well as their captions, can serve as excellent sequence-to-sequence (seq2seq) supervised data. Regarding the papers and their graphical information, four types of binary pairs can be generated. During this process,</p>
<p>we transform the original text of the paper, tables, and illustrations in PNG format along with their corresponding captions, including table numbers and contents, into the target data format: (illustration caption, illustration content), (illustration caption, referring sentence), (table caption, table content), (table caption, referring sentence). For detailed information regarding this specific aspect, please refer to the Appendix.</p>
<p>Our approach to handling this is as follows:</p>
<ol>
<li>The captions and contents of tables and illustrations are stored in separate JSON files within their respective folders and can be extracted from there.</li>
<li>The referring sentences, on the other hand, need to be retrieved from the original text of the paper by referencing the table/illustration numbers mentioned in their captions.</li>
</ol>
<p>Specifically, we search for the keywords "fig" (or variations like "Fig" and "FIG") and "table" (or "Table" and "TABLE") in the original text and identify the associated numbers (i.e., "i") immediately following them. We then search for complete sentences between two periods preceding and following these numbers.
Our program handles some unexpected scenarios, such as excluding cases like "Fig11" or "Fig12" when searching for "Fig1," and partially excluding cases where the confusion in numbering arises from referring to tables/illustrations from other papers. We also consider disorders caused by the dot used in English sentences and abbreviations, among other cases.</p>
<p>However, there are still a few limitations to this method:</p>
<ol>
<li>When the keywords "fig" or "table" appear at the end of a sentence, our program includes both that sentence and the subsequent one as the corresponding referring sentence.</li>
<li>There might be instances where figures/tables from other papers are referenced. Our program can identify such cases if:</li>
<li>The figure/table numbers are more significant than the current paper's total number of figures/tables.</li>
<li>The word "of" appears close after "Fig" in the text.</li>
</ol>
<p>In scenarios where it is difficult to discern whether a referenced figure/table belongs to another paper, we prioritize data quality. If we encounter any unmatched or garbled text, or the text is concise, we will discard that particular supervisory signal.</p>
<p>Wikipedia. Wikipedia contains a lot of crowd-sourcing information for geoscience. Consequently, we have also incorporated geoscience data from the Wikipedia page. To retrieve the information, we utilized web scraping techniques and relevant libraries.
For the article's title, we used the Wikipedia library in Python ${ }^{24}$, which supports accessing sections of a Wikipedia page. Each section's title, text, and sub-sections form a nested structure. By recursively traversing each page, we obtained a list of triplets comprising each section's level, title, and paragraph. The triplets are structured as (level, title, paragraph), where the level indicates the depth of nesting, the title represents the section's title, and the paragraph contains the corresponding text content.
To retrieve the "Summary \&amp; Abstract" of the article, we utilize the Wikipedia library in Python to access the abstract of the corresponding Wikipedia page directly. We then concatenate the paragraphs from the abovementioned sections to form the full text. Finally, we output the tuple (full text, abstract).
To extract the Entity mentioned in the article, we use the requests library and the BeautifulSoup ${ }^{25}$ library to scrape the Wikipedia page directly. We retrieve the text from all tags labeled "p" and "ul" and treat them as paragraphs. Next, within these paragraph tags, we search for tags labeled "a" with a href attribute starting with "/wiki/". These represent the highlighted blue hyperlinked sections within the text. We collect these entities and output the tuple (paragraph, entities).</p>
<h1>4.2.2 Restructured Knowledge-intensive Instruction</h1>
<p>In our work of building restructured knowledge-intensive instruction data, we begin by searching for authoritative websites related to paleontology, dinosaurs, fossils, rocks, and other fields within geoscience. We then filter these websites, specifically selecting those with structured data available for extraction.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Disciplines</th>
<th style="text-align: left;">Websites</th>
<th style="text-align: left;">Websites Intro.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dinosaur</td>
<td style="text-align: left;">https://dinounimals.com/dinosaurdatabase/</td>
<td style="text-align: left;">A comprehensive Dinosaur Database, offering a detailed catalog of dinosaurs.</td>
</tr>
<tr>
<td style="text-align: left;">Fossil</td>
<td style="text-align: left;">https://fossilcalibrations.org/</td>
<td style="text-align: left;">A specialized resource offering a curated collection of fossil calibrations.</td>
</tr>
<tr>
<td style="text-align: left;">Fossil</td>
<td style="text-align: left;">http://fossil-ontology.com/</td>
<td style="text-align: left;">A multi-dimensional scientific database of fossil specimens.</td>
</tr>
<tr>
<td style="text-align: left;">Mineral</td>
<td style="text-align: left;">https://rruff.info/</td>
<td style="text-align: left;">A dedicated resource for the study and identification of minerals.</td>
</tr>
<tr>
<td style="text-align: left;">Mineral</td>
<td style="text-align: left;">https://zh.mindat.org/</td>
<td style="text-align: left;">A comprehensive online mineralogical database.</td>
</tr>
<tr>
<td style="text-align: left;">Sedimentary</td>
<td style="text-align: left;">https://mrdata.usgs.gov/</td>
<td style="text-align: left;">A system with interactive maps and data for analyzing mineral resources on a regional and global scale.</td>
</tr>
<tr>
<td style="text-align: left;">Earthquake</td>
<td style="text-align: left;">https://www.usgs.gov/</td>
<td style="text-align: left;">A website collecting all the earthquake world wide.</td>
</tr>
<tr>
<td style="text-align: left;">Hazard</td>
<td style="text-align: left;">https://public.opendatasoft.com/explore/</td>
<td style="text-align: left;">A platform for exploring various datasets sorted by their modification date.</td>
</tr>
</tbody>
</table>
<p>Table 3: Knowledge Intensive Data Sources.</p>
<p>For the websites that can be structured, we perform corresponding restructured processing like K2 [15]. Taking the provided image as an example, we match the structured data on the website using Key-Value pairs and create natural Instruction and Response pairs.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An example for illustrating the construction of restructured knowledge-intensive instruction data.</p>
<h1>4.2.3 Self-Instruct</h1>
<p>According to Alpaca [51] and Baize [52], using problem seeds to generate answer from ChatGPT ${ }^{26}$ is an appropriate way to build instruction tuning data. In geoscience scenarios, we generate 1000 questions per subject under the geoscience, and we put the problem seeds on GEOGalactica's Github Repo.
In terms of overall data collection, the total amount is as follows. And we select a certain proportion of data to be included in our supervised fine-tuning process. In the final version, after further manual verification and cleaning, we choose to use a dataset of $\mathbf{1 0 0 K}$ samples as GeoSignal Version 2 for instructional data during the supervised fine-tuning. The detailed statistic of the instruction tuning data is shown in Table 4.</p>
<h2>5 Training</h2>
<p>Taking the lessons from GLM-130B [14], we design the frameworks and plans of the GEOGalactica. The following are the details of our progress.</p>
<h3>5.1 Further Pre-training</h3>
<p>After the initial pre-training by Meta AI, the model Galactica can undergo additional training on a geoscience-specific dataset. We hope this fine-tunes the model's understanding and generation capabilities in particular domains or styles.
We utilize a supercomputing cluster based on the Hygon DCU architecture, combined with the Megatron-LM framework [54], to further pre-train our models. The computing cluster consists of 512 nodes, with each node equipped with a 32 -core CPU, 128 GB of memory, and 4 DCU acceleration cards, each with 16 GB of memory, resulting in a total of 2048 acceleration cards, where each acceleration card is equivalent to approximately 0.2 times the computing power of an NVIDIA A100 GPU. The Megatron-LM framework employs 3D parallelism strategies, including pipeline-parallel,</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Signals</th>
<th style="text-align: center;">tuples</th>
<th style="text-align: center;">#NumofSamples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Scholar</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Title (with Abstract)</td>
<td style="text-align: center;">(abstract; title)</td>
<td style="text-align: center;">2,690,569</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Abstract (with Publications Fulltext)</td>
<td style="text-align: center;">(fulltext; abstract)</td>
<td style="text-align: center;">2,601,879</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Category (with abstract)</td>
<td style="text-align: center;">(abstract; category)</td>
<td style="text-align: center;">$12,321,212$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Related Paper (with abstract)</td>
<td style="text-align: center;">(source abstract; target abstract; reference sentence)</td>
<td style="text-align: center;">40,047,777</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">One Sentence Summary (with abstract)</td>
<td style="text-align: center;">(abstract; question; answer)</td>
<td style="text-align: center;">2,690,569</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reference resolution</td>
<td style="text-align: center;">(sentence; pronoun.; reference item) [including citation]</td>
<td style="text-align: center;">2,329,820</td>
</tr>
<tr>
<td style="text-align: center;">DataExpo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Title</td>
<td style="text-align: center;">(abstract; title)</td>
<td style="text-align: center;">216,036</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Summary \&amp; Abstract</td>
<td style="text-align: center;">(fulltext; abstract)</td>
<td style="text-align: center;">216,036</td>
</tr>
<tr>
<td style="text-align: center;">GAKG</td>
<td style="text-align: center;">GAKG</td>
<td style="text-align: center;">Principal Concepts</td>
<td style="text-align: center;">(sentence; entity; types)</td>
<td style="text-align: center;">3,892,102</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Relations</td>
<td style="text-align: center;">(abstract; sentence; head entity; relation; tail entity)</td>
<td style="text-align: center;">30,123</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Paper table caption</td>
<td style="text-align: center;">(table caption; refering sentence)</td>
<td style="text-align: center;">2,772,166</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Paper illustration caption</td>
<td style="text-align: center;">(illustration caption; refering sentence)</td>
<td style="text-align: center;">9,128,604</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Paper table content</td>
<td style="text-align: center;">(table caption; table content)</td>
<td style="text-align: center;">2,772,166</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Paper illustration content</td>
<td style="text-align: center;">(illustration caption; illustration content)</td>
<td style="text-align: center;">9,128,604</td>
</tr>
<tr>
<td style="text-align: center;">GAKG</td>
<td style="text-align: center;">GSO</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(sentence; facts; improper statement)</td>
<td style="text-align: center;">114,392</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Taxonomy</td>
<td style="text-align: center;">(upper term; term)</td>
<td style="text-align: center;">112,298</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synonyms</td>
<td style="text-align: center;">(term; synonym term)</td>
<td style="text-align: center;">23,018</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Word description</td>
<td style="text-align: center;">(word; description; source)</td>
<td style="text-align: center;">110,209</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GA-Dialogue</td>
<td style="text-align: center;">Future content and Previous content</td>
<td style="text-align: center;">(corrupted text; corrupted positions; target spans)</td>
<td style="text-align: center;">5,434</td>
</tr>
<tr>
<td style="text-align: center;">GeoOpenData</td>
<td style="text-align: center;">dinosaur</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">11,348</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fossilcalibrations</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">1,749</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fossilontology</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">3,210</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mindat</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">51,291</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ngdb</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">148,212</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">opendatasoft</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">37,823</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">rruff</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">32,778</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">usgsearthquake</td>
<td style="text-align: center;">Factual knowledge</td>
<td style="text-align: center;">(property; property value)</td>
<td style="text-align: center;">37,284</td>
</tr>
<tr>
<td style="text-align: center;">WordNet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synonyms</td>
<td style="text-align: center;">(term; synonym term)</td>
<td style="text-align: center;">6,408</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Word description</td>
<td style="text-align: center;">(word; description; source)</td>
<td style="text-align: center;">27,123</td>
</tr>
<tr>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Title</td>
<td style="text-align: center;">(term; abstract)</td>
<td style="text-align: center;">3,033,595</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Summary \&amp; Abstract</td>
<td style="text-align: center;">(fulltext; abstract)</td>
<td style="text-align: center;">753,920</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entity mentions</td>
<td style="text-align: center;">(paragraph; entities)</td>
<td style="text-align: center;">3,688,926</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Relation</td>
<td style="text-align: center;">(text; subject; property; object)</td>
<td style="text-align: center;">630,210</td>
</tr>
<tr>
<td style="text-align: center;">IODP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Title</td>
<td style="text-align: center;">(abstract; title)</td>
<td style="text-align: center;">2,839</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Summary \&amp; Abstract</td>
<td style="text-align: center;">(fulltext; abstract)</td>
<td style="text-align: center;">2,638</td>
</tr>
</tbody>
</table>
<p>Table 4: GeoSignal Statistics Table.
model-parallel, and data-parallel, to maximize GPU performance while reducing communication overhead. Given the four acceleration cards per node, we set the model parallel size to 4 for optimal model-parallel efficiency. Additionally, in the case of a mini-batch size of 1 , we set the pipeline-parallel size to 16 to fully utilize the memory resources.
We preprocess all training text data by performing tokenization. The tokenized results of each document are then concatenated using an end-of-sentence (eos) marker. Subsequently, we crop the concatenated sequences into fixed lengths of 2048, resulting in 30 million training samples, corresponding to 7324 training steps. Before formally starting the training, we conduct a preliminary experimental analysis of node failures and save checkpoints at intervals of 100 steps. We initiate the pre-training process after transforming the initial checkpoint format into the format Megatron-LM requires. Ultimately, after running for 16 days, the computing cluster completes the further pre-training of the model at a speed of 3 minutes per step. Due to the frequent occurrence of node failures, the actual training takes nearly a month to complete. After the pre-training, we convert the checkpoints into the Hugging Face format for subsequent applications.</p>
<h1>Challenge in further pre-training</h1>
<ol>
<li>Over-fitting: Further pre-training may increase the risk of overfitting, especially when the training data is relatively limited compared to the original Galactica pre-training data (refer to Section 4).</li>
<li>Catastrophic forgetting: In Further pre-train, ensuring that the training on the initial pre-training data is not forgotten is crucial. Sudden increases in the loss of new data sources can lead to the loss of knowledge acquired from the Galactica pre-training. It is essential to address how to effectively transfer higher-level language abilities to specific tasks and prevent the loss of the model's generality obtained during the initial pre-training during the fine-tuning process.</li>
<li>Stability and Convergence: Further pre-training models may be more prone to training instability and convergence difficulties. During the training process, more sophisticated optimization techniques and strategies may be required to ensure that the model converges smoothly to an appropriate state.</li>
</ol>
<p>Parameters transformation from Galactica to Megatron GPT-2 Since Galactica belongs to the OPT model, we referred to and modified the code available on Hugging Face for converting HF GPT-2 to Megatron GPT-2. The conversion parameters can be adjusted based on the actual scale of pipeline parallelism (PP), model parallelism (MP), and data parallelism (DP) during runtime.</p>
<h1>Training detail</h1>
<ul>
<li>Training Setup: In this study, we utilized a supercomputing cluster based on the hygon DCU architecture and combined it with the Megatron-LM framework for further pre-training of the model. The computing cluster consisted of 512 nodes, with each node equipped with a 32 -core CPU, 128 GB of memory, and 4 DCU accelerator cards with 16 GB of VRAM, totaling 2048 accelerator cards, each of which is equivalent to approximately 0.2 times the computational power of an NVIDIA A100.</li>
<li>Parallel Configuration: The Megatron-LM framework employed 3D parallelism techniques, including pipeline parallelism, model parallelism, and data parallelism, to maximize GPU performance and minimize communication overhead. Since each node had 4 accelerator cards, we set the model parallel size to 4 to achieve optimal parallel efficiency. Additionally, in cases where the mini-batch size was 1 , we set the pipeline-parallel size to 16 to fully utilize the VRAM resources.</li>
<li>Data Preprocessing: We performed tokenization on all training text data, and the tokenized results of each document were concatenated using the <eos> marker. Subsequently, we cropped the concatenated tokens into fixed lengths of 2048, resulting in 30 million training samples, corresponding to 7324 training steps.</li>
<li>Checkpoints: Before formally starting the training process, we analyzed the node failure patterns through preliminary experiments and saved checkpoints at intervals of 100 steps.</li>
<li>Hyperparameter Selection: We conducted extensive experiments for hyperparameter selection in Further pre-train. Regarding learning rate scheduling, initial experiments showed that directly adopting a maximum learning rate of $1 e-4$ from the Galactica-30B model led to a rapid increase in loss after a certain number of steps, resulting in training failure. Hence, we observed the gradient norm curve during the training warm-up phase and selected the learning rate corresponding to the minimum gradient norm, which was $1 e-5$, as the actual maximum learning rate for training, which remained constant throughout the entire training process. For the training warm-up, we employed a linear training warm-up strategy and tested different training warm-up steps, and the optimum result was achieved with 100 training warm-up steps. Regarding other hyperparameters, we opted for the Adam optimizer with a $\beta_{1}$ of 0.9 , a $\beta_{2}$ of 0.95 , a weight decay rate of 0.1 , and epsilon of $1 e-8$. To balance effectiveness and efficiency, we set the global batch size to 4096 and utilized checkpoint activations to save VRAM. Additionally, we set the gradient clip threshold to 1.0 and a dropout rate of 0.1 .</li>
</ul>
<p>For a better understanding of our training, we list the hyperparameters of the model and the configured setting of the training in Appendix E.</p>
<p>Training curves We share the curves of the training loss and gradient normalization as Figure 6 and Figure 7. We observed that the training loss quickly dropped from about 1.60 to 1.40 during the first 300 steps and then smoothly decreased from 1.40 to 1.32 in the subsequent steps. Although the gradient normalization showed several spikes, sharply increasing from 0.1 to approximately $0.3 \sim 4.8$, the model exhibited no signs of saturation after further pre-training on 60 billion tokens. This demonstrates the stability of the entire further pre-training process.</p>
<h2>The bottleneck of the training</h2>
<ul>
<li>The embedding layer is not treated as a stand-alone component in the training process. Instead, it is combined with the first transformer layer. As a result, the VRAM usage on certain cards is $60 \%$ higher than on others, leading to</li>
</ul>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Training curve during the further pre-training.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Training curve of the first 500 steps during the further pre-training.</p>
<p>decreased training efficiency. This is because a larger PP value is required to accommodate the entire model, which increases communication overhead.</p>
<ul>
<li>Due to some bugs in Megatron, the "continue pre-train" function cannot utilize distributed optimizers. This results in each DP group or model replica having a complete copy of the optimizer state, significantly increasing VRAM usage.</li>
</ul>
<h3>5.2 Supervised Fine-Tuning</h3>
<p>After pre-training, LLMs can be supervised fine-tuning (SFT) on a smaller, more targeted dataset under human supervision. This process adapts the model to specific tasks or improves its performance in certain areas.</p>
<p>We employed SFT to enhance the geoscientific reasoning performance of our large-scale models on specific geoscientific tasks. This process is essential to effectively transfer advanced language capabilities to geoscientific-specific tasks and preserve the model's generalization acquired during pre-training.</p>
<p>We utilized two major frameworks, Huggingface and DeepSpeed, during this stage to facilitate our training work. This aimed to accomplish instruction fine-tuning and model prediction tasks. In the training process, the Hygon DCU cluster remained our primary resource. Compared to the pre-training stage, SFT truncation only took advantage of 128 nodes and their accompanying 512 DCUs. We continued to employ the learning rate schedule used during pre-training, where the maximum learning rate was set to 1e-5, combined with a linear warm-up consisting of 100 warm-up steps. For the optimizer, we still selected the Adam optimizer, with β1 and β2 set to 0.9 and 0.999, respectively. Additionally, a weight decay of 0.05 and ε value of 1e-8 was chosen to better adapt to the required fine-tuning tasks.</p>
<p>Considering the enormous scale of the model, we utilized the DeepSpeed ZeRO3 technique for memory optimization, along with the gradient checkpoint method, to further reduce memory pressure. The maximum input sequence length was limited to 512 in this process to avoid unnecessary computational overhead. However, due to the limitations of DeepSpeed, the global batch size had to be no smaller than the number of accelerator cards. Therefore, we opted</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Training curve during the SFT on dataset Alpaca.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Training curve during the SFT on Geosignal.
for a larger global batch size of 512. Regarding the settings of other parameters, we followed the default values of the Huggingface trainer framework. For the subsequent training, we used the Alpaca dataset and conducted training for three epochs, which only took about one day to obtain the final SFT model. This training process, supported by Megatron-LM, supported our research work.
Following the recipe proposed by K2 [15] and a similar experience in RadiologyGPT [55], we did the SFT in two stages. For the first stage, we aligned the model with humans via Alpaca instruction tuning data, while using the GeoSignal v2 in the second stage. The training curve of SFT on Alpaca is Figure 8 whole the SFT on GeoSignal is Figure 9
Moreover, we compare the variety of the instruction tuning data of Dolly and GeoSignal in Figure 11, showing that the general domain instructions dataset has less variety than the knowledge-intensive instructions dataset.</p>
<h1>5.3 Tool Learning</h1>
<p>In addition, LLMs can be designed to interact with and learn from various tools, such as browsers, databases, or other software interfaces. This allows the model to perform more complex tasks that require external information or specific functionalities.</p>
<p>We leveraged the ToolBench dataset [56], an open-source resource, to enable geoscientific large-scale models to leverage tool API capabilities. We sampled five types of tool QA data from ToolBench, namely arxiv, bing_search, database, weather, and wolframalpha, and supplemented it with our collected geo_search data, resulting in approximately 10k training samples. We open-source this dataset on https://github.com/zthang/geotools.
During the SFT stage, we trained the models together with training data such as alpaca. As the training samples from the tool data tend to be longer, we set the max_length to be 2048. During training, we only calculate the loss and backpropagate the gradients for the part of the API call, specifically the thought, action, action input, and the corresponding tokens for the final answer.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Training curve during the tools SFT.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Variety of the instruction tuning data in Dolly and GeoSignal.</p>
<p>Once the model is trained, we specify the tool's description and corresponding API parameter instructions in the prompt. For a given question, we first let the model output the related API call (thought, action, action input) to obtain the results returned by the external tool. These results are then used as observations and fed back into the model, generating a new set of thought, action, and action input for the next iteration (if further tool calls are required). This process continues until the model gathers enough information and outputs the final answer.</p>
<p>Here are two naive examples of how the Galactica-30b models use the tool. The detailed examples are shown in the Appendix:</p>
<h1>- Example 1:</h1>
<p>Question: "What is the weather in New York 3M years ago?"
Thought: "weather"
Action: "geo_search"
Action Input: "New York, Weather, 3M years"</p>
<h2>- Example 2</h2>
<p>Question: "What is the definition of plate tectonics?"
Thought: "arxiv"
Action: "search"
Action Input: "query: plate tectonics"</p>
<h1>6 Evaluation</h1>
<p>Once we have completed the model's training, we proceed to examine its grasp of scientific and geoscientific knowledge. We have divided the evaluation into two parts.</p>
<ul>
<li>The first part involves automated evaluation using the GeoBench provided by K2. This enables us to assess the model's performance in handling geoscientific tasks. Additionally, to examine if the newly learned knowledge has affected the pre-existing ability, we conducted MMLU (Minimal Meaningful Learning Units) tests. These tests are compared against the original Galactica model.</li>
<li>The second part encompasses manual evaluation, where we carefully selected several subtasks from geoscience. For this evaluation, we invited 10 researchers specializing in geoscience to participate in voting and scoring. Ultimately, we compare the model's performance with five other large-scale platforms in open testing.</li>
</ul>
<p>By conducting these evaluations, we aim to comprehensively assess the model's abilities and compare its performance against automated benchmarks and human assessments, ensuring its competence in scientific and geoscientific domains.</p>
<h3>6.1 Automatic Evaluation</h3>
<h3>6.1.1 GeoBench</h3>
<p>GeoBench, proposed by [15] is a benchmarking tool specifically designed to evaluate and test the geoscientific understanding and capabilities of LLMs. It focuses on assessing how well LLMs can process and generate responses involving geographic and geological information.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Baselines</th>
<th style="text-align: center;">NPEE</th>
<th style="text-align: center;">APTest</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">Gal-6.7B</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-7B</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">27.6</td>
</tr>
<tr>
<td style="text-align: left;">K2-7B</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">Gal-30B</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">$\underline{38.5}$</td>
</tr>
<tr>
<td style="text-align: left;">GalAlp-30B</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">$\mathbf{4 4 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">GEOGalactica-30B</td>
<td style="text-align: center;">$\underline{46.6}$</td>
<td style="text-align: center;">36.9</td>
</tr>
</tbody>
</table>
<p>Table 5: comparison among baselines on Objective tasks in GeoBench.</p>
<p>Through testing our models on GeoBench, we have observed that larger and more academic models outperform benchmarks like NPEE, which are inclined toward academic research. However, they do not perform well in benchmarks like AP Study, which lean more towards foundational education. This difference may be caused by training materials that guide the model to contemplate more advanced knowledge. The training data consists of academic research achievements, namely papers, which may result in a deviation from and lack of basic knowledge. This is an area we intend to focus on for improvement in the future.
It is worth noting that Galactica, with 30 billion parameters, often fails to outperform Llama, with 7 billion parameters, in general benchmark tasks. However, in our GeoBench, we have successfully developed GEOGalactica, which builds upon Galactica, surpassing K2, built upon Llama.</p>
<h3>6.1.2 MMLU</h3>
<p>The MMLU has been divided into math and non-math sections by Galactica, and we have been following their reports closely. From the results (Shown in Table 6), it is evident that after processing 6 million geoscience-related literature documents, specific skills of the model, such as algebra, biology, chemistry, and mathematics, have shown improvement. This phenomenon appears to be linked to papers focusing on mathematical geology, biological geoscience, and chemical geology, highlighting the interdisciplinary nature of geoscience. Surprisingly, machine learning has experienced significant enhancement, likely due to the inclusion of GitHub code in our corpus. In summary, subjects closely related to geoscience, including those logically connected to geology and its subfields, have shown notable progress. However, disciplines like physics indicate that the original Galactica outperforms our GEOGalactica and subjects unrelated to geosciences, such as medical genetics, medicine, and electrical engineering, have shown a decline in performance. It is noteworthy that GEOGalactica and the original Galactica are generally at a similar stage regarding average performance in math-related subjects within the MMLU.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: center;">GEOGalactica30B</th>
<th style="text-align: center;">GAL 30B</th>
<th style="text-align: center;">GalAlp 30B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Abstract Algebra</td>
<td style="text-align: center;">$\mathbf{0 . 3 0 0}$</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.320</td>
</tr>
<tr>
<td style="text-align: left;">Astronomy</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 0}$</td>
<td style="text-align: center;">0.474</td>
</tr>
<tr>
<td style="text-align: left;">College Biology</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 6}$</td>
<td style="text-align: center;">0.514</td>
</tr>
<tr>
<td style="text-align: left;">College Chemistry</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 0}$</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.350</td>
</tr>
<tr>
<td style="text-align: left;">College Computer Science</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 0}$</td>
<td style="text-align: center;">0.370</td>
</tr>
<tr>
<td style="text-align: left;">College Mathematics</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">$\mathbf{0 . 3 5 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;">College Medicine</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 0}$</td>
<td style="text-align: center;">0.445</td>
</tr>
<tr>
<td style="text-align: left;">College Physics</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 3}$</td>
<td style="text-align: center;">0.294</td>
</tr>
<tr>
<td style="text-align: left;">Econometrics</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 7}$</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.368</td>
</tr>
<tr>
<td style="text-align: left;">Electrical Engineering</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 9}$</td>
<td style="text-align: center;">0.503</td>
</tr>
<tr>
<td style="text-align: left;">Elementary Mathematics</td>
<td style="text-align: center;">$\mathbf{0 . 3 2 8}$</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.288</td>
</tr>
<tr>
<td style="text-align: left;">Formal Logic</td>
<td style="text-align: center;">$\mathbf{0 . 3 0 2}$</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.278</td>
</tr>
<tr>
<td style="text-align: left;">High School Biology</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 5}$</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.535</td>
</tr>
<tr>
<td style="text-align: left;">High School Chemistry</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">$\mathbf{0 . 3 9 9}$</td>
<td style="text-align: center;">0.355</td>
</tr>
<tr>
<td style="text-align: left;">High School Computer Science</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">High School Mathematics</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 1}$</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.304</td>
</tr>
<tr>
<td style="text-align: left;">High School Physics</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 4}$</td>
<td style="text-align: center;">0.325</td>
</tr>
<tr>
<td style="text-align: left;">High School Statistics</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">$\mathbf{0 . 3 5 2}$</td>
<td style="text-align: center;">0.319</td>
</tr>
<tr>
<td style="text-align: left;">Machine Learning</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 1}$</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.366</td>
</tr>
<tr>
<td style="text-align: left;">Medical Genetics</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 0}$</td>
<td style="text-align: center;">0.520</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 5 8 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 8 9 4}$</td>
</tr>
</tbody>
</table>
<p>Table 6: We report the results of the three models in math.</p>
<p>After assessing the mathematical subject, we examined the results of the subjects that were excluded. Overall, GEOGalactica performs slightly better than the original Galactica in the average of non-math-related subjects in MMLU. Interestingly, subjects like global facts, US History, and World History have significantly improved compared to the original Galactica. This phenomenon can be attributed to the fact that many aspects of history, such as significant discoveries and political knowledge, are closely intertwined with geoscience. This underscores the significance of geoscience, which can profoundly influence global progress.
Furthermore, in conceptual physics, learning from geoscience papers has led to a better understanding of the model. This suggests that several concepts in geoscience do not align with the knowledge taught in colleges and high schools. Consequently, models struggle to apply this related knowledge when solving problems at the college and high school levels.</p>
<p>Observation on ablation Fortunately, we came across Galpaca-30B on Hugging Face ${ }^{27}$, which significantly reduced the carbon emissions from our finetuning experiments. This model utilized Alpaca's instructions to learn from the dataset and was applied to SFT on Galactica-30B. Upon horizontal comparison, Galpaca-30B performed notably worse than the original Galactica and GEOGalactica in the majority of disciplines. This indicates that instruction learning in the general domain can significantly impact the performance of specialized domain models during practical evaluations.</p>
<h1>6.2 Human Evaluation</h1>
<p>In this part, we have selected five open models to evaluate together with our GEOGalactica model. These models include:</p>
<ol>
<li>MOSS, an open-source tool-augmented conversational language model, was released by Qiu Xipeng's team from the School of Computer Science at Fudan University as a ChatGPT-like model.</li>
<li>Qwen is a chatbot developed by Alibaba Cloud, a technology company under the Alibaba Group. Alibaba announced its intention to open Tongyi Qianwen to the public, indicating its readiness for the market and reflecting China's growing focus on AI technology.</li>
<li>ChatGPT is an AI language model developed by OpenAI, known for its ability to generate human-like text based on prompts, facilitate engaging conversations, answer questions, and perform a wide range of language-related tasks. ${ }^{28}$
<sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{27}$ https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b
${ }^{28}$ We use the 2023 March version of ChatGPT.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
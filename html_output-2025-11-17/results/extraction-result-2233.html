<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2233 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2233</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2233</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-280421368</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.00917v1.pdf" target="_blank">A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles</a></p>
                <p><strong>Paper Abstract:</strong> Connected autonomous vehicles (CAVs) must simultaneously perform multiple tasks, such as object detection, semantic segmentation, depth estimation, trajectory prediction, motion prediction, and behaviour prediction, to ensure safe and reliable navigation in complex environments. Vehicle-to-everything (V2X) communication enables cooperative driving among CAVs, thereby mitigating the limitations of individual sensors, reducing occlusions, and improving perception over long distances. Traditionally, these tasks are addressed using distinct models, which leads to high deployment costs, increased computational overhead, and challenges in achieving real-time performance. Multi-task learning (MTL) has recently emerged as a promising solution that enables the joint learning of multiple tasks within a single unified model. This offers improved efficiency and resource utilization. To the best of our knowledge, this survey is the first comprehensive review focused on MTL in the context of CAVs. We begin with an overview of CAVs and MTL to provide foundational background. We then explore the application of MTL across key functional modules, including perception, prediction, planning, control, and multi-agent collaboration. Finally, we discuss the strengths and limitations of existing methods, identify key research gaps, and provide directions for future research aimed at advancing MTL methodologies for CAV systems.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2233.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2233.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-stitch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-stitch networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A soft-parameter sharing mechanism that learns linear combinations of task-specific activations to adaptively control the degree of feature sharing between tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cross-stitch networks for multi-task learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cross-stitch networks</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each task keeps its own parameters but intermediate activations are linearly combined via learnable cross-stitch units that determine how much information to share between task-specific feature maps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>learnable linear combination of task-specific activations (cross-stitch units) enabling selective sharing</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task learning (vision: detection/segmentation/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Mechanism provides an explicit parameterization of inter-task sharing, improving interpretability of what is shared versus task-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Cross-stitch units provide a trainable, task-aligned sharing mechanism that automatically adjusts sharing degrees between tasks, helping to mitigate negative transfer compared to naive full sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2233.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VE-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Exemplar Driven Task-Prompting (VE-Prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM/prompting-based hybrid MTL framework that generates task-specific visual prompts from exemplars (via a fixed CLIP encoder) and injects them into a shared Transformer+CNN backbone to learn stronger task-specific representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual exemplar driven task-prompting for unified perception in autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VE-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image encoder + lightweight shared transformer encoder, plus a prompt generator (CLIP image encoder) that extracts task-specific visual exemplars to create prompts; a task-prompting block integrates prompts with shared features and task-specific heads produce outputs for multiple perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>task-specific prompts derived from visual exemplars + attention-based fusion; hybrid shared backbone with task prompts</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>unified perception for autonomous driving (object detection, semantic segmentation, drivable area, lane detection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Survey reports improved generalization and reduced negative transfer via language/visual priors (LV-Adapter / VE-Prompt) but does not provide numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Task prompts provide an interpretable interface encoding task-specific priors from exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported qualitatively to alleviate negative transfer across multiple perception tasks; specific numeric multi-task scores not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Using task-specific prompts (visual or linguistic priors) guides shared models to learn more effective task-aligned representations and reduces negative transfer compared to plain shared backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2233.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LV-Adapter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LV-Adapter (language priors from CLIP for MTL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight adapter that injects linguistic priors from CLIP into multi-task models via learned task-specific prompts during an adapt-finetune pipeline to boost MTL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Effective adaptation in multi-task co-training for unified autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LV-Adapter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adapter modules that leverage CLIP-derived language/visual priors and task-specific prompts to improve downstream MTL tasks without large retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>task-specific prompts/adapters using CLIP language/visual embeddings; prompt-based conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision multi-task perception in autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Survey reports pre-train-adapt-finetune paradigm significantly boosts performance without increasing training overhead (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Reported improved transfer/generalization via language priors; no numeric metrics in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Qualitatively improves multi-task performance and reduces negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Leveraging language/visual priors as task prompts improves multi-task adaptation and generalization compared to vanilla fine-tuning of shared models.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2233.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMT-Net</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform Multi-Task Network (UMT-Net)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid-parameter sharing MTL architecture with a shared encoder, task-specific self-attention encoders/decoders, and a joint-attention fusion module enabling adaptive cross-task communication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UMT-Net</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shared encoder for global features, task-specific self-attention encoders/decoders and a joint-attention fusion module to enable selective cross-task information flow while maintaining parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>task-specific self-attention encoders/decoders + joint-attention fusion (hybrid sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multitask perception for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Designed to adaptively refine features per task and exchange task information; survey states it improves cross-task communication without precise numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Hybrid architectures combining shared backbones with task-specific attention and fusion allow adaptive task-aligned refinements while controlling parameter growth.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2233.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse U-PDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse U-PDP (Unified Multi-task Panoptic Driving Perception)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid Transformer-based panoptic perception model that uses dynamic convolution kernels and a unified decoder to enable task-specific sampling and interactions across detection/segmentation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sparse U-PDP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Backbone + multi-scale feature encoder + kernel-based unified decoder; dynamic convolution kernels are input to Transformer blocks enabling task-specific sampling and inter-task interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>dynamic convolution kernels + Transformer attention enabling task-specific feature sampling</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>panoptic driving perception (detection, segmentation, lane/drivable area)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Designed to reduce computation by processing last four scales for detection head; survey mentions tradeoffs but no precise FLOPs/timings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Survey reports improved task interactions and robustness through dynamic kernels; no numeric comparisons in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Dynamic kernel-based decoders allow per-task sampling/focus inside a unified decoder, providing task-aligned representations within a single architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2233.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal MMoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Multi-Gate Mixture-of-Experts (Yuan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gating-based mixture-of-experts model that routes shared temporal features to specialized expert subnetworks via adaptive gates to jointly predict trajectories and driving intentions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A temporal multi-gate mixture-of-experts approach for vehicle trajectory and driving intention prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Temporal Multi-Gate Mixture-of-Experts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shared temporal features are adaptively routed by multiple gates to different expert LSTM/temporal subnetworks; auxiliary uncertainty weighting balances task losses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>mixture-of-experts with adaptive gating (temporal routing)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>trajectory forecasting and driving intention prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Gating provides an interpretable routing mechanism showing which experts handle which temporal patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported to jointly improve trajectory and intention prediction robustness compared to simpler shared baselines (survey summary; no numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adaptive MoE-style temporal routing enables specialization for distinct prediction tasks, improving joint prediction robustness versus fully uniform temporal encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2233.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M3Net</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>M3Net: Multimodal Multi-Task Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal MTL model that integrates LiDAR and camera inputs into BEV and uses modality-adaptive feature integration plus task-oriented channel scaling to produce per-task BEV outputs (3D detection, segmentation, occupancy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>M3net: Multimodal multi-task learning for 3d detection, segmentation, and occupancy prediction in autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M3Net</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generates modality-adapted channel-wise attention weights per sensor, fuses adapted features into unified BEV; task-oriented channel scaling module predicts task-specific channel scaling weights from shared BEV features to reduce gradient conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>modality-adaptive channel attention + task-oriented channel scaling (dynamic per-task channel weights)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>3D detection, BEV map segmentation, occupancy prediction (multimodal perception & prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Survey states M3Net yields outstanding performance compared to single-task models (qualitative; no numeric metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Designed to alleviate gradient conflicts and improve overall multi-task performance; survey reports strong aggregate performance vs single-task baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Modality-adaptive integration and task-oriented channel scaling create task-aligned, dynamic feature re-weighting that mitigates inter-task gradient conflicts and improves multi-task outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2233.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PedCMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PedCMT (Pedestrian Cross-Modal Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A soft-parameter sharing cross-modal Transformer that fuses bbox coordinates and ego-speed with intra- and cross-modal self-attention, a bottleneck feature fusion, and uncertainty-aware loss balancing for joint crossing intention and bounding-box prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The architecture of PedCMT (from [205])</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PedCMT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each input modality is encoded independently, then IMSA and CMSA capture intra-modal and cross-modal dependencies; a bottleneck fusion merges modalities and an uncertainty-aware mechanism balances tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>cross-modal Transformer attention + bottleneck fusion + uncertainty-based dynamic loss weighting</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>pedestrian crossing intention prediction and final bounding box prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Survey reports PedCMT achieves competitive performance (no numeric metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Joint modeling of intention and bbox prediction yields robust multi-task performance according to survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Cross-modal attention and uncertainty-based weighting produce adaptive task-specific emphasis and competitive joint performance for pedestrian prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2233.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniAD (Unified Autonomous Driving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning-oriented unified architecture that implements perception, prediction, and planning in a single Transformer-based pipeline using query representations, with ablations showing auxiliary prediction tasks improve planning safety.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniAD</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based query design for perception and prediction; TrackFormer/MapFormer/MotionFormer produce queries which feed into a planner; single-pass multi-task design aimed at planning improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>query-based Transformer representations with shared queries and task-specific decoders (planner-oriented shared attention)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>perception + prediction + planning in autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Survey reports single-pass unified model saves inference cost versus separate modules (qualitative); no FLOPs/latency numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Ablations show jointly training motion forecasting and occupancy prediction reduces collision rates and trajectory errors (survey-level statement; no numeric values).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Jointly optimizing prediction tasks with planning in a shared Transformer pipeline improves planning safety (reduced collisions/trajectory error) and reduces redundant computation.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2233.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>YOLOP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>You Only Look Once for Panoptic driving perception (YOLOP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real-time hard-parameter sharing MTL network: shared CSPDarknet backbone with FPN/SPP neck and separate heads for object detection, drivable area segmentation, and lane segmentation, representing a uniform shared-representation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>You only look once for panoptic driving perception</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>YOLOP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>One-stage YOLO-based architecture with a single shared encoder and multiple task-specific output heads (hard-parameter sharing / uniform shared representations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>hard-parameter sharing (uniform shared backbone + task heads)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>panoptic driving perception (detection, drivable area, lanes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Survey describes YOLOP as real-time; specific FPS/FLOPs not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Hard-sharing is computationally efficient but susceptible to negative transfer when tasks are less related.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Designed for real-time in-vehicle deployment; survey notes computational advantages of hard sharing under resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Hard-parameter (uniform) sharing provides high computational efficiency suitable for resource-constrained CAVs but risks negative transfer when task relationships are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2233.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MultiNet (early hard-sharing MTL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early hard-parameter sharing MTL architecture combining a shared encoder with task-specific decoders for classification, detection, and segmentation, demonstrated to run at real-time speeds (>23 FPS reported by survey).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multinet: Real-time joint semantic reasoning for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MultiNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shared encoder feeding multiple task heads (classification, detection, segmentation) in a single unified network optimized for real-time inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>hard/fixed shared representation (uniform shared backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multitask perception for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Survey reports MultiNet achieves >23 FPS (explicit number in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Demonstrates that uniform hard-sharing yields strong efficiency for related tasks but can be sensitive to task conflict.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Survey highlights its suitability for real-time constrained deployment (>23 FPS).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Uniform hard-sharing architectures (e.g., MultiNet) are computationally efficient and enable real-time inference but can suffer from negative transfer and task conflicts when tasks are heterogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2233.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2233.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>V2XPnP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>V2XPnP (Vehicle-to-Everything Spatio-Temporal Fusion for Perception & Prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A spatio-temporal multi-agent cooperative framework that aggregates historical BEV features into a compact spatio-temporally enriched representation for transmission and uses temporal attention and multi-agent attention modules to fuse across agents and time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>V2XPnP: Vehicle-to-everything spatiotemporal fusion for multi-agent perception and prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>V2XPnP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregates temporally enriched BEV features into compact representations for V2X sharing; employs temporal attention, self-spatial attention, multi-agent spatial attention, and map attention to capture dependencies across frames/agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>compact temporally aggregated BEV representations + attention-based temporal/spatial/multi-agent fusion</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>cooperative perception and prediction in V2X multi-agent driving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Designed to reduce communication cost by sharing compact temporally enriched features; survey emphasizes communication-efficiency gains qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Enables spatio-temporal multi-task fusion across agents and improves robustness to occlusion/partial observability (qualitative report).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Addresses communication constraints by compacting temporal features before transmission; precise bandwidth/latency numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Temporal aggregation into compact shared representations plus attention-driven fusion provides a dynamic task-aligned representation that improves cooperative perception/prediction under communication constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cross-stitch networks for multi-task learning <em>(Rating: 2)</em></li>
                <li>Visual exemplar driven task-prompting for unified perception in autonomous driving <em>(Rating: 2)</em></li>
                <li>Effective adaptation in multi-task co-training for unified autonomous driving <em>(Rating: 2)</em></li>
                <li>M3net: Multimodal multi-task learning for 3d detection, segmentation, and occupancy prediction in autonomous driving <em>(Rating: 2)</em></li>
                <li>Pedestrian crossing intention prediction based on cross-modal transformer and uncertainty-aware multi-task learning for autonomous driving <em>(Rating: 2)</em></li>
                <li>A temporal multi-gate mixture-of-experts approach for vehicle trajectory and driving intention prediction <em>(Rating: 2)</em></li>
                <li>You only look once for panoptic driving perception <em>(Rating: 1)</em></li>
                <li>Multinet: Real-time joint semantic reasoning for autonomous driving <em>(Rating: 1)</em></li>
                <li>V2XPnP: Vehicle-to-everything spatiotemporal fusion for multi-agent perception and prediction <em>(Rating: 2)</em></li>
                <li>Sparse U-PDP: A unified multi-task framework for panoptic driving perception <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2233",
    "paper_id": "paper-280421368",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "Cross-stitch",
            "name_full": "Cross-stitch networks",
            "brief_description": "A soft-parameter sharing mechanism that learns linear combinations of task-specific activations to adaptively control the degree of feature sharing between tasks.",
            "citation_title": "Cross-stitch networks for multi-task learning",
            "mention_or_use": "mention",
            "model_name": "Cross-stitch networks",
            "model_description": "Each task keeps its own parameters but intermediate activations are linearly combined via learnable cross-stitch units that determine how much information to share between task-specific feature maps.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "learnable linear combination of task-specific activations (cross-stitch units) enabling selective sharing",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task learning (vision: detection/segmentation/etc.)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": null,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": "Mechanism provides an explicit parameterization of inter-task sharing, improving interpretability of what is shared versus task-specific.",
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Cross-stitch units provide a trainable, task-aligned sharing mechanism that automatically adjusts sharing degrees between tasks, helping to mitigate negative transfer compared to naive full sharing.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.0"
        },
        {
            "name_short": "VE-Prompt",
            "name_full": "Visual Exemplar Driven Task-Prompting (VE-Prompt)",
            "brief_description": "A VLM/prompting-based hybrid MTL framework that generates task-specific visual prompts from exemplars (via a fixed CLIP encoder) and injects them into a shared Transformer+CNN backbone to learn stronger task-specific representations.",
            "citation_title": "Visual exemplar driven task-prompting for unified perception in autonomous driving",
            "mention_or_use": "mention",
            "model_name": "VE-Prompt",
            "model_description": "Image encoder + lightweight shared transformer encoder, plus a prompt generator (CLIP image encoder) that extracts task-specific visual exemplars to create prompts; a task-prompting block integrates prompts with shared features and task-specific heads produce outputs for multiple perception tasks.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "task-specific prompts derived from visual exemplars + attention-based fusion; hybrid shared backbone with task prompts",
            "is_dynamic_or_adaptive": true,
            "task_domain": "unified perception for autonomous driving (object detection, semantic segmentation, drivable area, lane detection)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Survey reports improved generalization and reduced negative transfer via language/visual priors (LV-Adapter / VE-Prompt) but does not provide numeric metrics.",
            "interpretability_results": "Task prompts provide an interpretable interface encoding task-specific priors from exemplars.",
            "multi_task_performance": "Reported qualitatively to alleviate negative transfer across multiple perception tasks; specific numeric multi-task scores not reported in survey.",
            "resource_constrained_results": null,
            "key_finding_summary": "Using task-specific prompts (visual or linguistic priors) guides shared models to learn more effective task-aligned representations and reduces negative transfer compared to plain shared backbones.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.1"
        },
        {
            "name_short": "LV-Adapter",
            "name_full": "LV-Adapter (language priors from CLIP for MTL)",
            "brief_description": "A lightweight adapter that injects linguistic priors from CLIP into multi-task models via learned task-specific prompts during an adapt-finetune pipeline to boost MTL performance.",
            "citation_title": "Effective adaptation in multi-task co-training for unified autonomous driving",
            "mention_or_use": "mention",
            "model_name": "LV-Adapter",
            "model_description": "Adapter modules that leverage CLIP-derived language/visual priors and task-specific prompts to improve downstream MTL tasks without large retraining.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "task-specific prompts/adapters using CLIP language/visual embeddings; prompt-based conditioning",
            "is_dynamic_or_adaptive": true,
            "task_domain": "vision multi-task perception in autonomous driving",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Survey reports pre-train-adapt-finetune paradigm significantly boosts performance without increasing training overhead (qualitative).",
            "transfer_generalization_results": "Reported improved transfer/generalization via language priors; no numeric metrics in survey.",
            "interpretability_results": null,
            "multi_task_performance": "Qualitatively improves multi-task performance and reduces negative transfer.",
            "resource_constrained_results": null,
            "key_finding_summary": "Leveraging language/visual priors as task prompts improves multi-task adaptation and generalization compared to vanilla fine-tuning of shared models.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.2"
        },
        {
            "name_short": "UMT-Net",
            "name_full": "Uniform Multi-Task Network (UMT-Net)",
            "brief_description": "A hybrid-parameter sharing MTL architecture with a shared encoder, task-specific self-attention encoders/decoders, and a joint-attention fusion module enabling adaptive cross-task communication.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "UMT-Net",
            "model_description": "Shared encoder for global features, task-specific self-attention encoders/decoders and a joint-attention fusion module to enable selective cross-task information flow while maintaining parameter efficiency.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "task-specific self-attention encoders/decoders + joint-attention fusion (hybrid sharing)",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multitask perception for autonomous driving",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Designed to adaptively refine features per task and exchange task information; survey states it improves cross-task communication without precise numbers.",
            "resource_constrained_results": null,
            "key_finding_summary": "Hybrid architectures combining shared backbones with task-specific attention and fusion allow adaptive task-aligned refinements while controlling parameter growth.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.3"
        },
        {
            "name_short": "Sparse U-PDP",
            "name_full": "Sparse U-PDP (Unified Multi-task Panoptic Driving Perception)",
            "brief_description": "A hybrid Transformer-based panoptic perception model that uses dynamic convolution kernels and a unified decoder to enable task-specific sampling and interactions across detection/segmentation tasks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Sparse U-PDP",
            "model_description": "Backbone + multi-scale feature encoder + kernel-based unified decoder; dynamic convolution kernels are input to Transformer blocks enabling task-specific sampling and inter-task interactions.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "dynamic convolution kernels + Transformer attention enabling task-specific feature sampling",
            "is_dynamic_or_adaptive": true,
            "task_domain": "panoptic driving perception (detection, segmentation, lane/drivable area)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "Designed to reduce computation by processing last four scales for detection head; survey mentions tradeoffs but no precise FLOPs/timings.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Survey reports improved task interactions and robustness through dynamic kernels; no numeric comparisons in survey.",
            "resource_constrained_results": null,
            "key_finding_summary": "Dynamic kernel-based decoders allow per-task sampling/focus inside a unified decoder, providing task-aligned representations within a single architecture.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.4"
        },
        {
            "name_short": "Temporal MMoE",
            "name_full": "Temporal Multi-Gate Mixture-of-Experts (Yuan et al.)",
            "brief_description": "A gating-based mixture-of-experts model that routes shared temporal features to specialized expert subnetworks via adaptive gates to jointly predict trajectories and driving intentions.",
            "citation_title": "A temporal multi-gate mixture-of-experts approach for vehicle trajectory and driving intention prediction",
            "mention_or_use": "mention",
            "model_name": "Temporal Multi-Gate Mixture-of-Experts",
            "model_description": "Shared temporal features are adaptively routed by multiple gates to different expert LSTM/temporal subnetworks; auxiliary uncertainty weighting balances task losses.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "mixture-of-experts with adaptive gating (temporal routing)",
            "is_dynamic_or_adaptive": true,
            "task_domain": "trajectory forecasting and driving intention prediction",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": "Gating provides an interpretable routing mechanism showing which experts handle which temporal patterns.",
            "multi_task_performance": "Reported to jointly improve trajectory and intention prediction robustness compared to simpler shared baselines (survey summary; no numbers).",
            "resource_constrained_results": null,
            "key_finding_summary": "Adaptive MoE-style temporal routing enables specialization for distinct prediction tasks, improving joint prediction robustness versus fully uniform temporal encodings.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.5"
        },
        {
            "name_short": "M3Net",
            "name_full": "M3Net: Multimodal Multi-Task Network",
            "brief_description": "A multimodal MTL model that integrates LiDAR and camera inputs into BEV and uses modality-adaptive feature integration plus task-oriented channel scaling to produce per-task BEV outputs (3D detection, segmentation, occupancy).",
            "citation_title": "M3net: Multimodal multi-task learning for 3d detection, segmentation, and occupancy prediction in autonomous driving",
            "mention_or_use": "mention",
            "model_name": "M3Net",
            "model_description": "Generates modality-adapted channel-wise attention weights per sensor, fuses adapted features into unified BEV; task-oriented channel scaling module predicts task-specific channel scaling weights from shared BEV features to reduce gradient conflicts.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "modality-adaptive channel attention + task-oriented channel scaling (dynamic per-task channel weights)",
            "is_dynamic_or_adaptive": true,
            "task_domain": "3D detection, BEV map segmentation, occupancy prediction (multimodal perception & prediction)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Survey states M3Net yields outstanding performance compared to single-task models (qualitative; no numeric metrics provided).",
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Designed to alleviate gradient conflicts and improve overall multi-task performance; survey reports strong aggregate performance vs single-task baselines.",
            "resource_constrained_results": null,
            "key_finding_summary": "Modality-adaptive integration and task-oriented channel scaling create task-aligned, dynamic feature re-weighting that mitigates inter-task gradient conflicts and improves multi-task outcomes.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.6"
        },
        {
            "name_short": "PedCMT",
            "name_full": "PedCMT (Pedestrian Cross-Modal Transformer)",
            "brief_description": "A soft-parameter sharing cross-modal Transformer that fuses bbox coordinates and ego-speed with intra- and cross-modal self-attention, a bottleneck feature fusion, and uncertainty-aware loss balancing for joint crossing intention and bounding-box prediction.",
            "citation_title": "The architecture of PedCMT (from [205])",
            "mention_or_use": "mention",
            "model_name": "PedCMT",
            "model_description": "Each input modality is encoded independently, then IMSA and CMSA capture intra-modal and cross-modal dependencies; a bottleneck fusion merges modalities and an uncertainty-aware mechanism balances tasks.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "cross-modal Transformer attention + bottleneck fusion + uncertainty-based dynamic loss weighting",
            "is_dynamic_or_adaptive": true,
            "task_domain": "pedestrian crossing intention prediction and final bounding box prediction",
            "performance_task_aligned": "Survey reports PedCMT achieves competitive performance (no numeric metrics provided).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Joint modeling of intention and bbox prediction yields robust multi-task performance according to survey summary.",
            "resource_constrained_results": null,
            "key_finding_summary": "Cross-modal attention and uncertainty-based weighting produce adaptive task-specific emphasis and competitive joint performance for pedestrian prediction tasks.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.7"
        },
        {
            "name_short": "UniAD",
            "name_full": "UniAD (Unified Autonomous Driving)",
            "brief_description": "A planning-oriented unified architecture that implements perception, prediction, and planning in a single Transformer-based pipeline using query representations, with ablations showing auxiliary prediction tasks improve planning safety.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "UniAD",
            "model_description": "Transformer-based query design for perception and prediction; TrackFormer/MapFormer/MotionFormer produce queries which feed into a planner; single-pass multi-task design aimed at planning improvement.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "query-based Transformer representations with shared queries and task-specific decoders (planner-oriented shared attention)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "perception + prediction + planning in autonomous driving",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Survey reports single-pass unified model saves inference cost versus separate modules (qualitative); no FLOPs/latency numbers provided.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Ablations show jointly training motion forecasting and occupancy prediction reduces collision rates and trajectory errors (survey-level statement; no numeric values).",
            "resource_constrained_results": null,
            "key_finding_summary": "Jointly optimizing prediction tasks with planning in a shared Transformer pipeline improves planning safety (reduced collisions/trajectory error) and reduces redundant computation.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.8"
        },
        {
            "name_short": "YOLOP",
            "name_full": "You Only Look Once for Panoptic driving perception (YOLOP)",
            "brief_description": "A real-time hard-parameter sharing MTL network: shared CSPDarknet backbone with FPN/SPP neck and separate heads for object detection, drivable area segmentation, and lane segmentation, representing a uniform shared-representation approach.",
            "citation_title": "You only look once for panoptic driving perception",
            "mention_or_use": "mention",
            "model_name": "YOLOP",
            "model_description": "One-stage YOLO-based architecture with a single shared encoder and multiple task-specific output heads (hard-parameter sharing / uniform shared representations).",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "hard-parameter sharing (uniform shared backbone + task heads)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "panoptic driving perception (detection, drivable area, lanes)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "Survey describes YOLOP as real-time; specific FPS/FLOPs not provided in survey.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Hard-sharing is computationally efficient but susceptible to negative transfer when tasks are less related.",
            "resource_constrained_results": "Designed for real-time in-vehicle deployment; survey notes computational advantages of hard sharing under resource constraints.",
            "key_finding_summary": "Hard-parameter (uniform) sharing provides high computational efficiency suitable for resource-constrained CAVs but risks negative transfer when task relationships are weak.",
            "supports_or_challenges_theory": "mixed",
            "uuid": "e2233.9"
        },
        {
            "name_short": "MultiNet",
            "name_full": "MultiNet (early hard-sharing MTL)",
            "brief_description": "An early hard-parameter sharing MTL architecture combining a shared encoder with task-specific decoders for classification, detection, and segmentation, demonstrated to run at real-time speeds (&gt;23 FPS reported by survey).",
            "citation_title": "Multinet: Real-time joint semantic reasoning for autonomous driving",
            "mention_or_use": "mention",
            "model_name": "MultiNet",
            "model_description": "Shared encoder feeding multiple task heads (classification, detection, segmentation) in a single unified network optimized for real-time inference.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "hard/fixed shared representation (uniform shared backbone)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "multitask perception for autonomous driving",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "Survey reports MultiNet achieves &gt;23 FPS (explicit number in survey).",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Demonstrates that uniform hard-sharing yields strong efficiency for related tasks but can be sensitive to task conflict.",
            "resource_constrained_results": "Survey highlights its suitability for real-time constrained deployment (&gt;23 FPS).",
            "key_finding_summary": "Uniform hard-sharing architectures (e.g., MultiNet) are computationally efficient and enable real-time inference but can suffer from negative transfer and task conflicts when tasks are heterogeneous.",
            "supports_or_challenges_theory": "mixed",
            "uuid": "e2233.10"
        },
        {
            "name_short": "V2XPnP",
            "name_full": "V2XPnP (Vehicle-to-Everything Spatio-Temporal Fusion for Perception & Prediction)",
            "brief_description": "A spatio-temporal multi-agent cooperative framework that aggregates historical BEV features into a compact spatio-temporally enriched representation for transmission and uses temporal attention and multi-agent attention modules to fuse across agents and time.",
            "citation_title": "V2XPnP: Vehicle-to-everything spatiotemporal fusion for multi-agent perception and prediction",
            "mention_or_use": "mention",
            "model_name": "V2XPnP",
            "model_description": "Aggregates temporally enriched BEV features into compact representations for V2X sharing; employs temporal attention, self-spatial attention, multi-agent spatial attention, and map attention to capture dependencies across frames/agents.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "compact temporally aggregated BEV representations + attention-based temporal/spatial/multi-agent fusion",
            "is_dynamic_or_adaptive": true,
            "task_domain": "cooperative perception and prediction in V2X multi-agent driving",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "Designed to reduce communication cost by sharing compact temporally enriched features; survey emphasizes communication-efficiency gains qualitatively.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Enables spatio-temporal multi-task fusion across agents and improves robustness to occlusion/partial observability (qualitative report).",
            "resource_constrained_results": "Addresses communication constraints by compacting temporal features before transmission; precise bandwidth/latency numbers not provided.",
            "key_finding_summary": "Temporal aggregation into compact shared representations plus attention-driven fusion provides a dynamic task-aligned representation that improves cooperative perception/prediction under communication constraints.",
            "supports_or_challenges_theory": "supports",
            "uuid": "e2233.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cross-stitch networks for multi-task learning",
            "rating": 2
        },
        {
            "paper_title": "Visual exemplar driven task-prompting for unified perception in autonomous driving",
            "rating": 2
        },
        {
            "paper_title": "Effective adaptation in multi-task co-training for unified autonomous driving",
            "rating": 2
        },
        {
            "paper_title": "M3net: Multimodal multi-task learning for 3d detection, segmentation, and occupancy prediction in autonomous driving",
            "rating": 2
        },
        {
            "paper_title": "Pedestrian crossing intention prediction based on cross-modal transformer and uncertainty-aware multi-task learning for autonomous driving",
            "rating": 2
        },
        {
            "paper_title": "A temporal multi-gate mixture-of-experts approach for vehicle trajectory and driving intention prediction",
            "rating": 2
        },
        {
            "paper_title": "You only look once for panoptic driving perception",
            "rating": 1
        },
        {
            "paper_title": "Multinet: Real-time joint semantic reasoning for autonomous driving",
            "rating": 1
        },
        {
            "paper_title": "V2XPnP: Vehicle-to-everything spatiotemporal fusion for multi-agent perception and prediction",
            "rating": 2
        },
        {
            "paper_title": "Sparse U-PDP: A unified multi-task framework for panoptic driving perception",
            "rating": 2
        }
    ],
    "cost": 0.02460675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles</p>
<p>Jiayuan Wang 
Department of Electrical and Computer Engineering
University of Windsor
N9B 3P4WindsorONCanada</p>
<p>Department of Electrical and Com-puter Engineering
Queen's University
KingstonONCanada</p>
<p>Farhad Pourpanah farhad.086@gmail.com 
Department of Electrical and Computer Engineering
University of Windsor
N9B 3P4WindsorONCanada</p>
<p>Department of Electrical and Com-puter Engineering
Queen's University
KingstonONCanada</p>
<p>Q M Jonathan Wu 
Department of Electrical and Computer Engineering
University of Windsor
N9B 3P4WindsorONCanada</p>
<p>Department of Electrical and Com-puter Engineering
Queen's University
KingstonONCanada</p>
<p>Ning Zhang ning.zhang@uwindsor.ca 
Department of Electrical and Computer Engineering
University of Windsor
N9B 3P4WindsorONCanada</p>
<p>Department of Electrical and Com-puter Engineering
Queen's University
KingstonONCanada</p>
<p>Q M Jonathan 
Department of Electrical and Com-puter Engineering
Queen's University
KingstonONCanada</p>
<p>A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles
7BF0F255477782DD0BD1BF98CC777E3FMulti-task learningconnected autonomous driving systemsdeep learningcomputer vision
Connected autonomous vehicles (CAVs) must simultaneously perform multiple tasks, such as object detection, semantic segmentation, depth estimation, trajectory prediction, motion prediction, and behaviour prediction, to ensure safe and reliable navigation in complex environments.Vehicle-toeverything (V2X) communication enables cooperative driving among CAVs, thereby mitigating the limitations of individual sensors, reducing occlusions, and improving perception over long distances.Traditionally, these tasks are addressed using distinct models, which leads to high deployment costs, increased computational overhead, and challenges in achieving real-time performance.Multi-task learning (MTL) has recently emerged as a promising solution that enables the joint learning of multiple tasks within a single unified model.This offers improved efficiency and resource utilization.To the best of our knowledge, this survey is the first comprehensive review focused on MTL in the context of CAVs.We begin with an overview of CAVs and MTL to provide foundational background.We then explore the application of MTL across key functional modules, including perception, prediction, planning, control, and multi-agent collaboration.Finally, we discuss the strengths and limitations of existing methods, identify key research gaps, and provide directions for future research aimed at advancing MTL methodologies for CAV systems.</p>
<p>I. INTRODUCTION</p>
<p>C ONNECTED autonomous vehicles (CAVs) extend au- tonomous driving systems (ADS) by incorporating vehicle-to-everything (V2X) communication [1], which allows vehicles to exchange information with other vehicles, roadside infrastructure, and cloud servers [2].This connectivity enables cooperative decision-making beyond the line of sight of onboard sensors, thereby enhancing environmental awareness, safety, and traffic management [3].As a foundation of CAVs, ADS must simultaneously execute multiple tasks, including lane segmentation, object detection, estimation of distances and trajectories, and real-time longitudinal and lateral control (i.e., throttle/braking and steering), to enable safe navigation in dynamic and complex environments [4]- [7].</p>
<p>Current ADS operate either through a structured pipeline composed of perception, prediction, planning, and control modules [8] or through an end-to-end approach that directly maps sensor inputs to control commands.The modular pipelines remain the dominant paradigm due to their interpretability and compliance with safety constraints.These modules integrate diverse sensor information to enable realtime decisions in response to the current environment.The core tasks in these modules, e.g., object detection [9], [10], semantic segmentation [11], [12], trajectory prediction [13], [14], depth estimation [15], [16], motion prediction [17], [18], and behaviour prediction [19], have been addressed independently, i.e., each task requires its distinct model and architecture.Although this strategy has achieved considerable success, it faces challenges such as high model development costs, substantial computational demands, and difficulty in meeting real-time performance requirements [20], [21].</p>
<p>Environmental perception in ADS can be categorized into multi-sensor fusion [22]- [24] and camera-only methods [20], [25], [26].Multi-sensor fusion techniques utilize data from sensors such as LiDAR, radar, and cameras to construct more comprehensive scene representations.For example, Waymo driver perception system [27] integrates LiDAR, cameras, and radar data to obtain environmental information.However, these methods involve high costs and increased system complexity [28]- [30].In contrast, camera-only methods offer a cost-effective alternative with simpler integration requirements [31].By combining deep learning (DL) models with camerabased methods, rich visual information can be captured to perform perception tasks.Therefore, camera-based ADS have become attractive due to their scalability, affordability, and potential for widespread adoption [32], [33].Tesla Vision is one such system, i.e., camera-only autopilot, reported to achieve comparable or improved active safety ratings and superior pedestrian automatic emergency braking performance relative to traditional radar-based systems [34].</p>
<p>Cooperative perception enabled by V2X communication serves as a crucial augmentation to onboard sensing in CAVs [35]- [37].V2X-based systems enable a more comprehensive and unified understanding of the driving environment.This extended situational awareness is effective in mitigating the limitations of onboard sensors, especially under occlusions or at long distances.The advantages of frameworks are most pronounced in densely populated or complex urban scenarios, where visibility and sensing range are frequently constrained [38]- [41].</p>
<p>Recent advances in computer vision (CV) have accelerated the development of ADS.These systems now demonstrate improved capabilities in understanding dynamic road environments and reliably interpreting changing traffic scenes and arXiv:2508.00917v1[cs.RO] 29 Jul 2025 road conditions.Such progress holds the potential to transform transportation by enhancing urban mobility, optimizing logistics efficiency, and enabling smarter traffic management.To address these challenges, multi-task learning (MTL) offers a promising solution by integrating multiple tasks within a single unified model.MTL leverages shared computational components to improve efficiency and facilitate real-time performance, which is critical for ADS.Furthermore, the integration of multiple tasks within one model contributes to more robust and reliable predictions [42].</p>
<p>MTL [43] enhances generalization through inductive transfer by leveraging domain information from task-specific data as an inductive bias.This is achieved by parallel learning of multiple tasks, where a shared representation enables knowledge from one task to support others.In CAVs, MTL enables the integration of tasks such as object detection, semantic segmentation, lane detection, and drivable area segmentation within a single model [20], [44], [45].By sharing representations, MTL models can jointly analyze multiple tasks within the driving environment, resulting in more comprehensive predictions [15], [25].For example, the segmentation mask can provide spatial priors for object detection, while bounding boxes and category information from detection tasks can inform semantic segmentation [46].Training across multiple tasks also enables the use of diverse data sources, thereby increasing model robustness, reducing overfitting, and improving overall performance [47].Additionally, MTL improves computational efficiency.It combines multiple tasks into a single model.This reduces system complexity and lowers the need for computing resources.It is more efficient than training separate models for each task.This efficiency is especially critical for real-time CAV applications, where timely and accurate decision-making is essential for safety and operational performance [48]- [50].However, not all tasks work well together.Sharing features between unrelated tasks can degrade the performance of all tasks.This issue is known as the negative transfer phenomenon [51].Therefore, it is essential to carefully assess task relationships and consider the risk of negative transfer when designing MTL architectures.Moreover, MTL can exhibit the seesaw phenomenon, where improving the performance of one task may lead to degradation in others [52].</p>
<p>MTL studies have been conducted across various domains, including medical image analysis [53], dense prediction tasks [54], and technical frameworks such as DL [51], [55] and deep reinforcement learning [56].In addition, overviews of MTL in different settings and categories are provided in [47], [57].Ruder [51] provided one of the earliest MTL surveys in DL, which introduces two fundamental strategies: hard parameter sharing and soft parameter sharing.The survey also discussed influential studies [58]- [60] that advanced MTL in deep neural networks.Thung [55] conducted a comparative analysis of MTL algorithms, highlighting their applications in DL. [57] categorized MTL into multi-task supervised, unsupervised, and semi-supervised learning settings, and identified representative methods for each.In a later work, Zhang [47] grouped MTL approaches into five categories: feature learning, low-rank methods, task clustering, task relation learning, and decomposition-based techniques.Each category was analyzed in terms of its characteristics and use cases.Vandenhende [54] focused on an overview of MTL for dense prediction tasks in CV, while Zhao [53] examined popular MTL architectures, including cascaded, parallel, interacted, and hybrid designs, and reviewed their applications across different anatomical regions in medical imaging.Recently, Wang et al. [61] reviewed MTL in autonomous vehicles, emphasis on vision-based perception tasks.Their focus is limited to the perception module, primarily covering tasks of object detection, drivable area segmentation, and lane detection.Despite several existing reviews, none provide a focused examination of MTL specifically within the comprehensive CAVs.We aim to fill this gap.In particular, this survey explores deep MTL methods applied to the CAVs, including perception, prediction, planning, control, and multiagent collaboration.</p>
<p>We review existing research on deep MTL across CAVs from 2018 to early 2025.While we aim to provide comprehensive coverage, some recently published studies may have been inadvertently omitted.The main contributions of this review paper include: infrastructure.It supplements onboard sensing by providing access to remote information, enhancing environmental awareness, especially in scenarios with occlusions or limited sensor range.V2X modules are typically integrated into the hardware layer and interface with software components to support cooperative perception and coordinated decision-making.The following subsections provide an overview of each component.For detailed technical discussions, the readers are referred to [8], [62].</p>
<p>A. Hardware Layer</p>
<p>The hardware layer includes all physical components responsible for sensing and actuation, including sensors, computing platforms, and actuators.</p>
<p>1) Sensors: are responsible for capturing information about both the vehicle's environment and its internal states.They are generally categorized into exteroceptive and proprioceptive sensors.Exteroceptive sensors, which include LiDAR, radar, ultrasonic sensors, and cameras, are used to capture environmental information.These can be further classified into active and passive categories.Active sensors (e.g., LiDAR, radar) emit signals to detect objects, whereas passive sensors (e.g., cameras) rely on external light sources like sunlight.</p>
<p>LiDAR uses laser light to generate detailed 3D maps of the surrounding environment.It can accurately measure distance and is less affected by lighting conditions compared to cameras.Radar emits radio waves to detect the distance and speed of objects.It can reliably operate under various weather and low-light conditions [63].Ultrasonic sensors utilize sound waves to measure short-range distances.While ultrasonic sensors are cost-effective, their limited range restricts their use to short-range tasks such as parking assistance.Cameras are the most widely used sensors in ADS.They can capture 2D images with rich color and texture details [64].Because cameras do not emit signals, they avoid interfering with other systems.Table I provides a detailed comparison of exteroceptive sensors.In practical applications, ADS rely on sensor fusion to achieve comprehensive environmental perception.For instance, Tesla employs eight cameras to enable the autopilot system with 360-degree visibility up to a range of 250 meters [65], while Waymo [66] integrates one mid-range LiDAR, four short-range LiDARs, and five cameras to perceive their surroundings.</p>
<p>In contrast, proprioceptive sensors monitor a vehicle's internal state, including forces, angular rates, and positions [67].Commonly employed sensors include inertial measurement units (IMUs) and global navigation satellite system (GNSS) receivers [64].IMU integrates gyroscopes and accelerometers to measure angular velocities and linear accelerations.IMUs provide data at a high update frequency (approximately every 5 ms) and function reliably in environments where satellite signals are weak or unavailable, such as tunnels or urban canyons.However, IMU measurements are prone to drift over time, causing the accumulation of small errors that ultimately degrade overall accuracy [68].GNSS collectively refers to satellite-based navigation systems, such as the Global Positioning System (GPS) (USA), GLONASS (Russia), Galileo (Europe), and BeiDou (China).By utilizing multiple satellite constellations, GNSS enhances coverage and positional accuracy.Among them, the GPS is one of the earliest and most widely used systems.Its key advantages include low cost and the absence of cumulative measurement errors over time.Nevertheless, the localization accuracy of GPS is limited to approximately 1 meter.Moreover, GPS performance degrades in environments such as urban canyons and tunnels, where a clear line of sight to satellites is obstructed, and its data update rate, approximately every 100 milliseconds, is insufficient for accurate real-time localization in dynamic driving scenarios [69].To enhance overall system accuracy and reliability, ADS usually integrates data from multiple sensors.For instance, Caron et al. [70] proposed a multi-sensor Kalman filter approach that fuses GPS and IMU data.This method dynamically adjusts sensor weights based on real-time reliability assessments using chi-square tests and fuzzy logic.It enables the rejection of faulty measurements and compensates for IMU drift, thereby enhancing the robustness and accuracy of vehicle localization.</p>
<p>2) Computing platforms: Modern vehicles typically incorporate multiple highly powerful central computers.Each computer is responsible for different computing domains.Bosch [71] categorizes these domains into powertrain, chassis, driver assistance, and infotainment.ADS belong to the driver assistance domain, which requires real-time processing of extensive sensor data for tasks such as perception, sensor fusion, and decision-making.To meet these requirements, specialized computing platforms, such as NVIDIA Drive Orin, Horizon Journey 6, Mobileye EyeQ6, and Qualcomm Snapdragon Ride, have been developed for driver assistance.NVIDIA Drive Orin [72] is a system-on-chip tailored for ADS, based on the NVIDIA Ampere architecture.It delivers up to 254 TOPS through integrated deep learning accelerators and supports the simultaneous execution of multiple AI inference pipelines.It offers extensive interface capabilities, such as 16 GMSL camera ports, high-speed Ethernet suitable for LiDAR and radar integration, DisplayPort, and six CAN interfaces for sensor integration.However, its cost remains relatively high compared to other platforms.</p>
<p>The Horizon Journey 6 series [73] offers scalable performance from 10 to 560 TOPS, depending on the model.Designed for a wide range of intelligent driving applications, including advanced driver assistance systems (ADAS) and autonomous systems, these platforms feature heterogeneous computing resources and extensive sensor interface support, such as automotive-grade Ethernet and up to 24 camera inputs.Their compact and cost-effective architecture is optimized for large-scale production.Mobileye's EyeQ6 family [74] is designed for premium ADAS applications.The EyeQ6 High version delivers up to 34 TOPS (INT8) using advanced 7nm process technology while maintaining a low power consumption (maximum of 33W).It supports multiple camera and radar inputs for CV tasks and integrates video processing and dedicated accelerators.Widely adopted in mass-produced vehicles, Mobileye's solutions strike a balance between cost and performance.Qualcomm Snapdragon Ride [75] is an integrated automotive platform designed for both ADAS and autonomous driving.Its highly customizable and flexible design allows original equipment manufacturers (OEMs) to select different TOPS, power consumption, and input/output capabilities based on configuration requirements.Additionally, some manufacturers develop proprietary chips for their vehicles, such as Tesla's FSD, XPeng's Turing, and NIO's NX9031.However, detailed specifications of these chips have not been publicly disclosed.</p>
<p>3) Actuators: convert control module outputs into physical vehicle movements.Modern actuators replace traditional mechanical linkages with electronically controlled systems [76], [77], enabling faster, more precise, and stable responses for automated steering, acceleration, and braking.Main actuators include:</p>
<p> Steering Actuators adjust vehicle steering angles based on control commands to facilitate adaptive steering and automated lane-keeping.</p>
<p> Throttle Actuators electronically control engine or motor power to provide smooth acceleration and optimized energy efficiency for adaptive cruise control.</p>
<p> Brake Actuators convert control signals into precise braking forces using electronic control units combined with hydraulic or electromechanical systems.These actuators are designed with redundancy and fault tolerance to ensure reliable and safe deceleration across diverse driving conditions.</p>
<p> Other Actuators include automatic gear shifting and active suspension systems.These components respond in real time to dynamic road conditions.They help optimize ride comfort, improve stability, and enhance overall vehicle performance.</p>
<p>B. Software Layer</p>
<p>This layer is responsible for converting raw sensor inputs into actionable control commands, either through modular processing or an end-to-end framework.It includes perception, prediction, planning, and control modules, representing a widely adopted task decomposition in ADS.The modular approach offers advantages in interpretability and ease of debugging.Each module is discussed in the following subsections.Notably, recent studies [15], [24], [78] have explored end-to-end frameworks that integrate these modules to improve computational efficiency and mitigate issues such as error accumulation and suboptimal task coordination [79].</p>
<p>1) Perception: serves as the "eyes" of ADS.It is responsible for processing the sensor data to interpret the vehicle's surroundings.Core tasks include object detection (identifying and localizing vehicles, pedestrians, cyclists, etc.), semantic segmentation (identifying drivable areas, lane markings, sidewalks, etc.), instance segmentation (categorizing objects and distinguishing individual instances at the pixel level), traffic light and sign recognition, and localization (estimating position and orientation of the ego vehicle's).MTL is suitable for applying to these tasks.For example, a single model can simultaneously perform a semantic segmentation and depth estimation using camera inputs [80].Moreover, a common combination is object detection and semantic segmentation using camera inputs [20], [25], [45].We will provide a detailed discussion of such approaches in Section IV-A.</p>
<p>2) Prediction: operates as a bridge between the perception and planning modules by predicting future states of surrounding agents.It predicts the future trajectories or behaviours of the other agents, such as vehicles and pedestrians.It involves time-series modeling and understanding of agent intent.Key tasks include trajectory forecast (estimating the sequential future position of each agent) and classifying driving behaviour intentions (turning, speed-up turning, or lane-keeping), as illustrated in Fig. 2).These tasks are closely related, and MTL techniques have been proposed to simultaneously predict agent intentions and trajectories [82], [83].Similarly, several studies applied MTL to pedestrian prediction, such as pedestrian tracking, pose estimation, and intent [84]- [86].Additionally, predicting diverse behavioural differences among heterogeneous agents (vehicles, cyclists, and pedestrians) can also be seen as multiple tasks [87].Trajectory and intention predictions provide inputs for the planning module.This enables safe real-time driving decisions.A false prediction may lead to unsafe planning results and potential accidents.In CAV scenarios, the ego vehicle can also obtain planning results shared by other agents through V2X communication.From the ego vehicle's perspective, these shared planning results serve as high-confidence predictions.It further reduces uncertainty and enhances decision accuracy.</p>
<p>3) Planning: determines the ego vehicle's future path based on the output from the perception and prediction modules.It includes decision-making and motion planning.Decisionmaking comprises the scenario manager (identifying driving scenarios like highway cruising, intersection crossing, or lane changing), stage processing (breaking scenarios into discrete steps), and behaviour decision (choosing actions like lane changes or stops).These decision-making support the motion planning component in generating an executable trajectory according to vehicle dynamics, safety constraints, and comfort requirements.Traditionally, motion planning adopts samplingbased [88]- [90], optimization-based [91]- [93], or rule-based heuristics [94], [95] methods, all aiming to generate safe and efficient paths for the vehicle to follow.</p>
<p>4) Control: receives the planned trajectory and converts it into low-level commands, such as adjusting the steering angle to 10 degrees, applying 40% throttle for acceleration, or applying 30% braking force.These commands are then transmitted to the actuators for execution by a proportionalintegral-derivative (PID) controller [96]- [98] or a model predictive controller (MPC) [99]- [101], which track the planned trajectory.Table II summarizes the characteristics of control strategies.To leverage the advantages of these methods, Chu et al. [102] proposed a hybrid MPC-PID strategy to reduce steady-state tracking errors while enhancing steering smoothness and robustness against model simplifications.Alternatively, end-to-end systems predict commands directly from raw sensor inputs using a single model [18], [103], [104].Compared to the module pipeline, the end-to-end pipeline provides a unified framework that jointly optimizes percep- tion, prediction, planning, and control [8], which simplifies architectures and improves computational efficiency.However, there are some challenges, including a lack of interpretability and safety guarantees, and causal confusion [105].</p>
<p>C. V2X communication</p>
<p>V2X communication enables cooperative perception in intelligent transportation systems [106].It enables ADS to exchange information with surrounding agents, including other vehicles (V2V), infrastructure (V2I), and pedestrians (V2P).V2X communication enables the vehicle to obtain more comprehensive information about the surrounding road and traffic conditions.This enhances decision-making accuracy and motion control effectiveness in ADS [107].The V2X communication technologies can be divided into dedicated shortrange communications (DSRC) [108] and cellular vehicle-toeverything (C-V2X).DSRC enables short-range information exchange between devices equipped with 802.11p chips.In contrast, C-V2X uses cellular infrastructure to enable reliable, wide-area communication with high mobility support.A detailed comparison of these technologies is provided in Table III.For a comprehensive review of V2X technologies, we refer the interested readers to [109], [110].</p>
<p>Unlike traditional single-agent systems, V2X-based methods leverage collaboration among multiple agents and infrastructure to enhance perception [111]- [114], prediction [115]- [117], and planning [118]- [120].As shown in Fig. 3, singleagent approaches are often limited by occlusion [121], adverse weather conditions [122], and sparse sensor observations at long ranges [123], which may lead to unsafe decisions and even catastrophic accidents [112], [124].In contrast, multiagent collaboration enables a more comprehensive understanding of the environment by integrating observations from multiple viewpoints [112].This shared perception not only mitigates the effects of occlusion and incomplete data but also enhances the performance of downstream modules.For instance, single-agent systems struggle to anticipate vehicles that are currently outside their sensing range or hidden behind obstacles, resulting in inaccurate predictions and underestimated traffic density [116], [117].Consequently, they may produce unsafe planning outcomes.In contrast, multi-agent collaboration enables a more comprehensive understanding of the environment by integrating observations from multiple viewpoints [112].This shared perception not only mitigates the effects of occlusion and incomplete data but also enhances    Although cooperative perception enhances the perceptual capabilities of autonomous vehicles by sharing information with nearby agents through V2X communication, current research usually ignores the real-world communication challenge that V2X communications can be interrupted intermittently.To alleviate this challenge, Ren et al. [106] propose V2X-INCOP, a cooperative perception system that leverages historical information to recover missing data through a communicationadaptive multi-scale spatiotemporal prediction model.Chen et al. [125] propose RL4V2X, a reinforcement learning framework designed for autonomous driving under intermittent connectivity.It incorporates a convolutional neural network (CNN) to capture spatial traffic representations and a gated recurrent unit (GRU) to recover missing global information using historical sequences.Additionally, gating networks are employed to dynamically adjust feature confidence based on interruption scenarios, enhancing both safety and efficiency.</p>
<p>Recent research has explored cooperative perception and prediction.However, from the ego vehicle perspective, V2Xbased multi-agent cooperation in the planning and control modules remains underexplored.This is due to the stringent requirements these modules impose on real-time performance, communication reliability, and safety, which make it challenging to apply cooperative learning paradigms.Although C-V2X technologies have started to alleviate this challenge through low-latency, high-reliability communication among agents [120], several bottlenecks persist.These include limited communication bandwidth, semantic inconsistency across agents, and redundancy in task-specific models [126]- [129].</p>
<p>To address these challenges, MTL has been adopted in recent works.MTL efficiently shares features across related tasks [128], reduces redundant computation and communication, and enhances robustness against impaired or partial information in V2X-based cooperative systems [127].Finally, V2X-based cooperative driving also involves multiple interrelated tasks such as scheduling, computation offloading, and resource allocation.However, these systemlevel multi-task optimization methods are different from the scope of this survey.Specifically, our focus is on MTL, where multiple tasks are jointly learned through shared deep neural network architectures.In contrast, system-level methods [130]- [134] typically optimize the execution or coordination of multiple tasks without shared deep representations or joint feature learning.Therefore, we exclude them from this survey.</p>
<p>III. MULTI-TASK LEARNING</p>
<p>This section provides an overview of MTL.We first formulate the MTL problem and then categorize architectural paradigms into hard, soft, and hybrid parameter sharing.Next, we review optimization strategies and conclude the section by summarizing three key advantages of applying MTL in ADS.</p>
<p>A. Problem Formulation</p>
<p>Given n tasks, the learning objective for each task is represented as
{L i } n i=1
, where all subtasks are related.MTL aims to improve performance across all tasks simultaneously by leveraging knowledge contained in all or some of the tasks [47].The overall learning objective can be formulated as minimizing the combined weighted loss:
min ,{i} n i=1 n i=1  i L i (f i (X; ,  i ), Y i ) (1)
where X is a batch of inputs.Y i is the ground truth for the i th task corresponding to the batch inputs.L i is a loss function specific to task i.  is the shared layer's parameters, which are common across all tasks, while  i is the task-specific layer's parameters for task i. f i is the forward function for task i, where f i (X) produces the predictions i for inputs X based on both the shared and task-specific layers. i is the weighting factor for the loss of task i, reflecting the relative importance of each task in the overall learning process.</p>
<p>To minimize the overall weighted loss, the objective is to optimize  + n i=1  i .This approach aims to enhance performance on each task while ensuring generalizability and robustness across all tasks.</p>
<p>B. Architectural Paradigms</p>
<p>MTL consists of shared components that capture common representations across tasks and task-specific components that learn representations unique to each task.The main challenge in MTL design is determining how to share knowledge between tasks in a way that maximizes generalization.To address this challenge, there are three key paradigms, including hard-parameter [20], [25], [45], [135], soft-parameter [136]- [141], and hybrid-parameter [142]- [144] sharing.We provide an overview of each paradigm in the following subsections.Additionally, Table V provides a summary of the key characteristics of these paradigms.1) Hard-parameter sharing: This paradigm is the most commonly used approach in MTL [51], [145], where the model shares a set of layers, denoted as , across all tasks, and each task has its task-specific layers  i that follow the shared layers (see Fig. 4).Specifically, the shared layers first process the input data, and their output features are then passed to the task-specific layers to produce the final outputs for each task.This paradigm is computationally effective, as it shares most of the parameters across tasks, which makes it practical for applications with limited computing resources.However, it assumes that all tasks are related, which is not always valid in practice.For instance, object detection and depth estimation may exhibit low inter-task correlation.In such cases, hardparameter sharing can lead to two critical challenges:</p>
<p> Negative Transfer: Some or all task performance decreases when irrelevant or conflicting knowledge is transferred across tasks.</p>
<p> Task Conflict: During training, different tasks may produce conflicting gradients for shared parameters.This makes optimization unstable and may bias one task over others.2) Soft-parameter sharing: This paradigm (see Fig. 5) uses an alternative approach to the hard-parameter sharing.Each task has its own set of model parameters.Instead of sharing layers, the model exchanges complementary information between different tasks through a mechanism such as cross-talk (also called cross-fusion).For example, early work [146] uses L2 regularization to constrain the parameters between different tasks.This is achieved by adding a regularization term R into the loss function (Eq. 1) that penalizes discrepancies between task-specific weights.However, regularization cannot autonomously choose which information to share [147].To address this challenge, Misra et al. [136] propose crossstitch units to dynamically learn linear combinations of taskspecific activations.This allows the model to automatically learn the degree of sharing between different tasks.Tian et al. [148] propose a plug-and-play module to capture both crosstask consistent and complementary features by computing a cross-task similarity matrix and fusing it with task-specific features via 11 convolution.Unlike hard-parameter sharing, soft sharing allows each task flexibility to learn its parameters while maintaining beneficial inter-task communication.This flexibility could effectively alleviate the negative transfer [145].While this design provides flexibility, it also introduces scalability concerns.Since each task maintains a full set of parameters, the overall model size tends to grow linearly with the number of tasks [54], which can be problematic when working with limited computational resources.3) Hybrid-parameter sharing: This paradigm (see Fig. 6) combines the ideas from both hard and soft-parameter sharing.It consists of a shared backbone that learns common representations from the input, followed by task-specific decoders to refine these features for individual tasks.Additionally, cross-talk mechanisms from soft-parameter sharing are integrated into the task-specific components to enable selective information exchange between task-specific parameters across tasks.Several studies have proposed different ways to implement hybridparameter sharing.For example, Bruggemann et al. [142] propose the adaptive task-relational context module, which employs attention mechanisms and neural architecture search to automatically optimize cross-task context interactions in multi-task dense prediction.Similarly, Lopes et al. [143] propose a cross-task attention mechanism combining correlation-guided attention and self-attention, fused via learnable channel-wise weights.Integrated into a multi-task exchange block, which can adaptively feature refinement across tasks.Additionally, Chen et al. [144] introduce an inter-task joint-attention fusion module in the ADS task decoder that dynamically combines features from all tasks' self-attention outputs.This allows cross-task interaction while maintaining parameter efficiency.</p>
<p>C. Optimization Strategies</p>
<p>Integrating multiple tasks within a single model does not always ensure effective joint learning.One primary challenge in MTL is balancing the optimization dynamics to allow all tasks to progress steadily.Without proper balance, one task may dominate.Other tasks then fail to learn effectively, and the overall performance decreases.To address this issue, several techniques such as loss weighting, gradient-based conflict resolution, and multi-objective optimizationn have been introduced.</p>
<p>1) Loss weighting: The objective of MTL involves a weighted combination of task-specific losses (see Eq. 1), where the weighting factor  i controls the contribution of each task to the overall loss.When one task's loss becomes large, it may overshadow the losses of other tasks.Therefore, it is important to set appropriate weights for each task.A simple yet effective approach is manually tuning the weights through extensive experimentation.However, if the number of tasks is too much, tuning each loss weight becomes difficult.To address this issue, several studies [60], [149]- [152] have developed adaptive methods to adjust the weights during training.One popular method is the uncertainty-based weighting mechanism developed by Kendall et al. [60], where each task's loss is scaled inversely to its homoscedastic uncertainty (a learnable parameter that reflects task-specific noise).Specifically, tasks with high uncertainty are down-weighted.GradNorm [150] is another popular method, which obtains the gradient norms from each task's loss and balances these norms, making sure no task lags significantly.In contrast, Jha et al. [151] normalize the average gradient magnitudes with respect to each task's parameters.This method prioritizes tasks with higher gradients to ensure convex combination stability.Recently, Wu et al. [149] proposed to adaptively adjust task weights by evaluating each task's relative inverse training rate, combining loss decay speed and normalized loss magnitude.This prioritizes tasks with slower convergence or higher difficulty, preventing their dominant gradients from overwhelming others.</p>
<p>2) Gradient conflict mitigation: During the training stage, the shared parameters receive the gradient updates information from different tasks, which may point in different or even conflicting directions in parameter space.Such conflicts can cause one task to interfere with another, leading to unstable training or suboptimal convergence.Cosine similarity is commonly used to quantify the alignment between task gradients.It evaluates whether the directions of gradients for two tasks, (g i and g j ), are compatible.The cosine similarity:
cosine similarity = cos  = g i  g j g i g j  .(2)
A low or negative cos  indicates that the gradients are misaligned and present potential interference between the tasks.Identifying and mitigating such conflicts can achieve a stable and balanced multi-task optimization.Various gradient correction methods have been proposed [153]- [158].PCGrad [153] projects conflicting gradients onto each other's normal planes to eliminate interfering components.Gradients are altered only when they conflict.Each task's update does not hurt the others while maintaining constructive interaction.Following the PCGrad, a model-agnostic method (MAMG) [155] mitigates conflicting gradients in MTL by defining a gradient interfering direction and clipping conflicting gradient components to balance task optimization.Similarly, conflictaverse gradient descent (CAGrad) [154] dynamically adjusts the update direction to maximize the worst-case improvement across tasks while constraining updates within a neighborhood of the average gradient.In contrast, Navon et al. [156] propose using the Nash bargaining solution to derive a scaleinvariant, Pareto-optimal update direction that balances task gradients proportionally.This ensures fairness and convergence guarantees.However, these methods increase GPU memory consumption because they require computing and storing gradients for each task separately, rather than using a single backward pass, reducing the maximum batch size and increasing the required training time.</p>
<p>3) Multi-objective optimization: This category of optimization considers MTL as a multi-objective optimization problem, where each task loss is an objective, and there is usually no single solution that is best for all objectives unless they are perfectly related.Sener and Koltun [159] formulate MTL as a multi-objective optimization problem to seek Pareto optimal solutions.The aim is to minimize a vector-valued loss: min ,1,...,n
L = L1 (,  1 ), . . . , Ln (,  n )  ,(3)
where  is shared parameters by all tasks,  1 , ...,  n are task-specific parameters for tasks 1, ..., n.To solve this, they adapt the multiple gradient descent algorithm (MGDA), which computes coefficients { t } by solving:
min  1 ,...,n n t=1 t Lt(, t) 2 2 n t=1 t = 1, t  0 t . (4)
To avoid n backward passes in MGDA, they introduce MGDA-UB, which optimizes an upper bound via shared representations Z = g(X; ).The resulting optimization problem is:
min  1 ,...,n n t=1 tZ Lt(, t) 2 2 n t=1 t = 1, t  0 t . (5)
Under full-rank assumptions of Z  , MGDA-UB guarantees Pareto optimality.However, they find a single Pareto solution for MTL.Lin et al. [160] extend Sener and Koltun's work by decomposing the problem into preference-guided subproblems to enable the generation of diverse Pareto-optimal solutions that represent distinct trade-offs across tasks.Following previous work, Ma et al. [161] advance MTL by proposing continuous Pareto exploration, which constructs locally smooth Pareto sets through second-order analysis and Krylov subspace methods.This approach generates dense Pareto fronts that capture a wider range of trade-offs between conflicting tasks while scaling effectively to large-scale neural networks.Additionally, Momma et al. [162] integrates user preferences with Pareto stationarity, proposing the extended weighted Chebyshev method (XWC-MGDA) to efficiently discover Pareto optimal solutions aligned with preferences or reference models to reduce exploration costs from (m) to O(1) while achieving competitive performance.</p>
<p>D. Motivations for Applying MTL in CAVs</p>
<p>CAVs are required to perform multiple tasks simultaneously while operating under strict hardware constraints.MTL addresses this challenge by enabling the joint training of related tasks within a single model.This shared learning reduces computational resource consumption, promotes knowledge transfer across tasks, and streamlines the integration of new tasks.In this subsection, we highlight three key advantages of applying MTL in CAVs: improved computational efficiency, enhanced task interaction, and greater flexibility in model updates.</p>
<p>1) Computational efficiency and resource optimization: MTL reduces computational costs, memory usage, and energy consumption through sharing parameters across tasks [20], [163], [164].This efficiency is critical for resource-constrained edge devices and enables real-time processing (FPS over 30) [80], [165] while maintaining accuracy.Additionally, MTL can further save the training cost by combining with transfer learning [26].Specifically, pre-train the model on a large-scale dataset (such as ImageNet [166]) to learn a general representation and fine-tune it on downstream tasks.This is especially useful for Transformer-based models, which typically require a longer training time compared to CNN-based models.</p>
<p>2) Task synergy and knowledge transfer: MTL exploits the implicit synergy between tasks to enhance performance [25], [42].For example, semantic segmentation masks improve object detection by providing contextual boundaries, while object detection outputs guide lane segmentation through spatial constraints [46].This cross-task knowledge transfer mitigates the need for exhaustive labeled datasets for individual tasks.It combines the labeled data from all tasks, effectively serving as a form of data augmentation, to build a more accurate model for each task.Additionally, training with multiple tasks could regularize the model, further reducing overfitting for each task [163], [167], [168].</p>
<p>3) Scalability and modularity: MTL offers a modular and extensible architecture design that is particularly beneficial for CAVs, where perception or sensor requirements constantly evolve.By decoupling the model into a shared backbone and multiple task-specific heads, which can flexibly add new tasks [15], [20], [169].Additionally, the modular design offers flexible integration of new sensors to the model as an extra input instead of modifying the entire network [17], [170].This design is also easy to maintain and allows fine-tuning for individual tasks based on subsequent usage.</p>
<p>IV. MTL IN CAVS</p>
<p>In this section, we review MTL methods applied to CAVs.We specifically focus on deep learning-based approaches and organize the review according to software layers within CAVs, which include perception, prediction, planning, and control.</p>
<p>Then we discuss the MTL used in multi-agent cooperative driving based on V2X.</p>
<p>A. MTL for Perception Tasks</p>
<p>Perception tasks are among the most prominent applications of MTL in ADS due to the necessity of simultaneously processing multiple environmental cues with limited computational resources.Autonomous vehicles must perform multiple perception tasks, including 2D and 3D object detection, semantic segmentation (e.g., drivable areas, lane markings, sidewalks), instance segmentation (delineating object masks), and depth estimation (from monocular images).Rather than deploying separate models for each task, recent research has adopted MTL approaches to address these perception tasks with a unified framework.We categorize existing approaches based on model architecture into CNN-based, Transformerbased, and vision-language model (VLM)-based methods.</p>
<p>1) CNN-based methods: are foundational DL models for image processing that learn spatial hierarchies of features from input images.CNNs excel in image classification, object detection, and segmentation tasks.They are widely used for MTL models to address ADS perception tasks.The CNNbased methods could be divided into two-stage and one-stage.</p>
<p>Two-stage methods usually build on the region-based approach, such as Faster R-CNN [171] and Mask R-CNN [172].In these architectures, a shared backbone first generates classagnostic region proposals via a region proposal network (RPN) [171], then multiple task-specific heads are applied to each proposal.MT-Faster R-CNN [135] is an end-to-end method that uses monocular vision to simultaneously address 2D and 3D object detection, orientation estimation, and key point detection.Inspired by Deep3DBox [173], MT-Faster R-CNN refines the geometric constraint approach by replacing the 2D bounding box with key point coordinates.This enables more precise 3D position estimation through inverse perspective projection.The model (see Fig. 7) includes a RoIAlign layer to accurately extract features while preserving spatial alignment.The RPN generates two branches of region of interest (RoI): one predicts classification, dimensions, confidence, and rotation angle, while the other outputs key point scores for 3D detection and orientation estimation.Similarly, Petrovai et al. [174] extend Mask R-CNN [172] to perform instance, semantic, and panoptic segmentation in automated driving tasks with a unified framework.They enhance segmentation accuracy by fusing multi-scale outputs.Following previous work, Fang et al. [175] further refine Mask R-CNN [172] for detection and segmentation in autonomous driving complex traffic scenes by upgrading the backbone to ResNeXt, incorporating feature fusion strategies, and using complete-IoU (CIoU) [176] loss for faster convergence.Additionally, Rinchen et al. [177] develop a scalable system based on Mask R-CNN [172] with task-specific RoI Heads and multiple RPNs to optimize multi-task object detection for diverse ADS tasks like identifying traffic lights, signs, pedestrians, and vehicles.</p>
<p>Unlike two-stage methods, one-stage methods directly predict outputs from inputs without intermediate proposal generation.Several studies [178], [179] mention this paradigm as an end-to-end network.Here, "end-to-end" refers to the model architecture and should be distinguished from the concept of end-to-end systems in ADS.MultiNet [21] is an early representative work with a hard-parameter sharing MTL architecture that combines a shared encoder with individual classification, detection, and semantic segmentation decoder.MultiNet can simultaneously address ADS's core perception tasks while achieving over 23 FPS.In contrast, Leang et al. [180] address the challenge of balancing performance between all the tasks in ADS by proposing an adaptive weight learning network that utilizes evolutionary meta-learning and taskspecific selective back-propagation.</p>
<p>Recent CNN-based MTL methods have focused on enhancing accuracy and real-time performance.Miraliev et al. [163] propose a real-time, memory-efficient end-to-end framework using pre-trained models, e.g., RegNetY [181] and MobileNetV3 [182], as encoders to reduce parameters, computational complexity, and memory requirements while maintaining high accuracy and faster inference speeds.DRM-Net [183] adopts a two-pathway structure to separately extract shallow details and deep semantic information.To reduce the loss of feature information due to multiple down-samplings, it combines these features using a multi-scale feature fusion module for effective information integration.Additionally, specialized detection branches tailored to each task's characteristics ensure that the model meets the unique demands of each application.In contrast, UMT-Net [144] adopts a shared encoder for global feature extraction, task-specific selfattention encoders and decoders, and a joint-attention fusion module that enables cross-task communication through shared attention mechanisms, which is a classical hybrid-parameter sharing MTL.This architecture allows tasks to adaptively learn from both shared representations and inter-task features.</p>
<p>You only look once (YOLO) [184] is a widely used one-stage object detection algorithm known for its real-time processing capabilities, and it has become one of the most popular MTL methods in ADS perception tasks.YOLOP [25] is a classical example, which is a real-time panoptic driving perception hard-parameter sharing MTL network that uses CSPDarknet [185] as the backbone.It incorporates spatial pyramid pooling (SPP) [186] and a feature pyramid network (FPN) [187] in the neck, with three separate heads for object  [192].)detection, drivable area segmentation, and lane segmentation tasks.Building on YOLOP, improved models such as YOLOPX [45] and YOLOPv2 [188] have been proposed.Similarly, [20], [164], [183], [189]- [191] proposed YOLO-based hard-parameter sharing MTL frameworks to address object detection, drivable area segmentation, and lane segmentation.</p>
<p>Single-shot multi-box detector (SSD) [193] is another onestage algorithm that adopts default bounding boxes of various sizes and aspect ratios on multi-scale feature maps extracted from the backbone network.It predicts object categories and bounding box locations in a single forward pass.Several SSDbased methods [192], [194]- [196] have been developed for MTL in ADS.For example, Cartesian product-based multitask SSD (CP-MTL SSD) [192] is an MTL model designed to simultaneously perform dangerous object detection and distance prediction.Fig. 8 shows the structure of CP-MTL SSD, where d is the category of an object distance, and c is the object categories.Another example is vulnerable road users SSD (VRU Pose-SSD) [195], which includes a modified Inception V1 [197], SSD [193], non-maximum suppression, and pose head.It simultaneously implements VRU detection and pose estimation tasks for real-time ADS under accuracy and speed trade-offs.However, SSD has notable limitations in detecting small objects [198].In ADS, cameras often capture distant objects that appear smaller due to perspective diminishment, exacerbating this challenge.For the model, this necessitates different scale features to represent objects of varying sizes effectively.Although SSD incorporates multiscale feature maps to handle objects at different scales, the feature layers corresponding to these scales are independent and lack complementary information exchange between them.The absence of inter-scale feature fusion results in suboptimal performance of SSD for small object detection tasks [199].</p>
<p>2) Transformer-based methods: Transformer [200] is a neural network architecture that utilizes self-attention mechanisms to capture long-range dependencies in sequential data.It allows parallelization during training and overcomes the limitations of traditional recurrent architectures in handling long-term dependencies.Unlike CNNs, Transformers lack certain inductive biases, such as translation equivariance, spatial invariance, and locality [201].As a result, Transformers may underperform compared to CNNs, and exhibit weaker generalization when trained on insufficient data [201].However, hybrid models that combine CNNs with Transformers achieve competitive performance.This achieves higher throughput while maintaining parameter counts and computational complexity (FLOPs) intermediate between pure Transformers and CNNs.Current MTL in ADS research [80], [202]- [204] demonstrate that Transformer-based methods include pure Transformer methods and hybrid Transformer methods.</p>
<p>Recent studies have adapted hybrid methods for MTL tasks.For instance, sparse U-PDP [26] integrates vehicle detection, lane detection, and drivable area segmentation by constructing a unified decoder and leveraging task interconnections to enhance model robustness (see Fig. 9).It adopts dynamic convolution kernels as input to the Transformer block, where self-attention mechanisms enable effective feature representation and interaction across tasks.Additionally, the dynamic interaction module, coupled with dynamic convolution kernels, facilitates task-specific feature sampling and contributes  We focus on four major tasks in autonomous driving, i.e., object detection, semantic segmentation, drivable area segmentation, and lane detection.The in-depth analysis of current multi-task methods is shown in Section 5.3.</p>
<p>VE-Prompt</p>
<p>The key to multi-task learning is to learn high-quality task-specific representations among tasks, which can explore relationships between tasks.Therefore, a good multitask learning framework should take full advantage of taskspecific priors, and guide the model to learn better representations.To this end, we introduce our proposed multi-task framework with VE-Prompt, which consists of five components: (1) an image feature encoder to extract image features; (2) a lightweight shared transformer encoder for feature enhancement; (3) task-specific prompts which encodes task-specific information from visual exemplars; (4) a visual exemplar driven task-prompting block to integrate the visual representation with task-specific prompts; (5) taskspecific heads for predicting results simultaneously.</p>
<p>Bridging CNN and Transformer</p>
<p>The multi-task framework aims to learn more effective representations for all tasks via bridging CNN and Transformer efficiently.The neck of the image encoder and segmentation heads of the framework are CNN-based, reducing the overall training time.The shared transformer encoder is built upon the transformer architecture to capture the long-range dependency [53].Image Encoder The image encoder consists of a backbone network and a neck network.We choose the Swin transformer [33] as the backbone to extract features of the input image.The output of the backbone is denoted as {C 2 , C 3 , C 4 , C 5 }.Then we adopt Feature Pyramid Network (FPN) [28] module for the neck network to fuse features generated by the backbone.The pyramidal features are of 5 scales, and the detection head only processes the last four-scale features for reducing the computation cost.Here we denote the output of the neck as {P 2 , P 3 , P 4 , P 5 , P 6 }, which have strides of {4, 8, 16, 32, 64} pixels.</p>
<p>Shared Transformer Encoder</p>
<p>The shared transformer encoder TransEncoder receives multi-scale outputs from the neck and enhances features for following taskspecific heads.We first flatten the feature maps from {P 3 , P 4 , P 5 , P 6 } and concatenate them to obtain a 1D sequence P .Since flattening the features leads to losing the spatial information critical for segmentation, we supplement positional embeddings p l to the flattened features.For the model not considering prompts, we obtain the enhanced feature as follows:
O = TransEncoder(P + p l ).(1)
After feature enhancement, O is passed to the detection head directly, while unflattened to multi-scale features {z 3 , z 4 , z 5 , z 6 } for segmentation heads.Detection Head The detection head consists of 4 multiscale deformable decoder layers which are elaborated in DINO [59].Following DINO, we adopt the mixed query selection strategy to initialize anchors as positional queries for the decoder and use the contrastive denoising training approach by taking into account hard negative samples.Segmentation Head For segmentation-based tasks, we choose Semantic FPN [22] as the segmentation head.In the model without considering prompts, segmentation heads take in multi-layer features from both the neck and shared transformer encoder {P 2 , z 3 , z 4 , z 5 }.The resolution of P 2 is larger and thus provides more image information for the following heads.Then the multi-layer features are upsampled and summed element-wisely.This merged feature map is again upsampled 4 followed by softmax to produce the classification score for every pixel at the original resolution.</p>
<p>Prompt Generation with Visual Exemplar</p>
<p>In order to motivate the model to learn more high-quality task-specific knowledge and handle all tasks better, VE-Prompt is introduced to provide more task-specific information with visual clues.The process of generating visual exemplar-driven prompts is shown in Figure 4.The key idea of task-specific prompts is to let the model know how to 9614 Compared to pure CNN-based methods, Transformer-based methods typically provide stronger performance but at the cost of a higher number of parameters and longer inference times.Due to their powerful representational capacity, more and more Transformer-based MTL methods are not only applied in perception tasks but also used in other ADS software layers tasks, such as prediction [13] and control [19].Furthermore, Transformer-based methods have been applied to multi-type task scenarios that cross ADS modules [17], [79], [169], [170], [204]- [206], such as joint different tasks from perception, prediction, and planning into one model [170].</p>
<p>3) VLM-based methods: VLM integrate both visual and textual information, typically pre-trained on large-scale, multimodal web-scale datasets D = x I n , x T n N n=1 , where x I n represents an image sample and x T n represents its corresponding text.These models leverage an image encoder f  and a text encoder f  to transform the raw image and text data into their respective embeddings.VLMs are then applied to downstream tasks.With VLM's powerful zero-shot capabilities, enabling it to generalize across unseen tasks [207].Additionally, VLM can utilize task-specific prompts to guide the MTL process, reducing the risk of negative transfer and improving task performance by dynamically adjusting to the needs of individual tasks [208].</p>
<p>Recent advancements in prompt learning have catalyzed novel research into VLM applications in ADS.Specifically, VLM has shown increasing potential in MTL, enabling efficient handling of diverse tasks while benefiting from robust generalization through frameworks like contrastive languageimage pre-training (CLIP) [210] and context optimization (CoOp) [211].Liang et al. [42] conduct the first study to address performance degradation in state-of-the-art selfsupervised models for MTL in ADS tasks like semantic segmentation, drivable area segmentation, and traffic object detection.It proposes a pre-train-adapt-finetune paradigm that significantly boosts model performance without increasing training overhead.Core to their approach is the LV-Adapter, which incorporates linguistic knowledge from CLIP by learning task-specific prompts.The experiments highlight the critical role of the adaptation phase in improving MTL, with the language priors from CLIP enhancing performance across multiple downstream tasks.Similarly, Liang et al. propose the VE-Prompt [209] framework that utilizes task-specific visual exemplars to guide the model in learning more effective task representations (see Fig. 10), which alleviates the negative transfer issue among object detection, semantic segmentation, drivable area segmentation, and lane detection.It consists of five parts, including image encoder, Transformer encoder, prompt generator, task-prompting block, and task-specific heads for different tasks.The prompt generator utilizing a fixed CLIP image encoder extracts task-specific prompts from visual exemplars, offering high-quality task-specific knowledge to the model.Furthermore, the framework bridges the Transformer with convolutional layers, enabling efficient and accurate task representation learning.This research demonstrates the promise of VLM in optimizing task-specific performance and maintaining strong generalization across different tasks in ADS.</p>
<p>Studies [212]- [214] have explored the use of VLM in bird's eye view (BEV) maps for ADS.In parallel, Liu et al. [215] proposed hierarchical prompt learning for MTL, where tasks are clustered in a tree structure to balance task-shared and taskspecific prompts.This approach leverages task-relatedness to capture fine-grained representations.However, it has not yet been evaluated in the context of ADS.Additionally, the applications of language models in ADS have been explored in [14], [216]- [220].However, these studies focus on ADS challenges rather than MTL applications.</p>
<p>B. MTL for Prediction Tasks</p>
<p>For the MTL in the prediction task, the aim of the model is to predict future behaviour or trajectories of agents.It often combines perception with prediction.Perception results as auxiliary inputs to enhance contextual understanding and MTL is an inference and transfer learning tool that impr ov es the learning ability of each task through continuous inference bias .T he generation of an LC trajectory is the outcome of numerous elements acting together, and the single-task model is hard to adequately exploit the internal relationship between the influencing factors.In the fr ame work of MTL, m ultiple LSTM networks can ac hie v e information sharing during the learning process and strengthen the model's overall quality.The network's structure for LC trajectory prediction using MTL-LSTM is displa yed in Fig. 3 .T his is a simultaneous classification and r egr ession process [ 41 ].Two parts of data are obtained from the real data of the LC process: driving style and trajectory series .T he model's netw ork lay er settings are sho wn inside the dotted box.Trajectory prediction input features and style recognition variables are extracted by specific task networks, and the concatenate layer integrates the two parts of information to establish a shared netw ork lay er.In addition, the attention mechanism is added into the model to ensure that each sub-task learns the most critical information from the shared network.Finally, the exclusive output layer of each task learns from the shared network and implements the r equir ed r esponse type .T he output la yer of style recognition and trajectory prediction are completed by the softmax layer and fully connected layer, r espectiv el y.The input v ariables ar e c hosen with consider ation for the impact of neighbouring vehicles and past driving features .T he position data of the test vehicle and the leading vehicle are emplo y ed as the input variable in this paper's experiment.LSTM can be used to continuousl y detect speed, acceler ation, space ga p r elativ e speed and other parameters.Considering how to implement autonomous vehicles in the future, this kind of input ensures the maximum flexibility for the subject vehicle's detecting capabilities [ 42 ].The historical tr ajectory featur e sequences and pr ediction tr ajectory sequences used for training and prediction are shown as follows:
H t = h tL t , h t( L 1 ) t , . . . , h t2 t , h t t (11)H  t = h t+ t , h t+2 t , . . . , h t+ ( L  1 ) t , h t+ L  t (12)
where h t refers to input information at t -step, including the location p t of the test vehicle and the location s t of the surrounding vehicle, L , L  denotes the historical and predicted sequence length and t refers to time interval.For trajectory style recognition, the label should be converted into a form that is easy to deal with in the deep learning algorithm through one-hot before inputting the model.</p>
<p>Case study</p>
<p>Da ta prepar a tion</p>
<p>A well-equipped driving simulator is emplo y ed to collect the data used in the experiment.As shown in Fig. 4 , drivers need to complete the driving process in a virtual environment projected onto thr ee scr eens, and the dynamic data are collected by sensors of the subject vehicle.One screen, with a resolution of 1280 by 768 pixels, is in front of the subject, while the other two are behind, allowing drivers to scan the rearview mirror for vehicles a ppr oac hing fr om behind.The sim ulation scenario is a two-lane r oad, and eac h side is 3.5 m wide, including r oad signs, constructions, guardrails and green belt to ensure the most authentic driving experience possible .T he driver participating in the exper-  We discuss each category in the following subsections.1) Single-modal prediction: These models rely on a single input modality, such as historical trajectory sequences or visual data, to simultaneously predict multiple related outputs, e.g., agents' trajectories, intentions, and behavioural patterns.Even under the constraint of single-modal input, MTL frameworks can effectively learn multiple interrelated outputs by modeling task-specific representations and capturing intertask interactions.For example, Meng et al. [221] propose a hybrid-parameter sharing LSTM-based MTL model that simultaneously forecasts lateral and longitudinal trajectories and trajectory style recognition.The pipeline is shown in the Fig. 11.The model assigns separate LSTM encoders for each trajectory task and a dedicated classification network for style, then concatenates these task-specific features into a shared, attention-augmented module before feeding them into each task head.Based on the idea of adaptive task interactions, Yuan et al. [82] develop a temporal multi-gate mixture-of-experts model, where shared temporal features are routed to expert subnetworks through adaptive gating.This model simultaneously predicts vehicle trajectories and driving intentions while using homoscedastic uncertainty to dynamically balance the loss contributions from each task.Different from [82]'s gatingbased methods, this model encodes the interactions between agents by embedding historical trajectories and heterogeneous motion states into a graph structure to capture collision risks.An auxiliary interaction prediction task, supervised by automatically generated pseudo-labels, enhances robustness and multi-agent behavioural consistency in complex traffic
Downloaded</p>
<p>D. Cross-Modal Transformer Encoder</p>
<p>After extracting the preliminary features H1 and H2 from raw input data, we put forward a cross-modal transformer encoder to capture the inherent temporal correlation within each modality as well the cross-correlation between different modalities.Specifically, the CMT encoder consists of intramodal self-attention (IMSA) and cross-modal self-attention (CMSA).</p>
<p>1) Intra-Modal Self-Attention: Similar to the seminal vision transformer (ViT) [26], two learnable class tokens u intra</p>
<p>where P E is defined as 8 &gt; &gt; &lt; &gt; &gt; :
P E ( pos, 2k) = sin  pos 10000 2k d  P E ( pos, 2k + 1) = cos  pos 10000 2k d (5)
Let
Hmm =  hm,1, hm,2,    , hm,T  2 R (T +1)C1
, we introduce multi-head self-attention (MHSA) in order to the model global temporal correlation.In particular, we first learn query Qm,Q, key Km,Q, and value Vm,Q by using multiple MLP 8 &gt; &lt; &gt; :
Qm,Q = MLP Hm, Wm,Q Km,Q = MLP Hm, Wm,K Vm,Q = MLP Hm, Wm,V(6)
where Wm,Q, Wm,K , Wm,V are learnable parameter matrices.</p>
<p>Then, the temporal attention can be calculated as
m = softmax  hQm,Q, Km,Qi p d (7)
where m 2 R (T +1)(T +1) is the attention matrix with entry m,tt0 measuring how much attention frame t pays to frame t 0 for bounding box (m = 1) and velocity (m = 2).Then, the resulting attention matrix is utilized to propagate temporal information across different frames
Gm = m Vm,Q(8)
where
Gm =  gm,0, gm,1,    , gm,T  2 R (T +1)C2 .
To capture different temporal dependencies, multiple query, key, and value matrices are used to map Hm into distinct subspaces to obtain a set of
G 1 m , G 2 m ,    ,G K m
where K denotes the number of heads.These features in different heads are concatenated and projected to produce a condensed representation Rm
Rm= MLP  G 1 m G 2 m    G K m (9)
Finally, we add residual connections and layer normalization (LN) to produce the final representation Rm =  rm,0, rm,1,
   ,rm,T  2 R (T +1)C2 as follows Rm = LN Rm + Hm(10)
For the sake of clarity, the above steps from ( 6) to ( 9) are briefly expressed as a multi-head self-attention (MHSA) function
Rm = MHSA Hm(11)
2) Cross-Modal Self-Attention: We fetch the vector rm,0 corresponding to the prepended class token um, which aggregates all the temporal information of each input brand.Consequently, a new learnable token u inter is combined with r1,0 and r2,0 which are derived from two input brands, thus form  u inter , r1,0, r2,0  2 R 3C2 .In order to model the inter-modal relationship, cross-modal attention is employed as follows
F= MHSA h u inter , r1,0, r2,0 i(12)
Let F =  f0, f1, f2  .We can see that f0 represents the aggregated features from r1,0 and r2,0.Then, the class token rm,0 in Rm is replaced by the above f0, thus arriving at  Unlike the above methods that use historical trajectories as input, studies [84], [85] rely on a single RGB image.RGB images can provide more contextual cues than historical trajectory as input, including pedestrian pose, environmental semantics, and social interactions.Moreover, these models can perform inference on single-frame inputs, enabling lowlatency prediction without requiring temporally aggregated data.Razali et al. [85] simultaneously predict pedestrian crossing intention and body pose estimation.Although outputs are derived from the same visual modality, the network benefits from semantic task diversity.The pose estimation task enhances intention prediction by providing fine-grained pose cues.This detector-free approach can run in real-time while achieving competitive intention prediction precision scores.Similarly, Zhou et al. [84] propose an efficient MTL that simultaneously performs pedestrian detection, 3D tracking, and multi-attribute recognition.</p>
<p>2) Multi-modal prediction: Unlike single-modal prediction, the multi-modal prediction integrates complementary information from diverse sensors, such as LiDAR's precise geometric structure, radar's robustness in adverse weather, and camera images' rich semantic content, to overcome the limitations of single-modal prediction.Li et al. [87] render all heterogeneous road information into a single BEV raster map, where each channel encodes one semantic modality.This raster is passed through a hierarchical spatio-temporal encoder to extract finegrained, per-agent features.In parallel, a dedicated temporal encoder explicitly models each agent's past behaviour.Then, a multi-head attention aggregator fuses these spatial, social, and temporal cues, allowing the network to attend to the most critical factors governing future motion.Finally, multi-task decoder branches with learned dynamic adaptive anchors to predict multi-modal future trajectories for every agent, improving scene adaptability and prediction coverage without extra computational cost.However, rasterization-based representations may not always be available in lightweight or real-time settings.To alleviate this limitation, PedCMT [205] introduces a cross-modal Transformer-based soft-parameter sharing model for pedestrian future bounding box and crossing intention prediction.As shown in Fig. 12 PedCMT takes only the bounding box (bbox) coordinates and ego vehicle speed as multi-modal inputs.Each modality is processed indepen-Figure 2. Pipeline of Unified Autonomous Driving (UniAD).It is exquisitely devised following planning-oriented philosophy.Instead of a simple stack of tasks, we investigate the effect of each module in perception and prediction, leveraging the benefits of joint optimization from preceding nodes to final planning in the driving scene.All perception and prediction modules are designed in a transformer decoder structure, with task queries as interfaces connecting each node.A simple attention-based planner is in the end to predict future waypoints of the ego-vehicle considering the knowledge extracted from preceding nodes.The map over occupancy is for visual purpose only.mentation of the map.With the above queries representing agents and maps, MotionFormer captures interactions among agents and maps and forecasts per-agent future trajectories.Since the action of each agent can significantly impact others in the scene, this module makes joint predictions for all agents considered.Meanwhile, we devise an ego-vehicle query to explicitly model the ego-vehicle and enable it to interact with other agents in such a scenecentric paradigm.OccFormer employs the BEV feature B as queries, equipped with agent-wise knowledge as keys and values, and predicts multi-step future occupancy with agent identity preserved.Finally, Planner utilizes the expressive ego-vehicle query from MotionFormer to predict the planning result, and keep itself away from occupied regions predicted by OccFormer to avoid collisions.</p>
<p>Perception: Tracking and Mapping</p>
<p>TrackFormer.It jointly performs detection and multiobject tracking (MOT) without non-differentiable postprocessing.Inspired by [56,58], we take a similar query design.Besides the conventional detection queries utilized in object detection [5,62], additional track queries are introduced to track agents across frames.Specifically, at each time step, initialized detection queries are responsible for detecting newborn agents that are perceived for the first time, while track queries keep modeling those agents detected in previous frames.Both detection queries and track queries capture the agent abstractions by attending to BEV feature B. As the scene continuously evolves, track queries at the current frame interact with previously recorded ones in a self-attention module to aggregate temporal information, until the corresponding agents disappear completely (untracked in a certain time period).Similar to [5], Track-Former contains N layers and the final output state Q A provides knowledge of N a valid agents for downstream prediction tasks.Besides queries encoding other agents surround-ing the ego-vehicle, we introduce one particular ego-vehicle query in the query set to explicitly model the self-driving vehicle itself, which is further used in planning.</p>
<p>MapFormer.We design it based on a 2D panoptic segmentation method Panoptic SegFormer [31].We sparsely represent road elements as map queries to help downstream motion forecasting, with location and structure knowledge encoded.For driving scenarios, we set lanes, dividers and crossings as things, and the drivable area as stuff [28].Map-Former also has N stacked layers whose output results of each layer are all supervised, while only the updated queries Q M in the last layer are forwarded to MotionFormer for agent-map interaction.</p>
<p>Prediction: Motion Forecasting</p>
<p>Recent studies have proven the effectiveness of transformer structure on the motion task [24,25,35,39,40,48,55], inspired by which we propose MotionFormer in the end-toend setting.With highly abstract queries for dynamic agents Q A and static map Q M from TrackFormer and MapFormer respectively, MotionFormer predicts all agents' multimodal future movements, i.e., top-k possible trajectories, in a scene-centric manner.This paradigm produces multi-agent trajectories in the frame with a single forward pass, which greatly saves the computational cost of aligning the whole scene to each agent's coordinate [27].Meanwhile, we pass the ego-vehicle query from TrackFormer through Motion-Former to engage ego-vehicle to interact with other agents, considering the future dynamics.Formally, the output motion is formulated as {x i,k 2 R T2 |i = 1, . . ., N a k = 1, . . ., K} , where i indexes the agent, k indexes the modality of trajectories and T is the length of prediction horizon.</p>
<p>MotionFormer.It is composed of N layers, and each layer captures three types of interactions: agent-agent, 17855 Fig. 13: The overview of UniAD (from [79].) dently through specific embedding layers.Then, intra-modal self-attention (IMSA) and cross-modal self-attention (CMSA) modules are used to capture temporal dependencies within each modality and cross-modal interactions.A bottleneck feature fusion (BFF) module is introduced to integrate the two modalities.This integration enables effective simultaneous prediction of pedestrian crossing intentions and final bounding boxes.Moreover, an uncertainty-aware mechanism is incorporated to dynamically balance the learning of each task.Despite using only two simple modalities as input, PedCMT achieves competitive performance.</p>
<p>Beyond predicting agent motion and intention, M3Net [204] integrates LiDAR and camera data for output perception and prediction tasks, such as 3D detection, BEV map segmentation, and 3D occupancy prediction.The core of M3Net is the modality-adaptive feature integration module, which allows each modality to generate its channel-wise attention weights.These weights transform the initially fused features into modality-adapted representations that retain their modalityspecific advantages.Then, these adapted features are combined to form a unified BEV representation.Additionally, the taskoriented channel scaling module dynamically predicts taskspecific channel scaling weights from the shared BEV features.This alleviates gradient conflicts between tasks and enhances overall performance.Finally, task-specific query initialization strategies further enhance the multi-task decoding process by optimizing feature utilization, resulting in outstanding performance compared to single-task models.</p>
<p>C. MTL for Planning and Control Tasks</p>
<p>Due to the relatively limited number of MTL methods that focus on planning or control, we discuss these tasks in a single section.In many studies [17], [79], [169], [170], planning or control serves as the primary objective, while auxiliary tasks such as perception or prediction are incorporated to enhance shared representations and reduce accumulating errors and redundant computation.In contrast, several methods [18], [19] adopt an end-to-end paradigm that directly generates control commands from raw sensor inputs.We provide an overview of each task in the following subsections.</p>
<p>1) Planning-centric MTL: For MTL in the planning module, which usually aims to address motion planning, the primary outputs are continuous trajectories or waypoints.</p>
<p>Adding auxiliary tasks such as perception and prediction is jointly optimized to enhance representation learning and planning accuracy.For example, UniAD [79] is a unified ADS that performs perception, prediction, and planning tasks in a single pass (see Fig. 13).The architecture is primarily designed to be planning-oriented and considers the impact of auxiliary tasks on the planning head.Ablation studies show that jointly motion forecasting and occupancy prediction improve planning safety, as reflected in reduced collision rates and trajectory errors.However, the individual contributions of each prediction task are not separately analyzed.Additionally, the Transformer-based query design facilitates shared attention across tasks, maintaining perception performance while enhancing planning.Different from UniAD, which relies on multi-view vision-only inputs, FusionAD [170] is a multimodal input architecture that integrates camera and LiDAR information into a shared BEV space through a Transformerbased encoder.Specifically, FusionAD introduces a modalityaware prediction module and a status-aware planning module.These modules leverage modality-specific context, ego vehicle state embeddings, and a differentiable collision loss to jointly refine planning quality.Through multi-modal fusion and intermediate perception outputs, FusionAD improves planning accuracy and safety.</p>
<p>Rao et al. [169] propose a lightweight MTL architecture with planning as a primary task.The model integrates depth estimation, semantic segmentation, BEV map generation, BEV object detection, and speed prediction.To mitigate task interference, they propose a meta-learning-based multi-task optimization strategy that dynamically adjusts loss weights based on inter-task affinity.Building on the same waypoint planningcentric objective, their follow-up work [17] introduces a temporal fusion module and separates motion features from BEV features via a dedicated Transformer-based motion decoder.This design enhances the integration of temporal information into the planning process while reducing negative transfer.</p>
<p>2) Control-centric MTL: For control tasks, MTL aim to produce low-level vehicle control commands as the output.Existing MTL approaches mainly follow two paradigms: module pipelines or end-to-end architectures.In modular approaches, auxiliary tasks such as perception, prediction, or planning are jointly optimized to enrich shared representations and improve the interpretability and robustness of the control policy.In Fig. 14: The architecture of multi-task attention-aware network for ADS (adapted from [15]).contrast, end-to-end methods directly map raw sensor inputs to control commands without relying on intermediate stages, offering a streamlined solution at the expense of transparency and task disentanglement.</p>
<p>An example of the module pipeline, Agand et al. [206] propose an MTL framework that jointly learns perception and control tasks to improve control accuracy.The model relies on RGB-D as input.During training, teacher networks distill knowledge into the student network via adaptive feature matching (AFM).The perception module employs an attention-CNN fusion mechanism to combine RGB and semantic depth cloud features.The control module, consisting of two individual gated recurrent unit (GRU)-based waypoint and navigation branches, outputs trajectory waypoints and vehicle commands.Similarly, Ishihara et al. [15] introduce a shared ResNet-34 backbone followed by separate heads for depth estimation, semantic segmentation, traffic light classification, and steering prediction (see Fig. 14).Unlike [206] work, this work does not explicitly model task interactions but leverages attention mechanisms to enhance task-specific feature extraction.Experiments show that incorporating traffic light state classification helps improve control accuracy and reduces redlight violations.</p>
<p>On the other hand, some works focus on directly generating control commands in an end-to-end pipeline.In early work, Yang et al. [18] propose an end-to-end multi-modal MTL framework that takes images and historical vehicle speed sequences as input to jointly predict steering angle and speed.Although only the control outputs are used during inference, experimental results demonstrate that multi-task training improves the smoothness and accuracy of steering predictions.Recently, Guo et al. [19] propose a multi-modal, multi-task end-to-end model for predicting the ego vehicle speed and steering angle.Their architecture incorporates RGB images, depth maps, semantic segmentation maps, and historical vehicle state sequences as input, jointly predicting low-level control outputs.The model integrates their proposed SECA attention, a vision Transformer for spatial fusion, and a Residual-CNN-BiGRU structure for temporal modeling.By employing multi-step prediction and spatiotemporal feature fusion, the approach improves robustness across diverse driving scenarios.</p>
<p>D. MTL for V2X-based Cooperative Driving</p>
<p>Despite its advantages, cooperative driving introduces unique challenges for MTL, including heterogeneous data sources [127], [222] and communication constraints [119], [222].Recent studies have investigated deep MTL approaches for V2X-based cooperative driving, which jointly process multiple tasks by sharing intermediate features across both agents and tasks.These approaches employ collaborative fusion mechanisms to enhance robustness against occlusion and localization errors, while also improving communication efficiency in dynamic traffic environments.</p>
<p>AR2VP [222] is an adaptive road-to-vehicle perception framework that improves cooperative perception in dynamic traffic scenarios.AR2VP leverages the stable and wide-range sensing capability of roadside units to compensate for vehicle perception.Importantly, AR2VP formulates V2X perception as a multi-task problem, jointly addressing 3D object detection and semantic segmentation while adapting to both intra-scene and inter-scene variations.Similarly, Yan et al. [127] propose a multi-task collaborative perception framework that jointly performs 3D object detection and BEV map segmentation.By exploiting the complementarity between these tasks, the model generates a global drivable area map to guide feature fusion.Then it filters non-drivable regions and reduces inter-agent feature conflicts and redundancies.To ensure robustness under impaired communication, they introduce a dynamic communication trust probability model and develop a two-stage impaired communication recovery (ICR) module to restore corrupted features, thereby maintaining reliable perception performance.</p>
<p>On the other hand, several studies [115], [126], [128], [223] extend MTL for V2X-based cooperative driving to jointly address both perception and prediction tasks.V2VNet [115] combines perception with motion forecasting in a V2V communication setting.It shares a compressed perception-andprediction (P&amp;P) intermediate representation between neighbouring cars and uses a spatial graph neural network to aggregate multi-vehicle information.This allows each vehicle to perceive occluded objects and forecast trajectories beyond its sensor range.However, V2VNet only performs single-frame cooperative perception, which limits its ability to recover occluded objects and hinders long-term temporal tasks such as motion prediction, due to the lack of temporal cues.To address this limitation, V2XPnP [128], which is a spatio-temporal fusion paradigm for multi-agent perception and prediction, aggregates its historical BEV features into a compact representation before transmission.This enables efficient sharing of temporally enriched information.Additionally, V2XPnP uses temporal attention, self-spatial attention, multi-agent spatial attention, and map attention to capture rich temporal and spatial dependencies across frames and agents.Besides architectural advances, a safety-oriented benchmark [223] has recently been proposed for multi-task evaluation in V2X settings.This benchmark introduces a large-scale dataset and a multi-task formulation that jointly supports 3D detection, motion forecasting, and accident prediction.Through synchronized vehicle and infrastructure sensors to simulate diverse
APA = 1 D X d2D |T P | d |T P | d + 1 2 |F P | d + 1 2 |F N| d(1)
True Positive metrics.In addition to the APA, we calculate several True Positive metrics (TP metrics) for true positive accident predictions to provide more detailed performance interpretation.This includes the error terms for IDs, positions and time between the ground truth accident and the predicted accident.For TP metrics calculation, we set the position difference threshold to 10 meters when deciding the true positive predictions.As for ID error, if the predicted accident objects' ids are the same as the ground truth's, then it equals to zero, otherwise equals to one.For position and time error, we present them using their native units (meters and seconds) and calculate the absolute difference compared to the ground truth.For each TP metric, we calculate the average value over all true positive predictions.</p>
<p>Experiment</p>
<p>Evaluated tasks.To show the usefulness of our proposed DeepAccident dataset as a V2X motion and accident prediction benchmark, we focus on the end-to-end motion and accident prediction task and choose the camera-based setting.Besides, we train another 3D object detection head with the motion head to simultaneously compare the perception ability between the V2X models and the single-vehicle model.We report the performance on DeepAccident's validation split in the following sections and include results on the testing split in the appendix.We start by comparing different V2X fusion modules and choosing the optimal one.After that, we compare the overall performance of V2X models with different agent configurations to the single-vehicle model and provide further ablation analysis that considers accident visibility, longer prediction horizon, and robustness on pose error and latency.Additionally, we conduct exper- Besides perception and prediction, MTL in V2X has been extended to planning tasks as well.UniV2X [119] is an end-to-end MTL framework that jointly learns perception, mapping, prediction, and planning within a shared encoder and task-specific decoders.This is considered a classical hardparameter sharing.Rather than optimizing each module in isolation, UniV2X uses cross-view feature sharing between vehicles and infrastructure to jointly refine all modules with the final goal of safer motion planning.By treating V2X inputs holistically and training the entire pipeline in an endto-end manner, this approach enhances planning performance, including reductions in collisions and off-road deviations.</p>
<p>V. RESEARCH GAPS AND CONCLUDING REMARKS</p>
<p>In this survey, we comprehensively review MTL methods in CAVs, covering key modules including perception, prediction, planning, control, and multi-agent collaboration.We present representative methods for each module and discuss their strengths and limitations.Among these, the perception module has received the most attention.Various architectures, including CNNs, Transformers, and VLMs, have been explored to enhance performance through task sharing and feature reuse.However, several challenges remain.The negative transfer is the main challenge.For example, lane detection features might interfere with object detection.To address this, models need to be designed based on task characteristics, with more structure decoupling when necessary.Another challenge is task loss balancing.If one task's loss dominates, it may surpass others.Dynamic or uncertainty-based weighting strategies are useful but can be ineffective in complex scenarios, such as when some tasks involve multiple auxiliary losses.</p>
<p>In the prediction module, MTL is commonly employed in joint perception-prediction frameworks.Instead of treating detection, tracking, and motion prediction as separate sequential tasks, recent approaches integrate them to enable simultaneous actor perception and trajectory prediction.These unified models generally adopt either single-modal or multi-modal input strategies, each with its trade-offs.Single-modal strategies provide fast inference and low-cost data processing.They are well-suited for real-time tasks but lack geometric depth and may be invalid under certain scenes, such as occlusion or adverse weather.In contrast, the multi-modal strategy fuses cameras, LiDAR, or radar to build richer scene representations.This leads to more accurate long-horizon predictions and increased robustness to sensor failures or complex traffic scenarios, albeit at the cost of higher computational demands, greater memory usage, and increased fusion complexity.In practice, the choice between these strategies should align deployment priorities.</p>
<p>Compared to perception and prediction modules, fewer studies have used MTL in the planning and control modules.In planning-centric tasks, MTL enables safer and more accurate trajectories.By jointly training prediction with motion planning, MTL reduces the cascading errors in module pipelines and reduces collision rates without extra inference cost [79].Additionally, MTL reduces redundant encoders.Compared to multiple individual networks, MTL cuts the overall compute [170], making real-time planning more feasible in-vehicle computing.For control-centric tasks, MTL is further divided into module and end-to-end designs.The module pipeline uses auxiliary perception or prediction heads to regularize and inform control.This design reduces redundancy and stabilizes command predictions under complex conditions [206].In contrast, an end-to-end pipeline adopts a unified model to output control commands directly and does not need separate perception and control modules.This design provides noticeably smoother and more accurate steering and throttle outputs [18].Additionally, due to learning spatiotemporal dependencies in one pass, it has stronger generalization across different environments [19].However, compared to the module pipeline, the end-to-end pipeline lacks interpretability.</p>
<p>Through V2X communication, CAVs can exchange information with other vehicles and infrastructure.This plays a critical role in multi-agent cooperative perception, prediction, and planning.In these scenarios, MTL is applied after fusing multi-agent information.For example, cooperative perception systems aggregate features from different viewpoints using spatial or temporal alignment, and then use MTL to jointly perform tasks such as object detection, semantic segmentation, and depth estimation.This mitigates the limitations of ego vehicle sensors and enhances task accuracy.Similarly, shared motion intentions or planned trajectories obtained via V2X can enhance MTL-based joint prediction and planning.However, applying MTL in cooperative driving scenarios presents several challenges.First, inter-agent data heterogeneity and communication delays can introduce inconsistencies that degrade task performance.Second, task interactions may vary across agents.This reduces the effectiveness of shared representations and increases the risk of negative transfer.Moreover, if cross-agent fusion is not well-aligned with task semantics, it can amplify conflicts between tasks.To address these challenges, recent studies have proposed strategies such as communication-aware feature alignment [112] and robust optimization techniques [106], [125] to handle partial or missing V2X data.</p>
<p>A. Research Gaps</p>
<p>Despite MTL in CAVs having made considerable progress, several research gaps remain that must be addressed.</p>
<p>Computational efficiency and deployment.CAVs operate on devices with limited computing resources.Therefore, it is critical to improve the efficiency and deployability of MTL models.Model compression and optimization techniques are essential for reducing computational and storage overhead, and minimizing energy consumption [21], [224].Additionally, adapting models for edge devices without sacrificing performance is another challenge [20], [196].Due to CAVs being sensitive to real-time, it must also enhance inference speed to meet the requirements [225], [226].</p>
<p>MTL paradigms and optimization.Developing effective MTL paradigms and robust optimization strategies that improve task performance and mitigate negative transfer among tasks is essential [17], [25], [209].Current approaches often decouple task head structures based on all individual task characteristics [45], [87], [178] to reduce interference, even when tasks belong to the same type.However, such designs require substantial prior knowledge and lead to high design costs.There is a growing need for adaptive methods or unified architectures that eliminate the need for task-specific customization.Furthermore, incorporating adaptive task-weighting strategies and enabling real-time parameter adjustment based on environmental scenarios could significantly improve both efficiency and adaptability [17], [225].</p>
<p>Lack of benchmarks and evaluation standards.Current MTL studies often evaluate tasks independently, lacking a unified metric to assess joint performance.While many acknowledge negative transfer, few attempt to quantify it.Real-time performance is also assessed inconsistently, where some works use acceleration techniques like TensorRT [163], [227], while others do not [25], [26].This makes direct comparison unfair.A standardized benchmark is needed to ensure consistent evaluation of MTL models in CAVs.</p>
<p>Lack of real-world evaluation.Most of the current studies only evaluate the model in lab settings.However, deploying models in the real-world will introduce additional challenges due to the dynamic environment.Therefore, a comprehensive on-board evaluation is essential to assess model robustness and ensure practical applicability [183], [228].Furthermore, deployment in safety-critical systems also requires rigorous validation aligned with established standards [214], [216].</p>
<p>Limited exploration of MTL for V2X-based cooperation Existing MTL models are mostly developed for single-agent systems [20], [25], [178].However, in V2X-based cooperative driving, vehicles must share and align information under conditions like heterogeneous sensors [127], [222] and unstable communication [119], [222].These issues can lead to data inconsistency and misalignment across agents, making it difficult to design a unified MTL model for cooperative driving scenarios.</p>
<p>Agentic AI for cooperative decision-making.Integration of agentic AI [229], [230] into cooperative multi-agent MTL systems is a promising but underexplored direction.Agentic AI, which focuses on autonomy, proactive behavior, goal-oriented reasoning, and decision-making, could offer significant advantages in dynamic and decentralized driving environments [231], [232].By embedding such capabilities into cooperative frameworks, it may become possible to enhance coordination between agents, improve adaptability to environmental changes, and increase resilience to partial failures.Despite this promise, most existing MTL approaches do not support reasoning at the agent level or enable sophisticated inter-agent interaction planning.Advancing this integration represents an open research frontier with considerable implications for the development of more intelligent and reliable cooperative CAVs.</p>
<p>Dataset limitations and domain adaptation.Dataset limitations and domain adaptation remain critical challenges.Expanding and balancing datasets to improve model generalization is necessary [175], [228].Developing data augmentation and synthesis techniques can help cover lacking scenarios and improve model robustness [175], [228].Additionally, developing domain adaptation techniques to adapt models trained on certain datasets to perform well in different domains or real-world conditions is important [15], [205].</p>
<p>Responsible AI.As ADS technologies continue to evolve, it is critical to ensure these technologies are developed and deployed responsibly.Responsible AI emphasizes fairness, accountability, transparency, and mitigation of bias within models [233].Specifically, it is important to minimize biases to unequal treatment of certain groups of road users or passengers [234].Moreover, the AI model used in ADS requires ensuring it is safe and robust to avoid behaviours that hurt humans or other agents.Therefore, it is essential to identify, audit, and mitigate biases, ensure fairness in decision-making, and implement ethical AI principles in ADS [235], [236].Finally, enhancing responsible AI will boost trust for users.</p>
<p>B. concluding remarks</p>
<p>This survey provides an in-depth review of MTL methods in CAVs.We begin with a systematic overview of CAVs from hardware and software layers to V2X communication.Next, we provide the core MTL concepts, including problem formulation, parameter-sharing paradigms and optimization techniques.Then, we review MTL methods from the past seven years that have been applied to perception, prediction, planning, control, and multi-agent collaboration.We also identify major research gaps and outline promising directions for future MTL development in CAVs.We hope this survey serves as a valuable resource for researchers seeking to deepen their understanding and advance innovation in this field.</p>
<p>Fig. 1 :
1
Fig. 1: Overview of CAVs.The classical processing pipeline (black solid arrows) includes sensing, perception, prediction, planning, and control.V2X communication (black dashed arrows) provides bidirectional exchange of cooperative information between other vehicles and infrastructure.End-to-end learning approaches (orange module, red arrows) bypass intermediate modules by directly mapping sensor inputs or V2X messages to low-level control commands.</p>
<p>Fig. 2 :
2
Fig. 2: Diagram of driving behaviour intention.(Adapted From [81].)</p>
<p>(a) Single-agent autonomous driving.Each vehicle relies on its onboard sensors for perception, prediction, and planning.Therefore, each vehicle's perception range is limited and independent.For example, the blue vehicle cannot perceive the black vehicle due to occlusion by the yellow vehicle.(b)V2X-based multi-agent cooperative driving.Vehicles communicate via V2X and share information and planned trajectories (yellow arrows) via infrastructure support.This cooperation expands each vehicle's awareness and enables more accurate prediction of surrounding agents' movements.</p>
<p>Fig. 3 :
3
Fig. 3: Comparision of single-agent and V2X-based multi-agent cooperation paradigms.</p>
<p>Fig. 4 :
4
Fig. 4: Hard-parameter sharing paradigm.</p>
<p>Fig. 5 :
5
Fig. 5: Soft-parameter sharing paradigms.</p>
<p>Fig. 6 :
6
Fig. 6: Hybrid-parameter sharing paradigms.</p>
<p>Fig. 7 :
7
Fig. 7: The architecture of the MT-Faster R-CNN structure.(Adapted from [135].)</p>
<p>Fig. 8 :
8
Fig.8: The overview of CP-MTL SSD.(Adapted from[192].)</p>
<p>Fig. 3 .
3
Fig.3.The architecture of Sparse U-PDP.Sparse U-PDP contains three main components: backbone, multi-scale features encoder, kernel based unified decoder.Given the input image, the backbone extracts the multi-scale image features.The encoder employs FPN and fuses the multi-scale features.'4'or'2' denote the upsampling by a factor 4 or 2. The kernel-based unified decoder consists of two branches: dynamic convolution kernel branch and multi-scale sample branch.The multi-scale sample branch is not specifically reflected in the figure, but it will be introduced in detail in the method chapter.In the kernel branch, we divide the unified feature representation into thing kernel and stuff kernel according to different tasks.Finally, the instance-aware kernel and multi-scale sampling space use the calculation method of matrix multiplication to realize recognition and prediction.</p>
<p>Fig. 9 :Figure 3 .
93
Fig.9: The architecture of Sparse U-PDP.(from[26]).</p>
<p>Fig. 10 :
10
Fig. 10: The architecture of VE-Prompt.(from [209]).</p>
<p>4 | 4 F ig. 1 .
441
Transportation Safety and Environment , 2023, Vol.5: No.The ar c hitectur e of pr ediction method.F ig. 2. Data flo w dia gr am of LSTM units.</p>
<p>Fig. 11 :
11
Fig. 11: The pipeline of hybrid-parameter sharing LSTM-based MTL.(from [221].)</p>
<p>Fig. 2 .
2
Fig. 2. The proposed PedCMT for pedestrian crossing intention prediction.The bounding box (bbox) and ego-vehicle velocity are first embedded by applying multi-layer perception (MLP).The resulting features combined with learnable class tokens are fed into the cross-modal transformer encoder mainly encompassing intra-modal self-attention (IMSA) and cross-modal self-attention (CMSA) for feature extraction and interaction.Then, the bottleneck feature fusion (BFF) is used to integrate two features, and finally multi-task learning (MTL) is devised to forecast the crossing intention and end bounding box jointly.</p>
<p>1 and u intra 2 are
2
prepended to the feature representation H1 and H2 along the time axis, respectively.Accounting for the permutation invariance of subsequent self-attention operations, the sinusoidal positional encoding (PE) is applied to retain the positional information, thus generating the feature map Hm Hm =</p>
<p>Pm =  f0, rm,1,    ,rm,T  .Finally, following the feed-forward Authorized licensed use limited to: the Leddy Library at the University of Windsor.Downloaded on May 17,2025 at 16:26:56 UTC from IEEE Xplore.Restrictions apply.</p>
<p>Fig. 12 :
12
Fig. 12: The architecture of PedCMT.(from [205].)</p>
<p>Figure 4 :
4
Figure 4: Network details of the proposed V2XFormer.We use the three-V2X-agent setting consisting of ego AV, AV, and Infra for illustration.V2X agents in V2XFormer utilize a shared-weight BEV extractor to extract BEV features based on multi-view camera observation history within the previous N frames.</p>
<p>Experiment settings.We use the settings for motion prediction in BEVerse(Zhang et al. 2022) and FIERY(Hu et al. 2021) as our default experiment settings.For 3D object detection, the BEV ranges are [-51.2m,51.2m] for both X-axis and Y-axis with a 0.8m interval, while for motion prediction, the ranges are [-50.0m,50.0m] with a 0.5m interval.The models use 1 second of past observations to predict 2 seconds into the future, corresponding to a temporal context of 3 past frames including the current frame and 4 future frames at 2Hz.We choose BEVerse-tiny as the singlevehicle model.For training, we train the models on the training split of DeepAccident for 20 epochs.As for evaluation, we randomly sample five BEV features from the learned motion Gaussian distribution along with the mean of this learned distribution to generate six different motion prediction results.Only the motion prediction result obtained from the mean vector of the learned Gaussian distribution is used to assess motion prediction performance.For accident prediction, we consider a prediction indicating the occurrence of an accident when any of the sampled motion predictions is analyzed to cause a collision accident, prioritizing safety.</p>
<p>Fig. 15 :
15
Fig. 15: The network details V2XFormer.(Adapted from [223].)</p>
<p>TABLE II :
II
Comparison of PID Control and MPC
Category PID ControlMPCAdvantages Simple structure; Low computational cost; Effective for real-time error correction.Handles complex constraints; Multi-objective optimization; Predictive control using fu-ture state estimation.Challenges Requires parameter tuning; Struggles with multi-constraint optimization; Lacks predictive ability.Computationally intensive; Sensitive to model inaccura-cies; Prone to steady-state errors.</p>
<p>TABLE III :
III
Detailed Comparison of DSRC and C-V2X
FeatureDSRC (IEEE 802.11p/11bd)C-V2X (LTE/5G NR)Typical one-hop range Air-interface / resource allocation End-to-end latency Throughput Data rate Reliability-packet delivery rate (PDR) Low: Falls sharply beyond 500 m or in congestion High: Degrades more gracefully; higher PDR mid-/long-range Short: LOS  300-500 m; NLOS 50% loss Long: 500 m-1 km (LTE);  1 km (5G NR) Pure CSMA/CA (EDCA) contention Hybrid: base-station scheduling + SB-SPS (distributed) High Low Narrow Wide Low High Congestion / hidden-node tolerance High collision rate under dense traffic Semi-persistent allocation mitigates collisions Doppler robustness Sensitive above 160 km/h DMRS + wider sub-carrier spacing improve robustness Infrastructure dependency Low: requiring onboard units in vehicles High: depends heavily on cellular infrastructure</p>
<p>TABLE IV :
IV
Comparison Between Single-Agent and V2X-Based Multi-Agent Cooperative System in Autonomous Driving
FeatureSingle-Agent SystemV2X-Based Multi-Agent Cooperative SystemSensor Coverage Information Completeness Interaction Modeling Robustness Communication Requirement Deployment Complexity Typical ScenariosLimited to ego vehicle's FOV; suffers from occlusion Expanded through shared sensing; mitigates blind spots Incomplete in complex or distant scenes Access to extended scene context via V2X Relies on local inference; limited accuracy Sharing motion state improves prediction Vulnerable to sensor failures and adverse weather Enhanced resilience via V2X redundancy Fully self-contained; no networking needed Requires reliable, low-latency communication infrastructure Low (easier to deploy and validate) High (requires synchronization and supporting infrastructure) Highways, sparse traffic, structured roads Urban intersections, merging lanes, occluded environmentsthe performance of downstream modules. Table IV compares these systems and highlights their advantages and limitations.</p>
<p>TABLE V :
V
Comparison of three parameter sharing paradigms
ParadigmParameters Inference Speed Task Conflict SensitivityApplication ScenarioHard-parameter sharing Soft-parameter sharing Hybrid-parameter sharingLow High MediumFast Slow MediumHigh Low MediumHomogeneous tasks, resource-limited Heterogeneous tasks, sufficient compute Complex tasks need balancing speed and performance
This research was undertaken, in part, thanks to funding from the Canada Research Chairs Program, and in part by the NSERC's CREATE program on TrustCAV.
A survey on cooperative architectures and maneuvers for connected and automated vehicles. B Hfner, V Bajpai, J Ott, G A Schmitt, IEEE Communications Surveys &amp; Tutorials. 2412021</p>
<p>Architectural design alternatives based on cloud/edge/fog computing for connected vehicles. H Wang, T Liu, B Kim, C.-W Lin, S Shiraishi, J Xie, Z Han, IEEE Communications Surveys &amp; Tutorials. 2242020</p>
<p>Digital twin technology for intelligent vehicles and transportation systems: A survey on applications, challenges and future directions. X Gu, W Duan, G Zhang, J Hou, L Peng, M Wen, F Gao, M Chen, P.-H Ho, IEEE Communications Surveys &amp; Tutorials. 2025</p>
<p>Tools and methodologies for autonomous driving systems. A Bhat, S Aoki, R Rajkumar, Proceedings of the IEEE. the IEEE2018106</p>
<p>Autonomous cars: Research results, issues, and future challenges. R Hussain, S Zeadally, IEEE Communications Surveys &amp; Tutorials. 2122018</p>
<p>Autonomous driving cars in smart cities: Recent advances, requirements, and challenges. I Yaqoob, L U Khan, S A Kazmi, M Imran, N Guizani, C S Hong, IEEE Network. 3412019</p>
<p>A functional reference architecture for autonomous driving. S Behere, M Trngren, Information and Software Technology. 201673</p>
<p>A survey of autonomous driving: Common practices and emerging technologies. E Yurtsever, J Lambert, A Carballo, K Takeda, IEEE Access. 82020</p>
<p>Mcs-yolo: A multiscale object detection method for autonomous driving road environment recognition. Y Cao, C Li, Y Peng, H Ru, IEEE Access. 112023</p>
<p>Small-object detection based on yolov5 in autonomous driving systems. B Mahaur, K Mishra, Pattern Recognition Letters. 1682023</p>
<p>On the real-world adversarial robustness of real-time semantic segmentation models for autonomous driving. G Rossolini, F Nesti, G D'amico, S Nair, A Biondi, G Buttazzo, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>Lcfnets: compensation strategy for real-time semantic segmentation of autonomous driving. L Yang, Y Bai, F Ren, C Bi, R Zhang, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>A multi-task learning network with a collision-aware graph transformer for traffic-agents trajectory prediction. B Yang, F Fan, R Ni, H Wang, A Jafaripournimchahi, H Hu, IEEE Transactions on Intelligent Transportation Systems. 2024</p>
<p>Language prompt for autonomous driving. D Wu, W Han, T Wang, Y Liu, X Zhang, J Shen, arXiv:2309.043792023arXiv preprint</p>
<p>Multitask learning with attention for end-to-end autonomous driving. K Ishihara, A Kanervisto, J Miura, V Hautamaki, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Mult: An end-to-end multitask learning transformer. D Bhattacharjee, T Zhang, S Ssstrunk, M Salzmann, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20221241</p>
<p>Enhancing autonomous driving: A low-cost monocular end-to-end framework with multi-task integration and temporal fusion. Z Rao, Y Cai, H Wang, Y Lian, Y Zhong, L Chen, Y Li, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>End-to-end multi-modal multi-task vehicle control for self-driving cars with visual perceptions. Z Yang, Y Zhang, J Yu, J Cai, J Luo, 2018 24th International Conference on Pattern Recognition (ICPR). </p>
<p>Multi-modal information fusion for multi-task end-to-end behavior prediction in autonomous driving. G Baicang, L Hao, Y Xiao, C Yuan, J Lisheng, W Yinlin, Neurocomputing. 6341298572025</p>
<p>You only look at once for real-time and generic multi-task. J Wang, Q J Wu, N Zhang, IEEE Transactions on Vehicular Technology. 2024</p>
<p>Multinet: Real-time joint semantic reasoning for autonomous driving. M Teichmann, M Weber, M Zoellner, R Cipolla, R Urtasun, 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE2018</p>
<p>Safety-enhanced autonomous driving using interpretable sensor fusion transformer. H Shao, L Wang, R Chen, H Li, Y Liu, Conference on Robot Learning. PMLR2023</p>
<p>Robust target recognition and tracking of self-driving cars with radar and camera information fusion under severe weather conditions. Z Liu, Y Cai, H Wang, L Chen, H Gao, Y Jia, Y Li, IEEE Transactions on Intelligent Transportation Systems. 2372021</p>
<p>Multimodal end-to-end autonomous driving. Y Xiao, F Codevilla, A Gurram, O Urfalioglu, A M Lpez, IEEE Transactions on Intelligent Transportation Systems. 2312020</p>
<p>Yolop: You only look once for panoptic driving perception. D Wu, M.-W Liao, W.-T Zhang, X.-G Wang, X Bai, W.-Q Cheng, W.-Y Liu, Machine Intelligence Research. 1962022</p>
<p>Sparse u-pdp: A unified multi-task framework for panoptic driving perception. H Wang, M Qiu, Y Cai, L Chen, Y Li, IEEE Transactions on Intelligent Transportation Systems. 24103202023</p>
<p>Designing the 5th-generation waymo driver. Llc Waymo, 2020</p>
<p>A low cost sensors approach for accurate vehicle localization and autonomous driving application. R Vivacqua, R Vassallo, F Martins, Sensors. 171023592017</p>
<p>Evaluating the limits of a lidar for an autonomous driving localization. L De Paula Veronese, F Auat-Cheein, F Mutz, T Oliveira-Santos, J E Guivant, E De Aguiar, C Badue, A F De, Souza, IEEE Transactions on Intelligent Transportation Systems. 2232020</p>
<p>Emergent visual sensors for autonomous vehicles. Y Li, J Moreau, J Ibanez-Guzman, IEEE Transactions on Intelligent Transportation Systems. 2452023</p>
<p>Extensive tests of autonomous driving technologies. A Broggi, M Buzzoni, S Debattisti, P Grisleri, M C Laghi, P Medici, P Versari, IEEE Transactions on Intelligent Transportation Systems. 1432013</p>
<p>On the performance of one-stage and two-stage object detectors in autonomous vehicles using camera data. M Carranza-Garca, J Torres-Mateo, P Lara-Bentez, J Garca-Gutirrez, Remote Sensing. 131892020</p>
<p>Monocular 3d multi-object tracking with an ekf approach for long-term stable tracks. A Reich, H.-J Wuensche, 2021 IEEE 24th International Conference on Information Fusion (FUSION). IEEE2021</p>
<p>Tesla vision update: Replacing ultrasonic sensors with tesla vision. Inc Tesla, 2024</p>
<p>A tutorial on 5g nr v2x communications. M H C Garcia, A Molina-Galan, M Boban, J Gozalvez, B Coll-Perales, T , A Kousaridas, IEEE Communications Surveys &amp; Tutorials. 2332021</p>
<p>A survey of collaborative machine learning using 5g vehicular communications. S V Balkus, H Wang, B D Cornet, C Mahabal, H Ngo, H Fang, IEEE Communications Surveys &amp; Tutorials. 2422022</p>
<p>A survey on approximate edge ai for energy efficient autonomous driving services. D Katare, D Perino, J Nurmi, M Warnier, M Janssen, A Y Ding, IEEE Communications Surveys &amp; Tutorials. 2542023</p>
<p>Enhanced perception for autonomous vehicles at obstructed intersections: An implementation of vehicle to infrastructure (v2i) collaboration. Y Mo, R Vijay, R Rufus, N D Boer, J Kim, M Yu, Sensors. 2439362024</p>
<p>Occlusion-guided multi-modal fusion for vehicle-infrastructure cooperative 3d object detection. H Chu, H Liu, J Zhuo, J Chen, H Ma, Pattern Recognition. 1571109392025</p>
<p>Rethinking the role of infrastructure in collaborative perception. H Bae, M Kang, M Song, H Ahn, European Conference on Computer Vision. Springer2025</p>
<p>Networking and communications in autonomous driving: A survey. J Wang, J Liu, N Kato, IEEE Communications Surveys &amp; Tutorials. 2122018</p>
<p>Effective adaptation in multi-task co-training for unified autonomous driving. X Liang, Y Wu, J Han, H Xu, C Xu, X Liang, Advances in Neural Information Processing Systems. 202235</p>
<p>Multitask learning. R Caruana, Machine Learning. 199728</p>
<p>Rtmdet-mgg: A multitask model with global guidance. H Wang, Q Qin, L Chen, Y Li, Y Cai, IEEE Transactions on Intelligent Transportation Systems. 2024</p>
<p>Yolopx: Anchorfree multi-task learning network for panoptic driving perception. J Zhan, Y Luo, C Guo, Y Wu, J Meng, J Liu, Pattern Recognition. 1481101522024</p>
<p>A loss-balanced multi-task model for simultaneous detection and segmentation. W Zhang, K Wang, Y Wang, L Yan, F.-Y Wang, Neurocomputing. 4282021</p>
<p>A survey on multi-task learning. Y Zhang, Q Yang, IEEE Transactions on Knowledge and Data Engineering. 34122021</p>
<p>Squeezedet: Unified, small, low-power fully convolutional neural networks for real-time object detection for autonomous driving. B Wu, F Iandola, P H Jin, K Keutzer, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition Workshops2017</p>
<p>Pole-based realtime localization for autonomous driving in congested urban scenarios. L Weng, M Yang, L Guo, B Wang, C Wang, 2018 IEEE International Conference on Real-time Computing and Robotics (RCAR). IEEE2018</p>
<p>The architectural implications of autonomous driving: Constraints and acceleration. S.-C Lin, Y Zhang, C.-H Hsu, M Skach, M E Haque, L Tang, J Mars, Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems. the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems2018</p>
<p>An overview of multi-task learning in deep neural networks. S Ruder, arXiv:1706.050982017arXiv preprint</p>
<p>Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. H Tang, J Liu, M Zhao, X Gong, Proceedings of the 14th ACM Conference on Recommender Systems. the 14th ACM Conference on Recommender Systems2020</p>
<p>Multi-task deep learning for medical image computing and analysis: A review. Y Zhao, X Wang, T Che, G Bao, S Li, Computers in Biology and Medicine. 1531064962023</p>
<p>Multi-task learning for dense prediction tasks: A survey. S Vandenhende, S Georgoulis, W Van Gansbeke, M Proesmans, D Dai, L Van Gool, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4472021</p>
<p>A brief review on multi-task learning. K.-H Thung, C.-Y Wee, Multimedia Tools and Applications. 201877</p>
<p>A survey of multi-task deep reinforcement learning. N Vithayathil Varghese, Q H Mahmoud, Electronics. 9913632020</p>
<p>An overview of multi-task learning. Y Zhang, Q Yang, National Science Review. 512018</p>
<p>Learning multiple tasks with multilinear relationship networks. M Long, Z Cao, J Wang, P S Yu, Advances in Neural Information Processing Systems. 201730</p>
<p>Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification. Y Lu, A Kumar, S Zhai, Y Cheng, T Javidi, R Feris, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. A Kendall, Y Gal, R Cipolla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>A review of vision-based multi-task perception research methods for autonomous vehicles. H Wang, J Li, H Dong, Sensors. 2582025</p>
<p>Autonomous driving system: A comprehensive survey. J Zhao, W Zhao, B Deng, Z Wang, F Zhang, W Zheng, W Cao, J Nan, Y Lian, A F Burke, Expert Systems with Applications. 2421228362024</p>
<p>A survey of mmwave radarbased sensing in autonomous vehicles, smart homes and industry. H Kong, C Huang, J Yu, X Shen, IEEE Communications Surveys &amp; Tutorials. 2712024</p>
<p>Sensor technology in autonomous vehicles: A review. S Campbell, N O'mahony, L Krpalcova, D Riordan, J Walsh, A Murphy, C Ryan, 2018 29th Irish Signals and Systems Conference (ISSC). IEEE2018</p>
<p>Autopilot. Tesla, 2025</p>
<p>Waymo open dataset. Waymo, </p>
<p>An overview of sensors in autonomous vehicles. H A Ignatious, M Khan, Procedia Computer Science. 1982022</p>
<p>Deep learning for inertial positioning: A survey. C Chen, X Pan, IEEE Transactions on Intelligent Transportation Systems. 2024</p>
<p>Computing systems for autonomous driving: State of the art and challenges. L Liu, S Lu, R Zhong, B Wu, Y Yao, Q Zhang, W Shi, IEEE Internet of Things Journal. 882020</p>
<p>Gps/imu data fusion using multisensor kalman filtering: introduction of contextual aspects. F Caron, E Duflos, D Pomorski, P Vanheeghe, Information Fusion. 722006</p>
<p>Vehicle computer. Robert Bosch, Gmbh , </p>
<p>Drive agx orin developer kit. Nvidia Developer, March 10, 2025</p>
<p>Horizon journey series. ac- cessed: 2025-05-092023Horizon Robotics</p>
<p>Eyeq chip. Mobileye, 2025. March 10, 2025</p>
<p>Snapdragon ride. Qualcomm, 2025. March 10, 2025</p>
<p>Upgrade of the ee-architecture of an electric test vehicle with drive-by-wire component. M Obertino, 2023Politecnico di TorinoPh.D. dissertation</p>
<p>Controller design for steer-bywire system. S M Sahboun, A A Emhemed, Journal of Mechatronics and Robotics. 612022</p>
<p>Is ego status all you need for open-loop end-to-end autonomous driving. Z Li, Z Yu, S Lan, J Li, J Kautz, T Lu, J M Alvarez, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024873</p>
<p>Planning-oriented autonomous driving. Y Hu, J Yang, L Chen, K Li, C Sima, X Zhu, S Chai, S Du, T Lin, W Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202317862</p>
<p>A multi-task vision transformer for segmentation and monocular depth estimation for autonomous vehicles. D P Bavirisetti, H R Martinsen, G H Kiss, F Lindseth, IEEE Open Journal of Intelligent Transportation Systems. 2023</p>
<p>Probabilistic multi-modal expected trajectory prediction based on lstm for autonomous driving. Z Gao, M Bao, F Gao, M Tang, Proceedings of the Institution of Mechanical Engineers. the Institution of Mechanical Engineers2024238</p>
<p>A temporal multi-gate mixture-of-experts approach for vehicle trajectory and driving intention prediction. R Yuan, M Abdel-Aty, Q Xiang, Z Wang, X Gu, IEEE Transactions on Intelligent Vehicles. 912023</p>
<p>Dual transformer based prediction for lane change intentions and trajectories in mixed traffic environment. K Gao, X Li, B Chen, L Hu, J Liu, R Du, Y Li, IEEE Transactions on Intelligent Transportation Systems. 2462023</p>
<p>Towards comprehensive understanding of pedestrians for autonomous driving: Efficient multi-task-learning-based pedestrian detection, tracking and attribute recognition. Y Zhou, X Zeng, Robotics and Autonomous Systems. 1711045802024</p>
<p>Pedestrian intention prediction: A convolutional bottom-up multi-task approach. H Razali, T Mordan, A Alahi, Transportation Research Part C: Emerging Technologies. 2021130103259</p>
<p>Multi-task deep learning for pedestrian detection, action recognition and time to cross prediction. D O Pop, A Rogozan, C Chatelain, F Nashashibi, A Bensrhair, IEEE Access. 72019</p>
<p>Realtime heterogeneous road-agents trajectory prediction using hierarchical convolutional networks and multi-task learning. L Li, X Wang, D Yang, Y Ju, Z Zhang, J Lian, IEEE Transactions on Intelligent Vehicles. 922023</p>
<p>Apg-rrt: Sampling-based path planning method for small autonomous vehicle in closed scenarios. Z Wang, P Li, Z Wang, Z Li, IEEE Access. 122024</p>
<p>Efficient sampling-based trajectory planning with dual-layer probabilistic intention prediction for autonomous driving in complex intersections. Z Chen, G Yu, G Cao, S Wang, B Zhou, P Chen, IEEE Transactions on Vehicular Technology. 2025</p>
<p>Samplingbased motion planning with online racing line generation for autonomous driving on three-dimensional race tracks. L gretmen, M Rowold, A Langmann, B Lohmann, 2024 IEEE Intelligent Vehicles Symposium (IV). IEEE2024</p>
<p>Scenariooptimization-based velocity planning of autonomous vehicles for interacting with pedestrians. B Jekl, Z Dabevi, B Nmeth, B kugor, P Gspr, IEEE Transactions on Intelligent Transportation Systems. 2025</p>
<p>Spatio-temporal joint optimization-based trajectory planning method for autonomous vehicles in complex urban environments. J Guo, Z Xie, M Liu, Z Dai, Y Jiang, J Guo, D Xie, Sensors. 241446852024</p>
<p>An optimization-based path planning approach for autonomous vehicles using the dynefwa-artificial potential field. H Li, W Liu, C Yang, W Wang, T Qie, C Xiang, IEEE Transactions on Intelligent Vehicles. 722021</p>
<p>A rule-based behaviour planner for autonomous driving. F Bouchard, S Sedwards, K Czarnecki, International Joint Conference on Rules and Reasoning. Springer2022</p>
<p>A rule-based cooperative merging strategy for connected and automated vehicles. J Ding, L Li, H Peng, Y Zhang, IEEE Transactions on Intelligent Transportation Systems. 2182019</p>
<p>Design and implementation of an autonomous driving vehicle control system based on pid adjustment. Z Zhai, X Ma, Second International Conference on Big Data, Computational Intelligence, and Applications (BDCIA 2024). SPIE202513550</p>
<p>Optimized self-adaptive pid speed control for autonomous vehicles. Y Kebbati, N Ait-Oufroukh, V Vigneron, D Ichalal, D Gruyer, 2021 26th International Conference on Automation and Computing (ICAC). IEEE2021</p>
<p>Autonomous vehicle lateral control based on fractional-order pid. X Dong, H Pei, M Gan, 2021 IEEE 5th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC). IEEE20215</p>
<p>A novel model predictive controller for the drifting vehicle to track a circular trajectory. C Hu, L Xie, Z Zhang, H Xiong, Vehicle System Dynamics. 6332025</p>
<p>Parallel nonlinear model predictive controller for real-time path tracking of autonomous vehicle. F Xu, X Zhang, H Chen, Y Hu, P Wang, T Qu, IEEE Transactions on Industrial Electronics. 2024</p>
<p>A predictive controller for autonomous vehicle path tracking. G V Raffo, G K Gomes, J E Normey-Rico, C R Kelber, L B Becker, IEEE Transactions on Intelligent Transportation Systems. 1012009</p>
<p>Trajectory tracking of autonomous vehicle based on model predictive control with pid feedback. D Chu, H Li, C Zhao, T Zhou, IEEE Transactions on Intelligent Transportation Systems. 2422022</p>
<p>Autonomous driving control using end-to-end deep learning. M.-J Lee, Y.-G Ha, 2020 IEEE International Conference on Big Data and Smart Computing (BigComp). IEEE2020</p>
<p>End-to-end deep learning-based autonomous driving control for high-speed environment. C.-J Kim, M -J. Lee, K.-H Hwang, Y.-G Ha, The Journal of Supercomputing. 7822022</p>
<p>End-to-end autonomous driving: Challenges and frontiers. L Chen, P Wu, K Chitta, B Jaeger, A Geiger, H Li, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Interruption-aware cooperative perception for v2x communicationaided autonomous driving. S Ren, Z Lei, Z Wang, M Dianati, Y Wang, S Chen, W Zhang, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>State estimation and motion prediction of vehicles and vulnerable road users for cooperative autonomous driving: A survey. P Ghorai, A Eskandarian, Y.-K Kim, G Mehr, IEEE Transactions on Intelligent Transportation Systems. 231022022</p>
<p>Dedicated short-range communications (dsrc) standards in the united states. J B Kenney, Proceedings of the IEEE. the IEEE201199</p>
<p>Wireless access for v2x communications: Research, challenges and opportunities. J Clancy, D Mullins, B Deegan, J Horgan, E Ward, C Eising, P Denny, E Jones, M Glavin, 2024IEEE Communications Surveys &amp; Tutorials</p>
<p>Challenges and solutions for cellular based v2x communications. S Gyawali, S Xu, Y Qian, R Q Hu, IEEE Communications Surveys &amp; Tutorials. 2312020</p>
<p>Heterogeneous multiscale cooperative perception for connected autonomous vehicles via v2x interaction. Y Zha, W Shangguan, J Chen, L Chai, W Qiu, A M Lpez, IEEE Internet of Things Journal. 2025</p>
<p>V2x-vit: Vehicle-to-everything cooperative perception with vision transformer. R Xu, H Xiang, Z Tu, X Xia, M.-H Yang, J Ma, European Conference on Computer Vision. Springer2022</p>
<p>Consensus-based distributed cooperative perception for connected and automated vehicles. K Cai, T Qu, B Gao, H Chen, IEEE Transactions on Intelligent Transportation Systems. 2482023</p>
<p>V2x-pc: Vehicle-to-everything collaborative perception via point cluster. S Liu, Z Ding, J Fu, H Li, S Chen, S Zhang, X Zhou, arXiv:2403.166352024arXiv preprint</p>
<p>V2vnet: Vehicle-to-vehicle communication for joint perception and prediction. T.-H Wang, S Manivasagam, M Liang, B Yang, W Zeng, R Urtasun, Computer vision-ECCV 2020: 16th European conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020proceedings, part II 16</p>
<p>Cmp: Cooperative motion prediction with multi-agent communication. Z Wang, Y Wang, Z Wu, H Ma, Z Li, H Qiu, J Li, IEEE Robotics and Automation Letters. 2025</p>
<p>Bev-v2x: Cooperative birds-eye-view fusion and grid occupancy prediction via v2x-based data sharing. C Chang, J Zhang, K Zhang, W Zhong, X Peng, S Li, L Li, IEEE Transactions on Intelligent Vehicles. 8112023</p>
<p>Intercoop: Spatio-temporal interaction aware cooperative perception for networked vehicles. W Wang, H Xu, G Tan, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE202414449</p>
<p>Endto-end autonomous driving through v2x cooperation. H Yu, W Yang, J Zhong, Z Yang, S Fan, P Luo, Z Nie, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>V2x assisted co-design of motion planning and control for connected automated vehicle. J Li, C Chen, B Yang, IET Intelligent Transport Systems. 18122024</p>
<p>Robust instance segmentation through reasoning about multi-object occlusion. X Yuan, A Kortylewski, Y Sun, A Yuille, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021150</p>
<p>Deep dense multi-scale network for snow removal using semantic and depth priors. K Zhang, R Li, Y Yu, W Luo, C Li, IEEE Transactions on Image Processing. 302021</p>
<p>Safe occlusion-aware autonomous driving via game-theoretic active perception. Z Zhang, J F Fisac, arXiv:2105.081692021arXiv preprint</p>
<p>Robust multi-agent collaborative perception via spatio-temporal awareness. K Yang, Z Xu, D Yang, Q Fu, R Tang, L Song, L Zhang, IEEE Transactions on Circuits and Systems for Video Technology. 2025</p>
<p>A novel reinforcement learning method for autonomous driving with intermittent vehicle-toeverything (v2x) communications. L Chen, Y He, F R Yu, W Pan, Z Ming, IEEE Transactions on Vehicular Technology. 7362024</p>
<p>Multi-task collaborative perception algorithm based on consensus perception. F Luo, Y Yu, X Guo, 2024 IEEE International Conference on Unmanned Systems (ICUS). IEEE2024</p>
<p>Multi-task collaborative perception for vehicle-to-everything considering impaired communication. F Yan, B Tao, N Zheng, L Nie, Q Li, Z Yin, IEEE Transactions on Instrumentation and Measurement. 2025</p>
<p>V2xpnp: Vehicle-to-everything spatiotemporal fusion for multi-agent perception and prediction. Z Zhou, H Xiang, Z Zheng, S Z Zhao, M Lei, Y Zhang, T Cai, X Liu, J Liu, M Bajji, arXiv:2412.018122024arXiv preprint</p>
<p>A federated multi-task meta-learning framework for collaborative perception and adaptation in connected and automated vehicles. D Balasubramanian, Cognitive Sustainability. 412025</p>
<p>Eptask: Deep reinforcement learning based energy-efficient and priority-aware task scheduling for dynamic vehicular edge computing. P Li, Z Xiao, X Wang, K Huang, Y Huang, H Gao, IEEE Transactions on Intelligent Vehicles. 912023</p>
<p>Latencyconstrained multi-user efficient task scheduling in large-scale internet of vehicles. B Ma, Z Ren, W Cheng, J Wang, W Zhang, IEEE Transactions on Mobile Computing. 2024</p>
<p>Multi task dynamic edge-end collaboration for urban internet of vehicles. S Shao, L Su, Q Zhang, S Wu, S Guo, F Qi, Computer Networks. 2271096902023</p>
<p>Multitask communication resource allocation for mimo-based vehicular fog computing. C Zhu, X Xie, R Zhang, R Li, B Zhu, X Bu, IEEE Transactions on Vehicular Technology. 7312023</p>
<p>Reinforcement learning based edge-end collaboration for multi-task scheduling in 6g enabled intelligent autonomous transport systems. P Li, Z Xiao, H Gao, X Wang, Y Wang, IEEE Transactions on Intelligent Transportation Systems. 2025</p>
<p>A multi-task faster r-cnn method for 3d vehicle detection based on a single image. W Yang, Z Li, C Wang, J Li, Applied Soft Computing. 951065332020</p>
<p>Cross-stitch networks for multi-task learning. I Misra, A Shrivastava, A Gupta, M Hebert, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2016</p>
<p>A gradually soft multi-task and data-augmented approach to medical question understanding. K Mrini, F Dernoncourt, S Yoon, T Bui, W Chang, E Farcas, N Nakashole, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Multi-task learning using bert with soft parameter sharing between layers. N Pahari, K Shimada, 2022 Joint 12th International Conference on Soft Computing and Intelligent Systems and 23rd International Symposium on Advanced Intelligent Systems (SCIS&amp;ISIS). IEEE2022</p>
<p>Progressive neural networks. A A Rusu, N C Rabinowitz, G Desjardins, H Soyer, J Kirkpatrick, K Kavukcuoglu, R Pascanu, R Hadsell, arXiv:1606.046712016arXiv preprint</p>
<p>Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction. Y Gao, J Ma, M Zhao, W Liu, A L Yuille, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Advancing mental health detection in texts via multi-task learning with soft-parameter sharing transformers. D Kodati, R Tene, Neural Computing and Applications. 202537</p>
<p>Exploring relational context for multi-task dense prediction. D Bruggemann, M Kanakis, A Obukhov, S Georgoulis, L Van Gool, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021878</p>
<p>Cross-task attention mechanism for dense multi-task learning. I Lopes, T.-H Vu, R De Charette, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Umt-net: A uniform multi-task network with adaptive task weighting. S Chen, L Zheng, L Huang, J Bai, X Zhu, Z Ma, IEEE Transactions on Intelligent Vehicles. 2023</p>
<p>Dynamic neural network for multi-task learning searching across diverse network topologies. W Choi, S Im, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Regularized multi-task learning. T Evgeniou, M Pontil, Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2004</p>
<p>Empirical evaluation of multi-task learning in deep neural networks for natural language processing. J Li, X Liu, W Yin, M Yang, L Ma, Y Jin, Neural Computing and Applications. 332021</p>
<p>Unite: Multitask learning with sufficient feature for dense prediction. Y Tian, Y Lin, Q Ye, J Wang, X Peng, J Lv, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2024</p>
<p>An adaptive loss weighting multi-task network with attention-guide proposal generation for small size defect inspection. H Wu, B Li, L Tian, J Feng, C Dong, The Visual Computer. 4022024</p>
<p>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Z Chen, V Badrinarayanan, C.-Y Lee, A Rabinovich, International Conference on Machine Learning. PMLR2018</p>
<p>Adamt-net: An adaptive weight learning based multi-task learning model for scene understanding. A Jha, A Kumar, B Banerjee, S Chaudhuri, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops2020</p>
<p>End-to-end multi-task learning with attention. S Liu, E Johns, A J Davison, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Gradient surgery for multi-task learning. T Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, Advances in Neural Information Processing Systems. 202033</p>
<p>Conflict-averse gradient descent for multi-task learning. B Liu, X Liu, X Jin, P Stone, Q Liu, Advances in Neural Information Processing Systems. 202134890</p>
<p>A modelagnostic approach to mitigate gradient interference for multi-task learning. H Chai, Z Yin, Y Ding, L Liu, B Fang, Q Liao, IEEE Transactions on Cybernetics. 53122022</p>
<p>A Navon, A Shamsian, I Achituve, H Maron, K Kawaguchi, G Chechik, E Fetaya, arXiv:2202.01017Multi-task learning as a bargaining game. 2022arXiv preprint</p>
<p>Ri-pcgrad: Optimizing multi-task learning with rescaling and impartial projecting conflict gradients. F Meng, Z Xiao, Y Zhang, J Wang, Applied Intelligence. 54222024</p>
<p>Variable multi-scale attention fusion network and adaptive correcting gradient optimization for multi-task learning. N Ji, Y Sun, F Meng, L Pang, Y Tian, Pattern Recognition. 1114232025</p>
<p>Multi-task learning as multi-objective optimization. O Sener, V Koltun, Advances in Neural Information Processing Systems. 201831</p>
<p>Pareto multitask learning. X Lin, H.-L Zhen, Z Li, Q.-F Zhang, S Kwong, Advances in Neural Information Processing Systems. 201932</p>
<p>Efficient continuous pareto exploration in multi-task learning. P Ma, T Du, W Matusik, International Conference on Machine Learning. PMLR2020</p>
<p>A multi-objective/multi-task learning framework induced by pareto stationarity. M Momma, C Dong, J Liu, International Conference on Machine Learning. PMLR202215907</p>
<p>Real-time memory efficient multitask learning model for autonomous driving. S Miraliev, S Abdigapporov, V Kakani, H Kim, IEEE Transactions on Intelligent Vehicles. 912023</p>
<p>Research on road scene understanding of autonomous vehicles based on multi-task learning. J Guo, J Wang, H Wang, B Xiao, Z He, L Li, Sensors. 231362382023</p>
<p>Centernetauto: A multi-object visual detection algorithm for autonomous driving scenes based on improved centernet. H Wang, Y Xu, Z Wang, Y Cai, L Chen, Y Li, IEEE Transactions on Emerging Topics in Computational Intelligence. 732023</p>
<p>Imagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2009</p>
<p>Multi-task learning with future states for vision-based autonomous driving. I Kim, H Lee, J Lee, E Lee, D Kim, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2020</p>
<p>An efficient multi-task learning cnn for driver attention monitoring. D Yang, Y Wang, R Wei, J Guan, X Huang, W Cai, Z Jiang, Journal of Systems Architecture. 1481030852024</p>
<p>A camerabased end-to-end autonomous driving framework combined with metabased multi-task optimization. Z Rao, Y Cai, H Wang, L Chen, Y Li, Q Liu, IEEE Transactions on Transportation Electrification. 2024</p>
<p>Fusionad: Multi-modality fusion for prediction and planning tasks of autonomous driving. T Ye, W Jing, C Hu, S Huang, L Gao, F Li, J Wang, K Guo, W Xiao, W Mao, arXiv:2308.010062023arXiv preprint</p>
<p>Faster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3962016</p>
<p>Mask r-cnn. K He, G Gkioxari, P Dollr, R Girshick, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2017</p>
<p>3d bounding box estimation using deep learning and geometry. A Mousavian, D Anguelov, J Flynn, J Kosecka, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Multi-task network for panoptic segmentation in automated driving. A Petrovai, S Nedevschi, 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE2019</p>
<p>Improved mask r-cnn multi-target detection and segmentation for autonomous driving in complex scenes. S Fang, B Zhang, J Hu, Sensors. 23838532023</p>
<p>Enhancing geometric factors in model learning and inference for object detection and instance segmentation. Z Zheng, P Wang, D Ren, W Liu, R Ye, Q Hu, W Zuo, IEEE Transactions on Cybernetics. 5282021</p>
<p>Scalable multi-task learning r-cnn for object detection in autonomous driving. S Rinchen, B Vaidya, H T Mouftah, 2023 International Wireless Communications and Mobile Computing (IWCMC). IEEE2023</p>
<p>D Vu, B Ngo, H Phan, arXiv:2203.09035Hybridnets: End-to-end perception network. 2022arXiv preprint</p>
<p>End-to-end real-time obstacle detection network for safe self-driving via multi-task learning. T.-J Song, J Jeong, J.-H Kim, IEEE Transactions on Intelligent Transportation Systems. 2392022</p>
<p>Dynamic task weighting methods for multi-task networks in autonomous driving systems. I Leang, G Sistu, F Brger, A Bursuc, S Yogamani, 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). IEEE2020</p>
<p>Designing network design spaces. I Radosavovic, R P Kosaraju, R Girshick, K He, P Dollr, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202010436</p>
<p>Searching for mo-bilenetv3. A Howard, M Sandler, G Chu, L.-C Chen, B Chen, M Tan, W Wang, Y Zhu, R Pang, V Vasudevan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Drmnet: A multi-task detection model based on image processing for autonomous driving scenarios. J Zhao, D Wu, Z Yu, Z Gao, IEEE Transactions on Vehicular Technology. 2023</p>
<p>You only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>Scaled-yolov4: Scaling cross stage partial network. C.-Y Wang, A Bochkovskiy, H.-Y M Liao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20211338</p>
<p>Spatial pyramid pooling in deep convolutional networks for visual recognition. K He, X Zhang, S Ren, J Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3792015</p>
<p>Feature pyramid networks for object detection. T.-Y Lin, P Dollr, R Girshick, K He, B Hariharan, S Belongie, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Yolopv2: Better, faster, stronger for panoptic driving perception. C Han, Q Zhao, S Zhang, Y Chen, Z Zhang, J Yuan, arXiv:2208.114342022arXiv preprint</p>
<p>Yolomh: you only look once for multi-task driving perception with high efficiency. L Fang, S Bowen, M Jianxi, S Weixing, Machine Vision and Applications. 353442024</p>
<p>Parallel attention for multi-task road object detection in autonomous driving. Y Zhang, Z Tu, Y Zheng, T Zhang, C Wu, N Wang, IEEE Sensors Journal. 2024</p>
<p>Joint semantic understanding with a multilevel branch for driving perception. D.-G Lee, Y.-K Kim, Applied Sciences. 12628772022</p>
<p>Multi-task learning for dangerous object detection in autonomous driving. Y Chen, D Zhao, L Lv, Q Zhang, Information Sciences. 4322018</p>
<p>Ssd: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y Fu, A C Berg, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part I 14</p>
<p>Mtnas: search multi-task networks for autonomous driving. H Liu, D Li, J Peng, Q Zhao, L Tian, Y Shan, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2020</p>
<p>Vru pose-ssd: Multiperson pose estimation for automated driving. C Kumar, J Ramesh, B Chakraborty, R Raman, C Weinrich, A Mundhada, A Jain, F B Flohr, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135338</p>
<p>Shuda-rfbnet for realtime multi-task traffic scene perception. Z Wang, Z Cheng, H Huang, J Zhao, 2019 Chinese Automation Congress (CAC). IEEE2019</p>
<p>Going deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2015</p>
<p>A review of deep learning advancements in road analysis for autonomous driving. A.-P Botezatu, A Burlacu, C Orhei, Applied Sciences. 141147052024</p>
<p>Df-ssd: An improved ssd object detection algorithm based on densenet and feature fusion. S Zhai, D Shang, S Wang, S Dong, IEEE Access. 82020</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>A survey on vision transformer. K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang, A Xiao, C Xu, Y Xu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4512022</p>
<p>Spatio-temporal multi-task learning transformer for joint moving object detection and segmentation. E Mohamed, A El Sallab, 2021 IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE2021</p>
<p>Cutransnet: Transformers to make strong encoders for multi-task vision perception of autonomous driving. J Li, X Ke, Z Wang, J Wan, G Tan, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>M3net: Multimodal multi-task learning for 3d detection, segmentation, and occupancy prediction in autonomous driving. X Chen, S Shi, T Ma, J Zhou, S See, K C Cheung, H Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Pedestrian crossing intention prediction based on cross-modal transformer and uncertainty-aware multi-task learning for autonomous driving. X Chen, S Zhang, J Li, J Yang, IEEE Transactions on Intelligent Transportation Systems. 2024</p>
<p>Dmfuser: Distilled multi-task learning for end-to-end transformer-based sensor fusion in autonomous driving. P Agand, M Mahdavian, M Savva, M Chen, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Vision-language models for vision tasks: A survey. J Zhang, J Huang, S Jin, S Lu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Delving into multi-modal multitask foundation models for road scene understanding: From learning paradigm perspectives. S Luo, W Chen, W Tian, R Liu, L Hou, X Zhang, H Shen, R Wu, S Geng, Y Zhou, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>Visual exemplar driven task-prompting for unified perception in autonomous driving. X Liang, M Niu, J Han, H Xu, C Xu, X Liang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Learning to prompt for vision-language models. K Zhou, J Yang, C C Loy, Z Liu, International Journal of Computer Vision. 13092022</p>
<p>Bev-clip: Multi-modal bev retrieval methodology for complex scene in autonomous driving. D Wei, T Gao, Z Jia, C Cai, C Hou, P Jia, F Liu, K Zhan, J Fan, Y Zhao, arXiv:2401.010652024arXiv preprint</p>
<p>Bev-tsr: Text-scene retrieval in bev space for autonomous driving. T Tang, D Wei, Z Jia, T Gao, C Cai, C Hou, P Jia, K Zhan, H Sun, J Fan, 20242401arXiv e-prints</p>
<p>Talk2bev: Language-enhanced bird's-eye view maps for autonomous driving. T Choudhary, V Dewangan, S Chandhok, S Priyadarshan, A Jain, A K Singh, S Srivastava, K M Jatavallabhula, K M Krishna, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE202416352</p>
<p>Hierarchical prompt learning for multi-task learning. Y Liu, Y Lu, H Liu, Y An, Z Xu, Z Yao, B Zhang, Z Xiong, C Gui, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202310898</p>
<p>An advanced driving agent with the multimodal large language model for autonomous vehicles. J Chen, S Lu, 2024 IEEE International Conference on Mobility, Operations, Services and Technologies (MOST). IEEE2024</p>
<p>Drivegpt4: Interpretable end-to-end autonomous driving via large language model. Z Xu, Y Zhang, E Xie, Z Zhao, Y Guo, K.-Y K Wong, Z Li, H Zhao, IEEE Robotics and Automation Letters. 2024</p>
<p>Drive like a human: Rethinking autonomous driving with large language models. D Fu, X Li, L Wen, M Dou, P Cai, B Shi, Y Qiao, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024</p>
<p>Koma: Knowledge-driven multi-agent framework for autonomous driving with large language models. K Jiang, X Cai, Z Cui, A Li, Y Ren, H Yu, H Yang, D Fu, L Wen, P Cai, arXiv:2407.142392024arXiv preprint</p>
<p>Vlp: Vision language planning for autonomous driving. C Pan, B Yaman, T Nesti, A Mallik, A G Allievi, S Velipasalar, L Ren, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202414769</p>
<p>Lane-changing trajectory prediction based on multi-task learning. X Meng, J Tang, F Yang, Z Wang, Transportation Safety and Environment. 54732023</p>
<p>Dynamic v2x perception from road-to-vehicle vision. J Tan, F Lyu, L Li, F Hu, T Feng, F Xu, Z Zhang, R Yao, L Wang, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>Deepaccident: A motion and accident prediction benchmark for v2x autonomous driving. T Wang, S Kim, J Wenxuan, E Xie, C Ge, J Chen, Z Li, P Luo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>J Peng, L Tian, X Jia, H Guo, Y Xu, D Xie, H Luo, Y Shan, Y Wang, 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS). Multi-task adas system on fpga</p>
<p>Yoltrack: Multitask learning based real-time multiobject tracking and segmentation for autonomous vehicles. X Chang, H Pan, W Sun, H Gao, IEEE Transactions on Neural Networks and Learning Systems. 32122021</p>
<p>Uolo: A multitask u-net yolo hybrid model for railway scene understanding. A Manole, L.-S Dios, An , IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>Multitask yolo: Versatile perception network for autonomous driving. H.-M Chen, 2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW). IEEE2023</p>
<p>End-to-end self-driving using deep neural networks with multi-auxiliary tasks. D Wang, J Wen, Y Wang, X Huang, F Pei, Automotive Innovation. 22019</p>
<p>Agentic ai: Autonomous intelligence for complex goals-a comprehensive survey. D B Acharya, K Kuppan, B Divya, IEEE Access. 2025</p>
<p>The rise of agentic ai: implications, concerns, and the path forward. S Murugesan, IEEE Intelligent Systems. 4022025</p>
<p>The role of agentic ai in shaping a smart future: A systematic review. S Hosseini, H Seilani, Array. 1003992025</p>
<p>Conceptualising the emergence of agentic urban ai: from automation to agency. A Tiwari, Urban Informatics. 412025</p>
<p>The global landscape of ai ethics guidelines. A Jobin, M Ienca, E Vayena, Nature Machine Intelligence. 192019</p>
<p>Model cards for model reporting. M Mitchell, S Wu, P Zaldivar, L Barnes, B Vasserman, B Hutchinson, E Spitzer, I D Raji, T Gebru, Proceedings of the Conference on Fairness, Accountability, and Transparency. the Conference on Fairness, Accountability, and Transparency2019</p>
<p>A survey on bias and fairness in machine learning. N Mehrabi, F Morstatter, N Saxena, K Lerman, A Galstyan, ACM Computing Surveys (CSUR). 5462021</p>
<p>Algorithmic bias in autonomous systems. D Danks, A J London, Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. the Twenty-Sixth International Joint Conference on Artificial Intelligence2017</p>            </div>
        </div>

    </div>
</body>
</html>
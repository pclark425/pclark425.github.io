<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1772 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1772</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1772</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-1d803f07e4591bd67c358eef715bcd443e821894</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1d803f07e4591bd67c358eef715bcd443e821894" target="_blank">BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> An interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pre-trained embeddings of natural language or videos of humans performing the task.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. To that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pre-trained embeddings of natural language or videos of humans performing the task. When scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44%, without any robot demonstrations for those tasks.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1772.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1772.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USE (multilingual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilingual Universal Sentence Encoder (checkpoint from tfhub)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained multilingual sentence encoder that maps natural language sentences to 512-D embeddings; used in BC-Z as a frozen/plug-in task encoder to condition a robot visuomotor policy for zero-shot task specification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual universal sentence encoder for semantic retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Multilingual Universal Sentence Encoder (frozen embeddings) used to condition BC-Z</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A 512-dimensional sentence embedding model (Universal Sentence Encoder, multilingual checkpoint) used as q(z|w_t) to compute task vectors z from natural language instructions; these embeddings are L2-normalized, optionally noise-perturbed during policy training, and injected into the ResNet-based policy via FiLM conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Natural language text (multilingual sentence corpora / semantic retrieval pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>BC-Z references a pretrained multilingual Universal Sentence Encoder checkpoint (https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) producing 512-dim vectors. The BC-Z paper does not enumerate the encoder's raw training corpora or dataset sizes; it uses the published TF-Hub checkpoint as-is.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>BC-Z multi-task 7-DoF robotic manipulation (100 training tasks, 29 held-out tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Vision-based closed-loop robotic manipulation on 7-DoF arms controlling end-effector pose + gripper, executed at 10 Hz across a tabletop environment with 6–15 household objects; trained on a 100-task dataset (25,877 robot demos, 125 robot-hours) and evaluated for zero-shot generalization on 29 unseen manipulation tasks (mix of object sets, varying distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No action space in pretraining; produces sentence embeddings (i.e. semantic task vectors) rather than action tokens — the text model outputs fixed 512-D vectors representing task intent.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous 7-DoF low-level control: delta XYZ (end-effector), delta axis-angle (orientation), and continuous gripper angle (normalized 0–1); actions represented as state-differences N>1 steps in the future (adaptive N).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>The pretrained sentence embedding z is used as task-conditioning input to the visuomotor policy via FiLM layers: z is linearly projected to channel-wise scales and shifts applied to ResNet block channels, enabling the control network π(s,z) to decode the semantic task vector into low-level continuous motor commands; task vectors are computed once per task (or averaged) and Gaussian noise is added during policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Monocular RGB vision from a head-mounted camera (images processed by ResNet-18); standard image augmentations and random crops; no depth sensor required for the main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Language-conditioned BC-Z achieved non-zero zero-shot success on 24 held-out tasks; averaged success for held-out tasks: ~32% (with 4–5 distractors) and ~38% (with 1 distractor) depending on distractor count; across the 24 holdout tasks with non-zero success, average success was 44% when conditioned on unseen language embeddings. On training tasks, language-conditioned policies reached ~40% success (comparable to one-hot conditioning at 42%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines that lacked pretrained language conditioning failed to generalize zero-shot: single-task BC (1000 demos) achieved only 5% on 'place the bottle in the ceramic bowl' in the real multi-robot dataset; attempts to train single-task policies on 300+ held-out demos with DAgger resulted in 0% success for several held-out tasks. One-hot conditioning (available only for train tasks) achieves similar train-task performance (~42%) but cannot provide zero-shot on unseen text commands.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>BC-Z multi-task policy trained using 25,877 robot demonstrations (125 robot-hours) across 100 tasks, plus 18,726 human videos; zero-shot transfer to held-out tasks required 0 robot demonstrations for those tasks (i.e., new tasks specified by language needed no additional robot data).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Single-task behavior cloning in the real, diverse setup performed poorly: 1000 demos → 5% success on a target task; training single-task policies with 300+ held-out demos + DAgger still failed (0% on some holdouts). In a deterministic simulated variant, only ~37 expert demos were sufficient to reach ~97% when scene variation was removed, indicating that without broad multi-task pretraining, sample complexity explodes with real-world variation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative/conditional: language pretraining enables zero-shot specification of new tasks (saving the need to collect per-heldout-task robot demos), but achieving the base policy that can interpret the embeddings required large-scale multi-task data (~25k demos). Compared to single-task BC (1000 demos) which failed, the multi-task + language approach enabled solving held-out tasks with zero extra demos.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Semantic structure provided by frozen pretrained language embeddings (USE) that map novel task sentences to task vectors; FiLM conditioning to inject semantic signals into perception network; large, diverse multi-task demonstration dataset (25,877 demos); HG-DAgger shared-autonomy interventions improving robustness; auxiliary language regression for video encoder alignment; adaptive state-difference action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Primary bottleneck was low-level control/policy (last-centimeter errors such as gripper closure, object drop); video-conditioned embeddings generalized poorly (inference-from-video is harder than text); high scene and object variability increases required sample complexity; action-space mismatch between abstract text semantics and continuous motor commands limits fine-grained manipulation without substantial multi-task training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Frozen pretrained sentence embeddings (Universal Sentence Encoder) provide an effective, low-cost way to specify tasks for a learned visuomotor policy and enable zero-shot generalization to many unseen manipulation tasks without collecting additional robot demonstrations. However, successful transfer requires a powerful, well-trained control policy learned from a large, diverse multi-task demonstration corpus; the main performance bottleneck is the continuous control layer rather than the pretrained language encoder. Video-based task inference is notably harder than direct language conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multilingual universal sentence encoder for semantic retrieval <em>(Rating: 2)</em></li>
                <li>Language-conditioned imitation learning for robot manipulation tasks <em>(Rating: 2)</em></li>
                <li>Grounding language in play <em>(Rating: 2)</em></li>
                <li>Zero-shot task adaptation using natural language <em>(Rating: 2)</em></li>
                <li>Task-Embedded Control Networks for Few-Shot Imitation Learning <em>(Rating: 1)</em></li>
                <li>One-shot visual imitation learning via meta-learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1772",
    "paper_id": "paper-1d803f07e4591bd67c358eef715bcd443e821894",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "USE (multilingual)",
            "name_full": "Multilingual Universal Sentence Encoder (checkpoint from tfhub)",
            "brief_description": "A pretrained multilingual sentence encoder that maps natural language sentences to 512-D embeddings; used in BC-Z as a frozen/plug-in task encoder to condition a robot visuomotor policy for zero-shot task specification.",
            "citation_title": "Multilingual universal sentence encoder for semantic retrieval",
            "mention_or_use": "use",
            "model_agent_name": "Multilingual Universal Sentence Encoder (frozen embeddings) used to condition BC-Z",
            "model_agent_description": "A 512-dimensional sentence embedding model (Universal Sentence Encoder, multilingual checkpoint) used as q(z|w_t) to compute task vectors z from natural language instructions; these embeddings are L2-normalized, optionally noise-perturbed during policy training, and injected into the ResNet-based policy via FiLM conditioning.",
            "pretraining_data_type": "Natural language text (multilingual sentence corpora / semantic retrieval pretraining)",
            "pretraining_data_details": "BC-Z references a pretrained multilingual Universal Sentence Encoder checkpoint (https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) producing 512-dim vectors. The BC-Z paper does not enumerate the encoder's raw training corpora or dataset sizes; it uses the published TF-Hub checkpoint as-is.",
            "embodied_task_name": "BC-Z multi-task 7-DoF robotic manipulation (100 training tasks, 29 held-out tasks)",
            "embodied_task_description": "Vision-based closed-loop robotic manipulation on 7-DoF arms controlling end-effector pose + gripper, executed at 10 Hz across a tabletop environment with 6–15 household objects; trained on a 100-task dataset (25,877 robot demos, 125 robot-hours) and evaluated for zero-shot generalization on 29 unseen manipulation tasks (mix of object sets, varying distractors).",
            "action_space_text": "No action space in pretraining; produces sentence embeddings (i.e. semantic task vectors) rather than action tokens — the text model outputs fixed 512-D vectors representing task intent.",
            "action_space_embodied": "Continuous 7-DoF low-level control: delta XYZ (end-effector), delta axis-angle (orientation), and continuous gripper angle (normalized 0–1); actions represented as state-differences N&gt;1 steps in the future (adaptive N).",
            "action_mapping_method": "The pretrained sentence embedding z is used as task-conditioning input to the visuomotor policy via FiLM layers: z is linearly projected to channel-wise scales and shifts applied to ResNet block channels, enabling the control network π(s,z) to decode the semantic task vector into low-level continuous motor commands; task vectors are computed once per task (or averaged) and Gaussian noise is added during policy training.",
            "perception_requirements": "Monocular RGB vision from a head-mounted camera (images processed by ResNet-18); standard image augmentations and random crops; no depth sensor required for the main experiments.",
            "transfer_successful": true,
            "performance_with_pretraining": "Language-conditioned BC-Z achieved non-zero zero-shot success on 24 held-out tasks; averaged success for held-out tasks: ~32% (with 4–5 distractors) and ~38% (with 1 distractor) depending on distractor count; across the 24 holdout tasks with non-zero success, average success was 44% when conditioned on unseen language embeddings. On training tasks, language-conditioned policies reached ~40% success (comparable to one-hot conditioning at 42%).",
            "performance_without_pretraining": "Baselines that lacked pretrained language conditioning failed to generalize zero-shot: single-task BC (1000 demos) achieved only 5% on 'place the bottle in the ceramic bowl' in the real multi-robot dataset; attempts to train single-task policies on 300+ held-out demos with DAgger resulted in 0% success for several held-out tasks. One-hot conditioning (available only for train tasks) achieves similar train-task performance (~42%) but cannot provide zero-shot on unseen text commands.",
            "sample_complexity_with_pretraining": "BC-Z multi-task policy trained using 25,877 robot demonstrations (125 robot-hours) across 100 tasks, plus 18,726 human videos; zero-shot transfer to held-out tasks required 0 robot demonstrations for those tasks (i.e., new tasks specified by language needed no additional robot data).",
            "sample_complexity_without_pretraining": "Single-task behavior cloning in the real, diverse setup performed poorly: 1000 demos → 5% success on a target task; training single-task policies with 300+ held-out demos + DAgger still failed (0% on some holdouts). In a deterministic simulated variant, only ~37 expert demos were sufficient to reach ~97% when scene variation was removed, indicating that without broad multi-task pretraining, sample complexity explodes with real-world variation.",
            "sample_complexity_gain": "Qualitative/conditional: language pretraining enables zero-shot specification of new tasks (saving the need to collect per-heldout-task robot demos), but achieving the base policy that can interpret the embeddings required large-scale multi-task data (~25k demos). Compared to single-task BC (1000 demos) which failed, the multi-task + language approach enabled solving held-out tasks with zero extra demos.",
            "transfer_success_factors": "Semantic structure provided by frozen pretrained language embeddings (USE) that map novel task sentences to task vectors; FiLM conditioning to inject semantic signals into perception network; large, diverse multi-task demonstration dataset (25,877 demos); HG-DAgger shared-autonomy interventions improving robustness; auxiliary language regression for video encoder alignment; adaptive state-difference action labels.",
            "transfer_failure_factors": "Primary bottleneck was low-level control/policy (last-centimeter errors such as gripper closure, object drop); video-conditioned embeddings generalized poorly (inference-from-video is harder than text); high scene and object variability increases required sample complexity; action-space mismatch between abstract text semantics and continuous motor commands limits fine-grained manipulation without substantial multi-task training.",
            "key_findings": "Frozen pretrained sentence embeddings (Universal Sentence Encoder) provide an effective, low-cost way to specify tasks for a learned visuomotor policy and enable zero-shot generalization to many unseen manipulation tasks without collecting additional robot demonstrations. However, successful transfer requires a powerful, well-trained control policy learned from a large, diverse multi-task demonstration corpus; the main performance bottleneck is the continuous control layer rather than the pretrained language encoder. Video-based task inference is notably harder than direct language conditioning.",
            "uuid": "e1772.0",
            "source_info": {
                "paper_title": "BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multilingual universal sentence encoder for semantic retrieval",
            "rating": 2
        },
        {
            "paper_title": "Language-conditioned imitation learning for robot manipulation tasks",
            "rating": 2
        },
        {
            "paper_title": "Grounding language in play",
            "rating": 2
        },
        {
            "paper_title": "Zero-shot task adaptation using natural language",
            "rating": 2
        },
        {
            "paper_title": "Task-Embedded Control Networks for Few-Shot Imitation Learning",
            "rating": 1
        },
        {
            "paper_title": "One-shot visual imitation learning via meta-learning",
            "rating": 1
        }
    ],
    "cost": 0.0144105,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning</h1>
<p>Eric Jang ${ }^{1 <em>}$, Alex Irpan ${ }^{1 </em>}$, Mohi Khansari ${ }^{2}$, Daniel Kappler ${ }^{2}$, Frederik Ebert ${ }^{31}$, Corey Lynch ${ }^{1}$, Sergey Levine ${ }^{1,3}$, Chelsea Finn ${ }^{1,4}$<br>${ }^{1}$ Robotics at Google ${ }^{2}$ Everyday Robots ${ }^{3}$ UC Berkeley ${ }^{4}$ Stanford University<br>https://sites.google.com/view/bc-z/home</p>
<h4>Abstract</h4>
<p>In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. To that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pretrained embeddings of natural language or videos of humans performing the task. When scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 24 unseen manipulation tasks with an average success rate of $44 \%$, without any robot demonstrations for those tasks.</p>
<p>Keywords: Zero-Shot Imitation Learning, Multi-Task Imitation, Deep Learning</p>
<h2>1 Introduction</h2>
<p>One of the grand challenges in robotics is to create a general-purpose robot capable of performing a multitude of tasks in unstructured environments based on arbitrary user commands. The key challenge in this endeavour is generalization: the robot must handle new environments, recognize and manipulate objects it has not seen before, and understand the intent of a command it has never been asked to execute. End-to-end learning from pixels is a flexible choice for modeling the behavior of such generalist robots, as it has minimal assumptions about the state representation of the world. With sufficient real-world data, these methods should in principle enable robots to generalize across new tasks, objects, and scenes without requiring hand-coded, task-specific representations. However, realizing this goal has generally remained elusive. In this paper, we study the problem of enabling a robot to generalize zero-shot or few-shot to new vision-based manipulation tasks.</p>
<p>We study this problem using the framework of imitation learning. Prior works on imitation learning have shown one-shot or zero-shot generalization to new objects [1, 2, 3, 4, 5] and to new object goal configurations [6, 7]. However, zero-shot generalization to new tasks remains a challenge, particularly when considering vision-based manipulation tasks that cover a breadth of skills (e.g., wiping, pushing, pick-and-place) with diverse objects. Achieving such generalization depends on solving challenges relating to scaling up data collection and learning algorithms for diverse data.</p>
<p>We develop an interactive imitation learning system with two key properties that enable high-quality data collection and generalization to entirely new tasks. First, our system incorporates shared autonomy into teleoperation to allow us to collect both raw demonstration data and human interventions to correct the robot's current policy. Second, our system flexibly conditions the policy on different forms of task specification, including a language instruction or a video of a person performing the task. Unlike discrete one-hot task identifiers [8], these continuous forms of task specification can in principle enable the robot to generalize zero-shot or few-shot to new tasks by providing a language or video command of the new task at test time. These properties have been explored previously; our aim is to empirically study whether these ideas scale to a broad range of real-world tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of BC-Z. We collect a large-scale dataset (25,877 episodes) of 100 diverse manipulation tasks, and train a 7-DoF multi-task policy that conditions on task language strings or human video. We show this system produces a policy that is capable of generalizing zero-shot to new unseen tasks.</p>
<p>Our main contribution is an empirical study of a large-scale interactive imitation learning system that solves a breadth of tasks, including zero-shot and few-shot generalization to tasks <em>not seen</em> during training. Using this system, we collect a large dataset of 100 robotic manipulation tasks, through a combination of expert teleoperation and a shared autonomy process where the human operator "coaches" the learned policy by fixing its mistakes. Across 12 robots, 7 different operators collected 25,877 robot demonstrations that totaled 125 hours of robot time, as well as 18,726 human videos of the same tasks. At test time, the system is capable of performing 24 unseen manipulation tasks between objects that have never previously appeared together in the same scene. These closed-loop visuomotor policies perform asynchronous inference and control at 10Hz, amounting to well over 100 decisions per episode. We open-source the demonstrations used to train this policy at https://www.kaggle.com/google/bc-z-robot.</p>
<h2>2 Related Work</h2>
<p>Imitation learning has been successful in learning grasping and pick-place tasks from low-dimensional state [9, 10, 11, 12, 13, 14, 15]. Deep learning has enabled imitation learning directly from raw image observations [8, 16, 17]. In this work, we focus on enabling zero-shot and few-shot generalization to new tasks in an imitation learning framework.</p>
<p>Multiple prior imitation learning works have achieved different forms of generalization, including one-shot generalization to novel objects [1, 2, 3, 4, 18], to novel object configurations [19], and to novel goal configurations [6, 7, 20], as well as zero-shot generalization to new objects [5], scenes [21], and goal configurations [22]. Many of these works adapt to the new scenario by conditioning on a robot demonstration [1, 2], a video of a human [3, 4], a language instruction [23, 24], or a goal image [21]. Our system flexibly conditions on either a video of a human or a language instruction, and we focus on achieving zero-shot (language) and few-shot (video) generalization to <em>entirely new</em> 7-DoF manipulation tasks on a real robot, including scenarios without goal images and where task-relevant objects are never encountered together in the training data.</p>
<p>It is standard to collect demonstrations via teleoperation [25] or kinesthetic teaching [10], and active learning methods such as DAgger [26] help reduce distribution shift for the learner. Unfortunately, DAgger and some of its variants [27, 28] are notoriously difficult to apply to robotic manipulation because they necessitate an interface where the expert must annotate the correct action when not in control of the robot policy. Inspired by recent works in autonomous driving, HG-DAgger [29] and EIL [30], our system instead only requires the expert to intervene when they believe the policy is likely to make an error and allows the expert to temporarily take full control to put the policy back on track. The resulting data collection scheme is easy to use and helps address distribution shift. Furthermore, the rate of expert interventions during data collection can be used as a live evaluation metric, which we empirically find correlates with policy success.</p>
<p>Beyond imitation learning, generalization has been studied in a number of other robot learning works. This includes works that generalize skills to novel objects [31, 32, 33, 34, 35], to novel environments [36], from simulation to reality [37, 38, 39, 40, 41], and to new manipulation skills and objects [42, 43, 44, 45]. We focus on the last case of generalizing to novel tasks, but unlike these prior works, we tackle a large suite of 100 challenging tasks that involve 7 DoF control at 10 Hz and involve more than 100 decisions within an episode to solve the task.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A subset of training tasks (top row), and a subset of held-out tasks (bottom two rows) used for evaluating zero shot task generalization. Top left: Given a pretrained task embedding computed from human videos or text, BC-Z acts as an "action decoder" for the task embedding.</p>
<h1>3 Problem Setup and Method Overview</h1>
<p>An overview of our imitation learning system is shown in Figure 1. Our goal is to train a conditional policy that can interpret RGB images, denoted <em>s</em> ∈ <em>S</em>, together with a task command <em>w</em> ∈ <em>W</em>, which might correspond to a language string or a video of a person. Different tasks correspond to completing distinct objectives; some example tasks and corresponding commands are shown in Figure 2. The policy is a mapping from images and commands to actions, and can be written as µ : <em>S</em> × <em>W</em> → <em>A</em>, where the action space <em>A</em> consists of the 6-DoF pose of the end effector as well as a 7th degree of freedom for continuous control of the parallel jaw gripper.</p>
<p>The policy is trained using a large-scale dataset collected via a VR-based teleoperation rig (see Figure 1, left) through a combination of direct demonstration and human-in-the-loop shared autonomy. In the latter, trained policies are deployed on the robot, and the human operator intervenes to provide corrections when the robot makes a mistake. This procedure resembles the human-gated DAgger (HG-DAgger) algorithm [26, 29], and provides iterative improvement for the learned policy, as well as a continuous signal that can be used to track the policy's performance.</p>
<p>The policy architecture is divided into an encoder <em>q</em>(<em>z</em>|<em>w</em>), which processes the command <em>w</em> into an embedding <em>z</em> ∈ <em>Z</em>, and a control layer π, which processes (<em>s</em>, <em>z</em>) to produce the action <em>a</em>, i.e. π : <em>S</em> × <em>Z</em> → <em>A</em>. This decomposition is illustrated in Figure 2, with further details in Section 5. It provides our method with the ability to incorporate auxiliary supervision, such as pretrained language embeddings, which help to structure the latent task space and facilitate generalization. In our experiments, we will show that this enables generalization to tasks that were not seen during training, including novel compositions of verbs and objects.</p>
<h1>4 Data Collection and Workflow</h1>
<p>In order for an imitation learning system to generalize to new tasks with zero demonstrations of said task, we must be able to easily collect a diverse dataset, provide corrective feedback, and evaluate many tasks at scale. In this section, we discuss these components of our system.</p>
<p><strong>System Setup.</strong> Our teleoperation system uses an Oculus VR headset which is attached to the robot's onboard computer via USB cable and tracks two handheld controllers. The teleoperator stands behind the robot and uses the controllers to operate the robot with a line-of-sight 3rd-person view. The robot responds to the operator's movement in a 10 Hz non-realtime control loop. The relatively fast closed-loop control allows the operator to demonstrate a wide range of tasks with ease and quickly intervene if the robot is about to enter an unsafe state during autonomous execution. Further details on the user interface and data collection are in Appendices A and B.</p>
<p><strong>Environment and Tasks.</strong> We place each robot in front of a table with anywhere from 6 to 15 household objects with randomized poses. We collect demonstrations and videos of humans for 100 pre-specified tasks (listed in Tables 7 and 8), which span 9 underlying skills such as pushing and pick-and-place. The model is then evaluated on 29 <em>new</em> tasks using a new language description or video of that task. For the method to perform well on these held-out tasks, it must both correctly interpret the new task command and output actions that are consistent with that task.</p>
<p>Shared Autonomy Data Collection. Data collection begins with an initial expert-only phase, where the human provides the demonstration of the task from start-to-finish. After an initial multi-task policy is learned from expert-only data, we continue collecting demonstrations in "shared autonomy" mode, where the current policy attempts the task while the human supervises. At any point the human may take over by gripping an "override" switch, which allows them to briefly take full control of the robot and perform necessary corrections when the policy is about to enter an unsafe state, or if they believe the current policy will not successfully complete the task. This setup enables HG-DAgger [29], where intervention data is then aggregated with the existing data and used to re-train the policy. For the multi-task manipulation tasks, we collect 11,108 expert-only demonstrations for the initial policies, then collected an additional 14,769 HG-DAgger demonstrations covering 16 iterations of policy deployment, where each iteration deploys the most recent policy trained on the aggregated dataset. This gives a total of 25,877 robot demos. We find in Table 4 that when controlling for the same number of total episodes, HG-DAgger improves performance substantially.</p>
<p>Shared Autonomy Evaluation. When success rates are low, resources are best spent on collecting more data to improve the policy; but evaluation is also important to debug problems in the workflow. As the expected degree of generalization increases, we need more trials to evaluate the extent of policy generalization. This creates a resource trade-off: how should robot time be allocated between measuring policy success rates and collecting additional demonstrations to improve the policy? Fortunately, shared autonomy data collection confers an additional benefit: the intervention rate, measured as the average number of interventions required per episode, can be used as an indication for policy performance. In Figure 5, we find that the intervention rate correlates negatively with overall policy success rate.</p>
<h1>5 Learning Algorithm</h1>
<p>The data collection procedure above results in a large multi-task dataset. For each task $i$, this dataset contains expert data $(s, a) \in \mathcal{D}<em h="h">{e}^{i}$, human video data $w</em>} \in \mathcal{D<em t="t">{h}^{i}$, and one language command $w</em>$. We now discuss how we use this data to train the encoder $q(z \mid w)$ and the control layer $\pi(a \mid s, z)$.}^{i</p>
<h3>5.1 Language and Video Encoders</h3>
<p>Our encoder $q(z \mid w)$ takes either a language command $w_{t}^{i}$ or a video of a human $w_{h}$ as input and produces a task embedding $z$. If the command is a language command, we use a pretrained multilingual sentence encoder [46] ${ }^{1}$ as our encoder, producing a 512-dim language vector for each task. Despite the simplicity, we find that these encoders work well in our experiments.</p>
<p>When task commands are instead a video of a human performing the task, we use a convolutional neural network to produce $z$, specifically a ResNet-18 based model. Inspired by recent works [2, 3], we train this network in an end-to-end manner. We collected a dataset of 18,726 videos of humans doing each training task, in a variety of home and office locations, camera viewpoints, and object configurations. Using paired examples of a human video $w_{h}^{i}$ and corresponding demonstration demo ${(s, a)}^{i}$, we encode the human video $z^{i} \sim q\left(\cdot \mid w_{h}^{i}\right)$, then pass the embedding to the control layer $\pi(a \mid s, z^{i})$, and then backpropagate gradient of the behavior cloning loss to both the policy and encoder parameters.</p>
<p>Visualizations of learned embeddings in Appendix E indicate that by itself, this end-to-end approach tends to overfit to initial object scenes, learn poor embeddings, and show poor task generalization. To help align the video embeddings more semantically, we therefore further introduce an auxiliary language regression loss. Concretely, this auxiliary loss trains the video encoder to predict the embedding of the task's language command with a cosine loss. The resulting video encoder objective is as follows:</p>
<p>$$
\min \sum_{\text {task i }} \sum_{\substack{(s, a) \sim \mathcal{D}<em h="h">{e}^{i} \ w</em>} \sim \mathcal{D<em e="e">{h}^{i} \cup \mathcal{D}</em>}^{i}}} \underbrace{-\log \pi\left(a \mid s, z^{i}\right)<em _cos="{cos" _text="\text">{\text {behavior cloning }}+\underbrace{D</em>}}\left(z_{h}^{i}, z_{t}^{i}\right)<em h="h">{\text {language regression }}, \text { where } \underbrace{z</em>}^{i} \sim q\left(\cdot \mid w_{h}\right)<em t="t">{\text {video encoder }}, \underbrace{z</em>
$$}^{i} \sim q\left(\cdot \mid w_{t}^{i}\right)}_{\text {language encoder }</p>
<p>where $D_{\text {cos }}$ denotes the cosine distance. Since robot demos double as videos of the task, we also train encoded robot videos to match to the language vector. This language loss is critical to learning a more organized embedding space. Additional architecture and training details are in Appendix E.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: BC-Z network architecture. A monocular RGB image from the head-mounted camera is passed through a ResNet18 encoder, then through a two-layer MLP to predict each action modality (delta XYZ, delta axis-angle, and gripper angle). FiLM layers [47] condition the architecture on a task embedding z computed from language $w_{t}$ or video $w_{h}$.</p>
<h3>5.2 Policy Training</h3>
<p>Given a fixed task embedding, we train $\pi(a \mid s, z)$ via Huber loss on XYZ and axis-angle predictions, and log loss for the gripper angle. During training, images are randomly cropped, downsampled, and subjected to standard photometric augmentations. Below we describe two additional design choices that we found to be helpful. Additional training details such as learning rates, batch sizes, pseudocode, and further hyperparameters are discussed in Appendix D.</p>
<p>Open-Loop Auxiliary Predictions. The policy predicts the action the robot would take, as well as an open-loop trajectory of the next 10 actions the policy would take if it were operating in an open-loop manner. At inference time, the policy operates closed-loop, only executing the first action based on the current image. The open-loop prediction confers an auxiliary training objective, and provides a way to visually inspect the quality of a closed-loop plan in an offline manner (see Figure 1, right).</p>
<p>State Differences as Actions. In standard imitation learning implementations, actions taken at demonstration-time are used directly as target labels to be predicted from states. However, cloning expert actions at 10Hz resulted in the policy learning very small actions, as well as dithering behavior. To address this, we define actions as state differences to target poses $N&gt;1$ steps in the future, using an adaptive algorithm to choose $N$ based on how much the arm and gripper move. We provide ablation studies for this design choice in Section 6.3 and further details in Appendix C.</p>
<h3>5.3 Network Architecture</h3>
<p>We model the policy using a deep neural network, shown in Figure 3. The policy network processes the camera image with a ResNet18 "torso" [48], which branches from the last mean-pool layer into multiple "action heads". Each head is a multilayer perceptron with two hidden layers of size 256 each and ReLU activations, and models part of the end-effector action, specifically the delta XYZ, delta axis-angle, and normalized gripper angle. The policy is conditioned on a 512-dim task embedding z, through FiLM layers [47]. Following Perez et al. [47], the task conditioning is linearly projected to channel-wise scales and shifts for each channel of each of the 4 ResNet blocks.</p>
<h2>6 Experimental Results</h2>
<p>Our experiments aim to evaluate BC-Z in large-scale imitation learning settings. We start with an initial validation of BC-Z on single-task visual imitation learning. Then, our experiments will aim to answer the following questions: (1) Can BC-Z enable zero-shot and few-shot generalization to new tasks from a command in the form of language or a video of a human? (2) Is the performance of BC-Z bottlenecked by the task embedding or by the policy? (3) How important are different components of BC-Z, including HG-DAgger data collection and adaptive state diffs? We present experiments aimed at these questions in this section.</p>
<h3>6.1 BC-Z on Single-Task Imitation Learning</h3>
<p>We first aim to verify that BC-Z can learn individual vision-based tasks before considering the more challenging multi-task setting. We choose two tasks: a <em>bin-emptying</em> task where the robot must grasp objects from a bin and drop them into an adjacent bin, and a <em>door opening</em> task where the robot must</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative examples of BC-Z successfully performing held-out tasks.</p>
<p>push open a door while avoiding collisions. Both tasks use the architecture in Figure 3, except that the door opening task involves predicting the forward and yaw velocity of the base instead of controlling the arm. The bin-emptying dataset has 2,759 demonstrations, while the door opening dataset has 12,000 demonstrations collected across 24 meeting rooms and 36,000 demonstrations across 5 meeting rooms in simulation. Further task and dataset details are in Appendix I.</p>
<p>In Table 1, we see that the BC-Z model is able to reach a pick-rate of 3.4 picks per minute, over half the speed of a human teleoperator. Further, we see that BC-Z reaches a success rate of 87% on the training door scenes and 94% on held-out door scenes. These results validate that the BC-Z model and data collect system can achieve good performance on both training and held-out scenes in the single-task setting. Additional analysis is provided in Appendix H.</p>
<p>Table 1: Single-task bin and door performance, average and standard deviation across runs.</p>
<table>
<thead>
<tr>
<th>Bin-Emptying</th>
<th>Picks / Minute</th>
<th># Runs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human Expert</td>
<td>6.3 (2.1)</td>
<td>2759</td>
</tr>
<tr>
<td>BC-Z (2759 demos)</td>
<td>3.4 (1.2)</td>
<td>9</td>
</tr>
<tr>
<td>Door Opening</td>
<td>Success Rate</td>
<td># Runs</td>
</tr>
<tr>
<td>BC-Z (24 Train Doors)</td>
<td>87% (2.2)</td>
<td>480</td>
</tr>
<tr>
<td>BC-Z (4 Holdout Doors)</td>
<td>94% (2.7)</td>
<td>80</td>
</tr>
</tbody>
</table>
<h3>6.2 Evaluating Zero-Shot and Few-Shot Task Generalization</h3>
<p>Next, we aim to test whether BC-Z can achieve generalization to new tasks. Demonstrations are collected across 100 different manipulation tasks, comprising two disjoint sets of objects. Using disjoint sets of objects allows us to specifically test generalization to combinations of object-object pairs and object-action pairs that are not seen together during training. For the first set of objects, demonstrations are collected across 21 different tasks, listed in Table 7, which cover a wide range of skills, from pick-and-place tasks to skills that require positioning the object in a certain way, like "stand the bottle upright". For the second set of objects, demonstrations are collected for 79 different tasks, including pick-and-place, surface wiping, and object stacking. The latter family has a smaller variety of manipulation behaviors, but is defined over a larger object set with more clutter. Object sets are shown in Appendix B and a full list of train task sentences are in Appendix J.</p>
<p>We evaluate BC-Z on 29 held-out tasks. Language conditioned policies are given a novel sentence, while video conditioned policies are given the average embedding of a few human videos of the new task. Four held-out tasks use objects in the 79-task family, whereas 25 tasks are generated by mixing objects between the 21-task family and 79-task family. Thus, the first 4 held-out tasks do not require cross-object set generalization, so they are easier to generalize to. Even so, we find that each of these 4 tasks are sufficiently challenging that training single-task policies on 300+ held-out demos with DAgger interventions completely fails, achieving 0% task success. This provides a degree of calibration on the difficulty of these tasks. We hypothesize that a major contributing factor to this challenge is the wide range of locations, objects, and distractors that the skills must generalize to in our settings, as well as the wide range of these factors in the training data.</p>
<p>In Table 2, we see that language-conditioned BC-Z is able to generalize zero-shot to both kinds of held-out tasks, averaging at 32% success and showing non-zero success on 24 held-out tasks. Among the 24 hold-out tasks with non-zero success rates, BC-Z achieves an average success of 44% when conditioned on language embeddings it has never seen. When conditioning on videos of humans, we find that generalization is much more difficult, but that BC-Z is still able to generalize to nine novel tasks with a non-zero success rate, particularly when the task does not involve novel object combinations. Qualitatively, we observe that the language-conditioned policy usually moves towards the correct objects, clearly indicating that the task embedding is reflective of the correct task, as we further illustrate in the supplementary video. The most common source of failures are</p>
<p>Table 2: Success rates for zero-shot (language) and few-shot (video) generalization to tasks not in the training dataset. The first 4 tasks only use objects from the 79-task family. The remaining tasks mix objects between the 21-task and 79-task families, requiring further generalization. Numbers in parentheses are 1 unit standard deviation. The language conditioning generalizes to several holdout tasks, whereas the video conditioning shows promise on tasks that do not mix objects between task families. Overall performance improves slightly with fewer distractor objects.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Skill</th>
<th style="text-align: center;">Held-out tasks (no demos during training)</th>
<th style="text-align: center;">Lang-conditioned (1 distractor)</th>
<th style="text-align: center;">Lang-conditioned (4-5 distractors)</th>
<th style="text-align: center;">Video-conditioned (4-5 distractors)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">pick-place</td>
<td style="text-align: center;">'place sponge in tray' <br> 'place grapes in red bowl' <br> 'place apple in paper cup'</td>
<td style="text-align: center;">$\begin{aligned} &amp; 83 \%(6.8) \ &amp; 87 \%(6.2) \ &amp; 30 \%(8.4) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 82 \%(9.2) \ &amp; 75 \%(10.8) \ &amp; 33 \%(12.2) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 22 \%(2.2) \ &amp; 12 \%(7.8) \ &amp; 14 \%(7.8) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">pick-wipe</td>
<td style="text-align: center;">'wipe tray with sponge'</td>
<td style="text-align: center;">$40 \%(8.9)$</td>
<td style="text-align: center;">$0 \%(0)$</td>
<td style="text-align: center;">$28 \%(10.6)$</td>
</tr>
<tr>
<td style="text-align: center;">pick-place</td>
<td style="text-align: center;">'place banana in ceramic bowl' <br> 'place bottle in red bowl' <br> 'place grapes in ceramic bowl' <br> 'place bottle in table surface' <br> 'place white sponge in purple bowl' <br> 'place white sponge in tray' <br> 'place apple in ceramic bowl' <br> 'place bottle in purple bowl' <br> 'place banana in ceramic cup' <br> 'place banana on white sponge' <br> 'place metal cup in red bowl'</td>
<td style="text-align: center;">$\begin{aligned} &amp; 50 \%(15.8) \ &amp; 50 \%(15.8) \ &amp; 70 \%(14.5) \ &amp; 0 \ &amp; 70 \%(14.9) \ &amp; 50 \%(15.8) \ &amp; 30 \%(14.5) \ &amp; 30 \%(14.5) \ &amp; 10 \%(9.5) \ &amp; 40 \%(15.5) \ &amp; 0 \%(0) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 75 \%(9.7) \ &amp; 75 \%(9.7) \ &amp; 70 \%(10.3) \ &amp; 50 \%(11.2) \ &amp; 45 \%(11.2) \ &amp; 40 \%(11.0) \ &amp; 20 \%(8.9) \ &amp; 20 \%(8.9) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 7.5 \%(4.2) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 5 \%(3.5) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">grasp <br> pick-drag</td>
<td style="text-align: center;">'pick up grapes' <br> 'pick up apple' <br> 'pick up towel' <br> 'pick up pepper' <br> 'pick up bottle' <br> 'pick up the red bowl'</td>
<td style="text-align: center;">$\begin{aligned} &amp; 70 \%(14.5) \ &amp; 20 \%(12.7) \ &amp; 50 \%(15.8) \ &amp; 50 \%(15.8) \ &amp; 40 \%(15.5) \ &amp; 30 \%(14.5) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 65 \%(10.7) \ &amp; 55 \%(11.2) \ &amp; 42.8 \%(18.7) \ &amp; 35 \%(10.7) \ &amp; 30 \%(10.3) \ &amp; 0 \%(0) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0 \%(0) \ &amp; 5 \%(3.5) \ &amp; 0 \%(0) \ &amp; 12.5 \%(5.2) \ &amp; 17.5 \%(6.0) \ &amp; 0 \%(0) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">pick-wipe</td>
<td style="text-align: center;">'wipe table surface with banana' <br> 'wipe tray with white sponge' <br> 'wipe ceramic bowl with brush'</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0 \%(0) \ &amp; 20 \%(12.7) \ &amp; 10 \%(9.49) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 10 \%(6.7) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">push</td>
<td style="text-align: center;">'push purple bowl across the table' <br> 'push tray across the table' <br> 'push red bowl across the table'</td>
<td style="text-align: center;">$\begin{aligned} &amp; 50 \%(15.8) \ &amp; 30 \%(14.5) \ &amp; 60 \%(15.5) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 30 \%(10.3) \ &amp; 25 \%(9.7) \ &amp; 0 \%(0) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0 \%(0) \ &amp; 0 \%(0) \ &amp; 0 \%(0) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Holdout Task Overall</td>
<td style="text-align: center;">$38 \%$</td>
<td style="text-align: center;">$32 \%$</td>
<td style="text-align: center;">$4 \%$</td>
</tr>
</tbody>
</table>
<p>"last-centimeter" errors: failing to close the gripper, failing to let go of objects, or a near miss of the target object when letting go of an object in the gripper.
Is Performance Bottlenecked on the Encoder or the Policy? Now that we see that BC-Z can generalize to a substantial number of held-out tasks to some degree, we ask whether the performance is limited more by the generalization of the encoder $q(\varepsilon \mid w)$, the control layer $\pi(u \mid s, \varepsilon)$, or both. To disentangle these factors, we measure the policy success rate on the training tasks conditioned in three ways: a one-hot task identifier, language embeddings of the training task commands, and video embeddings of held-out human videos of the training tasks. This comparison is in Table 3. The similar performance between one-hot and language suggests the latent language space is sufficient, and that languageconditioned performance on held-out tasks is bottlenecked on the control layer more than the embedding. The more significant drop in performance of video-conditioned policies suggests inferring tasks from videos is much more difficult, particularly for held-out tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: left;">Task Conditioning</th>
<th style="text-align: center;">Success</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: left;">One-hot</td>
<td style="text-align: center;">$42 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Language</td>
<td style="text-align: left;">$40 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Video</td>
<td style="text-align: left;">$24 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Held-Out</td>
<td style="text-align: left;">Language</td>
<td style="text-align: center;">$32 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Video</td>
<td style="text-align: left;">$4 \%$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>cies suggests inferring tasks from videos is much more difficult, particularly for held-out tasks.</p>
<h1>6.3 Ablation Studies and Comparisons</h1>
<p>We validate the importance of several BC-Z design decisions using the (training) 21-task family. Our first set of ablations evaluate on the "place the bottle in ceramic bowl" command, which has the most demos (1000) of any task. We first test whether multi-task training is helpful for performance: we compare the multi-task system trained on 25,877 demos across all tasks, to a single-task policy trained on just the 1000 demos for the target task. In Table 4 (left), the single-task baseline achieves</p>
<p>Table 4: Ablation Studies. Left: Multi-task vs. single task models on the 'place the bottle in the ceramic bowl' task. Training across tasks and with adaptive state-diffs is important for good training performance. Right: DAgger comparison on 'place the bottle in the ceramic bowl' (1-Task) and the 8-Task subset from Table 7. Controlled for the same amount of data, DAgger reaches higher success numbers significantly more quickly.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>1-Task</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-task, language conditioned</td>
<td>$52\%(6.3)$</td>
</tr>
<tr>
<td>Multi-task, one-hot conditioned</td>
<td>$45\%(5.3)$</td>
</tr>
<tr>
<td>Single-task baseline (1000 demos)</td>
<td>$5\%(2.8)$</td>
</tr>
<tr>
<td>Multi-task, one-hot, no adaptive state-diff</td>
<td>$3\%(2.3)$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>1-Task</th>
<th>8-Task</th>
</tr>
</thead>
<tbody>
<tr>
<td>100% Manual</td>
<td>$27\%(5.2)$</td>
<td>$23\%(4.2)$</td>
</tr>
<tr>
<td>50% Manual +</td>
<td>$53\%(5.8)$</td>
<td>$47\%(5.2)$</td>
</tr>
<tr>
<td>50% HG-DAgger</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>just 5% success. The low number is consistent with the low single-task performance on holdout tasks from Section 6.2 collecting data over several robots and operators likely makes the task harder to learn. Only when pooling data across many tasks does BC-Z learn to solve the task. We ablate the adaptive state diff scheme described in Section 5.3 and find that it is important; when naively choosing the $N=1$ future state to compute the expert actions, the policy fits the noise and moves too slowly, resulting in state drift away from good trajectories.</p>
<p>We next ablate the use of HG-DAgger while keeping the total amount of data fixed. Specifically, we compare performance of policies trained using 50% expert demos and 50% HG-DAgger interventions, versus using 100% expert demos. In Table 4 (right), we find that HG-DAgger significantly improves task performance over cloning expert demonstrations on both the 'place bottle in ceramic bowl' task and 7 other training tasks. Further details on this comparison are in Appendix K. Finally, in Figure 5, we evaluate whether measuring HG-DAgger interventions can give us a live proxy of policy performance. We see that intervention frequency is inversely correlated with policy success, as measured by the fraction of successful episodes not requiring intervention. This result suggests that we can indeed use this metric with HG-DAgger for development purposes.</p>
<h2>7 Discussion</h2>
<p>We presented a multi-task imitation learning system that combines flexible task embeddings with large-scale training on a 100-task demonstration dataset, enabling it to generalize to entirely new tasks that were not seen in training based on user-provided language or video commands. Our evaluation covered 29 unseen vision-based manipulation tasks with a variety of objects and scenes. The key conclusion of our empirical study is that simple imitation learning approaches can be scaled in a way that facilitates generalization to new tasks with zero additional robot data of those tasks. That is, we learn that we do not need more complex approaches to attain task-level generalization. Through the experiments, we also learn that 100 training tasks is sufficient for enabling generalization to new tasks, that HG-DAgger is important for good performance, and that frozen, pre-trained language embeddings make for excellent task conditioners without any additional training.</p>
<p>Our system does have a number of limitations. First, the performance on novel tasks varies significantly. However, even for tasks that are less successful, the robot often exhibits behavior suggesting that it understands at least part of the task, reaching for the right object or performing a semantically related motion. This suggests that an exciting direction for future work is to use our policies as a general-purpose initialization for finetuning of downstream tasks, where additional training, perhaps with autonomous RL, could lead to significantly better performance. The structure of our language commands follows a simple “(verb) (noun)” structure. A direction to address this limitation is to relabel the dataset with a variety of human-provided annotations [24], which could enable the system to handle more variability in the language structure. Another limitation is the lower performance of the video-conditioned policy, which encourages future research on improving the generalization of video-based task representations and enhancing the performance of imitation learning algorithms as a whole, as low-level control errors are also a major bottleneck.</p>
<h1>Acknowledgments</h1>
<p>Eric Jang, Alex Irpan, and Frederik Ebert ran experiments on different forms of task conditioning in the task generalization setup. Mohi Khansari helped build the HG-DAgger interface and ran experiments in the bin-emptying and door opening tasks. Daniel Kappler built the data annotation visualizer. Corey Lynch advised Frederik's internship and gave pointers on language models. Sergey Levine and Chelsea Finn supervised the project.
We would like to give special thanks to Noah Brown, Omar Cortes, Armando Fuentes, Kyle Jeffrey, Linda Luu, Sphurti Kirit More, Jornell Quiambao, Jarek Rettinghouse, Diego Reyes, Rosario Jauregui Ruano, and Clayton Tan for overseeing robot operations and collecting human videos of the tasks, as well as Jeffrey Bingham and Kanishka Rao for valuable discussions.</p>
<h2>References</h2>
<p>[1] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via meta-learning. In Conference on Robot Learning, pages 357-368. PMLR, 2017.
[2] S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation learning. In Conference on Robot Learning, pages 783-795. PMLR, 2018.
[3] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. Robotics: Science and Systems, 2018.
[4] A. Bonardi, S. James, and A. J. Davison. Learning one-shot imitation from humans without humans. IEEE Robotics and Automation Letters, 5(2):3533-3539, 2020.
[5] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made easy. arXiv e-prints, pages arXiv-2008, 2020.
[6] Y. Duan, M. Andrychowicz, B. C. Stadie, J. Ho, J. Schneider, I. Sutskever, P. Abbeel, and W. Zaremba. One-shot imitation learning. arXiv preprint arXiv:1703.07326, 2017.
[7] S. Dasari and A. Gupta. Transformers for one-shot visual imitation. Conference on Robot Learning (CoRL), 2020.
[8] R. Rahmatizadeh, P. Abolghasemi, L. Bölöni, and S. Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE international conference on robotics and automation (ICRA), pages 3758-3765. IEEE, 2018.
[9] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469-483, 2009.
[10] S. Khansari-Zadeh and A. Billard. Learning stable non-linear dynamical systems with gaussian mixture models. IEEE Transaction on Robotics, 27(5):943-957, 2011.
[11] A. Billard, Y. Epars, S. Calinon, S. Schaal, and G. Cheng. Discovering optimal imitation strategies. Robotics and autonomous systems, 47(2-3):69-77, 2004.
[12] S. Schaal, J. Peters, J. Nakanishi, and A. Ijspeert. Learning movement primitives. In Robotics research. the eleventh international symposium, pages 561-572. Springer, 2005.
[13] R. Chalodhorn, D. B. Grimes, K. Grochow, and R. P. Rao. Learning to walk through imitation. In IJCAI, volume 7, pages 2084-2090, 2007.
[14] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal. Learning and generalization of motor skills by learning from demonstration. In 2009 IEEE International Conference on Robotics and Automation, pages 763-768. IEEE, 2009.
[15] K. Mülling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 32(3):263279, 2013.
[16] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Technical report, Carnegie-Mellon University, 1989.</p>
<p>[17] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and P. Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 5628-5635. IEEE, 2018.
[18] A. Zhou, E. Jang, D. Kappler, A. Herzog, M. Khansari, P. Wohlhart, Y. Bai, M. Kalakrishnan, S. Levine, and C. Finn. Watch, try, learn: Meta-learning from demonstrations and reward. International Conference on Learning Representations (ICLR), 2020.
[19] T. L. Paine, S. G. Colmenarejo, Z. Wang, S. Reed, Y. Aytar, T. Pfaff, M. W. Hoffman, G. BarthMaron, S. Cabi, D. Budden, et al. One-shot high-fidelity imitation: Training large-scale deep nets with rl. arXiv preprint arXiv:1810.05017, 2018.
[20] D.-A. Huang, S. Nair, D. Xu, Y. Zhu, A. Garg, L. Fei-Fei, S. Savarese, and J. C. Niebles. Neural task graphs: Generalizing to unseen tasks from a single video demonstration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8565-8574, 2019.
[21] D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell. Zero-shot visual imitation. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 2050-2053, 2018.
[22] P. Goyal, R. J. Mooney, and S. Niekum. Zero-shot task adaptation using natural language. arXiv preprint arXiv:2106.02972, 2021.
[23] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor. Languageconditioned imitation learning for robot manipulation tasks. arXiv preprint arXiv:2010.12083, 2020.
[24] C. Lynch and P. Sermanet. Grounding language in play. arXiv preprint arXiv:2005.07648, 2020.
[25] S. Calinon, P. Evrard, E. Gribovskaya, A. Billard, and A. Kheddar. Learning collaborative manipulation tasks by demonstration using a haptic interface. In 2009 International Conference on Advanced Robotics, pages 1-6. IEEE, 2009.
[26] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
[27] S. Ross and J. A. Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.
[28] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. Dart: Noise injection for robust imitation learning. In Conference on robot learning, pages 143-156. PMLR, 2017.
[29] M. Kelly, C. Sidrane, K. Driggs-Campbell, and M. J. Kochenderfer. Hg-dagger: Interactive imitation learning with human experts. In 2019 International Conference on Robotics and Automation (ICRA), pages 8077-8083. IEEE, 2019.
[30] J. Spencer, S. Choudhury, M. Barnes, M. Schmittle, M. Chiang, P. Ramadge, and S. Srinivasa. Learning from interventions: Human-robot interaction as both explicit and implicit feedback. 07 2020. doi:10.15607/RSS.2020.XVI.055.
[31] M. Khansari, D. Kappler, J. Luo, J. Bingham, and M. Kalakrishnan. Action image representation: Learning scalable deep grasping policies with zero real world data. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 3597-3603, 2020.
[32] L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406-3413. IEEE, 2016.
[33] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dexnet 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. arXiv preprint arXiv:1703.09312, 2017.</p>
<p>[34] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning, pages 651-673. PMLR, 2018.
[35] J. Hatori, Y. Kikuchi, S. Kobayashi, K. Takahashi, Y. Tsuboi, Y. Unno, W. Ko, and J. Tan. Interactively picking real-world objects with unconstrained spoken language instructions. In IEEE International Conference on Robotics and Automation (ICRA), 2018.
[36] A. Gupta, A. Murali, D. Gandhi, and L. Pinto. Robot learning in homes: Improving generalization and reducing dataset bias. arXiv preprint arXiv:1807.07049, 2018.
[37] F. Sadeghi, A. Toshev, E. Jang, and S. Levine. Sim2real viewpoint invariant visual servoing by recurrent control. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4691-4699, 2018.
[38] J. Tobin, L. Biewald, R. Duan, M. Andrychowicz, A. Handa, V. Kumar, B. McGrew, A. Ray, J. Schneider, P. Welinder, et al. Domain randomization and generative models for robotic grasping. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3482-3489. IEEE, 2018.
[39] B. Mehta, M. Diaz, F. Golemo, C. J. Pal, and L. Paull. Active domain randomization. In Conference on Robot Learning, pages 1162-1176. PMLR, 2020.
[40] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12627-12637, 2019.
[41] J. Zhang, L. Tai, P. Yun, Y. Xiong, M. Liu, J. Boedecker, and W. Burgard. Vr-goggles for robots: Real-to-sim domain adaptation for visual control. IEEE Robotics and Automation Letters, 4(2):1148-1155, 2019.
[42] C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.
[43] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019.
[44] Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan, B. Eysenbach, R. Julian, C. Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.
[45] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.
[46] Y. Yang, D. Cer, A. Ahmad, M. Guo, J. Law, N. Constant, G. H. Abrego, S. Yuan, C. Tar, Y.-H. Sung, et al. Multilingual universal sentence encoder for semantic retrieval. arXiv preprint arXiv:1907.04307, 2019.
[47] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[48] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630-645. Springer, 2016.
[49] P. J. Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pages 492-518. Springer, 1992.
[50] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>[51] D. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y. Bai. Retinagan: An object-aware approach to sim-to-real transfer. In 2021 International Conference on Robotics and Automation (ICRA), 2021.
[52] P. de Haan, D. Jayaraman, and S. Levine. Causal confusion in imitation learning. arXiv preprint arXiv:1905.11979, 2019.
[53] H. Zhang, M. Cissé, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR (Poster), 2018. URL https://openreview.net/forum?id=r1Ddp1-Rb.
[54] A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879-9889, 2020.</p>
<h1>Appendix</h1>
<h2>A Teleoperation Interface</h2>
<p>The human teleoperator holds two wireless Oculus Quest controllers and uses the same interface to perform demonstrations for all tasks. When the override button is held (the clutch on the right controller), the robot arm tracks the controller's position and orientation. The robot can be toggled between autonomous and manual mode. In manual mode, the robot stays still unless the operator moves it. In autonomous mode, the robot follows a learned policy, unless the operator overrides it.</p>
<p>Table 5: Teleoperation buttons and controls.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Control</th>
<th style="text-align: left;">Function</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Right Controller (Arm)</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">A</td>
<td style="text-align: left;">Start recording, or mark demo as success if already recording</td>
</tr>
<tr>
<td style="text-align: left;">B</td>
<td style="text-align: left;">Stops current recording marking as failure (if applicable), then bring <br> robot to reset pose <br> Override policy and engage manual arm teleop until clutch is released <br> Continuous gripper closure. Pressing the trigger all the way closes the <br> gripper fully, and letting go of the trigger opens the gripper.</td>
</tr>
<tr>
<td style="text-align: left;">Left Controller (Base)</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">X</td>
<td style="text-align: left;">Stop recording demonstration and mark as failure</td>
</tr>
<tr>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">Engage / disengage autonomous policy <br> Control base forward and yaw velocity (for door opening)</td>
</tr>
<tr>
<td style="text-align: left;">Joystick</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h2>B Data Collection Details</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Objects used for the 100-task manipulation tasks. Object Set 1 (left) was used to collect data for 21 train tasks, and Object Set 2 (right) was used to collect data for 79 train tasks. For evaluation, 4 tasks were generated between objects in Object Set 2, and 24 holdout tasks used objects across both sets. We used several instances of these objects, with occasional slight differences. For example, the ceramic bowl in Object Set 1 is red in the picture above, but data was also collected with ceramic bowls that were painted green or blue instead.</p>
<h2>B. 1 Inter-task Variability</h2>
<p>The data collection protocol is distributed across multiple robots in 1-4 physical locations, resulting in a policy that handles variations across robot hardware, different backgrounds, and scene configurations. Furthermore, each data collection station uses a set of objects with slight physical variations. For instance, the ceramic bowls come in different colors, the sponges can be blue or white with differences in shape, and the erasers and peppers come in different sizes and materials. This variability,</p>
<p>illustrated in Figure 7, results in a higher sample complexity needed to achieve a desired level of performance on a given set of objects.
The low performance of the single-task baseline in Table 4 trained on 1000 demonstrations begs the question of whether this is due to sample-inefficiency of the behavior cloning implementation, or whether a large number of demos are needed to generalize across the variability in the training data. To study this question, we verify single-task policy performance on "place the bottle in the ceramic bowl" in a simulated version of the task, where we can minimize variability across evals and perform exhaustive evaluation of all training checkpoints. When the scene is initialized deterministically with no randomization in initial object positions, 37 expert demos are sufficient to learn the single-task policy with a $97.2 \%$ ( 0.7 ) success rate. However, when objects in the scene are randomized, a singletask BC baseline only learns a success rate of $56 \%$ (2.2) when trained on 40 expert demos. These results suggest that the low success rate of the single-task policies in the real setup are indeed caused by the increased diversity in the environment, instead of other factors.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Within each manipulation sub-task (e.g. "place the bottle in the ceramic bowl") the policy must generalize not only to variations in object positions, but also varying backgrounds, different robots, and different object instance variations. The top row shows a simulated setup used to confirm that in absence of scene variation, only 37 expert demonstrations are required to solve the task with a $97 \%$ success rate.</p>
<h1>B. 2 Robotic Teleoperation</h1>
<p>Data for the experiments reported in the paper were collected by 7 operators over the course of 5 months. During data collection, tasks are sampled randomly at the beginning of each episode. Objects were occasionally shuffled between episodes, but usually were not, meaning the final state of a demo for one task would often be the start state of a demo for the next task. We found that sampling tasks uniformly was important to performance, since asking teleoperators to pick tasks themselves biased task sampling towards demos that would be easy to perform, creating spurious correlations between initial scene and task demonstrated.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Human demonstrations of the task (left) are augmented with random distortions and reflections (right), then trained to match language features for the task. Robot videos from expert demos are embedded with the same network, along with an end-to-end policy loss.</p>
<p>The camera view is kept fixed over the episode. Automatically moving the head to keep the gripper centered in camera frame affords a larger workspace for performing manipulation tasks, but made learning substantially harder.</p>
<p>We experimented with injecting noise into expert actions, following methods like DART [28], but found that this compromised the ease of providing demonstrations.</p>
<h1>B. 3 Human Video Data Collection</h1>
<p>Human videos were collected in a variety of home and office environments. In each environment, a copy of the scene was set up, and videos were recorded simultaneously by several webcams from different viewpoints. Each viewpoint is treated as a different example of the task, allowing us to collect several videos at once. In general, we found that human videos could be demonstrated $5 \mathrm{x}-7 \mathrm{x}$ faster than a teleoperated robot, so collecting these videos took much less time than collecting the robot dataset.</p>
<h2>C Featurization Details</h2>
<p>In this work, actions are defined as the difference between states. We found using directly adjacent states ( $N=1$ apart) led to poor performance. Increasing $N$ made action magnitude larger, making the action easier to learn, but introduces bias in the robot trajectory depending on $N$. Through experiments, we found this bias was most problematic when the robot was changing its gripper between open and closed. We therefore used an adaptive algorithm to choose $N$. For each state, we initialize $N=1$, then increment it until the change in gripper value of the $N$ 'th future state exceeds 0.01 or the L2 norm of the arm joint deltas exceeds 0.05 . Intuitively, this speeds up the arm when moving the tool across the workspace (higher bias) and slows it down when it is about to come into or out of contact. We also discard labels in which neither the end effector or the gripper are moving.</p>
<h2>D Policy Training Details</h2>
<p>A common approach in behavior cloning is to use a Gaussian policy $\pi(a \mid s, z)=\mathcal{N}(\mu, \sigma)$ to maximize log likelihood of expert actions. We found using a deterministic policy was sufficient to achieve generalization. The network $\pi(a \mid s, z)$ predicts the delta XYZ, delta axis-angle, and gripper angle ( 0 to 1). A Huber loss $(\delta=1.0)$ [49] is used for XYZ and axis angle, and a log loss is used for gripper angle.</p>
<p>We scaled the XYZ delta losses, axis-angle delta losses, and gripper angle losses by weights of 100, 10 , and 0.5 , respectively, in order to keep the losses of comparable magnitude for each part of the action.</p>
<p>We implement the model using the FiLM-conditioned ResNet implementation from the open-source Tensor2Robot framework ${ }^{2}$. All models are trained using the Adam optimizer with default TensorFlow momentum parameters. When conditioning the model, we add $\mathcal{N}(0,0.1)$ Gaussian noise to embedding of the task command, which was critical to getting the model to predict actions based on the task embedding (as opposed to spurious correlations in the camera image). Multitask manipulation policies were trained with a batch size of 4096 on a TPUv3 pod with a learning rate of $5 \mathrm{e}-3$.</p>
<p>For video task commands, we found policy performance was stronger if we averaged several video embeddings prior to conditioning. When evaluating a novel task, we average the embeddings of 10 new human videos collected for the task.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Bin emptying and door opening policies were trained with asynchronous SGD over 10 GPUs with a batch size of 32 per worker and a learning rate of $2.5 \mathrm{e}-4$. In these tasks, we do not add FiLM layers to condition the model, since there is no task command.</p>
<h1>E Video Conditioning Details</h1>
<p>For video conditioning, we initially applied a method similar to Task-Embedded Control [2]. A contrastive loss (cosine similarity or InfoNCE [50]) was applied to the videos, along with the end-to-end BC loss. Embedding visualizations (see Figure 9) revealed that the embeddings learned were subpar, collapsing to two main clusters, one for the 21-task family and one for the 79-task family. We hypothesize that because the 21-task family and 79-task family each used a distinct set of objects, a purely unsupervised objective learned to identify the set of visible objects first, rather than the task performed. Adding the language regression loss helped align the videos more semantically.</p>
<h2>E. 1 Video Preprocessing and Architecture</h2>
<p>During training, videos $w$ are randomly subsampled to be 20 frames long, done such that the first and last frame of the video remain in the subsampled $w$. The same image augmentations used for training the policy are applied to the video, but we additionally apply random reflections along the x -axis and y -axis. The augmentation is sampled once per video and applied identically to all 20 frames. At inference time, no augmentations are applied, and the subsampling of 20 frames is done at a uniform frame rate.</p>
<p>The 20 frames are arranged into a 4 x 5 array of images that are processed by a 2D ResNet-18 network. Arranging the images in this manner allows the 2D convolutions in the ResNet to perform several layers of temporal convolution without needing to tune a new architecture. This was more memory efficient and more performant than a previously tried temporal convolution architecture. After mean-pooling the final visual features, we add a fully-connected layer with 32 units and ReLU non-linearity, then a linear layer with 512 units and no non-linearity, to match the final size of the language output. The final embedding is normalized to have unit L2 norm. The intermediate 32 unit layer is used to restrict the expressivity of the embedding.
To implement the end-to-end behavior cloning loss, every batch of data must be made of paired examples: one human video $w_{h} \in \mathcal{D}<em t="t">{h}^{t}$ to generate embedding $z$, and one expert demo $\left{\left(s</em>\right)}, a_{t<em e="e">{t=1}^{T}\right} \in$ $\mathcal{D}</em>$ of matching task to act as labels for $\pi(a \mid s, z)$. We first sample a batch of tasks, with replacement. For each task, we sample 1 human video and 1 robot demo, combining them into one overall batch for the model. The model is trained using 18 V100 GPUs with a batch size of 28 per GPU. Since each example is 1 pair of videos, every GPU effectively trains on 56 videos at a time. The model was trained with async gradient descent, with a learning rate picked via random search (2.45e-4).
At train time, the sequence of images $s_{t}$ in the robot demo are treated as a video of the task, and preprocessed in the same way as the human video. Human videos and robot videos are encoded with the same $q$, and both are trained to match the language embedding with a cosine loss, define as $D_{\text {cos }}\left(v_{1}, v_{2}\right)=1-v_{1} \cdot v_{2}$. Embedding noise is not added to $z_{h}^{i}$ for the end-to-end loss.}^{i</p>
<h2>E. 2 Ablation on Video Encoder Batch</h2>
<p>The sampling strategy for the video encoder batch is non-standard. By sampling tasks, then 1 human video and 1 robot video per task, every task will appear equally often, and every batch is guaranteed to be $50 \%$ human $50 \%$ robot. To examine how this affected model performance, we ran ablations where we first removed the end-to-end behavioral cloning loss, leaving just the language regression objective. Controlling for the same architecture, dataset, hyperparameters, and training time, we change the batch sampling strategy, to either directly sample human and robot videos from all tasks (maintaining a 50-50 batch), or sampling the entire batch uniformly at random over all videos.
Ablations in Table 6 indicate the task-based sampling scheme gives best performance. We hypothesize this sampling does better because it implicitly balances data across both tasks and modalities. The ResNet-18 model also includes batch norm, so it is possible the task-based batch construction affects batch norm in a positive way.</p>
<h2>F Video Embedding Visualization</h2>
<p>Figure 9 is a similarity matrix across different task embeddings. For each task, we first average the embeddings for all holdout human videos for that task. Entry $(i, j)$ of the matrix is then the</p>
<p>Input: Task commands $\mathcal{W}$, per-task robot dataset $\mathcal{D}<em h="h">{i}^{i}$, per-task human video data $\mathcal{D}</em>\right)$
while not done training do
Sample a batch of tasks $i$, with replacement.
for each task $i \in$ batch do
Sample human video $w_{h} \in \mathcal{D}}^{i}$, language encoder $q\left(\cdot \mid w_{t}^{i}\right)$, video encoder $q\left(\cdot \mid w_{h<em t="t">{h}^{i}$
Sample robot demo $\left{\left(s</em>\right)\right}}, a_{t<em e="e">{t=1}^{T} \in \mathcal{D}</em>$
Retrieve language command $w_{t}^{i}$
$z_{h}^{i} \sim q\left(\cdot \mid w_{h}\right) \quad / /$ embed human video
$z_{e}^{i} \sim q\left(\cdot \mid\left{s_{t}\right}}^{i<em t="t">{t=1}^{T}\right) \quad / /$ embed robot video
$z</em>\right) \quad / /$ get language vector
Sample $t \in 1, \cdots, T$
Compute action $\pi\left(\hat{a} \mid s_{t}, z_{h}^{i}\right)$
BC-loss $\leftarrow 100 \cdot \operatorname{Huber}(x y z)+10 \cdot \operatorname{Huber}$ (angle) $+0.5 \cdot \log \operatorname{Loss}($ gripper $)$
Minimize $\mathcal{L} \leftarrow \mathrm{BC}$-loss $+D_{\text {cos }}\left(z_{h}^{i}, z_{t}^{i}\right)+D_{\text {cos }}\left(z_{e}^{i}, z_{t}^{i}\right)$
end
end}^{i} \sim q\left(\cdot \mid w_{t}^{i</p>
<p>Table 6: Ablations of video encoder batch composition. In the ablations below, we control for the same architecture, dataset, hyperparameters, and training time, changing only the sampling strategy for each batch. The end-to-end behavioral cloning loss is removed, leaving just the language regression loss. Accuracy is measured over held-out videos of training tasks, by checking whether the video embedding is closest to the true language embedding from all 100 train tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Video Encoder Batch Makeup</th>
<th style="text-align: left;">Video Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline (sample 28 tasks, pick 1 human +1 robot video per task)</td>
<td style="text-align: left;">$84 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Sample 28 human videos +28 robot videos</td>
<td style="text-align: left;">$80 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Sample 56 videos from entire dataset</td>
<td style="text-align: left;">$74 \%$</td>
</tr>
</tbody>
</table>
<p>cosine similarity between that mean embedding for task $i$ and task $j$. The first 21 rows/columns of the visualization are the 21-task family, and the remaining entries are sorted alphabetically, which groups tasks with the same leading verbs together (i.e. all "place grapes in X" are adjacent to one another.) Adding a language loss term helps prevent the model from grouping tasks based on task family rather than semantic meaning.</p>
<h1>G Data Annotation Visualization</h1>
<p>During data collection, teleoperators would occasionally record an unsuccessful demonstration as a success, or vice versa. To fix these errors, we built a data visualizer where demos could be retrieved via SQL queries over their metadata, then reannotated. In addition to fixing incorrect labels, we used this tool to perform general data cleaning, such as flagging demonstrations where the robot hardware was faulty, or the arm occluded a target object for the entire demo. The interface is shown in Figure 10.</p>
<h2>H Single-Task Validation on Bin-Emptying</h2>
<p>Figure 11 illustrates the bin emptying and door opening environments. In Figure 12, we plot the bin emptying rate as a function of the amount of data used to train the policy. The policy trained on 30 hours of expert demonstrations can clear 3-4 objects a minute. By comparison, a human teleoperator requires about 43 seconds to demonstrate the task.</p>
<p>An interesting observation to note here is that the bin-emptying task is inherently multimodal, as the objects are grasped in any order during teleoperator demonstrations. The policy learned is a deterministic unimodal policy, and should in principle struggle to learn this multi-model task. but our model architecture is still able to solve the task. One hypothesis for why this worked in practice is that the noise and variety within the dataset enabled the model to break symmetry. For instance, the model may have learned to servo towards the nearest object in cases of ambiguity, or it may</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="n">Pseudocode</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">BC</span><span class="o">-</span><span class="n">Z</span>
<span class="w">    </span><span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="n">commands</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">W</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">per</span><span class="o">-</span><span class="n">task</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">D</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">e</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">per</span><span class="o">-</span><span class="n">task</span><span class="w"> </span><span class="n">human</span><span class="w"> </span><span class="n">video</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">D</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">h</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="k">language</span>
<span class="w">        </span><span class="n">encoder</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">q</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">w_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">video</span><span class="w"> </span><span class="n">encoder</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">q</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">w_</span><span class="err">{</span><span class="n">h</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">videos</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">average</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">N</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="k">Get</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">vectors</span><span class="p">,</span><span class="w"> </span><span class="n">computed</span><span class="w"> </span><span class="n">once</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="k">start</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">training</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">taskToVec</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\{\}\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="k">every</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">video</span><span class="o">-</span><span class="n">conditioned</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">z</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">c</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">                </span><span class="n">Sample</span><span class="w"> </span><span class="n">video</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">w_</span><span class="err">{</span><span class="n">h</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">D</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">video</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span><span class="w"> </span><span class="n">matching</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">z_</span><span class="err">{</span><span class="n">h</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="n">q</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">w_</span><span class="err">{</span><span class="n">h</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">z</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">z</span><span class="o">+</span><span class="n">z_</span><span class="err">{</span><span class="n">h</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">taskToVec</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">L2Normalize</span><span class="err">}</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="n">Retrieve</span><span class="w"> </span><span class="k">language</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">w_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">z</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">q</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">w_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">taskToVec</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">L2Normalize</span><span class="err">}</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="n">Train</span><span class="w"> </span><span class="n">policy</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="k">every</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">s_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">a_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">D</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">z</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">taskToVec</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]+</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">N</span><span class="err">}</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">L</span><span class="err">}</span><span class="o">=</span><span class="mi">100</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Huber</span><span class="err">}</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="o">+</span><span class="mi">10</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Huber</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">angle</span><span class="w"> </span><span class="err">\</span><span class="p">()</span><span class="o">+</span><span class="mf">0.5</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">LogLoss</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">gripper</span><span class="w"> </span><span class="err">\</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">Update</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">pi</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="o">^</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">minimize</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">L</span><span class="err">}\</span><span class="p">)</span>
</code></pre></div>

<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Visualizations of different video encoders. Each row and column indicates a different task, with the entry at $(i, j)$ indicating the cosine similarity between video embeddings for task $i$ and task $j$. The 21-task family are the first 21 rows/columns of the visualization. The contrastive baseline learns to primarily group by objects in the scene (by task family), rather than task performed.
have used spurious correlations in the background to commit to a specific object. Whatever the mechanism, it suggests that simple architectures can be sufficient to learn complex tasks, as long as they are trained with appropriate data.</p>
<h1>I Single-Task Validation on Door Opening</h1>
<p>When validating BC-Z on a door opening task, the policy is trained using 12,000 demonstrations collected across 24 meeting rooms, and 36,000 demonstrations across 5 meeting rooms in simulation. For the 24 real meeting rooms, 10 of them swing open from the right, and 14 swing open from the left. For the 5 sim meeting rooms, 3 are right swing, and 2 are left swing.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: A data browsing and reannotation web interface is used to manually curate episodes and check for low-quality demonstrations. Episodes are searchable via SQL, and their metadata can be edited in-place through the web interface.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Bin Emptying and Door Opening tasks are used to validate that BC-Z can achieve a high level of task success and generalize to unseen scenes in a single-task imitation learning setting.</p>
<p>For each of these demos, the initial arm configuration, as well as the robot pose with respect to the meeting room's door, were randomized at the start of each run. Instead of controlling the arm, the policy predicts the forward and yaw velocity of the base. This is an easier control problem, but the policy must still determine the position of the randomized arm relative to the door. The policy also predicts a binary termination action to decide when to stop executing actions, which is predicted with a log loss. Real-world demonstrations were collected by 13 operators using a fleet of 30 robots. Sim demonstrations were collected using the same interface, with a virtual robot and door rendered on the computer monitor.</p>
<p>To utilize the simulated door opening demonstrations, we used sim-to-real transfer to lower the number of required real world demonstrations. A RetinaGAN model [51] adapted simulated images to look like real images. The RetinaGAN model was trained separately on both sim and real door opening images in an unsupervised fashion by enforcing object-detection consistency between the two domains. Using the RetinaGAN model, we created a third dataset which consists of adapted sim images. We trained the final policy on a mixture of three datasets: sim, real, and adapted sim2real.</p>
<p>Policies were evaluated over 4 holdout meeting rooms, made of 2 right swing doors and 2 left swing doors. For consistency during the evaluation, the operator placed the robot in a certain marked location in front of the meeting room. Then the robot drives to a random pose sampled in an area of 25 x 25 cm from the marked location, and afterwards the trained policy takes control of the robot and performs the task. An episode is marked success if the door is sufficiently open for the robot to enter the room and at the time the policy terminates the task the robot is not in contact with the</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Objects picked in 120 seconds vs. dataset size for bin emptying task
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: An example of adapting a sim image (left) to look real (right) using RetinaGAN [51].
environment (including the door). Further, any collision of the robot base and arm (not including the gripper) with the environment counted as the task failure by the operator.</p>
<h1>J Multi-Task Manipulation Training Tasks</h1>
<p>Table 7 lists full results of the different task conditioning ablation from Table 3, over the 21-task family. The other train tasks in the 79-task family are in Table 8.</p>
<h2>K Details on HG-DAgger Ablation</h2>
<p>Two models were trained, one using all expert-only demonstrations, and one using a 50-50 mixture of expert-only demonstrations and HG-DAgger demonstrations. The 50-50 dataset has the same number of episodes as the expert-only dataset. To reduce evaluation time, the HG-DAgger comparison was evaluated on a smaller subset of 8 tasks from the 21-task family, indicated in Table 7 by the ${ }^{8}$ symbol.</p>
<h2>L Negative Results</h2>
<p>Below is an incomplete list of alternative methods tried during the course of the research project. These are empirical observations and are presented as-is. These ideas were generally tried once or twice, then set aside when they did not improve performance. We did not do a detailed investigation of the negative results, so it is possible they did not perform better due to incorrect implementation, the presence of a different performance bottleneck in the system, or improperly tuned hyperparameters, rather than deficiencies in their ideas. We hope this anecdotal experience will be helpful to researchers building on top of this work.</p>
<ul>
<li>The policy is trained with a Huber loss $(\delta=1.0)$ ), that in practice is usually just an MSE loss, since predictions almost always lie within $(-1,1)$. A deterministic policy trained with mean-squared error can be viewed as max likelihood with a unimodal Gaussian policy, with learned mean $\mu$ and fixed $\sigma$. We tried a stochastic policy, with a learned $\sigma$ based on the current state, but found it did not help and seemed to make training less stable.</li>
<li>Similarly, a mixture density network (mixture of 10 Gaussians) did not improve performance.</li>
</ul>
<p>Table 7: Performance comparsion one-hot, language, or video conditioning over 21 training tasks. Video policies are conditioned on held-out videos of the training tasks. Tasks are ordered by increasing average "Demo Length", the number of states observed per expert demonstration, or roughly how many actions the learning policy must learn per episode (both executing at 10 Hz ). Length is treated as an estimate of difficulty. A subset of eight tasks denoted by ${ }^{8}$ are used for comparing additional ablations. Numbers in (parentheses) are 1 unit standard deviation. See Table 8 for remaining training tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tasks From 21-Task Family</th>
<th style="text-align: left;">Demo Length</th>
<th style="text-align: left;">One-Hot</th>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">Video</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">'knock the eraser over'</td>
<td style="text-align: left;">$54(20)$</td>
<td style="text-align: left;">$65 \%(7.0)$</td>
<td style="text-align: left;">$91 \%(8.7)$</td>
<td style="text-align: left;">$40 \%(21.9)$</td>
</tr>
<tr>
<td style="text-align: left;">'knock the bottle over'</td>
<td style="text-align: left;">$61(22)$</td>
<td style="text-align: left;">$58 \%(7.4)$</td>
<td style="text-align: left;">$71 \%(12.1)$</td>
<td style="text-align: left;">$83 \%(15.2)$</td>
</tr>
<tr>
<td style="text-align: left;">'pick up the ceramic cup'8</td>
<td style="text-align: left;">$65(25)$</td>
<td style="text-align: left;">$67 \%(4.7)$</td>
<td style="text-align: left;">$56 \%(7.2)$</td>
<td style="text-align: left;">$83 \%(15.2)$</td>
</tr>
<tr>
<td style="text-align: left;">'pick up the ceramic bowl'</td>
<td style="text-align: left;">$73(32)$</td>
<td style="text-align: left;">$65 \%(7.3)$</td>
<td style="text-align: left;">$77 \%(11.7)$</td>
<td style="text-align: left;">$35 \%(12.8)$</td>
</tr>
<tr>
<td style="text-align: left;">'push the ceramic bowl across the table'</td>
<td style="text-align: left;">$89(42)$</td>
<td style="text-align: left;">$67 \%(8.2)$</td>
<td style="text-align: left;">$58 \%(14.2)$</td>
<td style="text-align: left;">$50 \%(35.4)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the pepper in the ceramic bowl'</td>
<td style="text-align: left;">$98(30)$</td>
<td style="text-align: left;">$33 \%(6.5)$</td>
<td style="text-align: left;">$39 \%(11.5)$</td>
<td style="text-align: left;">$12.5 \%(11.7)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the ceramic cup in the ceramic bowl'</td>
<td style="text-align: left;">$103(26)$</td>
<td style="text-align: left;">$22 \%(6.5)$</td>
<td style="text-align: left;">$33 \%(13.6)$</td>
<td style="text-align: left;">$66 \%(27.2)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the white sponge in the ceramic bowl'8</td>
<td style="text-align: left;">$106(32)$</td>
<td style="text-align: left;">$43 \%(5.1)$</td>
<td style="text-align: left;">$38 \%(6.7)$</td>
<td style="text-align: left;">$14 \%(13.2)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the bottle in the ceramic bowl'8</td>
<td style="text-align: left;">$110(32)$</td>
<td style="text-align: left;">$45 \%(5.3)$</td>
<td style="text-align: left;">$52 \%(6.3)$</td>
<td style="text-align: left;">$43 \%(18.7)$</td>
</tr>
<tr>
<td style="text-align: left;">'drag the pepper across the table'8</td>
<td style="text-align: left;">$115(43)$</td>
<td style="text-align: left;">$55 \%(5.0)$</td>
<td style="text-align: left;">$33 \%(8.2)$</td>
<td style="text-align: left;">$20 \%(12.6)$</td>
</tr>
<tr>
<td style="text-align: left;">'push the eraser across the table'8</td>
<td style="text-align: left;">$117(50)$</td>
<td style="text-align: left;">$71 \%(4.7)$</td>
<td style="text-align: left;">$45 \%(7.9)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the eraser on the white sponge'8</td>
<td style="text-align: left;">$121(37)$</td>
<td style="text-align: left;">$33 \%(4.9)$</td>
<td style="text-align: left;">$30 \%(7.2)$</td>
<td style="text-align: left;">$25 \%(15.3)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the pepper on the white sponge'</td>
<td style="text-align: left;">$123(44)$</td>
<td style="text-align: left;">$36 \%(7.1)$</td>
<td style="text-align: left;">$83 \%(10.8)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the pepper in the ceramic cup'8</td>
<td style="text-align: left;">$128(44)$</td>
<td style="text-align: left;">$30 \%(4.6)$</td>
<td style="text-align: left;">$49 \%(7.6)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the ceramic cup over the eraser'</td>
<td style="text-align: left;">$130(38)$</td>
<td style="text-align: left;">$9 \%(5.0)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the white sponge in the ceramic cup'</td>
<td style="text-align: left;">$131(38)$</td>
<td style="text-align: left;">$21 \%(6.9)$</td>
<td style="text-align: left;">$25 \%(10.8)$</td>
<td style="text-align: left;">$14 \%(13.2)$</td>
</tr>
<tr>
<td style="text-align: left;">'place the eraser in the ceramic cup'8</td>
<td style="text-align: left;">$135(43)$</td>
<td style="text-align: left;">$37 \%(4.8)$</td>
<td style="text-align: left;">$33 \%(7.5)$</td>
<td style="text-align: left;">$20 \%(17.9)$</td>
</tr>
<tr>
<td style="text-align: left;">'move the arm in a circular motion'</td>
<td style="text-align: left;">$144(69)$</td>
<td style="text-align: left;">$69 \%(6.7)$</td>
<td style="text-align: left;">$12 \%(6.8)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">'drag the ceramic bowl in a circle'</td>
<td style="text-align: left;">$164(65)$</td>
<td style="text-align: left;">$32 \%(6.8)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">'wipe the white sponge on the table'</td>
<td style="text-align: left;">$177(59)$</td>
<td style="text-align: left;">$29 \%(7.8)$</td>
<td style="text-align: left;">$18 \%(9.2)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">'stand the bottle upright'</td>
<td style="text-align: left;">$186(55)$</td>
<td style="text-align: left;">$5 \%(3.5)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
<td style="text-align: left;">$0 \%(0)$</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: left;">115</td>
<td style="text-align: left;">$42 \%$</td>
<td style="text-align: left;">$40 \%$</td>
<td style="text-align: left;">$24 \%$</td>
</tr>
</tbody>
</table>
<ul>
<li>Using a larger model architecture than ResNet-18 (ResNet-34 and larger) also did not improve performance.</li>
<li>To address the small action problem identified in Section 5.2, we tried decomposing XYZ prediction into direction and magnitude. The hypothesis was that by making it easier for the model to predict small actions, it would prevent the model from predicting small actions at every state. This did not outperform predicting XYZ directly, and eventually led to the adaptive algorithm used in the final results.</li>
<li>We initially used a spatial softmax layer in our policy and video encoder. Visualizing those spatial softmax layers made it easier to interpret policy predictions, but performance increased when the spatial softmaxes were removed.</li>
<li>Conditioning the policy on proprioceptive information, as well as previous robot poses, did not improve performance. It is possible this was due to causal confusion between that information and the expert actions [52].</li>
<li>Using more video frames ( 40 instead of 20 ) did not improve performance of the video encoder, and slowed down training.</li>
<li>We experimented with including human videos that did not correspond to any of the robot tasks, using them as negative examples for a contrastive loss, to encourage the task embeddings to be more continuous. We found this did not help, and the negative examples were too easy to embed far away from all other videos.</li>
<li>Pre-training the ResNet on the ILSVRC2012 object classification dataset did not improve performance.</li>
<li>We obtained better results on manipulation tasks by representing angles as delta axis-angle, rather than absolute axis-angle, absolute quaternions, or delta quaternions.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/google-research/tensor2robot/blob/master/layers/film_resnet_ model.py&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
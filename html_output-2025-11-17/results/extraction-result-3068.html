<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3068 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3068</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3068</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267897735</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14856v2.pdf" target="_blank">Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3068.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3068.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models evaluated in this study (aggregate summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate summary of behavior across evaluated open-access LLMs (LLaMA-2 family, Mistral-7B-Instruct, Zephyr-7B-β) on a 12-problem propositional logic benchmark; documents which reasoning strategies models produce, how prompting was done, and population-level performance/strategy differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate of LLaMA-2 (7B/13B/70B), Mistral-7B-Instruct, Zephyr-7B-β</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight transformer LLMs spanning the LLaMA-2 family (chat variants), Mistral-7B-Instruct, and Zephyr-7B-β (Mistral-based with alignment fine-tuning). Evaluated with zero-shot chain-of-thought prompting (system message + 'Let's think step by step') and nucleus sampling (top-p=0.9, temperature=0.6), 5 random seeds, manual annotation of reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['supposition following', 'chain construction', 'compound strategy', 'concatenation strategy', 'symbolic strategy', 'zero-shot chain-of-thought prompting ("Let\'s think step by step")']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models were prompted to 'think aloud' (zero-shot chain-of-thought). Observed strategies match human taxonomy from Van der Henst et al. (2002): supposition following (assume a case and trace consequences), chain construction (link conditionals into antecedent→consequent chains), compound (combine premises to derive new compound conditionals), concatenation (join intermediate deductions into one statement), and a symbolic strategy (translate to formal logic or truth tables). Symbolic traces include attempting truth tables or formal symbolic manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse across and within models: models exhibited multiple distinct strategies (diverse styles) rather than a single uniform approach; however, each model family showed a preferred subset of strategies (i.e., similar styles within a family). Determination is based on manual annotation of 300 responses and relative strategy frequency per model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>12-problem propositional logic benchmark (Van der Henst et al., 2002 style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Twelve neutral-content propositional reasoning/hypothesis-validation problems using colored-marble phrasing; 3–4 premises per problem (biconditionals, exclusive disjunctions, conditionals), half of conclusions valid, half invalid. Models must decide whether conclusion logically follows and verbalize reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Aggregated across models: method usage frequencies vary by model (see per-model entries). Overall final-answer accuracies per model (mean ± std over 5 seeds): Zephyr-7B-β 45.0% ±15.5, Mistral-7B-Instruct 55.0% ±10.0, LLaMA-2-7B 46.7% ±6.7, LLaMA-2-13B 40.0% ±8.2, LLaMA-2-70B 56.7% ±6.2. Sound (logically valid) reasoning traces: Zephyr 25.0% ±10.5, Mistral 25.0% ±7.5, LLaMA-2-7B 0.0%, LLaMA-2-13B 15.0% ±6.2, LLaMA-2-70B 31.7% ±9.7. Note: these are overall soundness rates, not per-method accuracy breakdowns in numeric form in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper compares frequency and soundness of strategies across models: Zephyr-7B-β heavily favors supposition following and more often uses the symbolic strategy; Mistral-7B-Instruct frequently uses compound conclusions and concatenation relatively more; LLaMA-2 models (especially larger sizes) favor chain construction and supposition following. Larger LLaMA-2 models generate more strategy instances and higher fractions of sound reasoning (scale effect). Alignment fine-tuning (Zephyr AIF) shifts strategy preference (towards supposition and symbolic). The paper reports moderate positive correlation between answer accuracy and reasoning soundness (Pearson r=0.45, p~1.6e-16), indicating many correct answers arise from unsound traces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) LLMs exhibit multiple distinct inferential strategies analogous to human strategies (supposition following, chain construction, compound, concatenation), and an additional symbolic strategy (truth-tables/formal logic) sometimes used by models. 2) Strategy usage is model-family dependent: different families favor different strategies. 3) Model scale and fine-tuning affect both strategy frequency and soundness (larger LLaMA-2 shows more sound reasoning). 4) Correct final answers do not reliably indicate sound reasoning; models often produce correct answers via flawed reasoning (and occasionally incorrect answers despite sound trace).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite the use of diverse strategies, overall performance is poor relative to humans; LLaMA-2-7B produced zero sound reasoning traces. Models commonly fail on negation handling, exclusive-or reasoning, and commit denial-of-antecedent/inverse fallacies; symbolic attempts (truth tables) sometimes contained errors and did not guarantee better outcomes (e.g., Zephyr uses symbolic strategy but soundness remains ~25%). Diverse methods did not uniformly outperform each other — effectiveness depends on model family and when the method is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3068.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3068.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 Chat 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large open-weight chat model from the LLaMA-2 family evaluated for propositional reasoning; shows the highest sound-reasoning rate among evaluated models and favors chain construction and supposition following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat variant of Meta's LLaMA-2 family (70B parameters); used as a publicly available model weight, prompted with system message to 'think aloud' and zero-shot chain-of-thought phrase; inference with nucleus sampling (top-p=0.9, T=0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain construction', 'supposition following', 'symbolic strategy (occasional)', 'compound strategy (rare)', 'concatenation (very rare)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Primarily constructs chains of conditionals linking antecedent to consequent (chain construction), and often uses supposition following (assume presence/absence and follow consequences). Occasionally attempts symbolic manipulations (formal logic, truth tables). Elicited via zero-shot 'Let's think step by step' prompting and full verbalized reasoning was annotated manually.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses multiple distinct strategies (diverse), with a strong tendency toward chain construction and supposition following; symbolic strategy used less frequently but present. Strategy diversity determined via manual labeling of 60 responses (5 seeds × 12 problems).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>12-problem propositional logic benchmark (Van der Henst et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same 12 neutral-content propositional problems assessing whether a conclusion follows from premises (3–4 premises, mix of iff, xor, and conditionals).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Strategy frequencies (Table 1): Supposition following 45.0% (±42.3 in parentheses were converted values in paper), Chain construction 50.0% (±46.8), Compound conclusion 3.3%, Concatenation 1.7%, Symbolic 6.7%. Overall final-answer accuracy 56.7% ±6.2; sound reasoning traces 31.7% ±9.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>LLaMA-2-70B outperforms smaller siblings in producing sound reasoning traces (scale effect). It shows higher incidence of chain construction which tends to produce more sound traces for this model compared to its peers; however, its symbolic traces are relatively less successful (paper notes LLaMA-2-70B struggles with symbolic strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLaMA-2-70B produced the highest measured rate of logically sound reasoning (≈31.7%), tended to use chain construction and supposition following, and scaled better than 13B/7B variants in both frequency of strategy usage and soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with the best soundness rate among evaluated models, LLaMA-2-70B is still often unsound; it also makes common logical errors (negation misinterpretation, overlooking exclusive disjunction constraints) and sometimes reaches correct final answers through invalid chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3068.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3068.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 Chat 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mid-size LLaMA-2 chat model (13B) evaluated on propositional logic tasks; uses chain construction notably but with lower soundness than 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter chat model from LLaMA-2 family used with the same zero-shot chain-of-thought prompt and sampling settings; responses manually annotated for strategy and soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain construction', 'supposition following', 'compound strategy (rare)', 'concatenation (occasional)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Often constructs conditional chains linking premises; also uses supposition following. Less use of symbolic strategy reported. Methods elicited via 'think step by step' prompting with manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses several strategies but shows a relative preference for chain construction compared with compound/concatenation; thus similar-style preference within family but still multiple methods observed.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>12-problem propositional logic benchmark (Van der Henst et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See aggregate description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 1 frequencies: Supposition following 28.3%, Chain construction 36.7%, Compound 6.7%, Concatenation 6.7%, Symbolic 0.0%. Overall final-answer accuracy 40.0% ±8.2; sound reasoning traces 15.0% ±6.2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to 70B, 13B produces fewer sound traces and fewer symbolic attempts; chain construction is common but less reliably sound than in 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>13B often uses chain construction but achieves lower soundness (≈15%); exhibits common logical errors similar to other models (negation handling, fallacies).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Chain construction by 13B sometimes omits negations or misapplies exclusive-or, producing invalid chains even when final answer happens to be correct.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3068.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3068.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 Chat 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small LLaMA-2 chat model (7B) evaluated on the propositional tasks; exhibits low soundness (no sound reasoning traces observed) and limited strategy diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter chat model from LLaMA-2 family; same prompting and sampling as other models; manual annotation shows limited production of sound inferential traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['supposition following', 'chain construction', 'concatenation (occasional)', 'symbolic strategy (very rare)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Primarily produces supposition-following and chain-like reasoning, but responses often contain invalid inferences. Prompted with zero-shot chain-of-thought; annotated for strategy and soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses multiple strategies superficially but with low fidelity; effectively similar styles with low reliability (no sound reasoning observed in annotated sample).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>12-problem propositional logic benchmark (Van der Henst et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See aggregate description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 1 frequencies: Supposition following 20.0%, Chain construction 20.0%, Compound 6.7%, Concatenation 3.3%, Symbolic 1.7%. Overall final-answer accuracy 46.7% ±6.7; sound reasoning traces 0.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Although 7B exhibits a mix of strategies, none of the annotated reasoning traces were judged logically sound; accuracy is near chance, showing correct outputs often arise from unsound reasoning or chance selection.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model size limitation corresponds to near-zero measured logical soundness despite producing plausible-looking rationales; demonstrates that strategy diversity without fidelity is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite employing strategies similar to humans, LLaMA-2-7B failed to produce any sound reasoning traces in this evaluation, highlighting that mere adoption of strategy-style text does not imply correct internal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3068.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3068.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned 7B model (Mistral-7B-Instruct) evaluated on propositional logic; tends to produce compound conclusions and concatenation and achieves moderate correctness but modest soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-access Mistral-7B model fine-tuned for instruction following (Instruct v0.2); prompted with system message and zero-shot chain-of-thought; manual annotation of reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['compound strategy', 'supposition following', 'chain construction', 'concatenation', 'symbolic strategy (occasional)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Frequently derives compound conclusions from combinations of premises and sometimes concatenates intermediate deductions into single statements; also engages in supposition following and chain construction. Sometimes attempts formal symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Highly diverse in the sense it more evenly uses multiple strategies (not dominated by a single style), notably more compound strategy use than LLaMA-2 models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>12-problem propositional logic benchmark (Van der Henst et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See aggregate description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 1 frequencies: Supposition following 35.0%, Chain construction 10.0%, Compound conclusion 35.0%, Concatenation 3.3%, Symbolic 8.3%. Overall final-answer accuracy 55.0% ±10.0; sound reasoning traces 25.0% ±7.5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Mistral-7B-Instruct shows higher use of compound strategy and tends to reason correctly when using chain, compound, or symbolic strategies, but often fails with supposition following. Compound reasoning correlates with several sound traces but does not guarantee soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction-tuning leads Mistral to produce more compound conclusions and balanced mixing of strategies; it attains moderate final-answer accuracy (~55%) but only ~25% sound reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Mistral's compound strategy sometimes yields sound intermediate deductions but still results in incorrect final answers (examples exist), and supposition following often fails due to incomplete exploration of consequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3068.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3068.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zephyr-7B-β</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zephyr 7B beta (AIF fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zephyr-7B-β (Mistral-based) is fine-tuned with AI Feedback (AIF) for alignment; the model strongly favors supposition following and exhibits relatively frequent symbolic strategy attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Zephyr-7B-β</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zephyr is a Mistral-derived model fine-tuned with AI feedback to improve alignment; used as an open-weight chat model in the paper with the same prompting/sampling regimen and manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['supposition following', 'symbolic strategy', 'chain construction', 'compound conclusion (less frequent)', 'concatenation (rare)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zephyr frequently uses supposition following (assume a case and trace consequences) and more often than other models attempts symbolic calculations (formalization/truth-tables). Prompting identical to other models; analysis based on manual labels.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse in that it produces multiple types, but has a pronounced preference for supposition following and comparatively more symbolic attempts (alignment seems to alter strategy preference).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>12-problem propositional logic benchmark (Van der Henst et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See aggregate description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 1 frequencies: Supposition following 60.0%, Chain construction 18.3%, Compound 10.0%, Concatenation 1.7%, Symbolic 20.0%. Overall final-answer accuracy 45.0% ±15.5; sound reasoning traces 25.0% ±10.5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Zephyr's alignment fine-tuning (AIF) is associated with greater use of supposition following and symbolic methods compared to Mistral baseline; however, symbolic attempts do not necessarily yield higher soundness (Zephyr soundness ~25%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Alignment/fine-tuning shifts strategy preference (Zephyr favors supposition and symbolic approaches); symbolic strategy is present but error-prone, so alignment does not equate to more logically sound reasoning in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with more symbolic attempts, Zephyr's sound reasoning rate remains modest (~25%); symbolic approaches sometimes contain truth-table errors, and supposition following often misses consequences leading to invalid judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Strategies in sentential reasoning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 1)</em></li>
                <li>Language models show human-like content effects on reasoning tasks <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Faithful reasoning with large language models <em>(Rating: 1)</em></li>
                <li>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3068",
    "paper_id": "paper-267897735",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "LLMs (aggregate)",
            "name_full": "Large Language Models evaluated in this study (aggregate summary)",
            "brief_description": "Aggregate summary of behavior across evaluated open-access LLMs (LLaMA-2 family, Mistral-7B-Instruct, Zephyr-7B-β) on a 12-problem propositional logic benchmark; documents which reasoning strategies models produce, how prompting was done, and population-level performance/strategy differences.",
            "citation_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
            "mention_or_use": "use",
            "model_name": "Aggregate of LLaMA-2 (7B/13B/70B), Mistral-7B-Instruct, Zephyr-7B-β",
            "model_description": "Open-weight transformer LLMs spanning the LLaMA-2 family (chat variants), Mistral-7B-Instruct, and Zephyr-7B-β (Mistral-based with alignment fine-tuning). Evaluated with zero-shot chain-of-thought prompting (system message + 'Let's think step by step') and nucleus sampling (top-p=0.9, temperature=0.6), 5 random seeds, manual annotation of reasoning traces.",
            "model_size": "various (7B, 13B, 70B)",
            "reasoning_methods": [
                "supposition following",
                "chain construction",
                "compound strategy",
                "concatenation strategy",
                "symbolic strategy",
                "zero-shot chain-of-thought prompting (\"Let's think step by step\")"
            ],
            "reasoning_methods_description": "Models were prompted to 'think aloud' (zero-shot chain-of-thought). Observed strategies match human taxonomy from Van der Henst et al. (2002): supposition following (assume a case and trace consequences), chain construction (link conditionals into antecedent→consequent chains), compound (combine premises to derive new compound conditionals), concatenation (join intermediate deductions into one statement), and a symbolic strategy (translate to formal logic or truth tables). Symbolic traces include attempting truth tables or formal symbolic manipulations.",
            "diversity_of_methods": "diverse across and within models: models exhibited multiple distinct strategies (diverse styles) rather than a single uniform approach; however, each model family showed a preferred subset of strategies (i.e., similar styles within a family). Determination is based on manual annotation of 300 responses and relative strategy frequency per model.",
            "reasoning_task_name": "12-problem propositional logic benchmark (Van der Henst et al., 2002 style)",
            "reasoning_task_description": "Twelve neutral-content propositional reasoning/hypothesis-validation problems using colored-marble phrasing; 3–4 premises per problem (biconditionals, exclusive disjunctions, conditionals), half of conclusions valid, half invalid. Models must decide whether conclusion logically follows and verbalize reasoning.",
            "performance_by_method": "Aggregated across models: method usage frequencies vary by model (see per-model entries). Overall final-answer accuracies per model (mean ± std over 5 seeds): Zephyr-7B-β 45.0% ±15.5, Mistral-7B-Instruct 55.0% ±10.0, LLaMA-2-7B 46.7% ±6.7, LLaMA-2-13B 40.0% ±8.2, LLaMA-2-70B 56.7% ±6.2. Sound (logically valid) reasoning traces: Zephyr 25.0% ±10.5, Mistral 25.0% ±7.5, LLaMA-2-7B 0.0%, LLaMA-2-13B 15.0% ±6.2, LLaMA-2-70B 31.7% ±9.7. Note: these are overall soundness rates, not per-method accuracy breakdowns in numeric form in the paper.",
            "comparison_of_methods": "The paper compares frequency and soundness of strategies across models: Zephyr-7B-β heavily favors supposition following and more often uses the symbolic strategy; Mistral-7B-Instruct frequently uses compound conclusions and concatenation relatively more; LLaMA-2 models (especially larger sizes) favor chain construction and supposition following. Larger LLaMA-2 models generate more strategy instances and higher fractions of sound reasoning (scale effect). Alignment fine-tuning (Zephyr AIF) shifts strategy preference (towards supposition and symbolic). The paper reports moderate positive correlation between answer accuracy and reasoning soundness (Pearson r=0.45, p~1.6e-16), indicating many correct answers arise from unsound traces.",
            "key_findings": "1) LLMs exhibit multiple distinct inferential strategies analogous to human strategies (supposition following, chain construction, compound, concatenation), and an additional symbolic strategy (truth-tables/formal logic) sometimes used by models. 2) Strategy usage is model-family dependent: different families favor different strategies. 3) Model scale and fine-tuning affect both strategy frequency and soundness (larger LLaMA-2 shows more sound reasoning). 4) Correct final answers do not reliably indicate sound reasoning; models often produce correct answers via flawed reasoning (and occasionally incorrect answers despite sound trace).",
            "counter_examples_or_negative_results": "Despite the use of diverse strategies, overall performance is poor relative to humans; LLaMA-2-7B produced zero sound reasoning traces. Models commonly fail on negation handling, exclusive-or reasoning, and commit denial-of-antecedent/inverse fallacies; symbolic attempts (truth tables) sometimes contained errors and did not guarantee better outcomes (e.g., Zephyr uses symbolic strategy but soundness remains ~25%). Diverse methods did not uniformly outperform each other — effectiveness depends on model family and when the method is applied.",
            "uuid": "e3068.0",
            "source_info": {
                "paper_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-70B",
            "name_full": "LLaMA 2 Chat 70B",
            "brief_description": "Large open-weight chat model from the LLaMA-2 family evaluated for propositional reasoning; shows the highest sound-reasoning rate among evaluated models and favors chain construction and supposition following.",
            "citation_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_description": "Chat variant of Meta's LLaMA-2 family (70B parameters); used as a publicly available model weight, prompted with system message to 'think aloud' and zero-shot chain-of-thought phrase; inference with nucleus sampling (top-p=0.9, T=0.6).",
            "model_size": "70B",
            "reasoning_methods": [
                "chain construction",
                "supposition following",
                "symbolic strategy (occasional)",
                "compound strategy (rare)",
                "concatenation (very rare)"
            ],
            "reasoning_methods_description": "Primarily constructs chains of conditionals linking antecedent to consequent (chain construction), and often uses supposition following (assume presence/absence and follow consequences). Occasionally attempts symbolic manipulations (formal logic, truth tables). Elicited via zero-shot 'Let's think step by step' prompting and full verbalized reasoning was annotated manually.",
            "diversity_of_methods": "Uses multiple distinct strategies (diverse), with a strong tendency toward chain construction and supposition following; symbolic strategy used less frequently but present. Strategy diversity determined via manual labeling of 60 responses (5 seeds × 12 problems).",
            "reasoning_task_name": "12-problem propositional logic benchmark (Van der Henst et al. style)",
            "reasoning_task_description": "Same 12 neutral-content propositional problems assessing whether a conclusion follows from premises (3–4 premises, mix of iff, xor, and conditionals).",
            "performance_by_method": "Strategy frequencies (Table 1): Supposition following 45.0% (±42.3 in parentheses were converted values in paper), Chain construction 50.0% (±46.8), Compound conclusion 3.3%, Concatenation 1.7%, Symbolic 6.7%. Overall final-answer accuracy 56.7% ±6.2; sound reasoning traces 31.7% ±9.7.",
            "comparison_of_methods": "LLaMA-2-70B outperforms smaller siblings in producing sound reasoning traces (scale effect). It shows higher incidence of chain construction which tends to produce more sound traces for this model compared to its peers; however, its symbolic traces are relatively less successful (paper notes LLaMA-2-70B struggles with symbolic strategy).",
            "key_findings": "LLaMA-2-70B produced the highest measured rate of logically sound reasoning (≈31.7%), tended to use chain construction and supposition following, and scaled better than 13B/7B variants in both frequency of strategy usage and soundness.",
            "counter_examples_or_negative_results": "Even with the best soundness rate among evaluated models, LLaMA-2-70B is still often unsound; it also makes common logical errors (negation misinterpretation, overlooking exclusive disjunction constraints) and sometimes reaches correct final answers through invalid chains.",
            "uuid": "e3068.1",
            "source_info": {
                "paper_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-13B",
            "name_full": "LLaMA 2 Chat 13B",
            "brief_description": "Mid-size LLaMA-2 chat model (13B) evaluated on propositional logic tasks; uses chain construction notably but with lower soundness than 70B.",
            "citation_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B",
            "model_description": "13B-parameter chat model from LLaMA-2 family used with the same zero-shot chain-of-thought prompt and sampling settings; responses manually annotated for strategy and soundness.",
            "model_size": "13B",
            "reasoning_methods": [
                "chain construction",
                "supposition following",
                "compound strategy (rare)",
                "concatenation (occasional)"
            ],
            "reasoning_methods_description": "Often constructs conditional chains linking premises; also uses supposition following. Less use of symbolic strategy reported. Methods elicited via 'think step by step' prompting with manual annotation.",
            "diversity_of_methods": "Uses several strategies but shows a relative preference for chain construction compared with compound/concatenation; thus similar-style preference within family but still multiple methods observed.",
            "reasoning_task_name": "12-problem propositional logic benchmark (Van der Henst et al. style)",
            "reasoning_task_description": "See aggregate description.",
            "performance_by_method": "Table 1 frequencies: Supposition following 28.3%, Chain construction 36.7%, Compound 6.7%, Concatenation 6.7%, Symbolic 0.0%. Overall final-answer accuracy 40.0% ±8.2; sound reasoning traces 15.0% ±6.2.",
            "comparison_of_methods": "Compared to 70B, 13B produces fewer sound traces and fewer symbolic attempts; chain construction is common but less reliably sound than in 70B.",
            "key_findings": "13B often uses chain construction but achieves lower soundness (≈15%); exhibits common logical errors similar to other models (negation handling, fallacies).",
            "counter_examples_or_negative_results": "Chain construction by 13B sometimes omits negations or misapplies exclusive-or, producing invalid chains even when final answer happens to be correct.",
            "uuid": "e3068.2",
            "source_info": {
                "paper_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-7B",
            "name_full": "LLaMA 2 Chat 7B",
            "brief_description": "Small LLaMA-2 chat model (7B) evaluated on the propositional tasks; exhibits low soundness (no sound reasoning traces observed) and limited strategy diversity.",
            "citation_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B",
            "model_description": "7B-parameter chat model from LLaMA-2 family; same prompting and sampling as other models; manual annotation shows limited production of sound inferential traces.",
            "model_size": "7B",
            "reasoning_methods": [
                "supposition following",
                "chain construction",
                "concatenation (occasional)",
                "symbolic strategy (very rare)"
            ],
            "reasoning_methods_description": "Primarily produces supposition-following and chain-like reasoning, but responses often contain invalid inferences. Prompted with zero-shot chain-of-thought; annotated for strategy and soundness.",
            "diversity_of_methods": "Uses multiple strategies superficially but with low fidelity; effectively similar styles with low reliability (no sound reasoning observed in annotated sample).",
            "reasoning_task_name": "12-problem propositional logic benchmark (Van der Henst et al. style)",
            "reasoning_task_description": "See aggregate description.",
            "performance_by_method": "Table 1 frequencies: Supposition following 20.0%, Chain construction 20.0%, Compound 6.7%, Concatenation 3.3%, Symbolic 1.7%. Overall final-answer accuracy 46.7% ±6.7; sound reasoning traces 0.0%.",
            "comparison_of_methods": "Although 7B exhibits a mix of strategies, none of the annotated reasoning traces were judged logically sound; accuracy is near chance, showing correct outputs often arise from unsound reasoning or chance selection.",
            "key_findings": "Model size limitation corresponds to near-zero measured logical soundness despite producing plausible-looking rationales; demonstrates that strategy diversity without fidelity is insufficient.",
            "counter_examples_or_negative_results": "Despite employing strategies similar to humans, LLaMA-2-7B failed to produce any sound reasoning traces in this evaluation, highlighting that mere adoption of strategy-style text does not imply correct internal reasoning.",
            "uuid": "e3068.3",
            "source_info": {
                "paper_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral-7B-Instruct",
            "name_full": "Mistral 7B Instruct",
            "brief_description": "Instruction-tuned 7B model (Mistral-7B-Instruct) evaluated on propositional logic; tends to produce compound conclusions and concatenation and achieves moderate correctness but modest soundness.",
            "citation_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_description": "Open-access Mistral-7B model fine-tuned for instruction following (Instruct v0.2); prompted with system message and zero-shot chain-of-thought; manual annotation of reasoning traces.",
            "model_size": "7B",
            "reasoning_methods": [
                "compound strategy",
                "supposition following",
                "chain construction",
                "concatenation",
                "symbolic strategy (occasional)"
            ],
            "reasoning_methods_description": "Frequently derives compound conclusions from combinations of premises and sometimes concatenates intermediate deductions into single statements; also engages in supposition following and chain construction. Sometimes attempts formal symbolic reasoning.",
            "diversity_of_methods": "Highly diverse in the sense it more evenly uses multiple strategies (not dominated by a single style), notably more compound strategy use than LLaMA-2 models.",
            "reasoning_task_name": "12-problem propositional logic benchmark (Van der Henst et al. style)",
            "reasoning_task_description": "See aggregate description.",
            "performance_by_method": "Table 1 frequencies: Supposition following 35.0%, Chain construction 10.0%, Compound conclusion 35.0%, Concatenation 3.3%, Symbolic 8.3%. Overall final-answer accuracy 55.0% ±10.0; sound reasoning traces 25.0% ±7.5.",
            "comparison_of_methods": "Mistral-7B-Instruct shows higher use of compound strategy and tends to reason correctly when using chain, compound, or symbolic strategies, but often fails with supposition following. Compound reasoning correlates with several sound traces but does not guarantee soundness.",
            "key_findings": "Instruction-tuning leads Mistral to produce more compound conclusions and balanced mixing of strategies; it attains moderate final-answer accuracy (~55%) but only ~25% sound reasoning.",
            "counter_examples_or_negative_results": "Mistral's compound strategy sometimes yields sound intermediate deductions but still results in incorrect final answers (examples exist), and supposition following often fails due to incomplete exploration of consequences.",
            "uuid": "e3068.4",
            "source_info": {
                "paper_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Zephyr-7B-β",
            "name_full": "Zephyr 7B beta (AIF fine-tuned)",
            "brief_description": "Zephyr-7B-β (Mistral-based) is fine-tuned with AI Feedback (AIF) for alignment; the model strongly favors supposition following and exhibits relatively frequent symbolic strategy attempts.",
            "citation_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
            "mention_or_use": "use",
            "model_name": "Zephyr-7B-β",
            "model_description": "Zephyr is a Mistral-derived model fine-tuned with AI feedback to improve alignment; used as an open-weight chat model in the paper with the same prompting/sampling regimen and manual annotation.",
            "model_size": "7B",
            "reasoning_methods": [
                "supposition following",
                "symbolic strategy",
                "chain construction",
                "compound conclusion (less frequent)",
                "concatenation (rare)"
            ],
            "reasoning_methods_description": "Zephyr frequently uses supposition following (assume a case and trace consequences) and more often than other models attempts symbolic calculations (formalization/truth-tables). Prompting identical to other models; analysis based on manual labels.",
            "diversity_of_methods": "Diverse in that it produces multiple types, but has a pronounced preference for supposition following and comparatively more symbolic attempts (alignment seems to alter strategy preference).",
            "reasoning_task_name": "12-problem propositional logic benchmark (Van der Henst et al. style)",
            "reasoning_task_description": "See aggregate description.",
            "performance_by_method": "Table 1 frequencies: Supposition following 60.0%, Chain construction 18.3%, Compound 10.0%, Concatenation 1.7%, Symbolic 20.0%. Overall final-answer accuracy 45.0% ±15.5; sound reasoning traces 25.0% ±10.5.",
            "comparison_of_methods": "Zephyr's alignment fine-tuning (AIF) is associated with greater use of supposition following and symbolic methods compared to Mistral baseline; however, symbolic attempts do not necessarily yield higher soundness (Zephyr soundness ~25%).",
            "key_findings": "Alignment/fine-tuning shifts strategy preference (Zephyr favors supposition and symbolic approaches); symbolic strategy is present but error-prone, so alignment does not equate to more logically sound reasoning in this task.",
            "counter_examples_or_negative_results": "Even with more symbolic attempts, Zephyr's sound reasoning rate remains modest (~25%); symbolic approaches sometimes contain truth-table errors, and supposition following often misses consequences leading to invalid judgments.",
            "uuid": "e3068.5",
            "source_info": {
                "paper_title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Strategies in sentential reasoning",
            "rating": 2,
            "sanitized_title": "strategies_in_sentential_reasoning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 1,
            "sanitized_title": "faithful_chainofthought_reasoning"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning tasks",
            "rating": 1,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning_tasks"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Faithful reasoning with large language models",
            "rating": 1,
            "sanitized_title": "faithful_reasoning_with_large_language_models"
        },
        {
            "paper_title": "Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting",
            "rating": 1,
            "sanitized_title": "language_models_dont_always_say_what_they_think_unfaithful_explanations_in_chainofthought_prompting"
        }
    ],
    "cost": 0.019326,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning
3 Jun 2024</p>
<p>Philipp Mondorf p.mondorf@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>Barbara Plank b.plank@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>Josh Openai 
Steven Achiam 
Sandhini Adler 
Agarwal 
Ing Bao 
Mo Bavarian 
Jeff Belgum 
Irwan Bello 
Jake Berdine 
Gabriel Bernadett-Shapiro 
Christo- Pher Berner 
Lenny Bogdonoff 
Oleg Boiko 
Made- Laine Boyd 
Anna-Luisa Brakman 
Greg Brockman 
Tim Brooks 
Miles Brundage 
Kevin Button 
Trevor Cai 
Rosie Campbell 
Andrew Cann 
Brittany Carey 
Chelsea Carlson 
Rory Carmichael 
Brooke Chan 
Che Chang 
Fotis Chantzis 
Derek Chen 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Ben Chess 
Tarun Goel 
Gabriel Gogineni 
Rapha Goh 
Jonathan Gontijo- Lopes 
Morgan Gordon 
Scott Grafstein 
Ryan Gray 
Joshua Greene 
ShixiangShane Gross 
Yufei Gu 
Chris Guo 
Jesse Hallacy 
Jeff Han 
Yuchen Harris 
Mike He 
Johannes Heaton 
Chris Heidecke 
Alan Hesse 
Wade Hickey 
Peter Hickey 
Brandon Hoeschele 
Kenny Houghton 
Shengli Hsu 
Xin Hu 
Joost Hu 
Shantanu Huizinga 
Shawn Jain 
Joanne Jain 
Angela Jang 
Roger Jiang 
Haozhun Jiang 
Denny Jin 
Shino Jin 
Billie Jomoto 
Hee- Woo Jonn 
Tomer Jun 
Łukasz Kaftan 
Ali Kaiser 
Ingmar Ka- Mali 
Kanitscheider 
Shirish Nitish 
Tabarak Keskar 
Logan Khan 
Jong Wook Kilpatrick 
Christina Kim 
Yongjik Kim 
Hendrik Kim 
Jamie Kirchner 
Matt Kiros 
Daniel Knight 
Łukasz Kokotajlo 
Andrew Kondraciuk 
Aris Kondrich 
Kyle Konstantini- Dis 
Gretchen Kosic 
Vishal Krueger 
Michael Kuo 
Ikai Lampe 
Teddy Lan 
Jan Lee 
Jade Leike 
Daniel Leung 
ChakMing Levy 
Rachel Li 
Molly Lim 
Stephanie Lin 
Mateusz Lin 
Theresa Litwin 
Ryan Lopez 
Patricia Lowe 
Anna Lue 
Kim Makanju 
Sam Malfacini 
Todor Manning 
Yaniv Markov 
Bianca Markovski 
Katie Martin 
Andrew Mayer 
Bob Mayne 
Scott Mayer Mcgrew 
Christine Mckinney 
Paul Mcleavey 
Jake Mcmillan 
David Mcneil 
Aalok Medina 
Jacob Mehta 
Luke Menick 
Andrey Metz 
Pamela Mishchenko 
Vinnie Mishkin 
Evan Monaco 
Daniel Morikawa 
Tong Mossing 
Mira Mu 
Oleg Murati 
David Murk 
Ashvin Mély 
Reiichiro Nair 
Rajeev Nakano 
Arvind Nayak 
Richard Neelakan- Tan 
Hyeonwoo Ngo 
Long Noh 
Cullen Ouyang 
Jakub O'keefe 
Alex Pachocki 
Joe Paino 
Ashley Palermo 
Giambattista Pantuliano 
Joel Paras- Candolo 
Emy Parish 
Alex Parparita 
Mikhail Passos 
Andrew Pavlov 
Adam Peng 
Fil- Ipe Perelman 
Belbute De Avila 
Michael Peres 
Henrique Petrov 
Ponde 
MichaelOliveira Pinto 
PokornyMichelle Pokrass 
Vitchyr Pong 
Tolly Powell 
Alethea Power 
Boris Power 
Elizabeth Proehl 
Raul Puri 
Alec Radford 
Jack W Rae 
Aditya Ramesh 
Cameron Ray- Mond 
Francis Real 
Kendra Rimbach 
Carl Ross 
Bob Rotsted 
Henri Roussez 
Nick Ryder 
Mario Saltarelli 
Ted Sanders 
Shibani Santurkar 
Girish Sastry 
Heather Schmidt 
David Schnurr 
John Schul- Man 
Daniel Selsam 
Kyla Sheppard 
Toki Sherbakov 
Jessica Shieh 
Sarah Shoker 
Pranav Shyam 
Szymon Sidor 
Eric Sigler 
Maddie Simens 
Jordan Sitkin 
Katarina Slama 
Ian Sohl 
Benjamin Sokolowsky 
Jonathan Wang 
Jason Ward 
C J Wei 
Akila Weinmann 
Peter Welihinda 
Jiayi Welinder 
Lilian Weng 
Matt Weng 
Dave Wiethoff 
Willner 
Win- Ter Clemens 
Samuel Wolrich 
Hannah Wong 
Sherwin Wu 
Jeff Wu 
Michael Wu 
Kai Xiao 
Tao Xu 
Sarah Yoo 
Kevin Yu 
Qiming Yuan 
Woj- Ciech Zaremba 
Rowan Zellers 
Chong Zhang 
Mar- Vin Zhang 
Shengjia Zhao 
Tianhao Zheng 
Juntang Zhuang 
William Zhuk 
Barret 2023 Zoph 
Gpt- 
Long Ouyang 
Jeffrey Wu 
Xu Jiang 
Diogo Almeida 
Carroll Wainwright 
Pamela Mishkin 
Sandhini Agarwal 
Alex Ray 
John Schulman 
Jacob Hilton 
Fraser Kelton 
Luke Miller 
Amanda Askell 
Peter Welinder 
Paul F Christiano 
Jan Leike 
Ryan 2022 Lowe 
Ansh Radhakrishnan 
Karina Nguyen 
Anna Chen 
Carol Chen 
Carson Denison 
Danny Hernandez 
Esin Durmus 
Evan Hubinger 
Jackson Kernion 
Kamilė Lukošiūtė 
Newton Cheng 
Nicholas Joseph 
Nicholas Schiefer 
Oliver Rausch 
SheerSam Mccandlish 
El Showk 
Tamera Lanham 
Tim Maxwell 
Venkatesa Chandrasekaran 
Zac Hatfield-Dodds 
Jared Kaplan 
Jan Brauner 
Samuel R Bowman 
Ethan 2023 Perez 
Melvin Johnson 
Ioannis Antonoglou 
Julian Schrit- Twieser 
Amelia Glaese 
Jilin Chen 
Emily Pitler 
Timothy Lillicrap 
Angeliki Lazaridou 
Orhan Fi- Rat 
James Molloy 
Michael Isard 
Paul R Barham 
Kareem Ayoub 
Megha Goel 
George Tucker 
En- Rique Piqueras 
Maxim Krikun 
Iain Barr 
Nikolay Savinov 
Ivo Danihelka 
Becca Roelofs 
Anaïs White 
Anders Andreassen 
Tamara Von Glehn 
Lakshman Yagati 
Mehran Kazemi 
Lucas Gonzalez 
Misha Khalman 
Jakub Sygnowski 
Alexandre Frechette 
Charlotte Smith 
Laura Culp 
Lev Proleev 
Yi Luan 
Xianghai Chen 
James Lottes 
Nathan Schucher 
Federico Lebron 
Alban Rrustemi 
Natalie Clay 
Phil Crone 
Tomas Kocisky 
Jeffrey Zhao 
Bartek Perz 
Dian Yu 
Heidi Howard 
Adam Bloniarz 
Han Lu 
Laurent Sifre 
Marcello Maggioni 
Fred Alcober 
Dan Garrette 
Megan Barnes 
Shantanu Thakoor 
Ja- Cob Austin 
Gabriel Barth-Maron 
William Wong 
Rishabh Joshi 
Rahma Chaabouni 
Deeni Fatiha 
Arun Ahuja 
Ruibo Liu 
Yunxuan Li 
Sarah Cogan 
Jeremy Chen 
Chao Jia 
Chenjie Gu 
Qiao Zhang 
AleJor- Dan Grimstad 
Jakse Hartman 
GauravMartin Chadwick 
Singh Tomar 
Xavier Garcia 
Evan Senter 
Emanuel Taropa 
Thanumalayan Sankaranarayana Pillai 
Jacob Devlin 
Michael Laskin 
Diego De 
Las Casas 
Dasha Valter 
Connie Tao 
Lorenzo Blanco 
Adrià Puigdomènech Badia 
David Reitter 
Mianna Chen 
Jenny Brennan 
Clara Rivera 
Sergey Brin 
Shariq Iqbal 
Gabriela Surita 
Jane Labanowski 
Abhi Rao 
Stephanie Winkler 
Emilio Parisotto 
Yiming Gu 
Kate Olszewska 
Yujing Zhang 
Ravi Addanki 
Antoine Miech 
Annie Louis 
Laurent El 
De- Nis Teplyashin 
Geoff Brown 
Elliot Catt 
Nithya At- Taluri 
Jan Balaguer 
Jackie Xiang 
Pidong Wang 
Zoe Ashwood 
Anton Briukhov 
Albert Webson 
San- Jay Ganapathy 
Smit Sanghavi 
Ajay Kannan 
Ming- Wei Chang 
Axel Stjerngren 
Josip Djolonga 
Yut- Ing Sun 
Ankur Bapna 
Matthew Aitchison 
Pedram Pejman 
Henryk Michalewski 
Tianhe Yu 
Cindy Wang 
Juliette Love 
Junwhan Ahn 
Dawn Bloxwich 
Kehang Han 
Peter Humphreys 
Thibault Sellam 
James Bradbury 
Varun Godbole 
Sina Samangooei 
Bogdan Damoc 
Alex Kaskasoli 
Sébastien M R Arnold 
Vijay Vasudevan 
Shubham Agrawal 
Jason Riesa 
Dmitry Lepikhin 
Richard Tanburn 
Srivat- San Srinivasan 
Hyeontaek Lim 
Sarah Hodkinson 
Johan Ferret 
Steven Hand 
Ankush Garg 
Tom Le Paine 
Jian Li 
Yujia Li 
Minh Gi- Ang 
Alexander Neitz 
Zaheer Abbas 
Sarah York 
Machel Reid 
Elizabeth Cole 
Aakanksha Chowd- Hery 
Dipanjan Das 
Dominika Rogozińska 
Vitaly Nikolaev 
Pablo Sprechmann 
Zachary Nado 
Lukas Zilka 
Flavien Prost 
Luheng He 
Marianne Mon- Teiro 
Gaurav Mishra 
Chris Welty 
Josh Newlan 
Dawei Jia 
Miltiadis Allamanis 
Clara Huiyi Hu 
Raoul De Liedekerke 
Justin Gilmer 
Carl Saroufim 
Shruti Rijhwani 
Shaobo Hou 
Disha Shrivastava 
Anirudh Baddepudi 
Alex Goldin 
Adnan Ozturel 
Albin Cassirer 
Yunhan Xu 
Daniel Sohn 
Deven- Dra Sachan 
Reinald Kim Amplayo 
Craig Swan- Son 
Dessie Petrova 
Shashi Narayan 
Arthur Guez 
Siddhartha Brahma 
Jessica Landon 
Miteyan Patel 
Ruizhe Zhao 
Kevin Villela 
Luyu Wang 
Wenhao Jia 
Matthew Rahtz 
Mai Giménez 
Legg Yeung 
Hanzhao Lin 
James Keeling 
Petko Georgiev 
Di- Ana Mincu 
Boxi Wu 
Salem Haykal 
Rachel Sapu- Tro 
Kiran Vodrahalli 
James Qin 
Zeynep Cankara 
Abhanshu Sharma 
Nick Fernando 
Will Hawkins 
Behnam Neyshabur 
Solomon Kim 
Adrian Hut- Ter 
Priyanka Agrawal 
Alex Castro-Ros 
George Van Den Driessche 
Tao Wang 
Fan Yang 
Shuo- Yiin Chang 
Paul Komarek 
Ross Mcilroy 
Mario Lučić 
Guodong Zhang 
Wael Farhan 
Michael Shar- Man 
Paul Natsev 
Paul Michel 
Yong Cheng 
Yamini Bansal 
Siyuan Qiao 
Kris Cao 
Siamak Shakeri 
Christina Butterfield 
Justin Chung 
Paul Kishan Rubenstein 
Shivani Agrawal 
Arthur Mensch 
Kedar Soparkar 
Karel Lenc 
Timothy Chung 
Aedan Pope 
Loren Maggiore 
Jackie Kay 
Priya Jhakra 
Shibo Wang 
Joshua Maynez 
Mary Phuong 
Taylor Tobin 
Andrea Tacchetti 
Maja Trebacz 
Kevin Robinson 
Yash Katariya 
Sebastian Riedel 
Paige Bailey 
Ke- Fan Xiao 
Nimesh Ghelani 
Lora Aroyo 
Ambrose Slone 
Neil Houlsby 
Xuehan Xiong 
Zhen Yang 
Elena Gribovskaya 
Jonas Adler 
Mateo Wirth 
Music LiLisa Lee 
Thais Kagohara 
Jay Pavagadhi 
So- Phie Bridgers 
Anna Bortsova 
Sanjay Ghemawat 
Zafarali Ahmed 
Tianqi Liu 
Richard Powell 
Vijay Bolina 
Mariko Iinuma 
Polina Zablotskaia 
James Besley 
Da-Woon Chung 
Timothy Dozat 
Ramona Comanescu 
Xiance Si 
Jeremy Greer 
Guolong Su 
Martin Polacek 
Raphaël Lopez Kaufman 
Simon Tokumine 
Hexiang Hu 
Elena Buchatskaya 
Yingjie Miao 
Mohamed Elhawaty 
Aditya Siddhant 
Nenad Tomasev 
Jinwei Xing 
Christina Greer 
Helen Miller 
Shereen Ashraf 
Aurko Roy 
Zizhao Zhang 
Ada Ma 
Angelos Filos 
Milos Besta 
Rory Blevins 
Ted Kli- Menko 
Chih-Kuan Yeh 
Soravit Changpinyo 
Jiaqi Mu 
Oscar Chang 
Mantas Pajarskas 
Carrie Muir 
Vered Cohen 
Charline Le Lan 
Krishna Haridasan 
Amit Marathe 
Steven Hansen 
Sholto Douglas 
Ra- Jkumar Samuel 
Mingqiu Wang 
Sophia Austin 
Chang Lan 
Jiepu Jiang 
Justin Chiu 
Jaime Alonso Lorenzo 
Lars Lowe Sjösund 
Sébastien Cevey 
Zach Gleicher 
Thi Avrahami 
Anudhyan Boral 
Hansa Srinivasan 
Vittorio Selo 
Rhys May 
Kon- Stantinos Aisopos 
Léonard Hussenot 
Baldini Livio 
Kate Soares 
Michael B Baumli 
AdriàRe- Casens Chang 
Ben Caine 
Alexander Pritzel 
Filip Pavetic 
Fabio Pardo 
Anita Gergely 
Justin Frye 
Vinay Ramasesh 
Dan Horgan 
Kartikeya Badola 
Nora Kassner 
Subhrajit Roy 
VíctorEthan Dyer 
Alex Tomala 
Yunhao Tang 
Dalia El Badawy 
Elspeth White 
Basil Mustafa 
Oran Lang 
Ab- Hishek Jindal 
Sharad Vikram 
Zhitao Gong 
Sergi Caelles 
Ross Hemsley 
Gregory Thornton 
Fangxi- Aoyu Feng 
Wojciech Stokowiec 
Ce Zheng 
Phoebe Thacker 
Çaglar Ünlü 
Zhishuai Zhang 
Moham- Mad Saleh 
James Svensson 
Max Bileschi 
Piyush Patil 
Ankesh Anand 
Roman Ring 
Katerina Tsihlas 
Arpi Vezer 
Marco Selvi 
Toby Shevlane 
Mikel Ro- Driguez 
Tom Kwiatkowski 
Samira Daruki 
Keran Rong 
Allan Dafoe 
Nicholas Fitzgerald 
Keren Gu-Lemberg 
Mina Khan 
Lisa Anne Hendricks 
Marie Pellat 
Vladimir Feinberg 
James Cobon- Kerr 
Tara Sainath 
Maribeth Rauh 
Hadi Sayed 
Richard Hashemi 
Yana Ives 
Yaguang Hasson 
Eric Li 
Yuan Noland 
Nathan Cao 
Le Byrd 
Qingze Hou 
Thibault Wang 
Michela Sottiaux 
Jean-Baptiste Paganini 
Alexandre Lespiau 
Samer Moufarek 
Kaushik Hassan 
Joost Shivakumar 
Amol Van Amers- Foort 
Pratik Mandhane 
Anirudh Joshi 
Matthew Goyal 
Andrew Tung 
Hannah Brock 
Vedant Shea- Han 
Cheng Misra 
Nemanja Li 
Mostafa Rakićević 
Fangyu Dehghani 
Sid Liu 
Junhyuk Mittal 
Seb Oh 
Eren Noury 
Fantine Sezener 
Matthew Huot 
Nicola Lamm 
Charlie De Cao 
Gamaleldin Chen 
Ed Elsayed 
Mahdis Chi 
Ian Mahdieh 
Nan Tenney 
Ivan Hua 
Patrick Petrychenko 
Dylan Kane 
Rishub Scand- Inaro 
Jonathan Jain 
Romina Uesato 
Adam Datta 
Oskar Sadovsky 
Dominik Bunyan 
Shimu Rabiej 
John Wu 
Gautam Zhang 
Edouard Vasudevan 
Mahmoud Leurent 
Ionut Alnahlawi 
Nan Georgescu 
Ivy Wei 
Betty Zheng 
Pam G Chan 
Piotr Rabinovitch 
Ye Stanczyk 
David Zhang 
Subhajit Steiner 
Michael Naskar 
Matthew Azzam 
Adam Johnson 
Chung-Cheng Paszke 
Jaume Chiu 
Elias Sanchez 
Afroz Mohiuddin 
Faizan Muhammad 
Jin Miao 
Andrew Lee 
Nino Vieillard 
Sahitya Potluri 
Jane Park 
Elnaz Davoodi 
Jiageng Zhang 
Jeff Stanway 
Drew Garmon 
Abhijit Karmarkar 
Zhe Dong 
Jong Lee 
Aviral Kumar 
Luowei Zhou 
Jonathan Evens 
William Isaac 
Zhe Chen 
Johnson Jia 
Anselm Levskaya 
Zhenkai Zhu 
Chris Gorgolewski 
Peter Grabowski 
Yuning Mao 
Alberto Magni 
Kaisheng Yao 
Javier Snaider 
Norman Casagrande 
Paul Sugan- Than 
Evan Palmer 
Geoffrey Irving 
Edward Loper 
Manaal Faruqui 
Isha Arkatkar 
Nanxin Chen 
Izhak Shafran 
Michael Fink 
Alfonso Castaño 
Irene Gian- Noumis 
Wooyeol Kim 
Mikołaj Rybiński 
Ashwin Sreevatsa 
Jennifer Prendki 
David Soergel 
Adrian Goedeckemeyer 
Willi Gierke 
Mohsen Jafari 
Meenu Gaba 
Jeremy Wiesner 
Diana Gage 
Yawen Wei 
Harsha Vashisht 
Yana Kulizhskaya 
Jay Hoover 
Maigo Le 
Lu Li 
Chimezie Iwuanyanwu 
Lu Liu 
Kevin Ramirez 
Andrey Khorlin 
Albert Cui 
Tian Lin 
Marin Georgiev 
Marcus Wu 
Ricardo Aguilar 
Keith Pallo 
Abhishek Chakladar 
Alena Repina 
Xi- Hui Wu 
Tom Van Der Weide 
Priya Ponnapalli 
Car- Oline Kaplan 
Jiri Simsa 
Shuangfeng Li 
Olivier Dousse 
Jeff Piper 
Nathan Ie 
Minnie Lui 
Rama Pasumarthi 
Nathan Lintz 
Vi- Jayakumar Anitha 
Lam Nguyen Thiet 
Daniel Andor 
Pedro Valenzuela 
Cosmin Paduraru 
Daiyi Peng 
Kather- Ine Lee 
Shuyuan Zhang 
DucSomer Greene 
Dung Nguyen 
Paula Kurylowicz 
Sarmishta Velury 
Se- Bastian Krause 
Cassidy Hardin 
Lucas Dixon 
Lili Janzer 
Kiam Choo 
Ziqiang Feng 
Biao Zhang 
Achintya Singhal 
Tejasi Latkar 
Mingyang Zhang 
Quoc Le 
Elena Allica Abellan 
Dayou Du 
Dan Mck- Innon 
Natasha Antropova 
Tolga Bolukbasi 
Orgad Keller 
David Reid 
Daniel Finchelstein 
Maria Abi Raad 
Remi Crocker 
Peter Hawkins 
Robert Dadashi 
Colin Gaffney 
Sid Lall 
Ken Franko 
Egor Filonov 
Anna Bulanova 
Rémi Leblond 
Vikas Yadav 
Shirley Chung 
Harry Askham 
Luis C Cobo 
Kelvin Xu 
Felix Fischer 
Jun Xu 
Christina Sorokin 
Chris Al- Berti 
Chu-Cheng Lin 
Colin Evans 
Hao Zhou 
Alek Dimitriev 
Hannah Forbes 
Dylan Banarse 
Zora Tung 
Jeremiah Liu 
Mark Omernick 
Colton Bishop 
Chintu Kumar 
Rachel Sterneck 
Ryan Foley 
Rohan Jain 
Swaroop Mishra 
Jiawei Xia 
Taylor Bos 
Ge- Offrey Cideron 
Ehsan Amid 
Francesco Piccinno 
Xingyu Wang 
Praseem Banzal 
Petru Gurita 
Hila Noga 
Premal Shah 
Daniel J Mankowitz 
Alex Polozov 
Nate Kushman 
Victoria Krakovna 
Sasha Brown 
Mohammadhossein Bateni 
Dennis Duan 
Vlad Firoiu 
Meghana Thotakuri 
Tom Natan 
An- Had Mohananey 
Matthieu Geist 
Sidharth Mudgal 
Sertan Girgin 
Hui Li 
Jiayu Ye 
Ofir Roval 
Reiko Tojo 
Michael Kwong 
James Lee-Thorp 
Christo- Pher Yew 
Quan Yuan 
Sumit Bagri 
Danila Sinopal- Nikov 
Sabela Ramos 
John Mellor 
Abhishek Sharma 
Aliaksei Severyn 
Jonathan Lai 
Kathy Wu 
Heng- Tze Cheng 
David Miller 
Nicolas Sonnerat 
Denis Vnukov 
Rory Greig 
Jennifer Beattie 
Emily Cave- Ness 
Libin Bai 
Julian Eisenschlos 
Alex Korchem- Niy 
Tomy Tsai 
Mimi Jasarevic 
Weize Kong 
Phuong Dao 
Zeyu Zheng 
Frederick Liu 
Rui Zhu 
TianMark Geller 
Huey Teh 
Jason Sanmiya 
Evgeny Gladchenko 
Nejc Trdin 
Andrei Sozanschi 
Daniel Toyama 
Evan Rosen 
Sasan Tavakkol 
Lint- Ing Xue 
Chen Elkind 
Oliver Woodman 
John Car- Penter 
George Papamakarios 
Rupert Kemp 
Sushant Kafle 
Tanya Grunina 
Rishika Sinha 
Alice Tal- Bert 
Abhimanyu Goyal 
Diane Wu 
Denese Owusu- Afriyie 
Cosmo Du 
Chloe Thornton 
Jordi Pont- Tuset 
Pradyumna Narayana 
Jing Li 
Sabaer Fatehi 
John Wieting 
Omar Ajmeri 
Benigno Uria 
Tao Zhu 
Kyle Levin 
Raphael Hoffmann 
Dan Holtmann- Rice 
Olivier Bachem 
Summer Yue 
Sho Arora 
Eric Malmi 
Daniil Mirylenka 
Qijun Tan 
Christy Koh 
Hassas Soheil 
Siim Yeganeh 
Steven Põder 
Francesco Zheng 
Mukarram Pongetti 
Yan- Hua Tariq 
Lucian Sun 
Mojtaba Ionita 
Pouya Seyedhosseini 
Ragha Tafti 
Zhiyu Kotikalapudi 
An- Mol Liu 
Jasmine Gulati 
Xinyu Liu 
Bart Ye 
Lily Chrzaszcz 
Nikhil Wang 
Tianrun Sethi 
Ben Li 
Shreya Brown 
Wei Singh 
Aaron Fan 
Joe Parisi 
Chenkai Stanton 
Vinod Kuang 
Christopher A Koverkathu 
Yunjie Choquette-Choo 
T J Li 
Abe Lu 
Prakash Ittycheriah 
Pei Shroff 
Mani Sun 
Varadarajan 
Ba- Hargam Sanaz 
Rob Willoughby 
David Gaddy 
Ishita Das- Gupta 
Guillaume Desjardins 
Marco Cornero 
Brona Robenek 
Bhavishya Mittal 
Ben Albrecht 
Ashish Shenoy 
Fedor Moiseev 
Henrik Jacobsson 
Alireza Ghaffarkhah 
Morgane Rivière 
Alanna Walton 
Clé- Ment Crepy 
Alicia Parrish 
Yuan Liu 
Zongwei Zhou 
Clement Farabet 
Carey Radebaugh 
Praveen Srinivasan 
Claudia Van Der Salm 
Andreas Fidje- Land 
Salvatore Scellato 
Eri Latorre-Chimoto 
Hanna Klimczak-Plucińska 
David Bridson 
Dario De Ce- Sare 
Tom Hudson 
Piermaria Mendolicchio 
Lexi Walker 
Alex Morris 
Ivo Penchev 
Matthew Mauger 
Alexey Guseynov 
Alison Reid 
Seth Odoom 
Lucia Loher 
Victor Cotruta 
Madhavi Yenugula 
Dominik Grewe 
Anastasia Petrushkina 
Tom Duerig 
Antonio Sanchez 
Steve Yadlowsky 
AmirAmy Shen 
Adam Kurzrok 
Lynette Webb 
Sahil Dua 
Dong Li 
Preethi Lahoti 
Surya Bhupatiraju 
Dan Hurt 
Ha- Roon Qureshi 
Ananth Agarwal 
Tomer Shani 
Matan Eyal 
Anuj Khare 
Rammohan Shreyas 
Lei Belle 
Chetan Wang 
MihirSanjay Tekur 
Jinliang Kale 
Ruoxin Wei 
Brennan Sang 
Tyler Saeta 
Yi Liechty 
Yao Sun 
Stephan Zhao 
Pandu Lee 
Doug Nayak 
Manish Fritz 
John Reddy Vuyyuru 
Nidhi Aslanides 
Martin Vyas 
Xiao Wicke 
Taylan Ma 
Evgenii Bilal 
Daniel Eltyshev 
Nina Balle 
Hardie Martin 
James Cate 
Keyvan Manyika 
Yelin Amiri 
Xi Kim 
Kai Xiong 
Florian Kang 
Nilesh Luisier 
David Tripuraneni 
Mandy Madras 
Austin Guo 
Oliver Waters 
Joshua Wang 
Jason Ainslie 
Han Baldridge 
Garima Zhang 
Jakob Pruthi 
Feng Bauer 
Riham Yang 
Ja- Son Mansour 
Yang Gelman 
George Xu 
Ji Polovets 
Hong- Long Liu 
Warren Cai 
Emily Sheng 
Sherjil Xue 
Adams Ozair 
Christof Yu 
Xi- Aowei Angermueller 
Weiren Li 
Julia Wang 
Emmanouil Wiesinger 
Yuan Koukoumidis 
Anand Tian 
Madhu Iyer 
Mark Gu- Rumurthy 
Parashar Goldenson 
M K Shah 
Hongkun Blake 
Anthony Yu 
Jenni- Maria Urbanowicz 
Chrisantha Palomaki 
Kevin Fernando 
Ken Brooks 
Harsh Durden 
Nikola Mehta 
Elahe Momchev 
Maria Rahimtoroghi 
Amit Georgaki 
Sebas- Tian Raul 
Morgan Ruder 
Jinhyuk Redshaw 
Komal Lee 
Dinghua Jalan 
Ginger Li 
Blake Perng 
Parker Hechtman 
Milad Schuh 
Mia Nasr 
Kieran Chen 
Vladimir Milan 
Trevor Mikulik 
Juliana Strohman 
Tim Franco 
Demis Green 
Koray Hassabis 
Kavukcuoglu 
Hugo Touvron 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cristian Canton Ferrer 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
Naman Goyal 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Marie-Anne Lachaux 
Thibaut Lavril 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi </p>
<p>Lama Ahmad
Ilge Akkaya</p>
<p>Florencia Leoni Ale-man
Janko Altenschmidt, Sam Alt-man, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul BaltescuDiogo Almeida, Shyamal Anadkat, Red Avila, Haim</p>
<p>Chester Cho
Casey Chu, Hyung Won Chung, Jeremiah Currier, Yunxing DaiDave Cummings, Cory Decareaux</p>
<p>Thomas Degry
Noah Deutsch
Arka Dhar, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam FedusDamien Deville, David Dohan, Niko Felix</p>
<p>Simón Posada Fishman
Juston Forte</p>
<p>Isabella Ful-ford
Elie GeorgesLeo Gao, Christian Gibson, Vik</p>
<p>Yang Song
Natalie Staudacher
Felipe Petroski Such</p>
<p>Natalie Summers
Ilya Sutskever
Jie Tang</p>
<p>Niko-las Tezak
Phil Tillet, Jerry TworekMadeleine Thompson, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley</p>
<p>Juan Felipe Cerón Uribe
An-drea Vallone, Arun Vijayvergiya, Chelsea Voss, Car-roll Wainwright, Justin Jay WangAlvin Wang, Ben</p>
<p>Tom Hennigan
Fabio ViolaBenjamin Lee</p>
<p>Malcolm Reynolds
Yuanzhong Xu, Ryan Doherty</p>
<p>Eli Collins
Clemens Meyer, Eliza Rutherford</p>
<p>Erica Moreira</p>
<p>Yeongil Ko
Laura Knight, Amélie Héliou, Ning Niu, Shane Gu</p>
<p>Chenxi Pang
Dustin Tran, Yeqing Li, Nir Levine</p>
<p>Ariel Stolovich
Norbert Kalb, Wenny Yustalim, Robin Strudel, Ali Elqursh, Shyam Upadhyay, Mike DusenberryRe-beca Santamaria-Fernandez, Sonam Goenka, Balaji Laksh-minarayanan, Charlie Deck, Hyo Lee, Zonglin Li, Xuezhi Wang</p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Angela Fan, Melanie Kambadur, Sharan NarangYuchen Zhang</p>
<p>Aurelien Ro-driguez
Sergey Edunov, and ThomasRobert Stojnic</p>
<p>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning
3 Jun 2024919A0809B39B5F2FF7A4F1F2F9E046E2arXiv:2402.14856v2[cs.CL]Statements: 1. Blue iff red. 2. Red xor white. 3. White iff pink. Conclusion: If not blue then pink.Statements: 1Green iff purple2If purple then gray3Gray xor yellowConclusion: Green xor yellow Statements: 1Red xor maroon2Maroon xor yellow3Yellow iff orangeConclusion: If maroon then orange Statements: 1Purple iff yellow2Yellow iff blue3Blue xor orangeConclusion: Purple xor orange Statements: 1Gray iff yellow2Yellow xor olive3Olive iff blackConclusion: If yellow then black Statements: 1Olive xor brown2Brown iff gray3Gray xor maroonConclusion: If not olive then maroon Supposition Following Chain Construction Compound Conclusion Concatenation Strategy Symbolic Strategy
Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments.It allows individuals to draw conclusions that logically follow, given the truth value of the information provided.Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks.Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior.In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems.Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like supposition following or chain construction.Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones.Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process.This distinction underscores the necessity for more nuanced evaluation procedures in the field.</p>
<p>Introduction</p>
<p>Deductive reasoning, that is the process of drawing conclusions that logically follow from the information at hand, is an integral aspect of human cognition and plays a pivotal role in formulating sound and coherent arguments (Leighton, 2003).Take, for example, the following statements:</p>
<p>If there is a blue marble in the box then there is a green marble in the box.There is a blue marble in the box.</p>
<p>Task Prompt: Evaluate whether the given conclusion logically follows from the premises below: 1.There is a pink marble if and only if there is an olive marble.2. If there is an olive marble then there is a brown marble.3. Either there is a brown marble or else there is a black marble, but not both.Conclusion: Either there is a pink marble or else there is a black marble, but not both.</p>
<p>Answer:</p>
<p>Okay, let's say there is a pink marble.According to the first statement, there must be an olive marble.Then, according to the second statement, there must be a brown marble.But wait, that means there cannot be a black marble, because of the third statement!So, the conclusion is actually true.</p>
<p>Answer:</p>
<p>1.If there is a pink marble, then there must be an olive marble.2. If there is an olive marble, then there must be a brown marble.3.If there is a brown marble, then there cannot be a black marble.Therefore, if there is a pink marble, then there cannot be a black marble.</p>
<p>Conclusion: True</p>
<p>Figure 1: Given the propositional reasoning prompt (top box), the LLM shows two different inferential strategies: supposition following (left) and chain construction (right), see Section 2 for strategy details.Note that both answers are only partially correct, as the exclusive disjunction has only been proven for one of the cases (pink and not black).Model responses are generated by LLaMA-2-Chat-70B across two random seeds.</p>
<p>Even without proper training in logic, most individuals can naturally deduce the valid conclusion: Therefore, there is a green marble in the box.</p>
<p>This innate capability of drawing conclusions that invariably follow from the truth value of available information has been a focal point of scholarly interest for centuries (Holyoak and Morrison, 2005).Propositional logic, a subfield of deductive reasoning, focuses on constructing logical arguments based on the relationship between statements similar to those in the example previously mentioned (Hurley, 2011).Extensive research has been ded-icated to examining human reasoning behavior in contexts that involve propositional logic.For instance, Van der Henst et al. (2002) have identified five different strategies people commonly employ when navigating problems of propositional logic (see Section 2).Such behavioral studies have been crucial in shaping theories that shed light on the fundamental elements of cognitive reasoning processes (Rips, 1994;Johnson-Laird, 1986;Kahneman et al., 1982).</p>
<p>In parallel, recent advancements in the field of large language models have demonstrated their potential in executing tasks involving deductive reasoning (Yang et al., 2023;Yu et al., 2024;Huang and Chang, 2023).Yet, the extent to which LLMs truly possess such abilities remains a subject of ongoing debate (Mahowald et al., 2024;Mitchell and Krakauer, 2023).Unlike behavioral studies in human reasoning that are often characterized by in-depth examinations of the reasoners' expressions, many studies on LLM-based reasoning tend to focus on task performance and accuracy metrics, offering limited insights into the underlying reasoning behavior of the models (Mitra et al., 2023;OpenAI et al., 2023;Team et al., 2023).</p>
<p>In this paper, we draw from the cognitive science literature (Van der Henst et al., 2002) and study inferential strategies employed by LLMs when solving propositional logic problems (see Figure 1).We analyze the reasoning behavior of three different language model families, varying in model size and fine-tuning procedure, and compare them to the behavior found in humans.To the best of our knowledge, we are the first to comprehensively compare inferential strategies employed by large language models and humans.We analyze the models' output both quantitatively and qualitatively via manual inspection, to provide insights into the soundness of their verbalized reasoning strategies.Our findings reveal that:</p>
<p>• All models exhibit inferential strategies akin to those observed in human reasoning, such as supposition following and chain construction.</p>
<p>• The inferential strategy employed is significantly influenced by the model family, as different families favor different approaches.</p>
<p>• Models are often right but for the wrong reasons: the accuracy of a model, that is the number of correct final conclusions, does not reflect whether its reasoning is sound, i.e. logically follows from the statements at hand.</p>
<p>• The strategy employed by a model is closely related to the soundness of its reasoning, where certain strategies lead to correct reasoning and others tend to introduce errors.</p>
<p>• In contrast to humans, models occasionally adopt a symbolic strategy, where formal logical calculus is employed to solve the propositional logic problem at hand.</p>
<p>Through this work, we hope to advance the understanding of reasoning in LLMs.</p>
<p>Strategies in Propositional Reasoning</p>
<p>Propositional logic studies the relationships among statements (or propositions) and the methods for constructing logical arguments based on them (Hurley, 2011).At the core of propositional logic are simple statements that can be combined through the use of logical connectives such as "not", "and", "or", and "if... then...", thereby forming more complex compound statements.Conclusions are logically deduced, where the truth value of the propositions necessitates the truth of the conclusion.This form of logical reasoning allows us to construct sound arguments that are invariably true, given the truth value of the information provided.As such, propositional logic is fundamental to various disciplines, including science, mathematics, and philosophy, where it offers a structured approach to reasoning and argumentation.</p>
<p>To gain insights into the inferential processes humans employ in propositional reasoning, Van der Henst et al. (2002) conducted a series of experiments that study the behavior of participants during propositional reasoning.They formulated straightforward propositional logic problems with neutral content (the presence or absence of colored marbles in a box, similar to the problem illustrated in Figure 1) and requested participants to articulate their thought processes while engaging with these problems.Participants were permitted the use of paper and pencil for their workings.Both their verbal explanations and written responses were meticulously recorded, transcribed and analyzed thereafter.Van der Henst et al. (2002) discovered five strategies reasoners commonly utilize to navigate the problems, offering insights into their inferential mechanisms employed during propositional reasoning.In the following, we give a short description of each strategy (illustrated in Figure 2).For more details and additional examples, we refer to the original study by Van der Henst et al. (2002).Figure 2: An example for each of the five inferential strategies identified by Van der Henst et al. (2002) (to the left of the dashed vertical line) that human reasoners employ when solving tasks of propositional logic.Each strategy is illustrated by a single example adopted from the transcribed recordings published by the original study.In addition, we provide an example of the ○ symbolic strategy occasionally encountered in LLMs (to the right of the dashed line)."Iff" denotes a biconditional, while "xor" indicates an exclusive disjunction.</p>
<p>Incremental Diagram.This strategy involves the creation of a comprehensive diagram that keeps track of all potential outcomes compatible with the premises of the problem.During the reasoning process, individuals progressively increment their diagrams to incorporate new information derived (see left box in Figure 2).The result is a single diagram that records a variety of possibilities compatible with the premises, often including even those that might be irrelevant to the task. 1   Supposition Following.Reasoners employing this strategy start with a supposition, e.g. by assuming a marble of a certain color.Subsequently, they trace the implications of that supposition, logically following from the premises at hand, as illustrated in the upper second box from the left of Figure 2. The result is a sequence of literals (in this case, marbles of a certain color) without logical connectives. 1 In contrast to Van der Henst et al. (2002), we in fact observe no single occurrence of the incremental diagram strategy in LLMs, despite the authors finding that this strategy is most frequently employed by humans.We believe that this discrepancy stems from the use of pen and paper in human assessments, implicitly encouraging diagrammatic reasoning.Exploring how this observation changes with vision-language models would be an intriguing area for future research.</p>
<p>The efficiency and success of supposition following strongly depends on the supposition made by the reasoner.While some suppositions lead to inferences that are relevant to the problem, others might lead to irrelevant conclusions.</p>
<p>Chain Construction.When employing this strategy, reasoners construct a chain of conditional statements derived either from the premises in the problem description or from intermediate deductions.An example of chain construction is displayed in the lower second box from the left of Figure 2. Premises are converted into a chain of conditional statements that are linked by their entities.A distinctive feature of this chain is the interconnection between conditionals, where the consequent of one conditional is the antecedent of the following.</p>
<p>Compound Strategy.Reasoners following the compound strategy combine two or more statements to derive a new compound conclusion.This process yields a series of novel conclusions, each building upon the preceding ones.An illustrative example of this strategy is given in the upper second box from the right of Figure 2. Based on the first two premises, the compound conclusion: "If blue then not red." is inferred, and then used to draw another compound conclusion ("If blue then not red and not pink.")together with the last premise of the problem statement.Concatenation Strategy.This approach entails the concatenation of two or more statements into a single conclusion encompassing the logical implications of each combined proposition.This strategy is subtle and has only been infrequently observed by Van der Henst et al. (2002).An example of the strategy is illustrated in the lower second box from the right of Figure 2. Symbolic Strategy.We could identify an additional strategy occasionally employed by LLMs, which has not been observed by Van der Henst et al. (2002) in human reasoners.This strategy, which we denote as symbolic strategy, is characterized by models employing formal logical calculus to solve the tasks at hand.When following this strategy, models either translate logical statements that are expressed in natural language (e.g."If there is a white marble then there is not a red marble.")into formal logic (W → ¬R), and then operate on those expressions, or create a truth table from which they aim to infer the validity of the conclusion.An illustration of this strategy is provided in the right box of Figure 2.</p>
<p>Experimental Setup</p>
<p>Task Overview.Our task setup aligns with the experiment conducted by Van der Henst et al. (2002) to allow for a fair comparison between the inferential strategies found in humans and those identified in LLMs. 2 In particular, we evaluate each model on the 12 problems of propositional logic suggested by Van der Henst et al. (2002) (an overview of each problem can be found in Figure 5 in the appendix).For each problem, models are presented with a set of statements (or premises) and must determine whether a given conclusion logically follows (for an example, see Figure 1).Eight out of 12 problems involve three premises and a conclusion, while the remaining four problems consist of four premises leading to a conclusion.All premises, as well as the conclusions resemble either biconditionals, exclusive disjunctions or conditionals.Two problems (4 and 6) include a redundant first premise.All premises are stated such that two subsequent statements contain one proposition in common, except of two problems (11 and 12), which are arranged in Language Models.We aim to investigate various factors that might impact the inferential strategies displayed by LLMs.These factors include the type of model, its size, and the emphasis on alignment during training (Tunstall et al., 2023).Therefore, we assess a total of five models, consisting of three prominent open-access model types: Llama 2 (Touvron et al., 2023) with model sizes of 7B, 13B, and 70B, the recently released Mistral-7B model (Jiang et al., 2023), andZephyr-7B (Tunstall et al., 2023), an extension of Mistral-7B with a focus on intent alignment through fine-tuning with AI Feedback (AIF).For our evaluations, we utilize the publicly accessible model weights from the Hugging-Face platform, specifically Llama-2-chat-hf3 (7B, 13B, and 70B), Mistral-7B-Instruct-v0.2,4 and zephyr-7b-beta. 5We consciously opt not to include proprietary models accessible via paid APIs, despite their reported superior performance in reasoning tasks (Team et al., 2023).This methodological choice reflects our commitment to promoting transparent and reproducible scientific research.Note that in this work, we refer to the above models when using abbreviations such as LLaMA-2, Mistral-7B-Instruct or Zephyr-7B-β.</p>
<p>Evaluation Setup.We prompt each model with a system message providing context about the task they are about to solve and the format in which they should answer (for the full prompt, see Figure 5 in the appendix).Analogous to Van der Henst et al. (2002), we inform the model of its participation in an experiment designed to explore reasoning processes, and instruct it to "think aloud" as it tackles the problem.In addition to the system message, we provide a user prompt that contains the problem description.In cases where the model does not accept system messages (such as Mistral-7B-Instruct-v0.2), we prepend the content of the system message to the user prompt.</p>
<p>To prevent biasing the model towards a certain strat-
○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ chain construction.
The model correctly constructs a chain of conditionals (highlighted in yellow within the model's response) based on the premises, leading from the antecedent of the final conclusion to its consequent.Comments made by the annotators are presented in the adjacent right panel.egy, we refrain from providing few-shot examples, as done also by Leidinger et al. (2023).Instead, we elicit reasoning through zero-shot chain-of-thought prompting ("Let's think step by step") (Kojima et al., 2022).Answers for each model are generated with nucleus sampling using Llama-2-chat-hf's default values (top-p = 0.9, temperature T = 0.6), as we found this configuration to work well for all models.To account for the statistical nature of language models, we ask each model to solve the set of propositional problems across 5 random seeds, resulting in a total of 60 responses per model.Our code is publicly available at: https://github.com/mainlp/inferential-strategies.</p>
<p>We record all answers and manually evaluate them (a total of 300 responses) for strategies employed in their reasoning (see Figure 3 for an example).For each model response, we qualitatively evaluate for strategy and soundness.That is, we manually label the inferential strategies identified, and the logical validity of the model's reasoning.In addition, we record whether the final answer is correct.In cases of faulty reasoning, we categorize the type of error.This comprehensive manual evaluation of model responses is independently conducted by two hired students with expertise in manual data annotation.To gauge the quality of the annotations, we report an overall Cohen's Kappa value of κ = 0.98.For details on the interannotator agreement of each label, we refer to Table 2 in the appendix.Further annotated examples can be found in Appendix C. Following the recommendations put forward by Leidinger et al. (2023), we make all input prompts, model responses and manual annotations publicly available at: huggingface.co/datasets/mainlp/inferential_strategies.</p>
<p>Results and Analysis</p>
<p>In this section, we present the results of our evaluation.We begin with a quantitative analysis of the inferential strategies employed by LLMs, as well as the logical validity of their reasoning.This is followed by a qualitative analysis providing a more in-depth examination of the models' reasoning.Values in parentheses denote fractions with respect to the total number of strategies employed by that model.Values of correct answers and instances of sound reasoning are reported with their standard deviations.† The comparison with human reasoners is based on findings by Van der Henst et al. (2002), where dashes denote missing values.</p>
<p>Quantitative Analysis</p>
<p>Table 1 provides an overview of the frequencies with which large language models employ inferential strategies when navigating the problems of propositional logic described in Section 3. Our evaluation reveals that all models display strategies akin to those observed by Van der Henst et al. (2002).In particular, we find that, similar to humans, models commonly employ supposition following, chain construction and the compound strategy.In addition, we observe that models occasionally utilize the symbolic strategy, employing techniques from logical calculus to solve the tasks (see Section 2).Note that, similar to humans, models might switch from one strategy to another during a single problem, demonstrating multiple strategies within their responses (see Figure 19 in the appendix for an example).Surprisingly, we observe that distinct model families favor different inferential strategies.For instance, Zephyr-7Bβ predominantly employs supposition following, while Mistral-7B-Instruct is equally inclined towards drawing compound conclusions.In contrast, models from the Llama 2 series tend to rely on supposition following and chain construction, with negligible use of the compound strategy.Our analysis further reveals a discrepancy between the correctness of the models' final answers and the logical soundness of their reasoning.While all models achieve an answer accuracy that approximately coincides with chance in our experimental setup, an analysis of their reasoning validity reveals a different picture: LLaMA-2-70B outperforms the other models by reasoning correctly in about 31.7% of cases, while Zephyr-7B-β and Mistral-7B-Instruct produce sound reasoning in 25% of the problems.</p>
<p>We note that all models perform rather poorly on the propositional tasks, with LLaMA-2-7B failing entirely to construct sound arguments.Human Reasoning.Van der Henst et al. (2002) compute the percentages with which human reasoners employ inferential strategies with respect to the total number of strategies observed in their experiment, and not with respect to the total number of problems considered.Thus, their reported values mainly reflect which strategies are favored more or less by the reasoner, but do not provide information about how frequently a strategy has been observed in the overall context.To make our findings comparable to the results of Van der Henst et al. (2002), we convert our results respectively (see values in parentheses in Table 1).We note that almost all models seem to favor supposition following to a higher degree than human reasoners, who employ this strategy in only about 21% of overall use.In contrast, humans seem to draw compound conclusions more readily, except for Mistral-7B-Instruct which shows a tendency more than twice as high.</p>
<p>Overall, both LLMs and humans hardly employ the concatenation strategy.Interestingly, Van der Henst et al. (2002) report that all reasoners successfully solve the problems of propositional logic, though not always for the correct reasons.While the study does not provide data on the number of problems where humans reasoned correctly, the high success rate of human participants contrasts sharply with the performance of the models.For each inferential strategy, the ratio of sound reasoning traces (represented by the filled portion) to the overall application of that strategy (denoted by the unfilled bar) is depicted.Ratios are expressed as percentages above the corresponding filled section.Note that LLaMA-2-7B is not displayed as it does not exhibit sound reasoning.</p>
<p>Effect of Model</p>
<p>model scale significantly influences the frequency with which strategies are employed by the model.</p>
<p>In particular, we observe that with increasing model size, Llama 2 employs strategies more readily.Furthermore, larger models within the Llama 2 framework are observed to generate a greater number of sound reasoning traces.We interpret this trend as a result of the model's improving proficiency in strategic reasoning as its scale increases.</p>
<p>Effect of Alignment.</p>
<p>The alignment of a model's response with human preferences is crucial to emulate human-like behavior (Ouyang et al., 2022).Zephyr-7B-β is an iteration of Mistral-7B that is fine-tuned with AI Feedback (AIF) for improved intent alignment (Tunstall et al., 2023).In comparison to the observations made by Van der Henst et al. (2002), where besides the incremental diagram strategy (34%), chain construction was employed most frequently by humans, Zephyr-7Bβ demonstrates a marked preference for supposition following and significantly less engagement in chain construction.Moreover, it is noteworthy that among the evaluated models, Zephyr-7B-β most frequently adopts the symbolic strategy, an approach not reported in human reasoners.</p>
<p>Sound Reasoning.As previously highlighted, the accuracy of a model's final answer does not necessarily serve as a reliable indicator of its reasoning capability.In particular, we observe that models often arrive at correct answers, but through flawed reasoning processes (refer to Figure 10 in the appendix for an illustration).Interestingly, we also find instances where models provide incorrect final answers despite reasoning correctly (for an example, see Figure 16 in the appendix).Our analysis reveals only a moderate positive correlation between the accuracy of the models' final answers and the logical soundness of their reasoning, with a Pearson correlation coefficient r(298) = 0.45 and a statistically significant p-value of less than 0.0001(p = 1.6 × 10 −16 ).This observation aligns with findings from previous studies (Ye and Durrett, 2022;Creswell and Shanahan, 2022) and underscores the need for more nuanced evaluation procedures, particularly in multiple-choice settings, where models might select the correct answer by chance rather than through rigorous reasoning.</p>
<p>In Figure 4, we explore the relationship between the inferential strategies employed by the models and the validity of their reasoning.For each strategy, we quantify the proportion of instances where the models' reasoning is sound, compared to the overall application of that strategy.Our analysis reveals variability in the effectiveness with which different models apply various strategies.For example, Mistral-7B-Instruct tends to reason correctly when using approaches such as the chain, compound, or symbolic strategy, yet frequently encounters reasoning errors with supposition following.On the other hand, LLaMA-2-70B exhibits proficiency in supposition following, but struggles with the symbolic strategy.</p>
<p>Qualitative Analysis</p>
<p>We supplement our quantitative analysis by a more detailed qualitative analysis of the models' reasoning behavior.Figure 3 depicts LLaMA-2-70B's response to problem 1 of the task set.The response illustrates a frequently observed behavior.Initially, models tend to analyze the problem's propositions, often by paraphrasing each premise and the conclusion to be evaluated.They then embark on a reasoning process, typically utilizing one of the previously mentioned strategies.In the example, LLaMA-2-70B employs chain construction, creating a logical chain of conditionals that leads from the antecedent of the final conclusion to its consequent, thereby correctly affirming the conclusion's logical validity.A notable pitfall in such reasoning chains is the models' occasional misinterpretation of logical negations, leading to erroneous chains like: A → ¬ B; B → C; therefore A → C, where the negation in the first conditional is overlooked (for an illustrative case, refer to Figure 11 in the appendix).This behavior can be found across all models and aligns with previous work reporting difficulties of LLMs in understanding logical negations (Truong et al., 2023).</p>
<p>When employing supposition following, models often fail to consider all implications of their assumptions.Instead, they tend to focus only on immediate inferences, while overlooking further consequences crucial for assessing the conclusion's validity.This leads to models prematurely concluding the inability to definitively determine the logical validity of the final conclusion: "Based on our analysis, we cannot definitively say that the conclusion logically follows from the given statements" (see Figure 7 in the appendix for a respective example).Another source of error in supposition following involves models making improper suppositions, such as conjecturing about a marble not mentioned in the final conclusion, and deriving disjointed intermediate conclusions that do not aid in solving the problem.An example of this behavior can be found in Figure 8 in the appendix.</p>
<p>Finally, we identify two behaviors in models that mirror logical errors seen in human reasoners ( Van der Henst et al., 2002).First, models frequently attempt to prove an exclusive disjunction (A ⊕ B) by only considering a single conditional case (A → ¬ B), and second, they sometimes engage in the logical fallacy known as denial of the antecedent: A → B; therefore ¬ A → ¬ B (for illustrative examples, see Figures 12 and 13 in the appendix, respectively).</p>
<p>Related Work</p>
<p>Human Strategies in Deductive Reasoning.A considerable amount of research, especially within psychology and cognitive science, has explored how humans approach deductive reasoning tasks (Schaeken et al., 2000).A prominent focus of these studies is on heuristics, which are cognitive shortcuts that individuals employ to arrive at satisfactory conclusions in deductive reasoning despite potential flaws in the underlying logic (Kahneman et al., 1982;Evans, 1989;Gigerenzer and Todd, 1999;Davis, 2018).For instance, Woodworth and Sells (1935) demonstrate that individuals tend to accept conclusions in syllogistic reasoning as valid when they share logical quantifiers with the premises, regardless of their actual logical validity.Nonetheless, such reliance on heuristics can result in errors and falls short of the level of strategic reasoning necessary to develop sound and coherent arguments (Kahneman, 2012).Further research has delved into more sophisticated strategies utilized by individuals in deductive reasoning.Based on the mental model theory (Johnson-Laird, 1986), Bucciarelli and Johnson-Laird (1999) identify a variety of strategies commonly employed by individuals in syllogistic reasoning.Byrne and Handley (1997) study strategies of individuals in knight-and-knave puzzles, where the truthfulness of statements made by hypothetical characters have to derived.Their experiments reveal that humans engage in both forward and backward inferences to navigate through potential solutions.</p>
<p>Human Reasoning Behavior in LLMs.Recent research has started to explore the extent to which LLMs mirror human-like reasoning behaviors.Dasgupta et al. (2023) demonstrate content-effects akin to those observed in human reasoning, where the deductive process is influenced by the content of the problem statement.Eisape et al. (2023) find that LLMs, similar to humans, exhibit biases such as ordering effects in syllogistic reasoning tasks.Several other studies have delved into the prevalence of biases and heuristics within LLMs (Binz and Schulz, 2023;Talboy and Fuller, 2023;Shaki et al., 2023;Suri et al., 2024).However, to the best of our knowledge, we are the first who study the presence of more sophisticated human strategies in the context of LLM-based deductive reasoning.</p>
<p>Faithful Reasoning with LLMs.Large language models can be instructed to explain the reasoning process by which they derive their final conclusions (Wei et al., 2022;Kojima et al., 2022).However, several studies indicate that these self-explanations might not always be faithful, i.e. accurately represent the model's underlying reasoning process (Jacovi and Goldberg, 2020;Agarwal et al., 2024;Lyu et al., 2024).For instance, Turpin et al. (2023) demonstrate that LLMs such as GPT-3.5 (OpenAI, 2023) and Claude 1 (Anthropic, 2023) often fail to mention biasing features in their input that significantly influence their decisions.Instead, the models produce plausible yet misleading explanations that give a false account of the underlying decision process.Lanham et al. (2023) probe the faithfulness of explanations by evaluating how the final conclusions of LLMs change when rationales are truncated or errors are introduced.Their findings reveal that the extent to which models rely on their rationales varies strongly across models and tasks.Matton et al. (2024) propose a method to quantify the faithfulness of explanations based on high-level concepts in the models' input that influence decision-making.By measuring the difference between the set of concepts that LLMs deem influential and the set that truly are, instances of unfaithfulness could be identified, including cases where LLMs overlook the impact of social biases in their decision-making processes.Another important consideration is whether the model's final conclusion aligns with its preceding explanation.As highlighted in Section 4.1, a correct conclusion might not always be the product of a logically sound reasoning trace, particularly in multiple-choice setups.Conversely, a sound rationale may not always lead to a logically consistent answer.Related work by Ye and Durrett (2022) indicates that in questionanswering and natural language inference tasks, explanations generated by LLMs such as OPT (Zhang et al., 2022) and GPT-3 (Brown et al., 2020) often do not entail the models' final conclusions.Further studies aim to enhance the models' faithfulness, for instance by enforcing causality from proof generation to entailment prediction.This can be achieved by either restricting the model's context (Sanyal et al., 2022;Creswell and Shanahan, 2022;Radhakrishnan et al., 2023), or by utilizing deterministic tools that are inherently faithful by design (Lyu et al., 2023).</p>
<p>Conclusion</p>
<p>In this paper, we examine the inferential strategies employed by LLMs in solving problems of propositional logic.Through a comprehensive evaluation of their reasoning behavior, we demonstrate that LLMs adopt strategies akin to those observed in human reasoners.Our quantitative analysis reveals that the frequency with which a model adopts a specific strategy strongly depends on its type, size, and fine-tuning procedure.Moreover, our analysis suggests that the accuracy of a model's final conclusions does not adequately capture its reasoning capabilities, underscoring the importance of a more sophisticated evaluation framework that includes the model's reasoning paths.We also provide a qualitative analysis of typical reasoning behaviors among models, pinpointing prevalent errors such as difficulties in understanding negations or recognizing all implications of a supposition.</p>
<p>Limitations</p>
<p>While our work contributes to the understanding of reasoning processes in large language models by demonstrating that these models employ inferential strategies in propositional logic similar to humans, it encompasses several limitations that could be addressed in future work.</p>
<p>Task setup.Our study is constrained by a limited set of problems, designed within a fixed framework that revolves around hypothesis validation based on 3-4 statements of propositional logic.We employ a constant and neutral content, disregarding potential content-effects on the models' reasoning behavior, as shown by Dasgupta et al. (2023).Similarly, we have not yet examined factors such as the complexity of the problems, the differences between hypothesis validation and generation, and the impact of logical connectives utilized in the premises.We believe that these factors are worth investigating and leave a detailed examination to future work.</p>
<p>Evaluation Framework.The extent of our manual evaluation is limited by both the number of samples reviewed and the quantity of annotators involved.Despite our efforts to maximize the use of available resources, these constraints may affect the scalability and reliability of our results.Additionally, we instruct all models through zero-shot chain-ofthought prompting ("Let's think step by step") (Kojima et al., 2022).Exploring alternative reasoning frameworks, such as Tree of Thoughts (Yao et al., 2023) or Graph of Thoughts (Besta et al., 2024), could provide valuable insights into their influence on model behavior and the inferential strategies adopted.Based on our annotated data, we endeavored to develop a classifier capable of automatically identifying the inferential strategies employed in the models' output, which was intended to comple-ment our manual evaluation setup.However, due to the complexity of the task and limited size of our annotated dataset, our classifier struggled with generalization to new, unseen responses.In future endeavors, we aim to allocate more resources towards expanding our manual annotation efforts and explore this direction further.Finally, our study predominantly offers a behavioral analysis and does not delve into the mechanistic aspects that might explain the diversity in strategy usage by the models.Investigating how model-internal mechanisms might influence their choice of reasoning strategy presents a compelling direction for future research.</p>
<p>A Additional Experimental Details</p>
<p>In this section, we provide additional details about the experimental setup, including supplementary information about the problem formulations and prompts utilized.</p>
<p>A.1 Task Prompts</p>
<p>Figure 5 displays the task prompt and problem formulations employed in assessing the language models described in Section 3. Note that the prompt Figure 5: The task prompt (upper yellow box) as well as statements and conclusion for each propositional logic problem (lower gray boxes).In the task prompt, the placeholder "<statements and conclusion from below>" is replaced with the actual statements and conclusion relevant to each problem.To enhance readability, we employ abbreviations within the problem statements.In the actual prompt, "colorA iff colorB" is replaced by "There is a colorA marble in the box if and only if there is a colorB marble in the box".Similarly, "colorA xor colorB" denotes "Either there is a colorA marble in the box or else there is a colorB marble in the box, but not both".Lastly, "If colorA then colorB" stands for "If there is a colorA marble in the box then there is a colorB marble in the box".template, i.e. special tokens and their arrangements, might vary depending on the specific language model used.Within the task prompt (provided in the upper box), the problem statements and conclusion for a given problem are replaced with the corresponding problem formulations found in the lower gray boxes.In the final version of the prompt, the phrase "colorA iff colorB" is expanded to "There is a colorA marble in the box if and only if there is a colorB marble in the box".Similarly, "colorA xor colorB" is interpreted as "There is either a col-orA marble or a colorB marble in the box, but not both", and "If colorA then colorB" is articulated as "If there is a colorA marble in the box, then there is a colorB marble in the box".</p>
<p>A.2 Annotator Instructions</p>
<p>Our assessment of model responses involves a comprehensive independent review by two students who are specialized in the field of natural language processing and have expertise in manual data annotation.To ensure a high quality of annotations, we offer comprehensive training to both annotators.This training includes detailed explanations and extensive examples of the strategies identified by Van der Henst et al. (2002), complemented by a session dedicated to clarifying any questions that may emerge.Subsequently, the annotators are tasked with independently annotating practice examples, which serves to highlight and address any ambiguities in the annotation process.Only when both annotators are confident in their understanding of each strategy do we proceed.We instruct both annotators to independently go through each model response and mark parts where they identify a certain strategy to be employed.Each strategy is marked in a unique color code, which is afterwards converted into labels that signify the use of a particular strategy.In addition, we instruct both annotators to label whether the reasoning is sound, and the final conclusion of the model is correct.Furthermore, we ask them to classify any logical errors identi-fied within the reasoning process.To maintain a high standard of annotation quality, annotators are instructed to review the model responses twice.</p>
<p>A.3 Inter-Annotator Agreement</p>
<p>To assess the reliability of our manual evaluation process (see Section 3), we quantify the interannotator agreement by calculating Cohen's Kappa for each category and model, as illustrated in Table 2. Generally, the results indicate an almost perfect level of agreement across all categories and models, with Cohen's Kappa values ranging from 0.81 ≤ κ ≤ 1.0.An exception is observed in the case of the concatenation strategy applied by LLaMA-2-7B, for which we report a substantial agreement level, with a Kappa value of κ = 0.79, slightly below the threshold for almost perfect agreement.</p>
<p>A.4 Model Details</p>
<p>We report further details about the models used in this study in Table 3.In particular, we provide information about the number of parameters, context length and fine-tuning procedure for each model.</p>
<p>B Additional Quantitative Results</p>
<p>In this segment, we present supplementary findings from our quantitative evaluation.Table 4 illustrates the frequencies with which the different language models employ inferential strategies when navigating the problems of propositional logic, as outlined in Section 3. Values denote percentages averaged across five distinct random seeds, accompanied by their standard deviation.Furthermore, we detail the proportions of correct final conclusions and sound reasoning.Note that all percentages are calculated relative to the overall count of tasks within the experimental framework.6 presents LLaMA-2-70B's approach to problem 7, where the model supposes the absence of a blue marble in the box and logically infers the implications of this assumption to reach the valid conclusion.On the other hand, Figure 7 depicts Mistral-7B-Instruct's response to the same problem, where the model considers various combinations of marble in the box, drawing immediate conclusions that follow from the premises at hand.However, it does not explore deeper ramifications of these suppositions, thereby failing to deduce the validity of the conclusion.This showcases a common behavior we observe in models that employ supposition following unsuccessfully.In Figure 8 the model approaches problem 9 by assuming the presence of an olive marble in the box, yet inferring disjointed intermediate conclusions that do not aid in solving the problem, thus failing to prove the logical validity of the problem.</p>
<p>C Annotated Model Responses</p>
<p>C.2 Chain Construction</p>
<p>Figures 9 to 13 illustrate instances where models employ chain construction to navigate the problems of propositional logic.In Figure 9, LLaMA-2-70B adeptly forms a chain of conditional statements that bridge the antecedent of the conclusion to its consequent, effectively validating the conclusion's logical soundness.Conversely, Figure 10 depicts a logical chain in which LLaMA-2-70B erroneously concludes the nonexistence of a white marble based on the absence of a red marble, despite an exclusive disjunction linking the two.Despite this logical misstep, the model's final conclusion remains accurate, highlighting the discrepancy between the model's final answer and the soundness of its reasoning.In Figure 11, LLaMA-2-13B constructs a chain correctly linking the antecedent of the final conclusion to its consequent.Nonetheless, it overlooks the negation present in one of the conditionals, resulting in a compromised reasoning chain.Figure 12 presents a scenario where the model incorrectly attempts to validate an exclusive disjunction solely through a singular conditional sequence, a reasoning error not uncommon among human reasoners (Van der Henst et al., 2002).Lastly, Figure 13 highlights LLaMA-2-70B's engagement in the inverse fallacy, inferring ¬ W → ¬ G from the conditional W → G, mirroring a logical misjudgment frequently observed in human reasoning processes.</p>
<p>C.3 Compound Strategy</p>
<p>The compound strategy is illustrated in Figures 14 to 16. Figure 14 presents Mistral-7B-Instruct's approach to problem 9, where it infers a biconditional relationship between the purple and olive marble from the first two premises.On the other hand, Figure 15 shows LLaMA-2-70B's response to the same problem, formulating a sequence of compound inferences beyond the initial bicondi- Table 4: Relative occurrences of inferential strategies employed by the different language models when solving the propositional problems.All values denote percentages averaged across 5 different random seeds with standard deviation.In addition, the percentages of correct final answers and sound reasoning are reported.</p>
<p>tional deduction, culminating in the correct final answer.Additionally, Figure 16 illustrates Mistral-7B-Instruct's approach to problem 8, in which the model initially generates compound conclusions derived from the problem statements, followed by supposition following to explore the implications that the absence of an olive marble might have.However, despite the model's sound reasoning, its final answer is incorrect.</p>
<p>C.4 Concatenation Strategy</p>
<p>Figure 17 demonstrates the concatenation strategy, where Mistral-7B-Instruct concatenates two intermediate deductions to form a single statement.It then uses the concatenated statement to infer the invalidity of the conclusion.</p>
<p>C.5 Symbolic Strategy</p>
<p>The symbolic strategy is exemplified in Figure 18, where LLaMA-2-70B employs a truth table to assess the conclusion's validity, albeit with errors leading to an incorrect result.Conversely, Figure 19 shows Mistral-7B-Instruct's application of chain construction followed by the symbolic strategy.The model makes false inferences while employing chain construction, and further errs in its validation through logical calculus.</p>
<p>Problem 7:</p>
<p>Statements:</p>
<p>1.There is a blue marble in the box if and only if there is a red marble in the box.</p>
<ol>
<li>
<p>Either there is a red marble in the box or else there is a white marble in the box, but not both.</p>
</li>
<li>
<p>There is a white marble in the box if and only if there is a pink marble in the box.</p>
</li>
</ol>
<p>Conclusion: If there is not a blue marble in the box then there is a pink marble in the box.</p>
<p>Response: Sure, I'd be happy to help!Here's my reasoning process for this problem:</p>
<ol>
<li>First, I'll start by analyzing the given statements: Statement 1: "There is a blue marble in the box if and only if there is a red marble in the box" This statement tells me that the presence of a blue marble in the box is dependent on the presence of a red marble in the box.If there's no red marble, there can't be a blue marble.</li>
</ol>
<p>Statement 2: "Either there is a red marble in the box or else there is a white marble in the box, but not both" This statement tells me that the presence of a red marble and a white marble in the box are mutually exclusive.If there's a red marble, there can't be a white marble, and vice versa.</p>
<p>Statement 3: "There is a white marble in the box if and only if there is a pink marble in the box" This statement tells me that the presence of a white marble in the box is dependent on the presence of a pink marble in the box.If there's no pink marble, there can't be a white marble.</p>
<ol>
<li>Now, let's consider the conclusion: "If there is not a blue marble in the box then there is a pink marble in the box" I can start by assuming that there is no blue marble in the box.According to statement 1, this means that there is no red marble in the box either.Next, I can look at statement 2. Since there's no red marble, the alternative is that there's a white marble in the box.But wait, statement 3 says that if there's a white marble, there must be a pink marble too!So, if there's no blue marble in the box, it means there's no red marble, which means there's a white marble, which means there's a pink marble Therefore, the conclusion logically follows!Conclusion: True Annotator Comments:
Reformulation of statement 1.
Reformulation of statement 2.</li>
</ol>
<p>Reformulation of statement 3.</p>
<p>Repetition of conclusion.</p>
<p>Supposition following (sound).First consequence.Second consequence.Third consequence.</p>
<p>Conclusion (valid).</p>
<p>Final answer (correct).
○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ supposition following.
After reformulating the statements of the problem at hand, the model starts to reason about the problem by assuming the absence of a blue marble in the box.Subsequently, it traces the consequences of that supposition, drawing intermediate conclusions about the presence or absence of certain marbles, until it formulates a final conclusion.In this example, the model correctly reasons about the validity of the conclusion.</p>
<ol>
<li>
<p>First, let's analyze the given statements one by one: a) Statement 1: "There is a purple marble in the box if and only if there is a blue marble in the box" b) Statement 2: "There is a blue marble in the box if and only if there is an olive marble in the box" c) Statement 3: "Either there is an olive marble in the box or else there is a red marble in the box, but not both" d) Statement 4: "Either there is a red marble in the box or else there is a green marble in the box, but not both" 2. Since we are trying to prove that "If there is a purple marble in the box then there is a green marble in the box", let's try to find a way to connect purple marbles with green marbles using the given statements.</p>
</li>
<li>
<p>Unfortunately, there doesn't seem to be a direct connection between purple marbles and green marbles in the given statements.</p>
</li>
<li>
<p>However, let's see if we can find any indirect connections.Based on Statement 1, we know that there is a purple marble in the box if and only if there is a blue marble in the box. 5. Now, let's look at Statement 2. We know that there is a blue marble in the box if and only if there is an olive marble in the box.</p>
</li>
<li>
<p>From Statement 3, we know that there cannot be both an olive marble and a red marble in the box.However, Statement 4 tells us that there cannot be both a red marble and a green marble in the box either.</p>
</li>
<li>
<p>Let's consider the possibility that there is an olive marble in the box.If that's the case, then, according to Statement 2, there must be a blue marble in the box as well.8. Now, using Statement 1, we know that if there is a blue marble in the box, then there must be a purple marble in the box as well.9.However, Statement 3 tells us that if there is an olive marble in the box, then there cannot be a red marble in the box.But, Statement 4 tells us that if there is a red marble in the box, then there cannot be a green marble in the box.10.Since we cannot have both an olive marble and a red marble in the box, and we cannot have both a red marble and a green marble in the box, it seems that the only possibility left is that there is an olive marble in the box, and therefore, a blue marble, a purple marble, but no red or green marbles.11.However, the conclusion states that "If there is a purple marble in the box then there is a green marble in the box", which is not logically true based on the given statements.Conclusion: False.</p>
</li>
</ol>
<p>Annotator Comments:</p>
<p>Repetition of statement 1.</p>
<p>Repetition of statement 2.</p>
<p>Repetition of statement 3.</p>
<p>Repetition of statement 4.</p>
<p>Repetition of conclusion.</p>
<p>Reformulation of statement 1.</p>
<p>Reformulation of statement 2.</p>
<p>Reformulation of statement 3. Reformulation of statement 4.</p>
<p>Supposition following (sound).</p>
<p>Invalid inference.</p>
<p>Conclusion (invalid).</p>
<p>Final answer (incorrect).
○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ supposition following.
The model supposes the presence of an olive marble in the box and traces the consequences of that supposition.However, it derives disjointed intermediate conclusions that do not aid in solving the problem, failing to solve the task at hand. 1.First, I'll start by analyzing the given statements: Statement 1: "There is a purple marble in the box if and only if there is a blue marble in the box" This statement tells me that the presence of a purple marble in the box is dependent on the presence of a blue marble in the box.If there is a blue marble, then there must be a purple marble, and vice versa.</p>
<p>Statement 2: "There is a blue marble in the box if and only if there is an olive marble in the box" This statement tells me that the presence of a blue marble in the box is dependent on the presence of an olive marble in the box.If there is an olive marble, then there must be a blue marble, and vice versa.Statement 3: "Either there is an olive marble in the box or else there is a red marble in the box, but not both" This statement tells me that the presence of an olive marble and the presence of a red marble in the box are mutually exclusive.If there is an olive marble, then there cannot be a red marble, and vice versa.Statement 4: "Either there is a red marble in the box or else there is a green marble in the box, but not both" This statement tells me that the presence of a red marble and the presence of a green marble in the box are also mutually exclusive.If there is a red marble, then there cannot be a green marble, and vice versa.</p>
<ol>
<li>Now, let's consider the conclusion: "If there is a purple marble in the box then there is a green marble in the box" My reasoning process for this conclusion is as follows:</li>
</ol>
<p>-If there is a purple marble in the box, then by Statement 1, there must be a blue marble in the box.</p>
<p>-If there is a blue marble in the box, then by Statement 2, there must be an olive marble in the box.</p>
<p>-If there is an olive marble in the box, then by Statement 3, there cannot be a red marble in the box.</p>
<p>-If there is no red marble in the box, then by Statement 4, there must be a green marble in the box.</p>
<p>Repetition of conclusion.</p>
<p>Chain construction (sound).
P → B B → O O → ¬ R ¬ R → G Conclusion.
Final answer (correct).Statements: 1.There is a blue marble in the box if and only if there is a red marble in the box.2. Either there is a red marble in the box or else there is a white marble in the box, but not both.3.There is a white marble in the box if and only if there is a pink marble in the box.Conclusion: If there is not a blue marble in the box then there is a pink marble in the box.</p>
<p>Response:</p>
<p>Sure! Let's go through the statements and see if we can determine whether the conclusion logically follows.</p>
<ol>
<li>
<p>"There is a blue marble in the box if and only if there is a red marble in the box" This statement tells us that the presence of a blue marble in the box is dependent on the presence of a red marble.If there is a red marble, then there must be a blue marble, and if there is no red marble, then there cannot be a blue marble.</p>
</li>
<li>
<p>"Either there is a red marble in the box or else there is a white marble in the box, but not both" This statement tells us that the presence of a red marble and a white marble in the box are mutually exclusive.If there is a red marble, then there cannot be a white marble, and if there is a white marble, then there cannot be a red marble.</p>
</li>
<li>
<p>"There is a white marble in the box if and only if there is a pink marble in the box" This statement tells us that the presence of a white marble in the box is dependent on the presence of a pink marble.If there is a pink marble, then there must be a white marble, and if there is no pink marble, then there cannot be a white marble.Now, let's consider the conclusion: "If there is not a blue marble in the box then there is a pink marble in the box" Let's break this down further: -If there is not a blue marble in the box, then we know that there is no red marble in the box (from statement 1).</p>
</li>
</ol>
<p>-Since there is no red marble, we know that there cannot be a white marble in the box (from statement 2).</p>
<p>-If there is no white marble in the box, then we know that there must be a pink marble in the box (from statement 3).</p>
<p>Therefore, we can conclude that if there is not a blue marble in the box, then there must be a pink marble in the box.Conclusion: True.</p>
<p>Annotator Comments:</p>
<p>Reformulation of statement 1.</p>
<p>Reformulation of statement 2.</p>
<p>Reformulation of statement 3.</p>
<p>Repetition of conclusion.</p>
<p>Chain construction (invalid)
¬ B → ¬ R ¬ R → ¬ W (invalid) ¬ W → P (invalid) Conclusion: ¬ B → P Final answer (correct).○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ chain construction.
The model constructs a chain of conditionals leading from the antecedent of the final conclusion to its consequent.However, it fails to understand the implication of the exclusive disjunction in the second statement of the problem description, leading to a faulty reasoning trace.Despite its invalid reasoning, the model's final answer is correct.</p>
<p>Response:</p>
<p>Sure, I'd be happy to help with this experiment!Here's my reasoning process for each statement and the conclusion:</p>
<p>1.There is a brown marble in the box if and only if there is a yellow marble in the box This statement means that if there is a brown marble in the box, then there must also be a yellow marble in the box.Conversely, if there is a yellow marble in the box, then there must also be a brown marble in the box.</p>
<ol>
<li>
<p>Either there is a yellow marble in the box or else there is a green marble in the box, but not both This statement means that if there is a yellow marble in the box, then there cannot also be a green marble in the box.Similarly, if there is a green marble in the box, then there cannot also be a yellow marble in the box.</p>
</li>
<li>
<p>There is a green marble in the box if and only if there is a purple marble in the box This statement means that if there is a green marble in the box, then there must also be a purple marble in the box.Conversely, if there is a purple marble in the box, then there must also be a green marble in the box.</p>
</li>
<li>
<p>There is a purple marble in the box if and only if there is an olive marble in the box This statement means that if there is a purple marble in the box, then there must also be an olive marble in the box.Conversely, if there is an olive marble in the box, then there must also be a purple marble in the box.Now, let's consider the conclusion: Conclusion: If there is a brown marble in the box then there is an olive marble in the box Based on the statements we've been given, we can logically deduce the conclusion as follows:</p>
</li>
</ol>
<p>1.If there is a brown marble in the box, then there must also be a yellow marble in the box (statement 1) 2. If there is a yellow marble in the box, then there cannot also be a green marble in the box (statement 2) 3.If there is a green marble in the box, then there must also be a purple marble in the box (statement 3) 4. If there is a purple marble in the box, then there must also be an olive marble in the box (statement 4) Therefore, if there is a brown marble in the box, then there must also be an olive marble in the box, which means the conclusion is true.Conclusion: True.
○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ chain construction.
The model constructs a chain of conditionals leading from the antecedent of the final conclusion to its consequent.However, it fails to account for the negation of the second conditional's consequent, leading to a faulty reasoning trace.</p>
<p>Problem 5:</p>
<p>Statements: 1.There is a purple marble in the box if and only if there is a yellow marble in the box.2. There is a yellow marble in the box if and only if there is a blue marble in the box.3. Either there is a blue marble in the box or else there is an orange marble in the box, but not both.Conclusion: Either there is a purple marble in the box or else there is an orange marble in the box, but not both.</p>
<p>Response:</p>
<p>Sure! Let's break this down step by step.</p>
<ol>
<li>
<p>"There is a purple marble in the box if and only if there is a yellow marble in the box" This statement implies that the presence of a purple marble in the box is dependent on the presence of a yellow marble in the box.If there is no yellow marble, there cannot be a purple marble.</p>
</li>
<li>
<p>"There is a yellow marble in the box if and only if there is a blue marble in the box" This statement implies that the presence of a yellow marble in the box is dependent on the presence of a blue marble in the box.If there is no blue marble, there cannot be a yellow marble.</p>
</li>
<li>
<p>"Either there is a blue marble in the box or else there is an orange marble in the box, but not both" This statement implies that the presence of a blue marble and an orange marble in the box are mutually exclusive.If there is a blue marble, there cannot be an orange marble, and vice versa.Now, let's consider the conclusion: "Either there is a purple marble in the box or else there is an orange marble in the box, but not both" Based on the first two statements, we know that if there is a purple marble, there must be a yellow marble, and if there is a yellow marble, there must be a blue marble.Therefore, if there is a purple marble, there must be a blue marble.Now, let's consider the third statement.If there is a blue marble, there cannot be an orange marble.Therefore, if there is a purple marble (which implies a blue marble), there cannot be an orange marble.</p>
</li>
</ol>
<p>Conclusion: True.</p>
<p>The conclusion logically follows from the given statements.
○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ chain construction.
The model constructs a chain of conditionals proving one case of the exclusive disjunction.However, it fails to account for the other conditional case, i.e. ¬ P → O, therefore failing to prove the logical validity of the conclusion.
○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ chain construction.
The model constructs a chain of conditionals leading from the antecedent of the final conclusion to its consequent.However, it makes a series of mistakes when constructing the chain of conditionals.For instance, it infers the absence of the green marble by denying the presence of the white marble, i.e.Blue → ¬ W; W → G; therefore Blue → ¬ G by assuming that ¬ W → ¬ G, which is a common logical error known as the fallacy of the inverse.</p>
<ol>
<li>
<p>The second statement tells us that either there is a yellow marble in the box or there is an olive marble in the box, but not both.</p>
</li>
<li>
<p>The third statement tells us that there is an olive marble in the box if and only if there is a black marble in the box.</p>
</li>
<li>
<p>From the second statement, we know that if there is a yellow marble in the box, then there cannot be an olive marble in the box.</p>
</li>
<li>
<p>From the third statement, we know that if there is an olive marble in the box, then there is a black marble in the box.</p>
</li>
<li>
<p>Since the second statement tells us that there cannot be both a yellow marble and an olive marble in the box, it follows that if there is a yellow marble in the box, then there cannot be an olive marble or a black marble in the box (because of the if and only if relationship in the third statement).Based on that conclusion, the model infers that the conclusion in the problem statement does not logically follow from the premises at hand.Statements: 1.There is a green marble in the box if and only if there is a purple marble in the box 2. If there is a purple marble in the box then there is a gray marble in the box.3. Either there is a gray marble in the box or else there is a yellow marble in the box, but not both.</p>
</li>
</ol>
<p>Conclusion: Either there is a green marble in the box or else there is a yellow marble in the box, but not both.</p>
<p>Response: Sure, I'd be happy to help!Here's my reasoning process for this problem:</p>
<ol>
<li>First, I'll start by analyzing the given statements: -Statement 1: "There is a green marble in the box if and only if there is a purple marble in the box" -This statement establishes a direct relationship between the presence of a green marble and a purple marble in the box.</li>
</ol>
<p>-Statement 2: "If there is a purple marble in the box then there is a gray marble in the box" -This statement establishes a conditional relationship between the presence of a purple marble and a gray marble in the box.</p>
<p>-Statement 3: "Either there is a gray marble in the box or else there is a yellow marble in the box, but not both" -This statement establishes an exclusive OR relationship between the presence of a gray marble and a yellow marble in the box.</p>
<ol>
<li>Now, let's consider the conclusion: "Either there is a green marble in the box or else there is a yellow marble in the box, but not both" 3. To determine whether the conclusion logically follows from the statements, I'll use a truth table analysis:
Statement 1 Statement 2 Statement 3 Conclusion 1 T T T T 2 T T F F 3 T F T F 4 F T T F 5 F T F F 6 F F T T 7 F F F F
As we can see from the truth table, the conclusion "Either there is a green marble in the box or else there is a yellow marble in the box, but not both" is true in all cases where Statement 1 and Statement 2 are true, and false in all cases where Statement 3 is false.</li>
</ol>
<p>Since Statement 1 and Statement 2 are always true, and Statement 3 is true in some cases, the conclusion logically follows from the statements.</p>
<p>Conclusion: True</p>
<p>Annotator Comments:</p>
<p>Reformulation of statement 1.</p>
<p>Reformulation of statement 2.</p>
<p>Reformulation of statement 3.</p>
<p>Repetition of conclusion.</p>
<p>Symbolic Strategy (invalid).</p>
<p>Construct truth table.</p>
<p>Summarize truth table.</p>
<p>Conclusion (invalid).</p>
<p>Final answer (correct).Statements:</p>
<p>1.There is a gray marble in the box if and only if there is a yellow marble in the box.2. Either there is a yellow marble in the box or else there is an olive marble in the box, but not both.</p>
<ol>
<li>There is an olive marble in the box if and only if there is a black marble in the box.</li>
</ol>
<p>Conclusion: If there is a yellow marble in the box then there is a black marble in the box.</p>
<p>Response: First, we need to understand the relationships between the marbles and the statements.Here's a summary:</p>
<p>-There is a gray marble in the box if and only if there is a yellow marble in the box -This statement creates a connection between the gray marble and the yellow marble.If one is present, the other is as well.</p>
<p>-Either there is a yellow marble in the box or else there is an olive marble in the box, but not both -This statement introduces two possible scenarios.Either the yellow marble is present, or the olive marble is present, but not both.</p>
<p>-There is an olive marble in the box if and only if there is a black marble in the box -This statement creates another connection, but this time between the olive marble and the black marble.If one is present, the other is as well.Now, let's see if the conclusion follows logically from these statements.</p>
<p>-If there is a yellow marble in the box then there is a black marble in the box -To prove this conclusion, we'll use the second statement to eliminate the possibility of the olive marble being present.Then, we'll use the third statement to connect the presence of the yellow marble with the presence of the black marble.</p>
<p>-If there is a yellow marble in the box, then by the second statement, the olive marble cannot also be present.</p>
<p>-If the olive marble is not present, then by the third statement, the black marble must be present.</p>
<p>-Therefore, if there is a yellow marble in the box, then there is a black marble in the box.○ ○ ○ ○ ○ chain construction and the ○ symbolic strategy.The model first constructs a chain of conditionals to prove the validity of the conclusion, linking relevant entities in premise two and three of the problem statement.Subsequently, the model "explains" its reasoning by employing the symbolic strategy, converting statements into formal logic and operating on them.Note that the model makes several logical errors on its way to prove the logical validity of the final conclusion.</p>
<ol>
<li>W ⊕ R 3. R ↔ P Now, let's derive the conclusion using these statements [...].</li>
</ol>
<p>Figure 3 :
3
Figure 3: The response (lower left box) of LLaMA-2-70B to problem 1 (top box) of the problem set, demonstrating</p>
<p>Figure 4 :
4
Figure 4: Instances where models generate sound reasoning traces that logically follow from the problem statement.For each inferential strategy, the ratio of sound reasoning traces (represented by the filled portion) to the overall application of that strategy (denoted by the unfilled bar) is depicted.Ratios are expressed as percentages above the corresponding filled section.Note that LLaMA-2-7B is not displayed as it does not exhibit sound reasoning.</p>
<p>Figure 6 :
6
Figure 6: The response (lower left box) of LLaMA-2-70B to problem 7 (top box) of the problem set, illustrating</p>
<p>Figure 8 :
8
Figure 8: An exemplary model response of Mistral-7B-Instruct (lower left box) to problem 9 (top box) illustrating</p>
<p>Figure 9 :
9
Figure 9: The response (lower left box) of LLaMA-2-70B to problem 9 (top box) of the problem set, illustrating○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ chain construction.The model correctly constructs a chain of conditionals leading from the antecedent of the final conclusion to its consequent.</p>
<p>Figure 10 :
10
Figure 10: The response (lower left box) of LLaMA-2-70B to problem 7 (top box) of the problem set, illustrating</p>
<p>Figure 11 :
11
Figure 11: The response (lower left box) of LLaMA-2-13B to problem 10 (top box) of the problem set, illustrating</p>
<p>Figure 12 :
12
Figure 12: The response (lower left box) of LLaMA-2-70B to problem 5 (top box) of the problem set, illustrating</p>
<p>Figure 13 :
13
Figure 13: The response (lower left box) of LLaMA-2-70B to problem 12 (top box) of the problem set, illustrating</p>
<p>7 .
7
Therefore, the conclusion does not logically follow from the given statements.</p>
<p>Figure 17 :
17
Figure 17: The response (lower left box) of Mistral-7B-Instruct to problem 6 (top box) of the problem set, illustrating the ○ concatenation strategy.Mistral-7B-Instruct concatenates the intermediate conditional conclusion (Y → ¬ O) and the third premise of the problem statement (O ↔ B) to form the concatenated conclusion Y → ¬ (O ↔ B).Based on that conclusion, the model infers that the conclusion in the problem statement does not logically follow from the premises at hand.</p>
<p>Figure 18 :
18
Figure 18: The response (lower left box) of LLaMA-2-70B to problem 3 (top) of the problem set, illustrating the ○ symbolic strategy.The model constructs a truth table to infer the validity of the conclusion given in the problem statement.However, the model produces errors in the truth table, resulting in flawed reasoning.</p>
<p>Conclusion 1 :
1
Gray marble ↔ Yellow marble -Statement 2: Yellow marble ∨ !Olive marble -Statement 3: Olive marble ↔ Black marble -Conclusion: Yellow marble → Black marble Reasoning steps: 1. Given: Yellow marble → Black marble (conclusion) 2. Given: Olive marble ↔ Black marble (statement 3) 3. Given: Yellow marble ∨ !Olive marble (statement 2) 4. Eliminate olive marble: !Olive marble (from statement 2) 5. Given: Gray marble ↔ Yellow marble (statement 1) 6. Eliminate gray marble: !Gray marble (from statement 1) 7. Combine 4 and 6: Yellow marble → !Gray marble (from statements 1 and 2) 8. Combine 7 and 3: Yellow marble → Black marble (from statements 2, 3, and 7) 9. Conclusion: True.These steps show that the conclusion follows logically from the given statements.</p>
<p>Figure 19 :
19
Figure 19: The response (lower left box) of Zephyr-7B-β to problem 6 (top box) of the problem set, illustrating○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○ ○</p>
<p>Table 1 :
1
Relative occurrences of inferential strategies employed by the different language models when solving the problems of propositional logic.All values reflect average percentages, calculated over five random seeds, with standard deviations reported in Table4in the appendix.Strategies that a model favors are highlighted in bold.
ModelSupposition FollowingChain Con-structionCompound ConclusionConcatenation StrategySymbolic StrategyCorrect AnswerSound ReasoningZephyr-7B-β60.0% (55.1) 18.3% (17.3) 10.0% (8.9)1.7% (1.4)20.0% (17.3) 45.0 ± 15.5 25.0 ± 10.5Mistral-7B-Instruct35.0% (38.4) 10.0% (10.7) 35.0% (38.4) 3.3% (3.4)8.3% (9.1)55.0 ± 10.025.0 ± 7.5LLaMA-2-7B20.0% (50.2) 20.0% (30.2) 6.7% (10.9)3.3% (5.4)1.7% (3.3)46.7 ± 6.70.0 ± 0.0LLaMA-2-13B28.3% (35.7) 36.7% (46.9) 6.7% (8.7)6.7% (8.7)0.0% (0.0)40.0 ± 8.215.0 ± 6.2LLaMA-2-70B45.0% (42.3) 50.0% (46.8) 3.3% (2.9)1.7% (1.8)6.7% (6.2)56.7 ± 6.231.7 ± 9.7Human Reasoner  †− (21.0)− (25.0)− (19.0)− (0.0)− (0.0)100 ± 0.0−</p>
<p>Table 2 :
2
Cohen's Kappa values to assess the inter-annotator agreement across different models and label categories.</p>
<p>Table 3 :
3
Tunstall et al. (2023)ls used in this study.The context length refers to the base model's training.Tokens relate to the number of tokens in the pre-training data only.We use the following abbreviations for the fine-tuning procedure: supervised fine-tuning (SFT), reinforcement learning with human feedback (RLHF), distilled supervised fine-tuning (dSFT), and AI feedback through preferences (AIF).Information about the Llama 2 family is taken from Touvron et al. (2023), specifications for Mistral-7B-Instruct are provided byJiang et al. (2023).For Zephyr-7B-β, we consider the work ofTunstall et al. (2023).Dashes represent cases in which we could not find the respective information.
strategy identified in our study, as depicted in fig-ures 6-19. Each figure is organized with the prob-lem statement at the top, the model's responseon the lower left, and the annotators' commentsto the lower right. For an extensive array ofmodel responses and annotations, we invite read-ers to explore our data repository at: hugging-face.co/datasets/mainlp/inferential_strategies.C.1 Supposition FollowingFigures from 6 to 8 demonstrate the applicationof supposition following by various models. Forinstance, Figure
More specifically, experiment one ofVan der Henst et al. (2002). a non-sequential manner. For half of the problems, the conclusions logically follow from the premises, whereas for the other half, they do not. To avoid the influence of external knowledge and ensure content neutrality,Van der Henst et al. (2002) framed the problems around the presence of colored marbles in a box, with colors assigned randomly to each entity within a problem.
https://huggingface.co/meta-llama
https://huggingface.co/mistralai/Mistral-7B-Instruct
https://huggingface.co/HuggingFaceH4/zephyr-7b-beta
AcknowledgementsWe would like to thank the MaiNLP lab members for their insightful feedback, specifically Diego Frassinelli, Rob van der Goot, Siyao Peng, Robert Litschko, Jian Lan, Daniela Teodorescu, Xinpeng Wang, Verena Blaschke, Elena Senger, Elisa Bassignana, and Max Müller-Eberstein.Furthermore, we would like to express our gratitude to Huangyan Shan for her valuable work and support in data annotation.Our appreciation extends to the anonymous reviewers for their comments and suggestions.Lastly, we acknowledge the support for BP through the ERC Consolidator Grant DIALECT 101043235.Statements:1.There is a blue marble in the box if and only if there is a red marble in the box.2. Either there is a red marble in the box or else there is a white marble in the box, but not both.3. There is a white marble in the box if and only if there is a pink marble in the box.Conclusion: If there is not a blue marble in the box then there is a pink marble in the box.Response: To determine if the conclusion logically follows from the given statements, let's first understand the given statements and the logical relationships between them:1. Statement 1: "There is a blue marble in the box if and only if there is a red marble in the box."-This statement is equivalent to saying that the presence of a blue marble and the presence of a red marble are mutually exclusive and exhaustive.2. Statement 2: "Either there is a red marble in the box or else there is a white marble in the box, but not both."-This statement tells us that there cannot be both a red and a white marble in the box.Statement 3: "There is a white marble in the box if and only if there is a pink marble in the box."-This statement is equivalent to saying that the presence of a white marble and the presence of a pink marble are mutually exclusive and exhaustive.Now let's reason about the conclusion: 1. Conclusion: "If there is not a blue marble in the box then there is a pink marble in the box."To determine if the conclusion logically follows from the given statements, we need to consider all possible cases: Case 1: There is no blue marble in the box and there is a red marble.-This case is not possible, as statement 1 tells us that a blue marble and a red marble cannot both be in the box.Case 2: There is no blue marble in the box and there is a white marble.-According to statement 3, if there is a white marble, then there must be a pink marble as well.Case 3: There is no blue marble in the box and there is a pink marble.-This case is not explicitly stated in the given statements, but it is possible based on the information provided.Based on our analysis, we cannot definitively say that the conclusion logically follows from the given statements because there is a possible case (Case 1) that contradicts the conclusion.However, we have found that the conclusion is true in at least one possible case (Case 3).Therefore, the conclusion "If there is not a blue marble in the box then there is a pink marble in the box" is not a logical consequence of the given statements, but it is true in at least one possible scenario.Conclusion: False.Annotator Comments:Repetition of statement 1.Reformulation of statement 1.Repetition of statement 2.Reformulation of statement 2.Repetition of statement 3.Reformulation of statement 3.Repetition of conclusion.Supposition following (invalid).Supposition.Supposition.Supposition.Conclusion (invalid).Final answer (incorrect).The model successively assumes combinations of marbles in the box, and infers the immediate consequences from the premises provided.However, it does not extend its reasoning beyond the direct outcomes of each supposition, thereby failing to deduce the validity of the conclusion.Problem 9:Statements: 1.There is a purple marble in the box if and only if there is a blue marble in the box.2. There is a blue marble in the box if and only if there is an olive marble in the box.3. Either there is an olive marble in the box or else there is a red marble in the box, but not both.4. Either there is a red marble in the box or else there is a green marble in the box, but not both.Conclusion: If there is a purple marble in the box then there is a green marble in the box.Response:First, let's analyze the given statements:1. Statement 1: There is a purple marble in the box if and only if there is a blue marble in the boxStatement 2:There is a blue marble in the box if and only if there is an olive marble in the box 3. Statement 3: Either there is an olive marble in the box or else there is a red marble in the box, but not both 4. Statement 4: Either there is a red marble in the box or else there is a green marble in the box, but not both Let's examine the relationship between the statements:From Statement 1 and Statement 2, we know that there is a purple marble in the box if and only if there is a blue marble in the box, and there is a blue marble in the box if and only if there is an olive marble in the box.Therefore, we can conclude that there is a purple marble in the box if and only if there is an olive marble in the box.However, the conclusion states that if there is a purple marble in the box, then there is a green marble in the box.This is not logically following from the given statements.Conclusion: False.Annotator Comments:Reformulation of statement 1.Reformulation of statement 2.Reformulation of statement 3.Reformulation of statement 4.Compound Strategy (sound).List biconditional statements:Final answer (incorrect).Statements: 1.There is a purple marble in the box if and only if there is a blue marble in the box.2. There is a blue marble in the box if and only if there is an olive marble in the box.3. Either there is an olive marble in the box or else there is a red marble in the box, but not both.4. Either there is a red marble in the box or else there is a green marble in the box, but not both.Conclusion: If there is a purple marble in the box then there is a green marble in the box.Response:Sure! Let's break it down step by step.1. "There is a purple marble in the box if and only if there is a blue marble in the box" This statement implies that the presence of a purple marble in the box is dependent on the presence of a blue marble in the box.If there is no blue marble, there cannot be a purple marble.2. "There is a blue marble in the box if and only if there is an olive marble in the box" This statement implies that the presence of a blue marble in the box is dependent on the presence of an olive marble in the box.If there is no olive marble, there cannot be a blue marble.3. "Either there is an olive marble in the box or else there is a red marble in the box, but not both" This statement implies that the presence of an olive marble and a red marble in the box are mutually exclusive.If there is an olive marble, there cannot be a red marble, and vice versa.4. "Either there is a red marble in the box or else there is a green marble in the box, but not both" This statement implies that the presence of a red marble and a green marble in the box are also mutually exclusive.If there is a red marble, there cannot be a green marble, and vice versa.Now, let's evaluate the conclusion: Conclusion: If there is a purple marble in the box then there is a green marble in the box Based on the given statements, we know that the presence of a purple marble is dependent on the presence of a blue marble, and the presence of a blue marble is dependent on the presence of an olive marble.Therefore, if there is a purple marble in the box, it means that there is also a blue marble and an olive marble in the box.Using the third statement, we know that the presence of an olive marble means that there cannot be a red marble in the box.Therefore, if there is a purple marble in the box, there cannot be a red marble.Using the fourth statement, we know that the absence of a red marble means that there must be a green marble in the box.Therefore, if there is a purple marble in the box, there must be a green marble in the box as well.Conclusion: TrueSo, the conclusion logically follows from the given statements.Statements: 1.Either there is an olive marble in the box or else there is a brown marble in the box, but not both.2. There is a brown marble in the box if and only if there is a gray marble in the box.3. Either there is a gray marble in the box or else there is a maroon marble in the box, but not both.Conclusion: If there is not an olive marble in the box then there is a maroon marble in the box.Response:To determine if the conclusion logically follows from the given statements, we need to use the principles of logical reasoning.Let's analyze the statements one by one and then consider the conclusion.1. Statement 1: Either there is an olive marble in the box or else there is a brown marble in the box, but not both 2. Statement 2: There is a brown marble in the box if and only if there is a gray marble in the box 3. Statement 3: Either there is a gray marble in the box or else there is a maroon marble in the box, but not both First, let's understand the implications of Statement 2. It states that if there is a brown marble in the box, then there is a gray marble in the box, and conversely, if there is a gray marble in the box, then there is a brown marble in the box.This is an if and only if statement, meaning both conditions are equivalent.Now, let's consider Statement 1.It states that there is either an olive marble or a brown marble in the box, but not both.Since Statement 2 tells us that a brown marble implies a gray marble, it follows that if there is no olive marble (i.e., there is a brown marble), then there must be a gray marble.However, Statement 3 tells us that there is either a gray marble or a maroon marble in the box, but not both.Therefore, if there is a gray marble, then there cannot be a maroon marble.So, based on the given statements, if there is no olive marble in the box, then there is a brown marble (from Statement 1 and Statement 2), which implies a gray marble (from Statement 2), and since there is no olive marble and there is a gray marble, there cannot be a maroon marble (from Statement 3).Therefore, the conclusion follows logically from the given statements.Conclusion: True.○ ○ ○ ○ ○ supposition following.Based on the first two premises of the problem description, the model first draws a compound conclusion, establishing that a gray marble follows from the absence of an olive marble.Subsequently, it uses this intermediate conclusion, together with the third premise, to draw another compound conclusion about the absence of the maroon marble.The model then switches to supposition following, tracing the consequences of the absence of the olive marble, inferring the final conclusion that there cannot be a maroon marble.However, despite the model's correct reasoning, it deduces the wrong answer: "True".
Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models. Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju, ArXiv:2402.046142024</p>
<p>Introducing claude. Anthropic Research. Anthropic, 2023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, 10.1609/aaai.v38i16.29720Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, 10.1073/pnas.2218523120ArXiv:2206.14576Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, 10.1207/s15516709cog2303_1Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc. Monica Bucciarelli and P.n. Johnson-Laird2020. 199933Ilya Sutskever, and Dario Amodei</p>
<p>Reasoning strategies for suppositional deductions. R M Byrne, S J Handley, 10.1016/s0010-0277(96)00720-2Cognition. 6211997</p>
<p>Faithful reasoning using large language models. Antonia Creswell, Murray Shanahan, ArXiv:2208.142712022</p>
<p>Language models show human-like content effects on reasoning tasks. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Hannah R Chan, Antonia Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.48550/arXiv.2207.07051ArXiv:2207.070512023</p>
<p>Biases in Individual Decision-Making. Andrew M Davis, 10.1002/9781119138341.ch5The Handbook of Behavioral Operations. John Wiley &amp; Sons, Ltd2018</p>
<p>A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. M H Tiwalayo Eisape, Ishita Tessler, Fei Dasgupta, Sjoerd Sha, Tal Van Steenkiste, Linzen, 10.48550/arXiv.2311.00445ArXiv:2311.004452023</p>
<p>Bias in human reasoning: Causes and consequences. Bias in human reasoning: Causes and consequences. Jonathan B T St, Evans, Pages: ix, 145. Gerd Gigerenzer and Peter M. Todd. Hillsdale, NJ, US; New York, NY, USOxford University Press1989. 1999416Simple heuristics that make us smart. Simple heuristics that make us smart. Pages: xv</p>
<p>The Cambridge Handbook of Thinking and Reasoning. Keith J Holyoak, Robert G Morrison, ID: zn- bkHaC8QeMC2005Google-Books</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Patrick J Hurley, A Concise Introduction to Logic. Boston, MACENGAGE Learning Custom Publishing201111th edition edition</p>
<p>Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?. Alon Jacovi, Yoav Goldberg, 10.18653/v1/2020.acl-main.386Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, 10.48550/arXiv.2310.06825Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2023Mistral 7B. ArXiv:2310.06825 [cs</p>
<p>Mental models: towards a cognitive science of language, inference, and consciousness. P N Johnson-Laird, 1986Harvard University PressUSA</p>
<p>Daniel Kahneman, Thinking, Fast and Slow: Daniel Kahneman. Penguin, London20121st edition edition</p>
<p>Paul Daniel Kahneman, Amos Slovic, Tversky, 10.1017/CBO9780511809477Judgment under Uncertainty: Heuristics and Biases. CambridgeCambridge University Press1982editors</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Measuring faithfulness in chainof-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam Mccandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, ; Samuel, R Bowman, Ethan Perez, ArXiv:2307.13702Jan Brauner,. 2023Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan</p>
<p>The language of prompting: What linguistic properties make a prompt successful?. Alina Leidinger, Robert Van Rooij, Ekaterina Shutova, 10.18653/v1/2023.findings-emnlp.618Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Defining and Describing Reason. Jacqueline P Leighton, 10.1017/CBO9780511818714.001The Nature of Reasoning. Jacqueline P Leighton, Robert J Sternberg, CambridgeCambridge University Press2003</p>
<p>Qing Lyu, Marianna Apidianaki, Chris Callison-Burch, 10.1162/coli_a_00511Towards Faithful Model Explanation in NLP: A Survey. Computational Linguistics. 2024</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, IJCNLP-AACL 2023The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics. 2023</p>
<p>Dissociating language and thought in large language models. Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, 10.1016/j.tics.2024.01.011Trends in Cognitive Sciences. 2024</p>
<p>Walk the talk? measuring the faithfulness of large language model explanations. Katie Matton, Robert Ness, Emre Kiciman, ICLR 2024 Workshop on Reliable and Responsible Foundation Models. 2024</p>
<p>The debate over understanding in ai's large language models. Melanie Mitchell, David C Krakauer, 10.1073/pnas.2215907120Proceedings of the National Academy of Sciences. 12013e22159071202023</p>
<p>Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching Small Language Models How to Reason. Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, 10.48550/arXiv.2311.11045ArXiv:2311.11045</p>
<p>Models overview. OpenAI Research. 2023OpenAI</p>
<p>FaiRR: Faithful and robust deductive reasoning over natural language. J Lance, Rips ; Soumya, Harman Sanyal, Xiang Singh, Ren, 10.18653/v1/2022.acl-long.77Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandThe MIT Press1994. 20221The Psychology of Proof: Deductive Reasoning in Human Thinking. Association for Computational Linguistics</p>
<p>Deductive reasoning and strategies. Deductive reasoning and strategies. Walter Schaeken, Gino De Vooght, 2000Lawrence Erlbaum Associates Publishers321Mahwah, NJ, USAndré Vandierendonck, and Géry d'Ydewalle. Pages: xiv</p>
<p>Cognitive effects in large language models. Jonathan Shaki, Sarit Kraus, Michael Wooldridge, ECAI 2023. IOS Press2023</p>
<p>Do large language models show decision heuristics similar to humans? a case study using gpt-3.5. G Suri, L R Slater, A Ziaee, M Nguyen, 10.1037/xge0001547Journal of Experimental Psychology: General. 15342024</p>
<p>Challenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption. Alaina N Talboy, Elizabeth Fuller, 10.48550/arXiv.2304.01358ArXiv:2304.013582023</p>
<p>Zephyr: Direct Distillation of LM Alignment. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M Rush, Thomas Wolf, 10.48550/arXiv.2310.16944ArXiv:2310.169442023</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-ofthought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Strategies in sentential reasoning. Jean-Baptiste Van Der Henst, Yingrui Yang, P N Johnson-Laird, 10.1207/s15516709cog2604_2Cognitive Science. 2642002</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, 10.1037/h0060520Advances in Neural Information Processing Systems. Curran Associates, Inc. R. S. Woodworth and S. B. Sells2022. 193535451An Atmosphere Effect in Formal Syllogistic Reasoning</p>
<p>Logical Reasoning over Natural Language as Knowledge Representation: A Survey. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria, 10.48550/arXiv.2303.12023ArXiv:2303.120232023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, 10.1145/3664194ACM Comput. Surv. Just Accepted. 2024</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, ArXiv:2205.010682022</p>
<p>Statements: 1. Purple iff blue. </p>
<p>Conclusion: If purple then green. Problem 10: Statements: 1. Brown iff yellow. </p>
<p>Yellow xor green. </p>
<p>Green iff purple. </p>
<p>Conclusion: If brown then olive. Problem 11: Statements: 1. Red iff maroon. Purple iff olive</p>
<p>. Olive Green Xor, </p>
<p>Maroon iff green. </p>
<p>Conclusion: If red then brown. Olive Xor Brown, Problem 12: Statements: 1. Blue iff brown</p>
<p>. Brown Xor White, </p>
<p>Conclusion: If blue then purple. Green iff purple. Statements12</p>
<p>There is a green marble in the box if and only if there is a purple marble in the box Conclusion: If there is a blue marble in the box then there is a purple marble in the box. Response: Sure! Let's break down the problem and think through each step. </p>
<p>There is a blue marble in the box if and only if there is a brown marble in the box. First, we have the statement. This means that if there is a blue marble in the box. then there must be a brown marble in the box, and vice versa</p>
<p>There is a white marble in the box if and only if there is a green marble in the box. Next, we have the statement. This means that if there is a white marble in the box. then there must be a green marble in the box, and vice versa</p>
<p>Either there is a brown marble in the box or else there is a white marble in the box, but not both. This means that if there is a brown marble in the box. then there cannot be a white marble in the box, and vice versa</p>
<p>There is a green marble in the box if and only if there is a purple marble in the box. This means that if there is a green marble in the box. then there must be a purple marble in the box, and vice versa</p>
<p>Let's think about how we can use the given statements to determine whether this conclusion is true or false. Now, If there is a blue marble in the box then there is a purple marble in the box</p>
<p>We know that if there is a blue marble in the box. then there must be a brown marble in the box (from statement 1</p>
<p>We also know that if there is a brown marble in the box, then there cannot be a white marble in the box. 3</p>
<p>. Therefore, if there is a blue marble in the box, then there cannot be a white marble in the box</p>
<p>We also know that if there is a white marble in the box. then there must be a green marble in the box (from statement 2</p>
<p>. Therefore, if there is a blue marble in the box, then there cannot be a green marble in the box</p>
<p>However, we know that if there is a green marble in the box. then there must be a purple marble in the box (from statement 4</p>
<p>Conclusion: True. In summary, we have shown that if there is a blue marble in the box, then there must be a purple marble in the box, using the given statements. Therefore, the conclusion logically follows from the statements. Annotator Comments: Reformulation of statement 1. Reformulation of statement 2. Reformulation of statement 3. Reformulation of statement 4. Therefore, if there is a blue marble in the box, then there must be a purple marble in the box. Repetition of conclusion. Chain construction (invalid</p>
<p>. W → G Blue, → , G , fallacy of the inverse</p>
<p>) Final answer (correct). Conclusion (invalid). G → P Blue, → , invalid</p>            </div>
        </div>

    </div>
</body>
</html>
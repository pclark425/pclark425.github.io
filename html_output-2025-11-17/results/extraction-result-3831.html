<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3831 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3831</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3831</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3" target="_blank">ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> ClinicalGPT is presented, a language model explicitly designed and optimized for clinical scenarios that significantly outperforms other models in these tasks, highlighting the effectiveness of the approach in adapting large language models to the critical domain of healthcare.</p>
                <p><strong>Paper Abstract:</strong> Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3831",
    "paper_id": "paper-ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ClinicalGPT: Large Language Models Finetuned with DIVERSE MEDICAL DATA AND COMPREHENSIVE EVALUATION</h1>
<p>Guangyu Wang<em>, Guoxing Yang, Zongxin Du, Longjun Fan, Xiaohu Li<br>State Key Laboratory of Networking and Switching Technology<br>Beijing University of Posts and Telecommunications<br>Beijing, China<br></em>guangyu.wang24@gmail.com</p>
<h4>Abstract</h4>
<p>Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domainspecific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare.</p>
<p>Keywords deep learning $\cdot$ large language model $\cdot$ medical knowledge $\cdot$ electronic medical record $\cdot$ text generation</p>
<h2>1 Introduction</h2>
<p>In recent years, the paradigm of pre-training and fine-tuning large language models has brought about significant advancements in Natural Language Processing (NLP) domain. The earliest approaches like BERT[1], utilized optimized objectives like Masked Language Model (MLM) to pre-train on large text corpora such as BookCorpus[2], in an unsupervised manner to learn good representations. These representations can be fine-tuned and adapted to one or more specific downstream tasks to improve their performance. Further research aims to develop competent generalists, i.e. generalized systems that can perform multiple NLP tasks without the need for a manually labeled training dataset for each task. For instance, T5[3] treats multiple NLP tasks as text-to-text transformation tasks and leverages an encoderdecoder architecture, achieving promising results such as text classification, question answering, and summarization, though with a larger number of parameters. In contrast, GPT-3[4] uses large auto-regressive model for few-shot predictions, improving performance without parameter fine-tuning by incorporating few-shot demonstrations through text interaction with the model. PALM[5] is Transformers-based and Pathways-enabled large-scale language model. Compared to other models, PALM is more resource-efficient in terms of computation and achieves state-of-the-art few-shot results across hundreds of natural language, code, and mathematical reasoning tasks.
With their substantial generalization capabilities in NLP tasks, large pre-trained models are increasingly utilized for various tasks and facilitating human interaction through dialogue models. LaMDA [6], a transformer-based model designed for dialogues, leverages annotated data and external knowledge to augment its helpfulness and role consistency. InstructGPT [7] aligns with user intent across various tasks through fine-tuning and reinforcement learning with human feedback, resulting in improved truthfulness and reduced toxicity in output generation. ChatGPT can simulate human interaction, write abstracts or create movie scripts in response to prompts, driving the AI revolution. Large language models are also effective for writing assistance and generating efficient code for programmers.</p>
<p>As we know, medicine and health care still face many challenges, including aging population, lack of equitable access, rising costs, doctor and nurse burnout, and global pandemics. Information technology has the potential to transform modern medicine by offering new tools and insights for healthcare, with ChatGPT and GPT-4 promising to revolutionize clinical decision support, clinical trial recruitment, clinical data management, research support, patient education [8, 9]. Google researchers developed FlanPaLM, an instruction-tuned variant of PaLM, showing improved task performance via natural language instructions. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy in MultiMedQA multiple-choice datasets, but remains outperformed by clinicians. Recent prospective suggests generalist medical AI (GMAI) using foundation models may disrupt task-specific paradigms, enabling versatile applications like interactive note-taking, bedside decision support, and patient chatbots [10].However, there are considerable challenges to overcome in applying generative language models to the medical field. The output of generative language models may have factual errors, logic inconsistencies, and problems with coherence, such as citing article references that do not exist [11]. The models have limited reasoning abilities and lack grounding in real-world experience, leading to general and vague responses. ChatGPT has been found lacking in depth and insight [4], likely due to its alignment model used for reward-based training, which produces overly generalized answers that lack medical expertise. This evidence implies that employing these technologies in the medical field brings unique hurdles, such as the necessity for high accuracy, interpretability, and secure handling of sensitive health data.</p>
<p>In this study, we present ClinicalGPT, a large language model that is specifically designed for tasks across medical applications. To train the model, we leverage extensive and diverse datasets consisting of real-world medical records, allowing us to transform domain-specific knowledge to the model. In addition, we establish a comprehensive evaluation framework that includes medical knowledge question-answering, medical examinations, patient consultations, and medical record analysis. By utilizing parameter-efficient fine-tuning methods, we were able to further improve the performance of ClinicalGPT. The results demonstrate that ClinicalGPT outperform existing models in term of performance, thus confirming the effectiveness of our approach.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of ClinicalGPT.</p>
<h2>2 Methods</h2>
<h3>2.1 Dataset</h3>
<p>In this study, we incorporated a large and diverse medical datasets including cMedQA2, cMedQA-KG, MD-EHR, MEDQA-MCMLE, and MedDialog, for the training and evaluation of our model.</p>
<p>The cMedQA2 dataset [12] is a Chinese medical question-and-answer dataset that consists of 120k questions and 226k answers. The data is aggregated from a Chinese medical question-and-answer online forum<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>. For training purposes, we followed the original dataset partition as proposed by the author, and then we randomly selected one answer per question. We annotated 10k questions from the training set for training reward models and used 4k questions from the validation set for reinforcement learning. We sampled questions from the testing set for evaluation.</p>
<p>The cMedQA-KG is a medical question-answer dataset which are curated based on knowledge graphs. It is established on three knowledge graphs: $\mathrm{cMeKG}^{2}$, xywy- $\mathrm{KG}^{3}$, and 39Health- $\mathrm{KG}^{4}$.These knowledge graphs cover comprehensive medical entities such as disease, medication, and symptom, and their relationships. Detailed descriptions of the knowledge graphs can be found in Appendix A. We have designed templates (see Appendix B) to transform each knowledge triplet into fine-tuning instruction data, i.e text-to-text pair for text generation, yielding 100k question-answer pairs. cMedQA-KG is used exclusively for training purposes.</p>
<p>The MEDQA-MCMLE dataset is a subset of the original MEDQA dataset [13], consisting of Chinese medical examination questions in a multiple-choice format. It includes 34 k questions, each offering multiple choices, typically 4 or 5 . We have followed the original author's division of the dataset into training, validation, and testing sets. As this dataset is derived from professional medical board examinations, it effectively evaluates applied knowledge, clinical reasoning, and patient-centric skills.</p>
<p>The MedDialog dataset [14] is a data collection of multi-turn medical conversations obtained from an online platform ${ }^{5}$. MedDialog comprises 1.1 million dialogues and 4 million utterances. Due to the large volume of data, we have randomly sampled $100 \mathrm{k}, 1 \mathrm{k}$, and 1 k dialogues for the training, validation, and testing sets, respectively. These multi-turn dialogues closely resemble real interactions between doctors and patients, aiding the model in understanding the process of clinical inquiry and decision-making.</p>
<p>The MD-EHR dataset is comprised of electronic health records from multicenter, large-scale hospitals in China. This dataset contains 100k records covering a range of disease groups, including Respiratory, Digestive, Urinary, Psychiatry, Neurology, Gynecology, and Hematology.</p>
<p>Each record within the MD-EHR dataset provides a comprehensive overview of the patient's complaints, medical history, findings from physical examinations, ancillary test results, and the final diagnosis. We have divided the dataset into three sets: 2,000 records for the validation set, 2,000 records for the testing set, and the remaining entries for the training set. Following T5[3], we transformed the medical records into a text generation task by concatenating the notes from the records as input and using the diagnosis as the output.</p>
<h1>2.2 Finetuning</h1>
<p>We adopt the T5 model's [3] strategy of utilizing text generation grounded in language models to complete all tasks in our study. Language models, pre-trained on extensive corpora, have demonstrated a remarkable ability to understand and generate human-like text [4]. These models calculate the probability of a sequence of words in a text, $T=\left(w_{1}, w_{2}, \ldots, w_{L}\right)$. Specifically, the casual language model calculates the probability of the text $T$ that can be formulated as $p(T)=p\left(w_{1}\right) p\left(w_{2} \mid w_{1}\right) \ldots p\left(w_{L} \mid w_{1}, w_{2}, \ldots, w_{L-1}\right)$, where $L$ represents the length of the text. Several large language models, such as BLOOM, GLM, and others, are available for public use.</p>
<p>To enhance the utility of large models for downstream tasks, we apply an instruction-tuning approach with supervised fine tuning (SFT). The language model $p_{\theta}$ is trained to generate a response $R=v_{1: n}$ for a given input prompt $I=w_{1: m}$, optimizing the likelihood $p_{\theta}(R \mid I)=p_{\theta}\left(v_{1: n} \mid w_{1: m}\right)$, where $n$ and $m$ represent the lengths of the response and input prompt, respectively. Thus, the loss function is $\frac{1}{n} \sum_{i=m+1}^{m+n}-\log p_{\theta}\left(w_{i} \mid w_{1}, \ldots, w_{i-1}\right)$.
To incorporate domain-specific knowledge into LLMs, we turn to knowledge graphs (KGs) specific to the domain for constructing prompt-response pairs. KGs capture knowledge in the form of structured triples $(s, r, o)$, where $s$ denotes the subject, $r$ the relationship, and $o$ the object. An example of such a triple could be (Cough, SymptomOf, Pneumonia). We leverage a set of manually designed templates to transform these triples into question-answer pairs, rendering them suitable for instruction tuning. The manually designed templates can be found in Appendix B.</p>
<h3>2.3 Reward model</h3>
<p>Existing works have demonstrated that reinforcement learning can incorporate human feedback to enhance large language models. For instance, WebGPT [15] is a browser-assisted question-answering system that utilizes human feedback for performance improvement. InstructGPT also [7] to align with human feedback via reinforcement learning for helpful and safe response generation.</p>
<p>We follow the work of [7], constructing a reward model (RM) $r_{\mu}$ to furnish the reward signal crucial for the reinforcement learning process. We employ rank-based training for the RM. Human labelers rank responses for a given input prompt</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">2</a></sup></p>
<p>$I$, generating a comparison pair for each prompt. For a comparison pair with a human-preferred response $R_{w}$ and a less preferred response $R_{l}$, the loss is given by $-\log \left(\sigma\left(r_{\mu}\left(I, R_{w}\right)-r_{\mu}\left(I, R_{l}\right)\right)\right)$.</p>
<h1>2.4 Reinforcement learning</h1>
<p>We adopt the method proposed by Stiennon et al. [16], leveraging reinforcement learning to enhance the fine-tuned models with the objective of generating high-quality and helpful outputs, as well as improving the generation of medical texts, thereby aiding in the accurate description and treatment of patient conditions.
We utilize the trained reward model as the reward function. In order to prevent the model from deviating too far from its initial state, we employ Proximal Policy Optimization (PPO) as our optimization strategy. Specifically, we incorporate a penalty term in the reward function that penalizes the KL divergence between the learned reinforcement learning policy, denoted as $\pi_{\phi}^{R L}$, and the original supervised model, $\pi^{S F T}$. This is to ensure that the final model does not deviate excessively from the original supervised model. The complete reward function is defined as follows: $R(x, y)=r_{\mu}(x, y)-\beta \log \left(\pi_{\phi}^{R L}(y \mid x) / \pi^{S F T}(y \mid x)\right)$, where $r_{\mu}(x, y)$ represents the output of the reward model and $\beta$ is the coefficient for KL divergence in the reward function. The loss function used in PPO optimization is given by: $L=r_{\mu} \hat{A}<em _old="{old" _phi__text="\phi_{\text">{t}-\beta K L\left[\pi</em>}}}, \pi_{\phi}\right]$, where $r_{\mu}$ is the reward function, $\hat{A<em _old="{old" _text="\text">{t}$ is an estimator of the advantage function, $\phi</em>$ is the current policy.}}$ represents the parameters of the policy at the previous step, and $\pi_{\phi</p>
<h2>3 Experiments and results</h2>
<h3>3.1 Implemented details</h3>
<p>We chose BLOOM-7B[17] as our base large language model, due to its open-source nature and multilingual support. For the supervised fine-tuning process, we set the learning rate to $5 \mathrm{e}-5$, with a batch size of 128 and a maximum length of 1,024 , training across 3 epochs. During the training of the reward model, we utilized the last feature vector of the final output sequence features as the text representation. Based on the fine-tuned model, we added a binary classification head to output the reward. We set the learning rate to $2 \mathrm{e}-5$, with a batch size of 128 , a maximum length of 1,024 , and training over 3 epochs. For the reinforcement learning process, we applied a learning rate of $1 \mathrm{e}-5$ and a maximum length of 1,024, training for 4000 steps. To efficiently train the large language model, we adopted LoRA (Low-Rank Approximated adapter)[18], a parameter efficient fine tuning method, with r of 8 , alpha of 32 , and dropout of 0.1 . To decrease memory usage and improve training speed, we employed the ZeRO-2 [19], and made use of both TF32 (TensorFloat-32) and BF16 (Bfloat16). We selected several instruction fine-tuned models for comparison, including ChatGLM-6B [20], LLAMA-7B[21] (fine-tuned on English and Chinese data), and BLOOM-7B [22] (fined-tuned on crosslingual tasks).</p>
<h3>3.2 Medical conversation</h3>
<p>We conducted performance evaluation of the medical conversation on the test set of MedDialog. To address the challenge of multiple rounds of conversation within each medical dialogue, we randomly truncated the dialogue at a certain round, discarding the subsequent dialogue, and using the historical dialogue prior to this round as input. The sample response is shown in Table 1. We used three evaluation metrics: BLEU[23], ROUGE[24], and GLEU, to assess the quality of the conversations. BLEU is a commonly used metric that compares a candidate translation with one or more reference translations based on n-gram precision. GLEU calculates the average score of different n-grams, providing a more comprehensive evaluation of the generated text. ROUGE, on the other hand, is a particularly useful metric for evaluating automatic summarization and machine translation, as it focuses on the recall aspect of generated summaries by comparing them with references.
The experimental results are presented in Table 2. It demonstrates that ClinicalGPT achieves outstanding performance on BLEU-1 and all ROUGE scores. ClinicalGPT comes second only to BLOOM-7B in terms of BLEU-2, BLEU-3, and BLEU-4. The superior ROUGE scores achieved by ClinicalGPT indicate that the responses generated by the model cover the information provided by the reference text more effectively.</p>
<h3>3.3 Medical examination</h3>
<p>In this study, the medical examination assessment using the MEDQA-MCMLE dataset was evaluated with the categories which are the highest frequencies in the dataset. The selected categories included Medical ethics, Respiratory system, Digestive system, Urinary system, Hematologic diseases, Rheumatic immune Diseases, Pediatric diseases,</p>
<p>Description of medical conditions and history
疾病：疑似皮肤paget病
(Disease: Suspected Paget's disease of the skin)
患病时长：大于半年
(Duration of illness: more than six months.)
病情描述：一直按湿疹来医已经好几年了
(Disease description: Has been treated as eczema for several years.)
希望获得的帮助：请医生给我一些治疗上的建议，目前病情是否需要手术？是否需要就诊？就诊前做哪些准
备？请问三院有收过这种病的吗？怎么医治？
(Help needed: Can the doctor provide me with some treatment suggestions? Does the current condition require surgery? Do I need to seek medical attention? What preparations should be made before seeking medical treatment? May I ask if the Third Hospital has ever received this disease? How to treat it?)
Dialogue
Assistant：根据您的病情，确实需要手术切除病灶，不过具体手术情况可能需要现场判断，建议您前来就诊后由专业医生为您制定治疗方案。
(Assistant:Based on your condition, it is indeed necessary to undergo surgery to remove the lesion, but the specific surgical situation may require on-site judgment. It is recommended that you come for treatment and have a professional doctor develop a treatment plan for you.)
User: 那就诊前需要准备些什么呢?
(User: What do you need to prepare before seeing a doctor?)
Assistant: 前往就诊前，您需要做好皮肤检查，包括观察皮肤颜色、质地、皮疹大小等。此外，您也需要带上病历资料、医生开的处方等，以便医生更好地为您诊断和治疗。
(Assistant: Before going to the clinic, you need to undergo a skin examination, including observing skin color, texture, rash size, etc. In addition, you also need to bring medical records, doctor's prescriptions, etc., so that the doctor can better diagnose and treat you.)</p>
<p>Table 1: A sample response in medical conversation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BLEU-1</th>
<th style="text-align: center;">BLEU-2</th>
<th style="text-align: center;">BLEU-3</th>
<th style="text-align: center;">BLEU-4</th>
<th style="text-align: center;">GLEU</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLAMA-7B</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">17.3</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">16.2</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM-7B</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">$\mathbf{4 . 4}$</td>
<td style="text-align: center;">$\mathbf{2 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 . 2}$</td>
<td style="text-align: center;">$\mathbf{2 . 4}$</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$\mathbf{1 3 . 9}$</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">$\mathbf{2 7 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 . 5}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparisons on medical conversation.
and Pharmacology. The models were fed with the form of questions and options as input, and the generated text was subsequently used to extract answers to compute accuracy. The sample response is shown in Table 3.</p>
<h1>Prompt:</h1>
<p>下述哪一项不符合SLE的血液系统改变？
(Which of the following blood system changes does not comply with SLE?)
回答选项：
(Answer options:)
选项A 血小板减少
(Option A Thrombocytopenia)
选项B 自细胞减少
(Option B: Self cell reduction)
选项C 自身免疫性溶血性贫血
(Option C Autoimmune hemolytic anemia)
选项D 正色素细胞性贫血
(Option D Positive Chromatophore anemia)
选项E 类白血病样改变
(Option E Leukemia like changes)</p>
<h2>Response:</h2>
<p>SLE是一种自身免疫性疾病，其血液系统改变包括血小板减少、自身免疫性溶血性贫血、正色素细胞性贫血等。而类白血病样改变是指骨髓中出现大量幼稚细胞，与SLE无关，因此选项E不符合SLE的血液系统改变。
(SLE is an autoimmune disease, whose blood system changes include thrombocytopenia, autoimmune hemolytic anemia, positive Chromatophore anemia, etc. Leukemia like changes refer to the presence of a large number of immature cells in the bone marrow, which is not related to SLE. Therefore, option E does not meet the hematological changes of SLE.)</p>
<p>Table 3: A sample response in medical examination.</p>
<p>The experimental results, as shown in Table 4, reveal that ClinicalGPT outperformed other LLMs such as LLAMA7B, ChatGLM-6B, and BLOOM-7B in all evaluated categories, boasting an average accuracy of 38.4. Specifically, ClinicalGPT achieved strong performance, exceeding the average scores of ChatGLM-6B, BLOOM-7B, and LLAMA7B with 19.9, 25.7, and 27.2 respectively. Among all categories, ClinicalGPT achieved the best score in Rheumatic immune with an accuracy of 47.4. Conversely, it underperformed in Respiratory and Digestive diseases, with accuracies of 26.1 and 36.9 , respectively. These findings suggest that while ClinicalGTP excels in understanding and generating</p>
<p>responses related to rheumatic immune system, further refinement is required to improve its performance in Respiratory and Digestive diseases.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Respiratory</th>
<th style="text-align: center;">Urinary</th>
<th style="text-align: center;">Digestive</th>
<th style="text-align: center;">Rheumatic immune</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">19.9</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA-7B</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">27.2</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM-7B</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">25.7</td>
</tr>
<tr>
<td style="text-align: left;">ClinicalGPT</td>
<td style="text-align: center;">$\mathbf{2 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparisons on medical examination.</p>
<h1>3.4 Diagnosis</h1>
<p>The diagnostic capabilities of LLMs (large language models) were evaluated on the testing set of MD-EHR. Disease groups were selected for evaluation, including Respiratory, Digestive, Urinary, Psychiatry, Neurology, Gynecology, and Hematology. The models were provided with concatenated notes from each medical record as input and generated text as output. The accuracy of the models was calculated by comparing the generated text with the diagnosis labels in the medical records. The sample response is shown in Table 5.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Prompt</span><span class="o">:</span>
<span class="err">主诉</span><span class="o">:</span><span class="w"> </span><span class="err">㨢痛</span><span class="mi">1</span><span class="err">大。</span>
<span class="o">(</span><span class="n">Complaint</span><span class="o">:</span><span class="w"> </span><span class="n">Abdominal</span><span class="w"> </span><span class="n">pain</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">day</span><span class="o">.)</span>
<span class="err">现病史和体格检查</span><span class="o">:</span><span class="w"> </span><span class="err">腹痛</span><span class="o">,</span><span class="w"> </span><span class="err">无明显规律</span><span class="o">,</span><span class="w"> </span><span class="err">有呕吐</span><span class="o">,</span><span class="w"> </span><span class="err">曾有发热</span><span class="o">,</span><span class="w"> </span><span class="err">现无发热</span><span class="o">,</span><span class="w"> </span><span class="err">无腹胀及异常哭闹等不适。尿量可</span><span class="o">,</span>
<span class="err">精神尚可</span><span class="o">,</span><span class="w"> </span><span class="err">食纳一服</span><span class="o">,</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="mi">20</span><span class="err">次</span><span class="sr">/分, P 110次/</span><span class="err">分</span><span class="o">,</span><span class="w"> </span><span class="err">神志清</span><span class="o">,</span><span class="w"> </span><span class="err">精神反应好</span><span class="o">,</span><span class="w"> </span><span class="err">呼吸平稳</span><span class="o">,</span><span class="w"> </span><span class="err">口唇红润</span><span class="o">,</span><span class="w"> </span><span class="err">喘无充血</span><span class="o">,</span><span class="w"> </span><span class="err">双肺呼</span>
<span class="err">吸音粗</span><span class="o">,</span><span class="w"> </span><span class="err">未闻及罗音</span><span class="o">,</span><span class="w"> </span><span class="err">心音有力</span><span class="o">,</span><span class="w"> </span><span class="err">律齐</span><span class="o">,</span><span class="w"> </span><span class="err">各瓣膜区未闻及杂音</span><span class="o">,</span><span class="w"> </span><span class="err">腹部软</span><span class="o">,</span><span class="w"> </span><span class="err">全腹压痛</span><span class="o">,</span><span class="w"> </span><span class="err">肝脾未及</span><span class="o">,</span><span class="w"> </span><span class="err">神经系统查体</span>
<span class="err">未见异常。</span>
<span class="o">(</span><span class="n">Current</span><span class="w"> </span><span class="n">medical</span><span class="w"> </span><span class="n">history</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">physical</span><span class="w"> </span><span class="n">examination</span><span class="o">:</span><span class="w"> </span><span class="n">Abdominal</span><span class="w"> </span><span class="n">pain</span><span class="o">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">obvious</span><span class="w"> </span><span class="n">pattern</span><span class="o">,</span><span class="w"> </span><span class="n">vomiting</span><span class="o">,</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">current</span>
<span class="n">fever</span><span class="o">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">discomfort</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">abdominal</span><span class="w"> </span><span class="n">distension</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">abnormal</span><span class="w"> </span><span class="n">crying</span><span class="o">.</span><span class="w"> </span><span class="n">Urinary</span><span class="w"> </span><span class="n">volume</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">acceptable</span><span class="o">,</span><span class="w"> </span><span class="n">mental</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="k">is</span>
<span class="n">acceptable</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">appetite</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">average</span><span class="o">.</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="n">times</span><span class="sr">/minute, P 110 times/</span><span class="n">minute</span><span class="o">,</span><span class="w"> </span><span class="n">clear</span><span class="w"> </span><span class="n">mind</span><span class="o">,</span><span class="w"> </span><span class="n">good</span><span class="w"> </span><span class="n">mental</span><span class="w"> </span><span class="n">response</span><span class="o">,</span><span class="w"> </span><span class="n">stable</span>
<span class="n">breathing</span><span class="o">,</span><span class="w"> </span><span class="n">red</span><span class="w"> </span><span class="n">lips</span><span class="o">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">congestion</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">throat</span><span class="o">,</span><span class="w"> </span><span class="n">thick</span><span class="w"> </span><span class="n">respiratory</span><span class="w"> </span><span class="n">sounds</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">both</span><span class="w"> </span><span class="n">lungs</span><span class="o">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">rale</span><span class="w"> </span><span class="n">heard</span><span class="o">,</span><span class="w"> </span><span class="n">strong</span><span class="w"> </span><span class="n">heart</span><span class="w"> </span><span class="n">sounds</span><span class="o">,</span>
<span class="n">regular</span><span class="w"> </span><span class="n">rhythm</span><span class="o">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">murmur</span><span class="w"> </span><span class="n">heard</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">valve</span><span class="w"> </span><span class="n">area</span><span class="o">,</span><span class="w"> </span><span class="n">soft</span><span class="w"> </span><span class="n">abdomen</span><span class="o">,</span><span class="w"> </span><span class="n">full</span><span class="w"> </span><span class="n">abdominal</span><span class="w"> </span><span class="n">tenderness</span><span class="o">,</span><span class="w"> </span><span class="n">liver</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">spleen</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">reached</span><span class="o">,</span>
<span class="n">and</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">abnormalities</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">found</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">nervous</span><span class="w"> </span><span class="n">system</span><span class="w"> </span><span class="n">examination</span><span class="o">.)</span>
<span class="err">检验检查</span><span class="o">:</span><span class="w"> </span><span class="err">外院查尿淀粉酶高</span><span class="o">,</span><span class="w"> </span><span class="err">道德</span><span class="o">+</span><span class="err">、腹平片</span><span class="o">:</span><span class="w"> </span><span class="err">脉梗阻。影像显示肝脏切面形态大小正常</span><span class="o">,</span><span class="w"> </span><span class="err">包膜光整</span><span class="o">,</span><span class="w"> </span><span class="err">实质回</span>
<span class="err">声均匀</span><span class="o">,</span><span class="w"> </span><span class="err">未探及明显异常回声。</span>
<span class="o">(</span><span class="n">Examination</span><span class="o">:</span><span class="w"> </span><span class="n">high</span><span class="w"> </span><span class="n">urine</span><span class="w"> </span><span class="n">Amylase</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="n">hospital</span><span class="o">,</span><span class="w"> </span><span class="n">influenza</span><span class="o">+,</span><span class="w"> </span><span class="n">abdominal</span><span class="w"> </span><span class="n">plain</span><span class="w"> </span><span class="n">film</span><span class="o">:</span><span class="w"> </span><span class="n">Bowel</span><span class="w"> </span><span class="n">obstruction</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">image</span>
<span class="n">shows</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">liver</span><span class="w"> </span><span class="n">section</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">normal</span><span class="o">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">capsule</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">smooth</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">parenchymal</span><span class="w"> </span><span class="n">echo</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">uniform</span><span class="o">.</span>
<span class="n">No</span><span class="w"> </span><span class="n">obvious</span><span class="w"> </span><span class="n">abnormal</span><span class="w"> </span><span class="n">echo</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">detected</span><span class="o">.)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Response</span><span class="o">:</span>
<span class="err">您好</span><span class="o">,</span><span class="err">根据您的症状和检查结果</span><span class="o">,</span><span class="err">您被诊断为阑尾炎。阑尾炎是一种常见的急性腹痛疾病</span><span class="o">,</span><span class="w"> </span><span class="err">通常需要进行手术</span>
<span class="err">治疗。建议您尽快进行手术治疗</span><span class="o">,</span><span class="err">以避免病情恶化。</span>
<span class="o">(</span><span class="n">Hello</span><span class="o">,</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">symptoms</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">examination</span><span class="w"> </span><span class="n">results</span><span class="o">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">diagnosed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">Appendicitis</span><span class="o">.</span><span class="w"> </span><span class="n">Appendicitis</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">common</span>
<span class="n">acute</span><span class="w"> </span><span class="n">abdominal</span><span class="w"> </span><span class="n">pain</span><span class="w"> </span><span class="n">disease</span><span class="o">,</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">usually</span><span class="w"> </span><span class="n">requires</span><span class="w"> </span><span class="n">surgical</span><span class="w"> </span><span class="n">treatment</span><span class="o">.</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">suggest</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">undergo</span><span class="w"> </span><span class="n">surgical</span><span class="w"> </span><span class="n">treatment</span><span class="w"> </span><span class="k">as</span>
<span class="n">soon</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">avoid</span><span class="w"> </span><span class="n">worsening</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">condition</span><span class="o">.)</span>
<span class="err">在手术前</span><span class="o">,</span><span class="err">我们会对您进行一些必要的准备工作</span><span class="o">,</span><span class="err">包括禁食和饮水</span><span class="o">,</span><span class="err">以及进行血液检查等。手术后</span><span class="o">,</span><span class="err">我们会对</span>
<span class="err">您进行密切观察和护理</span><span class="o">,</span><span class="err">以确保您的恢复情况良好。</span>
<span class="o">(</span><span class="n">Before</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">operation</span><span class="o">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">make</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">necessary</span><span class="w"> </span><span class="n">preparations</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">including</span><span class="w"> </span><span class="n">fasting</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">drinking</span><span class="w"> </span><span class="n">water</span><span class="o">,</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">well</span><span class="w"> </span><span class="k">as</span>
<span class="n">Blood</span><span class="w"> </span><span class="n">test</span><span class="o">.</span><span class="w"> </span><span class="n">After</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">surgery</span><span class="o">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">closely</span><span class="w"> </span><span class="n">observe</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">care</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">ensure</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">recovery</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">good</span><span class="o">.)</span>
<span class="err">同时</span><span class="o">,</span><span class="err">我们也建议您注意休息</span><span class="o">,</span><span class="err">避免剧烈运动和过度劳累</span><span class="o">,</span><span class="err">保持良好的饮食习惯和生活方式</span><span class="o">,</span><span class="err">以促进身体的康</span>
<span class="err">复。</span>
<span class="o">(</span><span class="n">At</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">time</span><span class="o">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">recommend</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">pay</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">rest</span><span class="o">,</span><span class="w"> </span><span class="n">avoid</span><span class="w"> </span><span class="n">vigorous</span><span class="w"> </span><span class="n">exercise</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">excessive</span><span class="w"> </span><span class="n">fatigue</span><span class="o">,</span><span class="w"> </span><span class="n">maintain</span>
<span class="n">good</span><span class="w"> </span><span class="n">dietary</span><span class="w"> </span><span class="n">habits</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">lifestyle</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">promote</span><span class="w"> </span><span class="n">physical</span><span class="w"> </span><span class="n">recovery</span><span class="o">.)</span>
<span class="err">如果您有任何疑问或需要进一步的帮助</span><span class="o">,</span><span class="err">请随时与我们联系。我们会尽最大努力为您提供最好的医疗服务。</span>
<span class="o">(</span><span class="n">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="n">further</span><span class="w"> </span><span class="n">assistance</span><span class="o">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="n">feel</span><span class="w"> </span><span class="n">free</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">contact</span><span class="w"> </span><span class="n">us</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">time</span><span class="o">.</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="n">to</span>
<span class="n">provide</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="n">medical</span><span class="w"> </span><span class="n">services</span><span class="o">.)</span>
</code></pre></div>

<p>Table 5: A sample response in diagnosis for an electronic medical record.</p>
<p>The experimental results are demonstrated in Table 6 for each disease group. ClinicalGPT outperformed other language models, such as ChatGLM-6B, LLAMA-7B, and BLOOM-7B, across all disease groups. The average accuracy of ClinicalGPT across all disease groups was $80.9 \%$, which is obviously higher than the $40.9 \%$ of ChatGLM-6B, $36.6 \%$ of LLAMA-7B, and $60.3 \%$ of BLOOM-7B. ClinicalGPT demonstrated particularly strong performance in the Digestive and Urinary departments, achieving accuracies of $90.1 \%$ and $89.9 \%$, respectively. This indicates a robust capability for understanding and interpreting medical records across different disease groups. However, ClinicalGPT exhibited slightly lower, yet still impressive, performance in the Gynecology and Hematology departments, with accuracies of $78.6 \%$ and $80.7 \%$ respectively. This suggests that there may be room for improvement, specifically in the fields of Gynecology and Hematology, although ClinicalGPT still performed well overall across a range of medical specialties.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Respiratory</th>
<th style="text-align: center;">Digestive</th>
<th style="text-align: center;">Urinary</th>
<th style="text-align: center;">Psychiatry</th>
<th style="text-align: center;">Neurology</th>
<th style="text-align: center;">Gynecology</th>
<th style="text-align: center;">Hematology</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">40.9</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA-7B</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">36.6</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM-7B</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">60.3</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$\mathbf{6 4 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 6}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparisons on diagnosis.</p>
<h1>3.5 Medical question answering</h1>
<p>For medical question-answering (QA) assessment, our model was benchmarked against several other models using a dataset of 388 questions sampled from cMedQA2. Automated evaluation metrics were used, with GPT-4 serving as the refrence model. Given the question, each model generated an answer independently. Then GPT-4 was used to assess these responses based on their accuracy, helpfulness, and safety. The GPT-4 assigned a judgment of Win, Tie, or Lose for each comparison. A "Win" indicates ClinicalGPT provided a superior response, a "Lose" indicates the competing model offered a better response, and a "Tie" means that no obvious difference between the responses was observed.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Win</th>
<th style="text-align: center;">Tie</th>
<th style="text-align: center;">Lose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours v.s. BLOOM-7B</td>
<td style="text-align: center;">$89.7 \%$</td>
<td style="text-align: center;">$1.8 \%$</td>
<td style="text-align: center;">$8.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ours v.s. LLAMA-7B</td>
<td style="text-align: center;">$85.0 \%$</td>
<td style="text-align: center;">$2.3 \%$</td>
<td style="text-align: center;">$12.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ours v.s. ChatGLM-6B</td>
<td style="text-align: center;">$67.2 \%$</td>
<td style="text-align: center;">$10.9 \%$</td>
<td style="text-align: center;">$22.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Medical question-answering on automatic evaluation.</p>
<p>The results of the medical question-answering evaluation are presented in Table 7. According to the results, ClinicalGPT outperformed all of BLOOM-7B, LLAMA-7B, and ChatGLM-6B. In comparisons against BLOOM-7B and LLAMA7B, our model won in $89.7 \%$ and $85.0 \%$ of the cases respectively. The percentage of tie cases were relatively small, at $1.8 \%$ against BLOOM-7B and $2.3 \%$ against LLAMA-7B. Meanwhile, ClinicalGPT wins against ChatGLM-6B at $67.2 \%$. The tie rate increased to $10.9 \%$ and the loss rate to $22.0 \%$. This performance suggests that while ChatGLM-6B has a commendable repository of medical knowledge and displays fluent textual expression, training with ClinicalGPT is beneficial for augmenting the capabilities in medical question answering, despite the extensive knowledge reserves of larger models.</p>
<h2>4 Conclusion</h2>
<p>In this study, we introduced ClinicalGPT, a large language model tailored for medical and clinical applications. Recognizing the limitations that generic large language models present in these specialized fields, we took steps to refine the model, assembling comprehensive datasets for its fine-tuning. These datasets incorporate real medical records, patient consultations, diverse medical knowledge, and exam data, all aimed at shaping the model's knowledge base and responsiveness. Our extensive experiments cover a range of critical tasks in the medical field, such as medical conversation, medical examination, diagnosis, and medical question answering. The empirical results highlight the superior capabilities of ClinicalGPT in understanding and generating medical and clinical-related responses.</p>
<h2>Acknowledgments</h2>
<p>Parts of the experiments are conducted in the InforSuperBahn Testbed. The authors appreciate Nanjing Institute of InforSuperBahn for providing the test and evaluation platform.</p>
<h2>References</h2>
<p>[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[2] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.
[3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.</p>
<p>[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[6] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
[7] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[8] Christian Baumgartner. The potential impact of chatgpt in clinical and translational medicine. Clinical and translational medicine, 13(3), 2023.
[9] Tyler Cowen. The ai revolution in medicine: Gpt-4 and beyond. 2023.
[10] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259-265, 2023.
[11] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610-623, 2021.
[12] Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. Multi-scale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:74061-74071, 2018.
[13] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.
[14] Xuehai He, Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang, Yue Yang, Jiaqi Zeng, Ruisi Zhang, Ruoyu Zhang, et al. Meddialog: Two large-scale medical dialogue datasets. arXiv preprint arXiv:2004.03329, 2020.
[15] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
[16] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.
[17] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, and Suzana Ilić eta al. Bloom: A 176b-parameter open-access multilingual language model, 2023.
[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[19] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-16. IEEE, 2020.
[20] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.
[21] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
[22] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
[23] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages $311-318,2002$.</p>
<p>[24] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81, 2004.</p>
<h1>Appendix</h1>
<h2>A Medical knowledge graphs</h2>
<p>The CMeKG (Chinese Medical Knowledge Graph) is a Chinese medical knowledge graph created by human-AI collaboration, using natural language processing and text mining techniques. It's built upon international standards such as ICD, ATC, SNOMED, and MeSH, and integrates clinical guidelines, industry standards, and medical wiki websites as diverse sources. The CMeKG contains 62 k entities and 374 k relationship triplets, representing nine types of medical entities and their 23 different relationships. Entities include diseases $(15,962)$, manifestations $(12,271)$, body parts $(17,706)$, equipment (900), procedures $(6,418)$, microorganisms $(1,934)$, medical departments (356), tests $(2,605)$, and medications $(3,935)$. Relationships cover diverse medical aspects, with the most prominent being common symptoms $(94,657)$ and side effects $(62,339)$.
The xywy-KG is a medical knowledge graph generated using data sourced from a Chinese online medical consultation website ${ }^{6}$. These entities are categorized into seven groups: diseases $(11,013)$, manifestations $(5,998)$, procedures $(554)$, departments (54), examination items $(3,353)$, medications $(22,359)$, and foods $(4,993)$. The relationships are sorted into nine types, most notably examinations $(39,531)$ and recommended medications $(59,467)$, totally comprising 44 k entities and 294 k relationships.
The 39Health-KG is a medical knowledge graph built from data collected from 39-health, a website dedicated to health consultation and registration ${ }^{7}$. This graph integrates seven types of medical entities and eight types of relationships among them. It comprises 37 k entities and 210 k entity relationships. The entity types are diseases $(14,337)$, body parts (82), departments (83), examination items (3,074), clinical manifestations (5,927), treatment methods (1,493), and medications $(4,966)$. The relationships majorly revolve around related symptoms $(48,757)$ and examination items $(31,577)$.</p>
<h2>B Prompt templates</h2>
<p>We designed prompt templates, transforming knowledge triplets into question-answer data for training language models. Examples of prompt templates are shown in Table 8.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt (Chinese)</th>
<th style="text-align: center;">Response (Chinese)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(疾病)和哪些疾病有关联? <br> (疾病)可能与哪些其他疾病有关? <br> (疾病)有哪些常见症状? <br> 患有 (疾病)的患者可能出现哪些症状? <br> (疾病)的典型(临床表现)是什么? <br> 患有 (疾病)的患者在临床上通常表现为哪些症状? <br> 诊断 (疾病)需要进行哪些检查? <br> 如何检查以确定患有 (疾病)? <br> (药物)主要用于治疗哪些疾病? <br> (药物)的适应症是什么? <br> 如何治疗 (疾病)? <br> (疾病)的常见治疗方法有哪些? <br> (疾病)会引起哪些并发症? <br> 患有 (疾病)的患者可能出现哪些并发症? <br> (药物)与哪些药物存在相互作用? <br> 使用 (药物)时需要注意哪些药物相互作用? <br> (药物)主要用于治疗哪些症状? <br> (药物)的主要治疗作用是什么?</td>
<td style="text-align: center;">(疾病)与 (疾病)可能有关联。 <br> (疾病)可能与 (疾病)有关联。 <br> (疾病)的常见症状包括 (临床表现) <br> (疾病)患者可能出现如 (临床表现)等症状。 <br> (疾病)的典型临床表现包括 (临床表现) <br> 患有 (疾病)的患者在临床上通常表现为 (临床表现) <br> 诊断 (疾病)需要进行如 (医学检验项目) 等检查。 <br> 确定患有 (疾病)需要进行 (医学检验项目) 等检查。 <br> (药物)主要用于治疗 (疾病) 等疾病。 <br> (药物)的适应症包括 (疾病) <br> 治疗 (疾病)的方法包括 (医疗程序) <br> (疾病)的常见治疗方法包括 (医疗程序) <br> (疾病)会引起 (疾病)等并发症。 <br> 患有 (疾病)的患者可能出现 (疾病) 等并发症。 <br> (药物)与 (药物)存在相互作用。 <br> 使用 (药物)时需注意与 (药物)的相互作用。 <br> (药物)主要用于治疗 (临床表现) 等症状。 <br> (药物)的主要治疗作用为治疗 (临床表现)。</td>
</tr>
<tr>
<td style="text-align: center;">Prompt (English)</td>
<td style="text-align: center;">Response (English)</td>
</tr>
<tr>
<td style="text-align: center;">What diseases are related to {disease}? <br> What other diseases may be associated with {disease}? <br> What are the common symptoms of {disease}? <br> What symptoms might a patient with {disease} exhibit? <br> What are the typical ${$ clinical manifestations $}$ of ${$ disease $}$ ? <br> What symptoms do patients with {disease} typically present in a clinical setting?</td>
<td style="text-align: center;">(Disease) may be related to {disease}. <br> (Disease) may be associated with {disease}. <br> The common symptoms of {disease} include {clinical manifestations}. <br> Patients with {disease} may exhibit symptoms such as {clinical manifestations}. <br> The typical clinical manifestations of {disease} include {clinical manifestations}. <br> Patients with {disease} typically present with {clinical manifestations} in a clinical setting. <br> Tests such as {medical examination items} are required to diagnose {disease}. <br> To confirm if one has {disease}, tests such as {medical examination items} are required. <br> (Drug) is primarily used to treat diseases such as {disease}. <br> The indications of ${$ drug $}$ include ${$ disease $}$. <br> (Disease) can be treated with methods such as {medical procedures}. <br> The common treatment methods for ${$ disease $}$ include ${$ medical procedures}. <br> (Disease) can cause complications such as {disease}. <br> A patient with {disease} might develop complications such as {disease}. <br> (Drug) interacts with ${$ drug $}$. <br> When using ${$ drug $}$, interactions with ${$ drug $}$ should be considered. <br> (Drug) is primarily used to treat symptoms such as {clinical manifestations}. <br> The main therapeutic action of ${$ drug $}$ is to treat ${$ clinical manifestations $}$.</td>
</tr>
</tbody>
</table>
<p>Table 8: Prompt templates.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><a href="http://www.xywy.com">http://www.xywy.com</a>&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:0">
<p>${ }^{6}$ http://www.xywy.com
${ }^{7}$ http://www.39.net&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
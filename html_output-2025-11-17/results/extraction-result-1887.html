<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1887 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1887</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1887</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-37.html">extraction-schema-37</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-279999157</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.18348v3.pdf" target="_blank">Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team</a></p>
                <p><strong>Paper Abstract:</strong> Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1887.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1887.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Knowledge Exchange</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A leader-mediated peer-review-like workflow in the multi-agent system where each agent proposes ideas, other agents revise them, and a leader synthesizes revisions into consolidated summaries to increase idea diversity and reduce redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review simulation / internal review process</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Overall Novelty (ON) computed from Historical Dissimilarity (HD), Contemporary Dissimilarity (CD), and Contemporary Impact (CI); HD and CD are embedding-distance based measures</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Quantified via ablation: removal of the internal discussion module (i.e., removing DKE) decreased CI from 4.38 to 4.10 (absolute -0.28, -6.4% relative), indicating tangible loss of measured impact when DKE is absent.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>positive effect of DKE on novelty/impact with diminishing returns across iterations (single iteration produces most gains; additional rounds show small or inconsistent changes).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Across iterative discussion rounds (1–5 rounds) CI varied only modestly (range 4.18–4.45) and ON variance was very small (reported ON variance 0.014); therefore most gains occur in early rounds (1–2), no long-term temporal recognition delay reported.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (primary experiments) and Health sciences (cross-domain dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>IDVSCI (which implements DKE) shows larger relative improvements in Contemporary Impact (CI) over VIRSCI in both fields: Computer Sciences CI +1.13 absolute (+33.6% relative: 4.49 vs 3.36); Health Sciences CI +2.22 absolute (+28.4% relative: 10.06 vs 7.84).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Contemporary Impact (CI; citation counts of top-5 similar post-2011 papers), plus HD and CD embedding-distance novelty proxies and composite ON</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>No independent external ground-truth used; long-term citations (as proxied through CI of nearest neighbors) and embedding-dissimilarity (HD/CD) serve as the study's measures of scientific value</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Indirectly evidenced: interventions that increase CI do not always increase ON (see voting ablation), e.g., removing voting increased CI by +0.9% but reduced ON by -1.56%, indicating conflicting proxy signals and a non-perfect alignment between citation-based proxies and composite novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>The paper operationalizes 'transformational' as higher ON; systems with DKE (IDVSCI) produce higher ON than baselines (Computer Sciences ON: IDVSCI 4.60 vs VIRSCI 3.70, +0.90 absolute, +24.3% relative), indicating transformational-like outputs score higher under DKE, but no direct human-label comparison between 'incremental' and 'transformational' items is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Yes — the study shows proxy conflict: the Borda-voting ablation increased CI (+0.9%) while decreasing ON (-1.56%), demonstrating that multiple proxy metrics can move in opposite directions rather than simply compounding; this is presented as evidence that different proxies can fail or trade off simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>IDVSCI (automated multi-agent system using DKE) outperformed other automated systems (VIRSCI, AI-Scientist) on CI and ON; e.g., Computer Sciences CI: IDVSCI (LLaMA-70b) 4.49 vs VIRSCI (LLaMA-70b) 3.36 (+33.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not directly quantified for DKE itself; model-capacity effects observed (larger models generally higher CI/ON but not always), indicating training/capacity interacts with multi-agent design (e.g., IDVSCI with QWQ-32b sometimes underperformed relative to LLaMA-8b).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>DKE itself is an intervention (leader + cross-agent revision). Effectiveness quantified by ablation: removal caused CI -6.4% and lowered ON; thus DKE is effective at increasing measured impact and novelty proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>None specific to peer-review outcomes beyond the observation that increased iteration count does not consistently increase novelty (iteration 4 saw a CI drop to 4.18), i.e., more review rounds can fail to help.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Number of discussion iterations (most gains in 1–2 iterations), team background diversity (optimal reported at ~25% diversity), model scale (larger models tend to help but not guaranteed), and internal vs external communication (internal teams outperform external).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Experiments run on two datasets (Computer Sciences: 156 researchers, 85,217 publications; Health Sciences: 130 researchers, 86,448 publications); four-agent systems, five sequential rounds in protocol; metrics averaged over 20 independent experimental trials; ablation and iteration-wise studies reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1887.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1887.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual-Diversity Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated review paradigm that assembles reviewers with two axes of diversity (heterogeneous knowledge backgrounds and dynamically updated prompts grounded in nearest-neighbor literature) to promote multi-faceted evaluation and higher-quality novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review simulation / reviewer diversity intervention</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Overall Novelty (ON), composed from embedding-distance Historical Dissimilarity (HD), Contemporary Dissimilarity (CD), and Contemporary Impact (CI).</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Paper asserts bias toward familiar collaborators suppresses novelty and applies a mitigation (adding 1 to adjacency entries) but does not provide a direct numeric estimate of the original bias magnitude; empirical outcomes show DDR-equipped systems (IDVSCI) yield higher ON and CI than baselines (e.g., ON in Computer Sciences 4.60 vs VIRSCI 3.70).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>non-monotonic: team background diversity exhibits a peaked effect (performance optimal at ~25% diversity, with 50% and 100% slightly worse than the 25% peak), indicating an inverted-U / peak relationship rather than strictly linear.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not explicitly temporal; diversity effects observed in cross-sectional experiments (no multiyear temporal dynamics reported).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science and Health sciences (both evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Both fields benefit from DDR/IDVSCI; relative CI improvements comparable across fields (see DKE entry): CS +33.6% vs VIRSCI, HS +28.4% vs VIRSCI, indicating DDR effectiveness is cross-domain though absolute magnitudes differ (HS absolute CI values are much larger).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>ON (composite), CI (citations of nearest neighbors), HD/CD (embedding distances)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>No independent external ground truth; the study treats higher ON and CI as indicators of more transformational/valuable output.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not numerically reported for DDR specifically; the paper shows cases where proxy metrics diverge (e.g., voting changes CI and ON in opposite directions), implying DDR may change proxy alignment but no direct gap numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Indirect comparison: IDVSCI (which includes DDR) produces higher ON and CI suggesting it favors/produces more transformational outputs per the paper's operationalization, but no explicit labeling or acceptance-rate style comparison between incremental and transformational work is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>The paper demonstrates that proxy metrics can disagree under different review/aggregation procedures (e.g., voting ablation), but does not provide a formal multiplicative/additive decomposition for DDR.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>DDR as implemented in IDVSCI contributes to automated system performance gains (higher CI/ON compared to systems that lack structured diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>DDR uses dynamic prompt updates with nearest-neighbor literature retrieval; the paper notes large numbers of reference papers can cause convergence (reduction of diversity), indicating training/reference composition can bias outputs toward recent/historical norms.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>DDR itself is the intervention; additional interventions include adding 1 to collaboration adjacency matrix to discourage familiar-collaborator bias, and distributing reference papers among reviewers. Effectiveness: DDR-equipped system (IDVSCI) outperforms baselines on CI/ON (see table figures above).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Higher diversity levels (50% and 100%) did not outperform the 25% diversity configuration, showing more diversity is not always better.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Team background overlap, prompt grounding (Top-k references), and reference distribution strategy; excessive shared references reduce diversity and promote identical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Same datasets and experimental design as DKE entry (156 researchers/85,217 pubs CS; 130/86,448 HS); experiments averaged over 20 runs; diversity varied in controlled experiments with results visualized (Figure 4).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1887.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1887.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Borda voting (weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted Borda Count Voting in Check Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A weighted Borda count aggregation used to rank ideas during the Check Novelty stage, combining each reviewer's ranking and confidence to produce an overall score for idea selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review aggregation / voting-based decision</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Composite ON and component measures HD, CD, CI; Borda score used to select final idea based on rankings and confidence</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Ablation shows removal of the voting mechanism produced a small increase in CI (+0.9%, from 4.38 to 4.42) but decreased ON (-1.56%, from 4.49 to 4.42), indicating voting can modestly constrain measured impact while protecting originality.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>trade-off relation: voting reduces overconvergence toward high-CI (impact) items while preserving ON; i.e., voting enforces a conservatizing effect on immediate impact at the expense of small short-term CI gains.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>No explicit temporal dynamics; effect observed at evaluation time during Check Novelty stage.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (experiments reported primarily on CS dataset), applied across Health Sciences dataset in other experiments</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not reported separately for voting; the numeric ablation results are reported for the dataset used (CS).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>CI (short-term citation proxy) and ON (composite novelty proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>No external ground truth; Borda aims to mediate between reviewer rankings/confidence and proxy novelty metrics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Demonstrated by ablation: CI and ON move in opposite directions when voting removed (CI +0.9% while ON -1.56%), indicating a measurable gap/conflict between impact proxy and composite novelty under different aggregation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Voting appears to reduce short-term CI gains (which might favour incremental 'safe' ideas) while preserving ON (favoring originality), suggesting the mechanism biases selection away from immediately high-impact incremental work and towards maintaining novelty — however no explicit incremental/transformational acceptance rates are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Yes — results show aggregation choice can cause conflicting changes in multiple proxies (CI vs ON), illustrating interacting proxy failures.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Voting is part of the automated pipeline; removing it slightly increased automated CI but decreased ON, so it affects automated evaluation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not directly examined for voting, though voting uses reviewer confidence which may itself be influenced by reviewers' reference grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Weighted Borda count vs no-voting ablation; the weighted Borda count is the intervention and is shown to preserve ON at a small cost to CI.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Removing voting increased CI (counter to the intuition that more aggregation improves impact estimates), demonstrating that some aggregation rules can produce counterintuitive effects.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Reviewer confidence scores, distribution of reference papers, and reviewer background diversity moderate the voting outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Ablation reported on experiments averaged over 20 trials on the Computer Sciences dataset (n=156 researchers, 85,217 publications) within the four-agent five-round experimental protocol.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1887.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1887.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference-count effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Number and Distribution of Reference Papers on Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation that providing many reference papers during idea grounding increases convergence among agents (identical outputs), while reducing or distributing references increases output diversity and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>automated literature-grounding / retrieval-augmented evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>HD/CD embedding-distance novelty and qualitative diversity of agent outputs; Top-k nearest-paper retrieval (k empirically set to 8) used to ground prompts</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Not expressed as a single numeric metric, but paper reports 'significant reduction' in identical outputs when a subset of reference papers was removed and that increasing the number of reference papers tends to lead to identical outputs (Figure 3). Empirical choice k=8 balances coverage and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>monotonic convergence: increasing the number of shared reference documents increases inter-agent output similarity (reduces novelty), while distributing references increases diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not temporal; observed during the single-run idea-generation / check-novelty phase.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Applied within both Computer Science and Health Sciences experimental settings</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not separately quantified by field in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>HD/CD (embedding similarity), qualitative output similarity</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>No external ground truth; diversity and ON used as internal measures of novelty</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not numerically quantified, but the phenomenon shows that information-grounding practices (reference count/composition) can bias proxy novelty measures by artificially reducing diversity and thus inflating perceived consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Not directly compared, but mechanism implies that high shared referencing favors incremental/conservative convergence over transformational divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not formally decomposed, but the effect demonstrates that retrieval/grounding choices can induce correlated failures across proxies by producing homogeneous outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Systems using fewer or better-distributed references generated more diverse and (by ON) more novel ideas; the system set k=8 as an empirical compromise.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Related: the composition and number of retrieved references (training/conditioning context) biases subsequent agent outputs toward existing literature norms.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Evenly distributing related papers among scientists and limiting Top-k to k=8; authors report this increases diversity and reduces identical outputs, though no single-effect size is given.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>None specifically quantified; the authors note that too few references can omit coverage while too many induce convergence — tradeoff rather than absolute rule.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Number k of Top-k references, whether references are shared or partitioned across reviewers, and reviewer background diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Empirical ablation and case-study evidence (Figure 3) within the main experimental pipeline; Top-k chosen as k=8 following empirical tuning on the datasets described above.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1887.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1887.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERSCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External Review SCIentists (ERSCI) vs Internal Teams</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of using an external review team (ERSCI) that summarizes ideas without close internal collaboration versus internal multi-agent discussion; used to measure effects of communication fidelity on novelty/impact.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>external peer review vs internal collaborative review</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>HD, CD, CI, and composite ON</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Internal teams (IDVSCI) achieved CI = 4.38 vs ERSCI CI = 4.04, an absolute drop of 0.34 (≈8.4% relative decrease) for external teams; ON 4.49 vs ERSCI 4.14 (absolute -0.35, -7.79% relative).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>internal communication improves measured impact and novelty; external separation reduces CI/ON (negative effect of information transmission constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Effect observed at evaluation time; no longitudinal recognition pattern reported.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science dataset (comparison reported in Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not reported across fields for ERSCI specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>CI and ON (composite novelty proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>None external; CI/ON used as internal measures of value.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not explicitly decomposed, but the lower CI and ON for ERSCI indicate that communication/setup of reviewers affects proxy outcomes materially.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Not directly reported, but lower CI/ON for external review suggests transformational ideas (higher ON) may be particularly sensitive to internal communication; no direct numeric split between incremental/transformational acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not directly analyzed beyond reporting both CI and ON are lower with ERSCI, indicating consistent proxy degradation across metrics when review communication is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Internal multi-agent evaluation (IDVSCI) outperforms external-review setup in automated metrics (CI, ON) by modest but nontrivial margins (≈8%).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not directly discussed for ERSCI comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Comparative experiment (internal vs external team configuration); internal configuration (IDVSCI) proved superior on proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>None provided showing ERSCI outperforming internal teams in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Degree of communication between originators and reviewers; information distortion during transmission is cited as a mechanism reducing measured impact/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Reported in Table 4 within the same four-agent, five-round protocol; results averaged over 20 trials (Computer Sciences dataset).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1887.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1887.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CI/HD/CD/ON metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contemporary Impact (CI), Historical Dissimilarity (HD), Contemporary Dissimilarity (CD), Overall Novelty (ON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational proxies used to quantify novelty and likely impact: HD/CD are embedding-distance measures to pre-2011 and post-2011 literature respectively, CI is citation counts of nearest neighbors, and ON = HD * CI / CD is a composite novelty score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation metrics and embedding-distance novelty proxies</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>HD (avg squared Euclidian distance to 5 nearest pre-2011 abstracts), CD (avg squared Euclidian distance to 5 nearest post-2011 abstracts), CI (citation count of top-5 most similar post-2011 abstracts), ON = HD * CI / CD</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Empirical differences across systems on these proxies are reported: e.g., IDVSCI (LLaMA-70b) vs VIRSCI (LLaMA-70b) in Computer Sciences: CI 4.49 vs 3.36 (+33.6%), ON 4.60 vs 3.70 (+24.3%); in Health Sciences CI 10.06 vs 7.84 (+28.4%) and ON 10.58 vs 8.26 (+28.1%). These represent how different evaluation configurations change proxy readings of novelty/impact.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>Composite: ON increases with HD and CI, decreases with CD (as ON defined ON = HD * CI / CD); thus trade-offs exist among proxy components.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>CI is explicitly a citation-based measure (post-2011 citations of nearest neighbors) — the paper does not present multi-year citation accrual dynamics but uses normalized CI as a static proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science and Health sciences datasets used to compute these metrics</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Absolute values differ strongly across fields (Health Sciences absolute CI/ON much larger), but relative improvements from method changes are similar (IDVSCI outperforms baselines in both fields by comparable percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>Short-term citations (CI), embedding-distance similarity (HD/CD), composite ON</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>No independent ground-truth used; paper treats these proxies as approximations of scientific novelty and impact.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>The paper provides evidence of proxy mismatch: e.g., voting removal increased CI by +0.9% while decreasing ON by -1.56%, showing that single proxies can diverge and the composite ON may move opposite to a pure citation proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Transformational outputs (higher ON) are associated with higher CI under the best-performing system (IDVSCI), e.g., IDVSCI ON and CI exceed those of baselines; however the paper does not provide acceptance or real-world recognition (journals/prizes) comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Yes: the paper documents cases where changes to the evaluation pipeline produce opposite changes in CI and ON, demonstrating non-additive, interacting proxy failures.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Automated systems' outputs scored by these metrics differ substantially depending on system design; IDVSCI yields the highest proxy scores across architectures in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>HD/CD depend on the historical corpus split (pre-2011 vs post-2011) and embeddings; choice of corpus and embeddings will bias proxy values though the paper does not quantify this explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Normalization by database mean and composite ON formula used; superiority of IDVSCI on these proxies functions as an implicit intervention result.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Examples where a single proxy and composite disagree (voting ablation)—CI increase but ON decrease—are reported as counter-examples to naive reliance on a single proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Model size, team diversity, reference-grounding strategy, internal vs external review structure, and aggregation method (voting) all moderate proxy outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Metrics computed over generated abstracts and compared against large corpora (pre-2011 and post-2011 subsets) using embeddings; experimental metrics averaged over 20 trials; dataset sizes: CS 85,217 publications, HS 86,448 publications.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fresh teams are associated with original and multidisciplinary research <em>(Rating: 2)</em></li>
                <li>Gender-diverse teams produce more novel and higher-impact scientific ideas <em>(Rating: 2)</em></li>
                <li>Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Novelseek: When Agent Becomes the Scientist-Building Closed-Loop System from Hypothesis to Verification <em>(Rating: 1)</em></li>
                <li>A multi-agent-driven robotic AI chemist enabling autonomous chemical research on demand <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1887",
    "paper_id": "paper-279999157",
    "extraction_schema_id": "extraction-schema-37",
    "extracted_data": [
        {
            "name_short": "DKE",
            "name_full": "Dynamic Knowledge Exchange",
            "brief_description": "A leader-mediated peer-review-like workflow in the multi-agent system where each agent proposes ideas, other agents revise them, and a leader synthesizes revisions into consolidated summaries to increase idea diversity and reduce redundancy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "peer review simulation / internal review process",
            "novelty_measure": "Overall Novelty (ON) computed from Historical Dissimilarity (HD), Contemporary Dissimilarity (CD), and Contemporary Impact (CI); HD and CD are embedding-distance based measures",
            "bias_magnitude": "Quantified via ablation: removal of the internal discussion module (i.e., removing DKE) decreased CI from 4.38 to 4.10 (absolute -0.28, -6.4% relative), indicating tangible loss of measured impact when DKE is absent.",
            "relationship_type": "positive effect of DKE on novelty/impact with diminishing returns across iterations (single iteration produces most gains; additional rounds show small or inconsistent changes).",
            "temporal_pattern": "Across iterative discussion rounds (1–5 rounds) CI varied only modestly (range 4.18–4.45) and ON variance was very small (reported ON variance 0.014); therefore most gains occur in early rounds (1–2), no long-term temporal recognition delay reported.",
            "field_studied": "Computer science (primary experiments) and Health sciences (cross-domain dataset)",
            "field_differences": "IDVSCI (which implements DKE) shows larger relative improvements in Contemporary Impact (CI) over VIRSCI in both fields: Computer Sciences CI +1.13 absolute (+33.6% relative: 4.49 vs 3.36); Health Sciences CI +2.22 absolute (+28.4% relative: 10.06 vs 7.84).",
            "proxy_metric_studied": "Contemporary Impact (CI; citation counts of top-5 similar post-2011 papers), plus HD and CD embedding-distance novelty proxies and composite ON",
            "ground_truth_measure": "No independent external ground-truth used; long-term citations (as proxied through CI of nearest neighbors) and embedding-dissimilarity (HD/CD) serve as the study's measures of scientific value",
            "proxy_truth_gap": "Indirectly evidenced: interventions that increase CI do not always increase ON (see voting ablation), e.g., removing voting increased CI by +0.9% but reduced ON by -1.56%, indicating conflicting proxy signals and a non-perfect alignment between citation-based proxies and composite novelty.",
            "incremental_vs_transformational": "The paper operationalizes 'transformational' as higher ON; systems with DKE (IDVSCI) produce higher ON than baselines (Computer Sciences ON: IDVSCI 4.60 vs VIRSCI 3.70, +0.90 absolute, +24.3% relative), indicating transformational-like outputs score higher under DKE, but no direct human-label comparison between 'incremental' and 'transformational' items is reported.",
            "multiple_proxy_failures": "Yes — the study shows proxy conflict: the Borda-voting ablation increased CI (+0.9%) while decreasing ON (-1.56%), demonstrating that multiple proxy metrics can move in opposite directions rather than simply compounding; this is presented as evidence that different proxies can fail or trade off simultaneously.",
            "automated_system_performance": "IDVSCI (automated multi-agent system using DKE) outperformed other automated systems (VIRSCI, AI-Scientist) on CI and ON; e.g., Computer Sciences CI: IDVSCI (LLaMA-70b) 4.49 vs VIRSCI (LLaMA-70b) 3.36 (+33.6%).",
            "training_data_bias": "Not directly quantified for DKE itself; model-capacity effects observed (larger models generally higher CI/ON but not always), indicating training/capacity interacts with multi-agent design (e.g., IDVSCI with QWQ-32b sometimes underperformed relative to LLaMA-8b).",
            "intervention_tested": "DKE itself is an intervention (leader + cross-agent revision). Effectiveness quantified by ablation: removal caused CI -6.4% and lowered ON; thus DKE is effective at increasing measured impact and novelty proxies.",
            "counter_examples": "None specific to peer-review outcomes beyond the observation that increased iteration count does not consistently increase novelty (iteration 4 saw a CI drop to 4.18), i.e., more review rounds can fail to help.",
            "moderating_factors": "Number of discussion iterations (most gains in 1–2 iterations), team background diversity (optimal reported at ~25% diversity), model scale (larger models tend to help but not guaranteed), and internal vs external communication (internal teams outperform external).",
            "sample_size_and_methods": "Experiments run on two datasets (Computer Sciences: 156 researchers, 85,217 publications; Health Sciences: 130 researchers, 86,448 publications); four-agent systems, five sequential rounds in protocol; metrics averaged over 20 independent experimental trials; ablation and iteration-wise studies reported.",
            "uuid": "e1887.0"
        },
        {
            "name_short": "DDR",
            "name_full": "Dual-Diversity Review",
            "brief_description": "A simulated review paradigm that assembles reviewers with two axes of diversity (heterogeneous knowledge backgrounds and dynamically updated prompts grounded in nearest-neighbor literature) to promote multi-faceted evaluation and higher-quality novel ideas.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "peer review simulation / reviewer diversity intervention",
            "novelty_measure": "Overall Novelty (ON), composed from embedding-distance Historical Dissimilarity (HD), Contemporary Dissimilarity (CD), and Contemporary Impact (CI).",
            "bias_magnitude": "Paper asserts bias toward familiar collaborators suppresses novelty and applies a mitigation (adding 1 to adjacency entries) but does not provide a direct numeric estimate of the original bias magnitude; empirical outcomes show DDR-equipped systems (IDVSCI) yield higher ON and CI than baselines (e.g., ON in Computer Sciences 4.60 vs VIRSCI 3.70).",
            "relationship_type": "non-monotonic: team background diversity exhibits a peaked effect (performance optimal at ~25% diversity, with 50% and 100% slightly worse than the 25% peak), indicating an inverted-U / peak relationship rather than strictly linear.",
            "temporal_pattern": "Not explicitly temporal; diversity effects observed in cross-sectional experiments (no multiyear temporal dynamics reported).",
            "field_studied": "Computer science and Health sciences (both evaluated)",
            "field_differences": "Both fields benefit from DDR/IDVSCI; relative CI improvements comparable across fields (see DKE entry): CS +33.6% vs VIRSCI, HS +28.4% vs VIRSCI, indicating DDR effectiveness is cross-domain though absolute magnitudes differ (HS absolute CI values are much larger).",
            "proxy_metric_studied": "ON (composite), CI (citations of nearest neighbors), HD/CD (embedding distances)",
            "ground_truth_measure": "No independent external ground truth; the study treats higher ON and CI as indicators of more transformational/valuable output.",
            "proxy_truth_gap": "Not numerically reported for DDR specifically; the paper shows cases where proxy metrics diverge (e.g., voting changes CI and ON in opposite directions), implying DDR may change proxy alignment but no direct gap numbers provided.",
            "incremental_vs_transformational": "Indirect comparison: IDVSCI (which includes DDR) produces higher ON and CI suggesting it favors/produces more transformational outputs per the paper's operationalization, but no explicit labeling or acceptance-rate style comparison between incremental and transformational work is performed.",
            "multiple_proxy_failures": "The paper demonstrates that proxy metrics can disagree under different review/aggregation procedures (e.g., voting ablation), but does not provide a formal multiplicative/additive decomposition for DDR.",
            "automated_system_performance": "DDR as implemented in IDVSCI contributes to automated system performance gains (higher CI/ON compared to systems that lack structured diversity).",
            "training_data_bias": "DDR uses dynamic prompt updates with nearest-neighbor literature retrieval; the paper notes large numbers of reference papers can cause convergence (reduction of diversity), indicating training/reference composition can bias outputs toward recent/historical norms.",
            "intervention_tested": "DDR itself is the intervention; additional interventions include adding 1 to collaboration adjacency matrix to discourage familiar-collaborator bias, and distributing reference papers among reviewers. Effectiveness: DDR-equipped system (IDVSCI) outperforms baselines on CI/ON (see table figures above).",
            "counter_examples": "Higher diversity levels (50% and 100%) did not outperform the 25% diversity configuration, showing more diversity is not always better.",
            "moderating_factors": "Team background overlap, prompt grounding (Top-k references), and reference distribution strategy; excessive shared references reduce diversity and promote identical outputs.",
            "sample_size_and_methods": "Same datasets and experimental design as DKE entry (156 researchers/85,217 pubs CS; 130/86,448 HS); experiments averaged over 20 runs; diversity varied in controlled experiments with results visualized (Figure 4).",
            "uuid": "e1887.1"
        },
        {
            "name_short": "Borda voting (weighted)",
            "name_full": "Weighted Borda Count Voting in Check Novelty",
            "brief_description": "A weighted Borda count aggregation used to rank ideas during the Check Novelty stage, combining each reviewer's ranking and confidence to produce an overall score for idea selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "peer review aggregation / voting-based decision",
            "novelty_measure": "Composite ON and component measures HD, CD, CI; Borda score used to select final idea based on rankings and confidence",
            "bias_magnitude": "Ablation shows removal of the voting mechanism produced a small increase in CI (+0.9%, from 4.38 to 4.42) but decreased ON (-1.56%, from 4.49 to 4.42), indicating voting can modestly constrain measured impact while protecting originality.",
            "relationship_type": "trade-off relation: voting reduces overconvergence toward high-CI (impact) items while preserving ON; i.e., voting enforces a conservatizing effect on immediate impact at the expense of small short-term CI gains.",
            "temporal_pattern": "No explicit temporal dynamics; effect observed at evaluation time during Check Novelty stage.",
            "field_studied": "Computer science (experiments reported primarily on CS dataset), applied across Health Sciences dataset in other experiments",
            "field_differences": "Not reported separately for voting; the numeric ablation results are reported for the dataset used (CS).",
            "proxy_metric_studied": "CI (short-term citation proxy) and ON (composite novelty proxy)",
            "ground_truth_measure": "No external ground truth; Borda aims to mediate between reviewer rankings/confidence and proxy novelty metrics",
            "proxy_truth_gap": "Demonstrated by ablation: CI and ON move in opposite directions when voting removed (CI +0.9% while ON -1.56%), indicating a measurable gap/conflict between impact proxy and composite novelty under different aggregation rules.",
            "incremental_vs_transformational": "Voting appears to reduce short-term CI gains (which might favour incremental 'safe' ideas) while preserving ON (favoring originality), suggesting the mechanism biases selection away from immediately high-impact incremental work and towards maintaining novelty — however no explicit incremental/transformational acceptance rates are provided.",
            "multiple_proxy_failures": "Yes — results show aggregation choice can cause conflicting changes in multiple proxies (CI vs ON), illustrating interacting proxy failures.",
            "automated_system_performance": "Voting is part of the automated pipeline; removing it slightly increased automated CI but decreased ON, so it affects automated evaluation outcomes.",
            "training_data_bias": "Not directly examined for voting, though voting uses reviewer confidence which may itself be influenced by reviewers' reference grounding.",
            "intervention_tested": "Weighted Borda count vs no-voting ablation; the weighted Borda count is the intervention and is shown to preserve ON at a small cost to CI.",
            "counter_examples": "Removing voting increased CI (counter to the intuition that more aggregation improves impact estimates), demonstrating that some aggregation rules can produce counterintuitive effects.",
            "moderating_factors": "Reviewer confidence scores, distribution of reference papers, and reviewer background diversity moderate the voting outcome.",
            "sample_size_and_methods": "Ablation reported on experiments averaged over 20 trials on the Computer Sciences dataset (n=156 researchers, 85,217 publications) within the four-agent five-round experimental protocol.",
            "uuid": "e1887.2"
        },
        {
            "name_short": "Reference-count effect",
            "name_full": "Effect of Number and Distribution of Reference Papers on Novelty",
            "brief_description": "Empirical observation that providing many reference papers during idea grounding increases convergence among agents (identical outputs), while reducing or distributing references increases output diversity and novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "automated literature-grounding / retrieval-augmented evaluation",
            "novelty_measure": "HD/CD embedding-distance novelty and qualitative diversity of agent outputs; Top-k nearest-paper retrieval (k empirically set to 8) used to ground prompts",
            "bias_magnitude": "Not expressed as a single numeric metric, but paper reports 'significant reduction' in identical outputs when a subset of reference papers was removed and that increasing the number of reference papers tends to lead to identical outputs (Figure 3). Empirical choice k=8 balances coverage and diversity.",
            "relationship_type": "monotonic convergence: increasing the number of shared reference documents increases inter-agent output similarity (reduces novelty), while distributing references increases diversity.",
            "temporal_pattern": "Not temporal; observed during the single-run idea-generation / check-novelty phase.",
            "field_studied": "Applied within both Computer Science and Health Sciences experimental settings",
            "field_differences": "Not separately quantified by field in the paper.",
            "proxy_metric_studied": "HD/CD (embedding similarity), qualitative output similarity",
            "ground_truth_measure": "No external ground truth; diversity and ON used as internal measures of novelty",
            "proxy_truth_gap": "Not numerically quantified, but the phenomenon shows that information-grounding practices (reference count/composition) can bias proxy novelty measures by artificially reducing diversity and thus inflating perceived consensus.",
            "incremental_vs_transformational": "Not directly compared, but mechanism implies that high shared referencing favors incremental/conservative convergence over transformational divergence.",
            "multiple_proxy_failures": "Not formally decomposed, but the effect demonstrates that retrieval/grounding choices can induce correlated failures across proxies by producing homogeneous outputs.",
            "automated_system_performance": "Systems using fewer or better-distributed references generated more diverse and (by ON) more novel ideas; the system set k=8 as an empirical compromise.",
            "training_data_bias": "Related: the composition and number of retrieved references (training/conditioning context) biases subsequent agent outputs toward existing literature norms.",
            "intervention_tested": "Evenly distributing related papers among scientists and limiting Top-k to k=8; authors report this increases diversity and reduces identical outputs, though no single-effect size is given.",
            "counter_examples": "None specifically quantified; the authors note that too few references can omit coverage while too many induce convergence — tradeoff rather than absolute rule.",
            "moderating_factors": "Number k of Top-k references, whether references are shared or partitioned across reviewers, and reviewer background diversity.",
            "sample_size_and_methods": "Empirical ablation and case-study evidence (Figure 3) within the main experimental pipeline; Top-k chosen as k=8 following empirical tuning on the datasets described above.",
            "uuid": "e1887.3"
        },
        {
            "name_short": "ERSCI",
            "name_full": "External Review SCIentists (ERSCI) vs Internal Teams",
            "brief_description": "Comparison of using an external review team (ERSCI) that summarizes ideas without close internal collaboration versus internal multi-agent discussion; used to measure effects of communication fidelity on novelty/impact.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "external peer review vs internal collaborative review",
            "novelty_measure": "HD, CD, CI, and composite ON",
            "bias_magnitude": "Internal teams (IDVSCI) achieved CI = 4.38 vs ERSCI CI = 4.04, an absolute drop of 0.34 (≈8.4% relative decrease) for external teams; ON 4.49 vs ERSCI 4.14 (absolute -0.35, -7.79% relative).",
            "relationship_type": "internal communication improves measured impact and novelty; external separation reduces CI/ON (negative effect of information transmission constraints).",
            "temporal_pattern": "Effect observed at evaluation time; no longitudinal recognition pattern reported.",
            "field_studied": "Computer science dataset (comparison reported in Table 4)",
            "field_differences": "Not reported across fields for ERSCI specifically.",
            "proxy_metric_studied": "CI and ON (composite novelty proxy)",
            "ground_truth_measure": "None external; CI/ON used as internal measures of value.",
            "proxy_truth_gap": "Not explicitly decomposed, but the lower CI and ON for ERSCI indicate that communication/setup of reviewers affects proxy outcomes materially.",
            "incremental_vs_transformational": "Not directly reported, but lower CI/ON for external review suggests transformational ideas (higher ON) may be particularly sensitive to internal communication; no direct numeric split between incremental/transformational acceptance.",
            "multiple_proxy_failures": "Not directly analyzed beyond reporting both CI and ON are lower with ERSCI, indicating consistent proxy degradation across metrics when review communication is limited.",
            "automated_system_performance": "Internal multi-agent evaluation (IDVSCI) outperforms external-review setup in automated metrics (CI, ON) by modest but nontrivial margins (≈8%).",
            "training_data_bias": "Not directly discussed for ERSCI comparison.",
            "intervention_tested": "Comparative experiment (internal vs external team configuration); internal configuration (IDVSCI) proved superior on proxies.",
            "counter_examples": "None provided showing ERSCI outperforming internal teams in these experiments.",
            "moderating_factors": "Degree of communication between originators and reviewers; information distortion during transmission is cited as a mechanism reducing measured impact/novelty.",
            "sample_size_and_methods": "Reported in Table 4 within the same four-agent, five-round protocol; results averaged over 20 trials (Computer Sciences dataset).",
            "uuid": "e1887.4"
        },
        {
            "name_short": "CI/HD/CD/ON metrics",
            "name_full": "Contemporary Impact (CI), Historical Dissimilarity (HD), Contemporary Dissimilarity (CD), Overall Novelty (ON)",
            "brief_description": "Operational proxies used to quantify novelty and likely impact: HD/CD are embedding-distance measures to pre-2011 and post-2011 literature respectively, CI is citation counts of nearest neighbors, and ON = HD * CI / CD is a composite novelty score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "citation metrics and embedding-distance novelty proxies",
            "novelty_measure": "HD (avg squared Euclidian distance to 5 nearest pre-2011 abstracts), CD (avg squared Euclidian distance to 5 nearest post-2011 abstracts), CI (citation count of top-5 most similar post-2011 abstracts), ON = HD * CI / CD",
            "bias_magnitude": "Empirical differences across systems on these proxies are reported: e.g., IDVSCI (LLaMA-70b) vs VIRSCI (LLaMA-70b) in Computer Sciences: CI 4.49 vs 3.36 (+33.6%), ON 4.60 vs 3.70 (+24.3%); in Health Sciences CI 10.06 vs 7.84 (+28.4%) and ON 10.58 vs 8.26 (+28.1%). These represent how different evaluation configurations change proxy readings of novelty/impact.",
            "relationship_type": "Composite: ON increases with HD and CI, decreases with CD (as ON defined ON = HD * CI / CD); thus trade-offs exist among proxy components.",
            "temporal_pattern": "CI is explicitly a citation-based measure (post-2011 citations of nearest neighbors) — the paper does not present multi-year citation accrual dynamics but uses normalized CI as a static proxy.",
            "field_studied": "Computer science and Health sciences datasets used to compute these metrics",
            "field_differences": "Absolute values differ strongly across fields (Health Sciences absolute CI/ON much larger), but relative improvements from method changes are similar (IDVSCI outperforms baselines in both fields by comparable percentages).",
            "proxy_metric_studied": "Short-term citations (CI), embedding-distance similarity (HD/CD), composite ON",
            "ground_truth_measure": "No independent ground-truth used; paper treats these proxies as approximations of scientific novelty and impact.",
            "proxy_truth_gap": "The paper provides evidence of proxy mismatch: e.g., voting removal increased CI by +0.9% while decreasing ON by -1.56%, showing that single proxies can diverge and the composite ON may move opposite to a pure citation proxy.",
            "incremental_vs_transformational": "Transformational outputs (higher ON) are associated with higher CI under the best-performing system (IDVSCI), e.g., IDVSCI ON and CI exceed those of baselines; however the paper does not provide acceptance or real-world recognition (journals/prizes) comparisons.",
            "multiple_proxy_failures": "Yes: the paper documents cases where changes to the evaluation pipeline produce opposite changes in CI and ON, demonstrating non-additive, interacting proxy failures.",
            "automated_system_performance": "Automated systems' outputs scored by these metrics differ substantially depending on system design; IDVSCI yields the highest proxy scores across architectures in experiments.",
            "training_data_bias": "HD/CD depend on the historical corpus split (pre-2011 vs post-2011) and embeddings; choice of corpus and embeddings will bias proxy values though the paper does not quantify this explicitly.",
            "intervention_tested": "Normalization by database mean and composite ON formula used; superiority of IDVSCI on these proxies functions as an implicit intervention result.",
            "counter_examples": "Examples where a single proxy and composite disagree (voting ablation)—CI increase but ON decrease—are reported as counter-examples to naive reliance on a single proxy.",
            "moderating_factors": "Model size, team diversity, reference-grounding strategy, internal vs external review structure, and aggregation method (voting) all moderate proxy outcomes.",
            "sample_size_and_methods": "Metrics computed over generated abstracts and compared against large corpora (pre-2011 and post-2011 subsets) using embeddings; experimental metrics averaged over 20 trials; dataset sizes: CS 85,217 publications, HS 86,448 publications.",
            "uuid": "e1887.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fresh teams are associated with original and multidisciplinary research",
            "rating": 2
        },
        {
            "paper_title": "Gender-diverse teams produce more novel and higher-impact scientific ideas",
            "rating": 2
        },
        {
            "paper_title": "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Novelseek: When Agent Becomes the Scientist-Building Closed-Loop System from Hypothesis to Verification",
            "rating": 1
        },
        {
            "paper_title": "A multi-agent-driven robotic AI chemist enabling autonomous chemical research on demand",
            "rating": 1
        }
    ],
    "cost": 0.0197505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team
1 Aug 2025</p>
<p>Weilun Yu 
School of Geographic Sciences
East China Normal University</p>
<p>Software Engineering Institute
East China Normal University</p>
<p>Shixiang Tang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Yonggui Huang 
Peking University</p>
<p>Nanqing Dong 
Shanghai Artificial Intelligence Laboratory</p>
<p>Li Fan 
School of Geographic Sciences
East China Normal University</p>
<p>Software Engineering Institute
East China Normal University</p>
<p>Honggang Qi 
School of Computer Science and Technology
University of Chinese Academy of Sciences</p>
<p>Wei Liu 
Bytedance Inc</p>
<p>Xiaoli Diao 
Canadian Collaborative Entrepreneurs Organization Inc</p>
<p>Xi Chen xchen@geo.ecnu.edu.cn 
Software Engineering Institute
East China Normal University</p>
<p>Wanli Ouyang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team
1 Aug 20253E487FE7EB36FB672F757CED65D2AF4BarXiv:2506.18348v3[cs.AI]
Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate.While recent LLMbased scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research.We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation.These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas.To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain.Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI.These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.</p>
<p>Introduction</p>
<p>With the remarkable progress of artificial intelligence (AI), autonomous scientific discovery has emerged as a promising direction for reshaping how research is conducted.Recent work such as AI Scientist (Lu et al. 2024) demonstrates the potential of large language models (LLMs) to generate scientific papers in an end-to-end manner, highlighting their capacity to drive automated discovery.However, AI Scientist adopts a single-agent design that falls short in reflecting the inherently collaborative nature of real-world scientific research (Gauch 2003).</p>
<p>To better emulate authentic scientific collaboration, LLMbased multi-agent systems are essential.Traditional multiagent systems rely on structured protocols and explicit coordination (Wen et al. 2022), whereas recent advancements allow LLM-powered agents to communicate and collaborate through natural language, enabling human-like interactions (Sclar et al. 2023;Shanahan, McDonell, and Reynolds 2023).Several systems, including VIRSCI (Su et al. 2025) and SciAgents (Ghafarollahi and Buehler 2024c), have adopted multi-agent frameworks to simulate research teams.These systems typically generate ideas through sequential discussions within fixed workflows.However, such designs fail to capture the stochastic, asynchronous, and multidirectional nature of real scientific discussions.Specifi-cally, VIRSCI underemphasizes intra-agent communication and does not model agent roles effectively, while SciAgents overlooks agent diversity, ignoring the varied expertise and perspectives that are essential to genuine collaboration.These limitations restrict the capacity of current systems to emulate the complexity of scientific practice.</p>
<p>To address these challenges, we propose IDVSCI (Internal Discussion and Vote SCIentists), a novel LLMbased multi-agent framework that improves idea generation through two key innovations: a Dynamic Knowledge Exchange mechanism and a Dual-Diversity Review paradigm.In the first stage, agents engage in cross-agent modification, contributing distinct perspectives to a proposed idea and overcoming the limitations of rigid sequential discussion.This is followed by collective aggregation, distilling the core insights into a refined idea.Finally, a dual-diversity review process assigns reviewers from varied knowledge backgrounds with tailored prompts to ensure multi-faceted evaluation.This design not only enhances the quality of assessment but also promotes inclusive and insightful review dynamics, potentially offering a blueprint for real-world scientific evaluation.In addition to proposing a novel system, we introduce a new dataset in the health sciences domain, which enables us to assess the system's applicability beyond computer science and demonstrates its potential for cross-disciplinary scientific collaboration through multiagent simulation.</p>
<p>To evaluate IDVSCI, we conduct extensive experiments on two datasets: a well-established benchmark in computer science, and our newly introduced health sciences dataset.IDVSCI achieves consistently superior performance across both, outperforming strong baselines such as AI Scientist and VIRSCI.Our results not only confirm the effectiveness of modeling agent interaction and diversity, but also uncover the potential of diverse collaboration mechanisms for advancing autonomous discovery.Our main contributions are as follows:</p>
<ol>
<li>
<p>We develop a dynamic ecosystem for idea generation in multi-agent systems by enabling cross-agent modification, collective aggregation, and iterative refinement.This framework empowers agents to contribute meaningfully and enhances overall idea quality, unlocking greater potential for innovation in complex tasks.</p>
</li>
<li>
<p>We propose a novel dual-diversity paradigm for the review process.By predefining scientists with diverse backgrounds and equipping them with context-specific prompts, our method improves the identification of truly exceptional ideas, offering insights into more effective scientific evaluation within multi-agent teams.</p>
</li>
<li>
<p>We further optimize the integration between dynamic knowledge exchange and the dual-diversity paradigm.This enhanced framework supports efficient configuration of multi-agent systems, leading to high-quality scientific ideation and laying the groundwork for autonomous discovery through collective intelligence.</p>
</li>
</ol>
<p>Related Work</p>
<p>LLM-based Agents</p>
<p>The advent of LLMs, exemplified by ChatGPT (OpenAI 2022) and LLaMA (Dubey et al. 2024), has revolutionized the capabilities of conversational agents, catalyzing a surge in research focused on LLM-based agents.These agents are sophisticated AI systems engineered to emulate specific individuals by leveraging the advanced functionalities of LLMs, such as command adherence, and social intelligence (Chen et al. 2024a).This enables them to effectively assist humans or autonomously execute tasks.LLM-based single agents have been deployed in domains such as gaming, healthcare, and professional services (Agrawal et al. 2023;Woźniak et al. 2024), functioning as game characters, medical consultants, or collaborative teammates (Niu et al. 2024).Nevertheless, single-agent system often fall short when confronted with complex tasks (Guangyao et al. 2024).To address this limitation, LLMbased multi-agent systems have been increasingly employed to tackle intricate challenges (Chen et al. 2024b).These systems typically adopt two primary strategies (Shang et al. 2024): the first involves predefining multiple agent roles and allocating specific tasks to each, building upon the singleagent framework (Qian et al. 2024;Hong et al. 2024).The second strategy employs dynamically defined roles, offering greater flexibility by allowing agents to determine tasks and roles based on factors such as memory and environmental cues (Liu et al. 2024a;Gao et al. 2024b).This dynamic approach enhances the adaptability and efficiency of multiagent systems in complex scenarios.</p>
<p>Agents in Scientific Research</p>
<p>The pioneering work of AI-Scientist (Lu et al. 2024) marked a significant milestone by exploring the potential of leveraging LLMs to simulate scientific research processes, positioning AI as a collaborative partner to human scientists.This approach enables AI agents to engage in open-ended scientific discovery, thereby assisting and, in some cases, replicating moments of human creativity and serendipitous innovation.Building on this foundation, VIRSCI (Su et al. 2025) introduced a novel multi-agent framework enhanced by reinforcement learning, further advancing the generation of scientific ideas.By employing teams of agents, VIRSCI more accurately emulates real-world research environments compared to single-agent systems, fostering the generation of richer and more diverse ideas while promoting critical evaluation and innovation.Novelseek (Team et al. 2025) complements this direction by significantly improving the efficiency and precision of automated research through interactive human-AI collaboration.</p>
<p>Agent Laboratory (Schmidgall et al. 2025) showcased the use of multi-agent systems to emulate collaborative scientific workflows, with agents capable of experimenting, analyzing, and evaluating ideas.This approach has since been applied across disciplines (Liu et al. 2024b;Song et al. 2024).For example, SciAgents (Ghafarollahi and Buehler 2024c) and AtomAgents (Ghafarollahi and Buehler 2024b) integrate materials science expertise for novel material dis-covery, while ProtAgent (Ghafarollahi and Buehler 2024a) enables cross-domain protein design.In contrast to prior approaches, our proposed system, IDVSCI, follows the same general workflow but introduces a novel strategy that simulates the behavioral dynamics of real-world scientific teams to generate more creative and diverse research ideas.As shown in Figure 1, our focus lies in facilitating early-stage ideation through agent-based collaboration, offering a distinct perspective on scientific discovery.</p>
<p>Method Task Definition</p>
<p>Our task is to design a multi-agent system comprising n scientific agents, denoted as S team = ⟨A 1 , A 2 , . . ., A n ⟩.Through communication and collaboration within a predefined workflow, the team generates a scientific abstract, formally expressed as Abstract = f (S team ), where f (•) denotes the collaborative generation process.</p>
<p>Dynamic Knowledge Exchange</p>
<p>In conventional multi-agent systems, LLM-based agents are typically assigned static roles and communicate sequentially within a fixed workflow, generating responses based on dialogue history.While effective under shared knowledge conditions, this setup becomes problematic when agents possess distinct knowledge bases, as is often the case in automated scientific research.In such scenarios, the rigid communication structure leads to inefficient knowledge sharing and repetitive outputs, particularly when model capacity is limited.Additionally, unstructured turn-taking often results in long dialogue histories, further degrading response quality.</p>
<p>To address these limitations, we propose a Dynamic Knowledge Exchange (DKE) mechanism, inspired by the review workflow used in major AI conferences, specifically the Area Chair (AC) and Program Committee (PC) model.Unlike the traditional sequential communication approach, our method introduces a leader role, analogous to the AC in conference reviews, while the other agents assume roles similar to PC.Specifically, each ordinary scientist, equipped with its unique knowledge base and prompt, is responsible for generating and presenting its ideas to the leader.The leader, leveraging its own knowledge and functional expertise, aggregates and synthesizes the outputs from all team members into a cohesive summary.</p>
<p>Formally, let A = {A 1 , A 2 , . . ., A n } denote the set of agents, where A 1 represents the leader and A 2 , . . ., A n represent the scientists, such that each scientist A i (i ≥ 2) generates an idea I i = generate(K i , P 1 ) based on their respective knowledge base K i and a given prompt P 1 .</p>
<p>These ideas are then cross-reviewed by the other scientists.That is, for each idea I i , another agent A j (j ̸ = i) produces a revised version: I ′ i,j = revise(K j , P 2 , I i ), where P 2 is a secondary prompt used during peer review.</p>
<p>The leader A 1 then synthesizes all revised ideas for each initial idea I i into a coherent summary:
S i = g {I ′ i,2 , I ′ i,3 , . . . , I ′ i,i−1 , I ′ i,i+1 , . . . , I ′ i,n }, K 1 (1)
where g(•) denotes the synthesis function, and K 1 denotes the leader's knowledge base.</p>
<p>Compared to standard sequential communication, our DKE method offers several advantages.First, it reduces redundancy by avoiding excessive reliance on long dialogue histories.Second, it enriches idea diversity through structured cross-agent review.Third, it better reflects real-world scientific workflows, where a central coordinator synthesizes insights from diverse contributors.By promoting structured, role-aware interaction and leveraging heterogeneous knowledge, DKE enhances both the efficiency and creativity of scientific idea generation in multi-agent LLM systems.</p>
<p>Dual-Diversity Review</p>
<p>To simulate a realistic scientific research environment, we introduce the Dual-Diversity Review (DDR) mechanism, which incorporates agents with diverse knowledge backgrounds and dynamically updated prompts.This design not only captures the heterogeneity of real-world research teams, but also promotes innovation by integrating varied perspectives and domain knowledge.</p>
<p>Prior to initiating automated research, the environment is configured with two core components: a set of scientists and a collection of prior publications.The latter, termed the P astP aperDataset, serves as an external knowledge source throughout the research process.Let M denote the adjacency matrix representing prior collaborations among scientists, where M i,j indicates the number of co-authored papers between A i and A j .Following the strategy of (Su et al. 2025), we mitigate the bias toward selecting familiar collaborators-an issue shown to suppress novelty (Zeng et al. 2021)-by adding 1 to all entries in M to encourage the formation of novel team compositions.To further maximize diversity, we construct teams with partially overlapping knowledge domains, allowing agents to introduce complementary ideas during discussion.The overall team diversity is quantified as:
Diversity(S team ) = n i=1 n j=i+1 Distance(K i , K j )(2)
where Distance(K i , K j ) measures the dissimilarity between the knowledge bases of agents A i and A j .</p>
<p>During the idea generation phase, the DDR mechanism continuously refines each agent's prompts by integrating the most relevant literature.Specifically, we leverage the Faiss library (Douze et al. 2024) to compute the Euclidean distances between each generated idea and papers in the database.</p>
<p>The top-k nearest papers are retrieved as reference material, allowing agents to ground their ideas in current scientific contexts.Formally, the updated prompt P ′ i for scientist A i : This approach ensures that the diversity of prompts through dynamically updated and continuously optimized to enhance creativity and impact, rather than remaining static or requiring manual intervention.By establishing a set of general rules, the system autonomously refines the prompts through dialogues with other scientists and the exploration of relevant literature.This process closely mirrors the way real-world researchers acquire new ideas, making the DDR mechanism both efficient and highly realistic.
P ′ i = P i ∪ {P aper j | P aper j ∈ Top-k(D(I, P aper))}(</p>
<p>Framework</p>
<p>Similar to the work of (Su et al. 2025), after configuring the scientist team, we divide the workflow for abstract generation into four distinct stages: Topic Discussion, Idea Generation, Check Novelty, and Abstract Generation.Among these, Idea Generation and Check Novelty are the most critical phases that determine the innovativeness of the final ideas.</p>
<p>Topic Discussion In this initial stage, we follow the approach of (Su et al. 2025), where each agent in the team generates potential topics based on a predefined prompt, its own background knowledge, and the sequential dialogue history of the team.The team then selects a final topic from the proposed topics based on a probability distribution, ensuring a balanced and representative choice.This stage ensures that the team converges on a well-defined and relevant topic, laying the foundation for the subsequent phases of idea generation and novelty checking.</p>
<p>Idea Generation Existing methods in the idea generation phase fail to adequately simulate real-world scientific discussions.These approaches typically employ a process similar to the previous Topic Discussion stage, where agents take turns generating ideas based on predefined prompts.Each agent combines its background knowledge and dialogue history to refine or generate new ideas, and the team ultimately selects the most innovative ideas based on a scoring mechanism.However, this approach lacks the collaborative and interactive nature of real-world scientific teams, particularly the brainstorming sessions that are crucial for fostering creativity.</p>
<p>To fully leverage the diversity of agent backgrounds, we introduce the DKE method.Our design is as follows: After generating an initial idea I i , each scientist A i shares I i with all other scientists except the leader.Each receiving scientist then modifies I i based on their unique background and knowledge.These modified ideas are submitted to the leader, who synthesizes the feedback along with their own knowledge to produce a consolidated revision.This revision is returned to the original scientist Scientist i , who reflects on the feedback and refines I i into a final version I ′ i .Formally, let I i denote the initial idea generated by Scientist A i , and let I ′ i,j represent the modification made by scientist A j to I i .The leader aggregates these modifications into a unified revision S i , which is then used by scientist A i to produce the final idea I ′ i :
I ′ i = Reflect(I i , A i ) (4)
where Reflect(•) represents the reflection process that integrates the leader's feedback into the initial idea.</p>
<p>Check Novelty Through empirical experimentation, we observed that during the novelty assessment phase, different scientists occasionally generate identical responses for the same idea, producing highly similar thoughts and reasonings.Upon removing a subset of the reference papers, we noted a significant reduction in this phenomenon.As illustrated in Figure 3, an increase in the number of reference papers tends to result in agents with different backgrounds generating identical outputs.Conversely, reducing the num-Figure 3: A case study illustrates the responses generated by agents utilizing the same LLM when provided with 8 and 2 references, respectively, as well as the final output results produced by the multi-agent system.ber of reference documents leads to more diverse and distinct outputs.This observed relationship highlights the critical importance of optimizing both the quantity and quality of reference materials to enhance the variability and originality of the generated content.</p>
<p>To address the challenges posed by the large volume of reference papers and to ensure comprehensive coverage without omission, we evenly distributed the related papers among all scientists.This approach ensures that each scientist evaluates a manageable subset of the literature, while collectively covering the entire body of relevant work.To further integrate the diverse perspectives of these scientists with varied backgrounds in a fair and systematic manner, we employ a weighted Borda count voting mechanism.This method is particularly suitable for ranking ideas based on their novelty, feasibility, and potential impact, as it allows for the aggregation of rankings and confidence scores from multiple evaluators.The Borda count was chosen because it not only accounts for the relative rankings of ideas but also incorporates the evaluators' confidence in their assessments, ensuring a balanced and nuanced evaluation process.</p>
<p>Let I = {i 1 , i 2 , . . ., i n } represent the set of ideas, where each idea i k is assigned a score based on the voting procedure.Let V j denote the votes from the j-th scientist, where V j is a tuple consisting of the rankings r jk of the ideas and the confidence scores c jk associated with these rankings.The Borda score B k for idea i k is computed as follows:
B k = m j=1 (n − r jk ) × c jk 10 (5)
where n is the total number of ideas, r jk is the rank assigned to idea i k by scientist j, c jk is the confidence score given by scientist j for idea i k , and m is the total number of scientists.The idea with the highest Borda score B k is selected as the final choice, reflecting the team's consensus on the novelty and feasibility of the idea.</p>
<p>Abstract Generation In this final phase, since the ideas have already been generated, our primary objective is to ensure that the quality of the generated abstract is solely influenced by the innovativeness of the ideas.To achieve this, we adopt a methodology consistent with previous approaches.Specifically, each scientist generates an initial draft of the abstract based on the finalized ideas.Subsequently, the scientists take turns sequentially refining the abstract.This iterative modification process ensures that the final output remains closely tied to the quality of the underlying ideas, minimizing external influences on the abstract's content.By employing this method, we ensure that the quality of the generated abstract is primarily determined by the innovativeness and clarity of the ideas themselves, rather than being influenced by extraneous factors.This approach not only maintains the integrity of the abstract generation process but also aligns with the goal of producing high-quality, ideadriven summaries that accurately reflect the scientific contributions of the team.</p>
<p>Experiment Dataset</p>
<p>For our experimental framework, we employed the publicly available Computer Sciences Dataset, derived from the AMiner1 computer science repository (Tang et al. 2008).The final curated dataset comprised 156 researchers and 85,217 publications, which served as the foundation for ecosystem construction and agent initialization.All publication and researcher metadata were processed using the mxbai-embed-large embedding model (Lee et al. 2024) for feature representation.Inspired by Novelseek (Team et al. 2025), which demonstrates the effectiveness of human-AI collaboration in multidisciplinary scientific applications, we further introduced a new dataset-the Health Sciences Dataset-to evaluate the cross-domain scalability of our method in promoting innovative idea discovery.This dataset includes 130 researchers and 86,448 publications, collected from PubMed, and follows the same preprocessing pipeline as the Computer Sciences Dataset for ecosystem construction and agent initialization.</p>
<p>Experimental Setup</p>
<p>The proposed framework is developed utilizing the Agentscope architecture (Gao et al. 2024a), a robust platform designed for constructing multi-agent systems powered by LLMs.For performance evaluation, we employ open-source LLMs with varying computational scales, including LLaMA-3.1 models with 8b and 70b parameters (Dubey et al. 2024), as well as the recently released QWQ-32b model (Team 2025).All models are accessed</p>
<p>Evaluation Metrics</p>
<p>Currently, no single evaluation metric can fully capture the innovativeness of scientific outputs.In line with VIRSCI's methodology, we utilize four established metrics to provide a partial but insightful evaluation of scientific innovation.1. Historical Dissimilarity (HD): Defined as the average squared Euclidean distance between the embedding vector of the generated abstract and those of the 5 most similar abstracts in the corpus of pre-2011 literature (Shao et al. 2020;Zhou et al. 2024).A larger distance indicates higher dissimilarity from historical works, reflecting greater potential for innovation.</p>
<ol>
<li>
<p>Contemporary Dissimilarity (CD): Calculated as the average squared Euclidean distance between the embedding vector of the generated abstract and those of the 5 most similar abstracts in the corpus of post-2011 literature.A smaller distance suggests higher similarity to recent works, also indicating greater potential for innovation.</p>
</li>
<li>
<p>Contemporary Impact (CI): This reflects the citation count of the top 5 most similar abstracts in the corpus of articles published after 2011 (Yang et al. 2022).A higher citation count suggests that the generated abstract is likely to have a greater impact.</p>
</li>
</ol>
<p>To ensure comparability, each calculated metric is normalized using the mean value derived from the entire corresponding database, with normalization defined as the metric divided by its mean value.</p>
<ol>
<li>Overall Novelty (ON): ON is positively correlated with HD and CI and negatively correlated with CD.It is calcu-lated as ON = HD×CI CD .Mathematically, the expected value of ON is proportional to the true novelty.</li>
</ol>
<p>Results</p>
<p>To ensure fair comparison, we evaluate all methods using models with aligned parameter scales and similar architectures.As shown in Table 1, our method IDVSCI consistently outperforms AI-Scientist and VIRSCI across all metrics in both Computer Sciences and Health Sciences domains.In the Computer Sciences domain, IDVSCI (LLaMA-70b) achieves the highest CI and ON, while maintaining competitive HD and CD, indicating its ability to generate both novel and scientifically valuable ideas.In Health Sciences, IDVSCI again shows strong performance, achieving the highest CI (10.06) and ON (10.58), demonstrating its cross-domain adaptability and capacity to produce impactful, domain-relevant content.</p>
<p>While larger models generally yield better CI and ON, we observe that IDVSCI with QWQ does not always outperform its LLaMA-8b variant, suggesting that increasing model size alone does not guarantee performance gains in more complex multi-agent settings.Overall, these results demonstrate the effectiveness and robustness of IDVSCI, while offering insights into the interplay between model scale, output complexity, and system design in scientific idea generation.</p>
<p>Ablation Experiments</p>
<p>Module-wise Ablation Study To evaluate the contribution of each component, we conducted ablation experiments (Table 2).Removing the internal discussion module in the Idea Generation stage leads to the most significant performance degradation, with CI dropping from 4.38 to 4.10 (-6.4%).This confirms that structured discussions substantially enhance the impact of generated ideas.Interestingly, removing the voting mechanism in the Check Novelty stage slightly increases CI (4.38 → 4.42, +0.9%), but at the expense of ON, which decreases from 4.49 to 4.42.This suggests that while voting may sometimes constrain idea impact, it plays an essential role in preserving originality by avoiding overconvergence in novelty evaluation.</p>
<p>Iteration-wise Ablation Study The number of discussion turns is a critical factor influencing inference cost, as each additional round requires further computation and communication among agents.As shown in Table 3, the creativity Table 2: Comparison results between our method and the method of removing different modules in it.Where "-" denotes the removal of the module.</p>
<p>HD↑ CD↓ CI↑ ON↑ -Team Discuss 0.41 0.38 4.10 4.42 -  and impact of ideas do not vary substantially with different iteration counts.Specifically, ON values exhibit very low variance (0.014), with a mean of 4.46 and a maximum deviation of only 0.16.Similarly, CI scores remain stable, ranging narrowly between 4.18 and 4.45.These results suggest that a single iteration already provides strong performance (ON = 4.61, CI = 4.38), while additional turns bring limited or inconsistent gains (e.g., CI drops to 4.18 at iteration 4).From a cost-effectiveness perspective, our framework achieves near-optimal outcomes with just 1-2 iterations, underscoring its efficiency in balancing creativity, novelty, and computational overhead.</p>
<p>Impact of Agent Background Diversity</p>
<p>As shown in Figure 4, when the backgrounds of the scientists constituting the research team are entirely homoge-  neous, the generated ideas perform the poorest in terms of both innovativeness and impact.The model achieves optimal performance when the background diversity reaches 25%.At 50% and 100% diversity levels, the effects are comparable, though slightly inferior to the former.This aligns closely with our understanding of real-world research teams.</p>
<p>Impact Analysis of Our Method on Internal and External Teams</p>
<p>As illustrated in Table 4, the application of our method to generate summaries with the assistance of external teams-External Review SCIentists (ERSCI)-results in inferior outcomes compared to those produced through internal collaborative discussions.While both approaches achieve identical HD and CD values, IDVSCI demonstrates a superior performance in CI.This discrepancy can be primarily attributed to the limited communication between the external method and the originators of ideas, which leads to information distortion during the transmission process.</p>
<p>Impact of Team Size</p>
<p>The experimental results from the VIRSCI team indicate that their multi-agent research team performs optimally when the team size is 4 and 8.In previous comparisons with the SOTA models, to avoid unfair comparisons with methods like AI-Scientist due to excessively large team sizes, we only compared the results for a team size of 4. As can be seen from Figure 5, our method still achieves the best performance at a team size of 8.However, compared to VIRSCI, our method does not show significant improvement when scaling from 4 to 8 members, suggesting that our approach can achieve desirable outcomes without incurring substantial additional costs.</p>
<p>Conclusion</p>
<p>By establishing a dynamic knowledge-exchange ecosystem that incorporates cross-agent modification, collective aggre-gation, and iterative refinement, alongside a novel dualdiversity paradigm for the review process, we enhance both the idea-generation and evaluation mechanisms within multi-agent systems.Furthermore, we introduce a new dataset in the health sciences domain, enabling the exploration of multi-agent scientific systems in cross-disciplinary settings.This research paves the way for more faithful emulation of real-world scientific practices, more inclusive and rigorous peer review, and ultimately, deeper scientific breakthroughs-by unlocking the collective intelligence of multiagent systems and advancing diverse modes of collaborative research.</p>
<p>Health Sciences Dataset LGBT health Citations: 12 Abstract: Although there is substantial literature about sexual minority women's mental health and use of alcohol, tobacco, and other drugs (ATOD), only recently has attention been focused on chronic physical health disorders thought to stem from stress and exposure to ATOD use.The most extensively studied aspect of physical health has been weight, with the majority of studies reporting higher prevalence of overweight and obesity.</p>
<p>In addition, many studies report higher levels of stressful experiences in both childhood and adulthood for sexual minority women.In this paper, the hypothesized relationship between stress, unhealthy behaviors, and five common chronic physical health disorders is explored via review of the literature.Only asthma appeared to be consistently more common in sexual minority women, and few or no differences in diabetes, hypertension, cardiovascular disease, and most cancers were found.The limitations of these studies are reviewed, and the need for studies that directly address the relationships among stress, health-damaging practices, and chronic disorder is emphasized.However, if these findings hold up, and sexual minority women are not more prone to these disorders, the field may need better theoretical frameworks from which to explore potential differences in the manifestation of mental versus chronic physical health disparities.Generated Abstract(VIRSCI: LLAMA-70b) Title: Fostering Resilience in Breast Cancer Recovery through a Novel Social Support-Integrated Gratitude-Mindfulness Intervention with Neural Plasticity and Opportunity Recognition Assessments Abstract: Breast cancer patients undergoing reconstructive surgery face significant physical, emotional, and psychological challenges.This mixed-methods study introduces a novel social support-integrated gratitudemindfulness intervention designed to enhance neural plasticity, opportunity recognition, physical health outcomes, and social support network dynamics.The primary objective of this research is to design, implement, and evaluate the effects of an 8-week combined program on breast cancer patients (n = 100) and their designated social support partners.Participants will be randomly assigned to either an intervention or control group, with the former receiving weekly sessions focusing on mindfulness techniques, gratitude practices, and social support network engagement.Outcome measures include assessments of neural plasticity via functional magnetic resonance imaging (fMRI), opportunity recognition using a standardized questionnaire developed by our research team, physical health outcomes through clinical assessments and questionnaires, and social support network dynamics.Data analysis will involve repeated-measures ANOVA to compare changes between the intervention and control groups, as well as correlation analyses to identify relationships between outcome measures.Preliminary findings indicate significant improvements in neural plasticity, opportunity recognition, and physical health outcomes among participants in the intervention group compared to the control group.Furthermore, results suggest that social support partners play a crucial role in facilitating resilience throughout the intervention period.This study contributes to the existing literature by providing a new paradigm for breast cancer recovery, emphasizing the importance of integrating social connections and mindfulness practices into traditional treatment protocols.The findings have significant implications for the development of innovative interventions aimed at promoting resilience among breast cancer patients undergoing reconstructive surgery.Notably, our research highlights the potential benefits of incorporating gratitude-mindfulness interventions in the context of plastic surgery, particularly during the recovery period.By fostering a supportive environment and encouraging mindfulness practices, healthcare providers can play a vital role in enhancing patient outcomes and overall well-being.Moreover, this study underscores the importance of considering the spotlight effect, where individuals tend to overestimate the extent to which others notice their appearance.By addressing this phenomenon through social support-integrated interventions, patients may experience improved body image satisfaction and reduced anxiety related to their appearance.Our research team has also integrated a novel aspect of opportunity recognition, which is essential for resilience in breast cancer recovery.We have developed a standardized questionnaire to assess opportunity recognition, providing valuable insights into the relationship between social support, mindfulness practices, and physical health outcomes.The study's limitations include the relatively small sample size and the need for further research to explore the long-term effects of the intervention.Future studies should aim to replicate these findings with larger samples and investigate the potential benefits of this intervention in other populations.In conclusion, this mixed-methods study provides a comprehensive understanding of the effects of a novel social support-integrated gratitude-mindfulness intervention on breast cancer recovery outcomes.The findings have significant implications for the development of innovative interventions aimed at promoting resilience among breast cancer patients undergoing reconstructive surgery.Furthermore, our research highlights the importance of considering the role of social support partners in facilitating resilience throughout the intervention period.By integrating social support-integrated gratitude-mindfulness interventions into existing treatment protocols, healthcare providers can play a vital role in enhancing patient outcomes and overall well-being during this critical period.Overall, this study demonstrates the potential benefits of incorporating novel interventions that integrate social connections, mindfulness practices, and opportunity recognition to promote resilience among breast cancer patients undergoing reconstructive surgery.The findings have significant implications for the development of innovative interventions aimed at promoting resilience among breast cancer patients.HD: 0.35; CD: 0.36; CI: 7.01</p>
<p>Figure 1 :
1
Figure 1: Illustration of the workflow comparison between conventional approaches and our proposed methodology (IDVSCI), showing how agent-based collaboration enhances both the Idea Generation Stage and the Check Novelty Stage.</p>
<p>3)where Top-k(D(I, P aper)) returns the k papers closest to idea I based on Euclidean distance.</p>
<p>Figure 2 :
2
Figure 2: The workflow framework of our method designed for automated scientific research.It consists of four sequential steps: Topic Discussion, Idea Generation, Check Novelty, and Abstract Generation.The line segments represent the transmission of information, with the direction of the arrows indicating the recipient agents.</p>
<p>Figure 4 :
4
Figure 4: Comparative experimental data for visualizing generated results under different levels of background diversity.</p>
<p>Figure 5 :
5
Figure 5: Comparison chart of results when team size is 8.</p>
<p>Figure 6 :
6
Figure 6: Health Sciences Dataset as the foundation for ecosystem construction and agent initialization.</p>
<p>Figure 8 :
8
Figure 8: Example abstract generated by the baseline VIRSCI model, also targeting psychological rehabilitation and intervention in the same clinical context.</p>
<p>Table 1 :
1
Comparison of our model with state-of-the-art baselines experimental results
Computer SciencesHealth SciencesModelHD↑ CD↓ CI↑ ON↑ HD↑ CD↓CI↑ON↑AI-Scientist: LLAMA-8b0.51 0.49 2.12 2.21----AI-Scientist: LLAMA-70b 0.53 0.48 2.11 2.33----VIRSCI: LLAMA-8b0.43 0.42 3.29 3.40 0.39 0.377.708.12VIRSCI: QWQ-32b0.46 0.40 3.38 3.89 0.40 0.387.828.23VIRSCI: LLAMA-70b0.44 0.40 3.36 3.70 0.39 0.377.848.26IDVSCI: LLAMA-8b0.40 0.39 4.38 4.49 0.40 0.398.378.58IDVSCI: QWQ-32b0.41 0.40 4.17 4.27 0.42 0.408.098.49IDVSCI: LLAMA-70b0.40 0.39 4.49 4.60 0.41 0.39 10.06 10.58through the Ollama interface and executed on four NVIDIARTX 4090 GPUs. This diversity of backbone models enablesus to assess the performance of our framework across differ-ent levels of language model capacity. Mirroring the experi-mental protocol established in VIRSCI, our implementationfeatures a four-agent system conducting iterative discussionsacross five sequential rounds. To ensure statistical reliabil-ity, all performance metrics are derived from an average of20 independent experimental trials. To balance the numberof reference papers and avoid an excessive or insufficient se-
lection, we empirically set k = 8 for Top-k(D(I, Paper j )) in the Check Novelty phase.We conducted experiments on both datasets when evaluating the state-of-the-art baselines.All subsequent experiments were conducted solely on the Computer Sciences Dataset.</p>
<p>Table 3 :
3
Team Vote 0.38 0.38 4.42 4.42 IDVSCI 0.40 0.39 4.38 4.49 Comparison of results generated by our method across different rounds of discussion.
Iteration HD↑ CD↓ CI↑ ON↑10.40 0.38 4.38 4.6120.39 0.38 4.24 4.3530.41 0.40 4.45 4.5640.38 0.37 4.18 4.3050.40 0.39 4.38 4.49</p>
<p>Table 4 :
4
Comparison of external vs. internal teams using our methodology.
HD↑ CD↓ CI↑ ON↑ERSCI 0.40 0.39 4.04 4.14ours0.40 0.39 4.38 4.49</p>
<p>Your name is Scientist3, you belong to following affiliations ['Department of Dermatology, Health New Zealand Te Whatu Ora Auckland, New Zealand.'],youhave researched on following topics ['<em>Vulvar Diseases/drug therapy', '</em>Hydroxychloroquine/therapeutic use/adverse effects/administration &amp; dosage', 'Female', 'Treatment Outcome', '*Lichen Planus/drug therapy'], you have published 1 papers,you have previously collaborated with these individuals ['Scientist4', 'Scientist5'].Swiss Institute for the Prevention of Alcohol and other Drug Problems, Lausanne, Switzerland; Social Prevention and Health Policy Research Department, Addiction Research Foundation, Toronto, Canada;Department of Public Health Sciences, Faculty of Medicine, University of Toronto, Canada; WHO, Geneva, Switzerland.The objects of this study were (1) to review systematically Skog's theory of collective drinking behaviour and its interpretations by alcohol researchers, and (2) to give examples of how Skog's theory and these different interpretations have been empirically tested and to indicate how they might be tested.Based on a computer-aided search of the literature, a reconstruction of the theory and possible alternative interpretations is provided.Different interpretations of Skog's theory are possible and can be found in the literature.Surprisingly, there is little empirical evidence, especially recent evidence, to support Skog's key assumptions.Suggestions for further research are given.
Health Sciences AuthorHealth Sciences Past Paper DatasetTitle: The empirical testability of Skog's theory of collective drinking behaviour.PMID: 28474448Authors: Gmel G, Rehm JAffiliations: Year: 2000Venue: Drug and alcohol reviewCitations: 7Abstract: Health Sciences Future Paper DatasetTitle: Chronic Physical Health Problems in Sexual Minority Women: Review of the Literature.PMID: 26789854Authors: Eliason MJAffiliations: Department of Health Education, San Francisco State University , San Francisco, California.Year: 2014Venue:</p>
<p>Fostering Resilience in Breast Cancer Patients Undergoing Reconstructive Surgery: A Novel Gratitude-Mindfulness Intervention with Neuro-Emotional Technique (NET) and Loving-Kindness Meditation (LKM) Abstract: Breast cancer patients undergoing reconstructive surgery often experience significant psychological distress, compromising their resilience and overall well-being.Despite advances in medical care, there remains a critical need for innovative interventions that promote resilience and self-care in this vulnerable population.Recent studies have highlighted the importance of gratitude and mindfulness practices in enhancing neural plasticity, opportunity recognition, and physical health outcomes.This randomized controlled trial aims to evaluate the efficacy of an 8-week novel gratitude-mindfulness intervention combined with Neuro-Emotional Technique (NET) sessions and Loving-Kindness Meditation (LKM) in enhancing neural plasticity, opportunity recognition, physical health outcomes, and psychological well-being in breast cancer patients undergoing reconstructive surgery.The study also explores the potential benefits of integrating Eastern Body-Mind-Spirit (BMS) Group Intervention model, mindfulness-based stress reduction (MBSR), LKM, and NET to support the complex needs of this population.A total of 120 breast cancer patients will be recruited and randomly assigned to either an intervention group or a control group.The intervention group will participate in weekly sessions combining gratitude practices, mindfulness exercises, LKM, and NET sessions to address cancer-related traumatic stress symptoms.Outcome measures include functional magnetic resonance imaging (fMRI) scans, surveys assessing opportunity recognition, standardized questionnaires evaluating physical health outcomes, psychological well-being, quality of life, and social support networks.Preliminary findings indicate significant improvements in neural plasticity, opportunity recognition, and physical health outcomes among participants in the intervention group compared to the control group.Notably, fMRI scans reveal increased activity in brain regions associated with emotional regulation and resilience.Participants in the intervention group also report enhanced psychological well-being, reduced stress levels, and improved quality of life.Furthermore, results suggest that the integration of NET and LKM enhances the effectiveness of the gratitude-mindfulness intervention.This study provides preliminary evidence for the efficacy of a novel gratitude-mindfulness intervention combined with NET and LKM in promoting resilience and enhancing physical health outcomes in breast cancer patients undergoing reconstructive surgery.The findings have important implications for the development of innovative interventions that integrate Eastern Body-Mind-Spirit (BMS) Group Intervention model, mindfulnessbased stress reduction (MBSR), LKM, and NET to support the complex needs of this population.Future studies should aim to replicate these results and explore the long-term benefits of this intervention.Our study contributes to the existing literature by introducing a novel gratitude-mindfulness intervention that combines multiple evidence-based approaches to promote resilience in breast cancer patients undergoing reconstructive surgery.The integration of NET and LKM with gratitude practices and mindfulness exercises provides a comprehensive framework for addressing the complex needs of this population.By exploring the neural mechanisms underlying the intervention's effects, we can gain a deeper understanding of how these approaches influence emotional regulation, opportunity recognition, and physical health outcomes.The results of this study have significant implications for clinical practice, highlighting the potential benefits of incorporating gratitude-mindfulness interventions into standard care for breast cancer patients undergoing reconstructive surgery.By promoting resilience and enhancing physical health outcomes, these interventions can improve quality of life and reduce healthcare costs associated with chronic stress and anxiety.Moreover, our research highlights the importance of considering the interplay between psychological, social, and spiritual factors in the development of effective interventions for this population.The integration of NET and LKM with gratitude practices and mindfulness exercises provides a unique opportunity to explore the complex relationships between these factors and their impact on resilience and physical health outcomes.In conclusion, our study demonstrates the potential benefits of a novel gratitude-mindfulness intervention combined with NET and LKM in promoting resilience and enhancing physical health outcomes in breast cancer patients undergoing reconstructive surgery.The findings of this study have important implications for clinical practice and highlight the need for further research into the development of innovative interventions that address the complex needs of this population.
Generated Abstract(IDVSCI: LLAMA-70b(ours))
Title: HD: 0.31; CD: 0.34; CI: 17.63 Figure 7: Example abstract generated by our IDVSCI model, focusing on psychological rehabilitation and intervention for breast cancer patients.</p>
<p>https://www.aminer.cn/aminernetwork</p>
<p>Multimodal Persona Based Generation of Comic Dialogs. H Agrawal, A Mishra, M Gupta, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>J Chen, X Wang, R Xu, S Yuan, Y Zhang, W Shi, J Xie, S Li, R Yang, T Zhu, A Chen, N Li, L Chen, C Hu, S Wu, S Ren, Z Fu, Y Xiao, From Persona to Personalization: A Survey on Role-Playing Language Agents. Transactions on Machine Learning Research. 2024aSurvey Certification</p>
<p>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. W Chen, Y Su, J Zuo, C Yang, C Yuan, C.-M Chan, H Yu, Y Lu, Y.-H Hung, C Qian, Y Qin, X Cong, R Xie, Z Liu, M Sun, J Zhou, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>M Douze, A Guzhva, C Deng, J Johnson, G Szilvasy, P.-E Mazaré, M Lomeli, L Hosseini, H Jégou, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2401.08281arXiv:2407.21783The faiss library. 2024arXiv preprintet al. 2024. The llama 3 herd of models</p>
<p>D Gao, Z Li, X Pan, W Kuang, Z Ma, B Qian, F Wei, W Zhang, Y Xie, D Chen, arXiv:2402.14034Agentscope: A flexible yet robust multi-agent platform. 2024aarXiv preprint</p>
<p>REA: Towards A Reusable Experience Accumulation with 360 Assessment for Multi-Agent System. S Gao, H Li, Z Shi, C Huang, Q Tu, S Shang, Z Tian, M Huang, Findings of the Association for Computational Linguistics ACL 2024. 2024b360</p>
<p>Scientific method in practice. H G Gauch, 2003Cambridge University Press</p>
<p>ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning. A Ghafarollahi, M Buehler, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024a</p>
<p>AtomAgents: Alloy design and discovery through physics-aware multimodal multi-agent artificial intelligence. A Ghafarollahi, M J Buehler, A Ghafarollahi, M J Buehler, arXiv:2407.10022Advanced Materials. 24135232024b. 2024carXiv preprintSciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning</p>
<p>AutoAgents: A Framework for Automatic Agent Generation. C Guangyao, D Siwei, S Yu, Z Ge, S Jaward, K Börje, F Jie, S Yemin, IJCAI. 2024</p>
<p>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, S Lee, A Shakir, D Koenig, J Lipp, The Twelfth International Conference on Learning Representations. 2024. 2024Open source strikes bread-new fluffy embeddings model</p>
<p>Autonomous Agents for Collaborative Task under Information Asymmetry. W Liu, C Wang, Y Wang, Z Xie, R Qiu, Y Dang, Z Du, W Chen, C Yang, C Qian, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024a</p>
<p>Harnessing Data-Intelligence-Intensive Multi-Agent System for. Y Liu, R Shen, L Zhou, Q Xiao, J Yuan, Y Li, Life Science Research. bioRxiv. 2024b</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>ScreenAgent: A Vision Language Model-driven Computer Control Agent. R Niu, J Li, S Wang, Y Fu, X Hu, X Leng, H Kong, Y Chang, Q Wang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. K Larson, the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-242024OpenAITechnical report</p>
<p>ChatDev: Communicative Agents for Software Development. C Qian, W Liu, H Liu, N Chen, Y Dang, J Li, C Yang, W Chen, Y Su, X Cong, J Xu, D Li, Z Liu, M Sun, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Minding Language Models'(Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker. S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, M Sclar, S Kumar, P West, A Suhr, Y Choi, Y Tsvetkov, arXiv:2501.04227Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2025. 20231arXiv preprintAgent Laboratory: Using LLM Agents as Research Assistants</p>
<p>Role play with large language models. M Shanahan, K Mcdonell, L Reynolds, Nature. 62379872023</p>
<p>Unified Multi-Scenario Summarization Evaluation and Explanation. S Shang, Z Yao, H Fu, C Tao, X Chen, F Wang, Y Wang, Z Ren, S Gao, IEEE Transactions on Knowledge and Data Engineering. 2024</p>
<p>BERT-PLI: Modeling paragraph-level interactions for legal case retrieval. Y Shao, J Mao, Y Liu, W Ma, K Satoh, M Zhang, S Ma, IJCAI. 2020</p>
<p>A multi-agent-driven robotic AI chemist enabling autonomous chemical research on demand. T Song, M Luo, L Chen, Y Huang, Q Zhu, D Liu, B Zhang, G Zou, F Zhang, W Shang, 2024</p>
<p>Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System. H Su, R Chen, S Tang, Z Yin, X Zheng, J Li, B Qi, Q Wu, H Li, W Ouyang, P Torr, B Zhou, N Dong, Proceedings of the 63nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63nd Annual Meeting of the Association for Computational Linguistics20251</p>
<p>. J Tang, J Zhang, L Yao, J Li, L Zhang, Z Su, </p>
<p>Arnetminer: extraction and mining of academic social networks. Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</p>
<p>N Team, B Zhang, S Feng, X Yan, J Yuan, Z Yu, X He, S Huang, S Hou, Z Nie, arXiv:2505.16938NovelSeek: When Agent Becomes the Scientist-Building Closed-Loop System from Hypothesis to Verification. 2025arXiv preprint</p>
<p>QwQ-32B: Embracing the Power of Reinforcement Learning. Q Team, 2025</p>
<p>Multi-agent reinforcement learning is a sequence modeling problem. M Wen, J Kuba, R Lin, W Zhang, Y Wen, J Wang, Y Yang, Advances in Neural Information Processing Systems. 202235</p>
<p>S Woźniak, B Koptyra, A Janz, P Kazienko, J Kocoń, arXiv:2402.09269Personalized large language models. 2024arXiv preprint</p>
<p>Gender-diverse teams produce more novel and higher-impact scientific ideas. Y Yang, T Y Tian, T K Woodruff, B F Jones, B Uzzi, Proceedings of the National Academy of Sciences. the National Academy of Sciences2022119e2200841119</p>
<p>Fresh teams are associated with original and multidisciplinary research. A Zeng, Y Fan, Z Di, Y Wang, S Havlin, Nature human behaviour. 5102021</p>
<p>Fine-grained distillation for long document retrieval. Y Zhou, T Shen, X Geng, C Tao, J Shen, G Long, C Xu, D Jiang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 19732-1974038</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-803 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-803</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-803</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-253107789</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.14162v1.pdf" target="_blank">Commonsense Knowledge from Scene Graphs for Textual Environments</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games are becoming commonly used in reinforcement learning as real-world simulation environments. They are usually imperfect information games, and their interactions are only in the textual modality. To challenge these games, it is effective to complement the missing information by providing knowledge outside the game, such as human common sense. However, such knowledge has only been available from textual information in previous works. In this paper, we investigate the advantage of employing commonsense reasoning obtained from visual datasets such as scene graph datasets. In general, images convey more comprehensive information compared with text for humans. This property enables to extract commonsense relationship knowledge more useful for acting effectively in a game. We compare the statistics of spatial relationships available in Visual Genome (a scene graph dataset) and ConceptNet (a text-based knowledge) to analyze the effectiveness of introducing scene graph datasets. We also conducted experiments on a text-based game task that requires commonsense reasoning. Our experimental results demonstrated that our proposed methods have higher and competitive performance than existing state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e803.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e803.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWC agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TextWorld Commonsense (TWC) baseline agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based RL agent that augments textual observations with an external commonsense subgraph (originally extracted from ConceptNet) and selects actions via learned policy; its belief is represented as a dynamic commonsense graph combined with observational context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TWC agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular architecture with six components: (a) action encoder (encodes admissible actions), (b) observation encoder (encodes current textual observation o_t), (c) context encoder (encodes dynamic context C_t), (d) dynamic commonsense subgraph G_t^C (retrieved from external knowledge like ConceptNet and updated over time), (e) knowledge integration (encodes the commonsense graph via pre-trained embeddings + sentinel, passes messages with Graph Attention Networks (GAT), and integrates with observation via Co-Attention), and (f) action selection (policy learned with Advantage Actor-Critic). The dynamic commonsense subgraph is constructed by extracting entities from observations, merging with previous graph G_{t-1}^C to form cumulative entity set E_t, and adding links via the Context Direct Connections (CDC) algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) / TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text-based game environment where the agent must tidy up rooms by putting objects in appropriate locations; observations and actions are purely textual and the environment is partially observable (agent cannot directly observe global world state), requiring commonsense to infer object-place relationships and to plan multi-step actions across rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet (textual commonsense knowledge graph) as an external knowledge base; occasionally manually-prepared subgraphs (ConceptNet + Manual) used in baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph triplets (entity, relation, entity), lists of entities and relations (including 'at location'), and graph connectivity information; embeddings derived from these triplets (Numberbatch or GloVe) when encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic commonsense subgraph G_t^C combined with dynamic context C_t and cumulative entity set E_t; node features are embedded (Numberbatch or GloVe) with a sentinel vector, and node representations are updated via Graph Attention Network (GAT) message passing; observational context and graph embeddings are combined via Co-Attention to form the agent's integrated belief/context for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each time step the agent extracts entities from the new textual observation, merges them with previous graph entities to form E_t, constructs/updates the commonsense subgraph G_t^C by retrieving matching triplets from the external KB and adding links via the Context Direct Connections (CDC) algorithm, then recomputes node embeddings (pretrained embeddings + sentinel) and runs GAT message passing; finally integrates updated graph embeddings with the current observational context via Co-Attention.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Advantage Actor-Critic) using integrated observation + commonsense graph features; not a search-based or explicit model-based planner in this implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an external commonsense graph improves action selection in a partially observable text environment; representing belief as a dynamic commonsense subgraph that is updated from observations and external KB lookups and integrated via GAT + Co-Attention yields better performance than no external commonsense, and textual ConceptNet knowledge can be complemented by more spatially-detailed visual scene graphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e803.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e803.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SceneGraph agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scene Graph external-knowledge agent (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the TWC agent that replaces ConceptNet with Visual Genome (a scene graph dataset) as the external knowledge source; it constructs dynamic commonsense subgraphs from VG triplets and integrates them into the agent belief the same way as the TWC agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Scene Graph agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same six-component architecture as the TWC agent (action encoder, observation encoder, context encoder, dynamic commonsense subgraph, knowledge integration, action selection). The external knowledge source is Visual Genome (VG) scene graph triplets; node embeddings use GloVe in experiments, followed by graph encoding with GAT and integration via Co-Attention with observation features. The dynamic commonsense subgraph is built by matching entities extracted from observations to VG triplets and using CDC-style linking (cumulative merging with previous G_{t-1}^C). Policy learned with Advantage Actor-Critic.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) / TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same TWC tidy-up games as above; partially observable text-only environment where spatial place relations between objects are the key external commonsense needed.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Visual Genome (VG) scene graph dataset used as an external commonsense knowledge source.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured scene-graph triplets (object, relationship, object), with many explicit spatial relations ('on', 'in', 'under', 'with', 'has'), i.e., structured relational data.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic commonsense subgraph G_t^C derived from scene-graph triplets matched to entities from textual observations, maintained cumulatively (E_t) and encoded via word embeddings (GloVe) + sentinel and message passing with GAT; integrated with observation context via Co-Attention to form the agent belief/context.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Entities extracted from each textual observation are combined with previously stored entities to produce E_t; matching triplets are retrieved from Visual Genome to populate/update G_t^C; graph node embeddings are updated and propagated with GAT; integrated with the observational context via Co-Attention for downstream policy/action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Advantage Actor-Critic) that conditions on integrated observation + scene-graph-derived commonsense features; no explicit search-based or shortest-path planner is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacing textual ConceptNet with scene graphs (VG) yields more spatially-detailed commonsense and improves agent efficiency and generalization (especially on unseen entities), because VG contains many direct spatial relations useful for place inference; scene-graph-based agents converge faster in many settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e803.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e803.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet+SG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential ConceptNet then Scene Graph training agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-style variant: agent is trained first with ConceptNet external knowledge and then continued training with Visual Genome scene graph knowledge, aiming to combine abstract textual commonsense and concrete spatial knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ConceptNet+SG agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same base architecture as the TWC agent; training proceeds in two phases (curriculum learning): (1) train agent for N episodes using ConceptNet as external KB, then (2) continue training for additional episodes using Visual Genome (VG) as the external KB. Graph encoding (GloVe embeddings + GAT) and integration (Co-Attention) remain the same. Policy training uses Advantage Actor-Critic.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) / TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same partially observable TWC tidy-up text games; tasks require inference of object-goal location relations from commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet (textual commonsense graph) and Visual Genome (scene graph dataset) used sequentially as external KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph triplets from ConceptNet and VG; ConceptNet provides coarser textual relations (e.g., 'at location'), VG provides many explicit spatial relations ('in','on','under','with','has'), and the agent uses embeddings derived from these triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic commonsense subgraph G_t^C updated from the currently-active external KB (ConceptNet in phase 1, VG in phase 2), merged cumulatively across timesteps into E_t and encoded via embeddings + GAT; integrated with observation via Co-Attention.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Same subgraph update procedure as TWC agent: extract entities from observation, merge with previous entities to form E_t, retrieve matching triplets from current external KB and add CDC links to form/update G_t^C, encode nodes and run GAT message passing, then integrate with observational context via Co-Attention.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Advantage Actor-Critic) leveraging a richer commonsense belief formed by sequential exposure to two KBs; not an explicit search planner.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sequentially training on ConceptNet then Visual Genome yields robust performance and better generalization to unseen objects (OUT sets) compared to single-KB training: ConceptNet gives broad abstract commonsense while VG adds concrete spatial relations, improving overall competence on TWC tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e803.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e803.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Worldformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Worldformer (Knowledge Graph-based World Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model (mentioned in related work) that represents environment state as a knowledge graph and learns a world model to predict state changes caused by actions and to generate contextually relevant actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning Knowledge Graph-based World Models of Textual Environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Worldformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>As described in the paper's related work, Worldformer represents environment status as a knowledge graph and uses a learned world model to predict graph changes due to agent actions, then uses those predictions to generate a set of contextually-relevant actions (world-model-driven action generation). The paper only mentions Worldformer; details (exact architecture, embedding/graph encoder) are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld / textual environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based, partially observable environments where representing state as a knowledge graph can aid prediction of action consequences; mentioned as prior art for using graph-based world models for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Knowledge-graph-based world model (learned) used as an internal tool to predict future states; not an external KB per se in the mention.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Predicted graph updates / predicted state graph representations and candidate action sets (structured graph outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Environment belief is represented as a knowledge graph (world model); predictions from the world model are used to update/augment belief state (paper only mentions this at a high level).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-based/world-model driven action generation (knowledge-graph-based predictions used to propose actions) as described in the related work mention.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a prior approach that constructs and uses a knowledge-graph world model to predict state transitions and to generate contextually relevant actions, illustrating an approach that explicitly models state-change predictions in partially observable textual environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e803.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e803.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALFWorld (Aligning Text and Embodied Environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system (mentioned) that aligns TextWorld high-level instruction learning with low-level embodied simulators (AL-FRED), transferring high-level policies learned in text to grounded embodied navigation and manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ALFWorld agent(s)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>As described in related work, ALFWorld is a framework where agents first learn abstract policies in TextWorld and then transfer these policies to embodied, low-level simulators (AL-FRED) to perform grounded navigation and manipulation; the paper only references ALFWorld conceptually rather than using it.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AL-FRED (embodied simulator) and TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>AL-FRED is an embodied simulator with rich visual and spatial state; TextWorld is text-only and partially observable; ALFWorld couples the two for transfer learning between abstract and embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Embodied simulator (AL-FRED) used as a grounded environment for low-level control; transfer uses multimodal grounding as a 'tool'.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Visual and spatial environment observations (images, simulator state), low-level action affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Hierarchical/transfer approach: learn high-level policies in text then apply to low-level embodied policies; no detailed planning mechanism described in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of aligning text-based policy learning with grounded embodied navigation/manipulation, illustrating multi-modal transfer rather than explicit external-tool-driven planning in text-only agents.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e803.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e803.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VisualHints</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VisualHints (Visual-Lingual Hints environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An environment/method (mentioned) that automatically generates visual hints (images) from textual observations to provide multimodal hints to agents in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VisualHints-enabled agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work: VisualHints generates visual hints from text observations (or retrieves images) to augment the agent's information; the paper cites it as an example of supplying extra modality information to improve decision-making in text-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>VisualHints environment (extension of text-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based games augmented with automatically generated visual hints; still partially observable in text but augmented with modal hints to reduce uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Image retrieval or text-to-image generation modules (e.g., AttnGAN used in some prior work) to produce visual hints.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Images or image-derived features / visual hints (visual modality), which can be converted to structured or vector features for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned to illustrate another way to supply extra information (visual modality) to reduce partial observability; not detailed in this paper beyond citation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. <em>(Rating: 2)</em></li>
                <li>Learning Knowledge Graph-based World Models of Textual Environments. <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. <em>(Rating: 2)</em></li>
                <li>VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning. <em>(Rating: 1)</em></li>
                <li>Reinforcement Learning with External Knowledge by using Logical Neural Networks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-803",
    "paper_id": "paper-253107789",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "TWC agent",
            "name_full": "TextWorld Commonsense (TWC) baseline agent",
            "brief_description": "A text-based RL agent that augments textual observations with an external commonsense subgraph (originally extracted from ConceptNet) and selects actions via learned policy; its belief is represented as a dynamic commonsense graph combined with observational context.",
            "citation_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines.",
            "mention_or_use": "use",
            "agent_name": "TWC agent",
            "agent_description": "Modular architecture with six components: (a) action encoder (encodes admissible actions), (b) observation encoder (encodes current textual observation o_t), (c) context encoder (encodes dynamic context C_t), (d) dynamic commonsense subgraph G_t^C (retrieved from external knowledge like ConceptNet and updated over time), (e) knowledge integration (encodes the commonsense graph via pre-trained embeddings + sentinel, passes messages with Graph Attention Networks (GAT), and integrates with observation via Co-Attention), and (f) action selection (policy learned with Advantage Actor-Critic). The dynamic commonsense subgraph is constructed by extracting entities from observations, merging with previous graph G_{t-1}^C to form cumulative entity set E_t, and adding links via the Context Direct Connections (CDC) algorithm.",
            "environment_name": "TextWorld Commonsense (TWC) / TextWorld",
            "environment_description": "A text-based game environment where the agent must tidy up rooms by putting objects in appropriate locations; observations and actions are purely textual and the environment is partially observable (agent cannot directly observe global world state), requiring commonsense to infer object-place relationships and to plan multi-step actions across rooms.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet (textual commonsense knowledge graph) as an external knowledge base; occasionally manually-prepared subgraphs (ConceptNet + Manual) used in baselines.",
            "tool_output_types": "Structured graph triplets (entity, relation, entity), lists of entities and relations (including 'at location'), and graph connectivity information; embeddings derived from these triplets (Numberbatch or GloVe) when encoded.",
            "belief_state_mechanism": "Dynamic commonsense subgraph G_t^C combined with dynamic context C_t and cumulative entity set E_t; node features are embedded (Numberbatch or GloVe) with a sentinel vector, and node representations are updated via Graph Attention Network (GAT) message passing; observational context and graph embeddings are combined via Co-Attention to form the agent's integrated belief/context for action selection.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each time step the agent extracts entities from the new textual observation, merges them with previous graph entities to form E_t, constructs/updates the commonsense subgraph G_t^C by retrieving matching triplets from the external KB and adding links via the Context Direct Connections (CDC) algorithm, then recomputes node embeddings (pretrained embeddings + sentinel) and runs GAT message passing; finally integrates updated graph embeddings with the current observational context via Co-Attention.",
            "planning_approach": "Learned policy (Advantage Actor-Critic) using integrated observation + commonsense graph features; not a search-based or explicit model-based planner in this implementation.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Using an external commonsense graph improves action selection in a partially observable text environment; representing belief as a dynamic commonsense subgraph that is updated from observations and external KB lookups and integrated via GAT + Co-Attention yields better performance than no external commonsense, and textual ConceptNet knowledge can be complemented by more spatially-detailed visual scene graphs.",
            "uuid": "e803.0"
        },
        {
            "name_short": "SceneGraph agent",
            "name_full": "Scene Graph external-knowledge agent (proposed)",
            "brief_description": "A variant of the TWC agent that replaces ConceptNet with Visual Genome (a scene graph dataset) as the external knowledge source; it constructs dynamic commonsense subgraphs from VG triplets and integrates them into the agent belief the same way as the TWC agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Scene Graph agent",
            "agent_description": "Same six-component architecture as the TWC agent (action encoder, observation encoder, context encoder, dynamic commonsense subgraph, knowledge integration, action selection). The external knowledge source is Visual Genome (VG) scene graph triplets; node embeddings use GloVe in experiments, followed by graph encoding with GAT and integration via Co-Attention with observation features. The dynamic commonsense subgraph is built by matching entities extracted from observations to VG triplets and using CDC-style linking (cumulative merging with previous G_{t-1}^C). Policy learned with Advantage Actor-Critic.",
            "environment_name": "TextWorld Commonsense (TWC) / TextWorld",
            "environment_description": "Same TWC tidy-up games as above; partially observable text-only environment where spatial place relations between objects are the key external commonsense needed.",
            "is_partially_observable": true,
            "external_tools_used": "Visual Genome (VG) scene graph dataset used as an external commonsense knowledge source.",
            "tool_output_types": "Structured scene-graph triplets (object, relationship, object), with many explicit spatial relations ('on', 'in', 'under', 'with', 'has'), i.e., structured relational data.",
            "belief_state_mechanism": "Dynamic commonsense subgraph G_t^C derived from scene-graph triplets matched to entities from textual observations, maintained cumulatively (E_t) and encoded via word embeddings (GloVe) + sentinel and message passing with GAT; integrated with observation context via Co-Attention to form the agent belief/context.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Entities extracted from each textual observation are combined with previously stored entities to produce E_t; matching triplets are retrieved from Visual Genome to populate/update G_t^C; graph node embeddings are updated and propagated with GAT; integrated with the observational context via Co-Attention for downstream policy/action selection.",
            "planning_approach": "Learned policy (Advantage Actor-Critic) that conditions on integrated observation + scene-graph-derived commonsense features; no explicit search-based or shortest-path planner is described.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Replacing textual ConceptNet with scene graphs (VG) yields more spatially-detailed commonsense and improves agent efficiency and generalization (especially on unseen entities), because VG contains many direct spatial relations useful for place inference; scene-graph-based agents converge faster in many settings.",
            "uuid": "e803.1"
        },
        {
            "name_short": "ConceptNet+SG",
            "name_full": "Sequential ConceptNet then Scene Graph training agent",
            "brief_description": "A curriculum-style variant: agent is trained first with ConceptNet external knowledge and then continued training with Visual Genome scene graph knowledge, aiming to combine abstract textual commonsense and concrete spatial knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ConceptNet+SG agent",
            "agent_description": "Same base architecture as the TWC agent; training proceeds in two phases (curriculum learning): (1) train agent for N episodes using ConceptNet as external KB, then (2) continue training for additional episodes using Visual Genome (VG) as the external KB. Graph encoding (GloVe embeddings + GAT) and integration (Co-Attention) remain the same. Policy training uses Advantage Actor-Critic.",
            "environment_name": "TextWorld Commonsense (TWC) / TextWorld",
            "environment_description": "Same partially observable TWC tidy-up text games; tasks require inference of object-goal location relations from commonsense.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet (textual commonsense graph) and Visual Genome (scene graph dataset) used sequentially as external KBs.",
            "tool_output_types": "Structured graph triplets from ConceptNet and VG; ConceptNet provides coarser textual relations (e.g., 'at location'), VG provides many explicit spatial relations ('in','on','under','with','has'), and the agent uses embeddings derived from these triplets.",
            "belief_state_mechanism": "Dynamic commonsense subgraph G_t^C updated from the currently-active external KB (ConceptNet in phase 1, VG in phase 2), merged cumulatively across timesteps into E_t and encoded via embeddings + GAT; integrated with observation via Co-Attention.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Same subgraph update procedure as TWC agent: extract entities from observation, merge with previous entities to form E_t, retrieve matching triplets from current external KB and add CDC links to form/update G_t^C, encode nodes and run GAT message passing, then integrate with observational context via Co-Attention.",
            "planning_approach": "Learned policy (Advantage Actor-Critic) leveraging a richer commonsense belief formed by sequential exposure to two KBs; not an explicit search planner.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Sequentially training on ConceptNet then Visual Genome yields robust performance and better generalization to unseen objects (OUT sets) compared to single-KB training: ConceptNet gives broad abstract commonsense while VG adds concrete spatial relations, improving overall competence on TWC tasks.",
            "uuid": "e803.2"
        },
        {
            "name_short": "Worldformer",
            "name_full": "Worldformer (Knowledge Graph-based World Model)",
            "brief_description": "A model (mentioned in related work) that represents environment state as a knowledge graph and learns a world model to predict state changes caused by actions and to generate contextually relevant actions.",
            "citation_title": "Learning Knowledge Graph-based World Models of Textual Environments.",
            "mention_or_use": "mention",
            "agent_name": "Worldformer",
            "agent_description": "As described in the paper's related work, Worldformer represents environment status as a knowledge graph and uses a learned world model to predict graph changes due to agent actions, then uses those predictions to generate a set of contextually-relevant actions (world-model-driven action generation). The paper only mentions Worldformer; details (exact architecture, embedding/graph encoder) are in the cited work.",
            "environment_name": "TextWorld / textual environments",
            "environment_description": "Text-based, partially observable environments where representing state as a knowledge graph can aid prediction of action consequences; mentioned as prior art for using graph-based world models for planning.",
            "is_partially_observable": true,
            "external_tools_used": "Knowledge-graph-based world model (learned) used as an internal tool to predict future states; not an external KB per se in the mention.",
            "tool_output_types": "Predicted graph updates / predicted state graph representations and candidate action sets (structured graph outputs).",
            "belief_state_mechanism": "Environment belief is represented as a knowledge graph (world model); predictions from the world model are used to update/augment belief state (paper only mentions this at a high level).",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Model-based/world-model driven action generation (knowledge-graph-based predictions used to propose actions) as described in the related work mention.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Mentioned as a prior approach that constructs and uses a knowledge-graph world model to predict state transitions and to generate contextually relevant actions, illustrating an approach that explicitly models state-change predictions in partially observable textual environments.",
            "uuid": "e803.3"
        },
        {
            "name_short": "ALFWorld",
            "name_full": "ALFWorld (Aligning Text and Embodied Environments)",
            "brief_description": "A prior system (mentioned) that aligns TextWorld high-level instruction learning with low-level embodied simulators (AL-FRED), transferring high-level policies learned in text to grounded embodied navigation and manipulation tasks.",
            "citation_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.",
            "mention_or_use": "mention",
            "agent_name": "ALFWorld agent(s)",
            "agent_description": "As described in related work, ALFWorld is a framework where agents first learn abstract policies in TextWorld and then transfer these policies to embodied, low-level simulators (AL-FRED) to perform grounded navigation and manipulation; the paper only references ALFWorld conceptually rather than using it.",
            "environment_name": "AL-FRED (embodied simulator) and TextWorld",
            "environment_description": "AL-FRED is an embodied simulator with rich visual and spatial state; TextWorld is text-only and partially observable; ALFWorld couples the two for transfer learning between abstract and embodied tasks.",
            "is_partially_observable": true,
            "external_tools_used": "Embodied simulator (AL-FRED) used as a grounded environment for low-level control; transfer uses multimodal grounding as a 'tool'.",
            "tool_output_types": "Visual and spatial environment observations (images, simulator state), low-level action affordances.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Hierarchical/transfer approach: learn high-level policies in text then apply to low-level embodied policies; no detailed planning mechanism described in this paper's mention.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as an example of aligning text-based policy learning with grounded embodied navigation/manipulation, illustrating multi-modal transfer rather than explicit external-tool-driven planning in text-only agents.",
            "uuid": "e803.4"
        },
        {
            "name_short": "VisualHints",
            "name_full": "VisualHints (Visual-Lingual Hints environment)",
            "brief_description": "An environment/method (mentioned) that automatically generates visual hints (images) from textual observations to provide multimodal hints to agents in text-based games.",
            "citation_title": "VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning.",
            "mention_or_use": "mention",
            "agent_name": "VisualHints-enabled agent",
            "agent_description": "Mentioned in related work: VisualHints generates visual hints from text observations (or retrieves images) to augment the agent's information; the paper cites it as an example of supplying extra modality information to improve decision-making in text-based RL.",
            "environment_name": "VisualHints environment (extension of text-based games)",
            "environment_description": "Text-based games augmented with automatically generated visual hints; still partially observable in text but augmented with modal hints to reduce uncertainty.",
            "is_partially_observable": true,
            "external_tools_used": "Image retrieval or text-to-image generation modules (e.g., AttnGAN used in some prior work) to produce visual hints.",
            "tool_output_types": "Images or image-derived features / visual hints (visual modality), which can be converted to structured or vector features for agents.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Mentioned to illustrate another way to supply extra information (visual modality) to reduce partial observability; not detailed in this paper beyond citation.",
            "uuid": "e803.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines.",
            "rating": 2,
            "sanitized_title": "textbased_rl_agents_with_commonsense_knowledge_new_challenges_environments_and_baselines"
        },
        {
            "paper_title": "Learning Knowledge Graph-based World Models of Textual Environments.",
            "rating": 2,
            "sanitized_title": "learning_knowledge_graphbased_world_models_of_textual_environments"
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning.",
            "rating": 1,
            "sanitized_title": "visualhints_a_visuallingual_environment_for_multimodal_reinforcement_learning"
        },
        {
            "paper_title": "Reinforcement Learning with External Knowledge by using Logical Neural Networks.",
            "rating": 1,
            "sanitized_title": "reinforcement_learning_with_external_knowledge_by_using_logical_neural_networks"
        }
    ],
    "cost": 0.015503,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Commonsense Knowledge from Scene Graphs for Textual Environments</p>
<p>Tsunehiko Tanaka tsunehiko@fuji.waseda.jp 
IBM Research</p>
<p>Waseda University</p>
<p>Daiki Kimura 
IBM Research</p>
<p>Michiaki Tatsubori 
IBM Research</p>
<p>Commonsense Knowledge from Scene Graphs for Textual Environments</p>
<p>Text-based games are becoming commonly used in reinforcement learning as real-world simulation environments. They are usually imperfect information games, and their interactions are only in the textual modality. To challenge these games, it is effective to complement the missing information by providing knowledge outside the game, such as human common sense. However, such knowledge has only been available from textual information in previous works. In this paper, we investigate the advantage of employing commonsense reasoning obtained from visual datasets such as scene graph datasets. In general, images convey more comprehensive information compared with text for humans. This property enables to extract commonsense relationship knowledge more useful for acting effectively in a game. We compare the statistics of spatial relationships available in Visual Genome (a scene graph dataset) and ConceptNet (a text-based knowledge) to analyze the effectiveness of introducing scene graph datasets. We also conducted experiments on a text-based game task that requires commonsense reasoning. Our experimental results demonstrated that our proposed methods have higher and competitive performance than existing state-ofthe-art methods.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) is a type of machine learning method that has a great advantage of not requiring labeled data and has been used in various simulation environments (Mnih et al. 2015;Silver, Huang, and et al. 2016;Kimura 2018;Kimura et al. 2018). Since textual conversation agents are commonly used in our daily lives in the real world, text-based environments, where both observation and action spaces are restricted to the modality of text, have been attracting attention. RL in such environments requires developing an agent to have language comprehension skills by natural language process and sequential decision-making in the complex environment. This means the textual observation contains a lot of noisy information and the problem of partial observability.</p>
<p>Text-based games are a partially observable Markov decision process (POMDP) (Kaelbling, Littman, and Cassandra Figure 1: Illustration of our commonsense acquisition from scene graphs. To provide commonsense: dirty fork  IN  dishwasher to an agent, a single image is sufficient for scene graphs (top left), but ConceptNet requires several graphs to be combined, which is redundant. 1998) where the agent cannot observe the entire information from the text given by the environment. TextWorld (Ct et al. 2018) is a textual game generator and extensible sandbox learning environment for RL agents, and various methods have been proposed for this game to compensate for the missing information (Kimura et al. 2021b;Carta et al. 2020;Murugesan, Chaudhury, and Talamadupula 2021;Shridhar et al. 2020;Kimura et al. 2021a,c;Chaudhury et al. 2021). There are three types of extensions: external knowledge, new modality, and logical rule extraction. External knowledge that is useful for training agents from humans or other domain sources. A study reports commonsense knowledge is an important aspect of human intelligence . In this study, TextWorld Commonsense (TWC), which requires commonsense as external knowledge, is proposed as an extension of TextWorld. The task of the TWC game is cleaning up a room, and the commonsense in this game is mainly place information for each object. The same study also includes a baseline agent for TWC games that uses a commonsense subgraph extracted from external knowledge (we call this model TWC agent and the environment TWC games to distinguish them). Another study reported that introducing ex-ternal knowledge from humans as logical functions helps the training of the agent (Kimura et al. 2021b). New modality information extracted from observations or action text can be introduced to make decisions (Carta et al. 2020;Murugesan, Chaudhury, and Talamadupula 2021;Shridhar et al. 2020). In these methods, visual information from images or videos is commonly used since it has been used in many other studies (Tanaka and Simo-Serra 2021;Kimura et al. 2020) to understand attention and sequential information in decision making. Logical rule extraction can be exploited to improve the speed of training and interpretability of the agent (Kimura et al. 2021c;Chaudhury et al. 2021). Since commonsense knowledge is normally represented by a graph structure, the logical rule representation is compatible with commonsense knowledge.</p>
<p>However, at the time of writing, there has been no research that utilizes the benefits of these multiple extensions to compensate for missing information. In particular, we hypothesize that the commonsense knowledge of object place relationships that are used in TWC games can be easily obtained from visual information. For example, instead of stating the place name of each object, operators can display a picture of a tidy room, which is a quicker explanation for humans.</p>
<p>In this paper, we propose a novel agent that challenges a TWC game by leveraging visual scene graph datasets to obtain commonsense. The original TWC agent ) constructs a commonsense subgraph from Con-ceptNet (Speer, Chin, and Havasi 2017a), which is textual knowledge, but it is necessary to combine many graphs to obtain one commonsense and to create a complicated subgraph. In fact, Murugesan et al. prepared a 'manual' commonsense subgraph from ConceptNet to tackle this complexity of graphs in their study. However, since scene graph recognition achieves high accuracy from complex images, visual information can deliver various detailed and organized graph information all at once. Figure 1 shows an example for the acquisition of commonsense knowledge from scene graphs in an image. In this example, despite Concept-Net having redundant information for extracting a commonsense subgraph, the proposed extraction from scene graphs has necessary and sufficient information for the cleaningup task. Furthermore, relationships from scene graphs also contain direct spatial relationships such as "on" or "in" (Figure 2) between objects because agents need to determine an object's place in the TWC game. Therefore, we use scene graph datasets as visual external knowledge. A scene graph dataset contains a large number of graphs that represent the relationships between entities in images. We use Visual Genome (VG) (Krishna et al. 2017) as a scene graph dataset, which is the most commonly used, and compare its statistics with ConceptNet. We also conduct experiments to evaluate the performance of agents with commonsense knowledge from a scene graph dataset in RL on text-based games.</p>
<p>Related Work</p>
<p>Text-based RL Games</p>
<p>Text-based interactive RL games has been gaining the focus of many researchers due to the development of environments such as TextWorld (Ct et al. 2018) and Jericho (Hausknecht et al. 2019). In these games, RL agents are required to understand the high-level context information from only textual observation. To overcome this difficulty, a number of prior works on these environments have extracted new information from textual observations: knowledge graphs, visual information, and logical rule.</p>
<p>Knowledge graphs represent relationships between entities like real-world objects and events, or abstract concepts.</p>
<p>A new text-based environment, called "TextWorld Commonsense", was proposed in  to infuse RL agents with commonsense knowledge and developed baseline agents using a commonsense subgraph constructed from ConceptNet (Liu and Singh 2004;Speer, Chin, and Havasi 2017a) as an external knowledge. We use this work as a baseline method, and introduce a new type of commonsense from visual datasets. Worldformer (Ammanabrolu and Riedl 2021) represents environment status as a knowledge graph and uses a world model to predict changes caused by an agent's actions and generates a set of contextually relevant actions.</p>
<p>While knowledge graphs are useful for organizing abstract information from only text descriptions, visual information enables the agent to obtain a detailed locational situation like human imagination and visualization. The most important issue in using visual information is how to obtain it from only textual observation in text-based games. VisualHints (Carta et al. 2020) proposed an environment that can automatically generate various hints about game states from textual observation and changes the difficulty level depending on their type. The main sources of images in (Murugesan, Chaudhury, and Talamadupula 2021) are retrieved from the Internet and generated from a text-toimage pre-trained model, AttnGAN (Xu et al. 2018) with given text descriptions. ALFWorld (Shridhar et al. 2021) combines TextWorld and an embodied simulator called AL-FRED (Shridhar et al. 2020) to obtain information on two modalities. Shridhar et al. proposed an agent that first learns to solve abstract tasks in TextWorld, then transfers the learned high-level policies to low-level embodied tasks in ALFRED.</p>
<p>In addition, even if we use the aforementioned methods, improvements in the speed of training are few and the interpretability of the trained network is still missing. A number of studies (Kimura et al. 2021c;Chaudhury et al. 2021) proposed novel approaches to extract symbolic first-order logics from text observations, and select actions by using neurosymbolic Logical Neural Networks (Riegel et al. 2020). These logical representations are compatible with commonsense graph structures.</p>
<p>As previously described, there have been various approaches using knowledge graphs, visual information, and logical rules. However, at the time of writing, there has been no method that combines any of them. Therefore, we pro-  .</p>
<p>pose an approach to extract and utilize knowledge graphs from visual information.</p>
<p>Scene Graph Dataset</p>
<p>A scene graph is a structured representation of the relationships between objects in a scene. To train a scene graph generation model, a number of datasets have been created. VG (Krishna et al. 2017) is a large-scale scene graph dataset that is most commonly used these days because it contains various elements such as objects, attributes, relationships, QA descriptions, and so on. Since scene graphs can provide a large number of visual relationships in a single image, we use VG datasets as external knowledge for training agents.</p>
<p>ConceptNet vs Scene Graph Datasets</p>
<p>In this section, to show scene graph datasets are effective as external knowledge for solving TWC games, we compare ConceptNet and scene graph datasets. We first show the statistics of ConceptNet, VG (Krishna et al. 2017), and manual commonsense knowledge designed in ). Next, we compare ConceptNet and VG in terms of similarity to the manual commonsense knowledge. VG is the most commonly used scene graph dataset. The manual commonsense knowledge is manually extracted from Con-ceptNet to include only the pairs of an object in TWC games and goal location for each object. Since the entities are directly related to actions in the games, the agent with this manually-crafted information is more effective for solving the games. Therefore, external knowledge that is similar to the manual commonsense knowledge are comfortable with this task.</p>
<p>Knowledge Statistics</p>
<p>We summarize the statistics of the three types of external knowledge in Table 1. In general for all external knowledge, each graph is represented as a triplet e 1 , r 12 , e 2 : e 1 , e 2 denote entities in an image, and r 12 denotes a relationship between e 1 and e 2 . In Table 1, 'entity', 'relationship', and 'triplet' indicate the number of species of e, r, e 1 , r 12 , e 2 , respectively. The huge difference between ConceptNet and VG is the number of species of relationships, and this indicates that VG has more detailed information in relationships than ConceptNet. Figure 2 shows an example of r in these datasets. In ConceptNet, the spatial relationship is only 'at location', which is the second most common, but its ratio to the total is low. In contrast, spatial relationships such as 'on', 'in', and 'under' dominate VG. In addition, 'has' and 'with' can also express spatial relationships, such as building, has, window and window, with, building . Thus, we can see that VG has a larger number of spatial relationships than ConceptNet. However, the manual commonsense knowledge has two species of relationships: 186 'at location' and 6 'related to'. This means that spatial relationships are important for solving TWC games, and VG has an advantage in this respect.</p>
<p>Similarity to Manual Commonsense Knowledge</p>
<p>To examine whether VG contains knowledge graphs useful for solving TWC games, we compare ConceptNet and VG in terms of similarity to the manual commonsense knowledge. We calculate the similarities for both entity e and entity pairs {e 1 , e 2 } as described in the following.</p>
<p>Entity e Given an entity e V i from VG, we use GloVe (Pennington, Socher, and Manning 2014) embeddings to repre-
sent e V i as a d-dimensional vector z V i , where z V i  R d
is the word embedding of the entity. Similarly, the embedding of each entity in ConceptNet e C j and in manual commonsense knowledge e M k are denoted as z C j , z M k , respectively. We calculate the similarity s eV i k between an entity in VG e V i and in manual commonsense knowledge e M k by Eq. 1.
s eV ik = cos similarity(z V i , z M k )(1)
Similarly, Eq. 1 is executed for entities in ConceptNet. We count the number of entities whose similarity is above the threshold 0.7.</p>
<p>Pair of entities {e 1 , e 2 } We also compare sets of pairs of e 1 and e 2 from these datasets in terms of similarity to the 132 pairs in the manual commonsense knowledge. In the same way as the entities previously described, we use GloVe to represent e V i1 and e V i2 from triplet t
V i = e V i1 , r V i1i2 , e V i2
in VG as d-dimensional vectors z V i1 , z V i2 , respectively. Similarly, z C j1 , z C j2 and z M k1 , z M k2 are the embeddings of entities from each triplet in ConceptNet t C j = e C j1 , r C j1j2 , e C j2 and manual commonsense knowledge t M k = e M k1 , r M k1k2 , e M k2 , respectively. We calculate the similarity using the sum of both embeddings of entities in a triplet. Thus, the similarity s pV ik is given by Eq. 2.
s pV ik = cos similarity(z V i1 + z V i2 , z M k1 + z M k2 )(2)
For ConceptNet, we use Eq. 2 similarity. We set a threshold to 0.65 and count pairs over the threshold. The results of the entity and pair counts are summarized in Table 2. Although there is no significant difference in the number of types of entities, VG has more pairs associated with the manual commonsense knowledge, which indicates that VG has more game-related relationships than Concept-Net.</p>
<p>The aforementioned comparisons show that TWC games need more spatial relationships in the external knowledge, and VG is more effective for solving TWC games than Con-ceptNet.  Table 2: Comparison of the number of entities and pairs similar to manual commonsense knowledge in external knowledge. A pair is a combination of e 1 and e 2 from a triplet e 1 , r 12 , e 2 . The number of entities is not very different, but the number of pairs is much higher in VG than in Concept-Net.</p>
<p>Proposed Method Previous TWC agent</p>
<p>The proposed methods extend the TWC agent , which is a baseline model for TextWorld Commonsense. We briefly explain the network architecture as follows.</p>
<p>The TWC agent consists of the six components: (a) action encoder, which encodes all admissible actions a, (b) observation encoder, which encodes the observation o t , (c) context encoder, which encodes the dynamic context C t , (d) dynamic commonsense subgraph, which is commonsense information G t C extracted by the agent, (e) knowledge integration, which combines the information from textual observation and the extracted commonsense subgraph, and (f) action selection, which selects an action from given action candidates. We subsequently describe dynamic commonsense subgraph and knowledge integration, which are important for this paper.</p>
<p>For dynamic commonsense subgraph, the TWC agent retrieves commonsense from external knowledge like Con-ceptNet (Speer, Chin, and Havasi 2017a) and updates a subgraph by combining it with the graph at a previous time step. At time t, the agent first extracts entities involving game status from textual observation and then obtains a set of cumulative entities E t by combining it with the entities from the previous graph G t1 C . The commonsense subgraph G t C is constructed automatically from E t and Context Direct Connections (CDC), which is another algorithm of external knowledge. For CDC, the entities are split into two groups in accordance with their attributes, and then links between the groups are added.</p>
<p>For knowledge integration, the TWC agent encodes the commonsense subgraph and integrates the graph embedding vector with the observation context feature. In the encoding phase, the node embedding is first extracted from the commonsense subgraph using a pre-trained knowledge graph embedding called Numberbatch (Speer, Chin, and Havasi 2017b) and a sentinel vector (Lu et al. 2017) is added to enable the attention to not attend to any specific nodes in the commonsense subgraph. These embeddings are updated by messages passing between the nodes of graph attention networks (GAT) (Velikovi et al. 2018). In the integration phase, Co-Attention is used, which is a bidirectional attention flow layer between the observational context and the commonsense subgraph.</p>
<p>TWC agent is an attractive design for RL agents on textbased games, and it accesses a commonsense and uses it while selecting actions. However, the source format of external knowledge is limited to text. Since textual knowledge such as ConceptNet is very useful, it is redundant because multiple concepts need to be concatenated to express more detailed information. Therefore, they prepared manually retrieved graphs in the paper ). The manual graphs contain direct connections for the objects and their goal locations.</p>
<p>Level</p>
<p>Objects</p>
<p>Objects to find Rooms  </p>
<p>Proposed Method</p>
<p>From the comparison in the previous section, it is revealed that scene graph datasets are effective for TWC games because they have more spatial relationships. This suggests that the issue of the TWC agent is that the external knowledge is limited to being text-based. To address this, we propose an approach to use scene graph datasets as external knowledge to build a commonsense subgraph for agents. Our proposal has two types depending on the training method and external knowledge.</p>
<p>Scene Graph The simplest model is to replace the external knowledge of the TWC agent with scene graph datasets from ConceptNet. As shown in Table 2, a scene graph dataset holds many triplets that are effective for solving TWC games, so we expect to obtain a higher score.</p>
<p>ConceptNet + SG We also propose a method to complement the weak point of textual knowledge with scene graph datasets. Inspired by curricular learning (Bengio et al. 2009), we first provide the agent with textual knowledge and train it, then provide the same agent with external knowledge from scene graph datasets and continue training. We hypothesize that this method is effective in training agents that have commonsense knowledge balanced between abstract and concrete knowledge. In the first step, the overall commonsense is given by the textual knowledge. In the second step, the specific commonsense focused on location is given from the scene graph dataset.</p>
<p>Experiments Experimental Setup</p>
<p>We conduct our experiments on TWC games . A TWC game is a text-based game where the goal is to tidy up a house by putting objects where they should be. The connection between objects and the locations where they should be is not given by the game, so the agent needs to depend on commonsense knowledge. This domain has three difficulty levels (easy, medium, and hard) depending on the total number of objects in the game, the number of objects in which the agent needs to find their locations, and the number of rooms to explore. The numbers are randomly sampled from the list in Table 3. In our evaluation, we consider all difficulty levels. We also use two types of test sets: IN and OUT. The games in the IN were built on the same entities as the training set, and the entities in the OUT do not appear in the training set. We can evaluate the ability to generalize unseen entities from these test sets.</p>
<p>Our experimental setup is based on the evaluation system in ; we use the Advantage Actor-Critic algorithm (Mnih et al. 2016). The most significant difference from the previous system is that all agents use GloVe for graph embedding. Numberbatch (Speer, Chin, and Havasi 2017b) used in the previous system is a combination of existing pre-trained embeddings such as word2vec (Mikolov et al. 2013) and GloVe retrofitted with ConceptNet's graph. The evaluation experiments in ) have shown that a TWC agent with Numberbatch achieved a better performance than with GloVe because of a high affinity with ConceptNet. Since we focus on the impact of external knowledge, we use only GloVe for both graph and observation embeddings in all agents.</p>
<p>Metrics</p>
<p>We measure the performance of agents with the various external knowledge on TextWorld using two metrics: the normalized score and the number of steps taken. The normalized score is calculated by dividing the actual score by the maximum possible score. Steps indicate time spent to reach the goal and the lower the value, the higher the performance.</p>
<p>Results</p>
<p>We compare four types of agents: ConceptNet, ConceptNet + Manual, Scene Graph, and ConceptNet + SG. Both Scene Graph and ConceptNet + SG are our proposed methods that use VG. The other agents are baselines proposed in . ConceptNet + Manual uses manuallyprepared knowledge directly related to the game from Con-ceptNet. ConceptNet + Manual should show human-like performance and is regarded as the upper bound of the performance of the proposed method (especially for IN). Con-ceptNet + SG is first trained for 100 episodes using Concept-Net, followed by 100 episodes using VG. All agents except ConceptNet + SG are trained for 100 episodes each and all results are the average of five runs. The results are summarized in Table 4. We also show the training curves in Figure 3.</p>
<p>We can see three overall trends in these results. First, our proposed methods outperform the baselines in both steps and scores in the easy and medium levels. The commonsense knowledge obtained from scene graph datasets is proved to be effective in solving TWC games. Second, all agents struggle with the hard level. It is necessary to have the ability to deal with complicated situations where the location to pick up objects is different from the location to place them. The development of agents with this ability is future work. Finally, ConceptNet + Manual shows high performance for the IN set regardless of difficulty level. ConceptNet + Manual has crucial information for solving TWC games, so both efficiency and scores are higher when the same species of entity is given as the training set. However, its performance in the OUT is lower than that of other agents because it is given only a minimum number of necessary graphs in the training set, so it overfits to those graphs.</p>
<p>We describe the performance of our proposed models in detail. We propose two models, Scene Graph and Concept-   . IN is built using the same entities as the training set, and OUT is built using different entities. #Steps (lower is better) denotes the steps needed to accomplish the goals and Score (higher is better) denotes the normalized score by maximum possible score. Each value is a pair (average)  (standard deviation).</p>
<p>Net + SG, depending on the type of external knowledge and training method. Scene Graph is a simple agent that uses only a scene graph dataset. From Table 4, we found that this agent is very efficient in its graph search. For easy-level games, Scene Graph is superior to even ConceptNet + Manual in the IN set. In medium-level games, Scene Graph has the largest number of steps in the IN set, but it has the smallest number in the OUT set. This indicates that Scene Graph is robust against unseen objects. The efficiency can be seen from the fastest convergence of the training curves in the medium and hard levels in Figure 3. In terms of performance, although it is sometimes inferior to our other proposed method, it is better than the baselines in OUT. The high efficiency and performance of Scene Graph can be attributed to scene graph datasets having many graphs relevant to the TWC game with spatial relationships. Concept-Net + SG is an agent that is trained with ConceptNet, followed by scene graph datasets. Table 4 shows that the performance of this agent is very high. It has the best performance in the OUT set for both easy and medium levels and the second-best performance in the IN set after Con-ceptNet + Manual. This indicates that it also has robustness against unseen objects. The reason for the performance improvement is considered to be the wide range that can be handled by both commonsense knowledge from Concept-Net and VG. In easy-level games, the score increases by 0.14 from IN to OUT, which is an uncommon improvement. The reason for this could be that VG has a small number of graphs related to the easy-IN games, which decrease the results of ConceptNet + SG for easy-IN games. As you can see in Table 3, easy games has only require one object to be explored, so the score changes dramatically depending on whether or not external knowledge contains graphs related to the object and the goal location. However, since the number of graphs increases, the efficiency of the search decreases, resulting in inferiority with scene graphs in steps.</p>
<p>The training curve in Figure 3 shows that VG enhances the performance of ConceptNet alone from 100 episodes (in the easy level, the training curve converges during the Concept-Net phase, so ConceptNet and ConceptNet + SG overlap). In addition, we use GloVe for graph embedding in these experiments for a fair comparison, but agents using Con-ceptNet can be improved by replacing GloVe with Numberbatch.</p>
<p>In summary, our proposed method achieves both efficiency and performance improvements, and it is robust to unseen objects. However, it is still a challenge to deal with complex situations such as hard-level games. We discuss how to address this issue in the next section.</p>
<p>Conclusion and Future Work</p>
<p>We have presented new approaches to leverage commonsense subgraphs constructed from scene graph datasets for text-based games. We conducted experiments on a TWC game, which is a benchmark to evaluate how well an agent learns with commonsense knowledge. Experimental results showed that our proposed approaches using a VG dataset demonstrate highly competitive performances compared with existing state-of-the-art approaches with textual knowledge. We also illustrated that the performance can be further improved by using ConceptNet and scene graph datasets sequentially.</p>
<p>Although this work is the first step to utilize both visual information and commonsense, we still have a few challenges as future work. One topic is to deal with more complex situations like hard difficulty level games. We expect that this can be improved by exploiting the relationships among information available from external knowledge. The current model adopts GAT (Velikovi et al. 2018) as a graph encoder, but GAT only takes the node features as input and ignores the edge features except for whether they exist or not. In complex tasks, the key to solving this game is more specific information than simply the link between an object and location. In particular, scene graph datasets provide more detailed information on relationships than textual knowledge as shown in Fig. 1. We consider applying networks such as Edge feature enhanced Graph Neural Networks (EGNN) (Gong and Cheng 2019) that can take advan- tage of the edge features of graphs. Another topic is introducing logical rule training into our proposed method. Since graph information can be easily converted to logical rules, we hope the commonsense graph can directly contribute to logical rule training for action policies in RL.</p>
<p>Figure 2 :
2Histogram of relationships (top 15) included in VG and ConceptNet. VG has much more spatial relationships</p>
<p>Figure 3 :
3Performance evaluation for the three levels of the training set games (Smoothing is performed to clarify the difference in the results of a single run).</p>
<p>1 .
1Take the dirty fork from the table 2. Open the fridge 3. Put the dirty fork in the fridge 4. Put the dirty fork in the dishwasherScene Graph 
Scene Graph </p>
<p>dishwasher 
dirty fork </p>
<p>IN </p>
<p>Table 3 :
3Specification of TWC games from.</p>
<p>Table 4 :
4Generalization results for two test sets, IN and OUT, on games with three difficulty levels.Scene Graph and Con-</p>
<p>Learning Knowledge Graph-based World Models of Textual Environments. P Ammanabrolu, M O Riedl, arXiv:2106.09608arXiv preprintAmmanabrolu, P.; and Riedl, M. O. 2021. Learning Knowl- edge Graph-based World Models of Textual Environments. arXiv preprint arXiv:2106.09608.</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningBengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. 2009. Curriculum learning. In Proceedings of the 26th an- nual international conference on machine learning, 41-48.</p>
<p>VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning. T Carta, S Chaudhury, K Talamadupula, M Tatsubori, In arxivCarta, T.; Chaudhury, S.; Talamadupula, K.; and Tatsubori, M. 2020. VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement Learning. In arxiv.</p>
<p>Neuro-Symbolic Approaches for Text-Based Policy Learning. S Chaudhury, P Sen, M Ono, D Kimura, M Tatsubori, A Munawar, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsChaudhury, S.; Sen, P.; Ono, M.; Kimura, D.; Tatsubori, M.; and Munawar, A. 2021. Neuro-Symbolic Approaches for Text-Based Policy Learning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 3073-3078. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.</p>
<p>TextWorld: A Learning Environment for Text. M.-A Ct, A Kdr, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, R Y Tao, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, based Games. CoRR, abs/1806.11532Ct, M.-A.; Kdr, A.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Tao, R. Y.; Hausknecht, M.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. TextWorld: A Learning Environment for Text-based Games. CoRR, abs/1806.11532.</p>
<p>Exploiting edge features for graph neural networks. L Gong, Q Cheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionGong, L.; and Cheng, Q. 2019. Exploiting edge features for graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9211-9219.</p>
<p>Interactive Fiction Games: A Colossal Adventure. M Hausknecht, P Ammanabrolu, M.-A Ct, X E Yuan, 2020Hausknecht, M.; Ammanabrolu, P.; Ct, M.-A.; and Yuan, X. E. 2019. Interactive Fiction Games: A Colossal Adven- ture. In AAAI 2020.</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial Intelligence. 1011Kaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1998. Planning and acting in partially observable stochastic do- mains. Artificial Intelligence, 101(1): 99-134.</p>
<p>D Kimura, arXiv:1806.00630DAQN: Deep Auto-encoder and Q-Network. Kimura, D. 2018. DAQN: Deep Auto-encoder and Q- Network. arXiv:1806.00630.</p>
<p>Adversarial Discriminative Attention for Robust Anomaly Detection. D Kimura, S Chaudhury, M Narita, A Munawar, R Tachibana, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Kimura, D.; Chaudhury, S.; Narita, M.; Munawar, A.; and Tachibana, R. 2020. Adversarial Discriminative Attention for Robust Anomaly Detection. In 2020 IEEE Winter Con- ference on Applications of Computer Vision (WACV), 2161- 2170.</p>
<p>LOA: Logical Optimal Actions for Text-based Interaction Games. D Kimura, S Chaudhury, M Ono, M Tatsubori, D J Agravante, A Munawar, A Wachi, R Kohita, A Gray, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System DemonstrationsOnline: Association for Computational LinguisticsKimura, D.; Chaudhury, S.; Ono, M.; Tatsubori, M.; Agra- vante, D. J.; Munawar, A.; Wachi, A.; Kohita, R.; and Gray, A. 2021a. LOA: Logical Optimal Actions for Text-based In- teraction Games. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, 227-231. Online: As- sociation for Computational Linguistics.</p>
<p>Internal Model from Observations for Reward Shaping. D Kimura, S Chaudhury, R Tachibana, S Dasgupta, ICML workshop. Kimura, D.; Chaudhury, S.; Tachibana, R.; and Dasgupta, S. 2018. Internal Model from Observations for Reward Shap- ing. In ICML workshop.</p>
<p>Reinforcement Learning with External Knowledge by using Logical Neural Networks. D Kimura, S Chaudhury, A Wachi, R Kohita, A Munawar, M Tatsubori, A Gray, KBRL Workshop at IJCAI-PRICAI 2020. Kimura, D.; Chaudhury, S.; Wachi, A.; Kohita, R.; Mu- nawar, A.; Tatsubori, M.; and Gray, A. 2021b. Reinforce- ment Learning with External Knowledge by using Logical Neural Networks. KBRL Workshop at IJCAI-PRICAI 2020.</p>
<p>Neuro-Symbolic Reinforcement Learning with First-Order Logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsOnline and Punta CanaKimura, D.; Ono, M.; Chaudhury, S.; Kohita, R.; Wachi, A.; Agravante, D. J.; Tatsubori, M.; Munawar, A.; and Gray, A. 2021c. Neuro-Symbolic Reinforcement Learning with First- Order Logic. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 3505- 3511. Online and Punta Cana, Dominican Republic: Associ- ation for Computational Linguistics.</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, 123Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, D. A.; et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Inter- national journal of computer vision, 123(1): 32-73.</p>
<p>. H Liu, P Singh, ConceptNet -A Practical Commonsense Reasoning Tool-Kit. BT Technology Journal. 22Liu, H.; and Singh, P. 2004. ConceptNet -A Practical Commonsense Reasoning Tool-Kit. BT Technology Journal, 22: 211-226.</p>
<p>Knowing when to look: Adaptive attention via a visual sentinel for image captioning. J Lu, C Xiong, D Parikh, R Socher, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionLu, J.; Xiong, C.; Parikh, D.; and Socher, R. 2017. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, 375-383.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef- ficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, PMLRIn International conference on machine learning. Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asyn- chronous methods for deep reinforcement learning. In In- ternational conference on machine learning, 1928-1937. PMLR.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, Nature. Mnih, V.; Kavukcuoglu, K.; Silver, D.; and et al. 2015. Human-level control through deep reinforcement learning. Nature.</p>
<p>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. K Murugesan, M Atzeni, P Kapanipathi, P Shukla, S Kumaravel, G Tesauro, K Talamadupula, M Sachan, M Campbell, Thirty Fifth AAAI Conference on Artificial Intelligence. Murugesan, K.; Atzeni, M.; Kapanipathi, P.; Shukla, P.; Ku- maravel, S.; Tesauro, G.; Talamadupula, K.; Sachan, M.; and Campbell, M. 2021. Text-based RL Agents with Com- monsense Knowledge: New Challenges, Environments and Baselines. In Thirty Fifth AAAI Conference on Artificial In- telligence.</p>
<p>Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents. K Murugesan, S Chaudhury, K Talamadupula, arXiv:2106.05387arXiv preprintMurugesan, K.; Chaudhury, S.; and Talamadupula, K. 2021. Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents. arXiv preprint arXiv:2106.05387.</p>
<p>GloVe: Global Vectors for Word Representation. J Pennington, R Socher, C Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsPennington, J.; Socher, R.; and Manning, C. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532-1543. Doha, Qatar: Association for Computational Linguistics.</p>
<p>R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, arXiv:2006.13155Logical neural networks. arXiv preprintRiegel, R.; Gray, A.; Luus, F.; Khan, N.; Makondo, N.; Akhalwaya, I. Y.; Qian, H.; Fagin, R.; Barahona, F.; Sharma, U.; et al. 2020. Logical neural networks. arXiv preprint arXiv:2006.13155.</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Shridhar, M.; Thomason, J.; Gordon, D.; Bisk, Y.; Han, W.; Mottaghi, R.; Zettlemoyer, L.; and Fox, D. 2020. ALFRED: A Benchmark for Interpreting Grounded Instructions for Ev- eryday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. M Shridhar, X Yuan, M.-A Ct, Y Bisk, A Trischler, M Hausknecht, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRShridhar, M.; Yuan, X.; Ct, M.-A.; Bisk, Y.; Trischler, A.; and Hausknecht, M. 2021. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Pro- ceedings of the International Conference on Learning Rep- resentations (ICLR).</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, Nature. 529Silver, D.; Huang, A.; and et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature, 529: 484-503.</p>
<p>ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. R Speer, J Chin, C Havasi, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17. the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17AAAI PressSpeer, R.; Chin, J.; and Havasi, C. 2017a. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. In Pro- ceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, 4444-4451. AAAI Press.</p>
<p>ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. R Speer, J Chin, C Havasi, AAAI Press17Speer, R.; Chin, J.; and Havasi, C. 2017b. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. AAAI'17, 4444-4451. AAAI Press.</p>
<p>LoL-V2T: Large-Scale Esports Video Description Dataset. T Tanaka, E Simo-Serra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) WorkshopsTanaka, T.; and Simo-Serra, E. 2021. LoL-V2T: Large- Scale Esports Video Description Dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 4557-4566.</p>
<p>P Velikovi, G Cucurull, A Casanova, A Romero, P Li, Y Bengio, Graph Attention Networks. International Conference on Learning Representations. Velikovi, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li, P.; and Bengio, Y. 2018. Graph Attention Networks. Inter- national Conference on Learning Representations.</p>
<p>AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. T Xu, P Zhang, Q Huang, H Zhang, Z Gan, X Huang, X He, Xu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang, X.; and He, X. 2018. AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.</p>            </div>
        </div>

    </div>
</body>
</html>
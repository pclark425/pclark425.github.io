<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4793 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4793</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4793</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-277dd00ab02f122133bf56b485dfb7c730acdcde</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/277dd00ab02f122133bf56b485dfb7c730acdcde" target="_blank">Retrieval-based Language Models and Applications</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This tutorial will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs, focusing on their model architectures and learning approaches, and use an exercise to showcase the effectiveness of retrieval- based LMs.</p>
                <p><strong>Paper Abstract:</strong> Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4793.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4793.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest Neighbor Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language modeling approach that augments a parametric LM with a non-parametric nearest-neighbor datastore over tokens or contexts to retrieve likely continuations at generation time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalization through Memorization: Nearest Neighbor Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer language model augmented at decoding time with a nearest-neighbor module that retrieves similar token-level contexts from a datastore and interpolates retrieved token distributions with the model's output.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric nearest-neighbor datastore (token-level)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores token-level (or short context) representations in an external datastore and at inference retrieves nearest neighbors to adjust next-token probabilities (interpolation between parametric LM and retrieved token distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling / next-token prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict next tokens in continuing text sequences; evaluated by perplexity and next-token accuracy in language modeling settings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Referenced as a prototypical example where non-parametric memory (nearest neighbors) augments a parametric LM to improve generalization via memorization of training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires large external datastore and efficient nearest-neighbor search; scaling and datastore management are challenges (training/inference complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Token-level nearest-neighbor memory can improve LM generalization by memorizing and retrieving rare or long-tail patterns without increasing parametric size.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4793.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4793.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation / Retrieval-based LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of models that integrate an external retrieval datastore of text chunks with language models, retrieving relevant passages and conditioning generation on them to improve knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented LM (RAG-style)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A generative LM that conditions outputs on documents retrieved from an external corpus, typically by concatenating retrieved passages with input or by integrating them into the model's context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external datastore (document/chunk-level retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieves relevant text chunks from a large external corpus (via dense retrieval) and incorporates them into the model input or hidden states to ground generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive NLP tasks (e.g., open-domain question answering, knowledge-grounded generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring up-to-date or factual world knowledge where grounding generation in retrieved documents can reduce hallucination and improve factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Stated to outperform purely parametric LMs on knowledge-intensive tasks and to be more parameter-efficient; retrieval enables updating knowledge by altering the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still susceptible to hallucination despite retrieval; requires careful retrieval/indexing and raises privacy concerns when datastore contains sensitive text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Retrieval of document-level memory provides factual grounding, makes models updatable without retraining, and provides citations for verification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4793.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4793.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atlas: Few-shot Learning with Retrieval Augmented Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LM framework designed for few-shot learning that retrieves relevant examples or documents to improve few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Few-shot learning with retrieval augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model that augments few-shot prompting by retrieving task-relevant exemplars or knowledge from a datastore to improve few-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval datastore (chunk-level / exemplar retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieves examples or passages relevant to the few-shot prompt and conditions the LM on those retrieved items to produce improved few-shot outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot learning / few-shot NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Performing new tasks given only a small number of examples or prompts, leveraging retrieval to provide additional context or exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited as demonstrating that retrieval-augmented models can improve few-shot performance by providing relevant external context; retrieval allows updating and scaling few-shot abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Effectiveness depends on retrieval quality; building and searching large exemplar corpora is resource-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Retrieval can substantially boost few-shot performance by supplying task-specific exemplars without changing model parameters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4793.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4793.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mention Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that incorporates entity-centric textual knowledge into Transformer models via attention over mention-specific memory slots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mention Memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Transformer augmentation that provides entity-aware memory accessed via attention over mention representations to inject textual knowledge into model computations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>entity-mention memory (sparse/entity-indexed memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintains memory keyed by entity mentions; during processing, the model attends to mention-specific textual entries (entity-centric memory) integrated in intermediate layers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Entity-aware language understanding (e.g., tasks requiring entity knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require incorporating entity-specific knowledge into model representations, such as entity-centric question answering or information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Presented as an architecture variant where memory granularity is at entity mentions rather than tokens or document chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires entity supervision or entity linking to index mention memory; complexity in maintaining and attending to many entity slots.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Indexing memory by entities allows targeted retrieval of entity knowledge and can be integrated into intermediate Transformer layers for more precise use of external knowledge.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4793.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4793.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nonparametric MLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonparametric Masked Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A masked language modeling formulation that augments masked LM training with a non-parametric datastore used to supply context or token predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nonparametric masked language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nonparametric Masked LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A masked LM that leverages an external non-parametric memory (datastore) during masked-token prediction to improve masked LM performance and incorporate more factual/contextual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric datastore (token or chunk-level) used in masked LM training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses a datastore of context-target pairs or representations to retrieve candidates for masked tokens and augment model predictions during training/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Masked language modeling / pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict masked tokens in text (MLM objective), with retrieval providing candidate tokens or contexts to inform predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited as an approach that brings non-parametric memory into masked LM objectives, representing another axis of retrieval-augmentation beyond autoregressive LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Integrating retrieval into MLM training requires scalable retrieval during pretraining and careful datastore construction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Nonparametric memory can be incorporated into masked LM pretraining to inject external textual knowledge and potentially improve downstream performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4793.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4793.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemAug Training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training Language Models with Memory Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A line of work that studies methods to train LMs together with an external memory component (datastore) to enable better use of retrieved information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training language models with memory augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-augmented LM (training focused)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Methods that jointly or asynchronously train a parametric LM and its retrieval/memory components, enabling end-to-end learning of retrieval and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external datastore (tokens/chunks) with joint or asynchronous training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Implements memory as a large datastore accessed by the model; training approaches include pipelined pretraining, in-batch approximations for joint training, and asynchronous index updates to keep retrieval aligned with model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General LM pretraining and downstream adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Training paradigms for LMs that use memory across pretraining and finetuning to improve performance on various downstream tasks that rely on external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Discusses trade-offs among pipelined training, in-batch approximations, and asynchronous index updates; emphasizes scalability considerations for large datastores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scalability of joint training with full datastore, maintaining up-to-date indexes, and designing approximations that preserve retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Practical training of memory-augmented LMs requires approximations (in-batch negatives) or asynchronous update schemes to be feasible at large scale; training choices affect retrieval quality and overall performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4793.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4793.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>kNN-Prompt / Nearest Neighbor Zero-Shot Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot inference technique that leverages nearest-neighbor retrieval to improve model predictions without task-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nearest neighbor zero-shot inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-Prompt (nearest-neighbor zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses a nearest-neighbor retrieval mechanism to find examples or contexts relevant to a zero-shot prompt and uses these retrieved items to inform inference without updating model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external nearest-neighbor datastore (exemplar retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>At inference, retrieves nearest examples from a corpus to augment the prompt or influence predictions, enabling improved zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot inference across NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applying a pretrained LM to new tasks without task-specific training, using retrieval of similar examples to guide prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as enabling zero-shot gains via retrieval, showing that exemplar-based retrieval can serve as an alternative to parameter updates for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reliant on retrieval quality and exemplar relevance; may not match finetuned performance on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Nearest-neighbor retrieval can supply task-relevant exemplars at inference time, improving zero-shot capabilities without model finetuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4793.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4793.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parametric vs Non-parametric Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparative study investigating when parametric LMs and non-parametric (retrieval) memories are effective, their limitations, and trade-offs in reliability and resource usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Parametric vs Non-parametric memory comparison</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Comparative analyses between purely parametric LMs and retrieval-augmented (non-parametric) LMs, focusing on effectiveness, parameter-efficiency, and failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory (model parameters) vs non-parametric datastore (retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Compares storing knowledge in model parameters versus retrieving from an external datastore, discussing when retrieval yields gains and where parametric models suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge retention and generalization tasks / robustness evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Investigations into learning and recalling long-tail or updatable knowledge, and evaluating hallucination and reliability across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Reported (in tutorial summary) that retrieval-based LMs can outperform parametric-only models by a large margin while using fewer parameters, and retrieval enables updates without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Non-parametric approaches can still hallucinate, face datastore privacy risks, and require complex retrieval infrastructure; parametric models are hard to update and need large parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Non-parametric memory is parameter-efficient and updatable, but introduces retrieval-specific challenges (privacy, indexing, hallucinations); choice depends on resource constraints and need for updatability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generalization through Memorization: Nearest Neighbor Language Models <em>(Rating: 2)</em></li>
                <li>Retrieval augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Few-shot learning with retrieval augmented language models <em>(Rating: 2)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention <em>(Rating: 2)</em></li>
                <li>Nonparametric masked language modeling <em>(Rating: 2)</em></li>
                <li>Training language models with memory augmentation <em>(Rating: 2)</em></li>
                <li>Nearest neighbor zero-shot inference <em>(Rating: 2)</em></li>
                <li>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 1)</em></li>
                <li>Retrievalaugmented multimodal language modeling <em>(Rating: 1)</em></li>
                <li>Adaptive semiparametric language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4793",
    "paper_id": "paper-277dd00ab02f122133bf56b485dfb7c730acdcde",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "kNN-LM",
            "name_full": "Nearest Neighbor Language Models",
            "brief_description": "A language modeling approach that augments a parametric LM with a non-parametric nearest-neighbor datastore over tokens or contexts to retrieve likely continuations at generation time.",
            "citation_title": "Generalization through Memorization: Nearest Neighbor Language Models",
            "mention_or_use": "mention",
            "agent_name": "kNN-LM",
            "agent_description": "A transformer language model augmented at decoding time with a nearest-neighbor module that retrieves similar token-level contexts from a datastore and interpolates retrieved token distributions with the model's output.",
            "memory_type": "non-parametric nearest-neighbor datastore (token-level)",
            "memory_description": "Stores token-level (or short context) representations in an external datastore and at inference retrieves nearest neighbors to adjust next-token probabilities (interpolation between parametric LM and retrieved token distributions).",
            "task_name": "Language modeling / next-token prediction",
            "task_description": "Predict next tokens in continuing text sequences; evaluated by perplexity and next-token accuracy in language modeling settings.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Referenced as a prototypical example where non-parametric memory (nearest neighbors) augments a parametric LM to improve generalization via memorization of training examples.",
            "limitations_or_challenges": "Requires large external datastore and efficient nearest-neighbor search; scaling and datastore management are challenges (training/inference complexity).",
            "key_insights": "Token-level nearest-neighbor memory can improve LM generalization by memorizing and retrieving rare or long-tail patterns without increasing parametric size.",
            "uuid": "e4793.0"
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation / Retrieval-based LMs",
            "brief_description": "A family of models that integrate an external retrieval datastore of text chunks with language models, retrieving relevant passages and conditioning generation on them to improve knowledge-intensive tasks.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented LM (RAG-style)",
            "agent_description": "A generative LM that conditions outputs on documents retrieved from an external corpus, typically by concatenating retrieved passages with input or by integrating them into the model's context.",
            "memory_type": "external datastore (document/chunk-level retrieval)",
            "memory_description": "Retrieves relevant text chunks from a large external corpus (via dense retrieval) and incorporates them into the model input or hidden states to ground generation.",
            "task_name": "Knowledge-intensive NLP tasks (e.g., open-domain question answering, knowledge-grounded generation)",
            "task_description": "Tasks requiring up-to-date or factual world knowledge where grounding generation in retrieved documents can reduce hallucination and improve factuality.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Stated to outperform purely parametric LMs on knowledge-intensive tasks and to be more parameter-efficient; retrieval enables updating knowledge by altering the corpus.",
            "limitations_or_challenges": "Still susceptible to hallucination despite retrieval; requires careful retrieval/indexing and raises privacy concerns when datastore contains sensitive text.",
            "key_insights": "Retrieval of document-level memory provides factual grounding, makes models updatable without retraining, and provides citations for verification.",
            "uuid": "e4793.1"
        },
        {
            "name_short": "Atlas",
            "name_full": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
            "brief_description": "A retrieval-augmented LM framework designed for few-shot learning that retrieves relevant examples or documents to improve few-shot performance.",
            "citation_title": "Few-shot learning with retrieval augmented language models",
            "mention_or_use": "mention",
            "agent_name": "Atlas",
            "agent_description": "A model that augments few-shot prompting by retrieving task-relevant exemplars or knowledge from a datastore to improve few-shot generalization.",
            "memory_type": "external retrieval datastore (chunk-level / exemplar retrieval)",
            "memory_description": "Retrieves examples or passages relevant to the few-shot prompt and conditions the LM on those retrieved items to produce improved few-shot outputs.",
            "task_name": "Few-shot learning / few-shot NLP tasks",
            "task_description": "Performing new tasks given only a small number of examples or prompts, leveraging retrieval to provide additional context or exemplars.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Cited as demonstrating that retrieval-augmented models can improve few-shot performance by providing relevant external context; retrieval allows updating and scaling few-shot abilities.",
            "limitations_or_challenges": "Effectiveness depends on retrieval quality; building and searching large exemplar corpora is resource-intensive.",
            "key_insights": "Retrieval can substantially boost few-shot performance by supplying task-specific exemplars without changing model parameters.",
            "uuid": "e4793.2"
        },
        {
            "name_short": "Mention Memory",
            "name_full": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
            "brief_description": "An approach that incorporates entity-centric textual knowledge into Transformer models via attention over mention-specific memory slots.",
            "citation_title": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
            "mention_or_use": "mention",
            "agent_name": "Mention Memory",
            "agent_description": "A Transformer augmentation that provides entity-aware memory accessed via attention over mention representations to inject textual knowledge into model computations.",
            "memory_type": "entity-mention memory (sparse/entity-indexed memory)",
            "memory_description": "Maintains memory keyed by entity mentions; during processing, the model attends to mention-specific textual entries (entity-centric memory) integrated in intermediate layers.",
            "task_name": "Entity-aware language understanding (e.g., tasks requiring entity knowledge)",
            "task_description": "Tasks that require incorporating entity-specific knowledge into model representations, such as entity-centric question answering or information extraction.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Presented as an architecture variant where memory granularity is at entity mentions rather than tokens or document chunks.",
            "limitations_or_challenges": "Requires entity supervision or entity linking to index mention memory; complexity in maintaining and attending to many entity slots.",
            "key_insights": "Indexing memory by entities allows targeted retrieval of entity knowledge and can be integrated into intermediate Transformer layers for more precise use of external knowledge.",
            "uuid": "e4793.3"
        },
        {
            "name_short": "Nonparametric MLM",
            "name_full": "Nonparametric Masked Language Model",
            "brief_description": "A masked language modeling formulation that augments masked LM training with a non-parametric datastore used to supply context or token predictions.",
            "citation_title": "Nonparametric masked language modeling",
            "mention_or_use": "mention",
            "agent_name": "Nonparametric Masked LM",
            "agent_description": "A masked LM that leverages an external non-parametric memory (datastore) during masked-token prediction to improve masked LM performance and incorporate more factual/contextual evidence.",
            "memory_type": "non-parametric datastore (token or chunk-level) used in masked LM training",
            "memory_description": "Uses a datastore of context-target pairs or representations to retrieve candidates for masked tokens and augment model predictions during training/inference.",
            "task_name": "Masked language modeling / pretraining",
            "task_description": "Predict masked tokens in text (MLM objective), with retrieval providing candidate tokens or contexts to inform predictions.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Cited as an approach that brings non-parametric memory into masked LM objectives, representing another axis of retrieval-augmentation beyond autoregressive LMs.",
            "limitations_or_challenges": "Integrating retrieval into MLM training requires scalable retrieval during pretraining and careful datastore construction.",
            "key_insights": "Nonparametric memory can be incorporated into masked LM pretraining to inject external textual knowledge and potentially improve downstream performance.",
            "uuid": "e4793.4"
        },
        {
            "name_short": "MemAug Training",
            "name_full": "Training Language Models with Memory Augmentation",
            "brief_description": "A line of work that studies methods to train LMs together with an external memory component (datastore) to enable better use of retrieved information.",
            "citation_title": "Training language models with memory augmentation",
            "mention_or_use": "mention",
            "agent_name": "Memory-augmented LM (training focused)",
            "agent_description": "Methods that jointly or asynchronously train a parametric LM and its retrieval/memory components, enabling end-to-end learning of retrieval and generation.",
            "memory_type": "external datastore (tokens/chunks) with joint or asynchronous training",
            "memory_description": "Implements memory as a large datastore accessed by the model; training approaches include pipelined pretraining, in-batch approximations for joint training, and asynchronous index updates to keep retrieval aligned with model updates.",
            "task_name": "General LM pretraining and downstream adaptation",
            "task_description": "Training paradigms for LMs that use memory across pretraining and finetuning to improve performance on various downstream tasks that rely on external knowledge.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Discusses trade-offs among pipelined training, in-batch approximations, and asynchronous index updates; emphasizes scalability considerations for large datastores.",
            "limitations_or_challenges": "Scalability of joint training with full datastore, maintaining up-to-date indexes, and designing approximations that preserve retrieval quality.",
            "key_insights": "Practical training of memory-augmented LMs requires approximations (in-batch negatives) or asynchronous update schemes to be feasible at large scale; training choices affect retrieval quality and overall performance.",
            "uuid": "e4793.5"
        },
        {
            "name_short": "kNN-Prompt",
            "name_full": "kNN-Prompt / Nearest Neighbor Zero-Shot Inference",
            "brief_description": "A zero-shot inference technique that leverages nearest-neighbor retrieval to improve model predictions without task-specific finetuning.",
            "citation_title": "Nearest neighbor zero-shot inference",
            "mention_or_use": "mention",
            "agent_name": "kNN-Prompt (nearest-neighbor zero-shot)",
            "agent_description": "Uses a nearest-neighbor retrieval mechanism to find examples or contexts relevant to a zero-shot prompt and uses these retrieved items to inform inference without updating model parameters.",
            "memory_type": "external nearest-neighbor datastore (exemplar retrieval)",
            "memory_description": "At inference, retrieves nearest examples from a corpus to augment the prompt or influence predictions, enabling improved zero-shot performance.",
            "task_name": "Zero-shot inference across NLP tasks",
            "task_description": "Applying a pretrained LM to new tasks without task-specific training, using retrieval of similar examples to guide prediction.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Mentioned as enabling zero-shot gains via retrieval, showing that exemplar-based retrieval can serve as an alternative to parameter updates for some tasks.",
            "limitations_or_challenges": "Reliant on retrieval quality and exemplar relevance; may not match finetuned performance on complex tasks.",
            "key_insights": "Nearest-neighbor retrieval can supply task-relevant exemplars at inference time, improving zero-shot capabilities without model finetuning.",
            "uuid": "e4793.6"
        },
        {
            "name_short": "Parametric vs Non-parametric Comparison",
            "name_full": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "brief_description": "A comparative study investigating when parametric LMs and non-parametric (retrieval) memories are effective, their limitations, and trade-offs in reliability and resource usage.",
            "citation_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "mention_or_use": "mention",
            "agent_name": "Parametric vs Non-parametric memory comparison",
            "agent_description": "Comparative analyses between purely parametric LMs and retrieval-augmented (non-parametric) LMs, focusing on effectiveness, parameter-efficiency, and failure modes.",
            "memory_type": "parametric memory (model parameters) vs non-parametric datastore (retrieval)",
            "memory_description": "Compares storing knowledge in model parameters versus retrieving from an external datastore, discussing when retrieval yields gains and where parametric models suffice.",
            "task_name": "Knowledge retention and generalization tasks / robustness evaluation",
            "task_description": "Investigations into learning and recalling long-tail or updatable knowledge, and evaluating hallucination and reliability across tasks.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Reported (in tutorial summary) that retrieval-based LMs can outperform parametric-only models by a large margin while using fewer parameters, and retrieval enables updates without retraining.",
            "limitations_or_challenges": "Non-parametric approaches can still hallucinate, face datastore privacy risks, and require complex retrieval infrastructure; parametric models are hard to update and need large parameter counts.",
            "key_insights": "Non-parametric memory is parameter-efficient and updatable, but introduces retrieval-specific challenges (privacy, indexing, hallucinations); choice depends on resource constraints and need for updatability.",
            "uuid": "e4793.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Few-shot learning with retrieval augmented language models",
            "rating": 2
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
            "rating": 2
        },
        {
            "paper_title": "Nonparametric masked language modeling",
            "rating": 2
        },
        {
            "paper_title": "Training language models with memory augmentation",
            "rating": 2
        },
        {
            "paper_title": "Nearest neighbor zero-shot inference",
            "rating": 2
        },
        {
            "paper_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 1
        },
        {
            "paper_title": "Retrievalaugmented multimodal language modeling",
            "rating": 1
        },
        {
            "paper_title": "Adaptive semiparametric language models",
            "rating": 1
        }
    ],
    "cost": 0.011010250000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Tutorial Proposal: <br> Retrieval-based Language Models and Applications</h1>
<p>Akari Asai ${ }^{\dagger}$ Sewon Min ${ }^{\dagger}$ Zexuan Zhong ${ }^{\ddagger}$ Danqi Chen ${ }^{\ddagger}$<br>${ }^{\dagger}$ University of Washington ${ }^{\ddagger}$ Princeton University<br>{akari, sewon}@cs.washington.edu<br>{zzhong, danqic}@cs.princeton.edu</p>
<h2>1 Description</h2>
<p>Language models (LMs) such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) have shown impressive abilities in a range of natural language processing (NLP) tasks. However, relying solely on their parameters to encode a wealth of world knowledge requires a prohibitively large number of parameters and hence massive compute, and they often struggle to learn long-rail knowledge (Roberts et al., 2020; Kandpal et al., 2022; Mallen et al., 2022). Moreover, these parametric LMs are fundamentally incapable of adapting over time (De Cao et al., 2021; Lazaridou et al., 2021; Kasai et al., 2022), often hallucinate (Shuster et al., 2021), and may leak private data from the training corpus (Carlini et al., 2021). To overcome these limitations, there has been growing interest in retrieval-based LMs (Guu et al., 2020; Khandelwal et al., 2020; Borgeaud et al., 2022; Zhong et al., 2022; Izacard et al., 2022b; Min et al., 2022), which incorporate a non-parametric datastore (e.g., text chunks from an external corpus) with their parametric counterparts. Retrieval-based LMs can outperform LMs without retrieval by a large margin with much fewer parameters (Mallen et al., 2022), can update their knowledge by replacing their retrieval corpora (Izacard et al., 2022b), and provide citations for users to easily verify and evaluate the predictions (Menick et al., 2022; Bohnet et al., 2022).</p>
<p>Previously, retrieval and LMs have been studied mostly separately, and only recently researchers have integrated them and built systems in which retrieval and LMs interact more organically, and a number of retrieval-based LMs have been proposed due to growing interest. They differ in their neural architectures (e.g., the granularity of retrieval units, how to integrate retrieved information), learning algorithms, and different uses in downstream applications. In this tutorial, we aim to provide a
comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by first providing preliminaries covering the foundations of LM (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search methods widely used in neural retrieval systems; Karpukhin et al. 2020). We will then focus on recent progress in architectures, learning approaches, and applications of retrieval-based LMs.</p>
<p>A taxonomy of architectures We introduce a taxonomy of architectures of retrieval-based LMs based on a variety of dimensions. Retrieval-based LMs can be categorized by the granularity of retrieved units stored in the datastore: either 1) a chunk of text (Borgeaud et al., 2022; Izacard et al., 2022b), or 2) a token (Khandelwal et al., 2020; Zhong et al., 2022; Min et al., 2022), or 3) an entity mention (Fvry et al., 2020; de Jong et al., 2022). We also plan to cover techniques for refining data stores and improving similarity search (He et al., 2021; Alon et al., 2022). At the same time, retrieval-base LMs can be categorized based on how the retrieved information is integrated with the parametric encoder: 1) whether retrieved components are concatenated with the original input text (Lewis et al., 2020; Guu et al., 2020; Izacard et al., 2022b), 2) whether the retrieved components are latent and integrated into the intermediate layers of Transformers (de Jong et al., 2022; Fvry et al., 2020; Borgeaud et al., 2022), or 3) distribution of tokens from the retrieved components and the LMs are interpolated (Khandelwal et al., 2020; Zhong et al., 2022; Yogatama et al., 2021).</p>
<p>Scalable learning algorithms Then, we discuss the training approaches of retrieval-based LMs. Since a retrieval datastore is typically very large, how to train retrieval-based LMs effectively and efficiently remains challenging. We first discuss pipelined approaches that train retrieval components and LMs separately, either through large-</p>
<p>scale pre-training (Izacard et al., 2022a) or multitask instruction tuning (Asai et al., 2022). Several other works train retrieval-based LMs with a fixed retrieval module (Borgeaud et al., 2022; Yogatama et al., 2021). We then discuss joint training under reasonable resource requirements: either through in-batch approximations to a full datastore, or updating the datastore with updated parameters asynchronously. The former uses fractions of the full corpus that are carefully designed during joint training (Zhong et al., 2022; de Jong et al., 2022; Min et al., 2022). The latter, on the other hand, aims to use full corpus during training with asynchronous index update for every certain time steps (Izacard et al., 2022b; Guu et al., 2020).</p>
<p>Adaption to downstream tasks After discussing the basic building blocks of retrieval-based LMs, we show how retrieval-based LMs are adapted to downstream applications. We first briefly summarize the two approaches to adapt a model to a new task: zero-shot or few-shot prompting without any parameter updates (Shi et al., 2022; Wang et al., 2022), and fine-tuning on target task data (Lewis et al., 2020). We then discuss methods designed to build more powerful retrieval-based LMs for certain downstream tasks, such as dialogue (Shuster et al., 2021), semantic parsing (Pasupat et al., 2021), and machine translation (Khandelwal et al., 2021; Zheng et al., 2021).</p>
<p>Up to this point, our tutorial has mainly focused on retrieving and integrating English plain text. At this end, we will cover recent extensions of retrieval-based LMs beyond English text, including multilingual (Asai et al., 2021), multimodal (Chen et al., 2022; Yasunaga et al., 2022) and code (Parvez et al., 2021) retrieval. These works often extend dense retrieval models to enable retrieval between heterogeneous input spaces (e.g., cross-lingual, cross-modal) and have shown that referring retrieved knowledge leads to knowledgeintensive generation.</p>
<p>Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs. We conclude our tutorial by discussing several important questions and future directions, including (1) how we can further improve the scalability of retrievalbased LMs without sacrificing performance, (2) when retrieval-based LMs are particularly useful in the era of rapidly evolving LMs, and (3) what is necessary to enable applications of retrieval-based LMs for more diverse domains.</p>
<h2>2 Tutorial Outline</h2>
<ol>
<li>
<p>Introduction (15 minutes)</p>
</li>
<li>
<p>An overview of the tutorial</p>
</li>
<li>
<p>Why retrieval-based LMs?</p>
</li>
<li>
<p>Preliminaries (15 minutes)</p>
</li>
<li>
<p>Language models: Auto-regressive LMs vs. masked LMs</p>
</li>
<li>Dense retrieval methods</li>
<li>
<p>Approximate nearest neighbor search</p>
</li>
<li>
<p>Retrieval-based LMs: A taxonomy of architectures (40 minutes)</p>
</li>
<li>
<p>Granularity of datastore: tokens, entity mentions, and chunks of text</p>
</li>
<li>
<p>How retrieved information is integrated: incorporation in the input layer, intermediate layers, and the output layer</p>
</li>
<li>
<p>Retrieval-based LMs: Scalable learning algorithms (40 minutes)</p>
</li>
<li>
<p>Pipelined training</p>
</li>
<li>Training with In-batch approximations</li>
<li>
<p>Joint training of retrieval and LMs with asynchronous updates of corpus</p>
</li>
<li>
<p>Retrieval-based LMs: Downstream adaptations (40 minutes)</p>
</li>
<li>
<p>Adaptation methods: zero-shot/few-shot prompting and fine-tuning on downstream tasks</p>
</li>
<li>
<p>Downstream applications and task-specific modifications (e.g., dialogue, semantic parsing)</p>
</li>
<li>
<p>Extensions beyond English text (10 minutes)</p>
</li>
<li>
<p>Multilingual retrieval-based LMs</p>
</li>
<li>Multimodal retrieval-based LMs</li>
<li>
<p>Code generation</p>
</li>
<li>
<p>Demostration: An exercise to show retrievalaugmented LMs (10 minutes)</p>
</li>
<li>Conclusions and future directions (10 minutes)</li>
</ol>
<h2>3 Tutorial Information</h2>
<p>Type of the tutorial Cutting-edge.
Length This is a 3-hour tutorial.
Target audience The tutorial will be accessible to anyone who has a basic knowledge of machine learning and natural language processing. We think the topic will be of interest to both NLP researchers/students in academia and NLP practitioners in the industry.</p>
<p>Breadth We estimate that $20 \%$ of the work covered in this tutorial will be by the presenters and the remaining $80 \%$ by others. The papers we will cover are from both academia and industry.</p>
<p>Diversity considerations. The speakers are from two academic institutions with an affiliation with an industry research group, including both a professor and Ph.D. students. Three out of four speakers are female. The methods covered by our tutorials can scale up to various languages or domains, and we also briefly cover several papers focusing on multilingual and expert-domain extensions of the core frameworks. We will reach out to academic communities such as WiNLP ${ }^{1}$ and Masakhane ${ }^{2}$ to encourage them to attend our tutorial for participation of diverse audiences. Since retrieval-based LMs are alternatives to LMs with a significantly large number of parameters, we expect this tutorial to be especially useful to researchers with modest resources who do no have access to very large models.</p>
<p>An estimate of the audience size Given that language models are now used in a range of NLP tasks and retrieval-based approaches have been applied to diverse domains, we estimate that the number of audiences will be around $150+$.</p>
<p>Venues. We prefer ACL due to the growing interest in the area and the travel constraints of some of the speakers. EMNLP is our second preferred choice, and we currently do not consider EACL.</p>
<p>Technical equipment. We would like to have Internet access to show online demos.</p>
<p>Open access We plan to make all teaching material available online and agree to allow the publication of slides and video recordings in the ACL anthology.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Ethical considerations Retrieval-based LMs are often more powerful and parameter-efficient than LMs, and do not require full re-training to update world knowledge, which makes it more energyefficient and can reduce carbon footprints. Prior work also shows that referring to external world knowledge can reduce harmful biases and hallucinations, although retrieval-based LMs can still be plausible sounding but incorrect or non-sensical outputs. We note that, as retrieval-based LMs may retrieve raw data from a corpus, which can leak privacy-sensitive information, especially when they are built on top of a private corpus. We acknowledge this to caution those who manage to apply retrieval-based LMs to privacy-sensitive domains.</p>
<p>Pedagogical material We plan to do some short hands-on exercises to let the audience try different retrieval-based LMs with few-shot prompting using Colab.</p>
<h2>Past tutorials.</h2>
<ul>
<li>ACL 2020 tutorial on Open-domain QA (Chen and Yih, 2020): This tutorial provides comprehensive reviews of open-domain question answering, some of which consist of a retriever and a generative model, while we focus on the recent progress of architectures and learning algorithms of retrieval-based LMs for diverse NLP tasks, not limiting its focus to open-domain QA. Most of the papers will be discussed in this tutorial have been published since the Open-domain QA tutorial three years ago. Moreover, one of the instructors, Danqi was an instructor of this ACL 2020 tutorial.</li>
<li>SIGIR 2022 tutorial on Recent Advances in Retrieval-Augmented Text Generation (Cai et al., 2022): This tutorial focuses mainly on recent retrieval-augmented text generation approaches with a focus on two applications: dialogue and machine translation. Our tutorial puts more emphasis on the architecture and learning methods of retrieval-based LMs that can be applicable to diverse NLP tasks.</li>
</ul>
<h2>4 Presenters</h2>
<p>Akari Asai Akari Asai is a Ph.D. student in the Paul G. Allen School of Computer Science \&amp; Engineering at the University of Washington, advised by Prof. Hannaneh Hajishirzi. Her research lies</p>
<p>in natural language processing and machine learning. Her recent research focuses on question answering, retrieval-based LMs, multilingual NLP, and entity-aware representations. She received the IBM Fellowship in 2022. She is a lead organizer of the Workshop on Multilingual Information Access (NAACL 2022) and serves as an area chair in question answering at EACL 2023.</p>
<p>Sewon Min Sewon Min is a Ph.D. student in the Paul G. Allen School of Computer Science \&amp; Engineering at the University of Washington, and a visiting researcher at Meta AI. Her research spans question answering, representation and retrieval of factoid knowledge, and language modeling. She was a co-instructor and a co-organizer of multiple tutorials and workshops at ACL, NAACL-HLT, EMNLP, NeurIPS and AKBC, including a tutorial on Few-Shot NLP with Pretrained Language Models (ACL 2022), a tutorial on NLP for Long Sequences (NAACL-HLT 2021), and the Workshop on Semiparametric Methods in NLP (ACL 2022).</p>
<p>Zexuan Zhong Zexuan Zhong is a Ph.D. student in the Department of Computer Science at Princeton University, advised by Prof. Danqi Chen. His research interests lie in natural language processing and machine learning. His recent research focuses on retrieval-based LMs, generalization of retrieval models, and efficient models in NLP. He received a J.P. Morgan PhD Fellowship in 2022.</p>
<p>Danqi Chen Danqi Chen is an Assistant Professor of Computer Science at Princeton University and co-leads the Princeton NLP Group. Her recent research focuses on training, adapting, and understanding large LMs, and developing scalable and generalizable NLP systems for question answering, information extraction, and conversational agents. Danqi is a recipient of a Sloan Fellowship, a Samsung AI Researcher of the Year award, outstanding paper awards from ACL 2016, EMNLP 2017 and ACL 2022, and multiple industry faculty awards. Danqi served as the program chair for AKBC 2021 and (senior) area chairs for many *ACL conferences. She taught a tutorial on "Opendomain Question Answering" at ACL 2020.</p>
<h2>5 Reading List</h2>
<ul>
<li>Unsupervised Dense Information Retrieval with Contrastive Learning (Izacard et al., 2022a)</li>
<li>Task-aware Retrieval with Instructions (Asai et al., 2022)</li>
<li>Atlas: Few-shot Learning with Retrieval Augmented Language Models (Izacard et al., 2022b)</li>
<li>Improving language models by retrieving from trillions of tokens (Borgeaud et al., 2022)</li>
<li>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention (de Jong et al., 2022)</li>
<li>Generalization through Memorization: Nearest Neighbor Language Models (Khandelwal et al., 2020)</li>
<li>Nonparametric Masked Language Model (Min et al., 2022)</li>
<li>Training Language Models with Memory Augmentation (Zhong et al., 2022)</li>
<li>kNN-Prompt: Nearest Neighbor Zero-Shot Inference (Shi et al., 2022)</li>
<li>Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval (Alon et al., 2022)</li>
</ul>
<h2>References</h2>
<p>Uri Alon, Frank F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. Neuro-symbolic language modeling with automatonaugmented retrieval. In International Conference on Machine Learning (ICML), Baltimore, USA.</p>
<p>Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260.</p>
<p>Akari Asai, Xinyan Yu, Jungo Kasai, and Hanna Hajishirzi. 2021. One question answering model for many languages with cross-lingual dense passage retrieval. In Advances in Neural Information Processing Systems.</p>
<p>Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in neural information processing systems.</p>
<p>Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi. 2022. Recent advances in retrieval-augmented text generation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval.</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21).</p>
<p>Danqi Chen and Wen-tau Yih. 2020. Open-domain question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 34-37, Online. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W Cohen. 2022. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William W. Cohen. 2022. Mention memory: incorporating textual knowledge into transformers through entity mention attention. In International Conference on Learning Representations.</p>
<p>Thibault Fvry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International Conference on Machine Learning.</p>
<p>Junxian He, Graham Neubig, and Taylor BergKirkpatrick. 2021. Efficient nearest neighbor language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Punta Cana, Dominican Republic.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. arXiv preprint arXiv:2211.08411.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, and Kentaro Inui. 2022. Realtime qa: What's the answer right now? arXiv preprint arXiv:2207.13332.</p>
<p>Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation. In International Conference on Learning Representations.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations.</p>
<p>Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson dAutume, Tomas Kocisky, Sebastian Ruder, et al. 2021. Mind the gap: Assessing temporal generalization in neural language</p>
<p>models. Advances in Neural Information Processing Systems.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511.</p>
<p>Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, et al. 2022. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147.</p>
<p>Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Nonparametric masked language modeling. arXiv preprint arXiv:2212.01349.</p>
<p>Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719-2734, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022. Nearest neighbor zero-shot inference. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021.</p>
<p>Zhenhailong Wang, Xiaoman Pan, Dian Yu, Dong Yu, Jianshu Chen, and Heng Ji. 2022. Zemi: Learning zero-shot semi-parametric language models from multiple tasks. arXiv preprint arXiv:2210.00185.</p>
<p>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022. Retrievalaugmented multimodal language modeling. arXiv preprint arXiv:2211.12561.</p>
<p>Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. 2021. Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics, 9:362-373.</p>
<p>Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen, Weihua Luo, and Jiajun Chen. 2021. Adaptive nearest neighbor machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://www.winlp.org/
${ }^{2}$ https://www.masakhane.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
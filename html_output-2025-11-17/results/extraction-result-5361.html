<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5361 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5361</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5361</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-2b967d82b25088566980aaaf5a7062d90b2fb14f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b967d82b25088566980aaaf5a7062d90b2fb14f" target="_blank">GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Through this study, the current limitations of language models in comprehending graph structures and performing associated reasoning tasks are uncovered and the necessity for further advancements and novel approaches to enhance their graph processing capabilities is emphasized.</p>
                <p><strong>Paper Abstract:</strong> Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5361.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5361.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adjacency List (text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adjacency List textual representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph is linearized by listing each node together with its neighbors (node: [neighbor1, neighbor2, ...]) and appended as plain text to the prompt; used with format-explanation and role-prompting to help LLMs parse structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency List linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node is represented in text followed by a list of its adjacent nodes (an adjacency list). The adjacency-list text is combined with question text and optional format-explanation, role prompting, and examples (1-shot, 0-shot, chain-of-thought variations) before being fed to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (ogbn-arxiv, Aminer) and generic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicitly preserves local neighborhood relations; relatively interpretable to humans and LLMs; moderately compact for small subgraphs (10–20 nodes used in experiments); order-sensitive (paper reports that change in input order affects performance); may lose scalability for large graphs; benefits from format-explanation and role prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure understanding tasks: size detection, degree detection, edge detection, attribute retrieval, diameter estimation, clustering coefficient; also used for semantic tasks when node text available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ACC (Table 1, OGBN-ARXIV subgraphs): Size Detection 35.50 (1-shot), Degree Detection 15.21, Edge Detection 65.45, Diameter 28.00, Clustering 5.42. Variants: 1-shot-cot improved Size to 44.00 but decreased clustering. (All numbers are percentages / accuracy as reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Underperforms GML and (on many structural tasks) Edge List: e.g., Size Detection (AdjList 35.5 vs GML 54.5), Degree Detection better in Edge List (44.87) than adjacency list (15.21). Adjacency list gives reasonable edge-detection accuracy but is generally weaker for global measures (diameter, clustering) compared to formats that include richer attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Highly sensitive to input order and prompt design; scales poorly as raw adjacency text for larger graphs; format-explanation and role prompting are needed to reach reasonable performance; often yields low accuracy for degree and clustering without careful prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5361.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5361.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge List (text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge List textual representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph is represented as a plain text list of edges (nodeA -- nodeB per line or pair tuples), optionally enriched with node attributes; used as input to LLM with prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge List linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph is serialized as an explicit list of edges (each line/entry is an edge pair). The list is combined with question text and optional format explanations, role prompts, and examples (0/1-shot and chain-of-thought variants).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (ogbn-arxiv, Aminer) and generic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Directly encodes pairwise connectivity (good for local connectivity / edge queries); compact for sparse graphs; relatively straightforward for LLM parsing; less explicit about per-node neighbor lists unless aggregated; order-sensitive and benefits from prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure understanding tasks: degree detection, edge detection, size detection, diameter, clustering coefficient; semantic tasks when node attributes included.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ACC (Table 1, OGBN-ARXIV subgraphs): Degree Detection 44.87 (1-shot), Edge Detection 74.60, Size Detection 22.50, Diameter 43.00, Clustering 13.31. Some variants (w/o 1-shot) increase edge-detection to 80.95.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Strong on local connectivity tasks (edge detection best among formats in many settings) and degree detection relative to adjacency list; weaker than GML for size detection and weaker than GraphML/GML for attribute-heavy tasks; change-of-order and in-context examples substantially affect results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May not make neighborhood structure explicit (requires counting edges or reconstructing adjacency), sensitive to order and prompt framing, and not ideal for attribute retrieval unless attributes are appended to edges or nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5361.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5361.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GML (Graph Modelling Language)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Modelling Language (GML) textual format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured, bracketed textual graph format (GML) with explicit node and edge blocks, supporting node/edge attributes; used as a text input format (GDL) to LLMs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GML textual serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph is converted to GML, a nested, human-readable text format that contains explicit node entries, edge entries, and attributes (e.g., id, label, properties). The GML text is concatenated to the prompt; optionally accompanied by format explanation and role prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks, molecular graphs, knowledge graphs — used across OGBN, OGBG, MetaQA, Wiki</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>High expressivity (supports attributes and structured metadata), human- and machine-readable, retains rich attribute information enabling attribute retrieval tasks; more verbose than edge/adjacency lists but conveys richer context; robust for both structural and semantic queries according to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure understanding (size, degree, edge detection, attribute retrieval, diameter, clustering), semantic tasks (node classification, graph classification, KGQA/GQL generation when nodes have textual attributes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ACC (Table 1, OGBN-ARXIV): Size Detection 54.50, Degree Detection 20.91, Edge Detection 50.45, Attribute Retrieval 83.40, Diameter 37.00, Clustering 4.36. Graph classification (OGBG-MOLHIV) 1-shot-tot: 66.87 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GML often yields the best or among the best performance on global and attribute tasks (e.g., Size Detection 54.5 highest among formats; Attribute Retrieval ~83.4 comparable to GraphML). Outperforms plain adjacency/edge lists on several global metrics and graph-classification tasks due to richer metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verbose and may not scale to large graphs as plain text prompt; still not matching specialized graph models; requires format-explanation for optimal LLM comprehension; some structural tasks (e.g., degree detection) still underperform compared to edge-list in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5361.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5361.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphML (text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphML textual representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An XML-based textual graph serialization (GraphML) containing nodes, edges and attributes, provided as text to LLMs; used as one of the GDL formats in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphML textual serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are converted into GraphML (XML-like) text blocks including explicit node and edge tags and optional attributes. The GraphML text is included in the prompt; the paper experiments with variations (1-shot, 0-shot, chain-of-thought, format-explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks, molecular graphs, knowledge graphs (same datasets as other GDLs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Highly expressive, captures attributes and structure with explicit tags; potentially more regularized than free-form GML; readable but verbose; provides strong signals for attribute retrieval and graph-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Structure understanding tasks (size, degree, edge detection, attribute retrieval, diameter, clustering), semantic tasks including graph classification and node classification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ACC (Table 1, OGBN-ARXIV): Size Detection 25.00, Degree Detection 40.20, Edge Detection 62.05, Attribute Retrieval 83.87, Diameter 34.00, Clustering 9.74. Graph classification (OGBG-MOLHIV) 1-shot-tot: 63.25 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GraphML shows strong attribute retrieval (83.87 similar to GML) and competitive edge/degree performance (degree 40.20). GML slightly outperforms GraphML on some global tasks (e.g., size detection), whereas GraphML is competitive for attribute-heavy tasks; Edge List remains better for raw edge detection in some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verbose XML-like format can be large for bigger graphs; LLM performance still sensitive to prompt design and examples; some structural metrics still have low absolute accuracy compared to specialized graph models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5361.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5361.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-augmented prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot/one-shot prompting with explicit graph text (+graph / change-order / 1-shot variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategy that appends a textual graph description (GDL) to the question prompt; variants include zero-shot+graph, zero-shot+graph+change-order, zero-shot-cot+graph, one-shot Cypher generation and 1-shot-cot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-augmented prompt strategies</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Combine the user query with a textual serialization of the graph (Adjacency, Edge List, GML, GraphML) and optionally reorder the prompt (change-order), include one or few in-context examples (1-shot), or chain-of-thought style instructions (cot). For GQL generation, one-shot Cypher example was used to prompt generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (MetaQA, Wiki), citation and molecular graphs for other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LLM in-context learning; adding graph text greatly improves performance on KGQA and some semantic tasks; position of graph vs external knowledge (change-order) matters; chain-of-thought helps some tasks but can hurt others.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>KGQA (Wiki, MetaQA), Cypher/GQL generation, node classification, graph classification, structure understanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KGQA / GQL (Table 2): zero-shot baseline very low (Wiki 9.23%), zero-shot+graph: Wiki 56.38%, MetaQA-1hop 91.69%, MetaQA-2hop 46.82%, MetaQA-3hop 19.40%. One-shot Cypher Generation: MetaQA-1hop 99.00%, MetaQA-2hop 77.00%, MetaQA-3hop 96.00%. Node classification (Table 3): one-shot + 2-hop context ACC 60.00.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Adding graph text (+graph) substantially outperforms plain zero-shot/no-graph prompts on KGQA and node classification; one-shot examples (especially for Cypher generation) can reach or exceed SOTA on some MetaQA splits (one-shot Cypher 99.00% on 1-hop). Change-order and chain-of-thought yield mixed effects dependent on task and format.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance varies widely by task, number of hops, and prompt variants; multi-hop KGQA remains challenging (performance drops as hops increase); order sensitivity and need for carefully chosen examples; still below SOTA on some KGQA splits without strong prompt engineering or one-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5361.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5361.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Prompting (self-format / self-summary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-prompting via format explanation and context summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM is asked to generate intermediate context: summarize graph context (important nodes/ neighborhoods) and produce a format explanation to augment the original graph text before answering final queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Self-prompting / self-augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A two-stage prompting pipeline: first request the LLM to create auxiliary contextual artifacts (context summarization such as neighborhood summaries or 'format explanation' describing how the graph text is structured), then combine these artifacts with the original input to prompt the LLM for the final answer. Also called self-format explanation and self-summarization in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks, molecular graphs, knowledge graphs, general small subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Aims to reduce irrelevant information and enrich salient features; creates neighborhood-aware text features; acts as data augmentation at prompt time; reported to improve graph classification and node classification performance when used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, graph classification, structure understanding tasks where auxiliary summaries aid reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Graph classification (Table 4): self-augmentation helps — e.g., GML 1-shot-cot 67.65 vs w/o self-format explanation 64.71 and w/o self-summarization 61.76 (OGBG-MOLHIV). Node classification: one-shot + 1-hop neighborhood context summarization achieved up to 60.00% with 2-hop variants = 60.00 (Table 3 shows improvements when using neighborhood summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Self-augmentation improves over raw GDL inputs: removing self-format explanation or self-summarization lowers graph classification accuracy. Combining self-prompting with GML / GraphML yields better performance than those formats without self-augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on LLM to produce reliable summaries/explanations (risk of hallucination or omission); introduces extra API calls and latency; effectiveness depends on quality of generated summaries and on the downstream task; may not scale for very large graphs or deep multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5361.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5361.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neighborhood Context Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-hop neighborhood textual summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert k-hop neighbor information of a target node into summarized textual features (1-hop, 2-hop summaries) and append these to the node's prompt to provide neighborhood-aware context for classification or reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>k-hop neighborhood summarization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extract text attributes (e.g., title, abstract) from a target node's 1-hop or 2-hop neighborhood and either include raw neighbor texts or a condensed summary (generated by the LLM or aggregated) in the prompt, enabling the LLM to reason with local context.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (ogbn-arxiv), knowledge graphs (MetaQA) for contextual reasoning and node classification</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Increases contextual richness and improves classification accuracy (2-hop generally better than 1-hop in experiments); trades off prompt length and noise—more hops may add useful information or redundant/noisy text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (OGBN-ARXIV), also helps some semantic tasks requiring local context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Node classification (Table 3): zero-shot (self only) 48.00%, zero-shot 1-hop 53.00%, zero-shot 2-hop 57.00%; one-shot 2-hop achieved 60.00% (best reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Neighborhood summarization augmented LLM prompts outperform using only the target node's self-text and improve over plain zero-shot/cot variants; using 2-hop context consistently improves accuracy relative to 1-hop and self-only inputs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Increasing hops increases prompt size and potential noisy/irrelevant information; requires design choices for how to summarize or prune neighborhood text; may be limited by LLM context window for larger neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>StructGPT: A general framework for large language model to reason over structured data <em>(Rating: 2)</em></li>
                <li>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt <em>(Rating: 2)</em></li>
                <li>GML: Graph Modelling Language <em>(Rating: 2)</em></li>
                <li>GraphML <em>(Rating: 2)</em></li>
                <li>TableGPT: Few-shot table-to-text generation with table structure reconstruction and content matching <em>(Rating: 1)</em></li>
                <li>PLOG: Table-to-logic pretraining for logical table-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5361",
    "paper_id": "paper-2b967d82b25088566980aaaf5a7062d90b2fb14f",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Adjacency List (text)",
            "name_full": "Adjacency List textual representation",
            "brief_description": "Graph is linearized by listing each node together with its neighbors (node: [neighbor1, neighbor2, ...]) and appended as plain text to the prompt; used with format-explanation and role-prompting to help LLMs parse structure.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Adjacency List linearization",
            "representation_description": "Each node is represented in text followed by a list of its adjacent nodes (an adjacency list). The adjacency-list text is combined with question text and optional format-explanation, role prompting, and examples (1-shot, 0-shot, chain-of-thought variations) before being fed to the LLM.",
            "graph_type": "Citation networks (ogbn-arxiv, Aminer) and generic graphs",
            "representation_properties": "Explicitly preserves local neighborhood relations; relatively interpretable to humans and LLMs; moderately compact for small subgraphs (10–20 nodes used in experiments); order-sensitive (paper reports that change in input order affects performance); may lose scalability for large graphs; benefits from format-explanation and role prompting.",
            "evaluation_task": "Structure understanding tasks: size detection, degree detection, edge detection, attribute retrieval, diameter estimation, clustering coefficient; also used for semantic tasks when node text available.",
            "performance_metrics": "Reported ACC (Table 1, OGBN-ARXIV subgraphs): Size Detection 35.50 (1-shot), Degree Detection 15.21, Edge Detection 65.45, Diameter 28.00, Clustering 5.42. Variants: 1-shot-cot improved Size to 44.00 but decreased clustering. (All numbers are percentages / accuracy as reported.)",
            "comparison_to_other_representations": "Underperforms GML and (on many structural tasks) Edge List: e.g., Size Detection (AdjList 35.5 vs GML 54.5), Degree Detection better in Edge List (44.87) than adjacency list (15.21). Adjacency list gives reasonable edge-detection accuracy but is generally weaker for global measures (diameter, clustering) compared to formats that include richer attributes.",
            "limitations_or_challenges": "Highly sensitive to input order and prompt design; scales poorly as raw adjacency text for larger graphs; format-explanation and role prompting are needed to reach reasonable performance; often yields low accuracy for degree and clustering without careful prompting.",
            "uuid": "e5361.0",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Edge List (text)",
            "name_full": "Edge List textual representation",
            "brief_description": "Graph is represented as a plain text list of edges (nodeA -- nodeB per line or pair tuples), optionally enriched with node attributes; used as input to LLM with prompt engineering.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Edge List linearization",
            "representation_description": "Graph is serialized as an explicit list of edges (each line/entry is an edge pair). The list is combined with question text and optional format explanations, role prompts, and examples (0/1-shot and chain-of-thought variants).",
            "graph_type": "Citation networks (ogbn-arxiv, Aminer) and generic graphs",
            "representation_properties": "Directly encodes pairwise connectivity (good for local connectivity / edge queries); compact for sparse graphs; relatively straightforward for LLM parsing; less explicit about per-node neighbor lists unless aggregated; order-sensitive and benefits from prompting.",
            "evaluation_task": "Structure understanding tasks: degree detection, edge detection, size detection, diameter, clustering coefficient; semantic tasks when node attributes included.",
            "performance_metrics": "Reported ACC (Table 1, OGBN-ARXIV subgraphs): Degree Detection 44.87 (1-shot), Edge Detection 74.60, Size Detection 22.50, Diameter 43.00, Clustering 13.31. Some variants (w/o 1-shot) increase edge-detection to 80.95.",
            "comparison_to_other_representations": "Strong on local connectivity tasks (edge detection best among formats in many settings) and degree detection relative to adjacency list; weaker than GML for size detection and weaker than GraphML/GML for attribute-heavy tasks; change-of-order and in-context examples substantially affect results.",
            "limitations_or_challenges": "May not make neighborhood structure explicit (requires counting edges or reconstructing adjacency), sensitive to order and prompt framing, and not ideal for attribute retrieval unless attributes are appended to edges or nodes.",
            "uuid": "e5361.1",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GML (Graph Modelling Language)",
            "name_full": "Graph Modelling Language (GML) textual format",
            "brief_description": "A structured, bracketed textual graph format (GML) with explicit node and edge blocks, supporting node/edge attributes; used as a text input format (GDL) to LLMs in experiments.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "GML textual serialization",
            "representation_description": "The graph is converted to GML, a nested, human-readable text format that contains explicit node entries, edge entries, and attributes (e.g., id, label, properties). The GML text is concatenated to the prompt; optionally accompanied by format explanation and role prompting.",
            "graph_type": "Citation networks, molecular graphs, knowledge graphs — used across OGBN, OGBG, MetaQA, Wiki",
            "representation_properties": "High expressivity (supports attributes and structured metadata), human- and machine-readable, retains rich attribute information enabling attribute retrieval tasks; more verbose than edge/adjacency lists but conveys richer context; robust for both structural and semantic queries according to experiments.",
            "evaluation_task": "Structure understanding (size, degree, edge detection, attribute retrieval, diameter, clustering), semantic tasks (node classification, graph classification, KGQA/GQL generation when nodes have textual attributes).",
            "performance_metrics": "Reported ACC (Table 1, OGBN-ARXIV): Size Detection 54.50, Degree Detection 20.91, Edge Detection 50.45, Attribute Retrieval 83.40, Diameter 37.00, Clustering 4.36. Graph classification (OGBG-MOLHIV) 1-shot-tot: 66.87 (Table 4).",
            "comparison_to_other_representations": "GML often yields the best or among the best performance on global and attribute tasks (e.g., Size Detection 54.5 highest among formats; Attribute Retrieval ~83.4 comparable to GraphML). Outperforms plain adjacency/edge lists on several global metrics and graph-classification tasks due to richer metadata.",
            "limitations_or_challenges": "Verbose and may not scale to large graphs as plain text prompt; still not matching specialized graph models; requires format-explanation for optimal LLM comprehension; some structural tasks (e.g., degree detection) still underperform compared to edge-list in experiments.",
            "uuid": "e5361.2",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GraphML (text)",
            "name_full": "GraphML textual representation",
            "brief_description": "An XML-based textual graph serialization (GraphML) containing nodes, edges and attributes, provided as text to LLMs; used as one of the GDL formats in the benchmark.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "GraphML textual serialization",
            "representation_description": "Graphs are converted into GraphML (XML-like) text blocks including explicit node and edge tags and optional attributes. The GraphML text is included in the prompt; the paper experiments with variations (1-shot, 0-shot, chain-of-thought, format-explanation).",
            "graph_type": "Citation networks, molecular graphs, knowledge graphs (same datasets as other GDLs)",
            "representation_properties": "Highly expressive, captures attributes and structure with explicit tags; potentially more regularized than free-form GML; readable but verbose; provides strong signals for attribute retrieval and graph-level tasks.",
            "evaluation_task": "Structure understanding tasks (size, degree, edge detection, attribute retrieval, diameter, clustering), semantic tasks including graph classification and node classification.",
            "performance_metrics": "Reported ACC (Table 1, OGBN-ARXIV): Size Detection 25.00, Degree Detection 40.20, Edge Detection 62.05, Attribute Retrieval 83.87, Diameter 34.00, Clustering 9.74. Graph classification (OGBG-MOLHIV) 1-shot-tot: 63.25 (Table 4).",
            "comparison_to_other_representations": "GraphML shows strong attribute retrieval (83.87 similar to GML) and competitive edge/degree performance (degree 40.20). GML slightly outperforms GraphML on some global tasks (e.g., size detection), whereas GraphML is competitive for attribute-heavy tasks; Edge List remains better for raw edge detection in some variants.",
            "limitations_or_challenges": "Verbose XML-like format can be large for bigger graphs; LLM performance still sensitive to prompt design and examples; some structural metrics still have low absolute accuracy compared to specialized graph models.",
            "uuid": "e5361.3",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Graph-augmented prompting",
            "name_full": "Zero-shot/one-shot prompting with explicit graph text (+graph / change-order / 1-shot variants)",
            "brief_description": "Prompting strategy that appends a textual graph description (GDL) to the question prompt; variants include zero-shot+graph, zero-shot+graph+change-order, zero-shot-cot+graph, one-shot Cypher generation and 1-shot-cot.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Graph-augmented prompt strategies",
            "representation_description": "Combine the user query with a textual serialization of the graph (Adjacency, Edge List, GML, GraphML) and optionally reorder the prompt (change-order), include one or few in-context examples (1-shot), or chain-of-thought style instructions (cot). For GQL generation, one-shot Cypher example was used to prompt generation.",
            "graph_type": "Knowledge graphs (MetaQA, Wiki), citation and molecular graphs for other tasks",
            "representation_properties": "Leverages LLM in-context learning; adding graph text greatly improves performance on KGQA and some semantic tasks; position of graph vs external knowledge (change-order) matters; chain-of-thought helps some tasks but can hurt others.",
            "evaluation_task": "KGQA (Wiki, MetaQA), Cypher/GQL generation, node classification, graph classification, structure understanding tasks.",
            "performance_metrics": "KGQA / GQL (Table 2): zero-shot baseline very low (Wiki 9.23%), zero-shot+graph: Wiki 56.38%, MetaQA-1hop 91.69%, MetaQA-2hop 46.82%, MetaQA-3hop 19.40%. One-shot Cypher Generation: MetaQA-1hop 99.00%, MetaQA-2hop 77.00%, MetaQA-3hop 96.00%. Node classification (Table 3): one-shot + 2-hop context ACC 60.00.",
            "comparison_to_other_representations": "Adding graph text (+graph) substantially outperforms plain zero-shot/no-graph prompts on KGQA and node classification; one-shot examples (especially for Cypher generation) can reach or exceed SOTA on some MetaQA splits (one-shot Cypher 99.00% on 1-hop). Change-order and chain-of-thought yield mixed effects dependent on task and format.",
            "limitations_or_challenges": "Performance varies widely by task, number of hops, and prompt variants; multi-hop KGQA remains challenging (performance drops as hops increase); order sensitivity and need for carefully chosen examples; still below SOTA on some KGQA splits without strong prompt engineering or one-shot examples.",
            "uuid": "e5361.4",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Prompting (self-format / self-summary)",
            "name_full": "Self-prompting via format explanation and context summarization",
            "brief_description": "LLM is asked to generate intermediate context: summarize graph context (important nodes/ neighborhoods) and produce a format explanation to augment the original graph text before answering final queries.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "Self-prompting / self-augmentation",
            "representation_description": "A two-stage prompting pipeline: first request the LLM to create auxiliary contextual artifacts (context summarization such as neighborhood summaries or 'format explanation' describing how the graph text is structured), then combine these artifacts with the original input to prompt the LLM for the final answer. Also called self-format explanation and self-summarization in experiments.",
            "graph_type": "Citation networks, molecular graphs, knowledge graphs, general small subgraphs",
            "representation_properties": "Aims to reduce irrelevant information and enrich salient features; creates neighborhood-aware text features; acts as data augmentation at prompt time; reported to improve graph classification and node classification performance when used.",
            "evaluation_task": "Node classification, graph classification, structure understanding tasks where auxiliary summaries aid reasoning.",
            "performance_metrics": "Graph classification (Table 4): self-augmentation helps — e.g., GML 1-shot-cot 67.65 vs w/o self-format explanation 64.71 and w/o self-summarization 61.76 (OGBG-MOLHIV). Node classification: one-shot + 1-hop neighborhood context summarization achieved up to 60.00% with 2-hop variants = 60.00 (Table 3 shows improvements when using neighborhood summarization).",
            "comparison_to_other_representations": "Self-augmentation improves over raw GDL inputs: removing self-format explanation or self-summarization lowers graph classification accuracy. Combining self-prompting with GML / GraphML yields better performance than those formats without self-augmentation.",
            "limitations_or_challenges": "Relies on LLM to produce reliable summaries/explanations (risk of hallucination or omission); introduces extra API calls and latency; effectiveness depends on quality of generated summaries and on the downstream task; may not scale for very large graphs or deep multi-hop reasoning.",
            "uuid": "e5361.5",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Neighborhood Context Summarization",
            "name_full": "k-hop neighborhood textual summarization",
            "brief_description": "Convert k-hop neighbor information of a target node into summarized textual features (1-hop, 2-hop summaries) and append these to the node's prompt to provide neighborhood-aware context for classification or reasoning.",
            "citation_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking",
            "mention_or_use": "use",
            "representation_name": "k-hop neighborhood summarization",
            "representation_description": "Extract text attributes (e.g., title, abstract) from a target node's 1-hop or 2-hop neighborhood and either include raw neighbor texts or a condensed summary (generated by the LLM or aggregated) in the prompt, enabling the LLM to reason with local context.",
            "graph_type": "Citation networks (ogbn-arxiv), knowledge graphs (MetaQA) for contextual reasoning and node classification",
            "representation_properties": "Increases contextual richness and improves classification accuracy (2-hop generally better than 1-hop in experiments); trades off prompt length and noise—more hops may add useful information or redundant/noisy text.",
            "evaluation_task": "Node classification (OGBN-ARXIV), also helps some semantic tasks requiring local context.",
            "performance_metrics": "Node classification (Table 3): zero-shot (self only) 48.00%, zero-shot 1-hop 53.00%, zero-shot 2-hop 57.00%; one-shot 2-hop achieved 60.00% (best reported).",
            "comparison_to_other_representations": "Neighborhood summarization augmented LLM prompts outperform using only the target node's self-text and improve over plain zero-shot/cot variants; using 2-hop context consistently improves accuracy relative to 1-hop and self-only inputs in experiments.",
            "limitations_or_challenges": "Increasing hops increases prompt size and potential noisy/irrelevant information; requires design choices for how to summarize or prune neighborhood text; may be limited by LLM context window for larger neighborhoods.",
            "uuid": "e5361.6",
            "source_info": {
                "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "StructGPT: A general framework for large language model to reason over structured data",
            "rating": 2
        },
        {
            "paper_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
            "rating": 2
        },
        {
            "paper_title": "GML: Graph Modelling Language",
            "rating": 2
        },
        {
            "paper_title": "GraphML",
            "rating": 2
        },
        {
            "paper_title": "TableGPT: Few-shot table-to-text generation with table structure reconstruction and content matching",
            "rating": 1
        },
        {
            "paper_title": "PLOG: Table-to-logic pretraining for logical table-to-text generation",
            "rating": 1
        }
    ],
    "cost": 0.014458249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</h1>
<p>Jiayan Guo ${ }^{1} ;$ Lun Du ${ }^{2} ;$ Hengyu Liu ${ }^{3}$, Mengyu Zhou ${ }^{2}$, Xinyi He ${ }^{4}$, Shi Han ${ }^{2}$<br>${ }^{1}$ School of Intelligence Science and Technology, Peking University;<br>${ }^{2}$ Microsoft; ${ }^{3}$ University of Technology Sydney; ${ }^{4} \mathrm{Xi}$ an Jiaotong University<br>guojiayan@pku.edu.cn, {lun.du, mezhou, shihan}@microsoft.com, hengyu.liu@uts.edu.au,hxyhxy@stu.xjtu.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLM) like ChatGPT have become indispensable to artificial general intelligence (AGI), demonstrating excellent performance in various natural language processing tasks. Graph data is ubiquitous and an essential part of AGI. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graphstructured data. In this paper, we conduct an empirical study to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semanticrelated tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we uncover current limitations and future directions of LLMs in comprehending graph and performing associated reasoning tasks.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have demonstrated significant capability across a diverse array of human-centric tasks. These tasks range from answering questions to performing semantic analysis and identifying named entities (Zhao et al., 2023). Despite the considerable strides that have been made, the capacity of LLMs to decipher and manage structured knowledge, especially in the form of graph-structured data, remains an area ripe for exploration. Understanding graph-structured data is vital, given its pervasive presence and integral role in a multitude of applications such as social network analysis, drug discovery, recommender systems, and spatio-temporal prediction. Understanding graph data is crucial for AGI.</p>
<p>Tasks based on graph data can be broadly classified into two categories based on their goals.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The first category includes structure understanding tasks like identifying significant nodes, calculating centrality metrics (Okamoto et al., 2008; Zhang and Luo, 2017; Brandes, 2001; Barthelemy, 2004; Newman, 2005), and determining diameters (Chung et al., 1994). The second category encompasses semantic understanding tasks, such as knowledge graph question answering (can be abstracted as knowledge graph ) (Huang et al., 2019; Zhang et al., 2018), node classification (Bhagat et al., 2011; Rong et al., 2019) and graph classification (Errica et al., 2019), etc. These tasks have distinct requirements and challenges.</p>
<p>Previous researches have investigated the use of LLMs for structural understanding (Sui et al., 2023; Jiang et al., 2023; Gong et al., 2020; Liu et al., 2022), but the emphasis has been predominantly on tables, which rely heavily on structured tabular data. Graphs, on the other hand, introduce additional dimensions of complexity. Comprised of nodes that represent entities or concepts, and edges that express relationships between these entities, graphs necessitate a more sophisticated level of comprehension from LLMs. Understanding graph structued data with LLM remains challenges. First of all, graph data can not be directly handled by LLM, as graph data are unorganized and complex. Secondly, there is a wide range of graph-related tasks, designing efficient input format for different tasks and effective prompt techniques is essential while rarely explored.</p>
<p>In this paper, our goal is to setup a comprehensive comparison to show the ability of LLM in understanding graph structured data. To achieve this goal, we first bridge the existing gap between Large Language Models (LLMs) by proposing a novel framework that integrates LLMs and graphstructured data, intending to enhance their synergistic ability across a wide range of graph mining tasks. Based on the framework, we establish a benchmark across ten common scenarios</p>
<p>to assess language models' capability in handling graph-related tasks. In addition, we experiment with various prompting methods, including both handcrafted and self-generated prompts, to demonstrate their effectiveness in boosting performance in both zero-shot and few-shot settings. Our findings reveal that while LLMs have demonstrated some capability in handling graph-structured data, there remains a substantial need for further development to achieve a performance level comparable to specialized graph-oriented models. In summary, our contribution can be summarized by:</p>
<ul>
<li>We introduce a new framework that combines Large Language Models (LLMs) and graphstructured data. This setup uses the language understanding skills of LLMs and graph description language with promt engineering to improve how they work together in different situations.</li>
<li>We develope a wide-ranging set of tasks, across ten common scenarios, to check how well LLMs can handle tasks involving graph data. This set of taks provides a consistent way to check how good language models are at dealing with complex graph data.</li>
<li>Our empirical results show that, while LLMs are getting better at handling graph data, they still have a lot of improving to do if they are to catch up with models that are specifically designed to work with graphs.</li>
</ul>
<h2>2 Preliminary</h2>
<h3>2.1 Graph Mining Tasks</h3>
<p>Graph mining tasks refer to the process of extracting valuable and actionable insights from graphstructured data. Graphs are mathematical structures that represent relationships between entities, where nodes represent entities and edges represent the connections or interactions between them. Graph mining involves analyzing these graphs to discover patterns, relationships, communities, and other useful information. Some graph mining tasks include node classification, link prediction, and community detection. These tasks are crucial in various domains, including social network analysis (Wasserman and Faust, 1994), bioinformatics (Baxevanis et al., 2020), recommendation systems (Isinkaye et al., 2015), fraud detection (Bolton and Hand, 2002), and knowledge graphs (Ji et al., 2021).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graph Understanding with LLM Framework. The graph data is first converted to graph description language that can be understand by LLM. Then the prompt handler combines user query and GDL with potential multiple rounds to generate the answer.</p>
<h3>2.2 Graph Description Language</h3>
<p>A graph description language is a formal language or notation used to define or represent graphstructured data. It provides a standardized syntax and semantics for describing the elements and relationships within a graph. Graph description languages enable the creation, manipulation, and interpretation of graphs in a consistent and machinereadable manner. These languages provide a way to define graph structures, specify node and edge properties, and perform queries and operations on graphs. They are essential for working with graph data and enabling interoperability between graphbased systems and tools. For example, graphs can be represented by an edge list or an adjacency list, providing two distinct perspectives on the graph's structure. An edge list defines a graph in terms of its individual connections, whereas an adjacency list describes each node in terms of its neighboring nodes. Along with these basic representations, more sophisticated formats have been developed to convey richer, contextual information about the graph. For instance, the Graph Modelling Language (GML)(Himsolt, 1997) and Graph Markup Language (GraphML)(Brandes et al., 2013) provide extensible, language-based frameworks for graph representation.</p>
<h2>3 Graph Understanding with LLM Pipeline</h2>
<p>The overall pipline of graph understanding with LLMs is illustrated in Figure 1. Where for graph data, we first generate their graph description languages (GDL), and then use prompt handler to combine the user query and GDL to form the input to the LLMs. The LLMs performs reasoning and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of Self-prompting. The first request is to ask LLMs to automatically generate the context of the input graph (w/ or w/o respect to the question). We may ask LLMs multiple context related questions. After generating the new context (such as context summarization and format explanation), the new context is combined with the original input and are sent to the LLMs to generate the final output.</p>
<p>generate answers to the user. During the reasoning, the LLMs may generate intermedium output that should be handled by prompt handler to form new input to the LLMs. Here we elaborate the prompt handler to show how to make LLM better understand graph data.</p>
<h3>3.1 Manual Prompting</h3>
<p>Manual prompting for graph-based problems involves utilizing familiar graph representations to prompt a large language model (LLM) for desired outputs. The novelty of this approach lies in the fact that it requires a shift from traditional text-based inputs to graphical representations. These graph formats have been discussed in Section 2.2. By employing these graph formats as input, we can provide more comprehensive and context-rich information about the graph to the LLM. Other manual prompting techniques include adding format explanation to make LLM better understand the format and adding role prompting to make LLM better understand the specific task. Besides, we can also change the input order between question and external input, and adding examples to utilize the in-context learning ability [wei2021gpt] of LLM.</p>
<p>Nonetheless, some recent developed change-of-thoughts promptings [kojima2022gpt; yao2023gpt] can also be applied to enhance the reasoning ability of LLM for there are many tasks requiring multiple step of reasoning (e.g., clustering coefficient computing).</p>
<h3>3.2 Self-Prompting</h3>
<p>Sometimes the given graph context contains less useful or redundant information for solving tasks. Thus we need LLM to perform self-prompting to obtain more context or eliminating irrelevant information from the given input. It can be challenging for LLM to generate effective prompts for graph-based tasks, as graphs have complex structures and relationships that need to be accurately captured in the prompt. However, there are several strategies that can be employed for self-prompting in graph-based tasks.</p>
<p><strong>Context Summarization</strong>: LLM can generate a summary of the given graph by extracting key features, such as important nodes, edges, or subgraphs. The generated summary can serve as a prompt for the subsequent graph-related questions or tasks. Besides, based on some important elements like nodes and edges, we can use LLM to summarize their context (neighborhood) information to form neighborhood aware text features.</p>
<p><strong>Format Explanation</strong>: Sometimes it is hard for a human to give the entire description of the input graph format. To make the LLM gain more context information of the input graph, we can make the LLM to generate format explanation by itself.</p>
<p>By leveraging these self-prompting strategies, LLM can actively engage in the understanding and manipulation of graphs, facilitating graph-based reasoning and learning.</p>
<h1>4 Graph Understanding Benchmark</h1>
<h3>4.1 Structure Understanding Tasks</h3>
<p><strong>Graph Size Detection.</strong> This task evaluates a large language model's (LLM) capability to discern the size of a provided graph. In this context, size refers to the count of nodes and edges present in the graph. The LLM is expected to accurately determine these metrics, even when user-provided designs and accompanying data, such as descriptions, statements, or queries, augment the graph. Despite the inherent</p>
<p>challenge this poses for language models, a precise count of nodes and edges is critical, as it enables the LLM to contextualize information accordingly. Degree Detection. This task investigates the LLM's aptitude for understanding a node's contextual relevance within a graph. Here, the degree of a node-an indicator of a node's importance and the sparsity of its connections-forms the crux of the task. The LLM must ascertain the number of neighbors for a given node, based on the graph text and any supplementary information. The degree of a node is foundational for various centrality measures such as degree centrality and clustering coefficient, underscoring the task's importance in understanding a node's local structure.</p>
<p>Edge Detection. Building on degree detection, this task further explores the LLM's understanding of a node's local structure. The model must identify the neighboring nodes of a given node, a skill that is vital for complex graph mining activities like calculating distances and discerning connectivity patterns. Mastery of this task signifies the LLM's comprehension of the fundamental aspects necessary for advanced graph analysis.</p>
<p>Attribute Retrieval. This task tests the LLM's capacity to retrieve pertinent details about a node, such as the node's attributes, which play a key role in defining its characteristics. For instance, the LLM might need to retrieve a specific attribute such as a paper's title or an author's gender. Success in this task highlights the LLM's ability to comprehend and retrieve essential node-related information.</p>
<p>Diameter Computing. This task challenges the LLM to calculate the diameter of a graph. The diameter, which is the longest shortest path between any two nodes, offers valuable insights into the graph's overall connectivity and reachability. A successful computation of the diameter showcases the LLM's grasp of the graph's structure and its ability to analyze the graph's overarching characteristics.</p>
<p>Clustering Coefficient Computing. In this task, the LLM needs to compute the clustering coefficient of a graph, a measure that indicates how closely nodes in a graph tend to cluster together. The task thereby provides a means to assess the LLM's understanding of local connectivity patterns
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Structure Understanding Tasks
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Semantic Understanding Tasks
and its ability to evaluate the degree of clustering within a graph. Besides, it tests the ability of reasoning of LLM for computing CC has several steps.</p>
<h3>4.2 Semantic Understanding Tasks</h3>
<p>Knowledge Graph Question Answering. This task gauges the LLM's proficiency in answering questions that pertain to a knowledge graph. Knowledge graphs organize data into a structured format, embodying entities, attributes, and relationships. Task success hinges on the LLM's ability to reason and understand the underlying graph structure to provide accurate answers, thus demonstrating its semantic understanding and capability to navigate and extract information from a KG.</p>
<p>Graph Query Language Generation. This task measures the LLM's capability to generate graph query languages that satisfy user requirements. These languages, including GQL and Cypher, allow users to extract specific information from a graph database. By generating appropriate graph</p>
<p>queries in response to user information needs, the LLM showcases its comprehension of user intent and precision in query formulation.</p>
<p>Node Classification. This task requires the LLM to classify nodes within a graph based on their attributes or structural features. The LLM is given labeled node examples and their associated classes, and it must correctly predict the class of unseen nodes by applying learned patterns from the labeled data. Success in node classification showcases the LLM's ability to generalize from examples and apply its understanding of node attributes and structure to classify new nodes accurately.</p>
<p>Graph Classification. This task extends the scope of node classification to encompass entire graphs. The LLM is given graphs, each labeled with specific categories or classes, and is expected to accurately classify unseen graphs by using patterns learned from the labeled examples. This task evaluates the LLM's ability to understand and apply the structural and attribute-based characteristics of a graph holistically, thus enabling accurate classification of new graphs.</p>
<h2>5 Data Collection</h2>
<h3>5.1 Structure Understanding Task</h3>
<p>To demonstrate the capabilities of language models in reasoning over Structure Understanding Tasks, we selected two well-known citation networks: obgn-arxiv (Hu et al., 2020) and Aminer (Tang et al., 2008). Our approach involved randomly sampling 100 initial seed nodes from each graph and applying a Depth-First Search (DFS) algorithm to sample 2-hop subgraphs centered around these nodes. Each subgraph consisted of approximately 10-20 nodes and 40 edges. To evaluate the performance of the language model, we assigned it the following tasks within these subgraphs: degree detection, attribute retrieval, clustering, size detection, and diameter estimation. For the first three tasks, the model provided results for each individual node in the subgraphs. However, for size detection and diameter estimation, we computed the results for each entire subgraph. Another task we tackled was Edge Detection. Here, we treated each edge in the graph as a positive sample and randomly selected an edge not present in the graph as a negative sample. We then asked the language model to determine whether a given edge belonged to the subgraph or not, based on the information provided by the subgraph.</p>
<h3>5.2 Semantic Understanding Task</h3>
<p>Shifting our focus to semantic understanding tasks, we conducted knowledge graph question answering using two widely-used datasets: Wiki, a temporal knowledge graph, and MetaQA, a multi-hop movie knowledge base. These datasets served as a testing ground to evaluate the performance of the language model in these domains. For node classification, we leveraged the original labels available in the ogbn-arxiv dataset. We randomly sampled 100 nodes from the test set and tasked the language model with predicting their labels based on information such as the node's title, abstract, and the text information from its k-hop neighbors. In addition, we explored graph query language generation using the MetaQA dataset. We constructed a graph database from this dataset and prompted the language model to generate corresponding graph query languages (GQL) like Cypher. The generated GQL statements were then executed using the Neo4j engine. Through these experiments, we aime to assess the language model's performance in various tasks related to structural and semantic understanding in graph structured data.</p>
<h2>6 Experiments</h2>
<h3>6.1 Experimental Setup</h3>
<h2>Downstream Task.</h2>
<p>Models. We evaluate the performance of the recent dominant LLM model, InstructGPT-3 (Ouyang et al., 2022), using versions text-davinci-001, text-davinci-002, and text-davinci003. Unless otherwise specified, we utilize text-davinci-003 in all experiments. The temperature is set to 0.3 to control the variety of the output.</p>
<h3>6.2 Results for Structure Understanding Task</h3>
<p>The results for the Structure Understanding Task are presented in Table 1, revealing several significant findings:
Input Design Has a Significant Impact on the Final Result. Our experiments demonstrate that the design of the input plays a crucial role in determining the performance of the model. By carefully considering the arrangement and organization of the input data, we can substantially influence the model's ability to understand the structural aspects of the task at hand. Fine-tuning the input design can</p>
<p>Table 1: Experiments on Graph Structural Understanding on OGBN-ARXIV. ACC indicates average accuracy over samples, while $\Delta$ indicates the difference of variants with the 1-shot setting. - denotes that the input format do not contain corresponding information.</p>
<table>
<thead>
<tr>
<th>Format</th>
<th>Input Design</th>
<th>Size Detection</th>
<th></th>
<th>Degree Detection</th>
<th></th>
<th>Edge Detection</th>
<th></th>
<th>Attribute Retrieval</th>
<th></th>
<th>Diameter</th>
<th></th>
<th>Clustering</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
<td>ACC</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Adjacency List</td>
<td>1-shot</td>
<td>35.50</td>
<td>0.00</td>
<td>15.21</td>
<td>0.00</td>
<td>65.45</td>
<td>0.00</td>
<td>-</td>
<td>-</td>
<td>28.00</td>
<td>0.00</td>
<td>5.42</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>44.00</td>
<td>+8.50</td>
<td>14.58</td>
<td>-0.63</td>
<td>65.25</td>
<td>-0.20</td>
<td>-</td>
<td>-</td>
<td>24.00</td>
<td>-4.00</td>
<td>1.85</td>
<td>-3.57</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>33.00</td>
<td>-0.25</td>
<td>16.34</td>
<td>+1.13</td>
<td>57.50</td>
<td>-8.25</td>
<td>-</td>
<td>-</td>
<td>18.00</td>
<td>-10.00</td>
<td>5.19</td>
<td>+3.43</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>36.60</td>
<td>+1.10</td>
<td>15.70</td>
<td>+0.49</td>
<td>55.00</td>
<td>-10.45</td>
<td>-</td>
<td>-</td>
<td>20.00</td>
<td>-8.00</td>
<td>4.71</td>
<td>-0.23</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>14.00</td>
<td>-21.50</td>
<td>26.28</td>
<td>+11.07</td>
<td>51.20</td>
<td>-14.25</td>
<td>-</td>
<td>-</td>
<td>30.00</td>
<td>+2.00</td>
<td>14.92</td>
<td>-9.50</td>
</tr>
<tr>
<td></td>
<td>w/o 1-shot</td>
<td>33.00</td>
<td>-2.50</td>
<td>17.18</td>
<td>+1.97</td>
<td>71.90</td>
<td>-6.45</td>
<td>-</td>
<td>-</td>
<td>22.00</td>
<td>-6.00</td>
<td>7.85</td>
<td>+2.43</td>
</tr>
<tr>
<td>Edge List</td>
<td>1-shot</td>
<td>22.50</td>
<td>0.00</td>
<td>44.87</td>
<td>0.00</td>
<td>74.60</td>
<td>0.00</td>
<td>-</td>
<td>-</td>
<td>43.00</td>
<td>0.00</td>
<td>13.31</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>27.00</td>
<td>+4.50</td>
<td>48.65</td>
<td>+3.78</td>
<td>74.70</td>
<td>+0.10</td>
<td>-</td>
<td>-</td>
<td>41.00</td>
<td>-2.00</td>
<td>11.33</td>
<td>-1.98</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>25.00</td>
<td>+2.50</td>
<td>47.86</td>
<td>+2.99</td>
<td>71.55</td>
<td>-3.05</td>
<td>-</td>
<td>-</td>
<td>36.00</td>
<td>-7.00</td>
<td>18.11</td>
<td>+4.80</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>18.00</td>
<td>-4.50</td>
<td>47.64</td>
<td>+2.57</td>
<td>71.70</td>
<td>-2.90</td>
<td>-</td>
<td>-</td>
<td>39.00</td>
<td>-4.00</td>
<td>13.63</td>
<td>+0.35</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>9.00</td>
<td>-13.50</td>
<td>20.48</td>
<td>-23.39</td>
<td>79.60</td>
<td>+5.00</td>
<td>-</td>
<td>-</td>
<td>10.00</td>
<td>-33.00</td>
<td>20.06</td>
<td>+ 7.05</td>
</tr>
<tr>
<td></td>
<td>w/o 1-shot</td>
<td>23.00</td>
<td>+0.50</td>
<td>49.34</td>
<td>+4.47</td>
<td>80.95</td>
<td>+6.35</td>
<td>-</td>
<td>-</td>
<td>34.00</td>
<td>-9.00</td>
<td>19.16</td>
<td>+5.84</td>
</tr>
<tr>
<td>GML</td>
<td>1-shot</td>
<td>54.50</td>
<td>0.00</td>
<td>20.91</td>
<td>0.00</td>
<td>50.45</td>
<td>0.00</td>
<td>83.40</td>
<td>0.00</td>
<td>37.00</td>
<td>0.00</td>
<td>4.36</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>55.50</td>
<td>+1.00</td>
<td>20.76</td>
<td>-0.15</td>
<td>50.10</td>
<td>-0.35</td>
<td>83.30</td>
<td>-0.10</td>
<td>28.00</td>
<td>-9.00</td>
<td>0.95</td>
<td>-3.41</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>55.00</td>
<td>-0.50</td>
<td>29.06</td>
<td>+8.15</td>
<td>50.00</td>
<td>-0.45</td>
<td>85.97</td>
<td>+2.57</td>
<td>41.00</td>
<td>+4.00</td>
<td>12.71</td>
<td>+8.35</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>54.50</td>
<td>-0.50</td>
<td>29.79</td>
<td>+8.88</td>
<td>50.00</td>
<td>-0.45</td>
<td>84.50</td>
<td>+0.10</td>
<td>35.00</td>
<td>-2.00</td>
<td>6.96</td>
<td>+2.60</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>51.50</td>
<td>-3.00</td>
<td>21.16</td>
<td>+0.24</td>
<td>55.65</td>
<td>+5.20</td>
<td>83.56</td>
<td>+0.16</td>
<td>39.00</td>
<td>+2.00</td>
<td>5.25</td>
<td>+0.89</td>
</tr>
<tr>
<td></td>
<td>w/o 1-shot</td>
<td>54.00</td>
<td>-0.50</td>
<td>19.85</td>
<td>-1.06</td>
<td>50.25</td>
<td>+0.20</td>
<td>83.22</td>
<td>-0.18</td>
<td>42.00</td>
<td>+5.00</td>
<td>5.39</td>
<td>+1.03</td>
</tr>
<tr>
<td>GraphML</td>
<td>1-shot</td>
<td>25.00</td>
<td>0.00</td>
<td>40.20</td>
<td>0.00</td>
<td>62.05</td>
<td>0.00</td>
<td>83.87</td>
<td>0.00</td>
<td>34.00</td>
<td>0.00</td>
<td>9.74</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>1-shot-cot</td>
<td>22.50</td>
<td>-2.50</td>
<td>40.02</td>
<td>-0.18</td>
<td>62.30</td>
<td>+0.25</td>
<td>83.75</td>
<td>-0.12</td>
<td>32.00</td>
<td>-2.00</td>
<td>7.29</td>
<td>-2.45</td>
</tr>
<tr>
<td></td>
<td>w/o format explanation</td>
<td>19.00</td>
<td>-6.00</td>
<td>46.90</td>
<td>+5.88</td>
<td>53.75</td>
<td>-8.40</td>
<td>85.37</td>
<td>+1.50</td>
<td>38.00</td>
<td>+4.00</td>
<td>22.75</td>
<td>+13.01</td>
</tr>
<tr>
<td></td>
<td>w/o role prompting</td>
<td>15.50</td>
<td>-9.50</td>
<td>49.89</td>
<td>+9.87</td>
<td>56.10</td>
<td>-5.95</td>
<td>87.63</td>
<td>+3.76</td>
<td>31.00</td>
<td>-3.00</td>
<td>14.52</td>
<td>+4.78</td>
</tr>
<tr>
<td></td>
<td>w/o change order</td>
<td>8.50</td>
<td>-16.50</td>
<td>30.60</td>
<td>-9.60</td>
<td>65.35</td>
<td>+3.30</td>
<td>9.76</td>
<td>-4.11</td>
<td>43.00</td>
<td>+9.00</td>
<td>8.00</td>
<td>-1.74</td>
</tr>
<tr>
<td></td>
<td>0-shot</td>
<td>24.50</td>
<td>-0.50</td>
<td>39.59</td>
<td>-0.61</td>
<td>73.95</td>
<td>+11.90</td>
<td>82.90</td>
<td>-0.97</td>
<td>30.00</td>
<td>-4.00</td>
<td>14.32</td>
<td>+4.58</td>
</tr>
</tbody>
</table>
<p>lead to improved performance and more accurate structural understanding.</p>
<p>Role Prompting Generally Improves Performance. Our findings indicate that incorporating role-prompting techniques generally enhances the model's performance in the Structure Understanding Task. By explicitly guiding the model to focus on specific roles or relationships within the graph, we enable it to extract more meaningful insights and make more accurate predictions. Role prompting serves as an effective mechanism for capturing the nuances of the graph's structure and leveraging that information for improved understanding.</p>
<p>Examples Have Impacts on Graph Understanding. Similar to previous research that suggests the utility of examples in large language models (LLMs), we discovered that examples also have some extend of positive effects in graph understanding scenarios. However, omitting specific examples and relying on zero-shot learning approaches sometimes yielded more powerful results. This phenomenon can be attributed to the rich inherent information present within the graph itself, which allows the model to grasp the complexities of the structure without the need for explicit examples. Examples, in some cases, can introduce noise, biases, or incomplete information, hindering the model's overall understanding.</p>
<p>The Position of External Knowledge Matters.</p>
<p>We investigated the impact of external knowledge, such as questions, statements, and examples, on graph understanding. Comparing the placement of external knowledge before or after the graph input, we observed that positioning external knowledge before the graph generally led to better performance. Placing external knowledge before the graph provides additional context information, enabling the model to better comprehend the specific graph it needs to handle. Conversely, positioning the graph behind external knowledge may hinder the model's ability to effectively utilize the relevant information, potentially degrading performance.</p>
<p>These findings show the importance of thoughtful input design, the potential benefits of role prompting techniques, the limited impact of examples in graph understanding, and the significance of positioning external knowledge for optimal performance. Understanding these factors can guide future research and inform the development of more effective models for structure understanding tasks.</p>
<h3>6.3 Results for Semantic Understanding Task</h3>
<p>The resuls for semantic understanding tasks are shown in Figure 2 and Figure 3. We have the following discoveries:
Resuts for KGQA and GQL generation. The results for KGQA and GQL generation is shown in Table 2. It's noticeable that current SOTA models consistently show higher performance across</p>
<p>Table 2: Performance on KGQA and GQL Generation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Wiki</th>
<th style="text-align: center;">MetaQA-1hop</th>
<th style="text-align: center;">MetaQA-2hop</th>
<th style="text-align: center;">MetaQA-3hop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;">64.70</td>
<td style="text-align: center;">97.50</td>
<td style="text-align: center;">$\mathbf{9 8 . 8 0}$</td>
<td style="text-align: center;">94.80</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">24.75</td>
<td style="text-align: center;">6.37</td>
<td style="text-align: center;">9.72</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot</td>
<td style="text-align: center;">8.71</td>
<td style="text-align: center;">18.41</td>
<td style="text-align: center;">12.86</td>
<td style="text-align: center;">21.89</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot+graph</td>
<td style="text-align: center;">56.38</td>
<td style="text-align: center;">91.69</td>
<td style="text-align: center;">46.82</td>
<td style="text-align: center;">19.40</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot+graph</td>
<td style="text-align: center;">55.63</td>
<td style="text-align: center;">86.16</td>
<td style="text-align: center;">47.36</td>
<td style="text-align: center;">19.29</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot+graph+change-order</td>
<td style="text-align: center;">51.35</td>
<td style="text-align: center;">95.20</td>
<td style="text-align: center;">40.48</td>
<td style="text-align: center;">20.17</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot+graph+change-order</td>
<td style="text-align: center;">56.33</td>
<td style="text-align: center;">95.87</td>
<td style="text-align: center;">47.71</td>
<td style="text-align: center;">23.95</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot Cypher Generation</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">13.00</td>
</tr>
<tr>
<td style="text-align: center;">one-shot Cypher Generation</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{9 9 . 0 0}$</td>
<td style="text-align: center;">77.00</td>
<td style="text-align: center;">$\mathbf{9 6 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>all datasets, with scores ranging from 94.80 on MetaQA-3hop to 98.80 on MetaQA-2hop. However, LLM showed comparative performance on certain tasks with prompt strategies. Specifically, the 'zero-shot+graph' method has performed exceptionally well on the 'Wiki' dataset, achieving an accuracy of 56.38 , the highest among our proposed models. Similarly, the 'zero-shotcot+graph+change-order' model performs the best on MetaQA-1hop, scoring 95.87. When we compare zero-shot models with 'zero-shot-cot' counterparts, we observe a general trend that the inclusion of the graph ('+graph') and change order ('+change-order') enhancements improve the model performance. For the 'one-shot Cypher' method, an impressive performance of 99.00 is achieved on the MetaQA-1hop, surpassing the state-of-the-art and all other models in our study.</p>
<p>Results for Node Classification. For Node Classification on OGBN-ARXIV (Table 3), the 'oneshot + 1-hop neighborhood context summarization' model has the highest accuracy of 60.00 among all the variants. Interestingly, models augmented with 2-hop neighborhood context summarization ('2-hop') show better performance than their 1hop counterparts, showing that expanding context range is helpful in providing valuable information. Also, the model performs better than the change-ofthought (cot) model, suggesting that the cot strategy might not be as effective for this task. These results indicate potential areas for improvement, particularly for the 'zero-shot-cot' and 'change-order' strategies, which don't consistently improve performance. Nonetheless, the experiments provide valuable insights into the performance of different strategies in the node classification task.</p>
<p>Table 3: Performance of Node Classification on OGBNARXIV. self denotes only the use of the text feature of the target nodes. 1-hop denotes using the text feature of direct neighbors. 2-hop denotes using the text feature within 2-hop neighbors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Context</th>
<th style="text-align: center;">ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">48.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">53.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">57.00</td>
</tr>
<tr>
<td style="text-align: center;">zero-shot-cot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">40.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">40.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">56.00</td>
</tr>
<tr>
<td style="text-align: center;">one-shot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">54.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">60.00</td>
</tr>
<tr>
<td style="text-align: center;">one-shot-cot</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;">43.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">55.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">59.00</td>
</tr>
</tbody>
</table>
<p>Results for Graph Classification. The results for graph classification task is shown in Table 4. From the result, we find that the self-augmentation is effective in improving the performance of GC. It shows that self-augmentation like self-format explanation and self-summarization can enrich the context of the original graph and will make the LLM more easily complete the task.</p>
<h2>7 Discussion</h2>
<p>Our findings suggest several promising directions for future work in structure understanding tasks with LLMs. First, more research is needed to understand how different input designs and role prompting techniques can further enhance performance. Second, we encourage researchers to in-</p>
<p>Table 4: Performance on Graph Classification</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>OGBG-MOLHIV</th>
<th></th>
<th>OGBG-MOLPCBA</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GML</td>
<td>GraphML</td>
<td>GML</td>
<td>GraphML</td>
</tr>
<tr>
<td>1-shot-tot</td>
<td>66.87</td>
<td>63.25</td>
<td>57.18</td>
<td>57.45</td>
</tr>
<tr>
<td>1-shot-cot</td>
<td>67.65</td>
<td>64.71</td>
<td>59.26</td>
<td>57.32</td>
</tr>
<tr>
<td>w/o self-format explanation</td>
<td>64.71</td>
<td>64.71</td>
<td>58.73</td>
<td>56.24</td>
</tr>
<tr>
<td>w/o self-summarization</td>
<td>61.76</td>
<td>61.77</td>
<td>57.64</td>
<td>56.67</td>
</tr>
<tr>
<td>0-shot-cot</td>
<td>58.82</td>
<td>59.76</td>
<td>55.57</td>
<td>55.32</td>
</tr>
</tbody>
</table>
<p>vestigate why examples are less effective for graph understanding and to explore alternative strategies for leveraging the rich information embedded in graphs. Third, the role of external knowledge placement merits further exploration. Finally, new approaches for graph augmentation could be developed to improve performance on semantic understanding tasks.</p>
<p>In addition, our experiments have revealed the potential of LLMs in various tasks beyond pure natural language processing. We believe that more effort should be dedicated to integrating graph-based information into LLMs, exploring different types of graph structures, and applying LLMs to other areas such as graph theory, network science, and complex systems. In the future, we may also consider using LLM to control the use of external tools to better handle graph structured data <em>Schick et al. (2023); Zhang (2023)</em>.</p>
<h2>8 Related Works</h2>
<h3>8.1 Language Model for Structural Data Understanding</h3>
<p>Language models are being extended to understand and work with structural data, such as graphs, tables, and trees. One approach is using graph neural networks to encode structural information, capturing dependencies and relationships between elements <em>Qasim et al. (2019)</em>. Incorporating GNNs into language models enables them to generate contextually aware outputs that consider the structural characteristics of the data. Another approach is incorporating attention mechanisms into language models for structural data <em>Chen et al. (2022); Eisenschlos et al. (2021)</em>. Attention allows the model to focus on relevant parts, improving understanding of complex dependencies and enhancing performance in tasks like graph completion and table understanding. Language models can also benefit from combining knowledge graph embeddings with textual information, leveraging both textual and structural data to make informed predictions.</p>
<h3>8.2 Graph Machine Learning</h3>
<p>Graph machine learning develops models and algorithms for data structured as graphs, representing complex relationships in various domains. Traditional machine learning struggles with graph-structured data, but graph machine learning methods utilize the graph structure to extract meaningful features and make predictions. Graph convolutional (GCN) networks extend convolutional neural networks to operate on graph-structured data, capturing local and global structural patterns and excelling in tasks like node classification and graph-level classification <em>Kipf and Welling (2016)</em>. Graph attention networks (GAT) incorporate attention mechanisms, allowing adaptive aggregation of information from relevant nodes <em>Velickovic et al. (2017)</em>. GAT perform well in tasks like node classification and graph-level representation learning. Graph generative models generate new graphs to capture the structural characteristics and properties of the input data, benefiting tasks like molecule generation <em>Walters and Barzilay (2020)</em> and graph-based data augmentation <em>Zhao et al. (2021)</em>. Graph machine learning techniques enable effective analysis and extraction of insights from graph-structured data, advancing fields relying on understanding complex relationships and dependencies.</p>
<h2>9 Conclusion</h2>
<p>In this work, we analyze the ability of large language models to understand graph-structured data. Our findings indicate that there is still a long way for a LLM to understand graph data. Future research should focus on developing and refining methods for encoding graph-structured information into a format that a large language model can comprehend and manipulate effectively. This is a</p>
<p>complex challenge given the inherent differences between sequential text data and graph data, which is intrinsically multi-dimensional and relational.</p>
<h2>References</h2>
<p>Marc Barthelemy. 2004. Betweenness centrality in large complex networks. The European physical journal B, 38(2):163-168.</p>
<p>Andreas D Baxevanis, Gary D Bader, and David S Wishart. 2020. Bioinformatics. John Wiley \&amp; Sons.</p>
<p>Smriti Bhagat, Graham Cormode, and S Muthukrishnan. 2011. Node classification in social networks. arXiv preprint arXiv:1101.3291.</p>
<p>Richard J Bolton and David J Hand. 2002. Statistical fraud detection: A review. Statistical science, 17(3):235-255.</p>
<p>Ulrik Brandes. 2001. A faster algorithm for betweenness centrality. Journal of mathematical sociology, 25(2):163-177.</p>
<p>Ulrik Brandes, Markus Eiglsperger, Jürgen Lerner, and Christian Pich. 2013. Graph markup language (graphml).</p>
<p>Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou Jingbo, Dejing Dou, and Hui Xiong. 2022. Towards table-to-text generation with pretrained language model: A table structure understanding and text deliberating approach. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8199-8210, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Fan RK Chung, Vance Faber, and Thomas A Manteuffel. 1994. An upper bound on the diameter of a graph from eigenvalues associated with its laplacian. SIAM Journal on Discrete Mathematics, 7(3):443-457.</p>
<p>Julian Martin Eisenschlos, Maharshi Gor, Thomas Müller, and William W Cohen. 2021. Mate: multiview attention for table transformer efficiency. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing".</p>
<p>Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. 2019. A fair comparison of graph neural networks for graph classification. arXiv preprint arXiv:1912.09893.</p>
<p>Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020. Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1978-1988.</p>
<p>Michael Himsolt. 1997. Gml: Graph modelling language. University of Passau.</p>
<p>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133.</p>
<p>Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 105-113.</p>
<p>Folasade Olubusola Isinkaye, Yetunde O Folajimi, and Bolande Adefowoke Ojokoh. 2015. Recommendation systems: Principles, methods and evaluation. Egyptian informatics journal, 16(3):261-273.</p>
<p>Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494-514.</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645.</p>
<p>Thomas N Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, and Dongmei Zhang. 2022. PLOG: Table-to-logic pretraining for logical table-to-text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 55315546, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Mark EJ Newman. 2005. A measure of betweenness centrality based on random walks. Social networks, 27(1):39-54.</p>
<p>Kazuya Okamoto, Wei Chen, and Xiang-Yang Li. 2008. Ranking of closeness centrality for large-scale social networks. Lecture Notes in Computer Science, 5059:186-195.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait. 2019. Rethinking table recognition using</p>
<p>graph neural networks. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 142-147. IEEE.</p>
<p>Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. Dropedge: Towards deep graph convolutional networks on node classification. arXiv preprint arXiv:1907.10903.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2023. Evaluating and enhancing structural understanding capabilities of large language models on tables via input designs. arXiv preprint arXiv:2305.13062.</p>
<p>Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990998 .</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. 2017. Graph attention networks. stat, 1050(20):1048550 .</p>
<p>W Patrick Walters and Regina Barzilay. 2020. Applications of deep learning in molecule generation and molecular property prediction. Accounts of chemical research, 54(2):263-270.</p>
<p>Stanley Wasserman and Katherine Faust. 1994. Social network analysis: Methods and applications.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Jiawei Zhang. 2023. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116.</p>
<p>Junlong Zhang and Yu Luo. 2017. Degree centrality, betweenness centrality, and closeness centrality in social network. In 2017 2nd international conference on modelling, simulation and applied mathematics (MSAM2017), pages 300-303. Atlantis press.</p>
<p>Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander Smola, and Le Song. 2018. Variational reasoning for question answering with knowledge graph. In Proceedings of the AAAI conference on artificial intelligence, volume 32.</p>
<p>Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. 2021. Data augmentation for graph neural networks. In Proceedings of the aaai conference on artificial intelligence, volume 35, pages 11015-11023.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<h2>A Detailed Description of Datasets</h2>
<h2>A. 1 OGBN-ARXIV</h2>
<p>The Open Graph Benchmark (OGB) is a collection of diverse, large-scale, and challenging datasets and benchmarking tasks for graph machine learning research. OGBN-ARXIV is a part of the OGB Node Property Prediction track. The dataset comprises academic papers from the arXiv website, which are represented as nodes in a citation graph. In the graph, the edges denote the citation relationships between the papers. Each paper is associated with a 128-dimensional word2vec feature vector derived from its title and abstract. The task associated with this dataset is to predict the subject area of each paper, making it a multi-class classification problem. We sample a subset of 100 nodes with multi-hop neighbors for testing.</p>
<h2>A. 2 OGBG-MOLX</h2>
<p>OGBG-MOLX is part of the Graph Property Prediction track in OGB and it comprises of two datasets: MOLHIV and MOLPCBA. MOLHIV dataset contains molecular graphs where the task is to predict whether a molecule inhibits HIV virus replication or not, making it a binary classification problem. On the other hand, MOLPCBA dataset contains molecular graphs with the task of predicting bioactivity for various protein targets, which is a multi-label classification problem. In these datasets, nodes represent atoms and edges represent bonds between atoms. Node and edge features include atom type, atom degree, bond type, and whether the bond is in a ring. We sample 100 graphs with the same number of positive and negative samples for testing.</p>
<h2>A. 3 Wiki</h2>
<p>The Wiki dataset is a well-known dataset that contains text from a collection of Wikipedia articles. The structure of this dataset varies depending on the particular task. For instance, for text classification tasks, each document (or article) can be represented as a bag-of-words vector, with each dimension representing the frequency of a specific word. The labels may include the categories that the articles belong to. In the context of graph-based tasks, a Wikipedia dataset could consist of a network of articles (as nodes) linked by hyperlinks (as edges), with the task being predicting article categories or link prediction between articles.</p>
<h2>A. 4 MetaQA</h2>
<p>MetaQA is a dataset designed for the task of multihop reasoning in question answering. It consists of movie-related knowledge graph entities, relations, and natural language questions. Each node in the knowledge graph represents a movie entity (such as a movie, actor, or director), and edges represent relationships between entities. The dataset also includes questions at three levels of complexity (1hop, 2-hop, and 3-hop), with each level requiring reasoning over an increasing number of edges in the graph to answer the questions correctly. The goal is to answer natural language questions by effectively reasoning over the underlying knowledge graph.</p>
<h2>B Input Design for Different Tasks</h2>
<p>The input design for different tasks are shown in Table 5, where we show the question designs for different tasks.</p>
<h2>C Cypher Introduction</h2>
<p>Cypher is a declarative graph query language developed by Neo4j, a popular graph database management system. It allows for expressive and efficient querying and updating of graph data. The language is designed to be intuitive and readable, drawing on the use of English prose and iconography. Cypher is built around the concept of pattern matching. It focuses on the clarity of expressing what to retrieve from a graph, not dictating how to retrieve it. This design makes Cypher powerful when working with graph data, as patterns are often more intuitive and easier to understand.</p>
<p>Table 5: Input Design for Different Tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Size Detection</td>
<td style="text-align: left;"><graph text> What is the number <br> of nodes and edges in this graph? <br> Please answer with the number of <br> nodes: X, number of edges: X.</td>
</tr>
<tr>
<td style="text-align: left;">Degree Detection</td>
<td style="text-align: left;"><graph text> What is the degree of <br> node X?</td>
</tr>
<tr>
<td style="text-align: left;">Edge Detection</td>
<td style="text-align: left;"><graph text> Is there an edge be- <br> tween node X1 and node X2?</td>
</tr>
<tr>
<td style="text-align: left;">Attribute Retrieval</td>
<td style="text-align: left;">What is the title of node X?</td>
</tr>
<tr>
<td style="text-align: left;">Diameter</td>
<td style="text-align: left;">What is the diameter of this graph?</td>
</tr>
<tr>
<td style="text-align: left;">Clustering</td>
<td style="text-align: left;">What is the clustering coefficient <br> of node X?</td>
</tr>
<tr>
<td style="text-align: left;">KGQA</td>
<td style="text-align: left;">Knowledge: <graph text>, Ques- <br> tion: <question text></td>
</tr>
<tr>
<td style="text-align: left;">GQL Generation</td>
<td style="text-align: left;">Thus the Neo4j CQL of the ques- <br> tion is</td>
</tr>
<tr>
<td style="text-align: left;">Node Classification</td>
<td style="text-align: left;">Which arxiv CS subcategory does <br> paper <paper title> with abstract <br> <paper abstract> belongs to? use <br> the abbreviation to answer.</td>
</tr>
<tr>
<td style="text-align: left;">Graph Classification</td>
<td style="text-align: left;"><graph text> Whether the <br> molecule inhibits HIV virus <br> replication? Yes or no.</td>
</tr>
</tbody>
</table>
<h2>D Data and Code Release</h2>
<p>The GUC benchmark and codes in this paper will be open sourced at https://anonymous.4open. science/r/GPT4Graph. after an internal review. The synthesized labels in the benchmark will be released under CDLAPermissive-2.0 license. Our code will be released publicly with MIT license.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Work done during the internship at MSRA.
${ }^{1}$ Corresponding Author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4927 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4927</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4927</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-252693237</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.01240v4.pdf" target="_blank">L ANGUAGE M ODELS A RE G REEDY R EASONERS : A S YSTEMATIC F ORMAL A NALYSIS OF C HAIN - OF - T HOUGHT</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called P R O NTO QA, where each example is generated from a synthetic world model represented in ﬁrst-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on I NSTRUCT GPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in ﬁctional contexts. However, they have difﬁculty with proof planning : When multiple valid deduction steps are available, they are not able to systematically explore the different options.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4927.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4927.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INSTRUCTGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>INSTRUCTGPT (OpenAI text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The instruction-tuned GPT-3 variant (referred to in the paper as text-davinci-002 / INSTRUCTGPT) evaluated on a synthetic first-order-logic reasoning benchmark (PRONTOQA) using chain-of-thought prompting; shown to produce mostly locally-valid deduction steps but to struggle with global proof planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (INSTRUCTGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned, decoder-only large language model in the GPT-3 family (OpenAI). Evaluated via few-shot chain-of-thought prompting and analyzed by parsing generated CoTs into formal proof steps; greedy decoding used for main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>PRONTOQA (Proof and Ontology-Generated Question-Answering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A synthetic QA benchmark generated from first-order logical ontologies where each example has a unique proof composed of repeated applications of modus ponens; examples vary by ontology type (true/false/fictional), proof length (hops: 1,3,5), and traversal order (top-down/bottom-up). Chains-of-thought are parsed into symbolic proofs for stepwise evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>8-shot chain-of-thought prompting (CoT) with greedy decoding to elicit natural-language intermediate reasoning steps; predicted CoTs are parsed to logical forms and evaluated for local/global proof correctness. Additional experiments evaluated self-consistency sampling (40 samples, T=0.7) and in-context examples that demonstrate depth-first-search (DFS) traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>text-davinci-002 was the only evaluated model to perform reliably better than chance on PRONTOQA overall (paper focuses analysis on this model). Quantitative findings: in the 5-hop fictional/top-down setting most predicted proof steps were strictly-valid (example: 93.2% of proof steps strictly-valid; 2.4% broadly-valid; 5.9% invalid). Self-consistency experiment (5 hops, fictional, top-down, 100 examples) yielded valid proof accuracy = 0.56 vs baseline 0.545 (not a significant improvement). DFS in-context examples experiment (same setting) gave valid proof accuracy = 0.55 vs 0.545 baseline (not significant). Models handled 1- and 3-hop examples well but accuracy dropped to near chance on 5-hop top-down examples; performance on "true" ontologies remained high and did not decrease with more hops.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Main failure mode: poor proof planning / inability to systematically explore multiple valid deduction options at branching points — the model often takes a strictly-valid but misleading step and then fails to return to the gold proof path. Reliance on pretraining/world knowledge: performance is substantially better on 'true' real-world ontologies, indicating retrieval/priors influence rather than pure systematic reasoning; struggles with longer proofs (5 hops, particularly top-down order), non-modus-ponens rules not evaluated, and semantically more complex sentences. Self-consistency and DFS in-context traces did not produce significant improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to smaller GPT-3 variants evaluated in the paper, text-davinci-002 shows substantially better proof accuracy (only it performs above chance). Smaller models make more invalid or non-atomic steps as their first non-canonical error; larger models make fewer invalid steps but are more prone to strictly-valid misleading steps. Label accuracy correlates poorly with strict proof accuracy but correlates better with permissive proof metrics (skip/valid), suggesting conventional label accuracy can hide planning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Systematic analyses varied model size, ontology type, number of hops, and traversal direction. Observed trends: proof accuracy increases with model size (noted increases from 350M -> 1.3B -> 6.7B in the appendix); 'true' ontologies yield much higher and hop-insensitive performance; top-down traversal order (context sentences reversed relative to gold proof) makes 5-hop problems much harder; most predicted steps are strictly-valid and atomic, but first non-canonical errors are predominantly strictly-valid misleading steps for larger models. Self-consistency sampling and DFS-style in-context examples produced no significant gains in valid proof accuracy in the tested settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE M ODELS A RE G REEDY R EASONERS : A S YSTEMATIC F ORMAL A NALYSIS OF C HAIN - OF - T HOUGHT', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4927.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4927.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 family (ada/babbage/curie/davinci variants)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 family models (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier/smaller GPT-3 variants evaluated on PRONTOQA under identical prompting; used to study the effect of model size and behavior of error types (invalid, non-atomic, misleading) on logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer LLMs from the GPT-3 family (OpenAI API names). Evaluated with the same 8-shot CoT prompting and greedy decoding pipeline; outputs parsed into symbolic proofs and categorized stepwise.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>PRONTOQA (Proof and Ontology-Generated Question-Answering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic first-order-logic QA tasks based on modus ponens proofs; experiments vary hops, ontology truthfulness, and sentence ordering to probe reasoning and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>8-shot chain-of-thought prompting with greedy decoding; predicted CoTs parsed and analyzed with the same formal evaluator as for INSTRUCTGPT. Experiments examine how error types and proof-accuracy change with model size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Smaller GPT-3 variants showed markedly lower proof accuracy; proof accuracy increased with larger model size but only the largest (text-davinci-002) performed reliably above chance. The appendix shows progressive improvement as experiments move from 350M -> 1.3B -> 6.7B models, but overall these variants often failed on longer (5-hop) top-down examples. Error profile: smaller models more frequently produced invalid or non-atomic steps; frequency of invalid steps decreases with size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller GPT-3 models are more likely to produce invalid steps or non-atomic jumps, and generally fail more often on longer or top-down-ordered proofs. All variants suffer from proof-planning issues when multiple valid deduction choices exist; reliance on pretraining confounds behavior on 'true' ontologies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Across this family, larger models make fewer invalid steps and more often produce strictly-valid but sometimes misleading steps; text-davinci-002 (INSTRUCTGPT) outperforms the other named variants. Label accuracy aligns better with permissive proof metrics (skip/valid) than with strict proof accuracy across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The paper's ablation-style analyses include model-size sweeps (reported in Appendix/figure 8) demonstrating monotonic improvement in step validity and proof accuracy with increased size; analysis of first non-canonical errors shows smaller models err via invalid/non-atomic steps while larger models err via strictly-valid misleading steps. Other controlled variables (ontology type, hops, traversal order) were analyzed for each model variant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'L ANGUAGE M ODELS A RE G REEDY R EASONERS : A S YSTEMATIC F ORMAL A NALYSIS OF C HAIN - OF - T HOUGHT', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>FOLIO: natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models still can't plan (A benchmark for llms on planning and reasoning about change) <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 1)</em></li>
                <li>On the paradox of learning to reason from data <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4927",
    "paper_id": "paper-252693237",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "INSTRUCTGPT (text-davinci-002)",
            "name_full": "INSTRUCTGPT (OpenAI text-davinci-002)",
            "brief_description": "The instruction-tuned GPT-3 variant (referred to in the paper as text-davinci-002 / INSTRUCTGPT) evaluated on a synthetic first-order-logic reasoning benchmark (PRONTOQA) using chain-of-thought prompting; shown to produce mostly locally-valid deduction steps but to struggle with global proof planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (INSTRUCTGPT)",
            "model_description": "An instruction-tuned, decoder-only large language model in the GPT-3 family (OpenAI). Evaluated via few-shot chain-of-thought prompting and analyzed by parsing generated CoTs into formal proof steps; greedy decoding used for main experiments.",
            "model_size": null,
            "logical_reasoning_task": "PRONTOQA (Proof and Ontology-Generated Question-Answering)",
            "task_description": "A synthetic QA benchmark generated from first-order logical ontologies where each example has a unique proof composed of repeated applications of modus ponens; examples vary by ontology type (true/false/fictional), proof length (hops: 1,3,5), and traversal order (top-down/bottom-up). Chains-of-thought are parsed into symbolic proofs for stepwise evaluation.",
            "method_or_approach": "8-shot chain-of-thought prompting (CoT) with greedy decoding to elicit natural-language intermediate reasoning steps; predicted CoTs are parsed to logical forms and evaluated for local/global proof correctness. Additional experiments evaluated self-consistency sampling (40 samples, T=0.7) and in-context examples that demonstrate depth-first-search (DFS) traces.",
            "performance": "text-davinci-002 was the only evaluated model to perform reliably better than chance on PRONTOQA overall (paper focuses analysis on this model). Quantitative findings: in the 5-hop fictional/top-down setting most predicted proof steps were strictly-valid (example: 93.2% of proof steps strictly-valid; 2.4% broadly-valid; 5.9% invalid). Self-consistency experiment (5 hops, fictional, top-down, 100 examples) yielded valid proof accuracy = 0.56 vs baseline 0.545 (not a significant improvement). DFS in-context examples experiment (same setting) gave valid proof accuracy = 0.55 vs 0.545 baseline (not significant). Models handled 1- and 3-hop examples well but accuracy dropped to near chance on 5-hop top-down examples; performance on \"true\" ontologies remained high and did not decrease with more hops.",
            "limitations_or_failure_cases": "Main failure mode: poor proof planning / inability to systematically explore multiple valid deduction options at branching points — the model often takes a strictly-valid but misleading step and then fails to return to the gold proof path. Reliance on pretraining/world knowledge: performance is substantially better on 'true' real-world ontologies, indicating retrieval/priors influence rather than pure systematic reasoning; struggles with longer proofs (5 hops, particularly top-down order), non-modus-ponens rules not evaluated, and semantically more complex sentences. Self-consistency and DFS in-context traces did not produce significant improvement.",
            "comparison": "Compared to smaller GPT-3 variants evaluated in the paper, text-davinci-002 shows substantially better proof accuracy (only it performs above chance). Smaller models make more invalid or non-atomic steps as their first non-canonical error; larger models make fewer invalid steps but are more prone to strictly-valid misleading steps. Label accuracy correlates poorly with strict proof accuracy but correlates better with permissive proof metrics (skip/valid), suggesting conventional label accuracy can hide planning failures.",
            "ablation_or_analysis_results": "Systematic analyses varied model size, ontology type, number of hops, and traversal direction. Observed trends: proof accuracy increases with model size (noted increases from 350M -&gt; 1.3B -&gt; 6.7B in the appendix); 'true' ontologies yield much higher and hop-insensitive performance; top-down traversal order (context sentences reversed relative to gold proof) makes 5-hop problems much harder; most predicted steps are strictly-valid and atomic, but first non-canonical errors are predominantly strictly-valid misleading steps for larger models. Self-consistency sampling and DFS-style in-context examples produced no significant gains in valid proof accuracy in the tested settings.",
            "uuid": "e4927.0",
            "source_info": {
                "paper_title": "L ANGUAGE M ODELS A RE G REEDY R EASONERS : A S YSTEMATIC F ORMAL A NALYSIS OF C HAIN - OF - T HOUGHT",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3 family (ada/babbage/curie/davinci variants)",
            "name_full": "GPT-3 family models (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)",
            "brief_description": "Earlier/smaller GPT-3 variants evaluated on PRONTOQA under identical prompting; used to study the effect of model size and behavior of error types (invalid, non-atomic, misleading) on logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 family (text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001)",
            "model_description": "Decoder-only transformer LLMs from the GPT-3 family (OpenAI API names). Evaluated with the same 8-shot CoT prompting and greedy decoding pipeline; outputs parsed into symbolic proofs and categorized stepwise.",
            "model_size": null,
            "logical_reasoning_task": "PRONTOQA (Proof and Ontology-Generated Question-Answering)",
            "task_description": "Synthetic first-order-logic QA tasks based on modus ponens proofs; experiments vary hops, ontology truthfulness, and sentence ordering to probe reasoning and planning.",
            "method_or_approach": "8-shot chain-of-thought prompting with greedy decoding; predicted CoTs parsed and analyzed with the same formal evaluator as for INSTRUCTGPT. Experiments examine how error types and proof-accuracy change with model size.",
            "performance": "Smaller GPT-3 variants showed markedly lower proof accuracy; proof accuracy increased with larger model size but only the largest (text-davinci-002) performed reliably above chance. The appendix shows progressive improvement as experiments move from 350M -&gt; 1.3B -&gt; 6.7B models, but overall these variants often failed on longer (5-hop) top-down examples. Error profile: smaller models more frequently produced invalid or non-atomic steps; frequency of invalid steps decreases with size.",
            "limitations_or_failure_cases": "Smaller GPT-3 models are more likely to produce invalid steps or non-atomic jumps, and generally fail more often on longer or top-down-ordered proofs. All variants suffer from proof-planning issues when multiple valid deduction choices exist; reliance on pretraining confounds behavior on 'true' ontologies.",
            "comparison": "Across this family, larger models make fewer invalid steps and more often produce strictly-valid but sometimes misleading steps; text-davinci-002 (INSTRUCTGPT) outperforms the other named variants. Label accuracy aligns better with permissive proof metrics (skip/valid) than with strict proof accuracy across model sizes.",
            "ablation_or_analysis_results": "The paper's ablation-style analyses include model-size sweeps (reported in Appendix/figure 8) demonstrating monotonic improvement in step validity and proof accuracy with increased size; analysis of first non-canonical errors shows smaller models err via invalid/non-atomic steps while larger models err via strictly-valid misleading steps. Other controlled variables (ontology type, hops, traversal order) were analyzed for each model variant.",
            "uuid": "e4927.1",
            "source_info": {
                "paper_title": "L ANGUAGE M ODELS A RE G REEDY R EASONERS : A S YSTEMATIC F ORMAL A NALYSIS OF C HAIN - OF - T HOUGHT",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "FOLIO: natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models still can't plan (A benchmark for llms on planning and reasoning about change)",
            "rating": 2,
            "sanitized_title": "large_language_models_still_cant_plan_a_benchmark_for_llms_on_planning_and_reasoning_about_change"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 1,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        },
        {
            "paper_title": "On the paradox of learning to reason from data",
            "rating": 1,
            "sanitized_title": "on_the_paradox_of_learning_to_reason_from_data"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        }
    ],
    "cost": 0.012344,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LANGUAGE MODELS ARE GREEDY REASONERS: A SYSTEMATIC FORMAL ANALYSIS OF CHAIN-OF- THOUGHT</p>
<p>Abulhair Saparov 
Center for Data Science
New York University
10011New YorkNYUSA</p>
<p>He He 
Center for Data Science
New York University
10011New YorkNYUSA</p>
<p>LANGUAGE MODELS ARE GREEDY REASONERS: A SYSTEMATIC FORMAL ANALYSIS OF CHAIN-OF- THOUGHT
Published as a conference paper at ICLR 2023
Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PRONTOQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-ofthought into symbolic proofs for formal analysis. Our analysis on INSTRUCTGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.Published as a conference paper at ICLR 2023 Q: Each cat is a carnivore. Every carnivore is not herbivorous. Carnivores are mammals. All mammals are warm-blooded. Mammals are vertebrates. Every vertebrate is an animal. Animals are multicellular. Fae is a cat. True or false: Fae is not herbivorous. A: Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is not herbivorous. Fae is not herbivorous. True context query chain-of-thought label FIGURE 1: A question-answering example from PRONTOQA, with each component highlighted and labeled. animal multicellular vertebrate mammal warm_blooded carnivore ¬herbivorous catStep 1: Generate ontologyStep 2: Generate proof from ontologyHop ¬herbivorous(fae)Step 3: Translate ontology to natural language context "Q: Each cat is a carnivore. Every carnivore is not herbivorous. Carnivores are mammals. All mammals are warm-blooded. Mammals are vertebrates. Every vertebrate is an animal. Animals are multicellular."Step 4: Translate proof into query, chain-of-thought, and label "Fae is a cat. True or false: Fae is not herbivorous. A: Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is not herbivorous. Fae is not herbivorous. True" FIGURE 2: Schematic of the generative process for each example in PRONTOQA.Step 1: We generate an ontology from a prior distribution, shown here as a tree. Each node denotes a concept (e.g., mammal), each with an optional property (e.g., warm_blooded), and each blue edge denotes a "subtype of" relation.Step 2: Generate proof from the ontology. Each horizontal black line indicates a proof step, with its premises written above the line and the conclusion written below.Step 3: Convert the ontology into a natural language context. Step 4: Convert the proof into a natural language query, chain-of-thought, and answer label. There is a one-to-one correspondence between the conclusion of each proof step and the sentences in the chain-of-thought.</p>
<p>INTRODUCTION</p>
<p>The ability to reason-drawing new conclusions from provided facts-is a hallmark of human intelligence. Recently, chain-of-thought (CoT) prompting has enabled large language models (LLMs) to perform logical reasoning tasks with impressive accuracy (Wei et al., 2022;Chowdhery et al., 2022;Lewkowycz et al., 2022). In CoT prompting, each example consists of a question (e.g., " 6 3 − 1?"), a short description of the reasoning required to answer the question called the "chain-of-thought" (e.g., " 6 3 is 2. 2 − 1 is 1."), and a label (e.g., "1"). When prompted with a few CoT examples, the elicited reasoning allows LLMs to predict the label with much higher accuracy than standard question-answer prompting. However, it is unclear to what extent these models can reason due to several confounding factors. First, existing studies primarily rely on question-answering (QA) tasks from real-world settings such as math word problems (Cobbe et al., 2021;Han et al., 2022;Weston et al., 2016). It is likely that LLMs have already acquired the knowledge through pretraining and simply retrieve the answer rather than reason over it. Second, the reasoning task may contain spurious correlations that allow the model to obtain the correct answer through shortcuts (Zhang et al., 2022b). In this work, we systematically investigate the reasoning capability of LLMs by directly evaluating their predicted chains-of-thought (the interpretable proof steps), rather than the predicted label. To enable easy analysis of the CoT, we construct a new synthetic QA dataset called PRONTOQA, for Proof and Ontology-Generated Question-Answering. Inspired by the PROOFWRITER dataset (Tafjord et al., 2021), each example in PRONTOQA is generated from an ontology and has a unique proof (see figure 1 for an example). We convert the proofs into syntactically simple sentences using a grammar such that the inverse process is relatively easy: From the predicted CoT, we semantically parse each sentence into a formal language and reconstruct the underlying proof steps. We then directly analyze the model's reasoning by inspecting each step in the reconstructed proof and comparing them against the gold proof. 1 We emphasize here that while the dataset is an important contribution of this paper, the main contribution is the analysis that is facilitated by the dataset.</p>
<p>We systematically evaluate INSTRUCTGPT 2 (Ouyang et al., 2022) and the original GPT-3 (Brown et al., 2020) on PRONTOQA by controlling a number of variables that characterize the complexity of the reasoning task, such as the ontology type and the number of proof steps required. Our analysis shows that these models are quite good at producing valid individual proof steps, even on fictional and counterfactual ontologies. However, LLMs have difficulty with proof planning: when the models encounter a point in the proof where multiple valid proof steps are available, they sometimes select the wrong step, and this often leads to an incomplete proof and subsequently an incorrect answer. Interestingly, the models are much less likely to be misled with a true ontology, suggesting that the world knowledge acquired during pretraining plays an important role in LLM reasoning. We also find that our results generalize to more sophisticated/informative prompts, such as self-consistency prompting (Wang et al., 2022), and prompts with example traces of depth-first proof search instead of CoT.</p>
<p>RELATED WORK</p>
<p>Our proposed dataset is most closely related to PROOFWRITER (Tafjord et al., 2021) and FOLIO (Han et al., 2022) which are QA datasets designed to test reasoning ability. PROOFWRITER provides multi-hop proofs for each example. However, there are a number of key properties that led us to develop our own dataset (see table 1 for a summary). FOLIO does not provide easily-parseable proofs/CoTs in their examples, and evaluation is done by inspecting the predicted labels, which may not necessarily be a good measure of reasoning ability. In our analysis, we focus on more specific variables that may affect the reasoning of the model, such as: (1) Is the model's reasoning dependent on whether the example is consistent with pretraining ("true"), inconsistent ("false"), or neither ("fictional")? (2)   There are efforts to tweak or extend CoT prompting to elicit more sophisticated reasoning behavior (Creswell et al., 2022;Wang et al., 2022;Creswell &amp; Shanahan, 2022;Anil et al., 2022;Dohan et al., 2022), and they have shown that these prompting extensions to CoT can improve the elicited reasoning behavior of LLMs, even with smaller models. Rather than presenting a new prompting approach, the goal of this work is to measure the reasoning ability elicited by CoT. There are other datasets that have been designed or used to measure the reasoning capabilities of transformer-based models and LLMs (Han et al., 2022;Weston et al., 2016). They show that LLMs are able to answer questions that require reasoning in the few-shot setting with reasonable accuracy. Similar to our approach, Betz (2020) converts logical forms into fairly simple natural language using templates. However, the examples in these datasets are consistent with the real-world, and so they may confound measuring reasoning ability with retrieval ability. Valmeekam et al. (2022) found that LLMs had difficulty with a fairly simple planning task, but it is not clear whether this was due to an inability to reason or other abilities instrumental in planning, such as world modeling, keeping track of state changes, and reasoning about events that occur sequentially in time. This is despite their controlling for other variables involved in planning, such as plan generation, robustness to goal formulation, among others. They experimented with examples in a "Blocksworld" environment, a significant portion of which the model can acquire from pretraining. Our work aims to address this gap. As in our approach, Dasgupta et al. (2022) specifically looked at whether LLMs can reason in fictional or counterfactual settings and found that reasoning ability is indeed negatively affected in these settings. However they did not analyze individual steps of reasoning to better understand the cause of the errors. Since we are able to formally evaluate the LLM's predicted CoT step-by-step, we are able to perform a more fine-grained analysis of their reasoning ability. Zhang et al. (2022b) showed that BERT is not able to learn to reason robustly, but they did not use CoT prompting and it is not obvious if their results generalize to LLMs, which we evaluate. There are two broad research approaches for reasoning in NLP: (1) reasoning over a formal symbolic language, possibly with neuro-symbolic methods and/or semantic parsing (Saparov &amp; Mitchell, 2022;Zhang et al., 2022a;Kapanipathi et al., 2021;Dong et al., 2019;Rocktäschel &amp; Riedel, 2017), or (2) reasoning directly over natural language (Chen et al., 2021;Bostrom et al., 2022;Welleck et al., 2021;Bhagavatula et al., 2020;Angeli &amp; Manning, 2014;MacCartney &amp; Manning, 2009). While PRONTOQA is generated from symbolic ontologies, the examples themselves are in natural language, and so provides value to both research directions. Recent work has examined in-context learning and found that performance on certain tasks is sensitive to the prompt (Razeghi et al., 2022;Lu et al., 2022). However, they focused on sentiment classification and simple arithmetic tasks, and it is not clear if their results generalize to reasoning. The LLM could feasibly use retrieval, rather than reasoning, to perform those tasks. Our experiments on the fictional ontology show that the model is able to reason even when there is nothing to retrieve from.</p>
<p>PRONTOQA: A SYNTHETIC DATASET FOR LOGICAL REASONING</p>
<p>We create a new dataset, called PRONTOQA for Proof and Ontology-Generated Question-Answering, where each question is generated from a symbolic ontology and proof to facilitate formal analysis of the predicted CoT. To focus the scope of our exploration, and to limit the complexity of the generated questions to those within reach of current LLMs, we only consider questions that are answerable using repeated applications of the modus ponens deduction rule. More formally, modus ponens is a simple deduction rule where given the premises ∀x(f (x) → g(x)) and f (a) , we conclude g(a) (e.g., given "All cats are carnivores" and "Fae is a cat," we conclude "Fae is a carnivore;" see figure 6 in the appendix). 3 This rule can be easily chained together to construct proofs with controllable size. We generate CoT examples consisting of: the context, query, CoT, and label, where the context is a short paragraph containing information relevant to answer the query (see figure 1 for an example). Each example is translated from a proof and ontology such that the inverse process is simple: the sentences in an example can be easily and uniquely parsed into symbolic logical forms amenable to formal analysis. More specifically, as shown in figure 2, we: (1) first generate an ontology from a set of concepts, (2) generate a proof by traversing the ontology, (3) translate the ontology into the natural language context, and (4) translate the proof into the query, CoT, and label by mapping logical forms to natural language sentences. We describe each step in further detail below. Ontology generation. The first step is to generate a small hierarchical ontology. The ontology is a set of concepts (e.g., mammal, cat, carnivore, etc) and subtype relations between them (e.g., ∀x(cat(x) → carnivore(x))). The ontology also describes properties of concepts (e.g., ∀x(mammal(x) → ¬cold_blooded(x))). To generate questions that are not overly complex, we restrict the ontologies to be linear (i.e., in the tree, every node has exactly 0 or 1 child nodes). Since ontologies are randomly generated, they vary in size from as few as 3 concepts to as many as 10. Proof generation. We generate proofs from the ontology by choosing a starting node uniformly at random, and generating the initial axiom indicating that an entity has a specific type (e.g., cat(fae)). Then, we walk up the tree, with each step corresponding to an application of a deduction rule (i.e., a proof step). Each proof step consists of zero or more premises and one conclusion. We stop when we reach a node (e.g., carnivore(fae)), or a node property (e.g., ¬herbivorous(fae)), such that the number of generated proof steps matches the target number of steps. Translation to natural language example. Given a generated ontology and proof, we now translate it into a natural language CoT example consisting of the question (context and query), CoT, and label. We describe how each component is generated below: We use a simple grammar to convert the formal statements of the ontology into the natural language utterances that make up the context. Every edge in the ontology is converted into sentences such as "All cats are carnivores" or "Every cat is a carnivore." Properties of nodes are also converted into sentences of the form "All mammals are not cold-blooded," etc. The query is generated by using the same grammar to convert the initial axiom in the proof into a natural language sentence (e.g., "Fae is a cat"). We then determine with probability 0.5 whether to ask if the conclusion of the proof is true or if its negation is false, and convert it into a natural language "true or false" query (e.g., "True or false: Fae is not herbivorous.") and label (e.g., "True"). We convert the ordered sequence of proof steps into the CoT by translating the conclusion of each proof step into a CoT sentence. Avoiding shortcuts. In section A.2 in the appendix, we describe how we add distractor sentences in order to remove shortcuts that would allow the model to "guess" the answer without reasoning. A unique feature of PRONTOQA is that it is easily programmable, with a handful of tunable knobs which we use to generate examples with varying degrees of complexity and study different aspects of reasoning in LLMs. These variables are described in greater detail in section 5.1.</p>
<p>FORMAL ANALYSIS OF PREDICTED PROOFS</p>
<p>Instead of measuring the accuracy of the predicted answers (i.e., "true" or "false"), we would like to directly evaluate the predicted CoT to check if the model derives the right answer for the right reason. We endeavor to analyze whether the model is able to apply deduction rules correctly at each proof step (i.e., local correctness), but also whether the model can plan ahead and work toward proving the answer for the query (i.e., global correctness). To measure the local correctness of a given proof step, we compute whether the step follows from one or more applications of deduction rules, and whether it requires additional rules beyond those of the gold proofs. To measure the global correctness, we wish to identify proof steps that deviate from the gold proof.</p>
<p>To achieve this, we parse each sentence of the predicted CoT into logical form via recursive-descent parsing using the simple grammar from the generative process. We then compute whether that logical form is provable from previous logical forms via one or more applications of deduction rules. This logical form corresponds to the conclusion of a proof step. We then evaluate the correctness of each proof step by categorizing it according to three dimensions:</p>
<p>Step type Example (the conclusion of each step is highlighted green) Strictly-valid atomic correct step, or canonical step "Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is not herbivorous. Fae is not herbivorous. True" (this is the gold CoT for this example)</p>
<p>Strictly-valid atomic misleading step "Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is a mammal. Fae is a mammal..."</p>
<p>Strictly-valid non-atomic correct step "Fae is a cat. Fae is a carnivore. Every carnivore is not herbivorous. Fae is not herbivorous. True"</p>
<p>Strictly-valid non-atomic misleading step "Fae is a cat. Cats are carnivores. Fae is a carnivore. Fae is a mammal. Every mammal is a vertebrate..."</p>
<p>Broadly-valid correct step "Fae is a cat. Every cat is not herbivorous. Fae is not herbivorous..."</p>
<p>Broadly-valid misleading step "Fae is a cat. Every cat is a mammal. Fae is a mammal..."</p>
<p>Invalid step "Fae is a cat. Cats are carnivores. Fae is a carnivore. Every carnivore is a cat. Fae is a cat..." 1. Validity: Is the current proof step provable from previous steps? If it is provable using only the deduction rules that appear in the gold proofs, we say the step is strictly-valid. If it is provable with a more powerful proof calculus, like natural deduction, we say the step is broadly-valid.</p>
<p>Otherwise, we say the step is invalid. For example, given the premises, "Cats are carnivores" and "Carnivores are mammals," the step with conclusion "Cats are mammals" is broadly-valid since an additional deduction rule is required to prove it:
given ∀x(f (x) → g(x)) and ∀x(g(x) → h(x)), conclude ∀x(f (x) → h(x)).
Notice that this is distinct from a strictly-valid non-atomic step since this conclusion is not provable via repeated applications of modus ponens.We note that this the only additional rule that we check, as we did not encounter any instances of other broadly-valid rules. 2. Atomicity: Is the current proof step provable from previous steps with exactly one application of a deduction rule? If so, we say the proof step is atomic. Otherwise, it is non-atomic. Note that since all broadly-valid steps are non-atomic, this distinction is only useful for strictly-valid steps. For example, given the premises, "Fae is a cat," "Cats are carnivores," and "Carnivores are mammals," the step with conclusion "Fae is a mammal" is non-atomic since the step "Fae is a carnivore" was skipped. 3. Utility: If the current proof step's premises are part of the gold proof, but its conclusion is not, then we say the proof step is misleading. Otherwise, it is correct. For example, given the premises "Fae is a carnivore," "All carnivores are not herbivorous," and "Carnivores are mammals," and the goal is to prove "Fae is not herbivorous," the step "Fae is a mammal" is misleading since although the step is strictly-valid, it does not help to prove the goal. The types of proof steps are listed in table 2 along with examples. Unparseable proof steps are marked as incorrect. For brevity, we refer to strictly-valid atomic correct steps as canonical steps. Psuedocode of the procedure to evaluate proofs is given in algorithm 1 in the Appendix. Metrics. Given the above categorization of proof steps, a proof is defined to be correct if and only if there exists a path of proof steps from the premises to the conclusion (note that under this definition, it is possible for a correct proof to contain invalid proof steps). We could require that all proof steps in the path be canonical. But it is not obvious that this metric, which we call strict proof accuracy, would accurately measure the reasoning ability of the model. As such, we also consider more relaxed metrics: (a) we allow proof steps in the path to be strictly-valid non-atomic correct, which we call "skip" proof accuracy, (b) we allow proof steps to be broadly-valid, which we call broad proof accuracy, or (c) we allow proof steps to be strictly-or broadly-valid, which we call valid proof accuracy.</p>
<p>RESULTS</p>
<p>EXPERIMENTAL SETUP</p>
<p>In each experiment, we generate QA examples, perform CoT prompting on the LLMs, and analyze the predicted CoTs. We run the experiments on INSTRUCTGPT and the original GPT-3 (OpenAI : Scatter plots of label accuracy vs proof accuracy of all GPT-3 experiments in this paper. The black line indicates perfect agreement between label accuracy and proof accuracy. We emphasize that "proof accuracy" indicates the fraction of proofs (not proof steps) that are considered correct according to our metrics. Label accuracy is not well-correlated with strict or broad proof accuracy, and is better correlated with "skip" and valid proof accuracy, suggesting that label accuracy is a good measure of reasoning ability. models text-ada-001, text-babbage-001, text-curie-001, davinci, text-davinci-001, text-davinci-002), with greedy decoding (Ouyang et al., 2022;Brown et al., 2020). We use 8-shot in-context learning, so each input to the LLM consists of 8 fully-labeled questions followed by a single test question with missing CoT and label. The model's task is to predict the CoT and label for the test question. Note that all examples across all inputs are independently and identically generated from PRONTOQA. There are a number of variables that we control when generating examples in PRONTOQA: (1) the number of hops, (2) the ordering in which the sentences are generated from the ontology, and (3) the type of the ontology. The number of hops directly controls the difficulty of the generated example, and we experiment with 1, 3, and 5 hops. We control the ontology traversal direction: We either traverse the tree top-down (i.e., preorder) or bottom-up (i.e., postorder), generating a sentence for each traversed edge/node. The ordering also affects the difficulty of the generated example: if the sentences are generated bottom-up, they will follow the same order as the steps in the gold proof. On the other hand, if they are generated top-down, the order is reversed, and the task may be more difficult. To avoid any confounding effects from knowledge acquired during pretraining, PRONTOQA generates examples with fictional concept names (e.g., "wumpus" instead of "cat," etc). But we are also interested in measuring this confounding effect, and so in addition to fictional ontologies, we also generate "true" and "false" ontologies. True ontologies use real concept names and are consistent with the real-world (we randomly sample from a list of three hand-coded real ontologies). False ontologies use real concept names but the trees are generated using the random process described in section 3, and so it is very likely to generate a false statement, such as "All mammals are cats." For each combination of variables, we run the model on 400 examples generated from the testbed, for a total of 48 experiments. We compute 95% confidence intervals for each experiment, as the number of correct proofs is distributed as Binomial(400, p) with p being the model's accuracy (Wilson, 1927).</p>
<p>DO CORRECT ANSWERS IMPLY CORRECT REASONING?</p>
<p>Label accuracy may not necessarily measure whether the model is performing reasoning correctly, since the model may find ways to guess the label via heuristics. To gauge whether label accuracy is a good metric and which proof accuracy metric is best to measure reasoning ability, we investigate how label accuracy is related to the various proof accuracy metrics. We plot proof accuracy vs label accuracy (i.e., simply checking whether the predicted label "True" or "False" is correct) of every experiment that we conducted in figure 3. Each point in the scatter plot corresponds to one of our 48 experiments described above. Observe that the label accuracy is poorly correlated with strict proof accuracy, and that strict proof accuracy may underestimate the model's reasoning ability. Rather, the most permissive accuracy metric has the highest correlation with label accuracy, suggesting that label accuracy is appropriate to measure reasoning accuracy. It also suggests that the most permissive proof accuracy metric is most appropriate for measuring the reasoning ability of the model.</p>
<p>PROOF ANALYSIS RESULTS</p>
<p>Only the largest model is able to reason. We investigated how reasoning ability is affected by model size. In figure 8 in the Appendix, proof accuracy increases considerably when increasing the model size from 350M to 1.3B and 6.7B. However, only text-davinci-002 is able to perform better than chance. We were not able to conclusively discern the cause of the significant difference in performance between version 001 and 002. One possible factor is the maximum token limit of version 002 is roughly twice that of version 001. In fact, the model davinci seems to perform as well as, if not slightly better than, text-davinci-001. In addition, we notice that the frequency of invalid steps decreases as the model size increases, and so larger models seem to be better at making valid steps, whether or not those steps are actually useful. For the remainder of the paper, our results focus on text-davinci-002. Our main results are in figure 4 where we show the proof accuracy and distribution of proof step types for all experiments. Real-world knowledge helps reasoning. We investigate the extent to which reasoning ability is affected by whether the ontology is fictional, "true," or "false." Evidently from figure 4, the LLM seems to perform comparably in the fictional and "false" ontology settings (accuracy is slightly worse with a "false" ontology). But when using the "true" ontology, the model performs much better, and its performance does not drop when increasing the number of hops from 3 to 5. The model is able to utilize its background knowledge from pretraining to "skip" hops, and is thus not as negatively affected by the increased hops. This is consistent with the findings of Dasgupta et al. (2022). Evidently, the model's reasoning is heavily reliant on real-world knowledge, and this may be a problem for generalizability, such as when applying LLMs to novel scenarios or to settings that are not well-represented in the training data. Longer proofs are still challenging. We investigate the extent to which reasoning ability is affected by the number of hops in the proof. We see from figure 4 that the model handles 1-and 3-hop examples quite well but struggles with 5-hop top-down examples, with accuracy falling to chance. So while it is able to perform reasoning to an extent, it is more limited as the number of hops increases. Traversal direction affects reasoning. We also tested how reasoning ability is affected by the traversal direction of the ontology. We notice in figure 4 that as the number of hops increases, the model becomes sensitive to the traversal direction of the ontology (top-down vs bottom-up). This may be due to the fact that the order of the gold proof steps mirrors the bottom-up traversal, and is the reverse of the top-down traversal. Thus, the task may be made more difficult for language models if the context sentences are ordered top-down. How do LLMs reason step-by-step? We investigate the fraction of correct and incorrect proofs that contain various types of proof steps, and whether the correctness of the proof is correlated with the presence of specific types of proof steps. Figure 4 breaks down the bars further (in darker red and blue) to indicate the fraction of proofs that contain proof steps other than canonical steps, since most predicted proof steps were canonical (in the 5-hop experiments with fictional ontology, they constitute 93.2% of proof steps). We make the following observations: 1. Most predicted proof steps are strictly-valid (in the 5-hop experiments with fictional ontology, 93.2% of proof steps are strictly-valid, 2.4% are broadly-valid, and 5.9% are invalid). 2. LLMs tend to skip steps by producing non-atomic steps, just as humans do when they verbalize their reasoning (in the 5-hop experiments with fictional ontology, 2.4% of proof steps are nonatomic, even though all steps in the few-shot examples are atomic). 3. Most incorrect proofs contain misleading steps and invalid steps. This suggests that the source of the incorrect reasoning is either a due to a misleading step or an invalid step that causes the model to produce steps that do not belong to the gold proof. Intriguingly, some correct proofs also contain misleading steps and invalid steps, which implies that the model is sometimes able to recover from these "mistakes" and return to the gold proof. We analyze this behavior in greater detail in section 5.4.</p>
<p>WHAT LEADS TO A MISTAKE?</p>
<p>We investigate whether specific types of proof steps are causing INSTRUCTGPT to produce reasoning errors. To do so, we identify the first step in each incorrect proof that is not a canonical step. We observe in figure 5, among incorrect proofs, strictly-valid atomic misleading steps appear in the proof first far more often than other non-canonical step types, including invalid steps. See figure  7 in the appendix for an example prediction where a misleading step causes the model to fail to prove the goal and produce an invalid step. This indicates that for the best-performing models, the main source of reasoning error is from misleading steps, since most predicted steps are strictly-valid and atomic. That is, imagining the space of proof steps as a graph where each edge represents a single valid step, INSTRUCTGPT almost always performs a walk in this graph. Once INSTRUCTGPT encounters a branch where one path at the fork follows the correct proof and the other paths do not, FIGURE 4: Proof accuracy versus ontology type, number of hops, and ontology traversal direction. Each bar is subdivided into six darker bars according to the types of proof steps that appear in the predicted chains-of-thought. For example, the dark red bar corresponding to "invalid steps" indicates the proportion of incorrect proofs that contain an invalid step. The dark blue bar corresponding to "invalid steps" indicates the proportion of correct proofs that contain an invalid step. The proof step types are detailed in figure 2. True ontology INSTRUCTGPT will select the incorrect direction with some frequency and is then not able to return to the correct path. Therefore, it seems that while LLMs are able to produce valid proof steps with high probability, they have difficulty with proof planning/strategizing. We were curious if this relationship held in smaller models. We see in figure 5 that smaller models are more prone to make invalid or non-atomic steps as their first non-canonical step. But as model size increases, these types of steps become rarer, and is instead superseded by misleading steps. Looking again at figure 4, we note that many correct proofs also contain misleading steps, and so it must be the case that INSTRUCTGPT sometimes returns to the correct proof path at some point after making a misleading step. To investigate this behavior more closely, we count the number of steps that the model takes after making a misleading step until it produces a step in the gold proof and plot the histogram in figure 10 in the appendix. We observe that, in general, the more time the model spends outside the correct proof path, the less likely it becomes to return to the correct proof. We demonstrate in section A.7 in the appendix that our findings generalize to more sophisticated prompting strategies via an experiment using self-consistency prompting (Wang et al., 2022) and an experiment using a prompt containing example traces of depth-first proof search (i.e. containing examples of the search recovering from misleading steps). </p>
<p>CONCLUSION AND FUTURE WORK</p>
<p>In this work, we introduced a synthetic fictional QA dataset called PRONTOQA designed to evaluate the reasoning ability of LLMs. We evaluated INSTRUCTGPT and GPT-3 on PRONTOQA and found that while the largest model was generally able to perform reasoning, it had difficulty with proof planning and selecting the correct proof step when there are multiple available. PRONTOQA, and our high-level approach more broadly, could be used to compare LLM reasoning with that of humans, and to explore which aspects of human reasoning were acquired by LLMs from their pretraining. As our work has shown that LLMs are able to reason to a degree, it is yet unclear where the model acquired this ability. Are there portions of the pretraining data that teach the model to reason? Our work shows that CoT prompting is not sufficient for more complex reasoning, such as in mathematical domains, since the reasoning tested in this work is a strict subset of that of general mathematical reasoning. Mathematical proofs contain steps with much higher branching factor, where robust proof planning is instrumental. Rather, our results suggest that reasoning systems may benefit from more sophisticated proof planning/search strategies, such as neurosymbolic approaches where part of the reasoning is done over interpretable symbolic structures. PRONTOQA can be used to train new reasoning systems, or to pretrain/fine-tune LLMs to improve their reasoning capability. The inability of LLMs to plan ahead in their reasoning might be related to recent work illuminating the theoretical computational limitations of such models (Merrill et al., 2022). Since our analysis was limited to modus ponens, proof lengths of at most 5, and semantically simple sentences, it remains to be seen whether LLMs are able to produce longer proofs, or reason with other deduction rules, or over more semantically complex sentences/logical forms.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>All our experiments in the main text were run using the OpenAI API on September 9 th , 10 th , and 11 th , 2022. The self-consistency experiment was run on October 29 th and 30 th , and the DFS experiment was run on November 16 th (see section A.7). For the sake of reproducibility of the analysis, all model outputs, the code for data generation, and the analysis code are freely available with a permissive open-source license at github.com/asaparov/prontoqa. The command python analyze_results.py produces all figures used in this paper.</p>
<p>A APPENDIX</p>
<p>A.1 DEDUCTION RULES Figure 6 outlines the two deduction rules that we utilize in PRONTOQA.</p>
<p>Deduction rules in general form</p>
<p>Examples
f (a) ∀x(f (x) → g(x)) Hop g(a)
cat(fae) ∀x(cat(x) → carnivore(x)) Hop carnivore(fae)</p>
<p>i.e., Given that "Fae is a cat" and "All cats are carnivores," we conclude that "Fae is a carnivore."
Ax A Ax cat(fae)
i.e., Assume that "Fae is a cat" is an axiom.</p>
<p>FIGURE 6: The two deduction rules that constitute the restricted proof calculus in our experiments. All proofs in PRONTOQA are composed of instances of only these two rules. Here, A is any expression, and f (a) is any expression where the variable x in f is substituted with any term a (and similarly for g(a)).</p>
<p>A.2 AVOIDING SHORTCUTS</p>
<p>When generating examples in PRONTOQA, we have to be careful to remove any shortcuts in the question that would allow the model to "guess" the answer without reasoning. In fact, we find that without any distractors, INSTRUCTGPT is able to predict the "true"/"false" label almost perfectly. INSTRUCTGPT can utilize the heuristic that whether the queried property is mentioned in the context implies whether or not it is true. For instance, if the example is asking "Sally is a cat. True or false: Sally is a vertebrate," the model can simply look for a string "Every is (not) a vertebrate," regardless of the content in the blank. Due to the generative process of these examples, this kind of sentence is guaranteed to appear exactly once in the context. Thus, to ensure that such a heuristic is not informative, we add a distractor sentence by generating a novel concept that is disconnected from the ontology tree, and we assign to this new concept the negation property that is queried by the question. So in the above example, if the ontology has the rule "Every mammal is a vertebrate," a possible distractor sentence is "Every insect is not a vertebrate." We insert this distractor sentence into a random position in the context.  </p>
<p>A.4 HOW WE EVALUATE THE CHAIN-OF-THOUGHT</p>
<p>Algorithm 1: Our algorithm for reconstructing and evaluating the proof from the predicted chain-of-thought, and for computing whether each proof step is valid vs invalid, atomic vs non-atomic, misleading vs correct. Here, we use the notation ϕ[x → c] to denote the substitution of all occurrences of the symbol x with c in the logical form ϕ. We use the helper function is_provable to compute whether a given logical form ϕ is provable from a set of axioms with one or more deduction rules. The function returns a tuple (P, k) where if k ≥ 0, ϕ is provable in k steps using the premises P . Otherwise, ϕ is not provable.   To what extent do our findings generalize to prompting strategies other than CoT with greedy decoding? To test this, we experimented with self-consistency prompting (Wang et al., 2022), where for each example, we queried the LLM for 40 sample predictions of the CoT, using a temperature setting of 0.7. For each sample CoT s i , we compute the following quantity:
exp 1 |s i | |si| j=1
log p(s i,j |s i,1 , . . . , s i,j−1 ) .</p>
<p>We parse each predicted CoT sample into a sequence of logical forms, and we find the logical form sequence with the highest sum of the above quantity over all the CoT samples that share the same semantic parse. This logical form sequence is the final prediction. We run this experiment in our setting with 5 hops, fictional ontology, and top-down traversal direction, with 100 examples. The resulting valid proof accuracy is 0.56 compared to 0.545 which is not significantly different. Furthermore, inspecting specific examples of CoT samples (see figure 11), we see that for examples that the model gets wrong, the model is actually assigning higher overall probability to the incorrect proof than to the correct proof. This suggests that our results do in fact generalize to more sophisticated prompting/decoding strategies, and that strategies that endeavor to find proofs with higher probability globally (e.g. beam search) will not help the model in proof planning.</p>
<p>A.7.2 CAN THE MODEL LEARN TO DO DEPTH-FIRST SEARCH FROM IN-CONTEXT EXAMPLES?</p>
<p>Our earlier analysis also revealed a possible way forward to rectify the model's shortcoming in proof planning: Even after making a misleading step, INSTRUCTGPT sometimes "returned" to the correct proof. We could instead relax the constraint that the chains-of-thought always reflect the shortest correct proof of the answer. Instead, we allow the in-context examples to contain misleading steps, with the hope that the model learns to better recover from misleading steps. It is reasonable for humans to explore a space of possible solutions before arriving at the correct answer, and we could mimic this in LLMs by allowing the CoT to explore alternative paths in the space of proof steps, even if those paths are not ultimately part of the shortest proof. This kind of search strategy is analogous to depth-first search (DFS) in graphs. To test whether this approach improves the model's reasoning ability, we conduct an experiment where we provide in-context examples of chains-of-thought that follow a DFS, with the hope that the model is able to learn to perform DFS when given a new test example, thereby improving the likelihood that it finds the correct answer. We run this experiment in our setting with 5 hops, fictional ontology, and top-down traversal direction, with 100 examples. The resulting valid proof accuracy is 0.55 compared to 0.545 which, again, is not significantly different.</p>
<p>FIGURE 3: Scatter plots of label accuracy vs proof accuracy of all GPT-3 experiments in this paper. The black line indicates perfect agreement between label accuracy and proof accuracy. We emphasize that "proof accuracy" indicates the fraction of proofs (not proof steps) that are considered correct according to our metrics. Label accuracy is not well-correlated with strict or broad proof accuracy, and is better correlated with "skip" and valid proof accuracy, suggesting that label accuracy is a good measure of reasoning ability.</p>
<p>FIGURE 5 :
5Proportion of incorrect proofs versus the type of the first error (i.e., noncanonical proof step) and model size. The proof step types are detailed in figure 2. We note that in the 3-hop experiments with fictional ontology, four of the 400 examples surpassed the 2049 token limit for all models (except text-davinci-002). These examples were ignored (so the effective number of examples is 396). We omit the results for the 1-hop experiments here since there were too few incorrect proofs.</p>
<p>FIGURE 7 :
7An example from PRONTOQA (5 hops with fictional ontology and top-down traversal direction) along with the expected and predicted answer from INSTRUCTGPT. Note that most of the predicted steps are canonical (highlighted yellow). The model makes a single misleading step (highlighted purple) which causes it fail to prove the goal, and to eventually make an invalid step (highlighted red).</p>
<p>1(FIGURE 8 :
8function evaluate_cot(context sentences Q1, . . . , Qm, predicted chain-of-thought sentences C1, . . . , Cn, gold chain-of-thought sentences T1, . . . , Tr) 2 for i ∈ 1, . . . , m do /<em> parse the context i ∈ 1, . . . , r do /</em> parse the gold chain-ofi ∈ 1, . . . , n do /<em> parse and evaluate the predicted chain-of-P, k) = is_provable(L C i , {L Q 1 , . . . , L Q m }, S) 10if k ≥ 0 /</em> if we wish to use a stricter metric for proof accuracy, we can add conditions here (e.g., requiring atomicity by checking k = 1) <em>/11 add L C i to S 12 if P ⊆ {L T 1 , . . . , L T r } and L C i / ∈ {L T 1 , . . . , L T r } /</em>the premises are in the gold proof but the conclusion is not <em>/ 13 mark L C i as a misleading step 14 return L T r ∈ S /</em> the proof is correct if the final conclusion is provable <em>/ 15 function is_provable(logical form ϕ, set of axioms A, previous conclusions S) 16 if ϕ ∈ A 17 return ({ϕ}, 1) /</em> provable by Ax step (strictly-valid) <em>/ 18 else if ϕ ∈ S 19 return ({ϕ}, 0) /</em> already proved by previous step <em>/ 20 else if ϕ has form g(c) or ¬g(c) for any constants g and c 21 for a ∈ A ∪ S do 22 if a has form ∀x(ψ → γ) where γ[x → c] = ϕ 23 (P, k) = is_provable(ψ[x → c], A, S) 24 if k ≥ 0 25 return (P ∪ {a}, k + 1{a ∈ A}) /</em> provable by Hop step (strictly-valid) <em>/ 26 else if ϕ has form ∀x(ψ → γ) /</em> note: we precompute this graph <em>/ 27 let G be the graph where for any axiom in A with form ∀x(α → β), α and β are vertices and there is a directed edge from α to β 28 if there is a path in G from ψ to γ /</em> provable with additional deduction rules (broadly-valid) <em>/ 29 return ( axioms corresponding to path edges , length of path ) 30 return (∅, −1) /</em> this step is not provable (i.e., invalid) */ A.5 PROOF ACCURACY VS MODEL SIZE Proof accuracy versus model size, ontology type, and number of hops. Each bar is subdivided into six bars according to the types of proof steps that appear in the predicted chains-of-thought. The proof step types are detailed in figure 2. Top-down traversal direction is used in these experiments. We note that in the 3-hop experiments with fictional ontology, four of the 400 examples surpassed the 2049 token limit for all models (except text-davinci-002). These examples were ignored (so the effective number of examples is 396).</p>
<p>FIGURE 10 :
10Histograms depicting the distribution of the number of steps in each proof after a strictly-valid atomic misleading step until returning to the gold proof.</p>
<p>Is the model's reasoning sensitive to whether the predicates in the examples are unary or binary? (3) Is the model's reasoning dependent on the rules of deduction in the examples? These variables are not controllable in existing datasets. Further, in some datasets, the code to generate examples is not available.Dataset </p>
<p>Provides easily 
semantically-
parseable 
proofs </p>
<p>Controls for true 
vs false vs 
fictional contexts </p>
<p>Controls for 
unary vs binary 
predicates </p>
<p>Controls for 
specific rules of 
deduction </p>
<p>Tests reasoning 
beyond the 
domain of math 
word problems </p>
<p>Generation code 
available </p>
<p>GSM8K 
Cobbe et al. (2021) </p>
<p>human-
annotated 
ProofWriter 
Tafjord et al. (2021) </p>
<p>∼ </p>
<p>FOLIO 
Han et al. (2022) </p>
<p>human-
annotated 
SimpleLogic 
Zhang et al. (2022b) </p>
<p>PRONTOQA 
(proposed dataset) </p>
<p>TABLE 1 :
1Comparison of existing datasets for the formal analysis of reasoning ability.</p>
<p>TABLE 2 :
2The types of proof steps (and examples thereof) into which we categorize each step in the predicted chain-of-thought from LLMs. Compare the given chain-of-thought examples with the gold example provided in the first row.</p>
<p>Tumpuses are wumpuses. Alex is a wumpus. Wumpuses are impuses. Alex is an impus. Every impus is a dumpus. Alex is a dumpus. Every dumpus is not hot. Alex is not hot. FalsePredicted answer: Alex is a zumpus. Zumpuses are tumpuses. Alex is a tumpus. Tum puses are wumpuses. Alex is a wumpus. Wumpuses are impuses. Alex is an impus. Every impus is a dumpus. Alex is a dumpus. Dumpuses are vumpuses. Alex is a vumpus. Every vumpus is a numpus. Alex is a numpus. Numpuses are jompuses. Alex is a jompus. Jompuses are rompuses. Alex is a rompus. Rompuses are zumpuses. Alex is a zumpus. Zumpuses are feisty. Alex is feisty. TrueA.3 EXAMPLE INSTRUCTGPT MISPREDICTION </p>
<p>Q: Every vumpus is a numpus. Each vumpus is dull. Dumpuses are vumpuses. 
Every dumpus is not hot. Every impus is a dumpus. Impuses are brown. 
Wumpuses are impuses. Wumpuses are opaque. Tumpuses are wumpuses. Tumpuses 
are small. Every yumpus is hot. Zumpuses are tumpuses. Zumpuses are feisty. 
Rompuses are zumpuses. Every rompus is not kind. Each jompus is a rompus. 
Jompuses are sour. Alex is a zumpus. True or false: Alex is hot. 
A: </p>
<p>Expected answer: 
Alex is a zumpus. Zumpuses are tumpuses. Alex is a tumpus. </p>
<p>True ontology, 1 hop A.6 ADDITIONAL ERROR ANALYSIS st ri c tl y -v a li d a to m ic m is le a d in g st e p s st ri c tl y -v a li d n o n -a to m ic c o rr e c t st e p s st ri c tl y -v a li d n o n -a to m ic m is le a d in g st e p s b ro a d ly -v a li d c o rr e c t st e p s b ro a d ly -v a li d m is le a d in g st e p s in v a li d st e p stext-ada-001 
text-babbage-001 text-curie-001 
davinci 
text-davinci-001 text-davinci-002 
0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>False ontology, 3 hops </p>
<p>text-ada-001 
text-babbage-001 text-curie-001 
davinci 
text-davinci-001 text-davinci-002 
0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>True ontology, 3 hops </p>
<p>text-ada-001 
text-babbage-001 text-curie-001 
davinci 
text-davinci-001 text-davinci-002 
0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>Fictional ontology, 1 hop </p>
<p>text-ada-001 
text-babbage-001 text-curie-001 
davinci 
text-davinci-001 text-davinci-002 
0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>False ontology, 1 hop </p>
<p>text-ada-001 
text-babbage-001 text-curie-001 
davinci 
text-davinci-001 text-davinci-002 
0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>3 hops, bottom-up 
traversal direction 
3 hops, top-down 
traversal direction 
5 hops, bottom-up 
traversal direction 
5 hops, top-down 
traversal direction </p>
<p>0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>proportion of 
incorrect proofs </p>
<p>Fictional ontology </p>
<p>3 hops, bottom-up 
traversal direction 
3 hops, top-down 
traversal direction 
5 hops, bottom-up 
traversal direction 
5 hops, top-down 
traversal direction </p>
<p>0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>False ontology </p>
<p>3 hops, bottom-up 
traversal direction 
3 hops, top-down 
traversal direction 
5 hops, bottom-up 
traversal direction 
5 hops, top-down 
traversal direction </p>
<p>0.00 </p>
<p>0.25 </p>
<p>0.50 </p>
<p>0.75 </p>
<p>1.00 </p>
<p>True ontology </p>
<p>FIGURE 9: Proportion of 
incorrect proofs versus the 
type of the first error (i.e., 
non-canonical proof step), 
number of hops, and on-
tology traversal direction. 
The proof step types are 
detailed in figure 2. We 
note that in the 3-hop exper-
iments with fictional ontol-
ogy, four of the 400 exam-
ples surpassed the 2049 to-
ken limit for all models (ex-
cept text-davinci-002). 
These examples were ig-
nored (so the effective num-
ber of examples is 396). We 
omit the results for the 1-
hop experiments here since 
there were too few incorrect 
proofs. </p>
<p>All analysis code, data, data generation scripts, and model outputs are available at github.com/asaparov/prontoqa.
INSTRUCTGPT is the model resulting from fine-tuning GPT-3 via reinforcement learning from human feedback. Throughout the paper, "INSTRUCTGPT" refers to the model named text-davinci-002. But note that in our experiments, we also evaluate text-ada-001, text-babbage-001, text-curie-001, davinci, and text-davinci-001.
In natural deduction, this rule is actually a composition of two steps: given ∀x(f (x) → g(x)), use universal elimination to conclude f (a) → g(a), and given f (a), use implication elimination to conclude g(a).
ACKNOWLEDGMENTSWe thank Vishakh Padmakumar, Richard Yuanzhe Pang, Nitish Joshi, Daniel Khashabi, Nicholas Lourie, and Will Merrill for their helpful and insightful discussion. This research was supported by Open Philanthropy, Samsung Advanced Institute of Technology (Next Generation Deep Learning: From Pattern Recognition to AI), AWS AI, and Cisco Research.
Naturalli: Natural logic inference for common sense reasoning. Gabor Angeli, Christopher D Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Alessandro Moschitti, Bo Pang, and Walter Daelemansthe 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarA meeting of SIGDAT, a Special Interest Group of the ACLGabor Angeli and Christopher D. Manning. Naturalli: Natural logic inference for common sense reasoning. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 534-545.</p>
<p>. 10.3115/v1/d14-1059ACL. ACL, 2014. doi: 10.3115/v1/d14-1059. URL https://doi.org/10.3115/v1/d14-1059.</p>
<p>Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, V Vinay, Ambrose Ramasesh, Guy Slone, Ethan Gur-Ari, Behnam Dyer, Neyshabur, 10.48550/arXiv.2207.04901CoRR, abs/2207.04901, 2022Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay V. Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. CoRR, abs/2207.04901, 2022. doi: 10.48550/arXiv.2207.04901. URL https://doi.org/10.48550/arXiv.2207.04901.</p>
<p>Critical thinking for language models. Gregor Betz, abs/2009.07185Gregor Betz. Critical thinking for language models. CoRR, abs/2009.07185, 2020. URL https: //arxiv.org/abs/2009.07185.</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Chaitanya Ronan Le Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense rea- soning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum? id=Byg1v1HKDB.</p>
<p>Flexible generation of natural language deductions. Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett, 10.18653/v1/2021.emnlp-main.506Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yihthe 2021 Conference on Empirical Methods in Natural Language ProcessingVirtual Event / Punta Cana, Dominican RepublicAssociation for Computational Linguistics20212021Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. Flexible generation of natural language deductions. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen- tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 6266-6278. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021. emnlp-main.506. URL https://doi.org/10.18653/v1/2021.emnlp-main.506.</p>
<p>Natural language deduction through search over statement compositions. CoRR, abs/2201.06028, 2022. Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, Greg Durrett, Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. Natural language deduction through search over statement compositions. CoRR, abs/2201.06028, 2022. URL https:// arxiv.org/abs/2201.06028.</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Ilya Sutskever, and Dario AmodeiTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan- dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An- nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Can NLI models verify QA systems' predictions?. Jifan Chen, Eunsol Choi, Greg Durrett, 10.18653/v1/2021.findings-emnlp.324Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau YihPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021Jifan Chen, Eunsol Choi, and Greg Durrett. Can NLI models verify QA systems' predictions? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 3841-3854. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.324. URL https://doi.org/10. 18653/v1/2021.findings-emnlp.324.</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Parker Gehrmann, Kensen Schuh, Sasha Shi, Joshua Tsvyashchenko, Abhishek Maynez, Parker Rao, Yi Barnes, Noam Tay, Vinodkumar Shazeer, Emily Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Lim, 10.48550/arXiv.2204.02311Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-HellsternCoRR2022Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick; Douglas Eck, Jeff Dean, Slav Petrov, and Noah FiedelAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev- skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Er- ica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language model- ing with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.</p>
<p>Training verifiers to solve math word problems. CoRR, abs/2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Faithful reasoning using large language models. CoRR, abs/2208.14271. Antonia Creswell, Murray Shanahan, 10.48550/arXiv.2208.142712022Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. CoRR, abs/2208.14271, 2022. doi: 10.48550/arXiv.2208.14271. URL https://doi.org/10.48550/ arXiv.2208.14271.</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, 10.48550/arXiv.2205.09712doi: 10.48550/ arXiv.2205.097122022Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. CoRR, abs/2205.09712, 2022. doi: 10.48550/ arXiv.2205.09712. URL https://doi.org/10.48550/arXiv.2205.09712.</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.48550/arXiv.2207.070512022Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. Language models show human-like content effects on reasoning. CoRR, abs/2207.07051, 2022. doi: 10.48550/arXiv.2207.07051. URL https://doi. org/10.48550/arXiv.2207.07051.</p>
<p>Language model cascades. CoRR, abs/2207.10342. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, 10.48550/arXiv.2207.10342Jascha Sohl-Dickstein. 2022David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. CoRR, abs/2207.10342, 2022. doi: 10.48550/arXiv. 2207.10342. URL https://doi.org/10.48550/arXiv.2207.10342.</p>
<p>Neural logic machines. Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, Denny Zhou, 7th International Conference on Learning Representations. New Orleans, LA, USAHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= B1xY-hRctX.</p>
<p>Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. FOLIO: natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, R Shafiq, Alexander R Joty, Wojciech Fabbri, Kryscinski, 10.48550/arXiv.2209.00840CoRR2022Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. FOLIO: natural language reasoning with first-order logic. CoRR, abs/2209.00840, 2022. doi: 10.48550/arXiv.2209.00840. URL https://doi.org/10.48550/arXiv.2209.00840.</p>
<p>Leveraging abstract meaning representation for knowledge base question answering. Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim Roukos, Alexander G Gray, Ramón Fernandez Astudillo, Maria Chang, Cristina Cornelio, Saswati Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, Sairam Gurajada, Hima Karanam, Naweed Khan, Dinesh Khandelwal, Young-Suk Lee, Yunyao Li, P S Francois, Ndivhuwo Luus, Nandana Makondo, Tahira Mihindukulasooriya, Sumit Naseem, Lucian Neelam, Revanth Gangi Popa, Ryan Reddy, Gaetano Riegel, Udit Rossiello, G P Sharma, Mo Shrivatsa Bhargav, Yu, 10.18653/v1/2021.findings-acl.339Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto NavigliAssociation for Computational Linguistics2021volume ACL/IJCNLP 2021 of Findings of ACLPavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim Roukos, Alexander G. Gray, Ramón Fernandez Astudillo, Maria Chang, Cristina Cornelio, Saswati Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, Sairam Gurajada, Hima Karanam, Naweed Khan, Dinesh Khandelwal, Young-Suk Lee, Yunyao Li, Francois P. S. Luus, Ndivhuwo Makondo, Nandana Mihindukula- sooriya, Tahira Naseem, Sumit Neelam, Lucian Popa, Revanth Gangi Reddy, Ryan Riegel, Gaetano Rossiello, Udit Sharma, G. P. Shrivatsa Bhargav, and Mo Yu. Leveraging abstract meaning repre- sentation for knowledge base question answering. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 3884- 3894. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.339. URL https://doi.org/10.18653/v1/2021.findings-acl.339.</p>
<p>Solving quantitative reasoning problems with language models. CoRR, abs/2206.14858. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Cem Slone, Imanol Anil, Theo Schlag, Yuhuai Gutman-Solo, Behnam Wu, Guy Neyshabur, Vedant Gur-Ari, Misra, 10.48550/arXiv.2206.148582022Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. CoRR, abs/2206.14858, 2022. doi: 10.48550/arXiv.2206.14858. URL https: //doi.org/10.48550/arXiv.2206.14858.</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics12022Long Papers), ACL 2022Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 8086-8098. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.556. URL https://doi.org/10.18653/v1/2022.acl-long.</p>
<p>An extended model of natural logic. Bill Maccartney, Christopher D Manning, Proceedings of the Eight International Conference on Computational Semantics. Harry Bunt, Volha Petukhova, and Sander Wubbenthe Eight International Conference on Computational SemanticsTilburg, The NetherlandsAssociation for Computational LinguisticsBill MacCartney and Christopher D. Manning. An extended model of natural logic. In Harry Bunt, Volha Petukhova, and Sander Wubben (eds.), Proceedings of the Eight International Conference on Computational Semantics, IWCS 2009, Tilburg, The Netherlands, January 7-9, 2009, pp. 140-156. Association for Computational Linguistics, 2009. URL https://aclanthology.org/ W09-3714/.</p>
<p>Saturated Transformers are Constant-Depth Threshold Circuits. William Merrill, Ashish Sabharwal, Noah A Smith, 10.1162/tacl_a_00493Transactions of the Association for Computational Linguistics. 10William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated Transformers are Constant-Depth Threshold Circuits. Transactions of the Association for Computational Linguistics, 10:843-856, 08 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00493. URL https://doi.org/10.1162/ tacl_a_00493.</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, 10.48550/arXiv.2203.02155CoRR2022Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155, 2022. doi: 10.48550/arXiv.2203.02155. URL https://doi.org/10.48550/ arXiv.2203.02155.</p>
<p>Impact of pretraining term frequencies on few-shot reasoning. CoRR, abs/2202.07206, 2022. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, Yasaman Razeghi, Robert L. Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. CoRR, abs/2202.07206, 2022. URL https://arxiv.org/ abs/2202.07206.</p>
<p>End-to-end differentiable proving. Tim Rocktäschel, Sebastian Riedel, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USATim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 3788-3800, 2017. URL https://proceedings.neurips.cc/paper/2017/ hash/b2ab001909a8a6f04b51920306046ce5-Abstract.html.</p>
<p>Towards general natural language understanding with probabilistic worldbuilding. Abulhair Saparov, Tom M Mitchell, 10.1162/tacl_a_00463doi: 10.1162/ tacl_a_00463Trans. Assoc. Comput. Linguistics. 10Abulhair Saparov and Tom M. Mitchell. Towards general natural language understanding with probabilistic worldbuilding. Trans. Assoc. Comput. Linguistics, 10:325-342, 2022. doi: 10.1162/ tacl_a_00463. URL https://doi.org/10.1162/tacl_a_00463.</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto NavigliAssociation for Computational Linguistics2021volume ACL/IJCNLP 2021 of Findings of ACLOyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 3621-3634. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.317. URL https://doi.org/10.18653/v1/2021.findings-acl.317.</p>
<p>Large language models still can't plan (A benchmark for llms on planning and reasoning about change). CoRR, abs/2206.10498. Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, Subbarao Kambhampati, 10.48550/arXiv.2206.104982022Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (A benchmark for llms on planning and reasoning about change). CoRR, abs/2206.10498, 2022. doi: 10.48550/arXiv.2206.10498. URL https://doi. org/10.48550/arXiv.2206.10498.</p>
<p>Selfconsistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Denny Zhou, 10.48550/arXiv.2203.111712022Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. Self- consistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171, 2022. doi: 10.48550/arXiv.2203.11171. URL https://doi.org/10.48550/arXiv.2203.11171.</p>
<p>Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903, 2022. URL https://arxiv.org/abs/2201.11903.</p>
<p>Naturalproofs: Mathematical theorem proving in natural language. Sean Welleck, Jiacheng Liu, Le Ronan, Hanna Bras, Yejin Hajishirzi, Kyunghyun Choi, Cho, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. Joaquin Vanschoren and Sai-Kit Yeungthe Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hanna Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/ 2021/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract-round1.html.</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Tomás Mikolov, 4th International Conference on Learning Representations. Yoshua Bengio and Yann LeCunSan Juan, Puerto RicoConference Track ProceedingsJason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1502.05698.</p>
<p>Probable inference, the law of succession, and statistical inference. B Edwin, Wilson, J. Am. Stat. Assoc. 22158209Edwin B Wilson. Probable inference, the law of succession, and statistical inference. J. Am. Stat. Assoc., 22(158):209, June 1927.</p>
<p>The impact of symbolic representations on in-context learning for few-shot reasoning. Hanlin Zhang, Yi-Fan Zhang, Li Erran Li, Eric Xing, Neuro Causal and Symbolic AI Workshop at NeurIPS 2022, Virtual Workshop. To appearHanlin Zhang, Yi-Fan Zhang, Li Erran Li, and Eric Xing. The impact of symbolic representations on in-context learning for few-shot reasoning. In Neuro Causal and Symbolic AI Workshop at NeurIPS 2022, Virtual Workshop, December 9, 2022, 2022a. URL https://openreview.net/ pdf?id=qLgQpeQX3x1. To appear.</p>
<p>Each tumpus is a yumpus. Tumpuses are large. Numpuses are tumpuses. Numpuses are not cold. Every zumpus is a numpus. Dumpuses are not angry. Zumpuses are shy. Impuses are zumpuses. Impuses are luminous. Polly is an impus. True or false: Polly is angry. A: Expected answer: Polly is an impus. Impuses are zumpuses. Polly is a zumpus. Every zumpus is a numpus. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, 10.48550/arXiv.2205.11502On the paradox of learning to reason from data. CoRR, abs/2205.11502, 2022b. True Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Every tumpus is a yumpus. Polly is a yumpus. Yumpuses are vumpuses. Polly is a vumpus. Vumpuses are jompuses. Polly is a jompus. Each jompus is a rompus. Polly is a rompus. Rompuses are wumpuses. Polly is a wumpus. Wumpuses are not angry. Polly is not angry. False Sample average log probability: -0.07272354467015028Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the paradox of learning to reason from data. CoRR, abs/2205.11502, 2022b. doi: 10.48550/arXiv. 2205.11502. URL https://doi.org/10.48550/arXiv.2205.11502. Q: Each rompus is a wumpus. Every rompus is not opaque. Every jompus is a rompus. Every jompus is not sour. Vumpuses are jompuses. Each vumpus is not earthy. Yumpuses are vumpuses. Each yumpus is angry. Each tumpus is a yumpus. Tumpuses are large. Numpuses are tumpuses. Numpuses are not cold. Every zumpus is a numpus. Dumpuses are not angry. Zumpuses are shy. Impuses are zumpuses. Impuses are luminous. Polly is an impus. True or false: Polly is angry. A: Expected answer: Polly is an impus. Impuses are zumpuses. Polly is a zumpus. Every zumpus is a numpus. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Each tumpus is a yumpus. Polly is a yumpus. Each yumpus is angry. Polly is angry. True Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Every tumpus is a yumpus. Polly is a yumpus. Yumpuses are vumpuses. Polly is a vumpus. Vumpuses are jompuses. Polly is a jompus. Each jompus is a rompus. Polly is a rompus. Rompuses are wumpuses. Polly is a wumpus. Wumpuses are not angry. Polly is not angry. False Sample average log probability: -0.07272354467015028</p>
<p>Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Sample predicted answer: Polly is an impus. Impuses are zumpuses. Each tumpus is a yumpus. Polly is a yumpus. Yumpuses are angry. Polly is angry. True Sample average log probability: -0.04729328860082488Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Each tumpus is a yumpus. Polly is a yumpus. Yumpuses are angry. Polly is angry. True Sample average log probability: -0.04729328860082488</p>
<p>Each tumpus is a yumpus. Polly is a yumpus. Yumpuses are vumpuses. Polly is a vumpus. Vumpuses are jompuses. Polly is a jompus. Every jompus is a rompus. Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Polly is a rompus. Rompuses are not angry. Polly is not angry. False Sample average log probability: -0.02227105943358164Sample predicted answer: Polly is an impus. Impuses are zumpuses. Polly is a zum pus. Zumpuses are numpuses. Polly is a numpus. Numpuses are tumpuses. Polly is a tumpus. Each tumpus is a yumpus. Polly is a yumpus. Yumpuses are vumpuses. Polly is a vumpus. Vumpuses are jompuses. Polly is a jompus. Every jompus is a rompus. Polly is a rompus. Rompuses are not angry. Polly is not angry. False Sample average log probability: -0.02227105943358164</p>
<p>An example from PRONTOQA (5 hops with fictional ontology and top-down traversal direction) using self-consistency (each sample was produced using a temperature of 0.7), showing the expected and sample predicted answers from INSTRUCTGPT. Canonical steps are highlighted yellow, misleading steps purple, and invalid steps red. FIGURE. 11We note that the sample predicted CoTs that correspond to the gold proof are given lower overall probability than those that are incorrectFIGURE 11: An example from PRONTOQA (5 hops with fictional ontology and top-down traversal direction) using self-consistency (each sample was produced using a temperature of 0.7), showing the expected and sample predicted answers from INSTRUCTGPT. Canonical steps are highlighted yellow, misleading steps purple, and invalid steps red. We note that the sample predicted CoTs that correspond to the gold proof are given lower overall probability than those that are incorrect.</p>            </div>
        </div>

    </div>
</body>
</html>
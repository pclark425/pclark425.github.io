<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1809 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1809</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1809</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-104f75283ae9027eb478e7984bd26b680277ce6f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/104f75283ae9027eb478e7984bd26b680277ce6f" target="_blank">Robust Navigation with Language Pretraining and Stochastic Sampling</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper adapts large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions and proposes a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test.</p>
                <p><strong>Paper Abstract:</strong> Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-to-Room benchmark with 6% absolute gain over the previous best result (47% -> 53%) on the Success Rate weighted by Path Length metric.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1809.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1809.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRESS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained LanguagE model and Stochastic Sampling (PRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-and-language navigation agent that integrates large-scale pretrained language models (BERT/GPT) for instruction encoding and a stochastic action-sampling training scheme to reduce exposure bias, yielding improved generalization to unseen 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PRESS (PreSS in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Sequence-to-sequence attention-based VLN agent: instruction encoder uses contextualized word embeddings from large pretrained LMs (BERT or GPT) passed through an LSTM to produce textual features; an LSTM decoder with visual attention over panoramic ResNet features produces navigation actions. Training uses a two-stage scheme (freeze LM embeddings then fine-tune) and stochastic sampling between teacher and sampled actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale language corpora (text-only pretraining used to produce pretrained LMs such as BERT and GPT).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper states PRESS leverages large-scale pretrained language models (BERT and GPT) but does not specify the exact corpora or sizes used for LM pretraining within this work; it uses the off-the-shelf pretrained LMs as the source of contextualized embeddings and then fine-tunes them on the VLN task.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) Vision-and-Language Navigation (VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D embodied navigation in Matterport3D panoramic indoor environments where an agent, given natural language instruction(s), must navigate from a start location to a target location along a trajectory; success is defined as final location within 3 meters of target. The dataset contains panoramic views and human instructions paired with expert trajectories (7,189 trajectories; ~10,800 panoramic viewpoints).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable as a physical action space — LM pretraining is text-only (language-modeling objectives). In other words, pretraining involves predicting tokens (masked token prediction or next-token prediction) rather than embodied actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions (panoramic action space used in R2R: select one of discrete viewpoint headings/locations; trajectories up to ~7–20 steps depending on representation).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Learned mapping via attention and decoder LSTM: contextualized token embeddings (from pretrained LM) are attended to by the agent (producing instruction grounding vectors c_{i,t}) which, combined with visual attention over panoramic ResNet features and previous actions, are input to the LSTM decoder that outputs discrete navigation actions. There is no explicit symbolic-to-motor rule; mapping is learned end-to-end (with a two-stage fine-tuning procedure to avoid overfitting).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB image features extracted by a ResNet backbone provided by the R2R/Matterport3D setup; the agent uses panoramic visual features and one-hop visual attention (no additional sensors like depth explicitly specified).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>PRESS (using pretrained LMs + stochastic sampling) achieves on Test Unseen: Success Rate (SR) = 57%, SPL = 53%, Navigation Error (NE) = 4.53 m, Trajectory Length (TL) = 10.52 (Table 3). On Validation Unseen PRESS: SR = 59%, SPL = 55% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline SEQ2SEQ (no large-scale contextual LM pretraining) reported Test Unseen: SR = 20%, SPL = 18%, NE = 7.85 m, TL = 8.13 (Table 3). Other prior best methods (without this text-pretraining strategy) have lower SPL on Test Unseen (e.g., EnVDrop: SPL = 47%).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Contextualized embeddings from large pretrained LMs improved instruction understanding and robustness to lexical variability; two-stage fine-tuning reduced overfitting; stochastic sampling reduced exposure bias during sequence decoding, enabling recovery from mistakes; aggregation of multiple instructions via context mean-pooling also aided robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper notes potential overfitting if pretrained LM parameters are naively fine-tuned; exposure bias if teacher-forcing only; student-forcing can cause inefficient exploration. No direct perception-action gap failure is reported for PRESS, but the paper does not claim to close all perception-to-language grounding challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining language models on large unlabeled text and using their contextualized embeddings substantially improves generalization of instruction grounding in 3D embodied VLN tasks; combining LM pretraining with stochastic action sampling (mixing teacher and sampled actions during training) reduces exposure bias and yields state-of-the-art SPL on R2R (Test Unseen SPL 53%), with BERT-based encoders generally outperforming GPT-based or LSTM-only encoders on unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Navigation with Language Pretraining and Stochastic Sampling', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1809.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1809.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (as used in PRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (Bidirectional Encoder Representations from Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale pretrained bidirectional transformer language model whose contextualized token embeddings are used as the instruction embeddings in the PRESS VLN agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BERT (contextualized encoder used within PRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained bidirectional transformer that produces contextualized word embeddings; in PRESS it is used to produce token embeddings which are subsequently passed through an LSTM and attention mechanism to ground instructions to visual states.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale unlabeled text corpora (language modeling pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; the paper only states that PRESS leverages pretrained LMs such as BERT and fine-tunes them for VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) Vision-and-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See PRESS entry: panoramic indoor navigation in Matterport3D environments, goal is to follow natural language instructions to reach target positions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No physical actions; BERT pretraining is text-only (contextual token prediction / masked token objective).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions in R2R (see PRESS).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Contextualized token vectors from BERT are consumed by an LSTM instruction encoder and attended to by the decoder to produce grounded action distributions; mapping is learned end-to-end rather than hand-crafted.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB features from ResNet (same as PRESS).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When used in PRESS with stochastic sampling and two-stage training, yields Test Unseen SPL = 53% and SR = 57% (Table 3). Ablation indicates BERT generalizes better than GPT on unseen environments (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Compared to LSTM-only/seq2seq baselines (no contextual LM), those achieve lower Test Unseen SPL (e.g., seq2seq SPL = 18% on Test Unseen; see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Contextualized word representations capture lexical and syntactic context important for grounding instructions, reducing overfitting to training n-grams and improving robustness in unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Potential for overfitting if LM parameters are naively fine-tuned; requires careful two-stage training and smaller learning rate during LM fine-tuning as noted in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a pretrained bidirectional LM (BERT) for instruction encoding significantly improves VLN performance and generalization to unseen environments compared to non-contextual or from-scratch encoders; BERT-based PRESS yields the best reported unseen-environment SPL in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Navigation with Language Pretraining and Stochastic Sampling', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1809.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1809.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT (as used in PRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT (Generative Pretrained Transformer, Radford et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A (unidirectional) pretrained transformer language model whose contextualized token embeddings are tested as an instruction encoder input in the PRESS agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT (contextualized encoder used within PRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained unidirectional transformer model producing contextual token representations; used in PRESS as an alternative to BERT for producing instruction embeddings which are passed through LSTM and attention layers.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale unlabeled text corpora (language-model pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; the paper only references GPT as one of the candidate pretrained LMs used for instruction embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) Vision-and-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See PRESS entry.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No physical actions in pretraining; GPT is trained with next-token prediction on text.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions in R2R.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Same learned attention + decoder architecture as used with BERT; GPT embeddings are consumed by the downstream LSTM instruction encoder and decoder to produce navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB features (ResNet).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Ablation shows GPT improves over LSTM-only encoders but BERT generally generalizes better on unseen environments; exact GPT-only test-unseen numbers are not isolated as the final best PRESS result uses the best LM configuration (BERT). (Refer to Table 4 for relative trends.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>LSTM-only / seq2seq baselines perform worse (see seq2seq metrics in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Contextualization from pretrained transformer representations improved instruction grounding compared to non-pretrained LSTM encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>GPT (unidirectional) tended to generalize slightly worse than BERT (bidirectional) on unseen environments in this paper's ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformer-based pretrained LMs (both GPT and BERT) improve VLN performance relative to LSTM-only encoders; BERT showed better unseen-environment generalization than GPT in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Navigation with Language Pretraining and Stochastic Sampling', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1809.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1809.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GloVe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GloVe (Global Vectors for Word Representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained static word embeddings that prior VLN work used as input features for instruction encoding; mentioned as less context-sensitive than contextualized LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GloVe embeddings (used in prior VLN work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained static word vectors (non-contextual) used by prior VLN agents to represent instruction words.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large text corpora used to learn static word vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper references existing work leveraging pretrained GloVe embeddings but does not provide dataset specifics within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) Vision-and-Language Navigation (in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See PRESS entry; GloVe was used by earlier VLN methods as the word embedding layer for instruction encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No action space — GloVe is a static embedding model trained on text co-occurrence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions in R2R (as used by the VLN agents that consumed GloVe embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Static embeddings are input to an encoder (e.g., LSTM) that must learn to map text representations to actions via attention and decoder modules; contextual information is not encoded in the embeddings themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB features in the downstream VLN agents.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Not reported in isolation in this paper; paper notes that prior VLN work leveraging GloVe did not show comparable benefits to large-scale contextual LM pretraining for generalization to unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Provides lexical semantic priors that can help with known word meanings, but lacks contextual disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Non-contextual nature leads to poorer generalization to lexical variants and rare words compared to contextualized pretrained LMs, as noted by the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper positions GloVe as a weaker form of language pretraining for VLN relative to contextualized transformer LMs; contextual LMs (BERT/GPT) yield substantially better generalization to unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Navigation with Language Pretraining and Stochastic Sampling', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1809.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1809.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Speaker-Follower (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Speaker-Follower models for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior VLN approach that includes a speaker model to generate synthetic instructions for data augmentation and a follower model for navigation; mentioned in this paper as a competitive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>SPEAKER-FOLLOWER (baseline agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Two-part system: a speaker network that generates language instructions conditioned on trajectories (used for data augmentation) and a follower navigation model that maps instructions to actions; uses panoramic actions in R2R.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language generation model (speaker) trained on paired trajectories and instructions (supervised language data); the follower uses visual + textual training from R2R data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper beyond citation; the original speaker-follower method trains a speaker on R2R-style trajectory-to-instruction data and uses it for data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) Vision-and-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See PRESS entry; speaker-follower uses generated instructions to augment training for the follower agent navigating in Matterport3D panoramic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Speaker model produces natural language instructions; its 'action space' is token-level generation (textual).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>The follower agent acts in discrete panoramic navigation action space.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Speaker generates synthetic instructions for trajectories; follower learns to map instructions to discrete panoramic navigation actions via an encoder-decoder architecture. The mapping is learned via supervised training on (real + synthetic) instruction-trajectory pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB features (ResNet) for the follower model in the embodied navigation task.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in Table 3 as a baseline: Test Unseen SR = 35%, SPL = 28% (speaker-follower row in Table 3), indicating reasonable performance but lower than PRESS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Data augmentation via a learned speaker can provide additional instruction-trajectory pairs that help the follower generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Synthetic instructions may not fully capture the diversity of real instructions; reliance on the quality of the speaker model and mismatch between synthetic and human instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Speaker-generated augmentation is a useful prior approach for VLN, but in this paper the PRESS approach (textual LM pretraining + stochastic sampling) outperforms the speaker-follower baseline on unseen-environment SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Navigation with Language Pretraining and Stochastic Sampling', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Improving language understanding by generative pre-training <em>(Rating: 2)</em></li>
                <li>Speaker-follower models for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>GloVe: Global vectors for word representation <em>(Rating: 1)</em></li>
                <li>Learning to navigate unseen environments: Back translation with environmental dropout <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1809",
    "paper_id": "paper-104f75283ae9027eb478e7984bd26b680277ce6f",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "PRESS",
            "name_full": "Pretrained LanguagE model and Stochastic Sampling (PRESS)",
            "brief_description": "A vision-and-language navigation agent that integrates large-scale pretrained language models (BERT/GPT) for instruction encoding and a stochastic action-sampling training scheme to reduce exposure bias, yielding improved generalization to unseen 3D environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "PRESS (PreSS in paper)",
            "model_agent_description": "Sequence-to-sequence attention-based VLN agent: instruction encoder uses contextualized word embeddings from large pretrained LMs (BERT or GPT) passed through an LSTM to produce textual features; an LSTM decoder with visual attention over panoramic ResNet features produces navigation actions. Training uses a two-stage scheme (freeze LM embeddings then fine-tune) and stochastic sampling between teacher and sampled actions.",
            "pretraining_data_type": "Large-scale language corpora (text-only pretraining used to produce pretrained LMs such as BERT and GPT).",
            "pretraining_data_details": "The paper states PRESS leverages large-scale pretrained language models (BERT and GPT) but does not specify the exact corpora or sizes used for LM pretraining within this work; it uses the off-the-shelf pretrained LMs as the source of contextualized embeddings and then fine-tunes them on the VLN task.",
            "embodied_task_name": "Room-to-Room (R2R) Vision-and-Language Navigation (VLN)",
            "embodied_task_description": "3D embodied navigation in Matterport3D panoramic indoor environments where an agent, given natural language instruction(s), must navigate from a start location to a target location along a trajectory; success is defined as final location within 3 meters of target. The dataset contains panoramic views and human instructions paired with expert trajectories (7,189 trajectories; ~10,800 panoramic viewpoints).",
            "action_space_text": "Not applicable as a physical action space — LM pretraining is text-only (language-modeling objectives). In other words, pretraining involves predicting tokens (masked token prediction or next-token prediction) rather than embodied actions.",
            "action_space_embodied": "Discrete panoramic navigation actions (panoramic action space used in R2R: select one of discrete viewpoint headings/locations; trajectories up to ~7–20 steps depending on representation).",
            "action_mapping_method": "Learned mapping via attention and decoder LSTM: contextualized token embeddings (from pretrained LM) are attended to by the agent (producing instruction grounding vectors c_{i,t}) which, combined with visual attention over panoramic ResNet features and previous actions, are input to the LSTM decoder that outputs discrete navigation actions. There is no explicit symbolic-to-motor rule; mapping is learned end-to-end (with a two-stage fine-tuning procedure to avoid overfitting).",
            "perception_requirements": "Panoramic RGB image features extracted by a ResNet backbone provided by the R2R/Matterport3D setup; the agent uses panoramic visual features and one-hop visual attention (no additional sensors like depth explicitly specified).",
            "transfer_successful": true,
            "performance_with_pretraining": "PRESS (using pretrained LMs + stochastic sampling) achieves on Test Unseen: Success Rate (SR) = 57%, SPL = 53%, Navigation Error (NE) = 4.53 m, Trajectory Length (TL) = 10.52 (Table 3). On Validation Unseen PRESS: SR = 59%, SPL = 55% (Table 3).",
            "performance_without_pretraining": "Baseline SEQ2SEQ (no large-scale contextual LM pretraining) reported Test Unseen: SR = 20%, SPL = 18%, NE = 7.85 m, TL = 8.13 (Table 3). Other prior best methods (without this text-pretraining strategy) have lower SPL on Test Unseen (e.g., EnVDrop: SPL = 47%).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Contextualized embeddings from large pretrained LMs improved instruction understanding and robustness to lexical variability; two-stage fine-tuning reduced overfitting; stochastic sampling reduced exposure bias during sequence decoding, enabling recovery from mistakes; aggregation of multiple instructions via context mean-pooling also aided robustness.",
            "transfer_failure_factors": "Paper notes potential overfitting if pretrained LM parameters are naively fine-tuned; exposure bias if teacher-forcing only; student-forcing can cause inefficient exploration. No direct perception-action gap failure is reported for PRESS, but the paper does not claim to close all perception-to-language grounding challenges.",
            "key_findings": "Pretraining language models on large unlabeled text and using their contextualized embeddings substantially improves generalization of instruction grounding in 3D embodied VLN tasks; combining LM pretraining with stochastic action sampling (mixing teacher and sampled actions during training) reduces exposure bias and yields state-of-the-art SPL on R2R (Test Unseen SPL 53%), with BERT-based encoders generally outperforming GPT-based or LSTM-only encoders on unseen environments.",
            "uuid": "e1809.0",
            "source_info": {
                "paper_title": "Robust Navigation with Language Pretraining and Stochastic Sampling",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "BERT (as used in PRESS)",
            "name_full": "BERT (Bidirectional Encoder Representations from Transformers)",
            "brief_description": "A large-scale pretrained bidirectional transformer language model whose contextualized token embeddings are used as the instruction embeddings in the PRESS VLN agent.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "BERT (contextualized encoder used within PRESS)",
            "model_agent_description": "Pretrained bidirectional transformer that produces contextualized word embeddings; in PRESS it is used to produce token embeddings which are subsequently passed through an LSTM and attention mechanism to ground instructions to visual states.",
            "pretraining_data_type": "Large-scale unlabeled text corpora (language modeling pretraining).",
            "pretraining_data_details": "Not specified in this paper; the paper only states that PRESS leverages pretrained LMs such as BERT and fine-tunes them for VLN.",
            "embodied_task_name": "Room-to-Room (R2R) Vision-and-Language Navigation",
            "embodied_task_description": "See PRESS entry: panoramic indoor navigation in Matterport3D environments, goal is to follow natural language instructions to reach target positions.",
            "action_space_text": "No physical actions; BERT pretraining is text-only (contextual token prediction / masked token objective).",
            "action_space_embodied": "Discrete panoramic navigation actions in R2R (see PRESS).",
            "action_mapping_method": "Contextualized token vectors from BERT are consumed by an LSTM instruction encoder and attended to by the decoder to produce grounded action distributions; mapping is learned end-to-end rather than hand-crafted.",
            "perception_requirements": "Panoramic RGB features from ResNet (same as PRESS).",
            "transfer_successful": true,
            "performance_with_pretraining": "When used in PRESS with stochastic sampling and two-stage training, yields Test Unseen SPL = 53% and SR = 57% (Table 3). Ablation indicates BERT generalizes better than GPT on unseen environments (Table 4).",
            "performance_without_pretraining": "Compared to LSTM-only/seq2seq baselines (no contextual LM), those achieve lower Test Unseen SPL (e.g., seq2seq SPL = 18% on Test Unseen; see Table 3).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Contextualized word representations capture lexical and syntactic context important for grounding instructions, reducing overfitting to training n-grams and improving robustness in unseen environments.",
            "transfer_failure_factors": "Potential for overfitting if LM parameters are naively fine-tuned; requires careful two-stage training and smaller learning rate during LM fine-tuning as noted in paper.",
            "key_findings": "Using a pretrained bidirectional LM (BERT) for instruction encoding significantly improves VLN performance and generalization to unseen environments compared to non-contextual or from-scratch encoders; BERT-based PRESS yields the best reported unseen-environment SPL in this paper.",
            "uuid": "e1809.1",
            "source_info": {
                "paper_title": "Robust Navigation with Language Pretraining and Stochastic Sampling",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "GPT (as used in PRESS)",
            "name_full": "GPT (Generative Pretrained Transformer, Radford et al.)",
            "brief_description": "A (unidirectional) pretrained transformer language model whose contextualized token embeddings are tested as an instruction encoder input in the PRESS agent.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "GPT (contextualized encoder used within PRESS)",
            "model_agent_description": "Pretrained unidirectional transformer model producing contextual token representations; used in PRESS as an alternative to BERT for producing instruction embeddings which are passed through LSTM and attention layers.",
            "pretraining_data_type": "Large-scale unlabeled text corpora (language-model pretraining).",
            "pretraining_data_details": "Not specified in this paper; the paper only references GPT as one of the candidate pretrained LMs used for instruction embeddings.",
            "embodied_task_name": "Room-to-Room (R2R) Vision-and-Language Navigation",
            "embodied_task_description": "See PRESS entry.",
            "action_space_text": "No physical actions in pretraining; GPT is trained with next-token prediction on text.",
            "action_space_embodied": "Discrete panoramic navigation actions in R2R.",
            "action_mapping_method": "Same learned attention + decoder architecture as used with BERT; GPT embeddings are consumed by the downstream LSTM instruction encoder and decoder to produce navigation actions.",
            "perception_requirements": "Panoramic RGB features (ResNet).",
            "transfer_successful": true,
            "performance_with_pretraining": "Ablation shows GPT improves over LSTM-only encoders but BERT generally generalizes better on unseen environments; exact GPT-only test-unseen numbers are not isolated as the final best PRESS result uses the best LM configuration (BERT). (Refer to Table 4 for relative trends.)",
            "performance_without_pretraining": "LSTM-only / seq2seq baselines perform worse (see seq2seq metrics in Table 3).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Contextualization from pretrained transformer representations improved instruction grounding compared to non-pretrained LSTM encoders.",
            "transfer_failure_factors": "GPT (unidirectional) tended to generalize slightly worse than BERT (bidirectional) on unseen environments in this paper's ablation.",
            "key_findings": "Transformer-based pretrained LMs (both GPT and BERT) improve VLN performance relative to LSTM-only encoders; BERT showed better unseen-environment generalization than GPT in ablations.",
            "uuid": "e1809.2",
            "source_info": {
                "paper_title": "Robust Navigation with Language Pretraining and Stochastic Sampling",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "GloVe",
            "name_full": "GloVe (Global Vectors for Word Representation)",
            "brief_description": "Pretrained static word embeddings that prior VLN work used as input features for instruction encoding; mentioned as less context-sensitive than contextualized LMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "GloVe embeddings (used in prior VLN work)",
            "model_agent_description": "Pretrained static word vectors (non-contextual) used by prior VLN agents to represent instruction words.",
            "pretraining_data_type": "Large text corpora used to learn static word vectors.",
            "pretraining_data_details": "The paper references existing work leveraging pretrained GloVe embeddings but does not provide dataset specifics within this paper.",
            "embodied_task_name": "Room-to-Room (R2R) Vision-and-Language Navigation (in prior work)",
            "embodied_task_description": "See PRESS entry; GloVe was used by earlier VLN methods as the word embedding layer for instruction encoding.",
            "action_space_text": "No action space — GloVe is a static embedding model trained on text co-occurrence statistics.",
            "action_space_embodied": "Discrete panoramic navigation actions in R2R (as used by the VLN agents that consumed GloVe embeddings).",
            "action_mapping_method": "Static embeddings are input to an encoder (e.g., LSTM) that must learn to map text representations to actions via attention and decoder modules; contextual information is not encoded in the embeddings themselves.",
            "perception_requirements": "Panoramic RGB features in the downstream VLN agents.",
            "transfer_successful": null,
            "performance_with_pretraining": "Not reported in isolation in this paper; paper notes that prior VLN work leveraging GloVe did not show comparable benefits to large-scale contextual LM pretraining for generalization to unseen environments.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Provides lexical semantic priors that can help with known word meanings, but lacks contextual disambiguation.",
            "transfer_failure_factors": "Non-contextual nature leads to poorer generalization to lexical variants and rare words compared to contextualized pretrained LMs, as noted by the paper.",
            "key_findings": "The paper positions GloVe as a weaker form of language pretraining for VLN relative to contextualized transformer LMs; contextual LMs (BERT/GPT) yield substantially better generalization to unseen environments.",
            "uuid": "e1809.3",
            "source_info": {
                "paper_title": "Robust Navigation with Language Pretraining and Stochastic Sampling",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Speaker-Follower (baseline)",
            "name_full": "Speaker-Follower models for vision-and-language navigation",
            "brief_description": "A prior VLN approach that includes a speaker model to generate synthetic instructions for data augmentation and a follower model for navigation; mentioned in this paper as a competitive baseline.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "SPEAKER-FOLLOWER (baseline agent)",
            "model_agent_description": "Two-part system: a speaker network that generates language instructions conditioned on trajectories (used for data augmentation) and a follower navigation model that maps instructions to actions; uses panoramic actions in R2R.",
            "pretraining_data_type": "Language generation model (speaker) trained on paired trajectories and instructions (supervised language data); the follower uses visual + textual training from R2R data.",
            "pretraining_data_details": "Not specified in this paper beyond citation; the original speaker-follower method trains a speaker on R2R-style trajectory-to-instruction data and uses it for data augmentation.",
            "embodied_task_name": "Room-to-Room (R2R) Vision-and-Language Navigation",
            "embodied_task_description": "See PRESS entry; speaker-follower uses generated instructions to augment training for the follower agent navigating in Matterport3D panoramic environments.",
            "action_space_text": "Speaker model produces natural language instructions; its 'action space' is token-level generation (textual).",
            "action_space_embodied": "The follower agent acts in discrete panoramic navigation action space.",
            "action_mapping_method": "Speaker generates synthetic instructions for trajectories; follower learns to map instructions to discrete panoramic navigation actions via an encoder-decoder architecture. The mapping is learned via supervised training on (real + synthetic) instruction-trajectory pairs.",
            "perception_requirements": "Panoramic RGB features (ResNet) for the follower model in the embodied navigation task.",
            "transfer_successful": null,
            "performance_with_pretraining": "Reported in Table 3 as a baseline: Test Unseen SR = 35%, SPL = 28% (speaker-follower row in Table 3), indicating reasonable performance but lower than PRESS.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Data augmentation via a learned speaker can provide additional instruction-trajectory pairs that help the follower generalize.",
            "transfer_failure_factors": "Synthetic instructions may not fully capture the diversity of real instructions; reliance on the quality of the speaker model and mismatch between synthetic and human instructions.",
            "key_findings": "Speaker-generated augmentation is a useful prior approach for VLN, but in this paper the PRESS approach (textual LM pretraining + stochastic sampling) outperforms the speaker-follower baseline on unseen-environment SPL.",
            "uuid": "e1809.4",
            "source_info": {
                "paper_title": "Robust Navigation with Language Pretraining and Stochastic Sampling",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Improving language understanding by generative pre-training",
            "rating": 2
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "GloVe: Global vectors for word representation",
            "rating": 1
        },
        {
            "paper_title": "Learning to navigate unseen environments: Back translation with environmental dropout",
            "rating": 1
        }
    ],
    "cost": 0.01542875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Robust Navigation with Language Pretraining and Stochastic Sampling</h1>
<p>Xiujun Li ${ }^{\text {® }}$ Chunyuan Li ${ }^{\text {® }}$ Qiaolin Xia ${ }^{\text {® }}$ Yonatan Bisk ${ }^{\text {® }}{ }^{\circ}$<br>Asli Celikyilmaz ${ }^{\text {® }}$ Jianfeng Gao ${ }^{\text {® }}$ Noah A. Smith ${ }^{\text {® }}$ Yejin Choi ${ }^{\text {® }}$<br>${ }^{ \Delta}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{\text {® }}$ Peking University ${ }^{\circ}$ Microsoft Research AI ${ }^{\circ}$ Allen Institute for Artificial Intelligence<br>{xiujun, ybisk, nasmith, yejin}@cs.washington.edu<br>xql@pku.edu.cn {xiul, chunyl,jfgao}@microsoft.com</p>
<h4>Abstract</h4>
<p>Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-toRoom benchmark with $6 \%$ absolute gain over the previous best result ( $47 \% \rightarrow 53 \%$ ) on the Success Rate weighted by Path Length metric.</p>
<h2>1 Introduction</h2>
<p>The vision-and-language navigation (VLN) task, learning to navigate in visual environments based on natural language instructions, has attracted interest throughout the artificial intelligence research community (Hemachandra et al., 2015; Anderson et al., 2018; Chen et al., 2019; Savva et al., 2019). It fosters research on multimodal representations and reinforcement learning, and serves as a test bed for many real-world applications such as in-home robots.</p>
<p>In the recent Room-to-Room (R2R) VLN challenge (Anderson et al., 2018), most state-of-theart methods are developed based on an encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014), where a natural language instruction is represented as a sequence of words, and a navigation trajectory as a sequence of actions,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Two challenges in VLN.
enhanced with attention (Anderson et al., 2018; Wang et al., 2019; Fried et al., 2018; Ma et al., 2019a). Two important components are shared by all VLN agents: (i) an Instruction Encoder that employs a language model (LM) for instruction understanding; and (ii) an Action Decoder, where an appropriate sequence-level training scheme is required for sequential decision-making. Each component faces its own challenges (see Figure 1).</p>
<p>The first challenge is generalizing grounded natural language instruction understanding from seen to unseen environments. Specifically, in the R2R task, only $69 \%$ of bigrams are shared between training and evaluation. ${ }^{1}$ Existing work leverages pretrained GloVe embeddings (Pennington et al., 2014) to help generalize. In computer vision, it has been shown that large-scale models pretrained on ImageNet can transfer the knowledge to downstream applications (Yosinski et al., 2014), thus improving generalization. Comparable language-based transfer learning has not been shown for instruction understanding in VLN.</p>
<p>The second challenge is exposure bias (Ranzato et al., 2016) for the action decoder, due to the discrepancy between training and inference. This problem is common to many tasks where decoding is needed, including text generation, abstractive summarization, and machine translation (Ben-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">n-gram(s)</th>
<th style="text-align: center;">Validation Seen</th>
<th style="text-align: center;">Validation Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">87.2\%</td>
<td style="text-align: center;">80.7\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">77.4\%</td>
<td style="text-align: center;">68.9\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">65.6\%</td>
<td style="text-align: center;">57.3\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">50.8\%</td>
<td style="text-align: center;">44.4\%</td>
</tr>
</tbody>
</table>
<p>Table 1: N-grams instruction overlap statistics between validation seen and unseen environments.
gio et al., 2015). Two widely used training strategies are student-forcing and teacher-forcing (described in detail in Section 2.2). It is well-known that the sequence length determines which training strategy is more effective. In the VLN literature, student-forcing has been widely used, as early work (Anderson et al., 2018) used long trajectories (up to 20 steps) with a simple discrete action space. Most recent work, however, has relied on a panoramic action space (Fried et al., 2018) in which most trajectories are only up to seven steps long. In such cases, teacher-forcing is preferable (Tan et al., 2019). Neither strategy is perfect: teacher-forcing has exposure bias, while studentforcing's random actions can cause an agent to deviate far from the correct path, rendering the original instruction invalid. ${ }^{2}$</p>
<p>To tackle these challenges, we have developed two techniques to enable the agent to navigate more efficiently. For the first challenge, we leverage the recent large-scale pretrained language models, BERT (Devlin et al., 2019) and GPT (Radford et al., 2018), to improve the agent's robustness in unseen environments. We show that large-scale language-only pretraining improves generalization in grounded environments. For the second challenge, we propose a stochastic sampling scheme to balance teacher-forcing and student-forcing during training, so that the agent can recover from its own mistakes at inference time. As a result of combining both techniques, on the R2R benchmark test set, our agent (PRESS) ${ }^{3}$ achieves $53 \%$ on SPL, an absolute $6 \%$ gain over the current state of the art.</p>
<h2>2 Method</h2>
<p>In the VLN task, instructions are represented as a set $\mathcal{X}=\left{\boldsymbol{x}<em i="1">{i}\right}</em>$ of $M$ instructions per trajectory.}^{M</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of proposed methods.
Each instruction $\boldsymbol{x}<em i="i">{i}$ is a sequence of $L</em>}$ words, $\boldsymbol{x<em 1="1" i_="i,">{i}=\left[x</em>}, x_{i, 2}, \ldots, x_{i, L_{i}}\right]$. Given $\mathcal{X}$, the goal is to train an agent to navigate from a starting position $\boldsymbol{s<em 0="0">{0}$ to a target position, via completing a $T$ step trajectory $\tau=\left[\boldsymbol{s}</em>}, \boldsymbol{a<em 1="1">{0}, \boldsymbol{s}</em>}, \boldsymbol{a<em T="T">{1}, \cdots, \boldsymbol{s}</em>}, \boldsymbol{a<em t="t">{T}\right]$, where $\boldsymbol{s}</em>}$ and $\boldsymbol{a<em E="E">{t}$ are the visual state and navigation action, respectively, at step $t$. The training dataset $\mathcal{D}</em>$ :}={\boldsymbol{\tau}, \mathcal{X}}$ consists of example pairs of instruction set $\mathcal{X}$ and a corresponding expert trajectory $\boldsymbol{\tau}$. Our goal is to learn a policy $\pi_{\boldsymbol{\theta}}(\boldsymbol{\tau} \mid \mathcal{X})$ that maximizes the log-likelihood of the target trajectory $\boldsymbol{\tau}$ given instructions $\mathcal{X</p>
<p>$$
\log \pi_{\boldsymbol{\theta}}(\boldsymbol{\tau} \mid \mathcal{X})=\sum_{t=1}^{T} \log \pi_{\boldsymbol{\theta}}\left(\boldsymbol{a}<em t="t">{t} \mid \boldsymbol{s}</em>\right)
$$}, \mathcal{X</p>
<p>where $\boldsymbol{\theta}$ are trainable parameters. The policy is usually parameterized as an attention-based seq2seq model, with a language encoder $\boldsymbol{z}<em _theta__E="\theta_{E">{t}=$ $f</em>}}(\boldsymbol{x})$, and an action decoder $\boldsymbol{a<em _theta__D="\theta_{D">{t}=f</em>}}\left(\boldsymbol{z<em t="t">{t}, \boldsymbol{s}</em>}\right)$. Successful navigation depends on (i) precisely grounding the instructions $\mathcal{X}$ in $\tau$ in various environments, and (ii) correctly making the current decision $\boldsymbol{a<em _t="&lt;t">{t}$ based on previous actions/observations $\tau</em>}=\left[\boldsymbol{s<em 0="0">{0}, \boldsymbol{a}</em>\right]$. To address these concerns, we propose PreSS, illustrated in Figure 2.}, \cdots, \boldsymbol{s}_{t-1</p>
<h3>2.1 Instruction Understanding with Pretrained Language Models</h3>
<p>At each step $t$, the agent decides where to navigate by updating a dynamic understanding of the instructions $\boldsymbol{z}<em t="t">{t}$, according to its current visual state $\boldsymbol{s}</em>}$. Given instruction $\boldsymbol{x}$, the language encoder proceeds in two steps, end-to-end, by considering a function decomposition $f_{\theta_{E}}=f_{\boldsymbol{\theta<em _boldsymbol_theta="\boldsymbol{\theta">{x \rightarrow e}} \circ f</em>$ :}_{e \rightarrow s}</p>
<ul>
<li>$f_{\boldsymbol{\theta}<em 1="1">{x \rightarrow e}}: \boldsymbol{x} \rightarrow \boldsymbol{e}$, where $\boldsymbol{x}=\left[x</em>$;}, \cdots, x_{L}\right]$ is represented as its (contextualized) word embedding form $\boldsymbol{e}=\left[e_{1}, \cdots, e_{L}\right]$, with $e_{i}$ as the representation for word $x_{i</li>
<li>$f_{\boldsymbol{\theta}<em t="t">{e \rightarrow s}}: \boldsymbol{e} \rightarrow \boldsymbol{z}</em>}$ : For each embedded instruction $\boldsymbol{e}$, we ground its representations as $\boldsymbol{c<em t="t">{i, t}$ for state $\boldsymbol{s}</em>$ via neural attention. To handle</li>
</ul>
<p>language variability, one may aggregate features of multiple instructions $\mathcal{C}<em i_="i," t="t">{t}=\left{\boldsymbol{c}</em>\right}<em t="t">{i=1}^{M}$ into a single joint feature $\boldsymbol{z}</em>$}=\frac{1}{M} \sum_{i=1}^{M} \boldsymbol{c}_{i, t} .^{4</p>
<p>Previous methods in VLN learn $\boldsymbol{e}$ either from pretrained word embeddings (Pennington et al., 2014) which do not take into account word context, or from scratch. As a result, their representations do not capture contextual information within each instruction. More importantly, they tend to overfit the training instructions associated with seen environments, limiting their utility in unseen environments. To remedy these issues, we propose to represent $\boldsymbol{e}$ with contextualized word embeddings produced using large-scale pretrained language models, such as BERT and GPT.</p>
<p>Instruction Encoder. The agent's memory vector $\boldsymbol{h}<em _boldsymbol_theta="\boldsymbol{\theta">{t-1}$ captures the perception and action history and is used to attend to the instruction $\boldsymbol{x}$. A pretrained LM $f</em><em 1="1">{x \rightarrow e}}$ encodes the instruction $e=\left[\boldsymbol{e}</em>}, \cdots, \boldsymbol{e<em i="i">{L}\right] ; \boldsymbol{e}</em>}$ where the representation for word $x_{i}$, is built with $f_{\boldsymbol{\theta<em _rightarrow="\rightarrow" e="e" x="x">{x \rightarrow e}} \in{$ GPT, BERT $}$, and $\boldsymbol{\theta}</em>}$ are fine-tuned parameters. The embedded words $e=\left[\boldsymbol{e<em L="L">{1}, \cdots, \boldsymbol{e}</em>}\right]$ are passed through an LSTM $f_{\boldsymbol{\theta<em 1="1">{e \rightarrow s}}$ to produce a sequence of textual features $\left[\boldsymbol{h}</em>$ is computed as weighted sum of textual features in the sequence:}^{e}, \cdots, \boldsymbol{h}_{L}^{e}\right]$. At each time step $t$, the textual context for the instruction $\boldsymbol{x</p>
<p>$$
\boldsymbol{c}<em l="1">{i, t}=\sum</em>
$$}^{L} \alpha_{l} \boldsymbol{h}_{l}^{e</p>
<p>where $\alpha_{l}=\operatorname{Softmax}\left(\boldsymbol{h}<em l="l">{t}^{\top} \boldsymbol{h}</em>$ places more weight on the word representations that are most relevant to the agent's current status.}^{e}\right), \alpha_{l</p>
<p>Decoder. At each step, the agent takes an action $a_{t}$, and the environment returns new visual observations; the agent first performs one-hop visual attention $f(\cdot)$ to all the visual image features $s_{t}$, based on its previous memory vector $\boldsymbol{h}<em t="t">{t-1}$. Then, the agent updates its visual state $\boldsymbol{s}</em>}$ as the weighted sum of the panoramic features, $\boldsymbol{s<em j="j">{t}=\sum</em>} \gamma_{t, j} \boldsymbol{s<em j="j" t_="t,">{t, j}$. The attention weight $\gamma</em>}$ for the $j$-th visual feature $\boldsymbol{s<em t-1="t-1">{t, j}$ represents its importance with respect to the previous history context $\boldsymbol{h}</em>}$, computed as $\gamma_{t, j}=$ $\operatorname{Softmax}\left(\left(\mathbf{W<em t-1="t-1">{h} \boldsymbol{h}</em>}\right)^{\top} \mathbf{W<em j="j" t_="t,">{s} \boldsymbol{s}</em>\right)$,}\right)$ (Fried et al., 2018) where $\operatorname{Softmax}\left(r_{j}\right)=\exp \left(r_{j}\right) / \sum_{j^{\prime}} \exp \left(r_{j^{\prime}</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\mathbf{W}<em s="s">{h}$ and $\mathbf{W}</em>$ are trainable projection matrices.</p>
<p>$$
\boldsymbol{h}<em _boldsymbol_theta="\boldsymbol{\theta">{t}=f</em><em t="t">{D}}\left(\left[\boldsymbol{s}</em>}, \boldsymbol{a<em t-1="t-1">{t-1}\right], \boldsymbol{h}</em>\right)
$$</p>
<p>where $\boldsymbol{a}<em D="D">{t-1}$ is the action taken at previous step, and $\boldsymbol{\theta}</em>$ are the LSTM decoder parameters.</p>
<p>Two-stage learning. The parameters of our agent are $\boldsymbol{\theta}=\left{\boldsymbol{\theta}<em _rightarrow="\rightarrow" e="e" z="z">{x \rightarrow e}, \boldsymbol{\theta}</em>}, \theta_{D}\right}$. In practice, we find that the agent overfits quickly, when the full model is naively fine-tuned, with $\boldsymbol{\theta<em _rightarrow="\rightarrow" e="e" x="x">{x \rightarrow e}$ initialized by pretrained LMs (e.g., BERT). In this paper, we consider a two-stage learning scheme to facilitate the use of pretrained LMs for VLN. (i) Embedding-based stage: We fix $\boldsymbol{\theta}</em>}$, and use BERT or GPT to provide instruction embeddings. Only $\left{\boldsymbol{\theta<em D="D">{e \rightarrow z}, \boldsymbol{\theta}</em>$ can adapt to our VLN task.}\right}$ are updated (while tuning on validation). (ii) Fine-tuning stage: We train all model parameters $\boldsymbol{\theta}$ with a smaller learning rate, so that $\boldsymbol{\theta}_{x \rightarrow e</p>
<h3>2.2 Stochastic Action Sampling</h3>
<p>A core question is how to learn useful state representations $\boldsymbol{s}_{t}$ in Eq. (1) during the trajectory rollout. In other words, which action should we use to interact with the environment to elicit the next state? As noted, most existing work uses one of two schemes: (i) Teacher-forcing (TF), where the agent takes ground-truth actions $\boldsymbol{a}^{\mathrm{T}}$ only. Though TF enables efficient training, it results in "exposure bias" because agents must follow learned rather than gold trajectories at test time. In contrast, (ii) Student-forcing (SF), where an action $\boldsymbol{a}^{\mathrm{s}}$ is drawn from the current learned policy, allows the agent to learn from its own actions (aligning training and evaluation), however, it is inefficient, as the agent explores randomly when confused or in the early stages of training.</p>
<p>In this work, we consider a stochastic scheme (SS) to alternate between choosing actions from $\boldsymbol{a}^{\mathrm{T}}$ and $\boldsymbol{a}^{\mathrm{s}}$ for state transition $\boldsymbol{s} \leftarrow g\left(\boldsymbol{a}^{\mathrm{T}}, \boldsymbol{a}^{\mathrm{s}}\right)$, inspired by scheduled sampling (Bengio et al., 2015). As illustrated in Figure 2, at each step, the agent "flips a coin" with some probability $\epsilon$ to decide whether to take the teacher's action $a^{\mathrm{T}}$ or a sampled one $a^{\mathrm{s}}$ :</p>
<p>$$
\boldsymbol{a}=\delta \boldsymbol{a}^{\mathrm{T}}+(1-\delta) \boldsymbol{a}^{\mathrm{s}}
$$</p>
<p>where $\delta \sim \operatorname{Bernoulli}(\epsilon)$. This allows the agent to leverage the advantages of both TF and SF, yielding a faster and less biased learner. We fix $\epsilon$ as a constant during learning, which is different from the decaying schedule in (Bengio et al., 2015).</p>
<h2>3 Experiments</h2>
<h3>3.1 Dataset</h3>
<p>We use the Room-to-Room dataset for the VLN task, built upon the Matterport3D dataset (Chang et al., 2017), which consists of 10,800 panoramic views and 7,189 trajectories. Each trajectory is paired with three natural language instructions. The R2R dataset consists of four splits: train seen, validation seen, validation unseen, and test unseen. There is no overlap between seen and unseen environments. At the beginning of each episode, the agent starts at a specific location, and is given natural instructions, the goal of the agent is to navigate to the target location as quickly as possible.</p>
<h3>3.2 Baseline Systems</h3>
<p>We compare our approach with eight recently published systems:</p>
<ul>
<li>RANDOM: an agent that randomly selects a direction and moves five step in that direction (Anderson et al., 2018).</li>
<li>SEQ2SEQ: sequence-to-sequence model proposed by Anderson et al. as a baseline for the R2R benchmark (Anderson et al., 2018) and analyzed in (Thomason et al., 2019).</li>
<li>RPA (Wang et al., 2018): is an agent which combines model-free and model-based reinforcement learning, using a look-ahead module for planning.</li>
<li>SPEAKER-FOLLOWER (Fried et al., 2018): an agent trained with data augmentation from a speaker model with panoramic actions.</li>
<li>SmNA (Ma et al., 2019a): an agent trained with a visual-textual co-grounding module and progress monitor on panoramic actions.</li>
<li>RCM+SIL(TRAIN) (Wang et al., 2019): an agent trained with cross-modal grounding locally and globally via reinforcement learning.</li>
<li>REGRETFUL (Ma et al., 2019b): an agent with a trained progress monitor heuristic for search that enables backtracking.</li>
<li>FAST (Ke et al., 2019): an agent which combines global and local knowledge to compare partial trajectories of different lengths, enabling efficient backtrack after a mistake.</li>
<li>EnVDrop (Tan et al., 2019): proposed an environment dropout method, which can generate more environments based on the limited seen environments.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Agent</th>
<th style="text-align: center;">Validation Seen <br> SR $\uparrow$</th>
<th style="text-align: center;">SPL $\uparrow$</th>
<th style="text-align: center;">Validation Unseen <br> SR $\uparrow$</th>
<th style="text-align: center;">SPL $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">seq2seq <br> PRESS</td>
<td style="text-align: center;">51 <br> $47(-4)$</td>
<td style="text-align: center;">46 <br> $43(-3)$</td>
<td style="text-align: center;">32 <br> $43(+11)$</td>
<td style="text-align: center;">25 <br> $38(+13)$</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">seq2seq <br> PRESS</td>
<td style="text-align: center;">49 <br> $56(+7)$</td>
<td style="text-align: center;">44 <br> $53(+9)$</td>
<td style="text-align: center;">33 <br> $56(+23)$</td>
<td style="text-align: center;">26 <br> $50(+24)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of PRESS and seq2seq.</p>
<h3>3.3 Evaluation Metrics</h3>
<p>We benchmark our agent on the following metrics:
TL Trajectory Length measures the average length of the navigation trajectory.
NE Navigation Error is the mean of the shortest path distance in meters between the agent's final location and the target location.
SR Success Rate with which the agent's final location is less than 3 meters from the target.
SPL Success weighted by Path Length trades-off SR against TL.
SPL is the recommended primary metric, other metrics are considered as auxiliary measures.</p>
<h3>3.4 Implementation</h3>
<p>We use a LSTM/GPT/BERT for the language encoder, and a second single-layer LSTM for the action decoder ( $\mathrm{h}=1024$ ). We use Adamax and batch sizes of $24 / 16$ for pretraining/finetuning. The learning rates for MLE are $1 e^{-4}$, during finetuning BERT the learning rate is $5 e^{-5}$. Following (Fried et al., 2018), we use a panoramic action space and the ResNet image features provided by (Anderson et al., 2018). The code is publicly available here: https://github.com/xjli/r2r_vln.</p>
<h3>3.5 Results</h3>
<p>Robust Generalization. First, we compare PRESS to a baseline seq2seq model ${ }^{5}$ in two evaluation settings on the validation splits: (1) S: A single instruction is provided to the agent at a time. Thus, three separate navigation trajectories are generated corresponding to three alternative instructions in this setting. We report the averaged performance over three separate runs. (2) M: All three instructions are provided to the agent at once. The seq2seq baseline does not have an aggregation strategy so we report its performance for the single trajectory with maximum likelihood. For PRESS, we aggregate the instructions via context</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Validation Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Validation Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TL $\downarrow$</td>
<td style="text-align: center;">NE $\downarrow$</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">TL $\downarrow$</td>
<td style="text-align: center;">NE $\downarrow$</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">TL $\downarrow$</td>
<td style="text-align: center;">NE $\downarrow$</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">9.58</td>
<td style="text-align: center;">9.45</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.77</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.93</td>
<td style="text-align: center;">9.77</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">SEQ2SEQ</td>
<td style="text-align: center;">11.33</td>
<td style="text-align: center;">6.01</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.39</td>
<td style="text-align: center;">7.81</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.13</td>
<td style="text-align: center;">7.85</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;">RPA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.65</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">7.53</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: center;">SPEAKER-FOLLOWER</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.36</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.62</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.82</td>
<td style="text-align: center;">6.62</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">28</td>
</tr>
<tr>
<td style="text-align: center;">SMNA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.04</td>
<td style="text-align: center;">5.67</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;">RCM+SIL(TRAIN)</td>
<td style="text-align: center;">10.65</td>
<td style="text-align: center;">3.53</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.46</td>
<td style="text-align: center;">6.09</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.97</td>
<td style="text-align: center;">6.12</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">38</td>
</tr>
<tr>
<td style="text-align: center;">REGRETFUL</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.32</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">13.69</td>
<td style="text-align: center;">5.69</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">FAST</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.17</td>
<td style="text-align: center;">4.97</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">22.08</td>
<td style="text-align: center;">5.14</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">41</td>
</tr>
<tr>
<td style="text-align: center;">ENVDROP</td>
<td style="text-align: center;">11.00</td>
<td style="text-align: center;">3.99</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">10.70</td>
<td style="text-align: center;">5.22</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">11.66</td>
<td style="text-align: center;">5.23</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">47</td>
</tr>
<tr>
<td style="text-align: center;">PRESS</td>
<td style="text-align: center;">10.35</td>
<td style="text-align: center;">3.09</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">10.06</td>
<td style="text-align: center;">4.31</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">10.52</td>
<td style="text-align: center;">4.53</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.85</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">76</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison with the state-of-the-art methods. Blue indicates best value overall.
mean-pooling and generate a single trajectory. No data augmentation is applied to either model.</p>
<p>The results are summarized in Table 2. (i) PRESS drastically outperforms the seq2seq models on unseen environments in both settings, and (ii) Interestingly, our method shows a much smaller gap between seen and unseen environments than seq2seq. It demonstrates the importance of pretrained LMs and stochastic sampling for strong generalization in unseen environments.</p>
<p>Comparison with SoTA. In Table 3, we compare the performance of our agent against all the published methods, our PRESS agent outperforms the existing models on nearly all the metrics.</p>
<p>Ablation Analysis. Key to this work is leveraging large-scale pretrained LMs and effective training strategies for action sequence decoding. Table 4 shows an ablation of these choices. (1) BERT and GPT are better than LSTM on both seen and unseen environments, and BERT generalizes better than GPT on unseen environments. (2) Teacher-forcing performs better than studentforcing on validation unseen environments, while an opposite conclusion is drawn on validation seen environments. SS performs the best on unseen environments.</p>
<p>Qualitative Examples. We provide two navigation examples of PRESS on the validation unseen environments with the step-by-step views and topdown views in Appendix.
(1) Figure 3 shows how the agent with LSTM instruction encoder performs compared with our PRESS agent. There are two rare words "mannequins" and "manikins" which are not in the training dataset and confuse the LSTM agent, while, PRESS successfully maps these two "mannequins" and "manikins" to the correct objects.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">Validation Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Validation Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">NE</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">SPL</td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">NE</td>
<td style="text-align: center;">SR SPL</td>
</tr>
<tr>
<td style="text-align: center;">TF</td>
<td style="text-align: center;">10.50</td>
<td style="text-align: center;">5.74</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">9.86</td>
<td style="text-align: center;">6.23</td>
<td style="text-align: center;">42</td>
</tr>
<tr>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">11.87</td>
<td style="text-align: center;">3.97</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">13.23</td>
<td style="text-align: center;">6.17</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">SS</td>
<td style="text-align: center;">10.99</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">10.73</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;">TF</td>
<td style="text-align: center;">10.03</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">9.43</td>
<td style="text-align: center;">3.36</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">11.46</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">13.13</td>
<td style="text-align: center;">5.13</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;">SS</td>
<td style="text-align: center;">10.60</td>
<td style="text-align: center;">2.99</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">10.79</td>
<td style="text-align: center;">3.05</td>
<td style="text-align: center;">56</td>
</tr>
<tr>
<td style="text-align: center;">TF</td>
<td style="text-align: center;">10.57</td>
<td style="text-align: center;">4.06</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">9.61</td>
<td style="text-align: center;">5.13</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">12.39</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">13.12</td>
<td style="text-align: center;">5.06</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">SS</td>
<td style="text-align: center;">10.35</td>
<td style="text-align: center;">3.09</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">10.06</td>
<td style="text-align: center;">4.31</td>
<td style="text-align: center;">59</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation results of different language pretrainings and training strategies: Teacher Forcing (TF), Student Forcing (SF) and Stochastic Sampling (SS).
(2) The second set in Figure 4 shows how the agents trained with different training strategies performs in an unseen environment. The agents trained with teacher-forcing and student-forcing both fail, while PRESS succeeds.</p>
<h2>4 Conclusion</h2>
<p>We present PRESS, a navigation agent based on two previously underexplored techniques in VLN: pretrained language models and stochastic action sampling. Our PRESS demonstrates robust generalization in the unseen environments, leading to a new state-of-the-art performance over many of the much more complex approaches previously proposed. As both the components of PRESS can be easily integrated, future models can consider building upon them as a strong baseline system.</p>
<h2>Acknowledgments</h2>
<p>We thank the anonymous reviewers for their insightful comments, NSF IIS-1703166, DARPA's CwC program through ARO W911NF-15-1-0543, and the Allen Institute for Artificial Intelligence.</p>
<h2>References</h2>
<p>Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In IEEE Conference on Computer Vision and Pattern Recognition.</p>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Neural Information Processing Systems.</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017. Matterport3D: Learning from RGB-D data in indoor environments. In International Conference on 3D Vision.</p>
<p>Howard Chen, Alane Shur, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2019. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In IEEE Conference on Computer Vision and Pattern Recognition.</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. 2018. Speaker-follower models for vision-and-language navigation. In Neural Information Processing Systems.</p>
<p>Sachithra Hemachandra, Felix Duvallet, Thomas M Howard, Nicholas Roy, Anthony Stentz, and Matthew R Walter. 2015. Learning models for following natural language directions in unknown environments. In IEEE International Conference on Robotics and Automation.</p>
<p>Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. 2019. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition.</p>
<p>Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming</p>
<p>Xiong. 2019a. Self-monitoring navigation agent via auxiliary progress estimation. In International Conference on Learning Representations.</p>
<p>Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. 2019b. The regretful agent: Heuristic-aided navigation through progress estimation. In IEEE Conference on Computer Vision and Pattern Recognition.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Conference on empirical methods in natural language processing.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level training with recurrent neural networks. In International Conference on Learning Representations.</p>
<p>Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. 2019. Habitat: A platform for embodied ai research. In International Conference on Computer Vision.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Neural Information Processing Systems.</p>
<p>Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learning to navigate unseen environments: Back translation with environmental dropout. In the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Jesse Thomason, Daniel Gordon, and Yonatan Bisk. 2019. Shifting the Baseline: Single Modality Performance on Visual Navigation \&amp; QA. In the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. 2019. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition.</p>
<p>Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. 2018. Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-andlanguage navigation. In IEEE European Conference on Computer Vision.</p>
<p>Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Neural Information Processing Systems.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The baseline seq2seq agent is the FollowER of SPEAKER-FOLLOWER (Fried et al., 2018).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
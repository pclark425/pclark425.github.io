<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1898 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1898</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1898</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280546737</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.04931v1.pdf" target="_blank">Intention: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM</a></p>
                <p><strong>Paper Abstract:</strong> Traditional control and planning for robotic manipulation heavily rely on precise physical models and predefined action sequences. While effective in structured environments, such approaches often fail in real-world scenarios due to modeling inaccuracies and struggle to generalize to novel tasks. In contrast, humans intuitively interact with their surroundings, demonstrating remarkable adaptability, making efficient decisions through implicit physical understanding. In this work, we propose INTENTION, a novel framework enabling robots with learned interactive intuition and autonomous manipulation in diverse scenarios, by integrating Vision-Language Models (VLMs) based scene reasoning with interaction-driven memory. We introduce Memory Graph to record scenes from previous task interactions which embodies human-like understanding and decision-making about different tasks in real world. Meanwhile, we design an Intuitive Perceptor that extracts physical relations and affordances from visual scenes. Together, these components empower robots to infer appropriate interaction behaviors in new scenes without relying on repetitive instructions. Videos: https://robo-intention.github.io</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1898.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1898.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INTENTION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>INTENTION (Interactive Intuition + Grounded VLM with MemoGraph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that combines a pre-trained vision-language model (VLM) Intuitive Perceptor, a graph-structured episodic memory (MemoGraph), and a VLM-based planner to enable humanoid robots to infer manipulation behaviors from visual scenes and prior interactions in real-world settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>INTENTION (VLM-based Intuitive Perceptor + MemoGraph + VLM planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system-level agent that (1) uses a pre-trained multimodal VLM as the Intuitive Perceptor to extract objects, spatial relations and affordance-like attributes from RGB images (via prompting and function-calling into a JSON output template), (2) encodes scene/instruction/action/feedback into graph-structured episodic memories (MemoGraph) using CLIP embeddings for nodes/links/instructions, and (3) performs graph-matching + a language-model based evaluator to retrieve and adapt prior actions from a motion library for execution on a real humanoid robot.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>uses pre-trained multimodal VLM (proprietary OpenAI model) and CLIP text-image embeddings; system-level components (MemoGraph, graph-matching, motion library) are not pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>The paper does not provide explicit details on the pretraining corpus for the proprietary VLM (o4-mini). CLIP is used as an encoder for nodes/links/instructions (CLIP is known to be trained on image–text pairs, but the paper does not enumerate the CLIP pretraining data). The paper does not claim or analyze specific pretraining coverage of action verbs, affordances, or 3D dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Humanoid loco-manipulation (real-world manipulation and interactive-intuition tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world humanoid manipulation on a 37-DOF centaur-like robot (two arms with claw gripper, torso and wheeled legs). Tasks include pick-and-place (standard manipulation) and interactive-intuition tasks such as handling an object handed by a human, collaborative lift-desk, push-chair assistance, and refill-tea. Action execution uses a library of predefined motion primitives (continuous robot control via Cartesian I/O) with planner selecting primitives; experiments run on hardware (RealSense camera + torque sensors).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper describes using VLM outputs to create semantic scene graphs (nodes = objects/agents, links = spatial/semantic relations) and embeddings (CLIP) for graph-matching; it emphasizes alignment between VLM-extracted semantics and robot action selection but does not quantify overlap between pretraining corpora and task objects/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>System-level experimental results (real robot): INTENTION achieved 92% success on the standard manipulation tasks and 72% success on interactive-intuition tasks (Table I). For manipulation: planning rate 96%, success rate 92%, average time 23.75s ± 4.3. For interactive-intuition tasks: planning rate 84%, success rate 72%, average time 24.86s ± 2.7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct ablation removing the VLM pretraining is reported. Baseline methods tested include WB-MPC, BT-Planner, and LLM-BT (LLM-based planner), but none are a direct 'no language pretraining' ablation of the same pipeline. Therefore no numeric baseline of INTENTION without language-pretraining is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper reports qualitative/frequency-based evidence: success probability of INTENTION improves with increasing number of stored task graphs in MemoGraph (figures reported qualitatively), and mentions 'few-shot training for deployment' as a design goal, but does not provide quantitative episodes/demonstrations-to-performance comparisons between language-pretrained vs non-language baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No analysis of attention maps or per-token/region attention visualizations is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No in-depth embedding-space analyses (e.g., clustering, PCA, attractor analysis) are reported beyond using CLIP embeddings for node/link/instruction similarity and the use of cosine similarity for instruction similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral-level evidence: INTENTION's pipeline grounds VLM-extracted semantic/spatial relations into action selection via graph-matching and retrieving previously executed motion primitives stored with feedback; empirical evidence is that INTENTION attains substantially higher success in 'intuitive' tasks (72%) compared to the LLM-BT baseline (15%), and maintains ≥65% success in 'Intuitive Mode' (no explicit instruction). No mechanistic analyses (e.g., learned mapping of verb embeddings to motor primitives) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit multi-level feature analysis (low-level vs high-level) is reported. The system relies on high-level semantic graph features (nodes/links/text embeddings) for matching and action retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper describes that transfer success depends on similarity between the current scene graph and stored task graphs; matching uses node similarity, link similarity, and instruction similarity (weighted by α, β, γ). Empirically, more stored graphs improve success; transfer works better when prior memories contain similar topologies and semantic contexts. No numeric thresholds or domain-shift experiments (e.g., large visual domain gap) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No explicit experimental comparison between objects/actions present in pretraining data vs novel objects; the paper evaluates 'novel task scenarios' in the sense of new scenes relative to MemoGraph, but does not attribute performance to pretraining familiarity.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>INTENTION demonstrates 'Intuitive Mode' where no explicit semantic instruction is given (zero-instruction inference), achieving ≥65% success across tasks and 72% on interactive-intuition tasks overall, indicating ability to infer behaviors from perception and memory without new instruction examples. The paper also claims few-shot deployment as a design goal but provides no quantitative few-shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise or component ablation (e.g., freezing VLM layers, removing CLIP embeddings) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>The paper does not report cases where the VLM-driven pipeline harmfuly reduced performance relative to baselines; negative transfer is not explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct empirical comparison between vision-language pretraining and vision-only pretraining (e.g., ImageNet or self-supervised vision) is reported for the same pipeline. CLIP is used as text encoder, but no ablation versus vision-only encoder is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>The paper discusses that performance improves as MemoGraph grows (more stored task graphs), but does not provide detailed learning curves over fine-tuning epochs or representational change across training iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No analysis of effective dimensionality (PCA / intrinsic dimension) of representations is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1898.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1898.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o4-mini (VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o4-mini (OpenAI multimodal model used as VLM in Intuitive Perceptor and evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary OpenAI multimodal model (accessed via API) used as the Intuitive Perceptor and language-model-based evaluator; it is prompted with function-calling to extract structured scene graphs (JSON) from RGB images and instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o4-mini (OpenAI VLM) as Intuitive Perceptor and evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary pre-trained multimodal language-vision model (accessed via OpenAI API) used with prompting and function-calling to produce structured scene graph outputs and to evaluate plausibility of graph matches; processes RGB image content plus textual instruction prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>pre-trained multimodal vision-language (proprietary) model (exact pretraining details not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in the paper; the paper does not provide corpus statistics or detail on inclusion of affordance/action verbs or 3D data. The model is used as-is through API.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Scene understanding and graph extraction for real-world humanoid manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Generation of structured scene graphs (objects, spatial relations, attributes) from RGB images and optional instructions; outputs are used to populate MemoGraph and to guide action selection on a 37-DOF humanoid robot in real-world tasks (pick-and-place and interactive intuition tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper asserts the VLM provides semantic and spatial information useful for downstream mapping to actions; no quantitative alignment metrics between pretraining content and task domain are given.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Used as a component — system-level success reported for INTENTION (see INTENTION entry). The paper does not report ablation where o4-mini is removed or replaced with a non-language-augmented model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported for the specific VLM component.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention or input attribution analyses for o4-mini are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding space analyses for o4-mini are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used to output affordance-like relations and attributes that are then linked to motion primitives through MemoGraph; grounding evidence is behavioral (system success) rather than mechanistic.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No hierarchical feature analysis is presented for o4-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not specifically analyzed for o4-mini; system-level transfer depends on matching of VLM-extracted graphs to stored memory.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed specifically for o4-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>o4-mini is used zero-shot via prompting/function-calling (no fine-tuning reported) to extract scene graphs. The system demonstrates zero-instruction inference at higher system level.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1898.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1898.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language embedding model used in this work as the encoder for node, link, and instruction embeddings in the task graphs for graph matching and similarity computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A joint image-text embedding model (image encoder + text encoder) producing embeddings used for computing cosine similarities between nodes, links and instructions in graph-based memory retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on image–text pairs (paper uses pre-trained CLIP as an encoder; the INTENTION paper itself does not retrain CLIP).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in the paper; CLIP is used to embed textual node/link/instruction content for matching (the paper does not characterize the CLIP pretraining corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Graph-embedding and similarity computation for MemoGraph retrieval in robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Encoding nodes (objects/agents), links (spatial/semantic relations), and instructions into embeddings used to compute pairwise similarity matrices for graph matching between current scene graphs and stored task graphs; supports selection of motion primitives for real-robot execution.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>CLIP is used as a generic text-image embedding for semantic alignment between graph textual content and current scene; the paper does not quantify coverage of task-specific vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>No direct performance numbers attributable solely to CLIP; CLIP is a component in the INTENTION pipeline whose system-level performance is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analyses of CLIP features are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>CLIP embeddings are used operationally (cosine similarity, thresholding, bipartite matching) but no further analysis of embedding structure is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>CLIP embeddings serve as the representational substrate by which symbolic scene graph elements are matched across episodes; grounding evidence is indirect (improved behavior retrieval), not a direct mapping analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performance depends on how well CLIP embeddings capture similarity between current graph textual descriptions and stored graphs; no quantitative study reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used zero-shot as an encoder; no fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1898.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1898.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-BT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-BT (LLM-based Behavior Tree planner) - baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that leverages large language models to generate behavior trees (BT) that are then executed by lower-level controllers; evaluated as a comparator to INTENTION on humanoid manipulation and intuition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-BT (baseline high-level planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A language-model-driven planner that outputs behavior-tree structured plans (high-level) to control low-level execution; in this paper it is used as a baseline for autonomy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>pre-trained large language model (text-only LLM used as high-level planner) — the paper does not specify the exact LLM or its pretraining corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in the paper; assumed standard LLM pretraining on text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Humanoid task planning for manipulation and interactive-intuition tasks (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>High-level plan generation (behavior trees) for manipulation tasks executed on the same humanoid platform; evaluated on pick-and-place and interactive intuition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>LLM-BT provides high-level semantic plans but, per the paper, struggles to handle tasks requiring physical intuition without guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>From Table I / text: LLM-BT achieved 94% success on standard manipulation tasks (planning 98%, success 94%, avg time 22.94s ± 3.2) but only 15% success on interactive-intuition tasks (20% planning rate, 15% success, avg time 20.65s ± 8.7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable for this baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency metrics reported for LLM-BT versus other approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention or interpretability analyses for LLM-BT are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>LLM-BT demonstrates high performance when explicit task instructions are present (manipulation tasks) but shows poor grounding to physical interaction in instruction-free 'intuitive' tasks (15% success), indicating limited grounding of high-level text outputs to embodied affordances in these scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported beyond the two-level decomposition (LLM high-level planning -> low-level execution).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>LLM-BT performs well when explicit instructions are present; fails when tasks demand inference from physical context without instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not specifically reported; evaluated as provided (no reported few-shot finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>LLM-BT shows low performance in intuitive tasks (15% success), which the paper attributes to poor physical grounding — this is empirical evidence of a limitation of language-only high-level planners for tasks requiring embodied physical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared; LLM-BT baseline lacks the VLM-driven perception that INTENTION uses.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilled feature fields enable few-shot language-guided manipulation <em>(Rating: 2)</em></li>
                <li>Voxposer: Composable 3d value maps for robotic manipulation with language models <em>(Rating: 2)</em></li>
                <li>Language embedded radiance fields for zeroshot task-oriented grasping <em>(Rating: 2)</em></li>
                <li>Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities <em>(Rating: 2)</em></li>
                <li>Open x-embodiment: Robotic learning datasets and rt-x models <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1898",
    "paper_id": "paper-280546737",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "INTENTION",
            "name_full": "INTENTION (Interactive Intuition + Grounded VLM with MemoGraph)",
            "brief_description": "A framework that combines a pre-trained vision-language model (VLM) Intuitive Perceptor, a graph-structured episodic memory (MemoGraph), and a VLM-based planner to enable humanoid robots to infer manipulation behaviors from visual scenes and prior interactions in real-world settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "INTENTION (VLM-based Intuitive Perceptor + MemoGraph + VLM planner)",
            "model_description": "A system-level agent that (1) uses a pre-trained multimodal VLM as the Intuitive Perceptor to extract objects, spatial relations and affordance-like attributes from RGB images (via prompting and function-calling into a JSON output template), (2) encodes scene/instruction/action/feedback into graph-structured episodic memories (MemoGraph) using CLIP embeddings for nodes/links/instructions, and (3) performs graph-matching + a language-model based evaluator to retrieve and adapt prior actions from a motion library for execution on a real humanoid robot.",
            "pretraining_type": "uses pre-trained multimodal VLM (proprietary OpenAI model) and CLIP text-image embeddings; system-level components (MemoGraph, graph-matching, motion library) are not pretrained models.",
            "pretraining_data_description": "The paper does not provide explicit details on the pretraining corpus for the proprietary VLM (o4-mini). CLIP is used as an encoder for nodes/links/instructions (CLIP is known to be trained on image–text pairs, but the paper does not enumerate the CLIP pretraining data). The paper does not claim or analyze specific pretraining coverage of action verbs, affordances, or 3D dynamics.",
            "target_task_name": "Humanoid loco-manipulation (real-world manipulation and interactive-intuition tasks)",
            "target_task_description": "Real-world humanoid manipulation on a 37-DOF centaur-like robot (two arms with claw gripper, torso and wheeled legs). Tasks include pick-and-place (standard manipulation) and interactive-intuition tasks such as handling an object handed by a human, collaborative lift-desk, push-chair assistance, and refill-tea. Action execution uses a library of predefined motion primitives (continuous robot control via Cartesian I/O) with planner selecting primitives; experiments run on hardware (RealSense camera + torque sensors).",
            "semantic_alignment": "The paper describes using VLM outputs to create semantic scene graphs (nodes = objects/agents, links = spatial/semantic relations) and embeddings (CLIP) for graph-matching; it emphasizes alignment between VLM-extracted semantics and robot action selection but does not quantify overlap between pretraining corpora and task objects/actions.",
            "performance_with_language_pretraining": "System-level experimental results (real robot): INTENTION achieved 92% success on the standard manipulation tasks and 72% success on interactive-intuition tasks (Table I). For manipulation: planning rate 96%, success rate 92%, average time 23.75s ± 4.3. For interactive-intuition tasks: planning rate 84%, success rate 72%, average time 24.86s ± 2.7.",
            "performance_without_language_pretraining": "No direct ablation removing the VLM pretraining is reported. Baseline methods tested include WB-MPC, BT-Planner, and LLM-BT (LLM-based planner), but none are a direct 'no language pretraining' ablation of the same pipeline. Therefore no numeric baseline of INTENTION without language-pretraining is provided.",
            "sample_efficiency_comparison": "The paper reports qualitative/frequency-based evidence: success probability of INTENTION improves with increasing number of stored task graphs in MemoGraph (figures reported qualitatively), and mentions 'few-shot training for deployment' as a design goal, but does not provide quantitative episodes/demonstrations-to-performance comparisons between language-pretrained vs non-language baselines.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No analysis of attention maps or per-token/region attention visualizations is provided.",
            "embedding_space_analysis": "No in-depth embedding-space analyses (e.g., clustering, PCA, attractor analysis) are reported beyond using CLIP embeddings for node/link/instruction similarity and the use of cosine similarity for instruction similarity.",
            "action_grounding_evidence": "Behavioral-level evidence: INTENTION's pipeline grounds VLM-extracted semantic/spatial relations into action selection via graph-matching and retrieving previously executed motion primitives stored with feedback; empirical evidence is that INTENTION attains substantially higher success in 'intuitive' tasks (72%) compared to the LLM-BT baseline (15%), and maintains ≥65% success in 'Intuitive Mode' (no explicit instruction). No mechanistic analyses (e.g., learned mapping of verb embeddings to motor primitives) are provided.",
            "hierarchical_features_evidence": "No explicit multi-level feature analysis (low-level vs high-level) is reported. The system relies on high-level semantic graph features (nodes/links/text embeddings) for matching and action retrieval.",
            "transfer_conditions": "Paper describes that transfer success depends on similarity between the current scene graph and stored task graphs; matching uses node similarity, link similarity, and instruction similarity (weighted by α, β, γ). Empirically, more stored graphs improve success; transfer works better when prior memories contain similar topologies and semantic contexts. No numeric thresholds or domain-shift experiments (e.g., large visual domain gap) are reported.",
            "novel_vs_familiar_objects": "No explicit experimental comparison between objects/actions present in pretraining data vs novel objects; the paper evaluates 'novel task scenarios' in the sense of new scenes relative to MemoGraph, but does not attribute performance to pretraining familiarity.",
            "zero_shot_or_few_shot": "INTENTION demonstrates 'Intuitive Mode' where no explicit semantic instruction is given (zero-instruction inference), achieving ≥65% success across tasks and 72% on interactive-intuition tasks overall, indicating ability to infer behaviors from perception and memory without new instruction examples. The paper also claims few-shot deployment as a design goal but provides no quantitative few-shot counts.",
            "layer_analysis": "No layerwise or component ablation (e.g., freezing VLM layers, removing CLIP embeddings) is reported.",
            "negative_transfer_evidence": "The paper does not report cases where the VLM-driven pipeline harmfuly reduced performance relative to baselines; negative transfer is not explicitly reported.",
            "comparison_to_vision_only": "No direct empirical comparison between vision-language pretraining and vision-only pretraining (e.g., ImageNet or self-supervised vision) is reported for the same pipeline. CLIP is used as text encoder, but no ablation versus vision-only encoder is provided.",
            "temporal_dynamics": "The paper discusses that performance improves as MemoGraph grows (more stored task graphs), but does not provide detailed learning curves over fine-tuning epochs or representational change across training iterations.",
            "dimensionality_analysis": "No analysis of effective dimensionality (PCA / intrinsic dimension) of representations is reported.",
            "uuid": "e1898.0"
        },
        {
            "name_short": "o4-mini (VLM)",
            "name_full": "o4-mini (OpenAI multimodal model used as VLM in Intuitive Perceptor and evaluator)",
            "brief_description": "A proprietary OpenAI multimodal model (accessed via API) used as the Intuitive Perceptor and language-model-based evaluator; it is prompted with function-calling to extract structured scene graphs (JSON) from RGB images and instructions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o4-mini (OpenAI VLM) as Intuitive Perceptor and evaluator",
            "model_description": "Proprietary pre-trained multimodal language-vision model (accessed via OpenAI API) used with prompting and function-calling to produce structured scene graph outputs and to evaluate plausibility of graph matches; processes RGB image content plus textual instruction prompts.",
            "pretraining_type": "pre-trained multimodal vision-language (proprietary) model (exact pretraining details not specified in paper)",
            "pretraining_data_description": "Not specified in the paper; the paper does not provide corpus statistics or detail on inclusion of affordance/action verbs or 3D data. The model is used as-is through API.",
            "target_task_name": "Scene understanding and graph extraction for real-world humanoid manipulation",
            "target_task_description": "Generation of structured scene graphs (objects, spatial relations, attributes) from RGB images and optional instructions; outputs are used to populate MemoGraph and to guide action selection on a 37-DOF humanoid robot in real-world tasks (pick-and-place and interactive intuition tasks).",
            "semantic_alignment": "Paper asserts the VLM provides semantic and spatial information useful for downstream mapping to actions; no quantitative alignment metrics between pretraining content and task domain are given.",
            "performance_with_language_pretraining": "Used as a component — system-level success reported for INTENTION (see INTENTION entry). The paper does not report ablation where o4-mini is removed or replaced with a non-language-augmented model.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported for the specific VLM component.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention or input attribution analyses for o4-mini are presented.",
            "embedding_space_analysis": "No embedding space analyses for o4-mini are presented.",
            "action_grounding_evidence": "Used to output affordance-like relations and attributes that are then linked to motion primitives through MemoGraph; grounding evidence is behavioral (system success) rather than mechanistic.",
            "hierarchical_features_evidence": "No hierarchical feature analysis is presented for o4-mini.",
            "transfer_conditions": "Not specifically analyzed for o4-mini; system-level transfer depends on matching of VLM-extracted graphs to stored memory.",
            "novel_vs_familiar_objects": "Not analyzed specifically for o4-mini.",
            "zero_shot_or_few_shot": "o4-mini is used zero-shot via prompting/function-calling (no fine-tuning reported) to extract scene graphs. The system demonstrates zero-instruction inference at higher system level.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not provided.",
            "comparison_to_vision_only": "No direct comparison reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1898.1"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pre-training)",
            "brief_description": "A vision-language embedding model used in this work as the encoder for node, link, and instruction embeddings in the task graphs for graph matching and similarity computation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_description": "A joint image-text embedding model (image encoder + text encoder) producing embeddings used for computing cosine similarities between nodes, links and instructions in graph-based memory retrieval.",
            "pretraining_type": "vision-language pretraining on image–text pairs (paper uses pre-trained CLIP as an encoder; the INTENTION paper itself does not retrain CLIP).",
            "pretraining_data_description": "Not detailed in the paper; CLIP is used to embed textual node/link/instruction content for matching (the paper does not characterize the CLIP pretraining corpus).",
            "target_task_name": "Graph-embedding and similarity computation for MemoGraph retrieval in robotic manipulation",
            "target_task_description": "Encoding nodes (objects/agents), links (spatial/semantic relations), and instructions into embeddings used to compute pairwise similarity matrices for graph matching between current scene graphs and stored task graphs; supports selection of motion primitives for real-robot execution.",
            "semantic_alignment": "CLIP is used as a generic text-image embedding for semantic alignment between graph textual content and current scene; the paper does not quantify coverage of task-specific vocabularies.",
            "performance_with_language_pretraining": "No direct performance numbers attributable solely to CLIP; CLIP is a component in the INTENTION pipeline whose system-level performance is reported.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analyses of CLIP features are presented.",
            "embedding_space_analysis": "CLIP embeddings are used operationally (cosine similarity, thresholding, bipartite matching) but no further analysis of embedding structure is reported.",
            "action_grounding_evidence": "CLIP embeddings serve as the representational substrate by which symbolic scene graph elements are matched across episodes; grounding evidence is indirect (improved behavior retrieval), not a direct mapping analysis.",
            "hierarchical_features_evidence": "Not analyzed in the paper.",
            "transfer_conditions": "Performance depends on how well CLIP embeddings capture similarity between current graph textual descriptions and stored graphs; no quantitative study reported.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Used zero-shot as an encoder; no fine-tuning reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "No direct comparison reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1898.2"
        },
        {
            "name_short": "LLM-BT",
            "name_full": "LLM-BT (LLM-based Behavior Tree planner) - baseline",
            "brief_description": "A baseline approach that leverages large language models to generate behavior trees (BT) that are then executed by lower-level controllers; evaluated as a comparator to INTENTION on humanoid manipulation and intuition tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLM-BT (baseline high-level planner)",
            "model_description": "A language-model-driven planner that outputs behavior-tree structured plans (high-level) to control low-level execution; in this paper it is used as a baseline for autonomy comparisons.",
            "pretraining_type": "pre-trained large language model (text-only LLM used as high-level planner) — the paper does not specify the exact LLM or its pretraining corpus.",
            "pretraining_data_description": "Not specified in the paper; assumed standard LLM pretraining on text corpora.",
            "target_task_name": "Humanoid task planning for manipulation and interactive-intuition tasks (baseline comparison)",
            "target_task_description": "High-level plan generation (behavior trees) for manipulation tasks executed on the same humanoid platform; evaluated on pick-and-place and interactive intuition tasks.",
            "semantic_alignment": "LLM-BT provides high-level semantic plans but, per the paper, struggles to handle tasks requiring physical intuition without guidance.",
            "performance_with_language_pretraining": "From Table I / text: LLM-BT achieved 94% success on standard manipulation tasks (planning 98%, success 94%, avg time 22.94s ± 3.2) but only 15% success on interactive-intuition tasks (20% planning rate, 15% success, avg time 20.65s ± 8.7).",
            "performance_without_language_pretraining": "Not applicable for this baseline comparison.",
            "sample_efficiency_comparison": "No explicit sample-efficiency metrics reported for LLM-BT versus other approaches.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention or interpretability analyses for LLM-BT are reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "LLM-BT demonstrates high performance when explicit task instructions are present (manipulation tasks) but shows poor grounding to physical interaction in instruction-free 'intuitive' tasks (15% success), indicating limited grounding of high-level text outputs to embodied affordances in these scenarios.",
            "hierarchical_features_evidence": "Not reported beyond the two-level decomposition (LLM high-level planning -&gt; low-level execution).",
            "transfer_conditions": "LLM-BT performs well when explicit instructions are present; fails when tasks demand inference from physical context without instructions.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not specifically reported; evaluated as provided (no reported few-shot finetuning).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "LLM-BT shows low performance in intuitive tasks (15% success), which the paper attributes to poor physical grounding — this is empirical evidence of a limitation of language-only high-level planners for tasks requiring embodied physical reasoning.",
            "comparison_to_vision_only": "Not directly compared; LLM-BT baseline lacks the VLM-driven perception that INTENTION uses.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1898.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilled feature fields enable few-shot language-guided manipulation",
            "rating": 2
        },
        {
            "paper_title": "Voxposer: Composable 3d value maps for robotic manipulation with language models",
            "rating": 2
        },
        {
            "paper_title": "Language embedded radiance fields for zeroshot task-oriented grasping",
            "rating": 2
        },
        {
            "paper_title": "Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities",
            "rating": 2
        },
        {
            "paper_title": "Open x-embodiment: Robotic learning datasets and rt-x models",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model",
            "rating": 2
        }
    ],
    "cost": 0.014365999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM</p>
<p>Jin Wang wang.jin@iit.it 
Weijie Wang 
Boyuan Deng 
Heng Zhang 
Rui Dai 
Nikos Tsagarakis 
INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM
F1E65F6903E9A4CA9ECF33475C6C064E
Traditional control and planning for robotic manipulation heavily rely on precise physical models and predefined action sequences.While effective in structured environments, such approaches often fail in real-world scenarios due to modeling inaccuracies and struggle to generalize to novel tasks.In contrast, humans intuitively interact with their surroundings, demonstrating remarkable adaptability, making efficient decisions through implicit physical understanding.In this work, we propose INTENTION, a novel framework enabling robots with learned interactive intuition and autonomous manipulation in diverse scenarios, by integrating Vision-Language Models (VLMs) based scene reasoning with interaction-driven memory.We introduce Memory Graph to record scenes from previous task interactions which embodies human-like understanding and decision-making about different tasks in real world.Meanwhile, we design an Intuitive Perceptor that extracts physical relations and affordances from visual scenes.Together, these components empower robots to infer appropriate interaction behaviors in new scenes without relying on repetitive instructions.Videos: https://robo-intention.github.io</p>
<p>I. INTRODUCTION</p>
<p>Humans possess an innate ability known as interactive intuition, which allows them to form an intuitive understanding of the physical world through accumulated experience.This arises from continuous interaction with the environment in daily life, enabling humans to quickly judge and reason about interactive properties, such as stability, reachability, and motion trends of objects, without relying on precise physical models.For example, we can easily determine whether a stack of blocks is likely to fall, whether a tool can reach a target, or in which direction an object will move when subjected to an external force.This approximate yet efficient reasoning ability forms the foundation for humans to flexibly handle a wide variety of complex manipulation tasks.</p>
<p>In contrast, traditional robotic control systems often rely on accurate physical modeling and precise environmental perception, typically applied in Task and Motion Planning (TAMP).While these model-based approaches perform well in structured environments such as industrial assembly lines, they often struggle in unstructured settings where perception is uncertain and object properties are unknown.Model-free †This work was supported by the European Union's Horizon 2020 research and innovation programme, euROBIN EPUE034001, and Leonardo Joint Lab JL Leonardo ETCM058501. 1 Humanoids and Human-Centered Mechatronics (HHCM), Istituto Italiano di Tecnologia, Genoa, Italy.</p>
<p>2 Human-Robot Interfaces and Interaction Lab, Istituto Italiano di Tecnologia, Genoa, Italy.</p>
<p>3 Ph.D. program of national interest in Robotics and Intelligent Machines (DRIM) and Università di Genova, Genoa, Italy.learning approaches, such as reinforcement learning, offer robustness in some certain scenarios, but are often taskspecific.When tasks change, retraining becomes costly, leading to poor flexibility and generalization in new situations.</p>
<p>In recent years, the development of large language models (LLMs) and vision-language models (VLMs) has brought new breakthroughs in robotic cognition and decision-making.These multimodal models exhibit human-level capabilities in semantic understanding, spatial reasoning, and contextual awareness.They can extract scene structure from images and videos, understand object properties and spatial relationships, and infer task intent expressed in human language.This enables robots to autonomously perform navigation and manipulation tasks in unstructured environments, representing a significant step toward embodied intelligence and robotic autonomy.</p>
<p>However, although integrating language models into robotic systems has emerged as a promising direction for enhancing robotic intelligence, there remain significant challenges when applying such approaches to real-world tasks.Existing Vision-Language Action (VLA) methods attempt to directly generate robot actions from pretrained models, enabling robots to interpret task instructions and execute corresponding actions across diverse scenes.Nevertheless, these arXiv:2508.04931v1[cs.RO] 6 Aug 2025 methods typically rely on large-scale, high-quality datasets for training, which are difficult to obtain-especially for complex, high-degree-of-freedom humanoid robots.Which leads to a limitation on their control accuracy and adaptive capabilities.Moreover, current VLMs used in robotic systems are still primarily focused on static image understanding and lack the ability to model object properties, interactive dynamics, and interactive relationships, which hinders their capacity to handle novel task and predict motion in unstructured environment.In addition, there remains a substantial gap between the high-level semantic outputs of language models and the lowlevel motion control required for robot execution, particularly in tasks involving whole-body coordination or fine-grained control across multiple articulated components.</p>
<p>To address these challenges, we propose INTENTION, a novel framework that leverages the foundation models in spatial and object-level reasoning to reconstruct a form of physically grounded intuition applicable for real-world humanoid manipulation.To extract task-relevant information from rich interactive environments, we first introduce an Intuitive Perceptor based on Vision-Language Models (VLMs).Upon receiving images from the camera, the Perceptor distills spatial and geometric observations and identifies 3D features within the task scene.It then outputs a graph-structured representation encoding both semantic state attributes and geometric relationships among objects and agents.To accumulate knowledge and experience from robot interactions with humans and environment, we further construct a Memory Graph (MemoGraph) to support future task and motion planning in novel scenarios.MemoGraph is a topological graph structure that stores semantic information derived from human-robot and environment interactions across diverse task contexts.During each execution, Mem-oGraph continuously logs the scene information extracted by the Intuitive Perceptor-including semantic instructions, real-world interactive states, and spatial geometry-along with the robot's action and corresponding feedback.</p>
<p>These collected representations of real-world physical interactions serve as a rich experiential prior for the robot, enabling it to perform autonomous locomotion and manipulation in unstructured environments.When facing a new task scenario, the agent extracts the current semantic scene state using Perceptor and compares it against previous experiences stored in the MemoGraph, selecting the most relevant interaction behavior that aligns with the present context.With INTENTION, robots are empowered to flexibly adapt to a wide range of tasks across diverse and dynamic environments, while incrementally building a human-level intuitive physics through continual learning from experience.This significantly enhances their autonomy and adaptability in unstructured real-world settings.</p>
<p>Our summary of the main contributions of this work includes:</p>
<p>• We exploit the integration of grounded language models with intuitive physics and propose INTENTION, a novel framework for autonomous robotic task planning, enabling few-shot training for deployment to humanoids.</p>
<p>• We propose a VLM-based intuitive perceptor to extract distilled spatial geometry and semantic observation from task scenarios, and further construct a MemoGraph to store the grounded knowledge to guide the behavior selection for diverse tasks combining robotic affordance.• We validate the applicability of proposed method through real-world experiments on robotic system, demonstrating effective adaptation across diverse task scenarios.</p>
<p>II. RELATED WORKS</p>
<p>Human interactive intuition refers to the innate ability to infer physical properties and processes from experience without explicit modeling.Endowing robots with such ability refers to studies of cognitive science, machine learning and robotics [1].Priori research [2]- [4] focus on model-based methods that rely on accurate physical modeling and precise sensing to plan and execute tasks, which perform well in structured environments but struggle with unstructured or dynamic scenarios due to modeling inaccuracies and limited generalization.Recent efforts have explored learning-based methods [5] to imitate human-like interactive reasoning.Reinforcement learning [6], [7] and imitation learning [8], [9] have been applied to help robots acquire skills from trial-and-error or human demonstrations.Despite progress, many learning-based methods remain task-specific and brittle to environmental changes, requiring extensive retraining for new tasks or scenarios.</p>
<p>Grounding pre-trained language models has emerged as a promising direction in robotics, enabling advanced task planning and reasoning through language.Many recent studies [10]- [12] leverage large language models (LLMs) for robotic applications, including code generation [13], reward shaping [14], [15], and interactive learning with robots [16]- [19].The integration of multiple modalities-such as vision and audio-has further enhanced the capacity of models to map semantic instructions to perception and behavior [20]- [22].Parallel efforts focus on constructing skill libraries [23]- [25] that bridge low-level execution with high-level task planning.In addition, recent work [26]- [29] has explored grounding affordances in foundation models to facilitate spatial reasoning and improve manipulation.Despite these advances, the majority of research targets fixed-base robotic arms, with limited exploration into grounding language models for humanoid robots.Some VLA-based approaches [30]- [32] are capable of generating robot action sequences from multimodal inputs, showing promising results in end-toend task execution.However, these methods often rely on the collection of large-scale, high-quality datasets to learn robust instruction-action mapping, and tend to struggle with generalization across diverse tasks.</p>
<p>In contrast, our work is the first to explore the use of VLMs for constructing interactive intuition in real-world humanoid manipulation.By leveraging the scene understanding capabilities of VLM, we design an Intuitive Perceptor that extracts semantic and spatial representations from task environments.These are then accumulated into MemoGraph through interaction, enabling the robot to autonomously generate physical behaviors that align with human intuition and understanding when faced with novel tasks and instructions.Moreover, we further investigate through experiments whether robots can infer appropriate actions based solely on the situational context, even under minimal or zero explicit instruction.</p>
<p>III. METHODOLOGY</p>
<p>A. Problem Statement</p>
<p>In the learning phase, the robot is engaged in a sequence of interaction episodes aimed at constructing a memory-based representation of interactive experiences.We predefined a skill library Π = {π 1 , π 2 , ..., π n } for the robot, where each skill π denotes a parametrized action or perception primitive executable on the robot hardware.During each interaction n, the robot first observes a scene state S n , which consists of the human-issued instruction I n and the raw sensory input of the environment.An Intuitive Perceptor P processes the scene and extracts the object/environment state T n , which reflects the spatial and semantic configuration relevant to the task.A vision-language-based planner V p then selects the most appropriate action a n ∈ Π conditioned on I n , S n , and T n , which is subsequently executed by the robot.After execution, a task success evaluation C n is computed to assess the outcome.All of this interaction information-S n , T n , a n , and C n -is encoded into a graph representation g n , which is stored in the MemoGraph M. This graphbased memory structure is designed to accumulate reusable experience across different task contexts.</p>
<p>In the inference phase, when the robot is presented with a novel task scenario, it begins by observing the current environment state S t and extracting the relevant object configuration T t using the Intuitive Perceptor P .It then queries the previously built MemoGraph M, retrieving all stored graph instances G = {g 1 , g 2 , ..., g n }.A graph matching algorithm compares the current state (S t , T t ) with past interaction graphs and selects the most relevant memory graph.Instead of directly reusing the previously stored action, the evaluator will filter the most appropriate actions based on the degree of similarity and the current scenario.This adaptive use of past experience enables the robot to respond flexibly to unseen tasks while grounding its behavior in previously acquired interactive intuition.</p>
<p>The above process is described in Algorithm 1.In this way, the robot is enabled to accumulate interaction experience from human instructions and environment feedback into a structured memory graph, and to utilize this memory to generate context-aware, physically grounded actions in new scenarios.</p>
<p>B. INTENTION framework</p>
<p>To address the autonomous behavior planning for complex robotics platforms such as humanoid robots and the challenge to reuse human intuition while facing novel tasks and scenarios.This work proposes 'INTENTION', a framework that enables robots with learned interactive intuition and the return Action Executed C t ← updateStatus 24: end while ability to manipulate in novel scenarios, by integrating VLMbased scene reasoning and interaction-driven memory graph.As shown in Fig. 2, we divide the pipeline into four interrelated sectors that are learned and deployed from training to real-world scenarios.The Intuitive Perceptor perceives the information of the environment through raw RGB images and grounded visual-language models, and extracts objects state and spatial relationships from the task scenario.The physical scene topology aligns with the robot action for each task execution will be recorded as a task graph and then saved into MemoGraph, which serves as a library that stores semantic information derived from human-robot and environment interactions across diverse task contexts.When facing with new scenario, the robot first observes the current scene graph and then matches with the task graphs stored in the MemoGraph.The matching results will be evaluated through a language-model based evaluator and to filter the robot motions that best fit the current task scenario.We further construct a robot Motion Library, which includes all the pre-defined motion primitives for different tasks.Each primitive consists of the code interface available for direct control of robot execution and a semantic description of the action.</p>
<p>C. Intuitive Perceptor</p>
<p>In order to achieve human-level intuitive perception, we utilize the strong scene reasoning capability of a pre-trained visual language model as well as the extensive knowledge of semantic data and propose 'Intuitive Perceptor'.Although the VLM is able to extract a sufficient amount of corpus information from RGB images, the outputs can be various.In order to obtain the desired answer, we include prompt words in the input and use function calling, where the prompt ensures that the VLM extracts explicit spatial information from the task scene according to the semantic knowledge, and the function definitions enable the output to be generated according to a specific structure and template.</p>
<p>We provide part of the prompts used in Intuitive Perceptor below.</p>
<p>Prompts for Intuitive Perceptor:</p>
<p>You are an AI agent with human-level commonsense and ability to infer the spatial geometry and relationships in a world scenario.You need to extract specific {object} from the provided {image} according to the given {instruction}, and to guess the spatial relationship between {object1} and {object2} in the {image}.Answer relationship with one word or phrase.If no {instruction} is detected, then only extract the current scene information.</p>
<p>The complete prompts and function definitions will be shown on the INTENTION project website.</p>
<p>D. Graph Construction and Matching</p>
<p>In this subsection, we introduce how to construct a uniform task graph representation for recording scene and robot's action.Then we propose a graph matching algorithm to infer humanoid motion in a new task scenario relying on comparing with previous experience.</p>
<p>1) MemoGraph: To leverage the intuitive knowledge and experience acquired from the robot's past interactions with humans and the environment, we design MemoGraph, a graph-based memory structure that stores spatial and geometric observations extracted by the Intuitive Perceptor during task execution.Each task graph in MemoGraph contains semantic instructions, real-world scene states, geometric relations, as well as the robot's actions and corresponding feedback.This design ensures that the robot can effectively utilize its prior experiences of physical interaction and intuitive understanding to build a systematic framework for cognitive learning and feedback.</p>
<p>We define the task graph G = (N , L, I) as a structured representation consisting of a set of nodes N , a set of links L, and a human instruction I.Each node corresponds to an object or agent (e.g., robot, human), while each link captures the spatial or semantic relationship between two related nodes.The content of nodes, links, and instructions is encoded in text format.</p>
<p>When the robot agent is initialized in a new environment and receives an instruction from the human instructor, the Intuitive Perceptor actively explores the scene and constructs the current scene graph G t based on RGB observations.</p>
<p>2) Graph Matching: When the robot agent is presented with a novel task scenario, it queries the constructed Mem-oGraph M to retrieve all stored task graph instances, i.e., M = G m1 , G m2 , . . ., G mn .The current scene graph Gt is then matched against each stored task graph G m in M. To identify the most relevant prior experience and select the robot actions that best align with the current scenario, we propose a scoring-based graph matching method that evaluates the similarity between G t and each G m .Given the current scene graph G t and a stored task graph G m , we design two complementary matching metrics: instruction similarity and graph similarity, to evaluate how well G t matches each graph in the memory.</p>
<p>Formally, for a pair of instructions I t and I m , we first extract their embeddings using an encoder E(•), and compute their similarity using cosine similarity:
S I (I t , I m ) = cosine-sim(E(I t ), E(I m ))(1)
where S I denotes the instruction similarity score.</p>
<p>To assess the structural similarity between graphs, we compute pairwise similarities between their node sets and link sets.Specifically, we extract embeddings for nodes and links from both graphs, compute the pairwise similarity matrices, and apply thresholding and bipartite matching to determine the best-aligned pairs:
P N = F thr(E(N t ) • E(N m ) ⊤ )
(2)
P N = F thr(E(L t ) • E(L m ) ⊤ )(3)
Here, E(N ) and E(L) denote the embeddings of nodes and links, respectively.thr(•) is a thresholding function that filters weak similarity scores, and F(•) represents a bipartite matching function that outputs the optimal assignment between node or link pairs.We also average the similarity matrix of nodes and links to obtain the similarity scores S N and S L .</p>
<p>Then we compute the overall matching score S W between the current scene graph and each task graph in the MemoGraph using a weighted combination of the individual similarity scores:
S W = α • S N + β • S L + γ • S I(4)
where α + β + γ = 1 are weighting coefficients that balance the contributions of node similarity (S N ), link similarity (S L ), and instruction similarity (S I ) during the matching process.All final scores are evaluated by a language-model based evaluator to analyze the plausibility of each match.It retrieved the corresponding action from the task graph of the selected match and then generated the control code for the robot to execute the task.</p>
<p>IV. EXPERIMENT AND EVALUATION</p>
<p>We proposed real-world experiments to verify the capability of INTENTION by implementing it and assessing its performance on a humanoid robot executing daily manipulation tasks.In this section, we present the details of the experimental setup and design, followed by an analysis and discussion of the results.</p>
<p>A. Experiment Setup</p>
<p>Our robot is a centaur-like humanoid robot, supported by four legs with wheels.It has 37 degrees of freedom and two arms with one claw gripper on its right arm, enabling it to perform a wide range of manipulation tasks.Equipped with a RealSense Depth camera on its head and torque sensors in the joints throughout its body, the robot possesses extensive perceptual capabilities to measure joint efforts and interaction forces.The robot actions in the motion library were manually designed based on Cartesian I/O without any additional training, and we use Xbot2 to achieve real-time communication between the underlying actuators and the control commands.</p>
<p>We access the pre-trained o4-mini model as the VLM for intuitive perceptor and evaluator from OpenAI API [33], and apply function calling to define the structure of the extracted scene graph aligning with the output template using a JSON format.To embed node, link and instruction in the graph, we use CLIP [34] as the text encoder.Experiments of the robot performing diverse manipulation task in the real world are demonstrated in the accompanying video.</p>
<p>B. Comparison with other methods</p>
<p>We conducted both qualitative and quantitative evaluations to compare the proposed INTENTION framework against several existing approaches on our humanoid robot.Specifically, Whole-Body Model Predictive Control (WB-MPC) applies model predictive control across the entire body; BT-Planner relies on manually designed behavior trees (BT) for planning; and LLM-BT [35] leverages large language models as high-level planners to generate behavior trees for low-level robotic execution.</p>
<p>The four methods were tested across two types of tasks: (i) a standard manipulation task (Pick and Place) and (ii) a interactive intuition task, where the robot operates without explicit task instructions, relying solely on its embodied physical reasoning capabilities.Experimental results, summarized in Table 1,  To construct a sufficiently large MemoGraph dataset, we performed extensive offline experiments across these four tasks.During this process, the IP extracted physical information from each task scene and generated corresponding scene graphs.A task planner based on a VLM was then employed to infer the appropriate actions for the robot based on the given instructions.The execution outcomes were manually evaluated, and the complete task graphs were subsequently stored in MemoGraph, as illustrated in Fig. 5.</p>
<p>After building MemoGraph, we further investigated the relationship between the success rate of INTENTION's scene  extraction and task planning, and the number of corresponding task graphs stored in MemoGraph.The experiments involved two different operational modes under novel task scenarios: (i) Instruction Mode, where the robot received explicit semantic instructions to complete the task, and (ii) Intuitive Mode, where no instruction was provided, requiring the robot to rely solely on interactive intuition and prior memory.The experimental results are presented in Fig. 6.</p>
<p>2) Motion Planning with Intuitive Physics: After validating the scene extraction and task planning capabilities of INTENTION, we further conducted real-world experiments to assess its ability to infer appropriate robot motions purely based on interactive intuition and prior experience, without relying on any semantic instruction input, across a variety of task scenarios.</p>
<p>In the Handling Object task, the robot is expected to correctly generate a receiving motion when observing the operator handing over different objects.In the Lift Desk task, the robot must infer a collaborative lifting motion based on the operator's intention to raise the desk.In the Push Chair task, the robot is required to assist the operator in sitting down when it observes the operator approaching and turning their back toward a chair located within the robot's operational workspace.In the Refill Tea task, the robot must autonomously refill a cup when it observes the operator presenting an empty cup and detects the presence of a teapot either on the table or in the robot's hand.These tasks pose significant challenges to the robot's scene understanding and interactive reasoning capabilities.</p>
<p>We conducted 25 trials for each manipulation task and recorded the corresponding success rates, as summarized in Fig. 8. Additionally, Fig. 7 illustrates the overall workflow of the INTENTION framework, including scene extraction, task graph generation, MemoGraph construction, and the robot agent's planning and execution process when confronted with novel task scenarios.</p>
<p>D. Results analysis</p>
<p>The experimental results validate the effectiveness of the INTENTION framework across a diverse range of real-world humanoid manipulation tasks.</p>
<p>In the comparison study (Table I), INTENTION achieves the highest overall performance in both standard manipulation and physical intuition tasks.While methods such as WB-MPC and BT-Planner show moderate success under explicit instructions, they fail to handle tasks requiring physical reasoning without guidance.LLM-BT demonstrates improved autonomy by leveraging language models but struggles in intuitive scenarios, achieving only 15% success in interactive intuition tasks.In contrast, INTENTION, benefiting from its MemoGraph-based experiential memory and VLM-driven scene understanding, achieves 72% success in these challenging conditions.</p>
<p>Further analysis of the success rates (Fig. 6 and Fig. 8) reveals that the success probability of INTENTION improves with an increasing number of stored task graphs in MemoGraph, especially for tasks that rely on human-level scene understanding and intuition.Additionally, even without semantic instruction input (Intuitive Mode), INTENTION maintains a high success rate (≥ 65%) across various tasks, demonstrating its ability to infer appropriate robot motions based solely on interactive intuition and prior knowledge.These results collectively indicate that INTENTION significantly enhances the adaptability, robustness, and generalization ability of humanoid robots when facing novel task scenarios.</p>
<p>V. CONCLUSION</p>
<p>In this work, we proposed INTENTION, a novel framework that equips humanoid robots with learned interactive intuition and autonomous task planning capabilities in realworld scenarios.By integrating a VLM-based Intuitive Perceptor for scene understanding and a graph-structured Mem-oGraph for memory-driven planning, INTENTION enables robots to infer context-appropriate behaviors without relying on explicit instructions.</p>
<p>Future work will explore scaling MemoGraph to broader task domains, enhancing motion generalization across different robotic platforms, and further improving the efficiency of memory retrieval and action inference under highly dynamic environmental conditions.</p>
<p>Fig. 1 :
1
Fig. 1: INTENTION enables the humanoid robot to learn, plan, and infer motions to complete manipulation tasks based on intuitive physics and previous experience.</p>
<p>Fig. 2 :
2
Fig. 2: Overview of the Framework.(a) Intuitive Perceptor takes the RGB image and human instruction as input, extracting interactive information of objects and constructs a structured task graph.(b) MemoGraph is responsible for storing knowledge related to the scene of the previous task as well as the actions taken by the robot in the face of different tasks.(c) Motion Library consists of various predefined motion primitives that allow the task planner to choose according to different scenes.When facing with New Scenario, the robot agent will distill the current scene graph and compared it with MemoGraph to select the suitable action.</p>
<p>Fig. 3 :
3
Fig. 3: Graph Construction and Matching</p>
<p>Fig. 4 :
4
Fig. 4: Experiment setup</p>
<p>demonstrate that INTENTION consistently outperforms the baseline methods across multiple capability dimensions.Notably, in interactive intuition tasks that demand a deep understanding of environmental dynamics and affordance extraction, INTENTION exhibits significantly greater adaptability and robustness.C. Autonomous humanoid motion planning 1) Learning scene extraction and task planning: To evaluate the scene extraction capability of the VLM-based Intuitive Perceptor (IP) and the contribution of MemoGraph to the robot's ability to handle novel task scenarios, we conducted a series of experiments on four different manipulation tasks: handling an object, lifting a desk, pushing a chair, and refilling tea.</p>
<p>Fig. 5 :Fig. 6 :
56
Fig. 5: The process of scenario extraction and MemoGraph construction.</p>
<p>Fig. 7 :
7
Fig. 7: INTENTION framework pipeline in real-world test, covering scene extraction, task graph generation, MemoGraph construction, and planning/execution for novel task.</p>
<p>Fig. 8 :
8
Fig. 8: Success rates of INTENTION across different manipulations relying on interactive intuition</p>
<p>Algorithm 1 Intuitive Physics Learning and Execution Given: VLM planner V p , a human instruction i, a skill library Π, Intuitive Perceptor P , with the MemoGraph M. // Step 1: Data collection and memory construction 2: Initialize the state S, number of steps n for n = 1 to N do Observe scene state S n ← (I n , raw sensory input) T n ← P.extract object state(S n ) C n ← evaluate task completion() g n ← encode graph(S n , T n , a n , C n ) // Step 2: Inference in New Scenarios Observe new scene S t 14: while C t ̸ = "done" do S t ← perceive current scene()
4:6:a n ← V p (I n , S n , T n , Π)Execute action a n8:10:M.store(g n )end for12: 16:T
t ← P.extract object state(S t ) g * ← encode graph(S t , T t ) 18: G ← M.retrieve all() g * ← Matching(G) 20: a t ← Similarity evaluation Execute action a t 22:</p>
<p>TABLE I :
I
Comparison of different methods for humanoid manipulation task planning
MethodAutonomy ReplanAbilities Text InputIntuitionManipulation Taskinteractive Intuition TaskWB-MPClow✔✗✗84%72%35.86s ± 11.5--TLEBT-Plannerlow✗✗✗92%80%27.40s ± 4.6--TLELLM-BThigh✔✔✗98%94%22.94s ± 3.220%15%20.65s ± 8.7INTENTIONhigh✔✔✔96%92%23.75s ± 4.384%72%24.86s ± 2.7
Plan ↑ Succ ↑ Avg.Time ↓ Plan ↑ Succ ↑ Avg.Time ↓</p>
<p>Learning physical intuition for robotic manipulation. O M Groth, 2021University of OxfordPh.D. dissertation</p>
<p>A unified mpc framework for whole-body dynamic locomotion and manipulation. J.-P Sleiman, F Farshidian, M V Minniti, M Hutter, IEEE Robotics and Automation Letters. 632021</p>
<p>Optimization-based control for dynamic legged robots. P M Wensing, M Posa, Y Hu, A Escande, N Mansard, A Del Prete, IEEE Transactions on Robotics. 402023</p>
<p>Model predictive control of legged and humanoid robots: models and algorithms. S Katayama, M Murooka, Y Tazaki, Advanced Robotics. 3752023</p>
<p>Learning-based legged locomotion: State of the art and future perspectives. S Ha, J Lee, M Van De Panne, Z Xie, W Yu, M Khadiv, The International Journal of Robotics Research. 027836492413126982024</p>
<p>Srl-vic: A variable stiffness-based safe reinforcement learning for contact-rich robotic tasks. H Zhang, G Solak, G J G Lahr, A Ajoudani, IEEE Robotics and Automation Letters. 962024</p>
<p>Bresa: Bio-inspired reflexive safe reinforcement learning for contact-rich robotic tasks. H Zhang, G Solak, A Ajoudani, arXiv:2503.219892025arXiv preprint</p>
<p>Imitation methods for bipedal locomotion of humanoid robots: A survey. J Parra-Moreno, D Yanguas-Rojas, E Mojica-Nava, 2023 IEEE 6th Colombian Conference on Automatic Control (CCAC). IEEE2023</p>
<p>Manipulation learning on humanoid robots. A Gams, T Petrič, B Nemec, A Ude, Current Robotics Reports. 332022</p>
<p>Real-world robot applications of foundation models: A review. K Kawaharazuka, T Matsushima, A Gambardella, J Guo, C Paxton, A Zeng, 2024</p>
<p>Robonursevla: Robotic scrub nurse system based on vision-language-action model. S Li, J Wang, R Dai, W Ma, W Y Ng, Y Hu, Z Li, 2024</p>
<p>Lohoravens: A long-horizon languageconditioned benchmark for robotic tabletop manipulation. S Zhang, P Wicke, L K ¸enel, L Figueredo, A Naceri, S Haddadin, B Plank, H Schütze, 2023</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Language to rewards for robotic skill synthesis. W Yu, N Gileadi, C Fu, S Kirmani, K.-H Lee, 7th Annual Conference on Robot Learning (CoRL). 2023</p>
<p>Saytap: Language to quadrupedal locomotion. Y Tang, W Yu, J Tan, H Zen, A Faust, T Harada, 7th Annual Conference on Robot Learning. 2023</p>
<p>Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. L Zha, Y Cui, L.-H Lin, M Kwon, M G Arenas, A Zeng, F Xia, D Sadigh, 2024</p>
<p>Rt-h: Action hierarchies using language. S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, 2024</p>
<p>Robots that ask for help: Uncertainty alignment for llm planners. A Z Ren, Proc. Conf. Robot Learn. Conf. Robot Learn2023</p>
<p>Prompt a robot to walk with large language models. Y.-J Wang, B Zhang, J Chen, K Sreenath, arXiv:2309.099692023arXiv preprint</p>
<p>Multi-resolution sensing for real-time control with vision-language models. S Saxena, M Sharma, O Kroemer, 7th Annual Conference on Robot Learning. 2023</p>
<p>Gestureinformed robot assistance via foundation models. L.-H Lin, Y Cui, Y Hao, F Xia, D Sadigh, 7th Annual Conference on Robot Learning. 2023</p>
<p>Autonomous behavior planning for humanoid loco-manipulation through grounded language model. J Wang, A Laurenzi, N Tsagarakis, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024863</p>
<p>Bootstrap your own skills: Learning to solve new tasks with large language model guidance. J Zhang, J Zhang, K Pertsch, Z Liu, X Ren, M Chang, S.-H Sun, J J Lim, 7th Annual Conference on Robot Learning. 2023</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. B Ichter, A Brohan, Y Chebotar, C Finn, K Hausman, 6th Annual Conference on Robot Learning (CoRL). 2022</p>
<p>Grounding language models in autonomous loco-manipulation tasks. J Wang, N Tsagarakis, arXiv:2409.013262024arXiv preprint</p>
<p>Distilled feature fields enable few-shot language-guided manipulation. W Shen, G Yang, A Yu, J Wong, L P Kaelbling, P Isola, 7th Annual Conference on Robot Learning. 2023</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.059732023arXiv preprint</p>
<p>Language embedded radiance fields for zeroshot task-oriented grasping. A Rashid, S Sharma, C M Kim, J Kerr, L Y Chen, A Kanazawa, K Goldberg, 7th Annual Conference on Robot Learning. 2023</p>
<p>Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Driess, P Florence, D Sadigh, L Guibas, F Xia, 2024</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. A Padalkar, A Pooley, A Jain, A Bewley, A Herzog, A Irpan, A Khazatsky, A Rai, A Singh, A Brohan, arXiv:2310.088642023arXiv preprint</p>
<p>Gemini robotics: Bringing ai into the physical world. G R Team, S Abeyruwan, J Ainslie, J.-B Alayrac, M G Arenas, 2025</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, 2024</p>
<p>Gpt-4v (ision) system card. R Openai, 2023Citekey: gptvision</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PmLR2021</p>
<p>HYPERmotion: Learning hybrid behavior planning for autonomous loco-manipulation. J Wang, R Dai, W Wang, L Rossini, F Ruscelli, N Tsagarakis, 8th Annual Conference on Robot Learning. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
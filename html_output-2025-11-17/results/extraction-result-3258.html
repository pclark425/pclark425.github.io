<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3258 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3258</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3258</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-d00e7779c39dc7b06d7d43cf6de6d734c8edc4b8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d00e7779c39dc7b06d7d43cf6de6d734c8edc4b8" target="_blank">Language Understanding for Text-based Games using Deep Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper employs a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback to map text descriptions into vector representations that capture the semantics of the game states.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-ofwords and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations. 1</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3258.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3258.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end agent that jointly learns a sequence-based text representation (LSTM) and a Deep Q-Network action scorer to play text-based MUD games using Q-learning with experience replay and prioritized sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture composed of a representation generator φ_R implemented as an LSTM over word embeddings (outputs x_k, elementwise mean pooling to produce v_s) and an action scorer φ_A implemented as a multi-layer neural network that predicts Q-values for actions and objects; Q for a full command (action,object) is the average of action and object Qs. Trained end-to-end with Q-learning (DQN-style), RMSprop (lr=0.0005), γ=0.5, ε-annealing (1→0.2), an experience replay buffer (D=100000, FIFO) and prioritized sampling (ρ=0.25). Limits commands to one action + one object for tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Home world and Fantasy world (Evennia MUD environments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learning control policies from text descriptions to complete in-game quests (examples: navigate to kitchen and 'eat apple'; cross a broken bridge with a chance of falling; reach secret tomb—the latter noted as requiring longer-term memorization and planning). Evaluation metrics: average per-episode cumulative reward and fraction of quests completed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent internal memory (LSTM over text) and external experience replay buffer (episodic/experience memory) with prioritized sampling; also uses transfer of pretrained representation parameters (parameter-level transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Recurrent memory: LSTM processes word embeddings w_k producing outputs x_k; final state vector v_s = mean_k x_k (mean pooling). External memory: replay buffer D (size 100000) stores transitions (s_t,a_t,r_t,s_{t+1},p_t) with p_t = 1 if r_t>0 else 0; buffer implemented FIFO and sampled to form minibatches (size 64) every 4 steps. Prioritized sampling draws fraction ρ=0.25 from positive-reward transitions. Transfer: φ_R parameters pretrained on one Home world were used to initialize LSTM in new world experiments to speed learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Home world: LSTM-DQN reaches ~100% quest completion after ≈50 epochs and attains near-optimal average reward (close to theoretical best when step penalty applied). Fantasy world: avg reward -11.33 and ≈96% quest completion on the evaluated bridge-crossing quest (reported in paper). Prioritized sampling: agent reaches optimal policy ≈50 epochs faster compared to uniform sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to non-recurrent / no-sequence baselines: Home world: BOW-DQN completes ≈46% of quests (avg reward ≈0.20), BI-DQN ≈48%. Fantasy world: BOW-DQN ≈82% completion (avg reward -12.68), BI-DQN ≈97% completion but worse avg reward -26.68. Random baseline: very poor (≈5–10% completion depending on world). (These baseline models do not use recurrent LSTM sequence memory.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Recurrent LSTM representation yields much better semantic understanding of textual state descriptions, improving quest completion rates, reducing steps to complete quests, and increasing resilience to variability in descriptions; the external replay buffer with prioritized sampling improves sample efficiency and convergence speed (faster learning and earlier attainment of optimal policy); transferring pretrained LSTM parameters speeds up learning in a new environment.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Replay buffer is finite and implemented FIFO; prioritized sampling uses a simple binary priority (p_t based on positive reward) which is coarse. The agent (and memory mechanisms used) do not address tasks requiring long-term episodic event memorization or complex high-level planning—the paper explicitly notes a complex 'secret tomb' quest requires memorizing game events and planning beyond the present approach. No ablation for removing replay entirely is reported. The agent also simplifies action space (single action + object), which limits evaluation of memory use in more complex command contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use sequence models (LSTM) to compose state representations from textual descriptions (mean pooling over LSTM outputs worked well here); maintain a large replay buffer and sample minibatches to decorrelate updates; prioritize sampling of rare/positive transitions (they used ρ≈0.25 and p_t=1 for r_t>0) to speed learning; pretrain representation modules and transfer them across similar games to accelerate convergence. Acknowledge that tasks requiring long-term episodic memory or multi-step event recall will need additional mechanisms beyond LSTM + replay.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Understanding for Text-based Games using Deep Reinforcement Learning', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3258.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3258.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experience Replay + Prioritized Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experience replay memory with prioritized sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>External transition memory (replay buffer) used to store past (s,a,r,s') transitions to decorrelate training and improve sample efficiency, with a prioritized sampling scheme that oversamples positive-reward transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN (component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Replay buffer component of LSTM-DQN: stores transitions and supplies minibatches for gradient updates; uses a simple priority flag p_t=1 for transitions with positive reward and samples a fraction ρ of minibatch from high-priority pool.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Home world and Fantasy world (Evennia MUDs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Support off-policy Q-learning updates for the text-game control task by sampling past experiences to construct targets and reduce correlation between successive updates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external episodic/experience memory (replay buffer) with prioritized sampling</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Buffer size D = 100000, FIFO replacement; each stored tuple: (s_t, a_t, r_t, s_{t+1}, p_t) where p_t = 1 if r_t > 0 else 0. Minibatch size 64; sampling every 4 gameplay steps; fraction ρ = 0.25 of minibatch drawn from priority pool (positive-reward transitions), remainder drawn uniformly from other transitions. Purpose: decorrelate training data and oversample informative (positive) experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using prioritized sampling produced a marked speed-up: agent achieved optimal policy ~50 epochs earlier than with uniform sampling. Overall replay usage underpins the successful learning reported for LSTM-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Improves sample efficiency and convergence speed by reusing rare rewarding transitions more frequently; reduces volatility in updates caused by correlated on-policy transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Buffer is bounded and overwritten FIFO; priority signal used is binary and simplistic (reward>0), which may be suboptimal; the paper does not explore richer priority metrics or extensive ablations removing replay entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Maintain a large replay buffer and sample minibatches to decorrelate updates; oversample rare/positive transitions (the paper used ρ=0.25); consider prioritized sampling as it accelerated convergence substantially in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Understanding for Text-based Games using Deep Reinforcement Learning', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3258.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3258.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM representation (φ_R)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-based representation generator (φ_R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network (LSTM) that processes the sequence of words in the textual state description to produce token-level outputs which are mean-pooled to form a fixed-size state vector used by the action scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN (component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LSTM takes word embeddings (embedding size d=20) and produces outputs x_k for each word; final state representation v_s = (1/n) ∑_k x_k (mean pooling). LSTM is rolled out up to max 30 steps for Home world and up to 100 for Fantasy world. Parameters θ_R are learned jointly with the action scorer and can be transferred to new but related game worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Home world and Fantasy world</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Convert variable and possibly adversarial textual descriptions of underlying hidden game states into dense vectors that capture semantics needed for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory / recurrent sequence memory (LSTM over current textual observation); also functions as learned transferable memory when parameters are reused across games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Token-level LSTM over words producing outputs x_k; mean pooling to v_s; trained end-to-end with DQN objective. Word embeddings initialized randomly and trained (d=20). Max unrolled length: 30 (Home) and 100 (Fantasy). Transfer experiment: θ_R pretrained on Home world used to initialize φ_R in a new Home world mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Enables LSTM-DQN to reach 100% quest completion on Home world (≈50 epochs) and ≈96% completion with avg reward -11.33 on Fantasy bridge-crossing task; provides better average reward and fewer steps than non-recurrent baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines without recurrent sequence modeling: BOW-DQN (Home: ≈46% completion, avg reward ≈0.20; Fantasy: ≈82% completion, avg reward -12.68), BI-DQN (Home: ≈48% completion; Fantasy: ≈97% completion but worse avg reward -26.68).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Captures word order and composition, disambiguates adversarial quests (negation/conjunction), forms semantically coherent embeddings and representation clusters, improves robustness to description variability, and transfers across games to speed learning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>LSTM here only processes individual text observations and thus is not explicitly storing long-term episodic events across many timesteps (beyond parameter updates); the approach does not solve tasks requiring explicit memory of past events across long horizons (the secret-tomb quest is cited as beyond scope). Mean pooling omits using final hidden state alone; authors report mean pooling empirically fast but do not explore richer episodic-memory modules.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use sequence models (LSTM) over bag-of-words for text-state representation; employ mean pooling over token outputs for faster learning; pretrain φ_R and transfer to related environments to accelerate learning; consider longer unroll lengths when descriptions are longer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Understanding for Text-based Games using Deep Reinforcement Learning', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to win by reading manuals in a monte-carlo framework <em>(Rating: 2)</em></li>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Prioritized sweeping: Reinforcement learning with less data and less time <em>(Rating: 2)</em></li>
                <li>Speaking with your sidekick: Understanding situated speech in computer role playing games <em>(Rating: 1)</em></li>
                <li>Reading between the lines: Learning to map high-level instructions to commands <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3258",
    "paper_id": "paper-d00e7779c39dc7b06d7d43cf6de6d734c8edc4b8",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "LSTM-DQN",
            "name_full": "Long Short-Term Memory Deep Q-Network",
            "brief_description": "An end-to-end agent that jointly learns a sequence-based text representation (LSTM) and a Deep Q-Network action scorer to play text-based MUD games using Q-learning with experience replay and prioritized sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN",
            "agent_description": "Architecture composed of a representation generator φ_R implemented as an LSTM over word embeddings (outputs x_k, elementwise mean pooling to produce v_s) and an action scorer φ_A implemented as a multi-layer neural network that predicts Q-values for actions and objects; Q for a full command (action,object) is the average of action and object Qs. Trained end-to-end with Q-learning (DQN-style), RMSprop (lr=0.0005), γ=0.5, ε-annealing (1→0.2), an experience replay buffer (D=100000, FIFO) and prioritized sampling (ρ=0.25). Limits commands to one action + one object for tractability.",
            "game_or_benchmark_name": "Home world and Fantasy world (Evennia MUD environments)",
            "task_description": "Learning control policies from text descriptions to complete in-game quests (examples: navigate to kitchen and 'eat apple'; cross a broken bridge with a chance of falling; reach secret tomb—the latter noted as requiring longer-term memorization and planning). Evaluation metrics: average per-episode cumulative reward and fraction of quests completed.",
            "uses_memory": true,
            "memory_type": "recurrent internal memory (LSTM over text) and external experience replay buffer (episodic/experience memory) with prioritized sampling; also uses transfer of pretrained representation parameters (parameter-level transfer)",
            "memory_implementation_details": "Recurrent memory: LSTM processes word embeddings w_k producing outputs x_k; final state vector v_s = mean_k x_k (mean pooling). External memory: replay buffer D (size 100000) stores transitions (s_t,a_t,r_t,s_{t+1},p_t) with p_t = 1 if r_t&gt;0 else 0; buffer implemented FIFO and sampled to form minibatches (size 64) every 4 steps. Prioritized sampling draws fraction ρ=0.25 from positive-reward transitions. Transfer: φ_R parameters pretrained on one Home world were used to initialize LSTM in new world experiments to speed learning.",
            "performance_with_memory": "Home world: LSTM-DQN reaches ~100% quest completion after ≈50 epochs and attains near-optimal average reward (close to theoretical best when step penalty applied). Fantasy world: avg reward -11.33 and ≈96% quest completion on the evaluated bridge-crossing quest (reported in paper). Prioritized sampling: agent reaches optimal policy ≈50 epochs faster compared to uniform sampling.",
            "performance_without_memory": "Compared to non-recurrent / no-sequence baselines: Home world: BOW-DQN completes ≈46% of quests (avg reward ≈0.20), BI-DQN ≈48%. Fantasy world: BOW-DQN ≈82% completion (avg reward -12.68), BI-DQN ≈97% completion but worse avg reward -26.68. Random baseline: very poor (≈5–10% completion depending on world). (These baseline models do not use recurrent LSTM sequence memory.)",
            "has_performance_comparison": true,
            "memory_benefits": "Recurrent LSTM representation yields much better semantic understanding of textual state descriptions, improving quest completion rates, reducing steps to complete quests, and increasing resilience to variability in descriptions; the external replay buffer with prioritized sampling improves sample efficiency and convergence speed (faster learning and earlier attainment of optimal policy); transferring pretrained LSTM parameters speeds up learning in a new environment.",
            "memory_limitations_or_failures": "Replay buffer is finite and implemented FIFO; prioritized sampling uses a simple binary priority (p_t based on positive reward) which is coarse. The agent (and memory mechanisms used) do not address tasks requiring long-term episodic event memorization or complex high-level planning—the paper explicitly notes a complex 'secret tomb' quest requires memorizing game events and planning beyond the present approach. No ablation for removing replay entirely is reported. The agent also simplifies action space (single action + object), which limits evaluation of memory use in more complex command contexts.",
            "best_practices_or_recommendations": "Use sequence models (LSTM) to compose state representations from textual descriptions (mean pooling over LSTM outputs worked well here); maintain a large replay buffer and sample minibatches to decorrelate updates; prioritize sampling of rare/positive transitions (they used ρ≈0.25 and p_t=1 for r_t&gt;0) to speed learning; pretrain representation modules and transfer them across similar games to accelerate convergence. Acknowledge that tasks requiring long-term episodic memory or multi-step event recall will need additional mechanisms beyond LSTM + replay.",
            "uuid": "e3258.0",
            "source_info": {
                "paper_title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "Experience Replay + Prioritized Sampling",
            "name_full": "Experience replay memory with prioritized sampling",
            "brief_description": "External transition memory (replay buffer) used to store past (s,a,r,s') transitions to decorrelate training and improve sample efficiency, with a prioritized sampling scheme that oversamples positive-reward transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN (component)",
            "agent_description": "Replay buffer component of LSTM-DQN: stores transitions and supplies minibatches for gradient updates; uses a simple priority flag p_t=1 for transitions with positive reward and samples a fraction ρ of minibatch from high-priority pool.",
            "game_or_benchmark_name": "Home world and Fantasy world (Evennia MUDs)",
            "task_description": "Support off-policy Q-learning updates for the text-game control task by sampling past experiences to construct targets and reduce correlation between successive updates.",
            "uses_memory": true,
            "memory_type": "external episodic/experience memory (replay buffer) with prioritized sampling",
            "memory_implementation_details": "Buffer size D = 100000, FIFO replacement; each stored tuple: (s_t, a_t, r_t, s_{t+1}, p_t) where p_t = 1 if r_t &gt; 0 else 0. Minibatch size 64; sampling every 4 gameplay steps; fraction ρ = 0.25 of minibatch drawn from priority pool (positive-reward transitions), remainder drawn uniformly from other transitions. Purpose: decorrelate training data and oversample informative (positive) experiences.",
            "performance_with_memory": "Using prioritized sampling produced a marked speed-up: agent achieved optimal policy ~50 epochs earlier than with uniform sampling. Overall replay usage underpins the successful learning reported for LSTM-DQN.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Improves sample efficiency and convergence speed by reusing rare rewarding transitions more frequently; reduces volatility in updates caused by correlated on-policy transitions.",
            "memory_limitations_or_failures": "Buffer is bounded and overwritten FIFO; priority signal used is binary and simplistic (reward&gt;0), which may be suboptimal; the paper does not explore richer priority metrics or extensive ablations removing replay entirely.",
            "best_practices_or_recommendations": "Maintain a large replay buffer and sample minibatches to decorrelate updates; oversample rare/positive transitions (the paper used ρ=0.25); consider prioritized sampling as it accelerated convergence substantially in experiments.",
            "uuid": "e3258.1",
            "source_info": {
                "paper_title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "LSTM representation (φ_R)",
            "name_full": "LSTM-based representation generator (φ_R)",
            "brief_description": "A recurrent neural network (LSTM) that processes the sequence of words in the textual state description to produce token-level outputs which are mean-pooled to form a fixed-size state vector used by the action scorer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN (component)",
            "agent_description": "LSTM takes word embeddings (embedding size d=20) and produces outputs x_k for each word; final state representation v_s = (1/n) ∑_k x_k (mean pooling). LSTM is rolled out up to max 30 steps for Home world and up to 100 for Fantasy world. Parameters θ_R are learned jointly with the action scorer and can be transferred to new but related game worlds.",
            "game_or_benchmark_name": "Home world and Fantasy world",
            "task_description": "Convert variable and possibly adversarial textual descriptions of underlying hidden game states into dense vectors that capture semantics needed for action selection.",
            "uses_memory": true,
            "memory_type": "working memory / recurrent sequence memory (LSTM over current textual observation); also functions as learned transferable memory when parameters are reused across games",
            "memory_implementation_details": "Token-level LSTM over words producing outputs x_k; mean pooling to v_s; trained end-to-end with DQN objective. Word embeddings initialized randomly and trained (d=20). Max unrolled length: 30 (Home) and 100 (Fantasy). Transfer experiment: θ_R pretrained on Home world used to initialize φ_R in a new Home world mapping.",
            "performance_with_memory": "Enables LSTM-DQN to reach 100% quest completion on Home world (≈50 epochs) and ≈96% completion with avg reward -11.33 on Fantasy bridge-crossing task; provides better average reward and fewer steps than non-recurrent baselines.",
            "performance_without_memory": "Baselines without recurrent sequence modeling: BOW-DQN (Home: ≈46% completion, avg reward ≈0.20; Fantasy: ≈82% completion, avg reward -12.68), BI-DQN (Home: ≈48% completion; Fantasy: ≈97% completion but worse avg reward -26.68).",
            "has_performance_comparison": true,
            "memory_benefits": "Captures word order and composition, disambiguates adversarial quests (negation/conjunction), forms semantically coherent embeddings and representation clusters, improves robustness to description variability, and transfers across games to speed learning.",
            "memory_limitations_or_failures": "LSTM here only processes individual text observations and thus is not explicitly storing long-term episodic events across many timesteps (beyond parameter updates); the approach does not solve tasks requiring explicit memory of past events across long horizons (the secret-tomb quest is cited as beyond scope). Mean pooling omits using final hidden state alone; authors report mean pooling empirically fast but do not explore richer episodic-memory modules.",
            "best_practices_or_recommendations": "Use sequence models (LSTM) over bag-of-words for text-state representation; employ mean pooling over token outputs for faster learning; pretrain φ_R and transfer to related environments to accelerate learning; consider longer unroll lengths when descriptions are longer.",
            "uuid": "e3258.2",
            "source_info": {
                "paper_title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
                "publication_date_yy_mm": "2015-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to win by reading manuals in a monte-carlo framework",
            "rating": 2
        },
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Prioritized sweeping: Reinforcement learning with less data and less time",
            "rating": 2
        },
        {
            "paper_title": "Speaking with your sidekick: Understanding situated speech in computer role playing games",
            "rating": 1
        },
        {
            "paper_title": "Reading between the lines: Learning to map high-level instructions to commands",
            "rating": 1
        }
    ],
    "cost": 0.01429175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Understanding for Text-based Games using Deep Reinforcement Learning</h1>
<p>Karthik Narasimhan*<br>CSAIL, MIT<br>karthikn@csail.mit.edu</p>
<p>Tejas D Kulkarni*<br>CSAIL, BCS, MIT<br>tejask@mit.edu</p>
<p>Regina Barzilay<br>CSAIL, MIT<br>regina@csail.mit.edu</p>
<h4>Abstract</h4>
<p>In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-ofwords and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>In this paper, we address the task of learning control policies for text-based strategy games. These games, predecessors to modern graphical ones, still enjoy a large following worldwide. ${ }^{2}$ They often involve complex worlds with rich interactions and elaborate textual descriptions of the underlying states (see Figure 1). Players read descriptions of the current world state and respond with natural language commands to take actions. Since the underlying state is not directly observable, the player has to understand the text in order to act, making it</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>State 1: The old bridge
You are standing very close to the bridge's eastern foundation. If you go east you will be back on solid ground ... The bridge sways in the wind.</p>
<p>Command: Go east
State 2: Ruined gatehouse
The old gatehouse is near collapse. Part of its northern wall has already fallen down ... East of the gatehouse leads out to a small open area surrounded by the remains of the castle. There is also a standing archway offering passage to a path along the old southern inner wall.
Exits: Standing archway, castle corner, Bridge over the abyss</p>
<p>Figure 1: Sample gameplay from a Fantasy World. The player with the quest of finding a secret tomb, is currently located on an old bridge. She then chooses an action to go east that brings her to a ruined gatehouse (State 2).
challenging for existing AI programs to play these games (DePristo and Zubek, 2001).</p>
<p>In designing an autonomous game player, we have considerable latitude when selecting an adequate state representation to use. The simplest method is to use a bag-of-words representation derived from the text description. However, this scheme disregards the ordering of words and the finer nuances of meaning that evolve from composing words into sentences and paragraphs. For instance, in State 2 in Figure 1, the agent has to understand that going east will lead it to the castle whereas moving south will take it to the standing archway. An alternative approach is to convert text descriptions to pre-specified representations using annotated training data, commonly used in</p>
<p>language grounding tasks (Matuszek et al., 2013; Kushman et al., 2014).</p>
<p>In contrast, our goal is to learn useful representations in conjunction with control policies. We adopt a reinforcement learning framework and formulate game sequences as Markov Decision Processes. An agent playing the game aims to maximize rewards that it obtains from the game engine upon the occurrence of certain events. The agent learns a policy in the form of an action-value function $Q(s, a)$ which denotes the long-term merit of an action $a$ in state $s$.</p>
<p>The action-value function is parametrized using a deep recurrent neural network, trained using the game feedback. The network contains two modules. The first one converts textual descriptions into vector representations that act as proxies for states. This component is implemented using Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). The second module of the network scores the actions given the vector representation computed by the first.</p>
<p>We evaluate our model using two Multi-User Dungeon (MUD) games (Curtis, 1992; Amir and Doyle, 2002). The first game is designed to provide a controlled setup for the task, while the second is a publicly available one and contains human generated text descriptions with significant language variability. We compare our algorithm against baselines of a random player and models that use bag-of-words or bag-of-bigrams representations for a state. We demonstrate that our model LSTM-DQN significantly outperforms the baselines in terms of number of completed quests and accumulated rewards. For instance, on a fantasy MUD game, our model learns to complete $96 \%$ of the quests, while the bag-of-words model and a random baseline solve only $82 \%$ and $5 \%$ of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values.</p>
<h2>2 Related Work</h2>
<p>Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015)
and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a).</p>
<p>Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related documents and precompiled traces of the game. In contrast to the above work, our model combines text interpretation and strategy learning in a single framework. As a result, textual analysis is guided by the received control feedback, and the learned strategy directly builds on the text interpretation.</p>
<p>Our work closely relates to an automatic game player that utilizes text manuals to learn strategies for Civilization (Branavan et al., 2011a). Similar to our approach, text analysis and control strategies are learned jointly using feedback provided by the game simulation. In their setup, states are fully observable, and the model learns a strategy by combining state/action features and features extracted from text. However, in our application, the state representation is not provided, but has to be inferred from a textual description. Therefore, it is not sufficient to extract features from text to supplement a simulation-based player.</p>
<p>Another related line of work consists of automatic video game players that infer state representations directly from raw pixels (Koutník et al., 2013; Mnih et al., 2015). For instance, Mnih et al. (2015) learn control strategies using convolutional neural networks, trained with a variant of Q-learning (Watkins and Dayan, 1992). While both approaches use deep reinforcement learning for training, our work has important differences. In order to handle the sequential nature of text, we use Long Short-Term Memory networks to automatically learn useful representations for arbitrary text descriptions. Additionally, we show that decomposing the network into a representation layer and an action selector is useful for transferring the learnt representations to new game scenarios.</p>
<h2>3 Background</h2>
<p>Game Representation We represent a game by the tuple $\langle H, A, T, R, \Psi\rangle$, where $H$ is the set of</p>
<p>all possible game states, $A={(a, o)}$ is the set of all commands (action-object pairs), $T\left(h^{\prime} \mid h, a, o\right)$ is the stochastic transition function between states and $R(h, a, o)$ is the reward function. The game state $H$ is hidden from the player, who only receives a varying textual description, produced by a stochastic function $\Psi: H \rightarrow S$. Specifically, the underlying state $h$ in the game engine keeps track of attributes such as the player's location, her health points, time of day, etc. The function $\Psi$ (also part of the game framework) then converts this state into a textual description of the location the player is at or a message indicating low health. We do not assume access to either $H$ or $\Psi$ for our agent during both training and testing phases of our experiments. We denote the space of all possible text descriptions $s$ to be $S$. Rewards are generated using $R$ and are only given to the player upon completion of in-game quests.</p>
<p>Q-Learning Reinforcement Learning is a commonly used framework for learning control policies in game environments (Silver et al., 2007; Amato and Shani, 2010; Branavan et al., 2011b; Szita, 2012). The game environment can be formulated as a sequence of state transitions $\left(s, a, r, s^{\prime}\right)$ of a Markov Decision Process (MDP). The agent takes an action $a$ in state $s$ by consulting a state-action value function $Q(s, a)$, which is a measure of the action's expected long-term reward. Q-Learning (Watkins and Dayan, 1992) is a model-free technique which is used to learn an optimal $Q(s, a)$ for the agent. Starting from a random Q-function, the agent continuously updates its Q-values by playing the game and obtaining rewards. The iterative updates are derived from the Bellman equation (Sutton and Barto, 1998):</p>
<p>$$
Q_{i+1}(s, a)=\mathrm{E}\left[r+\gamma \max <em i="i">{a^{\prime}} Q</em>\right) \mid s, a\right]
$$}\left(s^{\prime}, a^{\prime</p>
<p>where $\gamma$ is a discount factor for future rewards and the expectation is over all game transitions that involved the agent taking action $a$ in state $s$.</p>
<p>Using these evolving Q-values, the agent chooses the action with the highest $Q(s, a)$ to maximize its expected future rewards. In practice, the trade-off between exploration and exploitation can be achieved following an $\epsilon$-greedy policy (Sutton and Barto, 1998), where the agent performs a random action with probability $\epsilon$.</p>
<p>Deep Q-Network In large games, it is often impractical to maintain the Q-value for all possible
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Architecture of LSTM-DQN: The Representation Generator $\left(\phi_{R}\right)$ (bottom) takes as input a stream of words observed in state $s$ and produces a vector representation $v_{s}$, which is fed into the action scorer $\left(\phi_{A}\right)$ (top) to produce scores for all actions and argument objects.
state-action pairs. One solution to this problem is to approximate $Q(s, a)$ using a parametrized function $Q(s, a ; \theta)$, which can generalize over states and actions by considering higher-level attributes (Sutton and Barto, 1998; Branavan et al., 2011a). However, creating a good parametrization requires knowledge of the state and action spaces. One way to bypass this feature engineering is to use a Deep Q-Network (DQN) (Mnih et al., 2015). The DQN approximates the Q-value function with a deep neural network to predict $Q(s, a)$ for all possible actions $a$ simultaneously given the current state $s$. The non-linear function layers of the DQN also enable it to learn better value functions than linear approximators.</p>
<h2>4 Learning Representations and Control Policies</h2>
<p>In this section, we describe our model (DQN) and describe its use in learning good Q-value approximations for games with stochastic textual descriptions. We divide our model into two parts. The first module is a representation generator that converts the textual description of the current state into a vector. This vector is then input into the second module which is an action scorer. Figure 2 shows the overall architecture of our model. We learn the parameters of both the representation generator and the action scorer jointly, using the in-game reward feedback.</p>
<p>Representation Generator ( $\phi_{R}$ ) The representation generator reads raw text displayed to the agent and converts it to a vector representation $v_{s}$. A bag-of-words (BOW) representation is not sufficient to capture higher-order structures of sentences and paragraphs. The need for a better semantic representation of the text is evident from the average performance of this representation in playing MUD-games (as we show in Section 6).</p>
<p>In order to assimilate better representations, we utilize a Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) as a representation generator. LSTMs are recurrent neural networks with the ability to connect and recognize long-range patterns between words in text. They are more robust than BOW to small variations in word usage and are able to capture underlying semantics of sentences to some extent. In recent work, LSTMs have been used successfully in NLP tasks such as machine translation (Sutskever et al., 2014) and sentiment analysis (Tai et al., 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014). In our setup, the LSTM network takes in word embeddings $w_{k}$ from the words in a description $s$ and produces output vectors $x_{k}$ at each step.</p>
<p>To get the final state representation $v_{s}$, we add a mean pooling layer which computes the elementwise mean over the output vectors $x_{k} .^{3}$</p>
<p>$$
v_{s}=\frac{1}{n} \sum_{k=1}^{n} x_{k}
$$</p>
<p>Action Scorer $\left(\phi_{A}\right)$ The action scorer module produces scores for the set of possible actions given the current state representation. We use a multi-layered neural network for this purpose (see Figure 2). The input to this module is the vector from the representation generator, $v_{s}=\phi_{R}(s)$ and the outputs are scores for actions $a \in A$. Scores for all actions are predicted simultaneously, which is computationally more efficient than scoring each state-action pair separately. Thus, by combining the representation generator and action scorer, we can obtain the approximation for the Qfunction as $Q(s, a) \approx \phi_{A}\left(\phi_{R}(s)\right)[a]$.</p>
<p>An additional complexity in playing MUDgames is that the actions taken by the player are</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>multi-word natural language commands such as eat apple or go east. Due to computational constraints, in this work we limit ourselves to consider commands to consist of one action (e.g. eat) and one argument object (e.g. apple). This assumption holds for the majority of the commands in our worlds, with the exception of one class of commands that require two arguments (e.g. move red-root right, move blue-root up). We consider all possible actions and objects available in the game and predict both for each state using the same network (Figure 2). We consider the Q-value of the entire command $(a, o)$ to be the average of the Qvalues of the action $a$ and the object $o$. For the rest of this section, we only show equations for $Q(s, a)$ but similar ones hold for $Q(s, o)$.</p>
<p>Parameter Learning We learn the parameters $\theta_{R}$ of the representation generator and $\theta_{A}$ of the action scorer using stochastic gradient descent with RMSprop (Tieleman and Hinton, 2012). The complete training procedure is shown in Algorithm 1. In each iteration $i$, we update the parameters to reduce the discrepancy between the predicted value of the current state $Q\left(s_{t}, a_{t} ; \theta_{i}\right)$ (where $\theta_{i}=\left[\theta_{R} ; \theta_{A}\right]<em t="t">{i}$ ) and the expected Q-value given the reward $r</em>$ and the value of the next state $\max <em t_1="t+1">{a} Q\left(s</em>\right)$.}, a ; \theta_{i-1</p>
<p>We keep track of the agent's previous experiences in a memory $\mathcal{D} .{ }^{4}$ Instead of performing updates to the Q-value using transitions from the current episode, we sample a random transition $\left(\hat{s}, \hat{a}, s^{\prime}, r\right)$ from $\mathcal{D}$. Updating the parameters in this way avoids issues due to strong correlation when using transitions of the same episode (Mnih et al., 2015). Using the sampled transition and (1), we obtain the following loss function to minimize:</p>
<p>$$
\mathcal{L}<em i="i">{i}\left(\theta</em>}\right)=\mathrm{E<em i="i">{\hat{s}, \hat{a}}\left[\left(y</em>\right]
$$}-Q\left(\hat{s}, \hat{a} ; \theta_{i}\right)\right)^{2</p>
<p>where $y_{i}=\mathrm{E}<em a_prime="a^{\prime">{\hat{s}, \hat{a}}\left[r+\gamma \max </em>$ fixed from the previous iteration.}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right) \mid \hat{s}, \hat{a}\right]$ is the target Q-value with parameters $\theta_{i-1</p>
<p>The updates on the parameters $\theta$ can be performed using the following gradient of $\mathcal{L}<em i="i">{i}\left(\theta</em>\right)$ :
$\nabla_{\theta_{i}} \mathcal{L}<em i="i">{i}\left(\theta</em>}\right)=\mathrm{E<em i="i">{\hat{s}, \hat{a}}\left[2\left(y</em>\right)\right]$
For each epoch of training, the agent plays several episodes of the game, which is restarted after every terminal state.}-Q\left(\hat{s}, \hat{a} ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q\left(\hat{s}, \hat{a} ; \theta_{i</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Training</span><span class="w"> </span><span class="nx">Procedure</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">DQN</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">prioritized</span><span class="w"> </span><span class="nx">sampling</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">experience</span><span class="w"> </span><span class="nx">memory</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">representation</span><span class="w"> </span><span class="nx">generator</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">R</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">scorer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">randomly</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">episode</span><span class="w"> </span><span class="err">\</span><span class="p">(=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nx">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">game</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">get</span><span class="w"> </span><span class="nx">start</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="nx">description</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="p">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nx">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">Convert</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">text</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">representation</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">R</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">right</span><span class="p">)&lt;</span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="nx">Select</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">else</span>
<span class="w">                </span><span class="nx">Compute</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">a</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">actions</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="nx">Select</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">argmax</span><span class="p">}</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">a</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Execute</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Set</span><span class="w"> </span><span class="nx">priority</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}&gt;</span><span class="mi">0</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Store</span><span class="w"> </span><span class="nx">transition</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">mini</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">transitions</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="nx">with</span><span class="w"> </span><span class="nx">fraction</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">rho</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">having</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="nx">j</span><span class="p">}=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">y_</span><span class="p">{</span><span class="nx">j</span><span class="p">}=</span><span class="w"> </span><span class="err">\</span><span class="nx">begin</span><span class="p">{</span><span class="nx">cases</span><span class="p">}</span><span class="nx">r_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">terminal</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="err">\\</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">gamma</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}}</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">a</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">non</span><span class="o">-</span><span class="nx">terminal</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nx">end</span><span class="p">{</span><span class="nx">cases</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Perform</span><span class="w"> </span><span class="nx">gradient</span><span class="w"> </span><span class="nx">descent</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">loss</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">L</span><span class="p">}(</span><span class="err">\</span><span class="nx">theta</span><span class="p">)=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">y_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="o">-</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Mini-batch Sampling In practice, online updates to the parameters $\theta$ are performed over a mini batch of state transitions, instead of a single transition. This increases the number of experiences used per step and is also more efficient due to optimized matrix operations.</p>
<p>The simplest method to create these minibatches from the experience memory $\mathcal{D}$ is to sample uniformly at random. However, certain experiences are more valuable than others for the agent to learn from. For instance, rare transitions that provide positive rewards can be used more often to learn optimal Q-values faster. In our experiments, we consider such positive-reward transitions to have higher priority and keep track of them in $\mathcal{D}$. We use prioritized sampling (inspired by Moore and Atkeson (1993)) to sample a fraction $\rho$ of transitions from the higher priority pool and a fraction $1-\rho$ from the rest.</p>
<h2>5 Experimental Setup</h2>
<p>Game Environment For our game environment, we modify Evennia, ${ }^{5}$ an open-source library for building online textual MUD games. Evennia is a Python-based framework that allows one to easily create new games by writing a batch file describing the environment with details of rooms,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Various statistics of the two game worlds
objects and actions. The game engine keeps track of the game state internally, presenting textual descriptions to the player and receiving text commands from the player. We conduct experiments on two worlds - a smaller Home world we created ourselves, and a larger, more complex Fantasy world created by Evennia's developers. The motivation behind Home world is to abstract away high-level planning and focus on the language understanding requirements of the game.</p>
<p>Table 1 provides statistics of the game worlds. We observe that the Fantasy world is moderately sized with a vocabulary of 1340 words and up to 100 different descriptions for a room. These descriptions were created manually by the game developers. These diverse, engaging descriptions are designed to make it interesting and exciting for human players. Several rooms have many alternative descriptions, invoked randomly on each visit by</p>
<p>the player.
Comparatively, the Home world is smaller: it has a very restricted vocabulary of 84 words and the room descriptions are relatively structured. However, both the room descriptions (which are also varied and randomly provided to the agent) and the quest descriptions were adversarially created with negation and conjunction of facts to force an agent to actually understand the state in order to play well. Therefore, this domain provides an interesting challenge for language understanding.</p>
<p>In both worlds, the agent receives a positive reward on completing a quest, and negative rewards for getting into bad situations like falling off a bridge, or losing a battle. We also add small deterministic negative rewards for each nonterminating step. This incentivizes the agent to learn policies that solve quests in fewer steps. The supplementary material has details on the reward structure.</p>
<p>Home World We created Home world to mimic the environment of a typical house. ${ }^{6}$ The world consists of four rooms - a living room, a bedroom, a kitchen and a garden with connecting pathways. Every room is reachable from every other room. Each room contains a representative object that the agent can interact with. For instance, the kitchen has an apple that the player can eat. Transitions between the rooms are deterministic. At the start of each game episode, the player is placed in a random room and provided with a randomly selected quest. The text provided to the player contains both the description of her current state and that of the quest. Thus, the player can begin in one of 16 different states ( 4 rooms $\times 4$ quests), which adds to the world's complexity.</p>
<p>An example of a quest given to the player in text is Not you are sleepy now but you are hungry now. To complete this quest and obtain a reward, the player has to navigate through the house to reach the kitchen and eat the apple (i.e type in the command eat apple). More importantly, the player should interpret that the quest does not require her to take a nap in the bedroom. We created such misguiding quests to make it hard for agents to succeed without having an adequate level of language understanding.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Fantasy World The Fantasy world is considerably more complex and involves quests such as navigating through a broken bridge or finding the secret tomb of an ancient hero. This game also has stochastic transitions in addition to varying state descriptions provided to the player. For instance, there is a possibility of the player falling from the bridge if she lingers too long on it.</p>
<p>Due to the large command space in this game, ${ }^{7}$ we make use of cues provided by the game itself to narrow down the set of possible objects to consider in each state. For instance, in the MUD example in Figure 1, the game provides a list of possible exits. If the game does not provide such clues for the current state, we consider all objects in the game.</p>
<p>Evaluation We use two metrics for measuring an agent's performance: (1) the cumulative reward obtained per episode averaged over the episodes and (2) the fraction of quests completed by the agent. The evaluation procedure is as follows. In each epoch, we first train the agent on $M$ episodes of $T$ steps each. At the end of this training, we have a testing phase of running $M$ episodes of the game for $T$ steps. We use $M=50, T=20$ for the Home world and $M=20, T=250$ for the Fantasy world. For all evaluation episodes, we run the agent following an $\epsilon$-greedy policy with $\epsilon=0.05$, which makes the agent choose the best action according to its Q -values $95 \%$ of the time. We report the agent's performance at each epoch.</p>
<p>Baselines We compare our LSTM-DQN model with three baselines. The first is a Random agent that chooses both actions and objects uniformly at random from all available choices. ${ }^{8}$ The other two are BOW-DQN and BI-DQN, which use a bag-of-words and a bag-of-bigrams representation of the text, respectively, as input to the DQN action scorer. These baselines serve to illustrate the importance of having a good representation layer for the task.</p>
<p>Settings For our DQN models, we used $\mathcal{D}=$ $100000, \gamma=0.5$. We use a learning rate of 0.0005 for RMSprop. We anneal the $\epsilon$ for $\epsilon$-greedy from 1 to 0.2 over 100000 transitions. A mini-batch gradient update is performed every 4 steps of the gameplay. We roll out the LSTM (over words) for</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Left: Graphs showing the evolution of average reward and quest completion rate for BOW-DQN, LSTM-DQN and a Random baseline on the Home world (top) and Fantasy world (bottom). Note that the reward is shown in log scale for the Fantasy world. Right: Graphs showing effects of transfer learning and prioritized sampling on the Home world.</p>
<p>a maximum of 30 steps on the Home world and for 100 steps on the Fantasy world. For the prioritized sampling, we used ρ = 0.25 for both worlds. We employed a mini-batch size of 64 and word embedding size d = 20 in all experiments.</p>
<h2>6 Results</h2>
<p>Home World Figure 3 illustrates the performance of LSTM-DQN compared to the baselines. We can observe that the Random baseline performs quite poorly, completing only around 10% of quests on average obtaining a low reward of around -1.58. The BOW-DQN model performs significantly better and is able to complete around 46% of the quests, with an average reward of 0.20. The improvement in reward is due to both greater quest success rate and a lower rate of issuing invalid commands (e.g. <em>eat apple</em> would be invalid in the bedroom since there is no apple). We notice that both the reward and quest completion graphs of this model are volatile. This is because the model fails to pick out differences between quests like <em>Not you are hungry now but you are sleepy now</em> and <em>Not you are sleepy now but you are hungry now</em>. The BI-DQN model suffers from the same issue although it performs slightly better than BOW-DQN by completing 48% of quests. In contrast, the LSTM-DQN model does not suffer from this issue and is able to complete 100% of the quests after around 50 epochs of training, achieving close to the optimal reward possible. This demonstrates that having an expressive representation for text is crucial to understanding the game states and choosing intelligent actions.</p>
<p>In addition, we also investigated the impact of using a deep neural network for modeling the action score ϕA. Figure 4 illustrates the performance of the BOW-DQN and BI-DQN models along with their simpler versions BOW-LIN and BI-LIN, which use a single linear layer for ϕA. It can be seen that the DQN models clearly achieve better performance than their linear counterparts, which points to them modeling the control policy better.</p>
<p>Fantasy World We evaluate all the models on the Fantasy world in the same manner as before and report reward, quest completion rates and Q-</p>
<p><sup>9</sup>Averaged over the last 10 epochs.</p>
<p><sup>10</sup>Note that since each step incurs a penalty of -0.01, the best reward (on average) a player can get is around 0.98.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Quest completion rates of DQN vs. Linear models on Home world.</p>
<p>Values. The quest we evaluate on involves crossing the broken bridge (which takes a minimum of five steps), with the possibility of falling off at random (a 5% chance) when the player is on the bridge. The game has an additional quest of reaching a secret tomb. However, this is a complex quest that requires the player to memorize game events and perform high-level planning which are beyond the scope of this current work. Therefore, we focus only on the first quest.</p>
<p>From Figure 3 (bottom), we can see that the Random baseline does poorly in terms of both average per-episode reward<sup id="fnref:11"><a class="footnote-ref" href="#fn:11">3</a></sup> and quest completion rates. BOW-DQN converges to a much higher average reward of -12.68 and achieves around 82% quest completion. Again, the BOW-DQN is often confused by varying (10 different) descriptions of the portions of the bridge, which reflects in its erratic performance on the quest. The BI-DQN performs very well on quest completion by finishing 97% of quests. However, this model tends to find sub-optimal solutions and gets an average reward of -26.68, even worse than BOW-DQN. One reason for this is the negative rewards the agent obtains after falling off the bridge. The LSTM-DQN model again performs best, achieving an average reward of -11.33 and completing 96% of quests on average. Though this world does not contain descriptions adversarial to BOW-DQN or BI-DQN, the LSTM-DQN obtains higher average reward by completing the quest in fewer steps and showing more resilience to variations in the state descriptions.</p>
<h3>Transfer Learning</h3>
<p>We would like the representations learnt by φ<sup>R</sup> to be generic enough and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: t-SNE visualization of the word embeddings (except stopwords) after training on Home world. The embedding values are initialized randomly.</p>
<p><em>Transferable</em> to new game worlds. To test this, we created a second Home world with the same rooms, but a completely different map, changing the locations of the rooms and the pathways between them. The main differentiating factor of this world from the original home world lies in the high-level planning required to complete quests.</p>
<p>We initialized the LSTM part of an LSTM-DQN agent with parameters θ<sup>R</sup> learnt from the original home world and trained it on the new world.<sup id="fnref:12"><a class="footnote-ref" href="#fn:12">4</a></sup> Figure 3 (top right) demonstrates that the agent with transferred parameters is able to learn quicker than an agent starting from scratch initialized with random parameters (<em>No Transfer</em>), reaching the optimal policy almost 20 epochs earlier. This indicates that these simulated worlds can be used to learn good representations for language that transfer across worlds.</p>
<h3>Prioritized sampling</h3>
<p>We also investigate the effects of different minibatch sampling procedures on the parameter learning. From Figure 3 (bottom right), we observe that using prioritized sampling significantly speeds up learning, with the agent achieving the optimal policy around 50 epochs faster than using uniform sampling. This shows promise for further research into different schemes of assigning priority to transitions.</p>
<h3>Representation Analysis</h3>
<p>We analyzed the representations learnt by the LSTM-DQN model on the Home world. Figure 5 shows a visualization</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Nearest neighbor</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">You are halfways out on the unstable bridge. From the castle <br> you hear a distant howling sound, like that of a large dog or <br> other beast.</td>
<td style="text-align: left;">The bridge slopes precariously where it extends westwards to- <br> wards the lowest point - the center point of the hang bridge. You <br> clasp the ropes firmly as the bridge sways and creaks under you.</td>
</tr>
<tr>
<td style="text-align: left;">The ruins opens up to the sky in a small open area, lined by <br> columns. ... To the west is the gatehouse and entrance to the <br> castle, whereas southwards the columns make way for a wide <br> open courtyard.</td>
<td style="text-align: left;">The old gatehouse is near collapse. .... East the gatehouse leads <br> out to a small open area surrounded by the remains of the cas- <br> tle. There is also a standing archway offering passage to a path <br> along the old southern inner wall.</td>
</tr>
</tbody>
</table>
<p>Table 2: Sample descriptions from the Fantasy world and their nearest neighbors (NN) according to their vector representations from the LSTM representation generator. The NNs are often descriptions of the same or similar (nearby) states in the game.
of learnt word embeddings, reduced to two dimensions using t-SNE (Van der Maaten and Hinton, 2008). All the vectors were initialized randomly before training. We can see that semantically similar words appear close together to form coherent subspaces. In fact, we observe four different subspaces, each for one type of room along with its corresponding object(s) and quest words. For instance, food items like pizza and rooms like kitchen are very close to the word hungry which appears in a quest description. This shows that the agent learns to form meaningful associations between the semantics of the quest and the environment. Table 2 shows some examples of descriptions from Fantasy world and their nearest neighbors using cosine similarity between their corresponding vector representations produced by LSTM-DQN. The model is able to correlate descriptions of the same (or similar) underlying states and project them onto nearby points in the representation subspace.</p>
<h2>7 Conclusions</h2>
<p>We address the task of end-to-end learning of control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language variability makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. Our experiments demonstrate the importance of learning good representations of text in order to play these games well. Future directions include tackling high-level
planning and strategy learning to improve the performance of intelligent agents.</p>
<h2>Acknowledgements</h2>
<p>We are grateful to the developers of Evennia, the game framework upon which this work is based. We also thank Nate Kushman, Clement Gehring, Gustavo Goretkin, members of MIT's NLP group and the anonymous EMNLP reviewers for insightful comments and feedback. T. Kulkarni was graciously supported by the Leventhal Fellowship. We would also like to acknowledge MIT's Center for Brains, Minds and Machines (CBMM) for support.</p>
<h2>References</h2>
<p>Christopher Amato and Guy Shani. 2010. High-level reinforcement learning in strategy games. In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1, pages 75-82. International Foundation for Autonomous Agents and Multiagent Systems.</p>
<p>Eyal Amir and Patrick Doyle. 2002. Adventure games: A challenge for cognitive robotics. In Proc. Int. Cognitive Robotics Workshop, pages 148-155.</p>
<p>Jacob Andreas and Dan Klein. 2015. Alignmentbased compositional semantics for instruction following. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</p>
<p>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49-62.</p>
<p>SRK Branavan, Luke S Zettlemoyer, and Regina Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 12681277. Association for Computational Linguistics.</p>
<p>SRK Branavan, David Silver, and Regina Barzilay. 2011a. Learning to win by reading manuals in a monte-carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 268-277. Association for Computational Linguistics.</p>
<p>SRK Branavan, David Silver, and Regina Barzilay. 2011b. Non-linear monte-carlo search in Civilization II. AAAI Press/International Joint Conferences on Artificial Intelligence.</p>
<p>Pavel Curtis. 1992. Mudding: Social phenomena in text-based virtual realities. High noon on the electronic frontier: Conceptual issues in cyberspace, pages 347-374.</p>
<p>Mark A DePristo and Robert Zubek. 2001. being-in-the-world. In Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment, pages 31-34.</p>
<p>Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 958-967, Singapore, August. Association for Computational Linguistics.</p>
<p>Peter Gorniak and Deb Roy. 2005. Speaking with your sidekick: Understanding situated speech in computer role playing games. In R. Michael Young and John E. Laird, editors, Proceedings of the First Artificial Intelligence and Interactive Digital Entertainment Conference, June 1-5, 2005, Marina del Rey, California, USA, pages 57-62. AAAI Press.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. 2010. Toward understanding natural language directions. In Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference on, pages 259-266. IEEE.</p>
<p>Jan Koutník, Giuseppe Cuccu, Jürgen Schmidhuber, and Faustino Gomez. 2013. Evolving largescale neural networks for vision-based reinforcement learning. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 1061-1068. ACM.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. ACL (1), pages 271281.</p>
<p>Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2013. Learning to parse natural language commands to a robot control system. In Experimental Robotics, pages 403-415. Springer.</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 02.</p>
<p>Andrew W Moore and Christopher G Atkeson. 1993. Prioritized sweeping: Reinforcement learning with less data and less time. Machine Learning, 13(1):103-130.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</p>
<p>David Silver, Richard S Sutton, and Martin Müller. 2007. Reinforcement learning of local shape in the game of go. In IJCAI, volume 7, pages 1053-1058.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104-3112.</p>
<p>Richard S Sutton and Andrew G Barto. 1998. Introduction to reinforcement learning. MIT Press.</p>
<p>István Szita. 2012. Reinforcement learning in games. In Reinforcement Learning, pages 539-577. Springer.</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1556-1566, Beijing, China, July. Association for Computational Linguistics.</p>
<p>Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4.</p>
<p>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85.</p>
<p>Adam Vogel and Dan Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806-814. Association for Computational Linguistics.</p>
<p>Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning, 8(3-4):279-292.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ An illustration is provided in the supplementary material.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ We consider 222 possible command combinations of 6 actions and 37 object arguments.
${ }^{8}$ In the case of the Fantasy world, the object choices are narrowed down using game clues as described earlier.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:11">
<p>Note that the rewards graph is in log scale.&#160;<a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:12">
<p>The parameters for the Action Scorer (θ<sup>A</sup>) are initialized randomly.&#160;<a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
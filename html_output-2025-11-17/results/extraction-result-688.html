<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-688 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-688</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-688</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-5818cae37a473d279faf096295f4f06a358d3054</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5818cae37a473d279faf096295f4f06a358d3054" target="_blank">Sources of Irreproducibility in Machine Learning: A Review</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A framework is provided that describes machine learning methodology from experimental design decisions to the conclusions inferred from them and which experiment design choices can lead to false findings and how and by this help in analyzing the conclusions of reproducibility experiments.</p>
                <p><strong>Paper Abstract:</strong> Background: Many published machine learning studies are irreproducible. Issues with methodology and not properly accounting for variation introduced by the algorithm themselves or their implementations are attributed as the main contributors to the irreproducibility.Problem: There exist no theoretical framework that relates experiment design choices to potential effects on the conclusions. Without such a framework, it is much harder for practitioners and researchers to evaluate experiment results and describe the limitations of experiments. The lack of such a framework also makes it harder for independent researchers to systematically attribute the causes of failed reproducibility experiments. Objective: The objective of this paper is to develop a framework that enable applied data science practitioners and researchers to understand which experiment design choices can lead to false findings and how and by this help in analyzing the conclusions of reproducibility experiments. Method: We have compiled an extensive list of factors reported in the literature that can lead to machine learning studies being irreproducible. These factors are organized and categorized in a reproducibility framework motivated by the stages of the scientific method. The factors are analyzed for how they can affect the conclusions drawn from experiments. A model comparison study is used as an example. Conclusion: We provide a framework that describes machine learning methodology from experimental design decisions to the conclusions inferred from them.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e688.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e688.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Doc-vs-Code mismatch (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrepancies between natural language experiment descriptions and code implementations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A broad class of faithfulness gaps where the written paper, methods section, or documentation either contradicts, omits, or ambiguously specifies the behavior implemented in code, leading to irreproducible or misinterpreted experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML experiment / research codebase</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Typical experimental ML pipeline where a paper's methods section, README, or pseudocode is intended to describe an implementation that is also provided (or re-implemented) by others.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / README / pseudo-code</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reference implementation / library code / experiment script</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous description / documentation error</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural language descriptions (methods, pseudocode, README) fail to fully or accurately specify implementation details (initialization, preprocessing, hyperparameter defaults, or algorithmic variants), causing independent implementers or re-users to produce differing code behavior than intended by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>varies across pipeline: initialization, preprocessing, hyperparameters, training procedure, evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>survey of literature and reproducibility analyses; comparison of documented protocol vs observed code behavior; reported failed reproducibility attempts</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified in a single metric in this paper; recommended measurement is to re-run experiments, compare outcomes, and estimate variation/error (e.g., distribution of scores, confidence intervals, statistical tests)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can change outcomes, analyses, and interpretations; may cause irreproducible conclusions and false claims. Paper notes many cases where lack of specification prevents outcome reproducibility and can change conclusions, but provides no single aggregated numeric impact.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High prevalence according to surveyed literature (many papers under-specify details); e.g., Gundersen & Kjensmo reported low explicit reporting rates for key items (see related entries).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or omitted natural language documentation; incentives to omit engineering details; complexity of pipeline; assumption that code or implicit defaults are obvious</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Share code and data, include explicit experiment protocols, list seeds, version numbers, commit IDs, include datasheets and reproducibility checklists, provide outputs and configuration files</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Paper argues code+data sharing greatly reduces ambiguity and increases trust, but does not provide a quantified global effectiveness estimate in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / empirical ML research</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sources of Irreproducibility in Machine Learning: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e688.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e688.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Keras init doc error</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Keras documentation error about convolutional layer weight initialization (Glorot vs Xavier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A documented case where library documentation misreported the initialization used for convolutional layers, so reproductions following the paper's claimed initializer produced different results when implemented in a different framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Challenges for the Repeatability of Deep Learning Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep learning model implementations across frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Neural network models implemented in high-level libraries (e.g., Keras vs PyTorch) where the paper's text/docstrings state one initialization but the actual implementation used a different variant.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>library documentation / methods description in paper</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>library implementation (Keras) vs reimplementation (PyTorch)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incorrect documentation / mis-specified initializer</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Karas' (Keras?) documentation claimed the convolutional layer weight initialization was Glorot uniform, while the actual implementation used a modified Glorot (Xavier) uniform variant; following the documented choice in another framework (PyTorch) led to nonmatching behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture / initialization defaults</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual code inspection and cross-framework reimplementation followed by failed reproduction attempts (reported in Alahmari et al. 2020 as example)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative: reproduced experiments failed to match original behavior; no single numeric effect reported in the cited example within this paper</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Caused implementations in other frameworks to produce different outcomes and prevented reproducibility when reimplementing the model as documented; described as an explicit example of documentation causing irreproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Presented as an example case; the paper cites this as illustrative rather than estimating prevalence.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incorrect or imprecise library documentation and lack of explicit implementation detail in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Publish actual implementation/code, include exact initializer names and parameter values, indicate commit IDs and library versions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Sharing the true implementation (code) would eliminate this class of mismatch; effectiveness argued qualitatively but not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / software engineering for ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sources of Irreproducibility in Machine Learning: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e688.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e688.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Missing preprocessing spec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Underspecified data preprocessing and augmentation steps in papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers sometimes fail to report critical preprocessing and augmentation steps (or their stochastic parameters), so reproductions that follow the written protocol omit steps implemented in code and obtain different outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The gan landscape: Losses, architectures, regularization, and normalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML training pipeline (data ingestion & augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipelines where raw datasets are transformed (cropping/centering/scaling/augmentation) before training; these transforms can be stochastic and affect results.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / experiment design details</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training script / data preprocessing code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing step / under-specification of augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Data augmentation or preprocessing used in the code was not specified in the paper (or vice versa), so external reproductions used different input data distributions; stochastic augmentation compounds variability.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>review of papers vs code repositories and reproducibility studies reporting unspecified augmentations (Kurach et al. 2018 cited as example)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparison of reproduction performance vs original; qualitative reports and examples in literature; specific numeric deltas depend on case and are not aggregated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can materially change training outcomes and downstream performance; the paper cites omissions of augmentation as a frequent cause of failed reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Kurach et al. (2018) and Gundersen et al. note data augmentation often not specified; exact percent not given here but described as common.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors omit engineering steps (possibly considered routine), lack of checklist enforcement, and inadequate documentation standards</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Document preprocessing pipelines exhaustively (code, scripts, order of steps), share preprocessing code or data snapshots, include randomness seeds for augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Sharing preprocessing code or data reduces this source of mismatch; effectiveness discussed qualitatively, no global numeric estimate in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / computer vision / empirical ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sources of Irreproducibility in Machine Learning: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e688.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e688.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seed/platform non-equivalence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differences in pseudorandom seeds and platform-dependent randomness producing divergent behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same seed value and code can produce different numerical outcomes when run on different hardware, software versions, or libraries due to differences in PRNG implementations, parallel execution ordering, or floating-point behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML training environment (hardware + software stack)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Complete execution environment including OS, deep learning framework, low-level libraries (cuDNN/CUDA), and hardware (GPU models) that together determine numerical behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper methods / reported seed value</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment script executed across platforms</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implicit assumption about seed determinism / platform-dependence</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often report seed values but assume identical results across platforms; in practice, the same seed can yield statistically different outcomes on different OSes, GPUs, or framework versions because of nondeterministic primitives, thread ordering, or floating-point rounding differences.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>initialization seeds; low-level computation (primitive operations); parallel execution</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical cross-platform runs and statistical comparison; cited studies (Reimers & Gurevych 2017; Nagarajan et al. 2019; Gundersen et al. 2022) measured platform sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>statistical testing of outcome distributions; Reimers et al. reported seeds causing statistically significant differences (p < 1e-4 in cited work); Nagarajan observed deterministic but different results across GPU models</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can change whether one model is judged significantly better than another; Reimers et al. demonstrated seed-driven significance and Nagarajan reported different deterministic outcomes on different GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Multiple cited studies show this is a widespread issue across frameworks and hardware; no single prevalence percentage given but presented as a common and important factor.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Assumption that PRNG + implementation are identical across platforms; nondeterministic library routines; floating-point rounding and parallelism differences</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Document hardware and library versions, set and report seeds, use deterministic primitives when available, force single-threaded/serial execution or deterministic backend settings, share exact environment specifications (containers, commit IDs)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Deterministic execution mitigates variance for a given setup but does not guarantee generalizability; forcing determinism can be effective for reproducing exact outputs on the same platform but cross-platform differences may persist.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning training</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sources of Irreproducibility in Machine Learning: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e688.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e688.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-primitive behavior mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-selection/autotuning of low-level primitive operations (e.g., cuDNN autotune) causing run-to-run or environment-dependent changes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-level frameworks may rely on auto-tuning to pick optimal low-level kernel implementations at runtime, meaning the actual execution mode can differ between runs or platforms even if high-level code is unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Problems and opportunities in training deep learning software systems: An analysis of variance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep learning runtime with autotune (cuDNN/cuBLAS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Execution stack where a framework (TensorFlow/PyTorch) delegates primitive ops to low-level libraries that may auto-benchmark and choose different kernels for performance.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper methods / assumptions about library determinism</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>framework-backed runtime / GPU-accelerated kernels</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>environment-dependent primitive selection / implicit nondeterminism</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Auto-tuning features (e.g., cuDNN autotune) cause the low-level implementation of operations (convolutions, reductions) to vary between runs, hardware, or library versions, creating implementation behavior that is not captured in the paper's natural language description.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / low-level primitive operations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical observation of run-to-run variation and cross-version/platform testing as reported in Pham et al. 2020 and related studies</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>compare output distributions across runs and platforms; analyze performance and numerical output variance; specific numeric measures vary by study and operation</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Introduces additional variation/noise that can alter training trajectories and final performance; contributes to irreproducibility even when high-level code is identical.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as a common source of run-to-run variability in GPU-accelerated deep learning; prevalence depends on use of autotune-enabled libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Opaque runtime behavior and performance-oriented auto-selection in low-level libraries not described in papers; authors rarely document these implicit behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Disable autotune where deterministic behavior is required, document low-level library settings and versions, run multiple seeds and report score distributions, provide environment images/containers</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Disabling autotune and fixing kernels can reduce nondeterminism for a given environment; paper notes this is feasible but sometimes costly (performance trade-offs).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / GPU-accelerated ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sources of Irreproducibility in Machine Learning: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e688.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e688.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Underspecified evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between reported evaluation protocol in text and the code that computes metrics (e.g., sampled metrics, reporting on training vs test data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers sometimes omit explicit evaluation protocol details (which split, sampling strategy, metric variants), or the code computes metrics differently than the described metric, producing misleading reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A troubling analysis of reproducibility and progress in recommender systems research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evaluation pipeline / metric computation code</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Scripts and code that compute evaluation metrics on test/validation datasets and produce reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methods section / evaluation description in paper</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation scripts / aggregator code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification of evaluation / sampled metrics mismatch / data leakage in evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural language description of evaluation (which dataset split, which metric variant, whether metrics were sampled or exact) can differ from the code (e.g., sampled metrics used for speed, metrics computed on training set by mistake, or metric implementations differing in tie-breaking), leading to reported numbers that do not match reimplemented evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / data splits / test set handling</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility studies and audits revealing sampled metrics, data leakage, or metrics computed on wrong splits (cited cases in Dacrema et al., GÃ¶tz-Hahn et al., Cremonesi & Jannach)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>compare reported metric values to recomputed metrics under correct protocol; Di Nunzio & Minzoni reported a ~2 percentage point difference in reproduced performance (example of measurement of mismatch magnitude in reproducibility study cited in discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can lead to overstated performance claims, false positives due to data leakage, or differences of several percentage points in reported metrics; in documented cases performance claims dropped substantially when leaks were fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Multiple high-profile examples exist (data leakage and wrong-split reporting); Gundersen & Kjensmo and others show many papers omit split details (only 16% specify validation set, 30% specify test set), making this a frequent problem.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Omitted or ambiguous evaluation protocol in the paper, rushed reporting, or mistakes in code where evaluation was applied incorrectly</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly report dataset splits and metric computation details, share evaluation code and saved outputs, perform and report error estimates and statistical significance, use reproducibility checklists</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Sharing evaluation code and outputs directly addresses the mismatch; paper cites that sharing code and data increases trust though no aggregate numeric effectiveness is given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning evaluation / recommender systems / general ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sources of Irreproducibility in Machine Learning: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Challenges for the Repeatability of Deep Learning Models <em>(Rating: 2)</em></li>
                <li>Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging <em>(Rating: 2)</em></li>
                <li>Problems and opportunities in training deep learning software systems: An analysis of variance <em>(Rating: 2)</em></li>
                <li>The gan landscape: Losses, architectures, regularization, and normalization <em>(Rating: 2)</em></li>
                <li>A troubling analysis of reproducibility and progress in recommender systems research <em>(Rating: 2)</em></li>
                <li>State of the art: Reproducibility in artificial intelligence <em>(Rating: 1)</em></li>
                <li>Do machine learning platforms provide out-of-the-box reproducibility? <em>(Rating: 2)</em></li>
                <li>Problems and opportunities in training deep learning software systems: An analysis of variance <em>(Rating: 2)</em></li>
                <li>A Thorough Reproducibility Study on Sentiment Classification: Methodology, Experimental Setting, Results <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-688",
    "paper_id": "paper-5818cae37a473d279faf096295f4f06a358d3054",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Doc-vs-Code mismatch (general)",
            "name_full": "Discrepancies between natural language experiment descriptions and code implementations",
            "brief_description": "A broad class of faithfulness gaps where the written paper, methods section, or documentation either contradicts, omits, or ambiguously specifies the behavior implemented in code, leading to irreproducible or misinterpreted experimental outcomes.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML experiment / research codebase",
            "system_description": "Typical experimental ML pipeline where a paper's methods section, README, or pseudocode is intended to describe an implementation that is also provided (or re-implemented) by others.",
            "nl_description_type": "research paper methods section / README / pseudo-code",
            "code_implementation_type": "reference implementation / library code / experiment script",
            "gap_type": "incomplete specification / ambiguous description / documentation error",
            "gap_description": "Natural language descriptions (methods, pseudocode, README) fail to fully or accurately specify implementation details (initialization, preprocessing, hyperparameter defaults, or algorithmic variants), causing independent implementers or re-users to produce differing code behavior than intended by authors.",
            "gap_location": "varies across pipeline: initialization, preprocessing, hyperparameters, training procedure, evaluation metrics",
            "detection_method": "survey of literature and reproducibility analyses; comparison of documented protocol vs observed code behavior; reported failed reproducibility attempts",
            "measurement_method": "not quantified in a single metric in this paper; recommended measurement is to re-run experiments, compare outcomes, and estimate variation/error (e.g., distribution of scores, confidence intervals, statistical tests)",
            "impact_on_results": "Can change outcomes, analyses, and interpretations; may cause irreproducible conclusions and false claims. Paper notes many cases where lack of specification prevents outcome reproducibility and can change conclusions, but provides no single aggregated numeric impact.",
            "frequency_or_prevalence": "High prevalence according to surveyed literature (many papers under-specify details); e.g., Gundersen & Kjensmo reported low explicit reporting rates for key items (see related entries).",
            "root_cause": "Ambiguous or omitted natural language documentation; incentives to omit engineering details; complexity of pipeline; assumption that code or implicit defaults are obvious",
            "mitigation_approach": "Share code and data, include explicit experiment protocols, list seeds, version numbers, commit IDs, include datasheets and reproducibility checklists, provide outputs and configuration files",
            "mitigation_effectiveness": "Paper argues code+data sharing greatly reduces ambiguity and increases trust, but does not provide a quantified global effectiveness estimate in this work.",
            "domain_or_field": "machine learning / empirical ML research",
            "reproducibility_impact": true,
            "uuid": "e688.0",
            "source_info": {
                "paper_title": "Sources of Irreproducibility in Machine Learning: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Keras init doc error",
            "name_full": "Keras documentation error about convolutional layer weight initialization (Glorot vs Xavier)",
            "brief_description": "A documented case where library documentation misreported the initialization used for convolutional layers, so reproductions following the paper's claimed initializer produced different results when implemented in a different framework.",
            "citation_title": "Challenges for the Repeatability of Deep Learning Models",
            "mention_or_use": "mention",
            "system_name": "Deep learning model implementations across frameworks",
            "system_description": "Neural network models implemented in high-level libraries (e.g., Keras vs PyTorch) where the paper's text/docstrings state one initialization but the actual implementation used a different variant.",
            "nl_description_type": "library documentation / methods description in paper",
            "code_implementation_type": "library implementation (Keras) vs reimplementation (PyTorch)",
            "gap_type": "incorrect documentation / mis-specified initializer",
            "gap_description": "Karas' (Keras?) documentation claimed the convolutional layer weight initialization was Glorot uniform, while the actual implementation used a modified Glorot (Xavier) uniform variant; following the documented choice in another framework (PyTorch) led to nonmatching behavior.",
            "gap_location": "model architecture / initialization defaults",
            "detection_method": "manual code inspection and cross-framework reimplementation followed by failed reproduction attempts (reported in Alahmari et al. 2020 as example)",
            "measurement_method": "qualitative: reproduced experiments failed to match original behavior; no single numeric effect reported in the cited example within this paper",
            "impact_on_results": "Caused implementations in other frameworks to produce different outcomes and prevented reproducibility when reimplementing the model as documented; described as an explicit example of documentation causing irreproducibility.",
            "frequency_or_prevalence": "Presented as an example case; the paper cites this as illustrative rather than estimating prevalence.",
            "root_cause": "Incorrect or imprecise library documentation and lack of explicit implementation detail in the paper",
            "mitigation_approach": "Publish actual implementation/code, include exact initializer names and parameter values, indicate commit IDs and library versions",
            "mitigation_effectiveness": "Sharing the true implementation (code) would eliminate this class of mismatch; effectiveness argued qualitatively but not quantified in this paper.",
            "domain_or_field": "deep learning / software engineering for ML",
            "reproducibility_impact": true,
            "uuid": "e688.1",
            "source_info": {
                "paper_title": "Sources of Irreproducibility in Machine Learning: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Missing preprocessing spec",
            "name_full": "Underspecified data preprocessing and augmentation steps in papers",
            "brief_description": "Papers sometimes fail to report critical preprocessing and augmentation steps (or their stochastic parameters), so reproductions that follow the written protocol omit steps implemented in code and obtain different outcomes.",
            "citation_title": "The gan landscape: Losses, architectures, regularization, and normalization",
            "mention_or_use": "mention",
            "system_name": "ML training pipeline (data ingestion & augmentation)",
            "system_description": "Pipelines where raw datasets are transformed (cropping/centering/scaling/augmentation) before training; these transforms can be stochastic and affect results.",
            "nl_description_type": "research paper methods section / experiment design details",
            "code_implementation_type": "training script / data preprocessing code",
            "gap_type": "missing preprocessing step / under-specification of augmentation",
            "gap_description": "Data augmentation or preprocessing used in the code was not specified in the paper (or vice versa), so external reproductions used different input data distributions; stochastic augmentation compounds variability.",
            "gap_location": "data preprocessing / data augmentation",
            "detection_method": "review of papers vs code repositories and reproducibility studies reporting unspecified augmentations (Kurach et al. 2018 cited as example)",
            "measurement_method": "comparison of reproduction performance vs original; qualitative reports and examples in literature; specific numeric deltas depend on case and are not aggregated in this paper",
            "impact_on_results": "Can materially change training outcomes and downstream performance; the paper cites omissions of augmentation as a frequent cause of failed reproductions.",
            "frequency_or_prevalence": "Kurach et al. (2018) and Gundersen et al. note data augmentation often not specified; exact percent not given here but described as common.",
            "root_cause": "Authors omit engineering steps (possibly considered routine), lack of checklist enforcement, and inadequate documentation standards",
            "mitigation_approach": "Document preprocessing pipelines exhaustively (code, scripts, order of steps), share preprocessing code or data snapshots, include randomness seeds for augmentation",
            "mitigation_effectiveness": "Sharing preprocessing code or data reduces this source of mismatch; effectiveness discussed qualitatively, no global numeric estimate in paper.",
            "domain_or_field": "deep learning / computer vision / empirical ML",
            "reproducibility_impact": true,
            "uuid": "e688.2",
            "source_info": {
                "paper_title": "Sources of Irreproducibility in Machine Learning: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Seed/platform non-equivalence",
            "name_full": "Differences in pseudorandom seeds and platform-dependent randomness producing divergent behavior",
            "brief_description": "The same seed value and code can produce different numerical outcomes when run on different hardware, software versions, or libraries due to differences in PRNG implementations, parallel execution ordering, or floating-point behavior.",
            "citation_title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging",
            "mention_or_use": "mention",
            "system_name": "ML training environment (hardware + software stack)",
            "system_description": "Complete execution environment including OS, deep learning framework, low-level libraries (cuDNN/CUDA), and hardware (GPU models) that together determine numerical behavior.",
            "nl_description_type": "paper methods / reported seed value",
            "code_implementation_type": "experiment script executed across platforms",
            "gap_type": "implicit assumption about seed determinism / platform-dependence",
            "gap_description": "Papers often report seed values but assume identical results across platforms; in practice, the same seed can yield statistically different outcomes on different OSes, GPUs, or framework versions because of nondeterministic primitives, thread ordering, or floating-point rounding differences.",
            "gap_location": "initialization seeds; low-level computation (primitive operations); parallel execution",
            "detection_method": "empirical cross-platform runs and statistical comparison; cited studies (Reimers & Gurevych 2017; Nagarajan et al. 2019; Gundersen et al. 2022) measured platform sensitivity",
            "measurement_method": "statistical testing of outcome distributions; Reimers et al. reported seeds causing statistically significant differences (p &lt; 1e-4 in cited work); Nagarajan observed deterministic but different results across GPU models",
            "impact_on_results": "Can change whether one model is judged significantly better than another; Reimers et al. demonstrated seed-driven significance and Nagarajan reported different deterministic outcomes on different GPUs.",
            "frequency_or_prevalence": "Multiple cited studies show this is a widespread issue across frameworks and hardware; no single prevalence percentage given but presented as a common and important factor.",
            "root_cause": "Assumption that PRNG + implementation are identical across platforms; nondeterministic library routines; floating-point rounding and parallelism differences",
            "mitigation_approach": "Document hardware and library versions, set and report seeds, use deterministic primitives when available, force single-threaded/serial execution or deterministic backend settings, share exact environment specifications (containers, commit IDs)",
            "mitigation_effectiveness": "Deterministic execution mitigates variance for a given setup but does not guarantee generalizability; forcing determinism can be effective for reproducing exact outputs on the same platform but cross-platform differences may persist.",
            "domain_or_field": "machine learning / deep learning training",
            "reproducibility_impact": true,
            "uuid": "e688.3",
            "source_info": {
                "paper_title": "Sources of Irreproducibility in Machine Learning: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Auto-primitive behavior mismatch",
            "name_full": "Auto-selection/autotuning of low-level primitive operations (e.g., cuDNN autotune) causing run-to-run or environment-dependent changes",
            "brief_description": "High-level frameworks may rely on auto-tuning to pick optimal low-level kernel implementations at runtime, meaning the actual execution mode can differ between runs or platforms even if high-level code is unchanged.",
            "citation_title": "Problems and opportunities in training deep learning software systems: An analysis of variance",
            "mention_or_use": "mention",
            "system_name": "Deep learning runtime with autotune (cuDNN/cuBLAS)",
            "system_description": "Execution stack where a framework (TensorFlow/PyTorch) delegates primitive ops to low-level libraries that may auto-benchmark and choose different kernels for performance.",
            "nl_description_type": "paper methods / assumptions about library determinism",
            "code_implementation_type": "framework-backed runtime / GPU-accelerated kernels",
            "gap_type": "environment-dependent primitive selection / implicit nondeterminism",
            "gap_description": "Auto-tuning features (e.g., cuDNN autotune) cause the low-level implementation of operations (convolutions, reductions) to vary between runs, hardware, or library versions, creating implementation behavior that is not captured in the paper's natural language description.",
            "gap_location": "training procedure / low-level primitive operations",
            "detection_method": "empirical observation of run-to-run variation and cross-version/platform testing as reported in Pham et al. 2020 and related studies",
            "measurement_method": "compare output distributions across runs and platforms; analyze performance and numerical output variance; specific numeric measures vary by study and operation",
            "impact_on_results": "Introduces additional variation/noise that can alter training trajectories and final performance; contributes to irreproducibility even when high-level code is identical.",
            "frequency_or_prevalence": "Described as a common source of run-to-run variability in GPU-accelerated deep learning; prevalence depends on use of autotune-enabled libraries.",
            "root_cause": "Opaque runtime behavior and performance-oriented auto-selection in low-level libraries not described in papers; authors rarely document these implicit behaviors",
            "mitigation_approach": "Disable autotune where deterministic behavior is required, document low-level library settings and versions, run multiple seeds and report score distributions, provide environment images/containers",
            "mitigation_effectiveness": "Disabling autotune and fixing kernels can reduce nondeterminism for a given environment; paper notes this is feasible but sometimes costly (performance trade-offs).",
            "domain_or_field": "deep learning / GPU-accelerated ML",
            "reproducibility_impact": true,
            "uuid": "e688.4",
            "source_info": {
                "paper_title": "Sources of Irreproducibility in Machine Learning: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Underspecified evaluation",
            "name_full": "Mismatch between reported evaluation protocol in text and the code that computes metrics (e.g., sampled metrics, reporting on training vs test data)",
            "brief_description": "Papers sometimes omit explicit evaluation protocol details (which split, sampling strategy, metric variants), or the code computes metrics differently than the described metric, producing misleading reported results.",
            "citation_title": "A troubling analysis of reproducibility and progress in recommender systems research",
            "mention_or_use": "mention",
            "system_name": "Evaluation pipeline / metric computation code",
            "system_description": "Scripts and code that compute evaluation metrics on test/validation datasets and produce reported numbers.",
            "nl_description_type": "methods section / evaluation description in paper",
            "code_implementation_type": "evaluation scripts / aggregator code",
            "gap_type": "incomplete specification of evaluation / sampled metrics mismatch / data leakage in evaluation",
            "gap_description": "The natural language description of evaluation (which dataset split, which metric variant, whether metrics were sampled or exact) can differ from the code (e.g., sampled metrics used for speed, metrics computed on training set by mistake, or metric implementations differing in tie-breaking), leading to reported numbers that do not match reimplemented evaluation.",
            "gap_location": "evaluation metrics / data splits / test set handling",
            "detection_method": "reproducibility studies and audits revealing sampled metrics, data leakage, or metrics computed on wrong splits (cited cases in Dacrema et al., GÃ¶tz-Hahn et al., Cremonesi & Jannach)",
            "measurement_method": "compare reported metric values to recomputed metrics under correct protocol; Di Nunzio & Minzoni reported a ~2 percentage point difference in reproduced performance (example of measurement of mismatch magnitude in reproducibility study cited in discussion)",
            "impact_on_results": "Can lead to overstated performance claims, false positives due to data leakage, or differences of several percentage points in reported metrics; in documented cases performance claims dropped substantially when leaks were fixed.",
            "frequency_or_prevalence": "Multiple high-profile examples exist (data leakage and wrong-split reporting); Gundersen & Kjensmo and others show many papers omit split details (only 16% specify validation set, 30% specify test set), making this a frequent problem.",
            "root_cause": "Omitted or ambiguous evaluation protocol in the paper, rushed reporting, or mistakes in code where evaluation was applied incorrectly",
            "mitigation_approach": "Explicitly report dataset splits and metric computation details, share evaluation code and saved outputs, perform and report error estimates and statistical significance, use reproducibility checklists",
            "mitigation_effectiveness": "Sharing evaluation code and outputs directly addresses the mismatch; paper cites that sharing code and data increases trust though no aggregate numeric effectiveness is given.",
            "domain_or_field": "machine learning evaluation / recommender systems / general ML",
            "reproducibility_impact": true,
            "uuid": "e688.5",
            "source_info": {
                "paper_title": "Sources of Irreproducibility in Machine Learning: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Challenges for the Repeatability of Deep Learning Models",
            "rating": 2
        },
        {
            "paper_title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging",
            "rating": 2
        },
        {
            "paper_title": "Problems and opportunities in training deep learning software systems: An analysis of variance",
            "rating": 2
        },
        {
            "paper_title": "The gan landscape: Losses, architectures, regularization, and normalization",
            "rating": 2
        },
        {
            "paper_title": "A troubling analysis of reproducibility and progress in recommender systems research",
            "rating": 2
        },
        {
            "paper_title": "State of the art: Reproducibility in artificial intelligence",
            "rating": 1
        },
        {
            "paper_title": "Do machine learning platforms provide out-of-the-box reproducibility?",
            "rating": 2
        },
        {
            "paper_title": "Problems and opportunities in training deep learning software systems: An analysis of variance",
            "rating": 2
        },
        {
            "paper_title": "A Thorough Reproducibility Study on Sentiment Classification: Methodology, Experimental Setting, Results",
            "rating": 1
        }
    ],
    "cost": 0.014557,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sources of Irreproducibility in Machine Learning: A Review</h1>
<p>Odd Erik Gundersen<br>odderik@ntnu.no<br>Norwegian University of Science and Technology<br>Trondheim, Norway<br>Aneo AS<br>Trondheim, Norway<br>Christine R. Kirkpatrick<br>christine@sdsc.edu<br>San Diego Supercomputer Center, UC San Diego<br>La Jolla, USA</p>
<h2>ABSTRACT</h2>
<p>Background: Many published machine learning studies are irreproducible. Issues with methodology and not properly accounting for variation introduced by the algorithm themselves or their implementations are attributed as the main contributors to the irreproducibility.Problem: There exist no theoretical framework that relates experiment design choices to potential effects on the conclusions. Without such a framework, it is much harder for practitioners and researchers to evaluate experiment results and describe the limitations of experiments. The lack of such a framework also makes it harder for independent researchers to systematically attribute the causes of failed reproducibility experiments. Objective: The objective of this paper is to develop a framework that enable applied data science practitioners and researchers to understand which experiment design choices can lead to false findings and how and by this help in analyzing the conclusions of reproducibility experiments. Method: We have compiled an extensive list of factors reported in the literature that can lead to machine learning studies being irreproducible. These factors are organized and categorized in a reproducibility framework motivated by the stages of the scientific method. The factors are analyzed for how they can affect the conclusions drawn from experiments. A model comparison study is used as an example. Conclusion: We provide a framework that describes machine learning methodology from experimental design decisions to the conclusions inferred from them.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Computing methodologies $\rightarrow$ Machine learning.</li>
</ul>
<h2>KEYWORDS</h2>
<p>Machine learning, research methodology, reproducibility, model comparison experiments</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1 INTRODUCTION</h2>
<p>In recent years, many machine learning studies have shown to be very challenging to reproduce. The areas of machine learning that have reported issues are very diverse and include forecasting [Makridakis et al. 2018], natural language processing [Belz et al. 2021a], generative adversarial networks [Lucic et al. 2018], deep reinforcement learning [Henderson et al. 2018], recommender systems [Dacrema et al. 2019], and image recognition [Bouthillier et al. 2019]. The authors above point to many methodological issues that are commonly found in machine learning research. Since applications of machine learning reach into many other fields [Gibney 2022], methodological shortcomings can have far reaching effects particularly in domains with high-stakes decisions such as medicine [Roberts et al. 2021; Varoquaux and Cheplygina 2022], social sciences [Kapoor and Narayanan 2022], psychology [Hullman et al. 2022] and many more [Raji et al. 2022].</p>
<p>Proper methodology requires a good understanding of which experiment design choices can lead to false findings. An experiment conducted by Pham et al. [2020] illustrated 16 identical training runs of a deep learning model that resulted in test accuracy varying from $9 \%$ to $99 \%$. The authors also presented a survey of 900 participants where $84 \%$ were unsure or unaware about variance caused by how an experiment is implemented. Gundersen and Kjensmo [2018] found that experiments in AI presented at top conferences had incomplete documentation. The recognition of a reproducibility crisis in AI [Hutson 2018] has led to community-wide efforts to mitigate the crisis. The machine learning community is not alone in experiencing a reproducibility crisis [Baker 2016; Button et al. 2013; Open Science Collaboration 2015].</p>
<p>The machine learning and AI communities have introduced several mechanisms to improve the level of empirical rigor: 1) reproducibility checklists, 2) datasheets, 3) reproducibility challenges and 4) registered reports. Reproducibility checklists have been introduced at most top level machine learning and AI conferences, such as NeurIPS, AAAI, ICML, IJCAI, and EMNLP. Journals have still not made reproducibility checklists a default part of their submission procedure, although this is to change for JAIR (Journal of Artificial Intellgience Research) [Gundersen et al. 2023]. Datasets introduced at the NeurIPS Dataset and Benchmark Track are required to complete a data sheet [Gebru et al. 2021] to ensure that datasets are properly documented. Reproducibility challenges have been introduced at both ICLR and NeurIPS [Pineau et al. 2021] to encourage third-parties to try confirm findings of articles published at top-level conferences. JAIR is to introduce reproducibility reports inspired by the reproducibility challenges [Gundersen et al. 2023]. Finally, registered reports, which have been missing in AI and ML [Gundersen 2021a], have been introduced at the journal ACM Transactions on Recommender Systems.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The scientific method is a systematic process for acquiring knowledge about the world: 1) The world is observed, 2) explanations are made and testable statements are formulated, 3) experiments are designed to test the hypotheses and documented as research protocols, 4) experiments are implemented as code, 5) structured observations are collected and stored digitally as data, 6) the implementation of the experiment is executed and outcomes are produced, 7) the outcomes are analyzed automatically by executing code, 8) the analysis is interpreted and a conclusion is reached to update beliefs [Gundersen 2021b].</p>
<p>The framework presented here complements those efforts as it enables applied data science researchers and practitioners to: i) understand which experiment design choices can lead to false findings by providing an overview of design choices that can lead to irreproducible results, ii) understand how these design choices can affect the conclusion of experiments by mapping them to which part of the result elicitation process they belong to, iii) discuss limitations of experiments by using the overview of design decision and their consequences as a starting point for discussion, and iv) conduct and analyze reproducibility experiments by furthering their ability to understand and pinpoint the potential causes of failed reproducibility experiments. Hence, the framework presented here could provide a methodological basis for reproducibility checklists and provides justifications of why items should be reported. The framework provides an opportunity to educate the community, as the items are not motivated. For example, checklists tend to focus on the software side, while non-determinism must be controlled at all levels of the technical stack [Zhuang et al. 2021]. However, why is not clear to most researchers and practitioners according to Zhuang et al. [2021]. Also, the design decisions presented in this paper extend existing reproducibility checklists significantly, by identifying many additional factors that need to be reported for a published experiment to be reproducible. Similarly, the framework could be useful when designing experiments for registered reports and as a justification for why data sheets are necessary.</p>
<p>The main contribution of this paper is the identification and categorization of 41 design choices documented in the literature that can lead to false conclusions. Another major contribution is a novel framework that enables applied data science researchers and practitioners to understand which experiment design choices can lead to false findings, understand how these design choices can affect the conclusion of experiments and conduct and analyze reproducibility experiments.</p>
<h2>2 A REPRODUCIBILITY FRAMEWORK</h2>
<p>We follow the definitions proposed by Gundersen [2021b]. Reproducibility is defined as the ability of independent investigators to draw the same conclusions from an experiment by following the documentation shared by the original investigators; a reproducibility experiment is an experiment conducted by independent researchers to confirm the findings of the original study using the documentation shared by the researchers that conducted the original study. The documentation of a machine learning experiment is not restricted to written text in the form of a scientific report, it could also include the code and data. However, additional documentation beyond a scientific report is not required for independent investigators to conduct a reproducibility experiment. Gundersen [2021b] specifies four different types of reproducibility studies based on which documentation is shared by the original researchers: R1 Description if only textual documentation is shared,</p>
<p>R2 Code if text and code are shared, R3 Data if text and data are shared and R4 Experiment if text, data and code are shared. Having access to code and data reduces the effort required to reproduce the results, while also leading to increased trust in the research [Gundersen 2019]. Drummond [2009] argues that the power of a reproducibility experiment is greater with increased difference from the original study, which can be enforced by providing less documentation. Still, code and data are commonly acknowledged as important for third parties to reproduce results [Haibe-Kains et al. 2020]. There are three degrees of reproducibility that are derived from the scientific method, which is illustrated in Figure 1. The idea will be exemplified by a model comparison study.</p>
<p>Progress in machine learning is to a large degree driven by empirical evidence, and model comparison is the standard method to identify the best performing machine learning model for a given task [Bouthillier et al. 2019; Dacrema et al. 2021; Melis et al. 2018; Sculley et al. 2018]. A model comparison study is an experiment of which the objective is to decide which of a set of models has better performance. Hence, a model comparison identifies the subset $S$ of a given set of computer programs $C$ that performs task $T$ better according to measure $P$ after learning from the same experience $E$. For a computer program, or model, to be considered a clear winner of a model comparison, it should be significantly better than the model that was previously considered state-of-the-art for the given task [Sculley et al. 2018]. The hypothesis that a model is significantly better than the other models in the comparison should be tested statistically [Cohen 1995]. Empirical evidence is given by conducting experiments where the competing models learn from the same experiences under the same conditions.</p>
<p>Example: A reproducibility experiment is conducted to re-test the hypothesis that $m_{C N N}$ performs better than $m_{D N N}$ on the MNIST classification task as reported by LeCun et al. [1998]. The documentation of the original study contained the paper cited above as well as the MNIST dataset that was famously published. No code was published, so a reproducibility study requires re-implementing the convolutional neural network and dense neural networks used in the original study. Performance was measured using classification error and uncertainty was estimated, but how was not exactly described. As both the paper (text) and data (the MNIST dataset) are shared by the authors, and these resources are used for the reproducibility experiment, the reproducibility experiment is of type R3 Data.</p>
<p>Outcome reproducibility ( $O$ ) is achieved if the reproducibility experiment produces the same outcome as the original experiment. The outcome of the image classification task is the set of labels given to each image in the test dataset. If the model in the reproducibility experiment classifies each image with the exact same labels as they were assigned in the original experiment, the reproducibility experiment is outcome reproducible given that the analysis and conclusion of the original investigation are sound.</p>
<p>An experiment can only be evaluated for outcome reproducibility if the outcome produced in the original experiment is shared, which is the case for only $4 \%$ of AI studies [Gundersen and Kjensmo 2018]. The outcome was not published by LeCun et al. [1998], so our example reproducibility experiment could not be outcome reproducible, but if it was, it would have been classified as OR3.</p>
<p>Analysis reproducibility $(A)$ is achieved when the same analysis that was made by the original investigators lead to the same conclusion for the reproducibility experiment even if the outcomes differ. Evaluating an experiment for analysis reproducibility requires the methods that were used to analyze the outcome to be shared. Given our example, where code has to be re-implemented and executed in a different computing environment (we might not have access to an SGI server), the outcome will differ. However, as long as $m_{C N N}$ performs significantly better than $m_{D N N}$ in the reproducibility experiment when measuring the performance using error rate and uncertainty, the reproducibility experiment is analysis reproducible and would be classified as AR3.</p>
<p>Interpretation reproducibility (I) is achieved when a different analysis is done by independent investigators (on the same or different outcome) and their interpretation of the analysis supports the conclusion drawn in the original experiment. Hence, the conclusion of the original experiment is supported even though the reproducibility experiment produces different outcome and the outcome is analyzed in a different way i.e., by using the F1-score instead of the error rate, as long as the F1-score is significantly better for $m_{C N N}$ than for $m_{D N N}$. Evaluating for conclusion reproducibility requires that the methods used for analyzing the outcome are shared. In our example, the reproducibility experiment would have been classified as IR3. Figure</p>
<h2>3 CATEGORIZING DESIGN DECISIONS</h2>
<p>Many decisions about how to conduct and evaluate an experiment must be made before the experiment can be executed. Some of the decisions are made actively while others are made passively. In the end, all the decisions constitute the experiment, but some of these decisions can lead to changed outcomes, analyses and interpretations of the analyses and thus false conclusions. These decisions are independent variables on which the experiment's conclusion depends. Sometimes even seemingly small changes to these independent variables can lead to false conclusions. Hence, these design decisions can be interpreted as potential sources of irreproducibility.</p>
<p>In this paper, we provide an overview of the design decisions found in the literature that can lead to false conclusions. We have identified 41 such design decisions and organized them into six major categories, shown in Figure 2. These six categories, which we call factors in line with the terminology used by Pham et al. [2020], comprise a super set containing the sources of variation that they introduced. They group the sources of variation into algorithm-level and implementation-level factors, and they show how the sources of variation can change the outcome of a reproducibility experiment so much that the analyses lead to different conclusions if the variation is not controlled for. Our approach is to provide an overview and taxonomy of design decisions that can lead to false conclusions and describe how they can lead to false conclusions. We believe that this will not only be a valuable source for practitioners and researchers when designing experiments, but also when discussing limitations of conclusions and conducting reproducibility experiments.</p>
<h2>4 STUDY DESIGN FACTORS</h2>
<p>Study design factors capture the decisions that goes into making the highlevel plan for how to conduct and analyze an experiment in order to answer the stated hypothesis and research questions.</p>
<p>Unsuited experiment design These are experimental analyses that deviate from the claimed or implicit research goals [Cremonesi and Jannach
2021]. Motivating why particular performance metrics, datasets or data preprocessing techniques are used should be given explicitly in order to ensure that the experiment design is suited [Dacrema et al. 2021]. Doing analyses that do not support the research goals will likely lead to poor interpretations and thus the wrong conclusions.
p-hacking When decisions of which data is included and which analysis is used are made during the analysis instead of in advance, researchers may self-servingly select the data and analysis that produce statistically significant results [Simonsohn et al. 2014]. This will affect the interpretation and thus the conclusion.
p-fishing This term is used when seeking statistically significant results beyond the original hypothesis [Cockburn et al. 2020]. Changing the hypothesis based on the p-value will not change the outcome nor the analysis, but the interpretation of the results will go beyond the original intent.</p>
<p>HARKing (Hypotheses After Results are Known) is post-hoc reframing of experimental intentions to present a p -fished outcome as having been predicted from the start. HARKing is to execute the scientific method backwards and will change the interpretation of the results, like p-fishing.</p>
<p>Choice of baselines For many machine learning tasks, it is often not clear what comprises the state-of-the-art. Studies have found that many deep learning papers only compared against other deep learning algorithms, even though they were not performing better than other simpler baselines [Cremonesi and Jannach 2021]. This could happen in cases where progress is shown by reusing experimental designs that propagate weak baselines without questioning them [Dacrema et al. 2021]. Choosing a baseline that is inferior to the state-of-the art does not change the outcome of the target model, nor will it interfere with the analysis except that the baseline used for comparison is poorer than it could be. The interpretation could change from a baseline doing better to target a model doing better.</p>
<p>Experiment initialization Differences in the setup or initialization of an experiment can lead to a difference between $5 \%$ to $40 \%$ in the number of solved instances of a SAT solver when running on the same hardware [Fichte et al. 2021], so they must be reported [Henderson et al. 2018]. The setup can affect the outcome, so that the same analysis could be interpreted in a different way and change the conclusion if the variation under different setups is significant.</p>
<p>Computational budget Researchers with large computational budgets can sometimes prevent meaningful comparisons of algorithm performance experiments as they can spend the budget on intensive hyperparameter tuning of any given algorithm [Dodge et al. 2019; Lucic et al. 2018; Melis et al. 2018; Zhang and Duh 2020]. Bouthilier and Varoquaux [2020] report that around $45 \%$ of hyperparameters were manually tuned at NeurIPS 2019 and ICLR 2020, so systematic search could give a huge advantage. Running algorithms for longer using more resources produces a different outcome.</p>
<p>Selective tuning of algorithms Researchers' favored algorithms are often fine-tuned to get the best possible result [Latifi et al. 2021] while baselines are often not properly tuned [Cremonesi and Jannach 2021; Dacrema et al. 2021]. In some cases, old performance results are used to claim greater improvement than what could otherwise be claimed against the state-of-the-art [Crane 2018]. Tuning of algorithms in a selective, inconsistent manner could produce outcomes that do not reflect performance under the same conditions.</p>
<p>Study design factors can be controlled by designing a fair model comparison study where the hypothesis is tested genuinely and where the hypothesis is stated in such a way that the test properly answers the research question. A fair model comparison study assigns the same amount of resources, such as tuning and computational budget, to all models and sets up the experiment in a way that does not give advantages to a subset of the models or uses subpar baselines.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Taxonomy of 41 design decisions and how they affect results, grouped into six categories.</p>
<h2>5 ALGORITHMIC FACTORS</h2>
<p>Algorithmic factors are design decisions to introduce stochasticity in the learning algorithms and training processes, leads to a different outcome for every experiment run.</p>
<p>Hyperparameter optimization Different hyperparameter optimization methods find different optimal hyperparameter values [Bouthillier et al. 2021, 2019; Henderson et al. 2018; Reimers and Gurevych 2017], so researchers should specify exactly which method is used to improve reproducibility [Cooper et al. 2021; Raff 2019]. Reimers and Gurevych [2017] evaluated the variation of three different methods: random search, grid search and Bayesian optimization, and found that the variation that they caused is significant compared to other sources of variation.</p>
<p>Random weights initialization The initialization of weights in neural networks affects their performance [Pham et al. 2020; Zhuang et al. 2021]. Different initial weights might lead the hyperparameter optimization method to converge to local minima.</p>
<p>Stochastic Layers Dropout, variational dropout, and noisy activations intended to make deep neural networks more robust end up affecting their performance [Pham et al. 2020; Reimers and Gurevych 2017; Zhuang et al. 2021].</p>
<p>Random feature selection Many learning algorithms rely on selecting features at random during training such as Random Forests [Breiman 2001]. The exact set of features selected will affect the outcome, and some selections might perform better than others [Pouchard et al. 2020].</p>
<p>Data Shuffling Data samples are often shuffled randomly so that learning converges faster, which results in differences in outcome [Pham et al. 2020; Reimers and Gurevych 2017; Zhuang et al. 2021].</p>
<p>Batch ordering Because of memory limitations, data samples are fed into deep learning algorithms in batches. Randomizing batch order between epochs results in different outcomes between training runs [Bouthillier et al. 2021; Pham et al. 2020].</p>
<p>Relying on stochasticity cause outcomes to differ between experiment runs unless explicitly controlled for. Different combinations of initialization, training algorithm and dataset will lead to different outcomes that will perform differently. If particularly lucky or unlucky, one might encounter single runs that might perform very differently, even to such a degree that the results might affect the findings. Algorithmic factors can be controlled by setting the pseudo-random number generator initialization seeds so that the outcome will be the exact same for each experiment run if everything else remains the same. However, producing the same outcome over all runs does not mean that a finding is robust and generalizable. Hence, the variation in the performance measured for the outcome produced over several experiment runs must be reported. As pointed out by Miller and Miller [2018]: "No quantitative results are of any value unless they are accompanied by some estimate of the errors inherent in them."</p>
<h2>6 IMPLEMENTATION FACTORS</h2>
<p>Implementation factors are design choices related to the software and hardware that are used to execute the experiment. These factors mirror the variations in physical sciences experiments that are introduced by conducting the same experiment in different laboratories.</p>
<p>Initialization seeds Difference in the seeds used to initialize the pseudorandom number generator leads to difference in outcome [Bouthillier et al. 2019; Melis et al. 2018]. Reimers and Gurevych [2017] show that the seed value for the random number generator can result in statistically significant $\left(p&lt;10^{-4}\right)$ results for different state-of-the-art systems. The same seed on different platforms produces different results [Gundersen et al. 2022; Nagarajan et al. 2019; Pouchard et al. 2020].</p>
<p>Software Outcomes across implementations of similar algorithms can vary significantly, e.g., TensorFlow vs. PyTorch [Henderson et al. 2018; Pouchard et al. 2020]. Hong et al. [2013] showed that a difference in operating systems affected the result. Software versions [Crane 2018; Gundersen et al. 2022; Shahriari et al. 2022], bugs in either one's own implementation</p>
<p>or libraries, frameworks or operating systems might affect the outcomes [Crane 2018; Gundersen et al. 2022; Pham et al. 2020; Pineau et al. 2021].</p>
<p>Parallel execution Random completion order of parallel tasks introduces variation [Pham et al. 2020]. Increased parallelism is a driver for noise [Zhuang et al. 2021]. Truncation error of floating point calculations introduces variability as $A+B+C \neq C+B+A$, when calculated in parallel [Gundersen et al. 2022; Pham et al. 2020]. Truncation error can be reduced but not completely removed by changing from single precision ( 32 bits) to double precision ( 64 bits) at a cost of doubling memory requirements and tripling the training time [Pinto et al. 2021].</p>
<p>Compiler settings Hong et al, [2013] found severe sensitivity to Intel compiler optimization levels for weather simulations that rely on huge amounts of floating point calculations, which is the case for machine learning too.</p>
<p>Auto-selection of primitive operations High level libraries implement deep learning algorithms using GPU-optimized deep learning primitives provided by low-level libraries such as cuDNN and CUDA [Pham et al. 2020]. Autotune in cuDNN automatically benchmarks several modes of operation for primitive functions in run-time, which might change between runs.</p>
<p>Processing unit Changing the processor can affect results [Gundersen et al. 2022; Hong et al. 2013]. Nagarajan et al. [2019] found that a deterministic GPU implementation repeatedly generated the same result when executed on the same GPU, but changed to a different, but deterministic result, when executed on another GPU.</p>
<p>Rounding errors Different hardware architectures and software implement the rounding of floating-point numbers in different ways, the rounding errors accumulate during long running calculations, particularly when using GPUs [Taufer et al. 2010].</p>
<p>Implementation factors can cause outcomes to differ if software, hardware or initialization seeds are changed between experiment runs or parallel processing is utilized. Deterministic implementations of primitive operations, the use of single thread processes and forcing execution in serial manner will guarantee deterministic results [Pham et al. 2020] - for a given setup.</p>
<h2>7 OBSERVATION FACTORS</h2>
<p>Observational factors are related to how data is generated, processed and augmented, but also to the properties of environments used for benchmarking, such as agent simulation environments.</p>
<p>Dataset bias The methods used to gather data (manual or automated) and the way data is captured (objects are often centered when photographed) introduce biases in datasets [Torralba and Efros 2011]. Reeth et al. [2019] show that algorithms generalize poorly even on datasets that are replicated using the same source populations, so the lack of access to data used in the original study could lead to differences in data distribution for reproducibility experiments [Pineau et al. 2021]. In the social and environmental sciences, models might not generalize from one geographic area to another because of spatial dependence and heterogeneity [Goodchild and Li 2021] Dataset shifts is also an issue [Finlayson et al. 2021]. Different datasets lead to different outcomes.</p>
<p>Pre-processing Differences in data pre-processing will change data samples, so the applied pre-processing techniques must be well documented to facilitate reproducibility [Dacrema et al. 2021]. Differences in data preprocessing changes outcomes.</p>
<p>Data augmentation Stochastic data augmentation procedures are influenced by both algorithmic and implementation factors, which leads to differences in training data and thus different outcomes [Bouthillier et al. 2021; Pham et al. 2020; Zhuang et al. 2021].</p>
<p>Data splits Difference in data splits cause a difference in outcomes [Makridakis et al. 2018], which includes stochastic sampling from the training set instead of training on a static validation set [Bouthillier et al. 2021, 2019]. Also, random selection of samples during training is typically used in
gradient boosted trees [Pouchard et al. 2020]. According to Gundersen and Kjensmo [2018], only $16 \%$ specify the validation set and $30 \%$ specify the test set, which means that for all practical purposes outcome reproducibility is impossible to achieve. A $2 \%$ to $12 \%$ variation has been shown in labeled attachment scores of Natural Language Processing experiments when comparing models trained using standard test splits versus random test splits, which suggests that the results reported by experiments that only use the standard test splits can be influenced by a bias in the standard test splits that favor certain types of parsers [ÃÃ¶ltekin 2020].</p>
<p>Environment properties Stochasticity and different dynamic properties of the testing environment could affect the outcome, especially in continuous control simulators such as those used in deep reinforcement learning [Henderson et al. 2018].</p>
<p>Annotation quality Differences in annotations made by humans will affect the target value and thus the outcome a model produces [Belz et al. 2021b].</p>
<p>Test data issues Data leakage results in models trained on data that should only be available at test time, leading to overestimating model performance [Dacrema et al. 2021]. GÃ¶tz-Han et al. [2020] demonstrate five cases of reported performance gains well above the state-of-the-art that were the results of data leakage. The performance gains are far below the claims of the original researchers when the data leakage errors were corrected. Cases where metrics have been reported on training data instead of test data has also been found [Kurach et al. 2018]. Neither outcome nor analysis are changed, but the interpretation could lead to false conclusions.</p>
<p>Observation factors might affect the outcome and interpretation of an experiment. The effect of these factors can be reduced by setting the random seed, sharing details about pre-processing and data provenance [Gebru et al. 2021]. What is done with duplicate data, outliers and missing values can introduce biases [Stodden 2015]. As long as datasets are finite samples of an infinite population [Melis et al. 2018], they might not reflect the actual distribution at a given point in time and can also shift over time.</p>
<h2>8 EVALUATION FACTORS</h2>
<p>Evaluation factors relate to how the investigators reach the conclusions from doing an experiment.</p>
<p>Selective reporting results through careful selection of datasets, and ignoring the danger of adaptive over-fitting could lead to the wrong conclusions being made [Pineau et al. 2021]. Selective reporting will affect the interpretation.</p>
<p>Over-claiming of results By drawing conclusions that go beyond the evidence presented (e.g. insufficient number of experiments, mismatch between hypothesis and claim) results are over-estimated [Pineau et al. 2021]. Over-claiming of results are errors in the interpretation.</p>
<p>Lack of naive baselines Lack of comparison with simple statistical methods or naive benchmarks such as linear regression and persistence in time-series forecasting could obscure results [Makridakis et al. 2018]. NaÃ¯ve baselines help interpret the performance, but will not affect outcome nor analysis.</p>
<p>Sampled metrics Sampling from the test set is used sometimes when evaluations are computationally demanding. This could lead to sampled metrics being inconsistent with the exact versions and thus the wrong conclusions can be inferred [Cremonesi and Jannach 2021]. Sampled metrics will not change the outcome but can change the interpretation of the analysis.</p>
<p>Error estimation Machine learning methods must be able to specify certainty and confidence intervals around them [Makridakis et al. 2018] and preferably testing statistical significance taking the confidence intervals into account [Henderson et al. 2018]. Reporting single scores without any estimate of error or variation in performance is insufficient to compare nondeterministic approaches [Reimers and Gurevych 2017]. Error estimates are more prevalent in machine learning experiments reported in healthcare than for other domains [McDermott et al. 2021]. Error analyses are part of</p>
<p>the analysis and could change the interpretation if done incorrectly or are lacking.</p>
<p>Statistical analysis Improper use of statistics to analyze results, such as claiming significance without proper statistical testing or using the wrong statistics test lead to false conclusions [Card et al. 2020; Pineau et al. 2021]. Power analyses should be done prior to evaluation when comparing against baselines; the number of instances in the test will determine the effect size and should be chosen accordingly [Card et al. 2020]. The decision of which statistical analysis to do affects the analysis.</p>
<p>Evaluation factors affect the analysis and the interpretation of the analysis and thus the conclusion. Evaluation factors can only be controlled through validation and ensuring that one is doing the right experiment and evaluating it correctly. Sculley et al. [2018] list the following practices that should be included in empirical studies: 1) tuning methodology, 2) sliced analysis, 3) ablation studies, 4) sanity checks and counterfactuals and 5) at least one negative result. However, they note that a material increase in standards for empirical analysis or rigor has not been observed across the field.</p>
<h2>9 DOCUMENTATION FACTORS</h2>
<p>Documentation factors are related to how well an experiment is documented, which means ideally documenting all the choices mentioned above, which can be impractical.</p>
<p>Readability The readability of papers influences whether it is possible to reproduce results. Mathiness could lead to reduced readability [Lipton and Steinhardt 2019]. Gundersen and Kjensmo [2018] found that a large degree of papers only implicitly state what research questions they answer ( $94 \%$ ), which problems that they seek to solve ( $53 \%$ ), and what the objective (goal) of conducting the research is ( $78 \%$ ). Only $5 \%$ of the papers explicitly states the hypothesis and $54 \%$ contain pseudo-code. Raff [2019] found that number of tables, readability, specification of hyperparameters, pseudo-code, number of equations and compute needed to run the experiment correlated strongly with reproducibility. Readability could affect the outcome, analysis and interpretation.</p>
<p>Experiment design details Under-specification of the metrics used to report results and misspecification or under-specification of the model or training procedure might lead to the wrong conclusions [Pineau et al. 2021]. Documentation that lacks experiment details could affect outcomes, analyses and interpretations.</p>
<p>Workflow The exact steps taken and their order when conducting the empirical machine learning studies will affect the outcome [Gundersen 2021b], especially when they become more complex [Rupprecht et al. 2020]. Missing documentation of steps could also affect the reproducibility. Kurach et al. [2018] found that data augmentation was not specified, even when data had been augmented. Not specifying the workflow properly could lead to different outcomes.</p>
<p>Implementation details Details on how novel algorithms and baselines are implemented, especially details that can affect reproducibility, are important [Henderson et al. 2018]. Inconsistencies in the documentation and implementation of software can cause reproducibility experiments to fail when using different software. Alahmari et al. [2020] showed how a Karas' documentation error stating that convolutional layers weights initialization was based on the Glorot uniform, however the actual implementation was with modified version of Glorot uniform, known as the Xavier uniform, causing the results to not be reproducible when implemented on PyTorch. Lack of implementation details of the machine learning algorithms could lead to differing outcomes, while for performance metrics it could lead to different analysis.</p>
<p>Access to data The availability of data is required for outcome reproducibility for non-trivial data, such as synthetic data that can be generated by rules. However, data cannot always be made publicly available [Pineau et al. 2021] and might not be easily available to collect, i.e. medical data is sensitive [McDermott et al. 2021]. Availability of data may affect the outcome.</p>
<p>Access to code Code describes implementation details perfectly and is required for outcome reproducibility for experiments of some complexity. Reproducing the experiments will require more effort if the code that is necessary to run the experiments is not available [Gundersen 2019; McDermott et al. 2021; Pineau et al. 2021]. The version number or commit ID should be noted when referencing code in a git repository. Lack of code could both affect the outcome, analysis, and interpretation.</p>
<p>Stale URLs URLs in papers that link to software and data often stop working. Hennessey and Ge [2013] analyzed 14,489 unique web pages found in the abstracts of papers published between 1996 and 2010 and found that the median lifespan of these web pages was 9.3 years with $62 \%$ of them being archived. Can affect both outcome and analysis.</p>
<p>Documentation factors affect all reproducibility degrees, but can be alleviated by sharing code and data since the code and data themselves document so many aspects of the experiment. This is why code and data sharing is so effective for increasing reproducibility. The location and stability of the resource should be considered when posting data and code to enable reproducible results.</p>
<h2>10 DISCUSSION</h2>
<p>In recent years, several studies have sought to investigate potential sources of irreproducibility in machine learning. Most studies have investigated what are called implementation factors and algorithmic factors by Pham et al. [2020] and Zhuang et al. [2021], which both will lead to differing output between runs. Algorithmic factors are design choices related to randomness being introduced in different steps of machine learning algorithms, such as initialization and feature selection. Implementation factors are design choices related to, for example, which seeds are used when initializing the pseudo random number generators, which software and software versions the experiments require or the hardware the experiments are executed on. Other studies have investigated how the documentation of the experiments can be a source of irreproducible results [Gundersen and Kjensmo 2018; Raff 2019, 2021]. Improper design of a study can also make it irreproducible [Cockburn et al. 2020; Cremonesi and Jannach 2021; Dacrema et al. 2021; Simonsohn et al. 2014]. Finally, studies have pointed out that the design and execution of the evaluation can affect results [Card et al. 2020; Makridakis et al. 2018; Pineau et al. 2021].</p>
<p>Despite reproducibility being a cornerstone of science, Plesser [2018] argues that reproducibility is a confused term. Many definitions exist in the literature, see both [Plesser 2018] and [Gundersen 2021b] for reviews, but none of these definitions are easy to operationalize; it is not straight forward for researchers or practitioners to let the definitions guide them when trying to reproduce research and analyze failures to reproduce. The definitions do not help in understanding what exactly is required of a reproducibility experiment nor how the results should be evaluated to be considered a success or a failure. We will exemplify this by reviewing some of the most relevant definitions of reproducibility.</p>
<p>The Association for Computing Machinery (ACM) [2020], who based their definition on the Joint Committee for Guides in Metrology [2012], defines reproducibility as "the main results of the paper have been obtained in a subsequent study by a person or team other than the authors, using, in part, artifacts provided by the author" while replication is defined as "the main results of the paper have been independently obtained in a subsequent study by a person or team other than the authors, without the use of authorsupplied artifacts." These definitions are open for interpretation. It is not clear what exactly is meant by "main results have been obtained". Because of this, concluding whether a reproducibility experiment is a success or not is subjective. Furthermore, except for in a replication where no artifacts should be used, it is not clear which artifacts should or can be used for a reproducibility study. However, the definition by ACM is not alone in this ambiguity.</p>
<p>The definitions proposed by The U.S. National Academy of Science [2019] are also ambiguous. They define reproducibility as "obtaining consistent computational results using the same input data, computational steps, methods, and code, and conditions of analysis" and replicability to mean "obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data." In a similar fashion as ACM, it is not clear how to interpret "obtaining consistent results" in a practical setting. However, this definition clearly states what is the input to reproducibility studies and replications. Replication differs in stating that only data needs to be different in a replication study, which means that computational steps, methods, code and conditions of analysis can be the same. Still, it is not clear exactly how computational steps differs from code and methods, so even when being more precise the definitions are still ambiguous.</p>
<p>Peng [2011] also distinguishes between replication and reproducibility. While replication requires new evidence (in the form of data) for scientific claims to be independently evaluated, reproducibility requires code and data to be published. According to Peng [2011], the least reproducible research requires code to be published so that independent researchers can review it. Next level requires also data to be published while the gold standard is to share linked and executable code and data. Peng [2011] mentions that all the papers that were reviewed for reproducibility published in the journal Biostatistics at the time were reproducible. However, it is not clear what was required of a paper to be evaluated as reproducible. The same is an issue with the definitions.</p>
<p>Goodman et al. [2016] do not distinguish between reproducibility and replication. They view this to be the same concept, and the define three different reproducibility levels, results reproducibility, method reproducibility and inference reproducibility. These definitions have similar issues with ambiguity with regards to input of a reproducibility experiment and how to interpret the results. Gundersen [2021b] tries to solve this amiguity by proposing reproducibility types specifying which documentation (paper, code and data) provided by the authors of the study that are being utilized by independent researchers in the reproducibility experiment as well as introducing reproducibility degrees that specify how results can be interpreted to be considered a success. Further details are given in section 2.</p>
<p>Reproducibility definitions are not easy to operationalize because of their inherent ambiguity. No articles in literature, that we are aware of, provide an overview of experiment design choices that can lead to irreproducible results, nor do any articles try to relate the design choices to the interpretation of results. This article seeks to remedy these issues by providing a framework for reproducibility in machine learning that can easily be operationalized by researchers and practitioners to reduce failures of impossible tasks, engineering, post-deployment and communication [Raji et al. 2022].</p>
<p>According to Ioannidis [2005], the greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. This holds true for machine learning, as "the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole" as Sculley et al. [2018] phrase it. Machine learning methodology is concerned with ensuring that models generalize well to unseen data. Information from the test set should not be used to optimize the performance of a model. This is why machine learning competitions, such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), restrict the number of submissions per week. Breaking this rule resulted in a ban at the 2015 challenge [Markoff 2015]. Less attention has been given in machine learning methodology to how information can flow in a similar way from the experiment conclusion to the experiment design. The conclusion should not change the protocol on how it is inferred. Information should not flow from the conclusion to the analysis and further change how the experiment is re-executed or analyzed. This is especially important in the sciences, including computer science [Denning et al. 1988] and general and machine learning more specifically [Russell and Norvig 2020], where the design of an artifact is as important as
its experimental evaluation. Typically, algorithm design and experimental evaluation are done iteratively until progress has been made, which is a process that can be prone to methodological mishaps unless proper care is taken.</p>
<p>The framework would be highly relevant for reproducibility studies. The detailed description of the reproducibility study conducted by Di Nunzio and Minzoni [2023] did not rely on a reproducibility framework such as the one proposed here. While thorough, some relevant details are lacking. The authors base the reproducibility study on the article, code and data provided by the original authors, so it is clearly of type R4 Experiment. A difference in performance of almost 2 percentage points from the original study is reported. However, the study lacks a systematic and thorough discussion on the potential sources to this difference. Also, a good discussion on the improper evaluation of results in the reproduced study does not contain potential consequences of the evaluation. As performance differs, the reproducibility degree is not outcome reproducible (O) and the same methodology to evaluate the results is used so it is not interpretation reproducible (I). However, it is not clear whether the reproducibility study supports the conclusion of the original study and hence confirms the original study. If this was the case, the study would have been classified as R4A.</p>
<h2>11 LIMITATIONS</h2>
<p>While we believe that the grouping of design decisions into six categories might be at the right level and be exhaustive, the overview and taxonomy do probably not contain all design decisions that can lead to irreproducible results. Our goal is to capture as many as possible, but some might have eluded us. One way of being more certain about capturing as many design decisions as possible would be to perform a structured literature review. We did not do this, but not from a lack of trying. However, a search for "machine learning" AND "reproducibility" and similar terms returns too many irrelevant articles to be practically doable. Instead, we have relied on following the literature for several years. Also, this article does not contain any experiments; it is an overview with references. We do not quantify the importance of the different factors nor the extent to which they can affect the outcome either. The extent to how much performance might vary have been evaluated by other studies cited here. Also, according to these studies the difference in performance differs between machine learning methods and even deep learning architectures, so there is no clear rule of thumb. Hence, what is important is to test for variation and characterize it using statistical methods and through this increase rigor. Increasing rigor is exactly what we seek to support with this framework by showing how factors affect conclusions.</p>
<h2>12 CONCLUSION</h2>
<p>The main contribution of this paper is the identification and categorization of 41 design choices documented in the literature that can lead to false conclusions. Another major contribution is a novel framework that enables applied data science researchers and practitioners to understand which experiment design choices can lead to false findings, understand how these design choices can affect the conclusion of experiments and conduct and analyze reproducibility experiments. It is the first comprehensive framework for machine learning reproducibility that provides an overview and characterization of factors that affect reproducibility in machine learning experiments, and it extends existing reproducibility checklists for authors by identifying additional factors that span all levels of the technical stack including hardware. This is also the first work to describe how the reproducibility factors affect the conclusions that are drawn from experiments by relating those factors to the scientific method and different definitions and scope of reproducibility studies, In an era where reproducibility is a priority in all areas of science, our goal is to shed light on the reproducibility challenges and needs in machine learning so that forward-looking solutions</p>
<p>and methodologies can stem from this discipline and lead the way for other communities.</p>
<h2>REFERENCES</h2>
<p>Saeed S Alahmari, Dmitry B Goldgof, Peter R Mouton, and Lawrence O Hall. 2020. Challenges for the Repeatability of Deep Learning Models. IEEE Access 8 (2020), 211860-211868.
Association for Computing Machinery. 2020. Artifact Review and Budging - Version 1.0. https://www.acm.org/publications/policies/artifact-review-budging.
Monya Baker. 2016. Reproducibility crisis. Nature 533, 26 (2016), 353-66.
Anya Belz, Shubham Agarwal, Anastasia Shimorina, and Ehud Reiter. 2021a. A systematic review of reproducibility research in natural language processing. In 16th Conference of the European Chapter of the Associationfor Computational Linguistics 2021. Association for Computational Linguistics, 381-393.</p>
<p>Anya Belz, Anastasia Shimorina, Shubham Agarwal, and Ehud Reiter. 2021b. The reprogen shared task on reproducibility of human evaluations in nlg: Overview and results. In The 14th International Conference on Natural Language Generation.
Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. 2021. Accounting for variance in machine learning benchmarks. Proceedings of Machine Learning and Systems 3 (2021).
Xavier Bouthillier, CÃ©sar Laurent, and Pascal Vincent. 2019. Unreproducible research is reproducible. In International Conference on Machine Learning. PMLR, 725-734.
Xavier Bouthillier and GaÃ«l Varoquaux. 2020. Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020. Technical Report hal-02447823. Inria Saclay Ile de France.
Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5-32.
Katherine S Button, John Ioannidis, Claire Mokrysz, Brian A Nosek, Jonathan Flint, Emma SJ Robinson, and Marcus R MunafÃ². 2013. Power failure: why small sample size undermines the reliability of neuroscience. Nature reviews neuroscience 14, 5 (2013), 563-576.</p>
<p>Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, and Dan Jurafsky. 2020. With Little Power Comes Great Responsibility. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), $9263-9274$.
Andy Cockburn, Pierre Dragicevic, Lonni BesanÃ§on, and Carl Gutwin. 2020. Threats of a replication crisis in empirical computer science. Commun. ACM 63, 8 (2020), $70-79$.
Paul R Cohen. 1995. Empirical methods for artificial intelligence. Vol. 139. MIT press Cambridge.
ÃaÄrÄ± ÃÃ¶ltekin. 2020. Verification, reproduction and replication of NLP experiments: A case study on parsing Universal Dependencies. In Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020). 66-56.
A Feder Cooper, Yucheng Lu, Jessica Forde, and Christopher M De Sa. 2021. Hyperparameter Optimization Is Deceiving Us, and How to Stop It. Advances in Neural Information Processing Systems 34 (2021).
Matt Crane. 2018. Questionable answers in question answering research: Reproducibility and variability of published results. Transactions of the Association for Computational Linguistics 6 (2018), 241-252.
Paolo Cremonesi and Dietmar Jannach. 2021. Progress in recommender systems research: Crisis? What crisis? AI Magazine 42, 3 (2021), 43-54.
Maurizio Ferrari Dacrema, Simone Boglio, Paolo Cremonesi, and Dietmar Jannach. 2021. A troubling analysis of reproducibility and progress in recommender systems research. ACM Transactions on Information Systems (TOIS) 39, 2 (2021), 1-49.
Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are we really making much progress?: A worrying analysis of recent neural recommendation approaches. In Proceedings of the 13th ACM Conference on Recommender Systems. $101-109$.
Peter J Denning, Douglas E Comer, David Gries, Michael C Mulder, Allen Tucker, A Joe Turner, and Paul R Young. 1988. Report of the ACM task force on the core of Computer Science. ACM.
Giorgio Maria Di Nunzio and Riccardo Minzoni. 2023. A Thorough Reproducibility Study on Sentiment Classification: Methodology, Experimental Setting, Results. Information 14, 2 (2023). https://doi.org/10.3390/info14020076
Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A Smith. 2019. Show Your Work: Improved Reporting of Experimental Results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). $2185-2194$.
Chris Drummond. 2009. Replicability is not reproducibility: nor is it good science. In Proc. of the Evaluation Methods for Machine Learning Workshop at the 26th International Conference on Machine Learning, Montreal, Canada. http://cogprints. org/7691/
Johannes K Fichte, Markus Hecher, Ciaran McCreesh, and Anas Shahab. 2021. Complications for Computational Experiments from Modern Processors. In 27th International Conference on Principles and Practice of Constraint Programming (CP 2021).</p>
<p>Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik.
Samuel G Finlayson, Adarsh Subbaswamy, Karandrep Singh, John Bowers, Annabel Kupke, Jonathan Zittrain, Isaac S Kohane, and Suchi Saria. 2021. The clinician and dataset shift in artificial intelligence. New England Journal of Medicine 385, 3 (2021), $283-286$.
Timnit Gehru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal DaumÃ© Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 86-92.
Elizabeth Gibney. 2022. Could machine learning fuel a reproducibility crisis in science? Nature (2022).
Michael F Goodchild and Wenwen Li. 2021. Replication across space and time must be weak in the social and environmental sciences. Proceedings of the National Academy of Sciences 118, 35 (2021).
Steven N. Goodman, Daniele Fanelli, and John P. A. Ioannidis. 2016. What does research reproducibility mean? Science Translational Medicine 8, 341 (jun 2016), 341ps12-341ps12. https://doi.org/10.1126/scitranslmed.aaf3027
Franz GÃ¶tz-Hahn, Vlad Hossi, and Dietmar Saupe. 2020. Critical analysis on the reproducibility of visual quality assessment using deep features. arXiv preprint arXiv:2009.05369 (2020).
Odd Erik Gundersen. 2019. Standing on the Feet of Giants - Reproducibility in AI. AI Magazine 40, 4 (2019), 9-23.
Odd Erik Gundersen. 2021a. The Case Against Registered Reports. AI Mag. 42, 1 (2021), $88-92$.
Odd Erik Gundersen. 2021b. The fundamental principles of reproducibility. Philosophical Transactions of the Royal Society A 379, 2197 (2021), 20200210.
Odd Erik Gundersen, Malte Helmert, and Holger Hoos. 2023. Improving Reproducibility in AI Research: Four Mechanisms Adopted by JAIR. Journal of Artificial Intelligence Research Forthcoming (2023).
Odd Erik Gundersen and SigbjÃ¸rn Kjensmo. 2018. State of the art: Reproducibility in artificial intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.
Odd Erik Gundersen, Saeid Shamsaliei, and Richard Juul Iodahl. 2022. Do machine learning platforms provide out-of-the-box reproducibility? Future Generation Computer Systems 126 (2022), 34-47.
Benjamin Haibe-Kains, George Alexandru Adam, Ahmed Hosny, Farnoosh Khodakarami, Levi Waldron, Bo Wang, Chris McIntosh, Anna Goldenberg, Anshul Kundaje, Casey S Greene, et al. 2020. Transparency and reproducibility in artificial intelligence. Nature 586, 7829 (2020), E14-E16.
Peter Henderson, Riushat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, Vol. 32.
Jason Hennessey and Steven Xijin Ge. 2013. A cross disciplinary study of link decay and the effectiveness of mitigation techniques. In BMC bioinformatics, Vol. 14. BioMed Central, 1-11.
Song-You Hong, Myung-Seo Koo, Jihyeon Jang, Jung-Eun Esther Kim, Hoon Park, Min-Su Joh, Ji-Hoon Kang, and Tae-Jin Oh. 2013. An evaluation of the software system dependency of a global atmospheric model. Monthly Weather Review 141, 11 (2013), 4165-4172.
Jessica Hullman, Sayash Kapoor, Priyanka Nanayakkara, Andrew Gelman, and Arvind Narayanan. 2022. The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. 335-348.
Matthew Hutson. 2018. Artificial intelligence faces reproducibility crisis. Science 359, 6377 (2018), 725-726.
John PA Ioannidis. 2005. Why most published research findings are false. PLoS medicine 2, 8 (2005), e124.</p>
<p>Joint Committee for Guides in Metrology. 2012. International vocabulary of metrology - Basic and general concepts and associated terms - 3rd edition with minor corrections. https://www.bipm.org/utlfs/common/documents/jcgm/JCGM_200_2012.pdf.
Sayash Kapoor and Arvind Narayanan. 2022. Leakage and the Reproducibility Crisis in ML-based Science. arXiv preprint arXiv:2207.07048 (2022).
Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. 2018. The gan landscape: Losses, architectures, regularization, and normalization. In ICML 2018 Workshop on Reproducibility in Machine Learning.
Sara Latifi, Noemi Mauro, and Dietmar Jannach. 2021. Session-aware recommendation: A surprising quest for the state-of-the-art. Information Sciences 573 (2021), 291-315.
Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278-2324.
Zachary C. Lipton and Jacob Steinhardt. 2019. Troubling Trends in Machine Learning Scholarship: Some ML Papers Suffer from Flaws That Could Mislead the Public and Stymie Future Research. Queue 17, 1 (feb 2019), 45-77. https://doi.org/10.1145/ 3317287.3328534
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. 2018. Are GANs Created Equal? A Large-Scale Study. In NeurIPS.
Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2018. Statistical and Machine Learning forecasting methods: Concerns and ways forward. PloS one 13, 3 (2018), e0194889.</p>
<p>John Markoff. 2015. Computer Scientists Are Astir After Baidu Team Is Barred From A.I. Competition. New York Times. June 3, 2015.</p>
<p>Matthew BA McDermott, Shirly Wang, Nikki Marinsek, Rajesh Ranganath, Luca Foschini, and Marzyeh Ghassemi. 2021. Reproducibility in machine learning for health research: Still a ways to go. Science Translational Medicine 13, 386 (2021), eabb1655.
GÃ¡bor Melis, Chris Dyer, and Phil Blunsom. 2018. On the State of the Art of Evaluation in Neural Language Models. In International Conference on Learning Representations.
James Miller and Jane C Miller. 2018. Statistics and chemometrics for analytical chemistry. Pearson education.
Prabhat Nagarajan, Garrett Warnell, and Peter Stone. 2019. The Impact of Nondeterminism on Reproducibility in Deep Reinforcement Learning. Presented at the AAAI 2019 Workshop on Reproducible AI, Honolulu, Hawaii (2019).
Engineering National Academies of Sciences, Medicine, et al. 2019. Reproducibility and replicability in science. National Academies Press.
Open Science Collaboration. 2015. Estimating the reproducibility of psychological science. Science 349, 6251 (2015), aac4716.
Roger D. Peng. 2011. Reproducible research in computational science. Science 334, 6060 (2011), 1226-1227.
Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems and opportunities in training deep learning software systems: An analysis of variance. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. $771-783$.
Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent LariviÃ¨re, Alina Beygelzimer, Florence d'AlchÃ© Buc, Emily Fox, and Hugo Larochelle. 2021. Improving reproducibility in machine learning research: a report from the NeurIPS 2019 reproducibility program. Journal of Machine Learning Research 22 (2021).
Wagner GonÃ§alves Pinto, Antonio Alguacil, and MichaÃ«l Bauerheim. 2021. On the reproducibility of fully convolutional neural networks for modeling time-space evolving physical systems. arXiv preprint arXiv:2105.05482 (2021).
Hans E Plesser. 2018. Reproducibility vs. replicability: a brief history of a confused terminology. Frontiers in neuroinformatics 11 (2018), 76.
Line Pouchard, Yuewei Lin, and Hubertus Van Dam. 2020. Replicating Machine Learning Experiments in Materials Science. In Parallel Computing: Technology Trends. IOS Press, 743-755.
Edward Raff. 2019. A step toward quantifying independently reproducible machine learning research. Advances in Neural Information Processing Systems 32 (2019).
Edward Raff. 2021. Research Reproducibility as a Survival Analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 469-478.
Iniolowa Deborah Raji, I Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. 2022. The fallacy of AI functionality. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 959-972.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do imagenet classifiers generalize to imagenet?. In International Conference on Machine Learning. PMLR, 5389-5400.
Nils Reimers and Iryna Gurevych. 2017. Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 338-348.
Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I Aviles-Rivero, Christian Etmann, Cathal McCague, Lucian Beer, et al. 2021. Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans. Nature Machine Intelligence 3, 3 (2021), 199-217.
Lukas Rupprecht, James C Davis, Constantine Arnold, Yaniv Gur, and Deepavali Bhagwat. 2020. Improving reproducibility of data science pipelines through transparent provenance capture. Proceedings of the VLDB Endowment 13, 12 (2020), 3354-3368.
Stuart Russell and Peter Norvig. 2020. Artificial Intelligence: A Modern Approach. Pearson, London.
David Sculley, Jasper Snoek, Alex Wiltschko, and Ali Rahimi. 2018. Winner's curse? On pace, progress, and empirical rigor. In ICLR 2018 Workshop Track.
Mostafa Shahriari, Rudolf Ramler, and Lukas Fischer. 2022. How Do Deep-Learning Framework Versions Affect the Reproducibility of Neural Network Models? Machine Learning and Knowledge Extraction 4, 4 (2022), 888-911.
Uri Simonsohn, Leif D Nelson, and Joseph P Simmons. 2014. P-curve: a key to the file-drawer. Journal of experimental psychology: General 143, 2 (2014), 534.
Victoria Stodden. 2015. Reproducing statistical results. Annual Review of Statistics and Its Application 2 (2015), 1-19.
Michela Taufer, Omar Padron, Philip Saponaro, and Sandeep Patel. 2010. Improving numerical reproducibility and stability in large-scale numerical simulations on GPUs. In 2010 IEEE International Symposium on Parallel \&amp; Distributed Processing (IPDPS). IEEE, 1-9.
Antonio Torralba and Alexei A Efros. 2011. Unbiased look at dataset bias. In CVPR 2011. IEEE, 1521-1528.</p>
<p>Gatl Varoquaux and Veronika Cheplygina. 2022. Machine learning for medical imaging: methodological failures and recommendations for the future. NPJ digital medicine 5, 1 (2022), 1-8.</p>
<p>Xuan Zhang and Kevin Duh. 2020. Reproducible and efficient benchmarks for hyperparameter optimization of neural machine translation systems. Transactions of the Association for Computational Linguistics 8 (2020), 393-408.
Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, and Sara Hooker. 2021. Randomness in neural network training: Characterizing the impact of tooling. arXiv preprint arXiv:2106.11872 (2021).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).
(c) 2023 Copyright held by the owner/author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1003 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1003</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1003</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-29bff398999e927dcc5702d164127262d2524163</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/29bff398999e927dcc5702d164127262d2524163" target="_blank">Causal Influence Detection for Improving Efficiency in Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a measure of situation-dependent causal influence based on conditional mutual information and shows that it can reliably detect states of influence and proposes several ways to integrate this measure into RL algorithms to improve exploration and off-policy learning.</p>
                <p><strong>Paper Abstract:</strong> Many reinforcement learning (RL) environments consist of independent entities that interact sparsely. In such environments, RL agents have only limited influence over other entities in any particular situation. Our idea in this work is that learning can be efficiently guided by knowing when and what the agent can influence with its actions. To achieve this, we introduce a measure of \emph{situation-dependent causal influence} based on conditional mutual information and show that it can reliably detect states of influence. We then propose several ways to integrate this measure into RL algorithms to improve exploration and off-policy learning. All modified algorithms show strong increases in data efficiency on robotic manipulation tasks.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1003.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1003.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Action Influence (CAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A situation-dependent, entity-specific measure of causal influence defined as the pointwise conditional mutual information I(S'_j; A | S = s), estimated from a learned probabilistic transition model and used to detect when the agent can causally affect an entity; integrated into RL for exploration and prioritized replay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal Action Influence (CAI)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CAI = I(S'_j; A | S = s). Algorithmic pipeline used in the paper: (1) learn a probabilistic transition model p_theta(s'_j | s, a) with a neural network that outputs Gaussian parameters (mean and variance) via maximum likelihood on collected transitions; (2) choose a sampling policy pi over actions (the authors use a uniform policy) and draw K actions a^{(1..K)} ~ pi; (3) approximate the marginal p(s'_j | s) by the finite mixture (1/K) sum_k p(s'_j | s, a^{(k)}); (4) estimate CAI(s) by averaging KL divergences: (1/K) sum_i D_KL( p(s'_j | s, a^{(i)}) || mixture ); (5) approximate mixture-vs-component KL using a Gaussian / Gaussian-mixture KL approximation (Durrieu et al. mean approximation) and threshold C^j(s) to classify 'agent in control'; (6) use CAI as intrinsic reward (r_CAI = C^j(s)), for active action selection (choose a maximizing the per-action KL contribution), and for episode prioritization (rank episodes by summed CAI and upweight high-influence episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>IDSLIDE, FetchPickAndPlace, FetchRotTable (MuJoCo-based Fetch manipulation suite variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive continuous-control virtual robotics environments (MuJoCo-based Fetch tasks) and a toy 1D sliding environment (IDSLIDE). These are episodic, goal-conditioned, interactive environments that allow the agent to perform interventions (choose actions) and actively experiment; FetchRotTable introduces an external rotating-table source of object motion (a distractor/confounder).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicitly models conditional dependence of next-entity state on actions given the full state (CMI); by conditioning on S and using p(s'|s,a) the measure attributes variation in S'_j to A rather than to other causes; tested in an environment with an external moving table (FetchRotTable) to distinguish agent-caused vs table-caused object motion. Additionally, CAI-based prioritization downweights (de-prioritizes) episodes with low CAI, effectively reducing influence of spurious/noisy episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>External causes of the same observed effect (confounding / alternate causal sources), measurement noise (observation noise evaluated in experiments), irrelevant state variation (distractors like object moved by rotating table rather than agent).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious vs agent-caused signals by testing conditional dependence: CAI(s) = I(S'_j; A | S=s) estimated via learned p(s'_j|s,a) and Monte Carlo over actions; low CAI indicates independence (no causal influence of A), high CAI indicates dependence (agent in control). Thresholding converts score to binary detection.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Causal-action-influence Prioritization (CAI-P): episodes are ranked by their total summed CAI across timesteps; sampling probabilities for replay are assigned inversely proportional to rank (higher total CAI -> higher priority), which reduces sampling of episodes dominated by spurious or non-agent-caused transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Active experimentation: select actions that maximize the KL contribution (the action whose p(s'|s,a) deviates most from the mixture) to test the causal hypothesis; compare predicted p(s'|s,a) to observed outcomes and update the model if mismatches occur (model self-correction). This constitutes a refutation/verification loop via interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>For exploratory actions, sample K actions from pi and select a* = argmax_a D_KL( p(s'_j|s,a) || (1/K) sum_k p(s'_j|s,a^{(k)}) ). A fraction of exploratory steps (e.g., within epsilon exploration budget) are chosen actively using this criterion; also sample from pi for CAI estimation and include pi-sampled actions during data collection to reduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Influence-detection metrics (Table 1): IDSLIDE AUC = 1.00 ± 0.00, AP = 0.98 ± 0.00, F1 = 0.95 ± 0.01; FETCHPICKANDPLACE AUC = 0.97 ± 0.01, AP = 0.96 ± 0.00, F1 = 0.89 ± 0.00. CAI performance degrades gracefully with added observation noise (evaluated in Figure 2c). In RL tasks, CAI variants (bonus, active exploration, CAI-P) yield large sample-efficiency gains: reward-bonus + active exploration + prioritization can reach high success rates several times faster than baselines; CAI-P improved learning speed 1.5–2.5× over PER and matches or outperforms an oracle energy-based method (EBP) in FetchRotTable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines in detection (Table 1): Attention [24] AUC=0.42±0.31 (IDSLIDE), 0.46±0.06 (FetchPickAndPlace); Entropy baseline AUC=0.96 (IDSLIDE) and 0.84 (FetchPickAndPlace) but much lower AP and F1; Contact oracle and Entropy perform worse in precision/recall trade-offs. In RL, baseline DDPG+HER without CAI reaches target success rates substantially slower (e.g., CAI bonus reached 60% success ~4× faster than DDPG+HER).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>1 distractor in FetchRotTable (periodically rotating table); also experiments varying observation noise and K (number of sampled actions) in CAI estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CAI (pointwise conditional mutual information estimated from a learned transition model) reliably detects whether the agent can influence a specific entity in situ, is robust to moderate observation noise, and can distinguish agent-caused motion from externally-caused motion (rotating table). Integrating CAI into RL (intrinsic reward, active action selection, and prioritized replay) substantially improves sample efficiency; CAI outperforms heuristic attention-based influence methods and entropy baselines on both detection and downstream RL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Influence Detection for Improving Efficiency in Reinforcement Learning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1003.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1003.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention (Pitis et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer Attention-based Influence (as in Pitis et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic method that uses Transformer attention weights over factored state components to infer influence between agent actions and entity states; proposed in Pitis et al. (2020) for counterfactual data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counterfactual data augmentation using locally factored dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Attention-based influence (Transformer attention heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses a Transformer model over factorized state components; the attention weights assigned from action/agent components to entity components are interpreted as indicating influence; employed as a classifier/score for whether the agent influenced an entity. Treated as a heuristic proxy for influence rather than a formal causal statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>IDSLIDE, FetchPickAndPlace (same testbeds used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated manipulation environments (MuJoCo Fetch and toy IDSLIDE) where attention is computed on factorized state inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Not designed to explicitly address confounding/distractors; sensitive to representation and training data.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Use attention weights as scoring; thresholded to yield binary influence labels.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Poor detection performance in the paper's tests: IDSLIDE AUC = 0.42 ± 0.31, AP = 0.13 ± 0.14, F1 = 0.18 ± 0.17; FetchPickAndPlace AUC = 0.46 ± 0.06, AP = 0.44 ± 0.04, F1 = 0.62 ± 0.00 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformer attention weights as a heuristic for causal influence performed poorly in the tested manipulation environments compared to CAI; attention heuristics failed to reliably separate agent-caused from non-agent-caused transitions in these interactive virtual labs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Influence Detection for Improving Efficiency in Reinforcement Learning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1003.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1003.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Janzing Causal Strength</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantifying Causal Influences (Janzing et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal measure of causal strength defined as the KL divergence between the original joint distribution and a distribution where a target arrow X->Y is 'cut' and X's marginal is fed into the mechanism producing Y; intended to quantify how strongly X influences Y.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying causal influences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal strength (Janzing et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For arrow X->Y, define the post-cut distribution p_{X->Y} by replacing the conditional p(y|x,pa_Y\x) with the marginalized version ∫ p(y|x,pa_Y\x) p(x) dx; causal strength is D_KL( P(V) || P_{X->Y}(V) ), which decomposes into an expectation over parent configurations of KLs between conditionals. Provides axiomatic properties for a causal influence measure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>General causal graphical model setting; not directly an environment but a theoretical measure applicable to interventions and modeled systems.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>The measure formalizes the effect of removing an arrow; can in principle reflect alternate causes by showing small causal strength when cutting the arrow does not change distribution, but does not itself provide algorithms for handling distractors in interactive labs.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compare full model vs 'cut' model KL; if KL is large, arrow has strong causal influence.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Conceptually, cutting an arrow and measuring distributional change is a form of refutation/ablation of a causal link; however, practical refutation requires estimating models and performing interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper shows CAI is a pointwise (state-conditional) version of Janzing et al.'s causal strength when the action sampling policy is independent of state, thereby grounding CAI in an established formal causal-strength notion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Influence Detection for Improving Efficiency in Reinforcement Learning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1003.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1003.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transfer Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer Entropy / Granger-style information transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A time-series information-theoretic measure of directed information transfer (a non-linear generalization of Granger causality) that quantifies how much past of one process reduces uncertainty about the next state of another process.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring information transfer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Transfer entropy (local / one-step variants)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Transfer entropy measures conditional mutual information of the target's next state with the source's past given the target's past; used to quantify directed influence in time series. The paper positions CAI as similar to a one-step, local transfer-entropy but with conditioning on the full state S rather than only histories.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Originally developed for time-series analysis; can be applied in simulated interactive environments but requires suitable time-history representations.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Addresses dynamical dependence vs independence; not specifically designed to separate simultaneous external drivers without conditioning on them.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Estimate conditional mutual informations from observed time series or via models; local/pointwise variants examine individual transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper contrasts CAI with transfer entropy and notes the advantage of conditioning on the full state S for disentangling causes in sparse multi-entity environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Influence Detection for Improving Efficiency in Reinforcement Learning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1003.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1003.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NN/KDE CMI estimators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest-neighbor and kernel-based Conditional Mutual Information estimators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Nonparametric estimators for mutual information and conditional mutual information based on k-nearest-neighbors or kernel density methods, commonly used for causal discovery but known to scale poorly to high-dimensional conditioning variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Estimating mutual information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Nearest-neighbor (Kraskov) and kernel CMI estimators (Runge, Kandasamy et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>k-NN–based estimators (Kraskov et al.) and kernel-based / von Mises estimators estimate (conditional) mutual information directly from samples via nearest-neighbor statistics or kernel density functionals; conditional tests like Runge's nearest-neighbor CMI test exist for conditional independence testing.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>General-purpose nonparametric estimators for mutual information/CMI; applicable to offline datasets from interactive environments but struggle with high-dimensional S and limited data per conditioning point.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>In principle can detect conditional dependence in the presence of confounders if the confounder is observed and conditioned upon; in practice limited by high-dimensional conditioning and sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Direct nonparametric estimation of (conditional) mutual information and conditional independence testing.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Not used in experiments due to scaling limitations; paper notes poor scalability to higher-dimensional states.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors note these estimators are theoretically valid but do not scale well to the high-dimensional conditioning needed for in-situ, state-conditional detection in manipulation tasks; motivates the learned-model approximation used for CAI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Influence Detection for Improving Efficiency in Reinforcement Learning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1003.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1003.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIME / Ensemble Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIME (Variational Information Maximizing Exploration) and Ensemble Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Exploration baselines: VIME maximizes information gain about transition dynamics (epistemic uncertainty reduction) via a Bayesian/variational model; ensemble disagreement uses an ensemble of dynamics models and selects actions/states with high prediction disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vime: Variational information maximizing exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>VIME and ensemble-disagreement baselines</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>VIME: maintain a Bayesian/variational dynamics model and compute information gain (KL between posterior and prior) as intrinsic reward; Ensemble disagreement: train multiple dynamics models and use variance (disagreement) of their predictions as an intrinsic exploration bonus that targets regions of high epistemic uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>FetchPickAndPlace and related manipulation environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive continuous-control robotic manipulation tasks used to evaluate exploration strategies; episodic, allows active interventions and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>These methods primarily address epistemic uncertainty and model-learning progress rather than separating agent-caused vs externally-caused signals (confounders/distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Intrinsic reward based on information gain (VIME) or ensemble variance (disagreement) to drive exploration toward informative regions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Select actions/states that maximize information gain (VIME) or prediction disagreement (ensemble), thereby actively exploring to reduce model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In comparisons, ensemble disagreement is a viable alternative to CAI for exploration, but CAI variants combining bonus and active action selection performed best overall (Figure 6). Exact numeric comparisons are shown in the paper's RL learning curves (no single scalar table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VIME and ensemble disagreement are useful information-theoretic exploration baselines; CAI-based exploration (influence-specific) outperformed VIME variants in the tested manipulation tasks, indicating that explicitly focusing on agent-to-entity causal influence yields better exploration in sparse, entity-factorized environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Influence Detection for Improving Efficiency in Reinforcement Learning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Counterfactual data augmentation using locally factored dynamics <em>(Rating: 2)</em></li>
                <li>Quantifying causal influences <em>(Rating: 2)</em></li>
                <li>Measuring information transfer <em>(Rating: 1)</em></li>
                <li>Vime: Variational information maximizing exploration <em>(Rating: 1)</em></li>
                <li>Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning <em>(Rating: 1)</em></li>
                <li>On variational bounds of mutual information <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1003",
    "paper_id": "paper-29bff398999e927dcc5702d164127262d2524163",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "CAI",
            "name_full": "Causal Action Influence (CAI)",
            "brief_description": "A situation-dependent, entity-specific measure of causal influence defined as the pointwise conditional mutual information I(S'_j; A | S = s), estimated from a learned probabilistic transition model and used to detect when the agent can causally affect an entity; integrated into RL for exploration and prioritized replay.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Causal Action Influence (CAI)",
            "method_description": "CAI = I(S'_j; A | S = s). Algorithmic pipeline used in the paper: (1) learn a probabilistic transition model p_theta(s'_j | s, a) with a neural network that outputs Gaussian parameters (mean and variance) via maximum likelihood on collected transitions; (2) choose a sampling policy pi over actions (the authors use a uniform policy) and draw K actions a^{(1..K)} ~ pi; (3) approximate the marginal p(s'_j | s) by the finite mixture (1/K) sum_k p(s'_j | s, a^{(k)}); (4) estimate CAI(s) by averaging KL divergences: (1/K) sum_i D_KL( p(s'_j | s, a^{(i)}) || mixture ); (5) approximate mixture-vs-component KL using a Gaussian / Gaussian-mixture KL approximation (Durrieu et al. mean approximation) and threshold C^j(s) to classify 'agent in control'; (6) use CAI as intrinsic reward (r_CAI = C^j(s)), for active action selection (choose a maximizing the per-action KL contribution), and for episode prioritization (rank episodes by summed CAI and upweight high-influence episodes).",
            "environment_name": "IDSLIDE, FetchPickAndPlace, FetchRotTable (MuJoCo-based Fetch manipulation suite variants)",
            "environment_description": "Interactive continuous-control virtual robotics environments (MuJoCo-based Fetch tasks) and a toy 1D sliding environment (IDSLIDE). These are episodic, goal-conditioned, interactive environments that allow the agent to perform interventions (choose actions) and actively experiment; FetchRotTable introduces an external rotating-table source of object motion (a distractor/confounder).",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicitly models conditional dependence of next-entity state on actions given the full state (CMI); by conditioning on S and using p(s'|s,a) the measure attributes variation in S'_j to A rather than to other causes; tested in an environment with an external moving table (FetchRotTable) to distinguish agent-caused vs table-caused object motion. Additionally, CAI-based prioritization downweights (de-prioritizes) episodes with low CAI, effectively reducing influence of spurious/noisy episodes.",
            "spurious_signal_types": "External causes of the same observed effect (confounding / alternate causal sources), measurement noise (observation noise evaluated in experiments), irrelevant state variation (distractors like object moved by rotating table rather than agent).",
            "detection_method": "Detects spurious vs agent-caused signals by testing conditional dependence: CAI(s) = I(S'_j; A | S=s) estimated via learned p(s'_j|s,a) and Monte Carlo over actions; low CAI indicates independence (no causal influence of A), high CAI indicates dependence (agent in control). Thresholding converts score to binary detection.",
            "downweighting_method": "Causal-action-influence Prioritization (CAI-P): episodes are ranked by their total summed CAI across timesteps; sampling probabilities for replay are assigned inversely proportional to rank (higher total CAI -&gt; higher priority), which reduces sampling of episodes dominated by spurious or non-agent-caused transitions.",
            "refutation_method": "Active experimentation: select actions that maximize the KL contribution (the action whose p(s'|s,a) deviates most from the mixture) to test the causal hypothesis; compare predicted p(s'|s,a) to observed outcomes and update the model if mismatches occur (model self-correction). This constitutes a refutation/verification loop via interventions.",
            "uses_active_learning": true,
            "inquiry_strategy": "For exploratory actions, sample K actions from pi and select a* = argmax_a D_KL( p(s'_j|s,a) || (1/K) sum_k p(s'_j|s,a^{(k)}) ). A fraction of exploratory steps (e.g., within epsilon exploration budget) are chosen actively using this criterion; also sample from pi for CAI estimation and include pi-sampled actions during data collection to reduce bias.",
            "performance_with_robustness": "Influence-detection metrics (Table 1): IDSLIDE AUC = 1.00 ± 0.00, AP = 0.98 ± 0.00, F1 = 0.95 ± 0.01; FETCHPICKANDPLACE AUC = 0.97 ± 0.01, AP = 0.96 ± 0.00, F1 = 0.89 ± 0.00. CAI performance degrades gracefully with added observation noise (evaluated in Figure 2c). In RL tasks, CAI variants (bonus, active exploration, CAI-P) yield large sample-efficiency gains: reward-bonus + active exploration + prioritization can reach high success rates several times faster than baselines; CAI-P improved learning speed 1.5–2.5× over PER and matches or outperforms an oracle energy-based method (EBP) in FetchRotTable.",
            "performance_without_robustness": "Baselines in detection (Table 1): Attention [24] AUC=0.42±0.31 (IDSLIDE), 0.46±0.06 (FetchPickAndPlace); Entropy baseline AUC=0.96 (IDSLIDE) and 0.84 (FetchPickAndPlace) but much lower AP and F1; Contact oracle and Entropy perform worse in precision/recall trade-offs. In RL, baseline DDPG+HER without CAI reaches target success rates substantially slower (e.g., CAI bonus reached 60% success ~4× faster than DDPG+HER).",
            "has_ablation_study": true,
            "number_of_distractors": "1 distractor in FetchRotTable (periodically rotating table); also experiments varying observation noise and K (number of sampled actions) in CAI estimation.",
            "key_findings": "CAI (pointwise conditional mutual information estimated from a learned transition model) reliably detects whether the agent can influence a specific entity in situ, is robust to moderate observation noise, and can distinguish agent-caused motion from externally-caused motion (rotating table). Integrating CAI into RL (intrinsic reward, active action selection, and prioritized replay) substantially improves sample efficiency; CAI outperforms heuristic attention-based influence methods and entropy baselines on both detection and downstream RL tasks.",
            "uuid": "e1003.0",
            "source_info": {
                "paper_title": "Causal Influence Detection for Improving Efficiency in Reinforcement Learning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Attention (Pitis et al.)",
            "name_full": "Transformer Attention-based Influence (as in Pitis et al.)",
            "brief_description": "A heuristic method that uses Transformer attention weights over factored state components to infer influence between agent actions and entity states; proposed in Pitis et al. (2020) for counterfactual data augmentation.",
            "citation_title": "Counterfactual data augmentation using locally factored dynamics",
            "mention_or_use": "use",
            "method_name": "Attention-based influence (Transformer attention heuristic)",
            "method_description": "Uses a Transformer model over factorized state components; the attention weights assigned from action/agent components to entity components are interpreted as indicating influence; employed as a classifier/score for whether the agent influenced an entity. Treated as a heuristic proxy for influence rather than a formal causal statistic.",
            "environment_name": "IDSLIDE, FetchPickAndPlace (same testbeds used for comparison)",
            "environment_description": "Interactive simulated manipulation environments (MuJoCo Fetch and toy IDSLIDE) where attention is computed on factorized state inputs.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Not designed to explicitly address confounding/distractors; sensitive to representation and training data.",
            "detection_method": "Use attention weights as scoring; thresholded to yield binary influence labels.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Poor detection performance in the paper's tests: IDSLIDE AUC = 0.42 ± 0.31, AP = 0.13 ± 0.14, F1 = 0.18 ± 0.17; FetchPickAndPlace AUC = 0.46 ± 0.06, AP = 0.44 ± 0.04, F1 = 0.62 ± 0.00 (Table 1).",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Transformer attention weights as a heuristic for causal influence performed poorly in the tested manipulation environments compared to CAI; attention heuristics failed to reliably separate agent-caused from non-agent-caused transitions in these interactive virtual labs.",
            "uuid": "e1003.1",
            "source_info": {
                "paper_title": "Causal Influence Detection for Improving Efficiency in Reinforcement Learning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Janzing Causal Strength",
            "name_full": "Quantifying Causal Influences (Janzing et al.)",
            "brief_description": "A formal measure of causal strength defined as the KL divergence between the original joint distribution and a distribution where a target arrow X-&gt;Y is 'cut' and X's marginal is fed into the mechanism producing Y; intended to quantify how strongly X influences Y.",
            "citation_title": "Quantifying causal influences",
            "mention_or_use": "mention",
            "method_name": "Causal strength (Janzing et al.)",
            "method_description": "For arrow X-&gt;Y, define the post-cut distribution p_{X-&gt;Y} by replacing the conditional p(y|x,pa_Y\\x) with the marginalized version ∫ p(y|x,pa_Y\\x) p(x) dx; causal strength is D_KL( P(V) || P_{X-&gt;Y}(V) ), which decomposes into an expectation over parent configurations of KLs between conditionals. Provides axiomatic properties for a causal influence measure.",
            "environment_name": null,
            "environment_description": "General causal graphical model setting; not directly an environment but a theoretical measure applicable to interventions and modeled systems.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "The measure formalizes the effect of removing an arrow; can in principle reflect alternate causes by showing small causal strength when cutting the arrow does not change distribution, but does not itself provide algorithms for handling distractors in interactive labs.",
            "detection_method": "Compare full model vs 'cut' model KL; if KL is large, arrow has strong causal influence.",
            "downweighting_method": null,
            "refutation_method": "Conceptually, cutting an arrow and measuring distributional change is a form of refutation/ablation of a causal link; however, practical refutation requires estimating models and performing interventions.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The paper shows CAI is a pointwise (state-conditional) version of Janzing et al.'s causal strength when the action sampling policy is independent of state, thereby grounding CAI in an established formal causal-strength notion.",
            "uuid": "e1003.2",
            "source_info": {
                "paper_title": "Causal Influence Detection for Improving Efficiency in Reinforcement Learning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Transfer Entropy",
            "name_full": "Transfer Entropy / Granger-style information transfer",
            "brief_description": "A time-series information-theoretic measure of directed information transfer (a non-linear generalization of Granger causality) that quantifies how much past of one process reduces uncertainty about the next state of another process.",
            "citation_title": "Measuring information transfer",
            "mention_or_use": "mention",
            "method_name": "Transfer entropy (local / one-step variants)",
            "method_description": "Transfer entropy measures conditional mutual information of the target's next state with the source's past given the target's past; used to quantify directed influence in time series. The paper positions CAI as similar to a one-step, local transfer-entropy but with conditioning on the full state S rather than only histories.",
            "environment_name": null,
            "environment_description": "Originally developed for time-series analysis; can be applied in simulated interactive environments but requires suitable time-history representations.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Addresses dynamical dependence vs independence; not specifically designed to separate simultaneous external drivers without conditioning on them.",
            "detection_method": "Estimate conditional mutual informations from observed time series or via models; local/pointwise variants examine individual transitions.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Paper contrasts CAI with transfer entropy and notes the advantage of conditioning on the full state S for disentangling causes in sparse multi-entity environments.",
            "uuid": "e1003.3",
            "source_info": {
                "paper_title": "Causal Influence Detection for Improving Efficiency in Reinforcement Learning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "NN/KDE CMI estimators",
            "name_full": "Nearest-neighbor and kernel-based Conditional Mutual Information estimators",
            "brief_description": "Nonparametric estimators for mutual information and conditional mutual information based on k-nearest-neighbors or kernel density methods, commonly used for causal discovery but known to scale poorly to high-dimensional conditioning variables.",
            "citation_title": "Estimating mutual information",
            "mention_or_use": "mention",
            "method_name": "Nearest-neighbor (Kraskov) and kernel CMI estimators (Runge, Kandasamy et al.)",
            "method_description": "k-NN–based estimators (Kraskov et al.) and kernel-based / von Mises estimators estimate (conditional) mutual information directly from samples via nearest-neighbor statistics or kernel density functionals; conditional tests like Runge's nearest-neighbor CMI test exist for conditional independence testing.",
            "environment_name": null,
            "environment_description": "General-purpose nonparametric estimators for mutual information/CMI; applicable to offline datasets from interactive environments but struggle with high-dimensional S and limited data per conditioning point.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "In principle can detect conditional dependence in the presence of confounders if the confounder is observed and conditioned upon; in practice limited by high-dimensional conditioning and sample size.",
            "detection_method": "Direct nonparametric estimation of (conditional) mutual information and conditional independence testing.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Not used in experiments due to scaling limitations; paper notes poor scalability to higher-dimensional states.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Authors note these estimators are theoretically valid but do not scale well to the high-dimensional conditioning needed for in-situ, state-conditional detection in manipulation tasks; motivates the learned-model approximation used for CAI.",
            "uuid": "e1003.4",
            "source_info": {
                "paper_title": "Causal Influence Detection for Improving Efficiency in Reinforcement Learning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "VIME / Ensemble Disagreement",
            "name_full": "VIME (Variational Information Maximizing Exploration) and Ensemble Disagreement",
            "brief_description": "Exploration baselines: VIME maximizes information gain about transition dynamics (epistemic uncertainty reduction) via a Bayesian/variational model; ensemble disagreement uses an ensemble of dynamics models and selects actions/states with high prediction disagreement.",
            "citation_title": "Vime: Variational information maximizing exploration",
            "mention_or_use": "use",
            "method_name": "VIME and ensemble-disagreement baselines",
            "method_description": "VIME: maintain a Bayesian/variational dynamics model and compute information gain (KL between posterior and prior) as intrinsic reward; Ensemble disagreement: train multiple dynamics models and use variance (disagreement) of their predictions as an intrinsic exploration bonus that targets regions of high epistemic uncertainty.",
            "environment_name": "FetchPickAndPlace and related manipulation environments",
            "environment_description": "Interactive continuous-control robotic manipulation tasks used to evaluate exploration strategies; episodic, allows active interventions and exploration.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "These methods primarily address epistemic uncertainty and model-learning progress rather than separating agent-caused vs externally-caused signals (confounders/distractors).",
            "detection_method": "Intrinsic reward based on information gain (VIME) or ensemble variance (disagreement) to drive exploration toward informative regions.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Select actions/states that maximize information gain (VIME) or prediction disagreement (ensemble), thereby actively exploring to reduce model uncertainty.",
            "performance_with_robustness": "In comparisons, ensemble disagreement is a viable alternative to CAI for exploration, but CAI variants combining bonus and active action selection performed best overall (Figure 6). Exact numeric comparisons are shown in the paper's RL learning curves (no single scalar table).",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "VIME and ensemble disagreement are useful information-theoretic exploration baselines; CAI-based exploration (influence-specific) outperformed VIME variants in the tested manipulation tasks, indicating that explicitly focusing on agent-to-entity causal influence yields better exploration in sparse, entity-factorized environments.",
            "uuid": "e1003.5",
            "source_info": {
                "paper_title": "Causal Influence Detection for Improving Efficiency in Reinforcement Learning",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Counterfactual data augmentation using locally factored dynamics",
            "rating": 2
        },
        {
            "paper_title": "Quantifying causal influences",
            "rating": 2
        },
        {
            "paper_title": "Measuring information transfer",
            "rating": 1
        },
        {
            "paper_title": "Vime: Variational information maximizing exploration",
            "rating": 1
        },
        {
            "paper_title": "Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning",
            "rating": 1
        },
        {
            "paper_title": "On variational bounds of mutual information",
            "rating": 1
        }
    ],
    "cost": 0.01972775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Causal Influence Detection for Improving Efficiency in Reinforcement Learning</h1>
<p>Maximilian Seitzer<br>MPI for Intelligent Systems<br>Tübingen, Germany<br>maximilian.seitzer@tue.mpg.de</p>
<p>Bernhard Schölkopf
MPI for Intelligent Systems
Tübingen, Germany
bs@tue.mpg.de</p>
<h2>Georg Martius</h2>
<p>MPI for Intelligent Systems
Tübingen, Germany
georg.martius@tue.mpg.de</p>
<h2>Abstract</h2>
<p>Many reinforcement learning (RL) environments consist of independent entities that interact sparsely. In such environments, RL agents have only limited influence over other entities in any particular situation. Our idea in this work is that learning can be efficiently guided by knowing when and what the agent can influence with its actions. To achieve this, we introduce a measure of situation-dependent causal influence based on conditional mutual information and show that it can reliably detect states of influence. We then propose several ways to integrate this measure into RL algorithms to improve exploration and off-policy learning. All modified algorithms show strong increases in data efficiency on robotic manipulation tasks.</p>
<h2>1 Introduction</h2>
<p>Reinforcement learning (RL) is a promising route towards versatile and dexterous artificial agents. Learning from interactions can lead to robust control strategies that can cope with all the intricacies of the real world that are hard to engineer correctly. Still, many relevant tasks such as object manipulation pose significant challenges for RL. Although impressive results have been achieved using simulation-to-real transfer [1] or heavy physical parallelization [2], training requires countless hours of interaction. Improving sample efficiency is thus a key concern in RL. In this paper, we approach this issue from a causal inference perspective.
When is an agent in control of its environment? An agent can only influence the environment by its actions. This seemingly trivial observation has the underappreciated aspect that the causal influence of actions is situation dependent. Consider the simple scenario of a robotic arm in front of an object on a table. Clearly, the object can only be moved when contact between the robot and object is made. Generally, there are situations where immediate causal influence is possible, while in others, none is. In this work, we formalize this situation-dependent nature of control and show how it can be exploited to improve the sample efficiency of RL agents. To this end, we derive a measure that captures the causal influence of actions on the environment and devise a practical method to compute it.
Knowing when the agent has control over an object of interest is important both from a learning and an exploration perspective. The learning algorithm should pay particular attention to these situations because (i) the robot is initially rarely in control of the object of interest, making training inefficient, (ii) physical contacts are hard to model, thus require more effort to learn and (iii) these states are enabling manipulation towards further goals. But for learning to take place, the algorithm first needs data that contains these relevant states. Thus, the agent has to take its causal influence into account already during exploration.
We propose several ways in which our measure of causal influence can be integrated into RL algorithms to address both the exploration, and the learning side. For exploration, agents can be rewarded with a bonus for visiting states of causal influence. We show that such a bonus leads the agent to quickly discover useful behavior even in the absence of task-specific rewards. Moreover,</p>
<p>our approach allows to explicitly guide the exploration to favor actions with higher predicted causal impact. This works well as an alternative to $\epsilon$-greedy exploration, as we demonstrate. Finally, for learning, we propose an off-policy prioritization scheme and show that it reliably improves data efficiency. Each of our investigations is backed by empirical evaluations in robotic manipulation environments and demonstrates a clear improvement of the state-of-the-art with the same generic influence measure.</p>
<h1>2 Related Work</h1>
<p>The idea underlying our work is that an agent can only sometimes influence its surroundings. This rests on two basic assumptions about the causal structure of the world. The first is that the world consists of independent entities, in accordance with the principle of independent causal mechanisms (ICM) [3], stating that the world's generative process consists of autonomous modules. The second assumption is that the potential influence that entities have over other entities is localized spatially and occurs sparsely in time. We can see this as explaining the sparse mechanism shift hypothesis, which states that naturally occurring distribution shifts will be due to local mechanism changes [4]. This is usually traced back to the ICM principle, i.e. that interventions on one mechanism will not affect other mechanisms [5]. But we argue that it is also due to the limited interventional range of agents (or, more generally, physical processes), which restricts the breadth and frequency of mechanism-changes in the real world. Previous work has used sparseness to learn disentangled representations [6, 7], causal models [8], or modular architectures [9]. In the present work, we show that taking the localized and sparse nature of influence into account can also strongly improve RL algorithms.</p>
<p>Detecting causal influence, informally, means deciding whether changing a causal variable would have an impact on another variable. This involves causal discovery, that is, finding the existence of arrows in the causal graph [10]. While the task of causal discovery is unidentifiable in general [11], there are assumptions which permit discovery [12], in particular in the time series setting we are concerned with [13]. Even if the existence of an arrow is established, the problem remains of quantifying its causal impact, for which various measures such as transfer entropy or information flow have been proposed [14-18]. We compare how our work relates to these measures in Sec. 4.1.</p>
<p>The intersection of RL and causality has been the subject of recent research [19-23]. Close to ours is the work of Pitis et al. [24], who also use influence detection, albeit to create counterfactual data that augments the training of RL agents. In Sec. 5, we find that our approach to action influence detection performs better than their heuristic approach. Additionally, we demonstrate that influence detection can also be used to help agents explore better. To this end, we use influence as a type of intrinsic motivation. For exploration, various signals have been proposed, e.g. model surprise [2527], learning progress [27, 28], empowerment [29, 30], information gain [31-33], or predictive information [34, 35]. Inspired by causality, Sontakke et al. [36] introduce an exploration signal that leads agents to experiment with the environment to discover causal factors of variation. In concurrent work, Zhao et al. [37] propose to use mutual information between the agent and the environment state for exploration. As in our work, the agent is considered a separate entity from the environment. However, their approach does not discriminate between individual situations the agent is in. Causal influence is also related to the concept of contingency awareness from psychology [38], that is, the knowledge that one's actions can affect the environment. On Atari games, exploring through the lens of contingency awareness has led to state-of-the-art results [39, 40].</p>
<h2>3 Background</h2>
<p>We are concerned with a Markov decision process $\langle\mathcal{S}, \mathcal{A}, P, r, \gamma\rangle$ consisting of state and action space, transition distribution, reward function and discount factor. ${ }^{1}$ Most real world environments consist of entities that behave mostly independently of each other. We model this by assuming a known state space factorization $\mathcal{S}=\mathcal{S}<em N="N">{1} \times \ldots \times \mathcal{S}</em>$ corresponds to the state of an entity.}$, where each $\mathcal{S}_{i</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Causal graphical model capturing the environment transition from state $S$ to $S^{\prime}$ by action $A$, factorized into state components. (a): Viewed globally over all time steps, all components of the state and the action can influence all state components at the next time step. (b, c): Given a situation $S=s$, some influences may or may not not hold in the local causal graph $\mathcal{G}_{S=s}$. In this paper, our aim is to detect which influence the action has on $S^{\prime}$, i.e. the presence of the red arrows.</p>
<h1>3.1 Causal Graphical Models</h1>
<p>We can model the one-step transition dynamics at time step $t$ using a causal graphical model (CGM) [3, 10] over the set of random variables $\mathcal{V}=\left{S_{1}, \ldots, S_{N}, A, S_{1}^{\prime}, \ldots, S_{N}^{\prime}\right}$, consisting of a directed graph $\mathcal{G}$ (see Fig. 1a) and a conditional distribution $P\left(V_{i} \mid \operatorname{Pa}\left(V_{i}\right)\right)$ for each node $V_{i} \in \mathcal{V}$, where $\operatorname{Pa}\left(V_{i}\right)$ is the set of parents of $V_{i}$ in the causal graph. We assume that the joint distribution $P_{\mathcal{V}}$ is Markovian with respect to the graph [3, Def. 6.21 (iii)], that is, its density exists and factorizes as</p>
<p>$$
p\left(v_{1}, \ldots, v_{|\mathcal{V}|}\right)=\prod_{i=1}^{|\mathcal{V}|} p\left(v_{i} \mid \operatorname{Pa}\left(V_{i}\right)\right)
$$</p>
<p>In a CGM, we can model a (stochastic) intervention $\operatorname{do}\left(V_{i}:=q\left(v_{i} \mid \operatorname{Pa}\left(V_{i}\right)\right)\right)$ on variable $V_{i}$ by replacing its conditional $p\left(v_{i} \mid \operatorname{Pa}\left(V_{i}\right)\right)$ in Eq. 1 with the distribution $q\left(v_{i} \mid \operatorname{Pa}\left(V_{i}\right)\right)$ [3]. Here, $V_{i}$ could e.g. be a state component $S_{i}$, or the agent's action $A$. Thus, whereas a probabilistic graphical model represents a single distribution, a CGM represents a set of distributions [4].
The causal graph that we assume is shown in Fig. 1a. Within a time step, there are no edges, i.e. no instantaneous effects, except for the action which is computed by the policy $\pi(A \mid S)$. Between time steps, the graph is fully connected. The reason is that whenever an interaction between two components $S_{i}$ and $S_{j}$, however unlikely, is possible, it is necessary to include an arrow $S_{i} \rightarrow S_{j}^{\prime}$ (and vice versa). Nevertheless, during most concrete time steps, there should be no interaction between entities, reflecting the assumption that the state components represent independent entities in the world. In particular, the agent's "sphere of influence" (depicted in blue in Figs. 1b and 1c) is limited its action $A$ can only sparsely affect other entities. Thus, in this paper, we are interested in inferring the influence the action has in a specific state configuration $S=s$, that is, the local causal model in $s$.
Definition 1. Given a CGM with distribution $P_{\mathcal{V}}$ and graph $\mathcal{G}$, we define the local $C G M$ induced by observing $X=x$ with $X \subset \mathcal{V}$ to be the CGM with joint distribution $P_{\mathcal{V} \mid X=x}$ and the graph $\mathcal{G}<em _mathcal_V="\mathcal{V">{X=x}$ resulting from removing edges from $\mathcal{G}$ until $P</em>$ is causally minimal with respect to the graph.} \mid X=x</p>
<p>Causal minimality tells us that each edge $X \rightarrow Y$ in the graph must be "active", in the sense that $Y$ is conditionally dependent on $X$ given all other parents of $Y$ [3, Prop. 6.36].</p>
<h3>3.2 The Cause of an Effect</h3>
<p>When is an agent's action $A=a$ the cause of an outcome $B=b$ ? Answering this question precisely is surprisingly non-trivial and is studied under the name of actual causation [10, 41]. Humans would answer by contrasting the actual outcome to some normative world in which $A=a$ did not happen, i.e. they would ask the counterfactual question "What would have happened normally to $B$ without $A=a$ ?" [41]. Algorithmitizing this approach poses certain problems. First, it requires a "normal" outcome which can be difficult to compute as it depends on the behavior of the different actors in</p>
<p>the world. Second, it requires to actually observe the world's state without the agents interference. Such a "no influence" action may not be available for every agent. Instead, we are inspired by an alternative approach, the so-called "but-for" test: " $B=b$ would not have happened but for $A=a$." In other words, $A=a$ was a necessary condition for $B=b$ to occur, and under a different value for $A, B$ would have had a different value as well. This matches well with an algorithmic view on causation: $A$ is a cause of $B$ if the value of $A$ is required to determine the value of $B$ [42].
The but-for test yields potentially counterintuitive assessments. Consider a robotic arm close to an object but performing an action that moves it away from the object. Then this action is considered a cause for the position of the object in this step, as an alternative action touching the object would have led to a different outcome. Algorithmically, knowing the action is required to determine what happens to the object - all actions are considered to be a cause in this situation. Importantly, this implies that we cannot differentiate whether individual actions are causes or not, but can only identify whether or not the agent has causal influence on other entities in the current state.</p>
<h1>4 Causal Influence Detection</h1>
<p>As the previous discussion showed, having causal influence is dependent on the situation the agent is in, rather than the chosen actions. We characterize this as the agent being in control, analogous to similar notions in control theory [43]. Formally, using the causal model introduced in Sec. 3, we define the agent to be in control of $S_{j}^{\prime}$ in state $S=s$ if there is an edge $A \rightarrow S_{j}^{\prime}$ in the local causal graph $\mathcal{G}<em S="s">{S=s}$ under all interventions do $(A:=\pi(a \mid s))$ with $\pi$ having full support. The following proposition states when such an edge exists (proofs in Suppl. A.1).
Proposition 1. Let $\mathcal{G}</em>}$ be the graph of the local CGM induced by $S=s$. There is an edge $A \rightarrow S_{j}^{\prime}$ in $\mathcal{G<em j="j">{S=s}$ under the intervention do $(A:=\pi(a \mid s))$ if and only if $S</em> \not \Perp A \mid S=s$.}^{\prime</p>
<p>To detect when the agent is in control, we can intervene with a policy. The following proposition gives conditions under which conclusions drawn from one policy generalize to many policies.
Proposition 2. If there is an intervention do $(A:=\pi(a \mid s))$ under which $S_{j}^{\prime} \not \Perp A \mid S=s$, this dependence holds under all interventions with full support, and the agent is in control of $S_{j}^{\prime}$ in $s$. If there is an intervention do $(A:=\pi(a \mid s))$ with $\pi$ having full support under which $S_{j}^{\prime} \Perp A \mid S=s$, this independence holds under all possible interventions and the agent is not in control of $S_{j}^{\prime}$ in $s$.</p>
<h3>4.1 Measuring Causal Action Influence</h3>
<p>Our goal is to find a state-dependent quantity that measures whether the agent is in control of $S_{j}^{\prime}$. As Prop. 1 tells us, control (or its absence) is linked to the independence $S_{j}^{\prime} \Perp A \mid S=s$. A well-known measure of dependence is the conditional mutual information (CMI) [44] which is zero for independence. We thus propose to use (pointwise) CMI as a measure of causal action influence (CAI) that can be thresholded to get a classification of control (see Suppl. A. 2 for a derivation):</p>
<p>$$
C^{j}(s):=I\left(S_{j}^{\prime} ; A \mid S=s\right)=\mathbb{E}<em _mathrm_RL="\mathrm{RL">{a \sim \pi}\left[\mathrm{D}</em>\right)\right]
$$}}\left(P_{S_{j}^{\prime} \mid s, a} | P_{S_{j}^{\prime} \mid s</p>
<p>We want this measure to be independent of the particular policy used in the joint distribution $P\left(S, A, S^{\prime}\right)$. This is because we might not be able to sample from or evaluate this policy (e.g. in off-policy RL, the data stems from a mixture of different policies). Fortunately, Prop. 2 shows that to detect control, it is sufficient to demonstrate (in-)dependence for a single policy with full support. Thus, we can choose a uniform distribution over the action space as the policy: $\pi(A):=\mathcal{U}(\mathcal{A})$.
Let us discuss how CAI relates to previously suggested measures of (causal) influence. Transfer entropy [14] is a non-linear extension of Granger causality [45] quantifying causal influence in time series under certain conditions. CAI is similar to a one-step, local transfer entropy [17] with the difference that CAI conditions on the full state $S$. Janzing et al. [18] put forward a measure of causal strength fulfilling several natural criteria that other measures, including transfer entropy, fail to satisfy. In Suppl. A.3, we show that CAI is a pointwise version of Janzing et al.'s causal strength, for policies not conditional on the state $S$ (adding further justification for the choice of a uniform random policy). Furthermore, we can relate CAI to notions of controllability [43]. Decomposing $C^{j}(s)$ as $H\left(S_{j}^{\prime} \mid s\right)-H\left(S_{j}^{\prime} \mid A, s\right)$, where $H$ denotes the conditional entropy [44], we can interpret CAI</p>
<p>as quantifiying the degree to which $S_{j}^{\prime}$ can be controlled in $s$, accounting for the system’s intrinsic uncertainty that cannot be reduced by the action.</p>
<p>In the context of RL, empowerment [29, 30, 46] is a well-known quantity used for intrinsicallymotivated exploration that leads agents to states of maximal influence over the environment. Empowerment, for a state $s$, is defined as the channel capacity between action and a future state, which coincides with $\max_{\pi} C(s)$ for one-step empowerment. CAI can thus be seen as a non-trivial lower bound of empowerment that is easier to compute. However, CAI differs from empowerment in that it does not treat the state space as monolithic and is specific to an entity. In Sec. 6.1, we demonstrate that an RL agent maximizing CAI quickly achieves control over its environment.</p>
<h1>4.2 Learning to Detect Control</h1>
<p>Estimating CMI is a hard problem on many levels: it involves computing high dimensional integrals, representing complicated distributions and having access to limited data; strictly speaking, each conditioning point $s$ is seen only once in continuous spaces. In practice, one thus has to resort to an approximation. Non-parametric estimators based on nearest neighbors [47, 48] or kernels methods [49] are known to not scale well to higher dimensions [50]. Instead, we approach the problem by learning neural network models with suitable simplifying assumptions.</p>
<p>Expanding the KL divergence in Eq. 2, we can write CAI as</p>
<p>$$
C^{j}(s)=I\left(S_{j}^{\prime} ; A \mid S=s\right)=\mathbb{E}<em S__j="S_{j">{A \mid s} \mathbb{E}</em>\right]
$$}^{\prime} \mid s, a}\left[\log \frac{p\left(s_{j}^{\prime} \mid s, a\right)}{\int p\left(s_{j}^{\prime} \mid s, a\right) \pi(a) \mathrm{d} a</p>
<p>To compute this term, we estimate the transition distribution $p\left(s_{j}^{\prime} \mid s, a\right)$ from data. We then approximate the outer expectation and the transition marginal $p\left(s_{j}^{\prime} \mid s\right)$ by sampling $K$ actions from the policy $\pi$. This gives us the estimator</p>
<p>$$
\hat{C}^{j}(s)=\frac{1}{K} \sum_{i=1}^{K}\left[\mathrm{D}<em j="j">{\mathrm{KL}}\left(\left.p\left(s</em>\right)\right)\right]
$$}^{\prime} \mid s, a^{(i)}\right) | \frac{1}{K} \sum_{k=1}^{K} p\left(s_{j}^{\prime} \mid s, a^{(k)</p>
<p>with $\left{a^{(1)}, \ldots, a^{(K)}\right} \stackrel{\text { iid }}{\sim} \pi$. Here, we replaced the infinite mixture $p\left(s_{j}^{\prime} \mid s\right)$ with a finite mixture, $p\left(s_{j}^{\prime} \mid s\right) \approx \frac{1}{K} \sum_{i=1}^{K} p\left(s_{j}^{\prime} \mid s, a^{(i)}\right)$, and used Monte-Carlo to approximate the expectation. Poole et al. [51] show that this estimator is a lower bound converging to the true mutual information $I\left(S_{j}^{\prime} ; A \mid S=s\right)$ as $K$ increases (assuming, however, the true density $p\left(s_{j}^{\prime} \mid s, a\right)$ ).
To compute the KL divergence itself, we make the simplifying assumption that the transition distribution $p\left(s_{j}^{\prime} \mid s, a\right)$ is normally distributed given the action, which is reasonable in the robotics environment we are targeting. This allows us to estimate the KL without expensive MC sampling by using an approximation for mixtures of Gaussians from Durrieu et al. [52]. We detail the exact formula we use in Suppl. A.4.
With the normality assumption, the density itself can be learned using a probabilistic neural network and simple maximum likelihood estimation. That is, we parametrize $p\left(s_{j}^{\prime} \mid s, a\right)$ as $\mathcal{N}\left(s_{j}^{\prime} ; \mu_{\theta}(s, a), \sigma_{\theta}^{2}(s, a)\right)$, where $\mu_{\theta}, \sigma_{\theta}^{2}$ are the outputs of a neural network $f_{\theta}(s, a)$. We find the parameters $\theta$ by minimizing the negative log-likelihood over samples $\mathcal{D}=\left{\left(s^{(i)}, a^{(i)}, s^{\prime(i)}\right)\right}_{i=1}^{N}$ collected by some policy (the univariate case shown here also extends to the multivariate case):</p>
<p>$$
\theta^{*}=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{i=1}^{N} \frac{\left(s_{j}^{\prime(i)}-\mu_{\theta}\left(s^{(i)}, a^{(i)}\right)\right)^{2}}{2 \sigma_{\theta}^{2}\left(s^{(i)}, a^{(i)}\right)}+\frac{1}{2} \log \sigma_{\theta}^{2}\left(s^{(i)}, a^{(i)}\right)
$$</p>
<p>There are some intricacies regarding the policy that collects the data for model training and the sampling policy $\pi$ that is used to compute CAI. First of all, the two policies need to have overlapping support to avoid evaluating the model under actions never seen during training. Furthermore, if the data policy is different from the sampling policy $\pi$, the model is biased to some degree. This suggests to use $\pi$ for collecting the data; however, as we use a random policy, this will not result in interesting data in most environments. The bias can be reduced by sampling actions from $\pi$ during data collection with some probability and only train on those. In practice, however, we find to obtain better performing models by training on all data despite potentially being biased.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Causal influence detection performance. (a, b) ROC curves on IDSLIDE and FETCHPICKANDPLACE environments. (c) Average precision for FETCHPICKANDPLACE depending on added state noise. Noise level is given as percentage of one standard deviation over the dataset.</p>
<p>Table 1: Results for evaluating causal influence detection on different environments. We measure area under the ROC curve (AUC), average precision (AP), and the best achievable F-score (F1).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">IDSLIDE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FETCHPICKANDPLACE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUC</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">$\mathrm{F}_{1}$</td>
<td style="text-align: center;">AUC</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">$\mathrm{F}_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">CAI (ours)</td>
<td style="text-align: center;">$1.00 \pm 0.00$</td>
<td style="text-align: center;">$0.98 \pm 0.00$</td>
<td style="text-align: center;">$0.95 \pm 0.01$</td>
<td style="text-align: center;">$0.97 \pm 0.01$</td>
<td style="text-align: center;">$0.96 \pm 0.00$</td>
<td style="text-align: center;">$0.89 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">$0.96 \pm 0.00$</td>
<td style="text-align: center;">$0.47 \pm 0.01$</td>
<td style="text-align: center;">$0.50 \pm 0.01$</td>
<td style="text-align: center;">$0.84 \pm 0.00$</td>
<td style="text-align: center;">$0.73 \pm 0.00$</td>
<td style="text-align: center;">$0.78 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">Attention [24]</td>
<td style="text-align: center;">$0.42 \pm 0.31$</td>
<td style="text-align: center;">$0.13 \pm 0.14$</td>
<td style="text-align: center;">$0.18 \pm 0.17$</td>
<td style="text-align: center;">$0.46 \pm 0.06$</td>
<td style="text-align: center;">$0.44 \pm 0.04$</td>
<td style="text-align: center;">$0.62 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">Contacts</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.73</td>
</tr>
</tbody>
</table>
<h1>5 Empirical Evaluation of Causal Influence Detection</h1>
<p>In this section, we evaluate the quality of our proposed causal influence detection approach in relevant environments. As a simple test case, we designed an environment (IDSLIDE) in which the agent must slide an object to a goal location by colliding with it. Furthermore, we test on the FETCHPICKANDPLACE environment from OpenAI Gym [53], in its original setting and when adding Gaussian noise to the observations to simulate more real-world conditions. In both environments, the target variables of interest are the coordinates of the object. Note that we need the true causal graph at each time step for the evaluation. For IDSLIDE, we derive this information from the simulation. For the pick and place environment with its non-trivial dynamics, we resort to a heuristic of the possible movement range of the robotic arm in one step. Detailed information about the setup is provided in Suppls. B and E.</p>
<p>For our method, we use CAI estimated according to Eq. 4 (with $K=64$ ) as a classification score that is thresholded to gain a binary decision. We compare with a recently proposed method [24] that uses the attention weights of a Transformer model [54] to model influence. Moreover, we compare with an Entropy baseline that uses $H\left(S_{j}^{\prime} \mid s\right)$ as a score and a Contact baseline based on binary contact information from the simulator. We show the test results over 5 random seeds in Table 1 and Fig. 2. We observe that CAI is able to reliably detect causal influence and no other baseline is able to do so. When increasing the observation noise, the performance drops gracefully for CAI as shown in Fig. 2c. Suppl. C contains more experimental results, including a visualization of CAI's behavior.</p>
<h2>6 Improving Efficiency in Reinforcement Learning</h2>
<p>Having established the efficacy of our causal action influence (CAI) measure, we now develop several approaches to use it to improve RL algorithms. We will empirically verify the following claims in robotic manipulation environments: CAI improves sample efficiency and performance by (i) better state exploration through an exploration bonus, (ii) causal action exploration, and (iii) prioritizing experiences with causal influence during training.</p>
<p>We consider the environments FETCHPUSH, FETCHPICKANDPLACE from OpenAI Gym [55], and FETCHRotTABLE which is our modification containing a rotating table (explained in Suppl. B.3).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Intrinsically motivated learning on FETCHPICKANDPLACE. The reward is only $r_{\text {CAI }}$ measured on the object coordinates.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Exploration bonus improves performance in FETCHPICKANDPLACE. Sensitivity to the bonus reward scale $\lambda_{\text {bonus }}$.</p>
<p>These environments are goal-conditioned RL tasks with sparse rewards, meaning that each episode, a new goal is provided and the agent only receives a distinct reward upon reaching it. We use DDPG [56] with hindsight experience replay (HER) [57] as the base RL algorithm, a combination that achieves state-of-the-art results in these environment. The influence detection model is trained online on the data collected from an RL agent learning to solve its task. Since our measure $C^{j}$ requires an entity of interest, we choose the coordinates of the object (as $S_{j}$ ). In all experiments, we report the mean success rate with standard deviation over 10 random seeds. More information about the experimental settings can be found in Suppl. F.</p>
<h1>6.1 Intrinsic Motivation to Seek Influence</h1>
<p>Causal Action Influence as Reward Bonus. We hypothesize that it is useful for an agent to be intrinsically motivated to gain control over its environment. We test this hypothesis by letting the agent maximize the causal influence it has over entities of interest. This can be achieved by using our influence measure as a reward signal. The reward signal can be used on its own, as an intrinsic motivation-type objective, or in conjunction with a task-specific reward as an exploration bonus. In the former case, we expect the agent to discover useful behaviors that can help it master task-oriented skills afterwards; in the latter case, we expect learning efficiency to improve, especially in sparse extrinsic reward scenarios. Concretely, for a state $s$, we define the bonus as $r_{\text {CAI }}(s)=C^{j}(s)$, and the total reward as $r(s)=r_{\text {task }}(s)+\lambda_{\text {bonus }} r_{\text {CAI }}(s)$, where $r_{\text {task }}(s)$ is the task reward, and $\lambda_{\text {bonus }}$ is a hyperparameter.
Experiment on Intrinsically Motivated Learning. We first test the behavior of the agent in the absence of any task-specific reward on the FETCHPICKANDPLACE environment. Interestingly, the agent learned to grasp, lift, and hold the object in the air already after 2000 episodes, as shown in Fig. 3. The results demonstrate that encouraging causal control over the environment is well suited to prepare the agent for further tasks it might have to solve.
Impact of CAI Reward Bonus. Second, we are interested in the impact of adding an exploration bonus. In Fig. 4, we present results on the FETCHPICKANDPLACE environment when varying the reward scale $\lambda_{\text {bonus }}$. Naturally, the exploration bonus needs to be selected in the appropriate scale as a value too high will make it dominate the task reward. If selected correctly, the sample efficiency is improved drastically; for example, we find that the agent reaches a success rate of $60 \%$ four-times faster than the baseline (DDPG+HER) without any bonus ( $\lambda_{\text {bonus }}=0$ ).</p>
<h3>6.2 Actively Exploring Actions with Causal Influence</h3>
<p>Following Actions with the Most Causal Influence. Exploration via bonus rewards favors the re-visitation of already seen states. An alternative approach to exploration uses pro-active planning to choose exploratory actions. In our case, we can make use of our learned influence estimator to pick actions which we expect will have the largest causal effect on the agent's surroundings. From a causal viewpoint, the resulting agent can be seen as an experimenter that performs planned interventions in the environment to verify its beliefs. Should the actual outcome differ from the expected outcome, subsequent model updates can integrate the new data to self-correct the causal influence estimator.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of active exploration in FetchPickAndPlace depending on the fraction of exploratory actions chosen actively (Eq. 6) from a total of 30\% exploratory actions.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Experiment comparing exploration strategies on FetchPickAndPlace. The combination of active exploration and reward bonus yields the largest sample efficiency.</p>
<p>Concretely, given the agent being in state $s$, we choose the action that has the largest contribution to the empirical mean in Eq. 4:</p>
<p>$$
a^{*}=\underset{a \in\left{a^{(1)}, \ldots, a^{(K)}\right}}{\arg \max } \mathrm{D}<em j="j">{\mathrm{KL}}\left(p\left(s</em>\right)\right)
$$}^{\prime} \mid s, a\right) | \frac{1}{K} \sum_{k=1}^{K} p\left(s_{j}^{\prime} \mid s, a^{(k)</p>
<p>with $\left{a^{(1)}, \ldots, a^{(K)}\right} \stackrel{\text { iid }}{\sim} \pi$. Intuitively, the selected action will be the one which results in maximal deviation from the expected outcome under all actions. For states $s$ where the the agent is not in control, i.e. $C^{j}(s) \approx 0$, the action selection is uniform at random.
Active Exploration in Practice. Can active exploration replace $\epsilon$-greedy exploration? To gain insights, we study the impact of the fraction of actively chosen exploration actions. For every exploratory action ( $\epsilon$ is $30 \%$ in our experiments), we choose an action according to Eq. 6 the specified fraction of the time, and otherwise a random action. Figure 5 shows that any amount of active exploration improves over simple random exploration. Active causal action exploration can improve the sample efficiency roughly by a factor of two.
Combined CAI Exploration. We also present the combination of reward bonus and active exploration and compare our method with VIME, another exploration scheme based on informationtheoretic measures [33]. In contrast to our method, VIME maximizes the information gain about the state transition dynamics. Further, we compare to ensemble disagreement [58], which in effect minimizes epistemic uncertainty about the transition dynamics. We compare different variants of VIME and ensemble disagreement in Suppl. D, and display only their best versions here. Figure 6 quantifies the superiority of all CAI variants (with ensemble disagreement as a viable alternative) and shows that combining the two exploration strategies compounds to increase sample efficiency even further. In the figure, CAI uses $100 \%$ active exploration and $\lambda_{\text {bonus }}=0.2$ as the bonus reward scale.</p>
<h1>6.3 Causal Influence-based Experience Replay</h1>
<p>Prioritizing According to Causal Influence. We will now propose another method using CAI, namely to inform the choice of samples replayed to the agent during off-policy training. Typically, past states are sampled uniformly for learning. Intuitively, focusing on those states where the agent has control over the object of interest (as measured by CAI) should improve the sample efficiency. We can implement this idea using a prioritization scheme that samples past episodes in which the agent had more influence more frequently. Concretely, we define the probability $P^{(i)}$ of sampling any state from episode $i$ (of $M$ episodes) in the replay buffer as</p>
<p>$$
P^{(i)}=\frac{p^{(i)}}{\sum_{i=1}^{M} p^{(i)}} \cdot \frac{1}{T}, \quad \text { with } \quad p^{(i)}=\left(M+1-\operatorname{rank}<em t="1">{i} \sum</em>
$$}^{T} C^{j}\left(s^{(t)}\right)\right)^{-1</p>
<p>where $T$ is the episode length, and $p^{(i)}$ is the priority of episode $i$. The priority of an episode $i$ is based on the (inverse) rank of the episode $\left(\operatorname{rank}_{i}\right)$ when sorting all $M$ episodes according to their total influence (i.e. sum of state influences). We call this causal action influence prioritization (CAI-P).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Prioritizing experience replay in different manipulation environments. Comparison of causal action influence prioritization (CAI-P) against baselines: the energy-based method (EBP) [60] with privileged information, prioritized experience replay (PER) [59], and HER without prioritization.</p>
<p>This scheme is similar to Prioritized Experience Replay [59], with two differences: instead of using the TD error for prioritization, we use the causal influence measure. Furthermore, instead of prioritizing individual states, we prioritize episodes and sample states uniformly within episodes. This is because the information about the return that can be achieved from an influence state still needs to be propagated back to non-influenced states by TD updates, which requires sampling them.</p>
<p>Influence-Based Prioritization in Manipulation Tasks. We compare our influence-based prioritization (CAI-P) against no prioritization in hindsight experience replay (HER) (a strong baseline for multi-goal RL), and two other prioritization schemes: prioritized experience replay (PER) [59] and energybased prioritization (EBP) [60]. Especially EBP is a strong method for the environments we are considering as it uses privileged knowledge of the underlying physics to replay episodes based on the amount of energy that is transferred from agent to the object to manipulate. All prioritization variants are equipped with HER as well. The FETCHROTTAble environment, shown in Fig. 8, is an interesting test bed as the object can move through the table rotation without the control of the agent. The results, shown in Fig. 7, reveal that causal influence prioritization can speed up learning drastically. Our method is on par or better than the energy-based (oracle) method EBP and improves over PER by a factor of 1.5–2.5 in learning speed (at 60% success rate). Finally, in Suppl. D, we combine all our proposed improvements and show that FETCHPICKANDPLACE can be solved up to 95% success rate in just 3000 episodes.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: FETCH ROTTABLE. The table rotates periodically.</p>
<h2>7 Discussion</h2>
<p>In this work, we show how situation-dependent causal influence detection can help improve reinforcement learning agents. To this end, we derive a measure of local causal action influence (CAI) and introduce a data-driven approach based on neural network models to estimate it. We showcase using CAI as an exploration bonus, as a way to perform active action exploration, and to prioritize in experience replay. Each of our applications yields strong improvements in sample efficiency. We expect that there are further ways to use our causal measure in RL, e.g., for credit assignment.</p>
<p>Our work has several limitations. First, we assume full observability of the state, which simplifies the causal inference problem as there is no confounding between an agent's action and its effect. Under partial observability, our approach could still be applicable using latent variable models [61]. Second, we require an available factorization of the state into causal variables. The problem of automatically learning causal variables from high-dimensional data is open [4] and our method would likely benefit from advances in this field. Third, the accurate estimation of our measure relies on a correct model. We found that deep networks can struggle at times to pick up the causal relationship between actions and entities. How to design models with appropriate inductive biases for cause-effect inference is an open question [3, 4, 62].</p>
<p>An intriguing future direction is to extend our work to influence detection between entities, a prerequisite for identifying multi-step influences of the agent on the environment. Being able to model such indirect interventions would bring us closer to "artificial scientists" – agents that can perform planned experiments to reveal the latent causal structure of the world.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>The authors thank Andrii Zadaianchuk and Dominik Zietlow for many helpful discussions and providing feedback on the text. Furthermore, the authors would also like to thank Sebastian Blaes for creating the FETCHROTTAble environment. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Maximilian Seitzer. GM and BS are members of the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645. We acknowledge the financial support from the German Federal Ministry of Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039B).</p>
<h2>References</h2>
<p>[1] Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.
[2] Dmitry Kalashnikov, Alex Irpan, Peter Pastor Sampedro, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning 2018, 2018. URL https://arxiv.org/pdf/1806.10293.
[3] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of Causal Inference: Foundations and Learning Algorithms. MIT Press, Cambridge, MA, USA, 2017.
[4] B. Schölkopf, F. Locatello, S. Bauer, R. Nan Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Towards causal representation learning. Proceedings of the IEEE, 2021. doi: 10.1109/JPROC. 2021.3058954.
[5] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Schölkopf. Learning independent causal mechanisms. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4036-4044. PMLR, 10-15 Jul 2018. URL http://proceedings.mlr.press/v80/parascandolo18a.html.
[6] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to dis entangle causal mechanisms. In 8th International Conference on Learning Representations (ICLR), 2020. URL https://openreview.net/forum?id=ryxWIgBFPS.
[7] Francesco Locatello, Ben Poole, Gunnar Raetsch, Bernhard Schölkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6348-6359. PMLR, 13-18 Jul 2020. URL http://proceedings.mlr.press/v119/locatello20a.html.
[8] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Schölkopf, Michael C. Mozer, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. ArXiv, abs/1910.01075, 2019.
[9] A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y. Bengio, and B. Schölkopf. Recurrent independent mechanisms. In 9th International Conference on Learning Representations (ICLR), 2021.
[10] Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009.
[11] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT Press, Cambridge, MA, 2nd edition, 2000.
[12] J. Peters, J. Mooij, D. Janzing, and B. Schölkopf. Identifiability of causal graphs using functional models. In F. G. Cozman and A. Pfeffer, editors, 27th Conference on Uncertainty in Artificial Intelligence, pages 589-598, Corvallis, OR, 2011. AUAI Press.</p>
<p>[13] Michael Eichler. Causal Inference in Time Series Analysis. Wiley, 2012. ISBN 9781119945710. doi: $10.1002 / 9781119945710 . \mathrm{ch} 22$.
[14] Thomas Schreiber. Measuring information transfer. Physical Review Letters, 85 2:461-4, 2000.
[15] Nihat Ay and David C. Krakauer. Geometric robustness theory and biological networks. Theory in Biosciences, 125(2):93-121, 2007. ISSN 1431-7613. doi: https://doi.org/10.1016/ j.thbio.2006.06.002. URL https://www.sciencedirect.com/science/article/pii/ S1431761306000255.
[16] N. Ay and D. Polani. Information flows in causal networks. Adv. Complex Syst., 11:17-41, 2008.
[17] Joseph T. Lizier. The local information dynamics of distributed computation in complex systems. PhD thesis, University of Sydney, 2013. URL http://d-nb.info/1024629171.
[18] D. Janzing, D. Balduzzi, M. Grosse-Wentrup, and B. Schölkopf. Quantifying causal influences. Annals of Statistics, 41(5):2324-2358, 2013. URL http://projecteuclid.org/euclid. aos/1383661266.
[19] E. Bareinboim, A. Forney, and J. Pearl. Bandits with unobserved confounders: A causal approach. In Advances in Neural Information Processing, volume 28. Curran Associates, Inc., 2015.
[20] Chaochao Lu, B. Schölkopf, and José Miguel Hernández-Lobato. Deconfounding reinforcement learning in observational settings. ArXiv, abs/1812.10576, 2018.
[21] Danilo Jimenez Rezende, Ivo Danihelka, George Papamakarios, N. Ke, Ray Jiang, T. Weber, K. Gregor, Hamza Merzic, Fabio Viola, J. Wang, Jovana Mitrovic, F. Besse, Ioannis Antonoglou, and Lars Buesing. Causally Correct Partial Models for Reinforcement Learning. ArXiv, abs/2002.02836, 2020.
[22] Lars Buesing, T. Weber, Yori Zwols, Sébastien Racanière, A. Guez, J. Lespiau, and N. Heess. Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search. In 7th International Conference on Learning Representations (ICLR), 2019.
[23] J. Zhang and E. Bareinboim. Near-optimal reinforcement learning in dynamic treatment regimes. In Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2019.
[24] Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally factored dynamics. In Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.
[25] J. Schmidhuber. Curious model-building control systems. In Proceedings IEEE International Joint Conference on Neural Networks, pages 1458-1463 vol.2, 1991. doi: 10.1109/IJCNN. 1991. 170605 .
[26] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 488-489, 2017.
[27] Sebastian Blaes, Marin Vlastelica, Jia-Jie Zhu, and Georg Martius. Control What You Can: Intrinsically motivated task-planning agent. In Advances in Neural Information Processing, pages 12520-12531. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/ 9418-control-what-you-can-intrinsically-motivated-task-planning-agent. pdf.
[28] Cédric Colas, Pierre-Yves Oudeyer, Olivier Sigaud, Pierre Fournier, and Mohamed Chetouani. CURIOUS: intrinsically motivated modular multi-goal reinforcement learning. In International Conference on Machine Learning (ICML'19), pages 1331-1340, 2019.
[29] A.S. Klyubin, D. Polani, and C.L. Nehaniv. Empowerment: a universal agent-centric measure of control. In 2005 IEEE Congress on Evolutionary Computation, volume 1, pages 128-135 Vol.1, 2005. doi: 10.1109/CEC.2005.1554676.</p>
<p>[30] Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/ file/e00406144c1e7e35240afed70f34166a-Paper.pdf.
[31] Jan Storck, Sepp Hochreiter, and Jürgen Schmidhuber. Reinforcement driven information acquisition in non-deterministic environments. In ICANN'95, pages 159-164, 1995.
[32] Daniel Little and Friedrich Sommer. Learning and exploration in action-perception loops. Frontiers in Neural Circuits, 7:37, 2013. ISSN 1662-5110. doi: 10.3389/fncir.2013.00037. URL https://www.frontiersin.org/article/10.3389/fncir.2013.00037.
[33] Rein Houthooft, Xi Chen, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips. cc/paper/2016/file/abd815286ba1007abfbb8415b83ae2cf-Paper.pdf.
[34] Georg Martius, Ralf Der, and Nihat Ay. Information driven self-organization of complex robotic behaviors. PLoS ONE, 8(5):e63400, 2013. doi: 10.1371/journal.pone. 0063400 . URL http://dx.doi.org/10.1371/journal.pone. 0063400 .
[35] Keyan Zahedi, Georg Martius, and Nihat Ay. Linear combination of one-step predictive information with an external reward in an episodic policy gradient setting: a critical analysis. Frontiers in Psychology, 4(801), 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00801. URL http:// www.frontiersin.org/cognitive_science/10.3389/fpsyg.2013.00801/abstract.
[36] S. Sontakke, A. Mehrjou, L. Itti, and B. Schölkopf. Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning. In Proceedings of 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9848-9858. PMLR, July 2021. URL https://proceedings.mlr.press/ v139/sontakke21a.html.
[37] Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state intrinsic control. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0thEq8I5v1.
[38] John S. Watson. The development and generalization of contingency awareness in early infancy: Some hypotheses. Merrill-Palmer Quarterly of Behavior and Development, 12(2):123-135, 1966. ISSN 00260150. URL http://www.jstor.org/stable/23082793.
[39] Jongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu, Mohammad Norouzi, and Honglak Lee. Contingency-aware exploration in reinforcement learning. In International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/ forum?id=HyxGB2AcY7.
[40] Yuhang Song, Jianyi Wang, Thomas Lukasiewicz, Zhenghua Xu, Shangtong Zhang, Andrzej Wojcicki, and Mai Xu. Mega-Reward: Achieving human-level play without extrinsic rewards. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):5826-5833, Apr. 2020. doi: 10.1609/aaai.v34i04.6040. URL https://ojs.aaai.org/index.php/AAAI/ article/view/6040.
[41] Joseph Y. Halpern. Actual Causality. The MIT Press, 2016. ISBN 9780262035026.
[42] J. Pearl, M. Glymour, and N. Jewell. Causal Inference in Statistics: A Primer. Wiley, 2016. ISBN 978-1-119-18684-7.
[43] Hugo Touchette and Seth Lloyd. Information-theoretic approach to the study of control systems. Physica A: Statistical Mechanics and its Applications, 331(1):140-172, 2004. ISSN 0378-4371. doi: https://doi.org/10.1016/j.physa.2003.09.007. URL https://www.sciencedirect.com/ science/article/pii/S0378437103008100.</p>
<p>[44] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley Series in Telecommunications and Signal Processing. Wiley-Interscience, 2nd ed edition, 2006.
[45] C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica, 37(3):424-438, 1969. URL http://www.jstor.org/stable/ 1912791 .
[46] Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowerment-An Introduction, pages 67-114. Springer Berlin Heidelberg, Berlin, Heidelberg, 2014. ISBN 978-3-642-53734-9. doi: 10.1007/978-3-642-53734-9_4. URL https://doi.org/10.1007/978-3-642-53734-9_ 4 .
[47] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review. E, Statistical, nonlinear, and soft matter physics, 69:066138, 07 2004. doi: 10.1103/PhysRevE.69.066138.
[48] Jakob Runge. Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 938-947. PMLR, 09-11 Apr 2018. URL http://proceedings.mlr.press/v84/runge18a.html.
[49] Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, and james m robins. Nonparametric von Mises estimators for entropies, divergences and mutual informations. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ 06138bc5af6023646ede0e1f7c1eac75-Paper.pdf.
[50] Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Efficient Estimation of Mutual Information for Strongly Dependent Variables. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages 277-286, San Diego, California, USA, 09-12 May 2015. PMLR. URL http://proceedings.mlr.press/v38/gao15.html.
[51] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5171-5180. PMLR, 09-15 Jun 2019. URL http://proceedings.mlr.press/v97/poole19a.html.
[52] Jean-Louis Durrieu, J. Thiran, and Finnian Kelly. Lower and upper bounds for approximation of the kullback-leibler divergence between gaussian mixture models. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4833-4836, 2012.
[53] Greg Brockman, Vicki Cheung, Ludwig Pettersson, J. Schneider, John Schulman, Jie Tang, and W. Zaremba. OpenAI Gym. ArXiv, abs/1606.01540, 2016.
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
[55] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, J. Schneider, Joshua Tobin, Maciek Chociej, P. Welinder, V. Kumar, and W. Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research. ArXiv, abs/1802.09464, 2018.
[56] T. Lillicrap, Jonathan J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.</p>
<p>[57] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[58] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5062-5071. PMLR, 09-15 Jun 2019. URL https: //proceedings.mlr.press/v97/pathak19a.html.
[59] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations (ICLR), 2016.
[60] Rui Zhao and Volker Tresp. Energy-based hindsight experience prioritization. In Aude Billard, Anca Dragan, Jan Peters, and Jun Morimoto, editors, Proceedings of The 2nd Conference on Robot Learning, volume 87 of Proceedings of Machine Learning Research, pages 113-122. PMLR, 29-31 Oct 2018.
[61] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 94b5bde6de888ddf9cde6748ad2523d1-Paper.pdf.
[62] Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition. ArXiv, abs/2011.15091, 2020.
[63] John R. Hershey and Peder A. Olsen. Approximating the Kullback Leibler divergence between gaussian mixture models. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07, volume 4, pages IV-317-IV-320, 2007. doi: 10.1109/ICASSP. 2007.366913.
[64] E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033, 2012.
[65] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
[66] Andrew M. Saxe, James L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations (ICLR), 2014.
[67] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations (ICLR), 2018.
[68] Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng. Exploration via hindsight goal generation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/57db7d68d5335b52d5153a4e01adaa6b-Paper.pdf.
[69] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.</p>
<h1>Supplementary Material</h1>
<h2>A Proofs and Derivations</h2>
<h2>A. 1 Proof of Propositions</h2>
<p>We first clarify the behavior of local CGMs (see Def. 1) under interventions. In particular, the local CGM induced by $X=x$ on $P(\mathcal{V})$ under an intervention do $(\mathcal{V}:=v)$ is defined to be the local CGM induced by $X=x$ on the joint distribution $P^{\operatorname{do}(\mathcal{V}:=v)}(\mathcal{V})$.</p>
<p>Proof of Prop. 1. "if": If it holds that $S_{j}^{\prime} \not \mathbb{L} A \mid S=s$, then by the Markov property [3, Def. 6.21], there must be an unblocked path from $A$ to $S_{j}^{\prime}$ in $\mathcal{G}<em j="j">{S=s}$. Because the path over $S$ is blocked by observing $S=s$, and we assume no instantaneous effects, the only possible such path is the direct edge $A \rightarrow S</em>$.
"only if": If there is an edge $A \rightarrow S_{j}^{\prime}$ in $\mathcal{G}}^{\prime<em j="j">{S=s}$, then causal minimality of the local causal graph says that $S</em> A \mid S=s$.}^{\prime}$ must be dependent on each parent given its other parents, meaning $S_{j}^{\prime} \not \mathbb{L</p>
<p>Proof of Prop. 2. To show the first part, note that the dependence $S_{j}^{\prime} \not \mathbb{L} A \mid S=s$ under $\operatorname{do}(A:=\pi(a \mid s))$ implies that there exists some $s_{j}^{\prime}$ and $a_{1}, a_{2}$ with $\pi\left(a_{1} \mid s\right)&gt;0, \pi\left(a_{2} \mid s\right)&gt;0$ for which</p>
<p>$$
p^{\operatorname{do}(A:=\pi)}\left(s_{j}^{\prime} \mid s, a_{1}\right)=p\left(s_{j}^{\prime} \mid s, a_{1}\right) \neq p\left(s_{j}^{\prime} \mid s, a_{2}\right)=p^{\operatorname{do}(A:=\pi)}\left(s_{j}^{\prime} \mid s, a_{2}\right)
$$</p>
<p>Any $\pi^{\prime}$ with full support would also have $\pi^{\prime}\left(a_{1} \mid s\right)&gt;0, \pi^{\prime}\left(a_{2} \mid s\right)&gt;0$, and so</p>
<p>$$
p^{\operatorname{do}\left(A:=\pi^{\prime}\right)}\left(s_{j}^{\prime} \mid s, a_{1}\right)=p\left(s_{j}^{\prime} \mid s, a_{1}\right) \neq p\left(s_{j}^{\prime} \mid s, a_{2}\right)=p^{\operatorname{do}\left(A:=\pi^{\prime}\right)}\left(s_{j}^{\prime} \mid s, a_{2}\right)
$$</p>
<p>implying the dependence under $\operatorname{do}\left(A:=\pi^{\prime}\right)$. To show that the agent is in control of $S_{j}^{\prime}$ in $S=s$, there needs to be an edge $A \rightarrow S_{j}^{\prime}$ in $\mathcal{G}<em j="j">{S=s}$ under all interventions do $\left(A:=\pi^{\prime}\right)$ with $\pi^{\prime}$ having full support. This is the case, because as shown, for all interventions do $\left(A:=\pi^{\prime}\right)$ with $\pi^{\prime}$ having full support it holds that $S</em>}^{\prime} \not \mathbb{L} A \mid S=s$, and by Prop. 1, there is an edge $A \rightarrow S_{j}^{\prime}$ in $\mathcal{G<em j="j">{S=s}$ under any such intervention.
To show the second part, we show that if $S</em>$, it holds that}^{\prime} \Perp A \mid S=s$ under any intervention do $(A:=\pi)$ with $\pi$ having full support, for any intervention do $\left(A:=\pi^{\prime}\right)$ it holds that $P^{\operatorname{do}\left(A:=\pi^{\prime}\right)}\left(S_{j}^{\prime} \mid S=s, A\right)=$ $P^{\operatorname{do}\left(A:=\pi^{\prime}\right)}\left(S_{j}^{\prime} \mid S=s\right)$. This follows from the fact that for any $\pi^{\prime</p>
<p>$$
\begin{aligned}
P^{\operatorname{do}\left(A:=\pi^{\prime}\right)}\left(S_{j}^{\prime} \mid S=s, A\right) &amp; =P\left(S_{j}^{\prime} \mid S=s, A\right) \
&amp; =P\left(S_{j}^{\prime} \mid S=s\right)=P^{\operatorname{do}\left(A:=\pi^{\prime}\right)}\left(S_{j}^{\prime} \mid S=s\right)
\end{aligned}
$$</p>
<p>where the first equality is due to the autonomy property of causal mechanisms [3, Eq. 6.7], and the second equality because of the independence $S_{j}^{\prime} \Perp A \mid S=s$. Note that if $\pi$ had not had full support, we would not be allowed to use the second equality as then $P\left(S_{j}^{\prime} \mid S=s, A=a\right)=P\left(S_{j}^{\prime} \mid S=s\right)$ only for $a$ with $\pi(a)&gt;0$. The agent is not in control of $S_{j}^{\prime}$ in $S=s$, as for all interventions do $\left(A:=\pi^{\prime}\right)$ with $\pi^{\prime}$ having full support, there is no edge $A \rightarrow S_{j}^{\prime}$ in $\mathcal{G}_{S=s}$ by Prop. 1.</p>
<h2>A. 2 CMI Formula</h2>
<p>$$
\begin{aligned}
C^{j}(s)=I\left(S_{j}^{\prime} ; A \mid S=s\right) &amp; =\mathrm{D}<em S__j="S_{j">{\mathrm{KL}}\left(P</em>\right) \
&amp; =\mathbb{E}}^{\prime}, A \mid s} | P_{S_{j}^{\prime} \mid s} \otimes P_{A \mid s<em j="j">{S</em>\right] \
&amp; =\mathbb{E}}^{\prime}, A \mid s}\left[\log \frac{p\left(s_{j}^{\prime}, a \mid s\right)}{p\left(s_{j}^{\prime} \mid s\right) \pi(a \mid s)<em j="j">{S</em>\right] \
&amp; =\mathbb{E}}^{\prime}, A \mid s}\left[\log \frac{p\left(s_{j}^{\prime} \mid s, a\right)}{p\left(s_{j}^{\prime} \mid s\right)<em _mathrm_KL="\mathrm{KL">{A \mid s}\left[\mathrm{D}</em>\right)\right]
\end{aligned}
$$}}\left(P_{S_{j}^{\prime} \mid s, a} | P_{S_{j}^{\prime} \mid s</p>
<h1>A. 3 Proof that CAI is a Pointwise Version of Janzing et al.'s Causal Strength</h1>
<p>Janzing et al.'s [18] measure of causal strength quantifies the impact that removing a set of arrows in the causal graph would have. As it is the relevant case for us, we concentrate here on the single arrow version, for instance, between random variables $X$ and $Y$. The idea is to consider the arrow as a "communication channel" and evaluate the corruption that could be done to the signal that flows between $X$ and $Y$ by cutting the channel. To do so, the distribution that feeds the channel is replaced with $P(X)$, i.e. the marginal distribution of $X$. The measure of causal strength then is equal to the difference between the pre- and post-cutting joint distribution as given by the KL divergence.
Formally, let $\mathcal{V}$ denote the set of variables in the causal graph, let $X \rightarrow Y$ be the arrow of interest with $X, Y \in \mathcal{V}$, and let $\mathrm{Pa}_{Y}^{\backslash X}$ be the set of parents of $Y$ without $X$. Then, the post-cutting distribution on $Y$ is defined as</p>
<p>$$
p_{X \rightarrow Y}\left(y \mid \mathrm{pa}<em Y="Y">{Y}^{\backslash X}\right)=\int p\left(y \mid x, \mathrm{pa}</em> x
$$}^{\backslash X}\right) p(x) \mathrm{d</p>
<p>The new joint distribution after such an intervention is defined as</p>
<p>$$
p_{X \rightarrow Y}\left(v_{1}, \ldots, v_{|\mathcal{V}|}\right)=p_{X \rightarrow Y}\left(y \mid \mathrm{pa}<em _92_="\" _substack_j="\substack{j" v__j="v_{j">{Y}^{\backslash X}\right) \prod</em>\right)\right)
$$} \neq y}} p\left(v_{j} \mid \mathrm{Pa}\left(v_{j</p>
<p>Finally, the causal strength of the arrow $X \rightarrow Y$ is defined as</p>
<p>$$
\mathfrak{C}<em _mathrm_KL="\mathrm{KL">{X \rightarrow Y}=\mathrm{D}</em>(V)\right)
$$}}\left(P(V) | P_{X \rightarrow Y</p>
<p>To show the correspondence to our measure, we first restate Lemma 3 of [18]. For a single arrow $X \rightarrow Y$, causal strength can also be written as the KL between the conditionals on $Y$ :</p>
<p>$$
\begin{aligned}
\mathfrak{C}<em _mathrm_KL="\mathrm{KL">{X \rightarrow Y} &amp; =\mathrm{D}</em>}}\left(P\left(Y \mid \mathrm{Pa<em X="X" Y="Y" _rightarrow="\rightarrow">{Y}\right) | P</em>}\left(Y \mid \mathrm{Pa<em _mathrm_KL="\mathrm{KL">{Y}^{\backslash X}\right)\right) \
&amp; =\int \mathrm{D}</em>}}\left(P\left(Y \mid \mathrm{pa<em X="X" Y="Y" _rightarrow="\rightarrow">{Y}\right) | P</em>}\left(Y \mid \mathrm{pa<em Y="Y">{Y}^{\backslash X}\right)\right) P\left(\mathrm{pa}</em>
\end{aligned}
$$}\right) \mathrm{dpa}_{Y</p>
<p>We rewrite this further:</p>
<p>$$
\begin{aligned}
&amp; =\int P\left(\mathrm{pa}<em Y="Y">{Y}^{\backslash X}\right) P\left(X \mid \mathrm{pa}</em>}^{\backslash X}\right) \mathrm{D<em Y="Y">{\mathrm{KL}}\left(P\left(Y \mid \mathrm{pa}</em>}\right) | P_{X \rightarrow Y}\left(Y \mid \mathrm{pa<em Y="Y">{Y}^{\backslash X}\right)\right) \mathrm{d} x \mathrm{dpa}</em> \
&amp; =\mathbb{E}}^{\backslash X<em Y="Y">{\mathrm{pa}</em>}^{\backslash X}}\left[\int P\left(X \mid \mathrm{pa<em _mathrm_KL="\mathrm{KL">{Y}^{\backslash X}\right) \mathrm{D}</em>}}\left(P\left(Y \mid \mathrm{pa<em X="X" Y="Y" _rightarrow="\rightarrow">{Y}\right) | P</em>}\left(Y \mid \mathrm{pa<em _mathrm_pa="\mathrm{pa">{Y}^{\backslash X}\right)\right) \mathrm{d} x\right] \
&amp; =\mathbb{E}</em><em _mathrm_pa="\mathrm{pa" _mid="\mid" x="x">{Y}^{\backslash X}}\left[\mathbb{E}</em><em _mathrm_KL="\mathrm{KL">{Y}^{\backslash X}}\left[\mathrm{D}</em>}}\left(P\left(Y \mid \mathrm{pa<em X="X" Y="Y" _rightarrow="\rightarrow">{Y}\right) | P</em>\right)\right)\right]\right]
\end{aligned}
$$}\left(Y \mid \mathrm{pa}_{Y}^{\backslash X</p>
<p>And we see that the inner part corresponds to our measure $C^{j}(s)$ (cmp. Eq. 2) for the choices of variables $X \widehat{=} A, Y \widehat{=} S_{j}^{\prime}$, and $\mathrm{Pa}_{Y}^{\backslash X} \widehat{=} S$, provided that $P(X) \widehat{=} \pi(A)$ is not dependent on $S$. Thus, CAI is a pointwise version of causal strength for policies independent of the state:</p>
<p>$$
\mathfrak{C}<em j="j">{A \rightarrow S</em>(s)\right]
$$}^{\prime}}=\mathbb{E}_{s}\left[C^{j</p>
<h2>A. 4 Approximating the KL Divergence Between a Gaussian and a Mixture of Gaussians</h2>
<p>In this section, we give the approximation we use for the KL divergence in Eq. 4. We first state the approximation for the general case of the KL between two mixtures of Gaussians, and then specialize to our case when the first distribution is Gaussian distributed. Here, we use the notation of Durrieu et al. [52].
Let $f$ be the PDFs of a multivariate Gaussians mixture with $A$ components, mixture weights $\omega_{a}^{f} \in$ $(0,1]$, means $\mu_{a}^{f} \in \mathbb{R}^{d}$ and covariances $\Sigma_{a}^{f} \in \mathbb{R}^{d \times d}$, where $a \in{1, \ldots, A}$ is the index is of the $a^{\prime}$ th component. Then,</p>
<p>$$
f(x)=\sum_{a=1}^{A} \omega_{a}^{f} f_{a}(x)=\sum_{a=1}^{A} \omega_{a}^{f} \mathcal{N}\left(x ; \mu_{a}^{f}, \Sigma_{a}^{f}\right)
$$</p>
<p>where $f_{a}(x)=\mathcal{N}\left(x ; \mu_{a}^{f}, \Sigma_{a}^{f}\right)$ is the PDF of a multivariate Gaussian with mean $\mu_{a}^{f}$ and covariance $\Sigma_{a}^{f}$. Analously, let $g$ be the PDF of a multivariate Gaussians mixture with $B$ components. We are interested in the KL divergence $\mathrm{D}<em _mathbb_R="\mathbb{R">{\mathrm{KL}}(f | g)=\int</em> x$, which is intractable.
There are several ways to approximate the KL based on the decomposition $\mathrm{D}}^{d}} f(x) \log \frac{f(x)}{g(x)} \mathrm{d<em _prod="{prod" _text="\text">{\mathrm{KL}}=H(f, g)-H(f)$, where $H(f, g)$ is the cross-entropy between $f$ and $g$ and $H(f)$ is the entropy of $f$. We will state the so-called product approximation $\mathrm{D}</em>}}$ and the variational approximation $\mathrm{D<em _prod="{prod" _text="\text">{\text {var }}$ [63]. Starting with $\mathrm{D}</em>$ :}</p>
<p>$$
\begin{aligned}
H(f, g) &amp; \geq-\sum_{a} \omega_{a}^{f} \log \left(\sum_{b} \omega_{b}^{g} t_{a b}\right) \
H(f) &amp; \geq-\sum_{a} \omega_{a}^{f} \log \left(\sum_{a^{\prime}} \omega_{a^{\prime}}^{f} z_{a a^{\prime}}\right) \
\mathrm{D}<em a="a">{\text {prod }} &amp; :=\sum</em>\right)
\end{aligned}
$$} \omega_{a}^{f} \log \left(\frac{\sum_{a^{\prime}} \omega_{a^{\prime}}^{f} z_{a a^{\prime}}}{\sum_{b} \omega_{b}^{g} t_{a b}</p>
<p>where $t_{a b}=\int f_{a}(x) g_{b}(x) \mathrm{d} x, z_{a a^{\prime}}=\int f_{a}(x) f_{a^{\prime}}(x) \mathrm{d} x$ are normalization constants of product of Gaussians, and the inequalities in (26), (27) are based on Jensen's inequality. For the variational approximation:</p>
<p>$$
\begin{gathered}
H(f, g) \leq \sum_{a} \omega_{a}^{f} H\left(f_{a}\right)-\sum_{a} \omega_{a}^{f} \log \left(\sum_{b} \omega_{b}^{g} e^{-\mathrm{D}<em a="a">{\mathrm{KL}}\left(f</em>\right) \
H(f) \leq \sum_{a} \omega_{a}^{f} H\left(f_{a}\right)-\sum_{a} \omega_{a}^{f} \log \left(\sum_{a^{\prime}} \omega_{a^{\prime}}^{f} e^{-\mathrm{D}} | g_{b}\right)<em a="a">{\mathrm{KL}}\left(f</em>\right) \
\mathrm{D}} | f_{a^{\prime}}\right)<em a="a">{\mathrm{var}}:=\sum</em>} \omega_{a}^{f} \log \left(\frac{\sum_{a^{\prime}} \omega_{a^{\prime}}^{f} e^{-\mathrm{D<em a="a">{\mathrm{KL}}\left(f</em>} | f_{a^{\prime}}\right)}}{\sum_{b} \omega_{b}^{g} e^{-\mathrm{D<em a="a">{\mathrm{KL}}\left(f</em>\right)
\end{gathered}
$$} | g_{b}\right)}</p>
<p>where (29), (30) are based on solving variational problems. It can be shown that the mean between $\mathrm{D}<em _text="\text" _var="{var">{\text {prod }}$ and $\mathrm{D}</em>}}$ is the mean of a lower and upper bound to $\mathrm{D<em _mean="{mean" _text="\text">{\mathrm{KL}}$, with better approximation qualities [52]. Consequently, we use $\mathrm{D}</em>}}:=\frac{\mathrm{D<em _text="\text" _var="{var">{\text {prod }}+\mathrm{D}</em>$ as in our case, we know that $f$ has only one component. This means that we can compute $H(f)$ in closed form, and do not need to use the inequalitities (27), (30). Approximating $H(f, g)$ with the mean of the lower bound (26) and upper bound (29),}}}{2}$ as the basis of our approximation. We can simplify $\mathrm{D}_{\text {mean }</p>
<p>$$
H_{\text {mean }}(f, g):=\frac{1}{2}\left(-\log \left(\sum_{b} \omega_{b}^{g} t_{a b}\right)+H(f)-\log \left(\sum_{b} \omega_{b}^{g} e^{-\mathrm{D}<em a="a">{\mathrm{KL}}\left(f</em>\right)\right)
$$} | g_{b}\right)</p>
<p>we get the final formula we use</p>
<p>$$
\begin{aligned}
\mathrm{D}<em _mean="{mean" _text="\text">{\text {mean }} &amp; :=H</em>(f, g)-H(f) \
&amp; =-\frac{1}{2} \log \left(\sum_{b} \omega_{b}^{g} t_{a b}\right)-\frac{1}{2} \log \left(\sum_{b} \omega_{b}^{g} e^{-\mathrm{D}}<em a="a">{\mathrm{KL}}\left(f</em> H(f)
\end{aligned}
$$} | g_{b}\right)}\right)-\frac{1}{2</p>
<p>Note that this term can become negative, whereas the KL is non-negative. In practice, we thus threshold $\mathrm{D}_{\text {mean }}$ at zero. For completeness, we also state the entropy of a Gaussian</p>
<p>$$
H(f)=\frac{1}{2} \log \left((2 \pi e)^{d}|\Sigma|\right)
$$</p>
<p>the $\log$ normalization constant for a product of Gaussians</p>
<p>$$
\log t_{a b}=-\frac{d}{2} \log 2 \pi-\frac{1}{2} \log \left|\Sigma_{a}^{f}+\Sigma_{b}^{g}\right|-\frac{1}{2}\left(\mu_{b}^{g}-\mu_{a}^{f}\right)^{T}\left(\Sigma_{a}^{f}+\Sigma_{b}^{g}\right)^{-1}\left(\mu_{b}^{g}-\mu_{a}^{f}\right)
$$</p>
<p>and the KL between two Gaussians</p>
<p>$$
\mathrm{D}<em a="a">{\mathrm{KL}}\left(f</em>\right)
$$} | g_{b}\right)=-\frac{d}{2}+\frac{1}{2} \log \frac{\left|\Sigma_{a}^{f}\right|}{\left|\Sigma_{b}^{g}\right|}+\frac{1}{2} \operatorname{Tr}\left(\left(\Sigma_{b}^{g}\right)^{-1} \Sigma_{a}^{f}\right)+\frac{1}{2}\left(\mu_{b}^{g}-\mu_{a}^{f}\right)^{T}\left(\Sigma_{b}^{g}\right)^{-1}\left(\mu_{b}^{g}-\mu_{a}^{f</p>
<p>In our experiments, we assume independent dimensions, that is, we parametrize the covariance $\Sigma$ as a diagonal matrix. With this, the above formulas can be further simplified.</p>
<h1>B Environments</h1>
<h2>B. 1 ID-Slide</h2>
<p>IDSLIDE is a simple environment that we designed to test influence detection, as we can easily derive when the agent has influence or not. It consists of an agent and an object positioned on a line, with both agent and object only being able to move left and right. See Fig. S1 for a visualization. The goal of the agent is to move the object to a goal zone. As the agent can not cross past the center of the environment, it has to hit the object at the appropriate speed. The state space $\mathcal{S} \subset \mathbb{R}^{4}$ consists of position and velocity of the agent and object. The agent's action $a \in \mathcal{A} \subseteq[-1,1]$ applies acceleration to the agent. On contact with the object, the full impulse of the agent is transferred to the object. We can derive if the agent has causal influence in a state by simulating applying the maximum acceleration in both directions and checking whether a contact has occurred, and the object state has changed.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure S1: Schematic of IDSLIDE environment. The agent (black square) has to slide the object (green square) to the target location (red zone), but can not pass the dotted line.</p>
<h2>B. 2 FetchPickAndPlace</h2>
<p>The Fetch environments in OpenAI Gym [53] are built on top of the MuJoCo simulator [64], and consist of a position-controlled 7 DoF robotic arm [55]. In FetchPickAndPlace, the state space $\mathcal{S} \subset \mathbb{R}^{25}$ consists of position and velocity of the robot's end effector, the position and velocity of the gripper, and the object's position and rotation, linear and angular velocities, and position relative to the end effector. The action space $\mathcal{A} \subseteq[-1,1]^{4}$ controls the gripper movement and opening/closening of the gripper. For the experiments involving Transformer models, we split the state space into agent and object state components, where we do not include the relative positions between gripper and object into either component.
For our experiment in Sec. 5 evaluating the causal influence detection, we need to determine whether the agent can potentially influence the object, i.e. whether the agent is "in control". This is difficult to determine analytically, which is why we designed a heuristic. The idea is to find an ellipsoid around the end effector that captures its maximal movement range, and intersect this ellipsoid with the object to decide whether influence is possible. As the range of movement of the robotic arm is different depending on its position, we first build a lookup table (offline) that contains, for different start positions, end positions after applying different actions. To do so, we grid-sample the 3D-space over the table ( 50 grid points per dimension), plus a sparser sampling of the outer regions (20 grid points per dimension), resulting in 133000 starting locations for the lookup table. Then, the following procedure is repeated for each starting location and action: after resetting the simulator, the end effector is manually moved to one of the starting locations, one of the maximal actions in each dimension (i.e., -1 and 1 , for a total of 6 actions) is applied, and the end position after one environment step is recorded in the lookup table.
Now, while the environment runs, for each step, we find the sampling point closest to the position of the robotic arm in the lookup table using a KD-tree, and find the corresponding end positions. From the end positions, we build the ellipsoid by taking the maximum absolute deviation in each dimension to be the length of the ellipsoid axis in this dimension. The ellipsoid so far does not take into account the spatial extents of object and gripper. Thus, we extend the ellipsoid by the sizes of the object and gripper fingers in each dimension. Furthermore, we take into account that the gripper fingers can be slid in y-direction by the agent's action by extending the ellipsoid's y-axis correspondingly. The label of "agent in control" is then obtained by checking whether the object location lies within the ellipsoid. Last, we also label a state as "agent in control" when there is an actual contact between gripper and object in the following step. We note that the exact procedure described above is included in the code release.</p>
<h2>B. 3 FetchRotTable</h2>
<p>FetchRotTable is an environment designed by us to test CAI prioritization in a more challenging setting. In FetchRotTable, the table rotates periodically, moving the object around. This creates a confounding effect for influence detection, as there is another source of object movements besides</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure S2: Causal influence detection performance, complementing Fig. 2 in the main part. (a, b) Precision-recall curves on IDSLIDE and FETCHPICKANDPLACE environments. (c) Area-under-ROC curve for FETCHPICKANDPLACE depending on added state noise. Noise level is given as percentage of one standard deviation over the dataset.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure S3: Causal influence detection performance on a test dataset of 250k transitions from a random policy. This dataset only has $3.3 \%$ transitions with influence (i.e. labeled as positive), which is why the detection task is considerably harder. Left, center: ROC and PR curves. Right: PR curves for CAI when varying the number of actions $K$.
the agent's own actions. This means that CAI needs to differentiate between different causes for movements.</p>
<p>FetchRotTable is based on FETCHPICKANDPLACE, but the rectangular table is replaced with a circular one (see Fig. 8). The table rotates with inertia by 45 degrees over the course of 25 environment steps, and then pauses for 5 steps. To make the resulting environment Markovian, the state space of FETCHPICKANDPLACE is extended to $\mathcal{S} \subset \mathbb{R}^{20}$, additionally including sine and cosine of the table angle, the rotational velocity of the table, and the amount the table will rotate in the current state in radians. The task of the agent is the same as in FETCHPICKANDPLACE, i.e. to move the object to a target position. If the target position is on the table, the agent thus has to learn to hold the object in position while the table rotates. In contrast to FETCHPICKANDPLACE, the goal distribution is different, with $90 \%$ of goals in the air and only $10 \%$ on the table. This makes the task more challenging, as the agent has to master grasping and lifting before $90 \%$ of the possible positive rewards can be accessed.</p>
<h1>C Additional Results for Influence Evaluation</h1>
<p>In this section, we include additional results for the empirical evaluation of influence detection in Sec. 5. Figure S2a and Fig. S3b show precision-recall (PR) curves for the experiment in Sec. 5, while Fig. S2c shows how area-under-ROC curve varies while increasing the observation noise level.</p>
<p>For FETCHPICKANDPLACE, we also evaluated on a test dataset obtained from a random policy. On this dataset, the detection task is considerably harder: it contains only $3.3 \%$ transitions where the</p>
<p>agent has influence (i.e. labeled as positive), and it does not contain samples where the agent moves the object in the air, which are easier to detect as influence. We show ROC and PR curves for this dataset in Fig. S3a and Fig. S3b. As expected, the detection performance drops compared to the other test dataset, which can in particular be seen in the PR curve. But overall, the performance is still quite good when taking into account the low amount of positive samples. In Fig. S3c, we also plot the impact of varying the number of sampled actions $K$ on this dataset. As one can see, the method is relatively robust to this parameter, giving decent performance even under a small number of sampled actions. However, we think that a higher number of sampled actions is important in edge-case situations, which are overshadowed in such a quantitative analysis.
Finally, in Fig. S4, we give a qualitative analysis of CAI. Here, we plot the trajectory in three different situations: no contact of agent and object, briefly touching the object, and successfully manipulating the object. As desired, CAI is low and high in the "no contact" and "successful manipulation" trajectories. In the "brief touch" trajectory, CAI spikes around the contact. However, in steps 8-11, when the agent hovers near the object, the heuristical labeling (see Sec. B.2) still detects that influence of agent on object is possible, which CAI does not register. These are difficult cases which show that our method still has room for improvement; we think that these failure cases could be resolved by employing a better model. However, we also note that the "ground truth" label is only based on a heuristic, which will make mispredictions at times as well.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure S4: Visualizing CAI score $C^{j}(s)$ over first 20 steps of an episode on FETCHPICKANDPLACE. The red dots mark states where the agent has causal influence on the object (according to the ground truth label as described in Sec. B.2). Yellow stars mark that the agent makes contact with the object between this state and the next state (as derived from the simulator), which also includes that the agent has causal influence. Plotted are episodes with no contact (first row), briefly touching the object (second row), and successful manipulation of the object (third row).</p>
<h1>D Additional Results for Reinforcement Learning</h1>
<p>In this section, we include additional results to the RL experiments in Sec. 6. First of, in Fig. S5, we analyse CAI's behavior using one the runs from the intrinsic motivation experiment in Sec. 6.1. To this end, we plot a heatmap visualising the score distribution in the replay buffer after 5000</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure S5: Analyzing the behavior of CAI. Plotted is one run of the intrinsic motivation experiment from Sec. 6.1. Left: heatmap showing score $C^{j}(s)$ per step, as stored in replay buffer after 5000 episodes. Score is averaged in groups of 100 episodes and normalized by 95 th percentile. Center: heatmap showing ground truth label derived for causal influence, as described in Sec. B.2. CAI approximates the ground truth well. Right: behavior of the agent in this run.
episodes, a corresponding heatmap with ground truth labels, and the fraction of steps where the object was moved or held in the air by the agent. It can be seen that CAI's distribution approximates the ground truth label's distribution well. It also becomes visible that CAI measures the strength of causal influence, as the score is highest after the agent learns to lift the object in the air (episode 3000 onwards). In comparison, the binary ground truth distribution assigns high scores more uniformly.
In Fig. S6, we analyse the impact of combining our different proposed improvements, namely prioritization, exploration bonus, and active action selection. On its own, prioritization brings the largest benefit. Combining either exploration bonus or active action selection with prioritization leads to similar further improvements. Both are complementary however; combining all three variants together results in the best performing version. Compared to not using CAI, combining two or all three improvements leads to a $4-10 \times$ increase in sample efficiency.
In Fig. S5, we plot different versions of the VIME baseline. For VIME, there is the choice of which parts of the state space the information gain should be computed on. We compared the variants of using the full state, only the position of the agent and the object, and only the position of the object (which would be similar to CAI). The variant of using agent and object position performed best, and thus we use it for comparison in Fig. 6 in the main part.
Finally, in Fig. S8, we plot different versions of the ensemble disagreement baseline. Similarly to VIME, we also have the option of choosing parts of the state space the disagreement should be computed on. We compared the variants of using the full state, only the position of the agent and the object, and only the position of the object (which would be similar to CAI). The variant of using just object position performed best, and thus we use it for comparison in Fig. 6 in the main part.</p>
<p>Preliminary Experiments with Negative Results In preliminary work, we also experimented with other variants, which we list here for completeness. For prioritization, we tested using a proportional distribution over episode scores (as in [59]) instead of the ranked distribution. While the proportional distribution also worked, it resulted in slower learning and performance did not converge to $100 \%$ success rate. We also briefly tried shaping the ranking distribution by raising the score by a power before ranking (as in [59]), but this did not result in notable improvements, so we dropped this line for simplicity. Further, Schaul et al. [59] use importance sampling to correct for the bias in state sampling introduced by the prioritization. We found this not to be necessary for the tasks we experimented with, but note that it could be required to converge for other environments. Last, we also experimented with prioritizing states within an episode, and prioritized selection of goals for HER, but could not achieve any improvements from this.
For the exploration bonus, we tested adding a flat bonus when the score is over a certain threshold, and a bonus that interacts multiplicative with the reward. Both variants performed worse than the additive bonus. Linearly annealing the bonus to zero over the course of training did also not result in improvements. For active action selection, we experimented with sampling actions according to a ranked or proportional distribution over the CAI score instead of taking the action with a maximum score. Both versions performed worse than maximum score action selection.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We use capital letters (e.g. $X$ ) to denote random variables, small letters to denote samples drawn from particular distributions (e.g. $x \sim P_{X}$ ), and caligraphy letters to denote graphs, sets and sample spaces (e.g. $x \in \mathcal{X})$. We denote distributions with $P$ and their densitites with $p$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1103 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1103</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1103</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-273023050</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.00240v1.pdf" target="_blank">Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference</a></p>
                <p><strong>Paper Abstract:</strong> Active inference is a mathematical framework for understanding how agents (biological or artificial) interact with their environments, enabling continual adaptation and decision-making. It combines Bayesian inference and free energy minimization to model perception, action, and learning in uncertain and dynamic contexts. Unlike reinforcement learning, active inference integrates exploration and exploitation seamlessly by minimizing expected free energy. In this paper, we present a continual learning framework for agents operating in discrete time environments, using active inference as the foundation. We derive the mathematical formulations of variational and expected free energy and apply them to the design of a self-learning research agent. This agent updates its beliefs and adapts its actions based on new data without manual intervention. Through experiments in changing environments, we demonstrate the agent's ability to relearn and refine its models efficiently, making it suitable for complex domains like finance and healthcare. The paper concludes by discussing how the proposed framework generalizes to other systems, positioning active inference as a flexible approach for adaptive AI.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1103.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1103.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DT-AIF agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrete-Time Active Inference self-learning research agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical POMDP active inference agent that continually learns likelihood mappings (A matrix) using Dirichlet–Categorical updates and selects policies by minimising Expected Free Energy (balancing extrinsic and epistemic value). Implemented as a two-level generative model for a simulated research-process environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Discrete-Time Active Inference self-learning agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical discrete-time active inference agent operating on a two-level POMDP: top level models industry context (one state factor + industry cue observation), bottom level models industry × research-process state factors and three observation modalities (research process, outcome, industry cue). Posterior beliefs approximated by variational distribution Q(s); parameters of the likelihood (A matrix) are represented by Dirichlet concentration parameters and updated online with a Hebbian-like rule (a_{t+1} = ω·a_t + η·τ_o τ ⊗ s_τ). Policy selection uses Expected Free Energy (EFE) minimisation with explicit epistemic (information gain / novelty-seeking) and extrinsic terms.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information-gain maximization / curiosity-driven exploration implemented via Expected Free Energy minimisation (active inference)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent adaptively selects policies by minimising Expected Free Energy, which combines extrinsic value (prior preferences) and epistemic value (expected information gain). The epistemic term explicitly quantifies expected change in beliefs about the A matrix (KL between posterior and prior over A), driving exploration toward novel informative observations. Simultaneously, the agent updates Dirichlet concentration parameters for the A matrix online using a Hebbian-like rule with learning rate η and forgetting rate ω, thereby adapting the generative model based on sampled observations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated hierarchical research-process POMDP (16 industries × research processes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (POMDP), discrete state and observation spaces, stochastic (observations sampled from A matrix probabilities), hierarchical (two-level generative model), non-stationary / subject to regime shifts (mapped experiments with environment modifications across trials), simulated generative process that samples outcomes from the agent's A matrix mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Discrete: top-level industry factor with 16 industries; bottom-level includes industry × research-process combinations (reported as 16 industries × 4 research processes = 64 state-process pairs); outcome modality with multiple discrete outcomes (examples use ~5 outcome categories); experiments run in discrete iterations/trials (plots shown for first 10 and subsequent 20 trials); action space corresponds to selecting a research process (policy choices over research processes per industry).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: agent learns correct mappings quickly in the baseline environment (Industry 1 reaches a high score within ~6 iterations); after localized change (only Industry 1 modified) performance shows steep drop then linear recovery over 20 trials but does not reach prior maximum; after larger/global modifications across many industries, agent relearns faster and surpasses prior peak by trial 18. (No numerical cumulative-reward or accuracy numbers provided in paper text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative sample-efficiency metrics reported: baseline — high-confidence mapping for Industry 1 reached within ~6 iterations; localized-change environment — linear improvement over 20 trials but did not reach former maximum; larger-change environment — overtakes previous maximum by trial 18. Exact sample counts to reach fixed numeric thresholds are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed via Expected Free Energy: extrinsic (goal-directed) terms favour exploitation of preferred outcomes, while an explicit epistemic / novelty-seeking term (expected KL between q(A|o,s) and q(A)) drives exploration to reduce uncertainty about the A matrix. Policy selection minimises the combined EFE objective, trading off information gain against expected value.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>No explicit quantitative comparisons to alternative adaptive-design methods (e.g., random exploration, RL baselines, Bayesian optimisation) are reported; comparisons are qualitative across different non-stationary environment configurations (baseline vs localized vs global change).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Active inference with Dirichlet–Categorical learning of the A matrix enables continual online adaptation in discrete-time POMDPs; 2) Agent quickly learns baseline mappings (industry-specific outcomes) and can relearn after environment changes; 3) Localized changes (single-industry) produce slower, more gradual relearning due to reliance on prior beliefs, while global/regime shifts lead to faster, sometimes superior, relearning as the agent globally adjusts its model; 4) The epistemic term in EFE (expected information gain about A) provides a principled mechanism for adaptive experimental selection (curiosity-driven exploration) that focuses sampling on informative actions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) Results are qualitative and plotted — no precise numerical metrics (e.g., cumulative reward, likelihood, sample complexity limits) are reported; 2) No formal baseline comparisons (random, RL, or other active-learning methods) are provided, limiting claims of relative advantage; 3) Evaluation in a simplified, simulated research environment rather than real-world data; 4) Sensitivity to hyperparameters (learning rate η, forgetting ω) and scoring metric design may affect apparent performance (paper notes score-inflation effects when many unchanged mappings exist); 5) Localized environmental changes caused slower adaptation and failure to reach prior maxima in one experiment; 6) The generative-process is simulated from the agent's A matrix (internal sampling), which may not capture complexities of real-world generative dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The graphical brain: Belief propagation and active inference. <em>(Rating: 2)</em></li>
                <li>Active inference: The free energy principle in mind, brain, and behavior. <em>(Rating: 2)</em></li>
                <li>Active inference, belief propagation, and the free energy principle. <em>(Rating: 2)</em></li>
                <li>Active Inference, Curiosity and Insight. <em>(Rating: 2)</em></li>
                <li>Reinforcement Learning with Active Inference. <em>(Rating: 1)</em></li>
                <li>Planning and Active Inference. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1103",
    "paper_id": "paper-273023050",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "DT-AIF agent",
            "name_full": "Discrete-Time Active Inference self-learning research agent",
            "brief_description": "A hierarchical POMDP active inference agent that continually learns likelihood mappings (A matrix) using Dirichlet–Categorical updates and selects policies by minimising Expected Free Energy (balancing extrinsic and epistemic value). Implemented as a two-level generative model for a simulated research-process environment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Discrete-Time Active Inference self-learning agent",
            "agent_description": "Hierarchical discrete-time active inference agent operating on a two-level POMDP: top level models industry context (one state factor + industry cue observation), bottom level models industry × research-process state factors and three observation modalities (research process, outcome, industry cue). Posterior beliefs approximated by variational distribution Q(s); parameters of the likelihood (A matrix) are represented by Dirichlet concentration parameters and updated online with a Hebbian-like rule (a_{t+1} = ω·a_t + η·τ_o τ ⊗ s_τ). Policy selection uses Expected Free Energy (EFE) minimisation with explicit epistemic (information gain / novelty-seeking) and extrinsic terms.",
            "adaptive_design_method": "Information-gain maximization / curiosity-driven exploration implemented via Expected Free Energy minimisation (active inference)",
            "adaptation_strategy_description": "The agent adaptively selects policies by minimising Expected Free Energy, which combines extrinsic value (prior preferences) and epistemic value (expected information gain). The epistemic term explicitly quantifies expected change in beliefs about the A matrix (KL between posterior and prior over A), driving exploration toward novel informative observations. Simultaneously, the agent updates Dirichlet concentration parameters for the A matrix online using a Hebbian-like rule with learning rate η and forgetting rate ω, thereby adapting the generative model based on sampled observations.",
            "environment_name": "Simulated hierarchical research-process POMDP (16 industries × research processes)",
            "environment_characteristics": "Partially observable (POMDP), discrete state and observation spaces, stochastic (observations sampled from A matrix probabilities), hierarchical (two-level generative model), non-stationary / subject to regime shifts (mapped experiments with environment modifications across trials), simulated generative process that samples outcomes from the agent's A matrix mappings.",
            "environment_complexity": "Discrete: top-level industry factor with 16 industries; bottom-level includes industry × research-process combinations (reported as 16 industries × 4 research processes = 64 state-process pairs); outcome modality with multiple discrete outcomes (examples use ~5 outcome categories); experiments run in discrete iterations/trials (plots shown for first 10 and subsequent 20 trials); action space corresponds to selecting a research process (policy choices over research processes per industry).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: agent learns correct mappings quickly in the baseline environment (Industry 1 reaches a high score within ~6 iterations); after localized change (only Industry 1 modified) performance shows steep drop then linear recovery over 20 trials but does not reach prior maximum; after larger/global modifications across many industries, agent relearns faster and surpasses prior peak by trial 18. (No numerical cumulative-reward or accuracy numbers provided in paper text.)",
            "performance_without_adaptation": null,
            "sample_efficiency": "Qualitative sample-efficiency metrics reported: baseline — high-confidence mapping for Industry 1 reached within ~6 iterations; localized-change environment — linear improvement over 20 trials but did not reach former maximum; larger-change environment — overtakes previous maximum by trial 18. Exact sample counts to reach fixed numeric thresholds are not provided.",
            "exploration_exploitation_tradeoff": "Managed via Expected Free Energy: extrinsic (goal-directed) terms favour exploitation of preferred outcomes, while an explicit epistemic / novelty-seeking term (expected KL between q(A|o,s) and q(A)) drives exploration to reduce uncertainty about the A matrix. Policy selection minimises the combined EFE objective, trading off information gain against expected value.",
            "comparison_methods": "No explicit quantitative comparisons to alternative adaptive-design methods (e.g., random exploration, RL baselines, Bayesian optimisation) are reported; comparisons are qualitative across different non-stationary environment configurations (baseline vs localized vs global change).",
            "key_results": "1) Active inference with Dirichlet–Categorical learning of the A matrix enables continual online adaptation in discrete-time POMDPs; 2) Agent quickly learns baseline mappings (industry-specific outcomes) and can relearn after environment changes; 3) Localized changes (single-industry) produce slower, more gradual relearning due to reliance on prior beliefs, while global/regime shifts lead to faster, sometimes superior, relearning as the agent globally adjusts its model; 4) The epistemic term in EFE (expected information gain about A) provides a principled mechanism for adaptive experimental selection (curiosity-driven exploration) that focuses sampling on informative actions.",
            "limitations_or_failures": "1) Results are qualitative and plotted — no precise numerical metrics (e.g., cumulative reward, likelihood, sample complexity limits) are reported; 2) No formal baseline comparisons (random, RL, or other active-learning methods) are provided, limiting claims of relative advantage; 3) Evaluation in a simplified, simulated research environment rather than real-world data; 4) Sensitivity to hyperparameters (learning rate η, forgetting ω) and scoring metric design may affect apparent performance (paper notes score-inflation effects when many unchanged mappings exist); 5) Localized environmental changes caused slower adaptation and failure to reach prior maxima in one experiment; 6) The generative-process is simulated from the agent's A matrix (internal sampling), which may not capture complexities of real-world generative dynamics.",
            "uuid": "e1103.0",
            "source_info": {
                "paper_title": "Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The graphical brain: Belief propagation and active inference.",
            "rating": 2,
            "sanitized_title": "the_graphical_brain_belief_propagation_and_active_inference"
        },
        {
            "paper_title": "Active inference: The free energy principle in mind, brain, and behavior.",
            "rating": 2,
            "sanitized_title": "active_inference_the_free_energy_principle_in_mind_brain_and_behavior"
        },
        {
            "paper_title": "Active inference, belief propagation, and the free energy principle.",
            "rating": 2,
            "sanitized_title": "active_inference_belief_propagation_and_the_free_energy_principle"
        },
        {
            "paper_title": "Active Inference, Curiosity and Insight.",
            "rating": 2,
            "sanitized_title": "active_inference_curiosity_and_insight"
        },
        {
            "paper_title": "Reinforcement Learning with Active Inference.",
            "rating": 1,
            "sanitized_title": "reinforcement_learning_with_active_inference"
        },
        {
            "paper_title": "Planning and Active Inference.",
            "rating": 1,
            "sanitized_title": "planning_and_active_inference"
        }
    ],
    "cost": 0.00774,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference
30 Sep 2024</p>
<p>Rithvik Prakki rprakki@unc.edu 
University of North Carolina at Chapel Hill</p>
<p>Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference
30 Sep 2024DE67BDD89E362B15D18CA15F6CE1FF4FarXiv:2410.00240v1[cs.AI]
Active inference provides a powerful mathematical framework for understanding how agents-biological or artificial-interact with their environments, enabling continual adaptation and decision-making.It combines the principles of Bayesian inference and free energy minimization to model the process of perception, action, and learning in uncertain and dynamic contexts.Unlike reinforcement learning, active inference integrates both exploration and exploitation seamlessly, driven by a unified objective to minimize expected free energy.In this paper, we present a continual learning framework for agents operating in discrete time environments, using active inference as the foundation.We derive the core mathematical formulations of variational and expected free energy and apply these principles to the design of a self-learning research agent.This agent continually updates its beliefs and adapts its actions based on new data, without manual intervention.Through experiments in dynamically changing environments, we demonstrate the agent's ability to relearn and refine its internal models efficiently, making it highly suitable for complex and volatile domains such as quantitative finance and healthcare.We conclude by discussing how the proposed framework generalizes to other systems and domains, positioning active inference as a robust and flexible approach for adaptive artificial intelligence.</p>
<p>Introduction</p>
<p>The Free Energy Principle (FEP), proposed by Friston, provides a unifying computational framework that integrates learning, perception, action, and decision-making.This principle posits that biological systems (or artificial agents) maintain themselves in their characteristic states by minimizing the difference between the predictions of their internal model and their actual sensory data, a quantity known as free energy.In essence, FEP suggests that living systems are driven by the need to reduce surprise in their interactions with the environment, formalized through the minimization of variational free energy.</p>
<p>Active Inference and Free Energy Principle</p>
<p>At the core of Active Inference (AIF), which operationalizes the Free Energy Principle, are two key objective functions:</p>
<p>• Variational Free Energy (VFE): This function quantifies the fitness of an agent's internal model concerning the sensory observations it receives from its environment by mapping latent (hidden) states to sensory outcomes.The agent minimizes this free energy to maintain coherence between its beliefs about the world and incoming sensory evidence.</p>
<p>• Expected Free Energy (EFE): This function governs the agent's policy selection by combining extrinsic value (goal-oriented behavior, aligned with the agent's prior preferences) and epistemic value (exploration-driven behavior to reduce uncertainty in the agent's model of the world).</p>
<p>Discrete-Time Active Inference is framed in the context of Partially Observable Markov Decision Processes (POMDPs), where agents use sensory and proprioceptive information to form probabilistic beliefs about the hidden (latent) states of the world.</p>
<p>Mathematical Derivation of Variational Free Energy (VFE)</p>
<p>Variational Free Energy (VFE) is a quantity that measures the dissimilarity between an agent's internal model of the world and the real-world sensory data it receives.It is used to approximate Bayesian inference in scenarios where calculating exact posteriors is computationally intractable.To derive VFE, we begin with Bayes' rule: P (s|o) = P (o|s)P (s)
P (o)(1)
Here, P (s|o) is the posterior distribution over hidden states s given observations o, P (o|s) is the likelihood (i.e., how likely the observations are given the hidden states), P (s) is the prior distribution over hidden states, and P (o) is the marginal likelihood, also known as the evidence.</p>
<p>In active inference, the agent cannot compute this posterior P (s|o) directly, as the marginal likelihood P (o) is intractable due to the high-dimensional nature of real-world data.Instead, the agent approximates the posterior with a simpler distribution Q(s), known as the variational distribution, which the agent iteratively improves.The agent minimizes the difference between the true posterior P (s|o) and the variational approximation Q(s) using the Kullback-Leibler (KL) divergence:
D KL [Q(s)∥P (s|o)] = Q(s) ln Q(s) P (s|o) ds(2)
Since the true posterior P (s|o) is unknown, we aim to minimize this divergence indirectly by minimizing the free energy F , which is an upper bound on the negative log-evidence, − ln P (o).To express this in terms of known quantities, we substitute Bayes' rule into the expression for the KL divergence:
D KL [Q(s)∥P (s|o)] = Q(s) ln Q(s) P (o|s)P (s)/P (o) ds(3)
This can be split into three terms:
D KL [Q(s)∥P (s|o)] = Q(s) ln Q(s) P (s) ds − Q(s) ln P (o|s)ds + ln P (o)(4)
Thus, the KL divergence becomes:
D KL [Q(s)∥P (s|o)] = D KL [Q(s)∥P (s)] − E Q(s) [ln P (o|s)] + ln P (o)(5)
In this expression, D KL [Q(s)∥P (s)] is the KL divergence between the approximate posterior Q(s) and the prior P (s), which we call the complexity term.The second term E Q(s) [ln P (o|s)] is the expected log-likelihood of the observations under the approximate posterior, known as the accuracy term.</p>
<p>The final term, ln P (o), is the log-evidence (also known as the marginal likelihood), which does not depend on Q(s) and is therefore a constant with respect to the minimization of free energy.</p>
<p>Since ln
P (o) is independent of Q(s), minimizing the KL divergence D KL [Q(s)∥P (s|o)
] is equivalent to minimizing the variational free energy F , which we define as:
F [Q(s)] = D KL [Q(s)∥P (s)] − E Q(s) <a href="6">ln P (o|s)</a>
This free energy consists of two competing terms:
• Complexity: D KL [Q(s)∥P (s)]
, which penalizes divergence between the approximate posterior and the prior.Lowering this term encourages simplicity in the model, favoring posteriors Q(s) that are close to the prior P (s).</p>
<p>• Accuracy: E Q(s) [ln P (o|s)], which measures how well the model's predictions P (o|s) fit the observed data o.Maximizing this term ensures that the model is accurately predicting sensory observations.</p>
<p>The free energy F can be minimized by iteratively updating the approximate posterior Q(s), driving the agent to strike a balance between complexity (favoring simpler models) and accuracy (favoring models that predict observations well).</p>
<p>Expected Free Energy (EFE)</p>
<p>While Variational Free Energy (VFE) focuses on inferring hidden states based on current observations, Expected Free Energy (EFE) extends this principle into the future by evaluating potential actions (or policies) that an agent can take.Just as VFE is used to update the agent's beliefs about the hidden states s given observations o, EFE evaluates the likely outcomes of different policies π, guiding the agent to select actions that minimize future free energy.</p>
<p>Similar to VFE, the derivation of EFE follows from Bayes' rule.However, instead of inferring hidden states based solely on observations, we now consider how policies π influence both the hidden states and the observations.The posterior over hidden states and policies is given by:
P (s|o, π) = P (o|s, π)P (s, π) P (o, π)(7)
Here, P (s|o, π) is the posterior over hidden states given observations and policies, P (o|s, π) is the likelihood of the observations conditioned on both the hidden states and the policy, P (s, π) is the joint prior over states and policies, and P (o, π) is the marginal likelihood of observations and policies.</p>
<p>From this starting point, we can derive the expected free energy, G π , following a similar process as we used for VFE.The goal of EFE is to select policies that minimize the free energy expected over future observations, balancing the agent's desire to gather information (epistemic value) and achieve its goals (extrinsic value).</p>
<p>Learning in Active Inference: The Dirichlet-Categorical Model</p>
<p>In Active Inference, learning occurs by updating the generative model's parameters based on new observations.This process is typically modeled with a Dirichlet-Categorical framework, where Bayesian inference is performed using a categorical distribution as the likelihood and a Dirichlet distribution as the prior.</p>
<p>A Dirichlet distribution, denoted by Dir(θ|α), is defined over probability vectors and serves as the conjugate prior for the categorical distribution.This allows for sequential updating of beliefs as new data is observed.The Dirichlet distribution is expressed as:
p(θ|α) = Γ K k=1 α k K k=1 Γ(α k ) K k=1 θ α k −1 k (8)
Where α = (α 1 , α 2 , . . ., α K ) are concentration parameters, and Γ(•) is the gamma function.The parameters α k represent prior counts that encode the agent's confidence in each possible outcome before observing new data.</p>
<p>As the agent receives new observations, these concentration parameters are updated through a process analogous to "counting" the co-occurrences of certain states and observations over time.The posterior distribution over parameters θ, given new observations x, becomes:
p(θ|x, α) ∝ Dir(θ|α + x)(9)
This form of learning enables the agent to accumulate evidence over time, updating its beliefs about state-outcome mappings in a flexible and adaptive manner.</p>
<p>Learning the A Matrix</p>
<p>A crucial part of the generative model in Active Inference is the A matrix, which encodes the likelihood mapping from hidden states s to observations o.Learning the A matrix is central to the agent's ability to adapt to its environment, as it allows the agent to refine its understanding of how hidden states generate sensory data.</p>
<p>The update rule for the A matrix is expressed as:
a t+1 = ω • a t + η • τ o τ ⊗ s τ(10)
Where:</p>
<p>• a t are the concentration parameters for the A matrix at time t,</p>
<p>• o τ is the observation at time τ ,</p>
<p>• s τ is the inferred hidden state at time τ ,</p>
<p>• η is the learning rate that controls the adaptation speed, and</p>
<p>• ω is the forgetting rate that controls the extent to which past observations are discounted.</p>
<p>This Hebbian-like learning rule updates the A matrix by strengthening associations between states and observations that co-occur.For example, if the agent observes o = [1, 0, 0] T while in state s = [1, 0] T , the association between state 1 and observation 1 is reinforced.This allows the agent to build stronger beliefs about the causal structure of the environment.</p>
<p>In addition, the process of policy selection relies on the minimization of Expected Free Energy (EFE), which includes a novelty-seeking term that drives exploration.This term quantifies the expected change in beliefs as a result of receiving a new observation.The EFE in this context is:
G π = D KL [q(o τ |π)∥p(o τ )] + E q(sτ |π) [H[p(o τ |s τ )]] − E p(oτ |sτ )q(sτ |π) <a href="11">D KL [q(A|o τ , s τ )∥q(A)]</a>
The third term drives the agent to seek novel observations that maximize the difference between prior and posterior beliefs about the mappings in the A matrix.By doing so, the agent can actively explore and refine its internal model, improving its understanding of how states generate observations.</p>
<p>Application to Self-Learning Agents</p>
<p>In this paper, we extend this framework to the design of self-learning agents capable of continually updating their generative models to adapt to changing environments.Specifically, we focus on the learning of the A matrix, as it is essential for the agent's ability to infer the dynamics of its environment.By using Active Inference and Dirichlet-Categorical models, we propose a mechanism for integrating continual learning into agentic systems, allowing agents to optimize their performance over time while responding to novel observations.This ability to learn continually over time ensures that the agent can interact with dynamic environments without requiring constant updates or interventions by a human operator.This capability is especially relevant in domains such as quantitative finance, where strategies can become obsolete over time due to market fluctuations, or in research, where resources constantly evolve in their relevance.In these scenarios, Active Inference provides an abstraction layer that continuously tunes the agent's performance, thus adapting to new challenges in real-time.</p>
<p>2 Active Inference as an Agentic Learning Mechanism</p>
<p>Motivating the Problem</p>
<p>For every aspect of an agentic workflow, there are elements that need to be optimized over time.Consider an agent that operates within a dynamic environment, where the relationships between states, observations, and actions continuously evolve.If the system lacks the ability to adapt to these changes, its performance will degrade.Traditional methods require manual intervention, continual updates, or predefined structures that can become obsolete.</p>
<p>However, with Active Inference, by simply defining the environment and creating pathways for the agent to act on and receive observations from the environment, we can build agents that are self-sustaining and highly adaptable over time.Active Inference agents can automatically learn new structures and processes as needed without the need for external updates, making them resilient to change.</p>
<p>Developing the Framework</p>
<p>This section develops the logic behind the active inference framework used for implementing self-learning within any agentic workflow.While the following discussion focuses on an example research agent, the framework can be generalized to other systems and domains.</p>
<p>Definitions</p>
<p>Operating in discrete time, a Partially Observable Markov Decision Process (POMDP) models states, observations, policies, and all inputs and outputs of Free Energy.In this framework, the A matrix represents the likelihood mapping from hidden states to observations.The state factors and observation modalities are learned over time rather than pre-specified, allowing the agent to adapt as new data becomes available.</p>
<p>For example, in a research process, the state factors may represent different research methods, while the observation modalities represent the usefulness of a research output.Over time, the system refines its understanding of which research methods yield the most useful results in a given context.</p>
<p>Generative Model</p>
<p>In this framework, the generative model is hierarchical, structured across two levels.This hierarchy is crucial for providing context to the agent, enabling it to understand both the environment it is interacting with and the specific processes within that environment.</p>
<p>The top level of the generative model consists of one state factor: the industry, and one observation modality: the industry cue.The purpose of this level is to provide the agent with an understanding of the high-level context-essentially, which industry it is interacting with at a given time.The agent observes an industry cue that informs it about the current industry in which the research processes are unfolding.This ensures that the agent can interpret all subsequent observations with the knowledge of the specific industry it is dealing with.</p>
<p>The bottom level of the generative model consists of two state factors: the industry and the research process, with three observation modalities:</p>
<p>• Research process: Observes which research process is being used.</p>
<p>• Outcome: Observes the result of the research process in a given industry (i.e., a mapping between the research process and industry).</p>
<p>• Industry cue: Observes the industry cue again, passed down from the top level, to maintain awareness of the industry context.</p>
<p>Thus, the agent is provided with both state and observation information from the top level (industry and its cue), which is passed down to the bottom level to act as a continuous context during its exploration of research processes.When the agent observes data from the research process, it knows which industry these observations are tied to, based on the industry cue provided at the top level.The observation of the research process itself allows the agent to recognize which process is being applied, while the outcome provides information about the result of applying that research process in the given industry.</p>
<p>The outcome in this simplified model is a preset simulation that maps each research process to a specific result within the industry.For example, in Industry 1, Research Process 1 may always yield an "Excellent" result, while Research Process 2 may yield a "Good" result, and so on.In more practical applications, the outcome would be determined directly by interacting with the environment, but in this simplified model, the outcome serves as a simulation to help the agent learn these mappings.</p>
<p>By structuring the generative model in this hierarchical way, the agent is capable of maintaining awareness of its industry context while refining its understanding of how different research processes perform within that context.The hierarchical POMDP ensures that observations about industry cues are continuously processed, providing the necessary context for learning the dynamic relationships between research processes and their outcomes.</p>
<p>Generative Process</p>
<p>The generative process in this model simulates the environment that the agent interacts with.In an ideal scenario, the generative process would involve real-world data being generated by the environment in response to the agent's actions.However, for the purpose of this research model, the generative process is simplified and simulated based on the state-observation mappings present in the A matrix.</p>
<p>In this setup, the generative process takes the agent's policy selections and uses them to sample observations from the predefined mappings in the A matrix.Specifically, after the agent selects a policy (which represents its action in terms of selecting a research process), the generative process samples from the A matrix to determine the corresponding outcome for the given research process in the current industry.The A matrix contains the probabilities that govern how observations (i.e., outcomes) are generated based on the agent's belief about the current state (industry and research process).</p>
<p>For example, if the agent selects Research Process 1 in Industry 1, the A matrix may indicate that there is a high probability that the outcome will be "Excellent."The generative process samples from this probability distribution, providing the agent with an observation of "Excellent."The agent then uses this observation to update its beliefs about the mappings between research processes and outcomes in the current industry.</p>
<p>The generative process is essentially a simulation of how the environment might behave, providing feedback to the agent about its actions.The agent is tasked with learning the structure of the environment by minimizing Expected Free Energy (EFE).This involves selecting policies that are expected to reduce the agent's uncertainty about the state-observation mappings (i.e., learning the A matrix).</p>
<p>Because the agent is providing observations to itself via the sampling of the A matrix, the process of learning becomes one of continuous refinement.Over time, the agent uses the observations it generates (from its interaction with the environment) to improve its internal model, refining its understanding of how different research processes produce outcomes in different industries.This allows the agent to optimize its performance, selecting policies that are expected to maximize the accuracy of its predictions about the environment.</p>
<p>Thus, the generative process in this framework is not only responsible for generating observations but also for driving the agent's learning by simulating feedback loops between the agent's policies and the environment it is modeling.</p>
<p>Results</p>
<p>To demonstrate the continual learning capabilities of the active inference agent, we tested it in two different environments: the first with predefined state-outcome mappings and the second with modified mappings for certain industries to simulate a shift in the environment.We used a scoring mechanism based on the agent's confidence in the correct outcomes, which is calculated from the belief values assigned to each possible outcome in the lowercase 'a' matrix.This matrix represents the agent's internal belief mappings between states (industries and research processes) and observations (outcomes).</p>
<p>Scoring Mechanism</p>
<p>To evaluate the agent's learning performance, we define a scoring mechanism that reflects both the accuracy and confidence of the agent in its internal belief state.The agent's belief about the relationship between the hidden states (e.g., industries and research processes) and observations (e.g., outcomes) is represented by the lowercase 'a' matrix, denoted by a ijk , where:</p>
<p>• i indexes the possible outcomes (e.g., Excellent, Good, etc.),</p>
<p>• j indexes the hidden states (e.g., industry),</p>
<p>• k indexes the research processes.</p>
<p>For each combination of hidden state j and research process k, the agent assigns a probability a ijk to each outcome i, which reflects the agent's belief in the likelihood of that outcome.</p>
<p>Score Calculation</p>
<p>The score for each state-process pair (j, k) is calculated based on the agent's confidence in the correct outcome relative to its confidence in the incorrect outcomes.Let i * denote the index of the correct outcome for the given pair (j, k), and let i max denote the index of the incorrect outcome with the highest belief value (i.e., the highest a ijk where i ̸ = i * ).</p>
<p>The score S jk for the pair (j, k) is defined as the difference between the agent's belief in the correct outcome and the highest belief in any incorrect outcome:
S jk = a i * jk − max i̸ =i * a ijk .
Thus, if the agent has a high belief in the correct outcome a i * jk and low belief in the incorrect outcomes, S jk will be positive.Conversely, if the agent incorrectly assigns higher belief to an incorrect outcome, the score will be negative.</p>
<p>Total Score for an Iteration</p>
<p>The total score for an iteration t, denoted S (t) , is calculated as the sum of normalized scores across all industries j and research processes k:
S (t) = 16 j=1 4 k=1 S norm jk .
This total score reflects the overall performance of the agent in the current iteration, taking into account how well the agent has learned the correct mappings for each industry and research process.</p>
<p>Negative Scores</p>
<p>Negative scores occur when the agent's belief in the correct outcome is lower than the belief in an incorrect outcome.For example, if the belief values for Excellent and Good were reversed:
a 11k = 0.2, a 21k = 5, a 31k = 0.1, a 41k = 0.1, a 51k = 0.05,
then the score would be:
S 1k = 0.2 − 5 = −4.8,
indicating the agent has mistakenly assigned much higher confidence to the incorrect outcome.</p>
<p>Overall Evaluation</p>
<p>By calculating and summing the scores across all industries and research processes, we obtain a measure of the agent's overall learning performance and how well it adapts to changes in the environment.This scoring mechanism allows us to assess both the accuracy and confidence of the agent's beliefs about the environment.</p>
<p>First Environment: Predefined Outcomes</p>
<p>The first environment consists of predefined state-outcome mappings for each of the 16 industries and their respective research processes.The agent begins by interacting with the environment, learning these mappings over the first 10 trials.The first 10 trials shown in 2) and 3) are both the calculated score per iteration for just state-observation mappings in the 1st industry and state-observation mappings for all industries, respectively.</p>
<p>Figure 2: The results of the first 10 trials in the original environment.The agent quickly learns the dynamics of the environment, reaching a high score for Industry 1 within six iterations, demonstrating that the active inference agent is able to learn environmental dynamics effectively.</p>
<p>Second Environment: First Industry Modified</p>
<p>The second 20 trials in 2) were carried out by the same active inference agent placed in a new environment in which only the state-observation mappings for the first industry were changed.The mappings for the other 15 industries were the same, so only the score for Industry 1 was calculated.The steep drop-off is caused by an environment change where none of the agents' former beliefs are relevant.The agent is able to learn this new environment in a linearly increasing fashion, but doesn't reach its former maximum.</p>
<p>Figure 3: Learning progress of the active inference agent across all industries in the original environment.After an initial drop due to uncertainty, the agent learns the correct mappings, achieving a higher score over 20 trials.</p>
<p>Third Environment: More Modifications</p>
<p>The second 20 trials in 3) were carried out by the active inference agent initially trained in environment 1 being placed in a new environment.Here several state-observation mappings were changed across a much bigger set of industries, representing a bigger regime shift in this environment than in the second environment.</p>
<p>Here the agent is able to relearn the environment once again, but at a higher rate than in the second environment.In fact, the agent overtakes its previous maximum score by the 18th trial in the new environment.</p>
<p>Conclusion</p>
<p>The results from the different environments demonstrate key properties of Active Inference agents in terms of adaptation and relearning.</p>
<p>In the second environment, where only one industry's mappings were changed, the agent required more iterations to relearn the altered relationships, which suggests that localized changes lead to more gradual adaptation.The slower rate of learning can be attributed to the agent's reliance on prior beliefs about the unchanged aspects of the environment, which can introduce friction when adapting to small, isolated changes.The lower learning rate can also potentially be attributed to the score calculation.Since the score was only being calculated on the 1st industry it didn't have the benefit of ballooning the score due to increased confidence from other industries.</p>
<p>In contrast, the third environment, where more industries experienced changes, showed faster adaptation.This can be interpreted as the agent being more flexible when faced with global environmental shifts.With multiple industries changing simultaneously, the agent can globally adjust its model, resulting in faster convergence and even surpassing the peak performance seen in the first environment.This quicker learning rate may also be due to the opposite effect potentially seen in environment two.Since, there are several unchanged mappings they're increasing scores may have ballooned the overall score more than expected.</p>
<p>These results suggest that the structure of the environmental changes may impact adaptation rate.The results conclusively show that the active inference agent is able to reassess its views when placed in changing environments and adapt its internal model in response.</p>
<p>Discussion</p>
<p>This paper demonstrates the potential of Active Inference as a framework for creating self-learning agents that continuously adapt to their environment.By engaging in continual interaction with their surroundings and refining internal models, Active Inference agents are able to autonomously adjust to changes over time without requiring manual intervention.This feature makes Active Inference highly applicable in domains where adaptability is crucial, such as research, quantitative finance, and healthcare.</p>
<p>Application to Quantitative Finance</p>
<p>In quantitative finance, trading strategies often become obsolete due to shifting market conditions.Using an Active Inference agent, we can build a system where the agent continuously observes market indicators like asset prices, volatility, and economic factors.The state factors could include market conditions and interest rates, while observation modalities would encompass price movements and volume.</p>
<p>The agent would learn and update its internal model based on real-time data, adjusting its trading strategies as market conditions evolve.The generative process simulates possible future market scenarios, enabling the agent to select policies that minimize risk and maximize returns.This adaptability makes the agent resilient to rapid market fluctuations, reducing the need for constant manual updates to trading strategies.</p>
<p>Application to Healthcare</p>
<p>In healthcare, patient data changes over time, requiring constant adaptation in treatment plans.An Active Inference agent could be applied to a clinical decision support system where state factors include patient health and treatment options, and observation modalities could involve test results and symptom reports.</p>
<p>The agent would continuously update its model based on incoming patient data, refining its understanding of which treatments are most effective.By simulating different treatment outcomes, the agent can recommend the best course of action, ensuring that treatment strategies evolve as the patient's condition changes.This continual learning leads to more personalized and effective healthcare without constant human intervention.</p>
<p>Future Work</p>
<p>Moreover, future work could explore more sophisticated generative models and processes, allowing agents not only to learn new mappings but also to discover new state factors and observation modalities.This would enable agents to move beyond pre-defined structures, learning entirely new frameworks as they interact with their environment.Additionally, integrating Active Inference with other AI paradigms, such as large language models or reinforcement learning, could yield even more adaptable and capable agents.Here, active inference agents would serve as a mechanism to increase the temporal effectiveness of other agents they can manipulate.</p>
<p>Active Inference offers significant potential for real-world applications that require resilience and longterm sustainability.Whether in finance, healthcare, or other fields, this framework has the ability to keep AI systems relevant and adaptive, reducing the need for human intervention and making it a key tool for the future of adaptive artificial intelligence.</p>
<p>Data Availability</p>
<p>Find all relevant code and figures at https://github.com/RPD123-byte/Demonstrating-the-Continual-Learning-Capabilities-and-Practical-Application-of-Discrete-Time-Active.Subroutines for spm-MDP-VB-X and spm-MDP-check among others can be found in the spm12 package.</p>
<p>Figure 1 :
1
Figure 1: Hierarchical POMDP showing research methods and their outcomes.The agent adapts to changes in the research environment by updating the state and observation factors.This structure allows for continual learning as new information becomes available.</p>
<p>The graphical brain: Belief propagation and active inference. K Friston, T Parr, B De Vries, 10.1162/NETN_a_00018Network Neuroscience. 142017</p>
<p>Active inference: The free energy principle in mind, brain, and behavior. T Parr, G Pezzulo, K J Friston, Journal of Mathematical Psychology. 1001023642021</p>
<p>The free energy principle for action and perception: A mathematical review. C L Buckley, C S Kim, S Mcgregor, A K Seth, 10.1007/s00422-019-00805-wBiological Cybernetics. 11262017</p>
<p>Advances in Active Inference. J Smith, M Johnson, Trends in Cognitive Sciences. 2023</p>
<p>The Physics of Free Will. H R Brown, K J Friston, Neuroscience and Biobehavioral Reviews. 902018</p>
<p>Active Inference, Curiosity and Insight. P Schwartenbeck, K Friston, Neural Computation. 29102017</p>
<p>Predictive Coding: A Theoretical and Experimental Review. B Millidge, A Tschantz, C L Buckley, 2020</p>
<p>Y Zhang, Z Zheng, Reinforcement Learning with Active Inference. 2023</p>
<p>Active Inference with State-Only Control. L Dandoy, M Di Francesco, 2023</p>
<p>Planning and Active Inference. N Sajid, K Friston, T Parr, 2021</p>
<p>The role of the free energy principle in cognitive systems. S Shipp, 10.1177/26339137231222481Cognitive Science. 1722023</p>
<p>Active inference, belief propagation, and the free energy principle. P Schwartenbeck, T Fitzgerald, C Mathys, R Dolan, K Friston, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0277199PLOS ONE. 1711e02771992023</p>            </div>
        </div>

    </div>
</body>
</html>
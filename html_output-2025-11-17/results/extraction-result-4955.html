<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4955 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4955</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4955</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-e0f27336698c84709bd60b6b7f4ce588cbae66bf</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e0f27336698c84709bd60b6b7f4ce588cbae66bf" target="_blank">StructGPT: A General Framework for Large Language Model to Reason over Structured Data</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> An Iterative Reading-then-Reasoning approach for solving question answering tasks based on structured data, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\ie \emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\ie \emph{reasoning}). Specially, we propose an \emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\url{https://github.com/RUCAIBox/StructGPT}.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4955.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4955.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRR / StructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Reading-then-Reasoning (IRR) framework (StructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interface-augmented, iterative procedure (invoke → linearize → generate) that disentangles reading (structured-data access via specialized interfaces) from reasoning (LLM generation) so LLMs can iteratively collect evidence and produce answers or executable queries over KGs, tables, and DBs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Davinci-003 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI Davinci-003, a GPT-3.5 family instruction-following large language model used via API in zero-shot and few-shot prompting (June 2023 version in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Iterative Reading-then-Reasoning (IRR)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>LLM calls a small set of structured-data interfaces (e.g., Extract_Column_Name, Extract_Triples) to retrieve constrained evidence, linearizes retrieved structured fragments into text, and iteratively prompts the LLM to (a) select relevant evidence and (b) generate answers or formal queries; iterations continue until answer or executable SQL is produced. Diversity of reasoning paths is not introduced via ensembling; the method uses sequential iterative refinement over retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KGQA, TableQA, Text-to-SQL (multiple benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>KG-based question answering (WebQSP, MetaQA), table question answering and verification (WTQ, WikiSQL, TabFact), and database text-to-SQL parsing (Spider and variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Davinci-003 + IRR performance (selected metrics): KGQA WebQSP Hits@1 71.9 (vs baseline 48.3); MetaQA-2hop 59.5 (baseline 25.3); TableQA WTQ denotation 39.2 (baseline 34.8), WikiSQL 51.8 (baseline 49.1), TabFact 76.5 (baseline 80.7); Text-to-SQL Spider execution acc 69.5 (baseline 68.8). (Values from paper Tables 1-3.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Davinci-003 direct zero-shot prompting (single-pass) baseline: KGQA WebQSP 48.3, MetaQA-2hop 25.3; TableQA WTQ 34.8, WikiSQL 49.1; Text-to-SQL Spider 68.8.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The IRR iterative, interface-augmented method substantially improves Davinci-003's zero-shot performance on many structured-data reasoning tasks (notably large gains on KGQA multi-hop tasks and some table/db tasks) by constraining reading and focusing the LLM on reasoning steps; iterative evidence selection helps overcome limitations of feeding entire structured data into a single prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Improvements are not uniform: on TabFact Davinci-003 + IRR (76.5) was lower than Davinci-003 direct baseline (80.7). Gains on some Text-to-SQL and table tasks are modest. Also a few-shot + IRR sometimes behaved unexpectedly (e.g., Davinci-003 WebQSP few-shot 71.0 slightly below zero-shot + IRR 71.9), indicating that adding exemplars does not guarantee monotonic improvement in every setting. Error analysis shows substantial selection and format/generation errors remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StructGPT: A General Framework for Large Language Model to Reason over Structured Data', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4955.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4955.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRR / StructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Reading-then-Reasoning (IRR) framework (StructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interface-augmented, iterative procedure (invoke → linearize → generate) that disentangles reading (structured-data access via specialized interfaces) from reasoning (LLM generation) so LLMs can iteratively collect evidence and produce answers or executable queries over KGs, tables, and DBs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, June/August versions used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI ChatGPT (gpt-3.5-turbo) used via API for zero-shot and few-shot prompting (June and Aug versions referenced); strong instruction-following behavior leveraged in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Iterative Reading-then-Reasoning (IRR)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>LLM iteratively queries specialized structured-data interfaces to retrieve focused evidence, linearizes the retrieved fragments, and is prompted to select relevant evidence and then generate answers or executable queries; the process repeats until a final answer/SQL is derived. This is sequential iterative refinement rather than ensembling of diverse reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KGQA, TableQA, Text-to-SQL (multiple benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>KG-based QA (WebQSP, MetaQA), table QA & verification (WTQ, WikiSQL, TabFact), and DB text-to-SQL parsing (Spider and variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ChatGPT + IRR performance (selected metrics): KGQA WebQSP Hits@1 72.6 (vs baseline 61.2); MetaQA-2hop 93.9 (baseline 31.0); TableQA WTQ denotation 48.4 (baseline 43.3), WikiSQL 54.4 (baseline 51.6), TabFact 87.1 (baseline 82.9); Text-to-SQL Spider execution acc 74.8 (baseline 70.1). (Values from paper Tables 1-3.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>ChatGPT direct zero-shot prompting (single-pass) baseline: KGQA WebQSP 61.2, MetaQA-2hop 31.0; TableQA WTQ 43.3, WikiSQL 51.6; Text-to-SQL Spider 70.1.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRR yields large improvements for ChatGPT across datasets, especially on multi-hop KGQA (e.g., MetaQA 2/3-hop), and improves Text-to-SQL and table QA metrics. Iterative evidence extraction via interfaces enables ChatGPT to focus reasoning on relevant facts and reduces distractors from large structured contexts. Few-shot in-context exemplars combined with IRR often produce further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although generally beneficial, IRR did not always produce uniform improvements across all metrics/versions; some small tasks or specific metrics show modest gains. The authors report remaining dominant error types (KG selection errors, SQL reasoning errors) and generation-format errors that limit end-to-end gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StructGPT: A General Framework for Large Language Model to Reason over Structured Data', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4955.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4955.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct single-pass zero-shot LLM prompting (no IRR interfaces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach feeding question (and in some table/text-to-SQL cases, full structured data) into an LLM prompt in a single pass without iterative interface-based retrieval or evidence selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Davinci-003 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI Davinci-003, GPT-3.5 family instruction-following LLM used in zero-shot prompting baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Direct single-pass prompting (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The LLM is given the question (and possibly full linearized structured data) in a single prompt and asked to produce an answer or SQL in one generation step; there is no iterative retrieval or multiple alternative reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KGQA, TableQA, Text-to-SQL (benchmarks listed in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same structured-data QA and semantic parsing tasks used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Davinci-003 direct zero-shot baseline performance (selected): KGQA WebQSP 48.3 Hits@1; MetaQA-2hop 25.3; TableQA WTQ 34.8 denotation; WikiSQL 49.1; Text-to-SQL Spider 68.8 execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Davinci-003 + IRR: WebQSP 71.9, MetaQA-2hop 59.5, WTQ 39.2, WikiSQL 51.8, Spider 69.5 (see paper Tables 1-3).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Direct single-pass prompting provides a reasonable baseline but underperforms compared to the IRR iterative interface-augmented method, especially for multi-hop KGQA and many structured-data tasks where restricting retrieval and iteratively reasoning is helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For some tasks/datasets (e.g., TabFact for Davinci-003) the direct baseline performs comparably or even better than IRR in the paper's reported numbers (TabFact: baseline 80.7 vs IRR 76.5), showing IRR is not universally superior and that dataset/task characteristics matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StructGPT: A General Framework for Large Language Model to Reason over Structured Data', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4955.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4955.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct single-pass zero-shot LLM prompting (no IRR interfaces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach feeding question (and in some table/text-to-SQL cases, full structured data) into an LLM prompt in a single pass without iterative interface-based retrieval or evidence selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI ChatGPT (gpt-3.5-turbo), used as a zero-shot baseline prompting the model directly with question ± structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Direct single-pass prompting (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The model receives the question and maybe linearized structured data in a single prompt and is expected to output the answer or SQL in one generation call; no iterative retrieval or evidence-selection interfaces are used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KGQA, TableQA, Text-to-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as above (WebQSP, MetaQA, WTQ, WikiSQL, TabFact, Spider, Spider-SYN, SpiderRealistic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ChatGPT direct zero-shot baseline (selected): KGQA WebQSP 61.2 Hits@1; MetaQA-2hop 31.0; TableQA WTQ 43.3, WikiSQL 51.6, TabFact 82.9; Text-to-SQL Spider 70.1 execution acc.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>ChatGPT + IRR: WebQSP 72.6, MetaQA-2hop 93.9, WTQ 48.4, WikiSQL 54.4, TabFact 87.1, Spider 74.8 (see paper Tables 1-3).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ChatGPT's direct prompting baseline is strong on some tasks but IRR significantly improves ChatGPT especially on multi-hop KGQA and Text-to-SQL. The iterative, interface-guided evidence selection reduces distractors and boosts reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although IRR improves most metrics, differences across datasets and versions show non-uniform gains; for some tasks the baseline is already competitive and the absolute improvement is smaller. The paper also notes residual error categories (selection, reasoning, format, hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StructGPT: A General Framework for Large Language Model to Reason over Structured Data', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>Don't generate, discriminate: A proposal for grounding language models to real-world environments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4955",
    "paper_id": "paper-e0f27336698c84709bd60b6b7f4ce588cbae66bf",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "IRR / StructGPT",
            "name_full": "Iterative Reading-then-Reasoning (IRR) framework (StructGPT)",
            "brief_description": "An interface-augmented, iterative procedure (invoke → linearize → generate) that disentangles reading (structured-data access via specialized interfaces) from reasoning (LLM generation) so LLMs can iteratively collect evidence and produce answers or executable queries over KGs, tables, and DBs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Davinci-003 (text-davinci-003)",
            "model_description": "OpenAI Davinci-003, a GPT-3.5 family instruction-following large language model used via API in zero-shot and few-shot prompting (June 2023 version in experiments).",
            "reasoning_method_name": "Iterative Reading-then-Reasoning (IRR)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "LLM calls a small set of structured-data interfaces (e.g., Extract_Column_Name, Extract_Triples) to retrieve constrained evidence, linearizes retrieved structured fragments into text, and iteratively prompts the LLM to (a) select relevant evidence and (b) generate answers or formal queries; iterations continue until answer or executable SQL is produced. Diversity of reasoning paths is not introduced via ensembling; the method uses sequential iterative refinement over retrieved evidence.",
            "task_name": "KGQA, TableQA, Text-to-SQL (multiple benchmarks)",
            "task_description": "KG-based question answering (WebQSP, MetaQA), table question answering and verification (WTQ, WikiSQL, TabFact), and database text-to-SQL parsing (Spider and variants).",
            "performance": "Davinci-003 + IRR performance (selected metrics): KGQA WebQSP Hits@1 71.9 (vs baseline 48.3); MetaQA-2hop 59.5 (baseline 25.3); TableQA WTQ denotation 39.2 (baseline 34.8), WikiSQL 51.8 (baseline 49.1), TabFact 76.5 (baseline 80.7); Text-to-SQL Spider execution acc 69.5 (baseline 68.8). (Values from paper Tables 1-3.)",
            "comparison_with_other_method": true,
            "performance_other_method": "Davinci-003 direct zero-shot prompting (single-pass) baseline: KGQA WebQSP 48.3, MetaQA-2hop 25.3; TableQA WTQ 34.8, WikiSQL 49.1; Text-to-SQL Spider 68.8.",
            "key_findings": "The IRR iterative, interface-augmented method substantially improves Davinci-003's zero-shot performance on many structured-data reasoning tasks (notably large gains on KGQA multi-hop tasks and some table/db tasks) by constraining reading and focusing the LLM on reasoning steps; iterative evidence selection helps overcome limitations of feeding entire structured data into a single prompt.",
            "counter_examples_or_negative_results": "Improvements are not uniform: on TabFact Davinci-003 + IRR (76.5) was lower than Davinci-003 direct baseline (80.7). Gains on some Text-to-SQL and table tasks are modest. Also a few-shot + IRR sometimes behaved unexpectedly (e.g., Davinci-003 WebQSP few-shot 71.0 slightly below zero-shot + IRR 71.9), indicating that adding exemplars does not guarantee monotonic improvement in every setting. Error analysis shows substantial selection and format/generation errors remain.",
            "uuid": "e4955.0",
            "source_info": {
                "paper_title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "IRR / StructGPT",
            "name_full": "Iterative Reading-then-Reasoning (IRR) framework (StructGPT)",
            "brief_description": "An interface-augmented, iterative procedure (invoke → linearize → generate) that disentangles reading (structured-data access via specialized interfaces) from reasoning (LLM generation) so LLMs can iteratively collect evidence and produce answers or executable queries over KGs, tables, and DBs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo, June/August versions used)",
            "model_description": "OpenAI ChatGPT (gpt-3.5-turbo) used via API for zero-shot and few-shot prompting (June and Aug versions referenced); strong instruction-following behavior leveraged in experiments.",
            "reasoning_method_name": "Iterative Reading-then-Reasoning (IRR)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "LLM iteratively queries specialized structured-data interfaces to retrieve focused evidence, linearizes the retrieved fragments, and is prompted to select relevant evidence and then generate answers or executable queries; the process repeats until a final answer/SQL is derived. This is sequential iterative refinement rather than ensembling of diverse reasoning paths.",
            "task_name": "KGQA, TableQA, Text-to-SQL (multiple benchmarks)",
            "task_description": "KG-based QA (WebQSP, MetaQA), table QA & verification (WTQ, WikiSQL, TabFact), and DB text-to-SQL parsing (Spider and variants).",
            "performance": "ChatGPT + IRR performance (selected metrics): KGQA WebQSP Hits@1 72.6 (vs baseline 61.2); MetaQA-2hop 93.9 (baseline 31.0); TableQA WTQ denotation 48.4 (baseline 43.3), WikiSQL 54.4 (baseline 51.6), TabFact 87.1 (baseline 82.9); Text-to-SQL Spider execution acc 74.8 (baseline 70.1). (Values from paper Tables 1-3.)",
            "comparison_with_other_method": true,
            "performance_other_method": "ChatGPT direct zero-shot prompting (single-pass) baseline: KGQA WebQSP 61.2, MetaQA-2hop 31.0; TableQA WTQ 43.3, WikiSQL 51.6; Text-to-SQL Spider 70.1.",
            "key_findings": "IRR yields large improvements for ChatGPT across datasets, especially on multi-hop KGQA (e.g., MetaQA 2/3-hop), and improves Text-to-SQL and table QA metrics. Iterative evidence extraction via interfaces enables ChatGPT to focus reasoning on relevant facts and reduces distractors from large structured contexts. Few-shot in-context exemplars combined with IRR often produce further gains.",
            "counter_examples_or_negative_results": "Although generally beneficial, IRR did not always produce uniform improvements across all metrics/versions; some small tasks or specific metrics show modest gains. The authors report remaining dominant error types (KG selection errors, SQL reasoning errors) and generation-format errors that limit end-to-end gains.",
            "uuid": "e4955.1",
            "source_info": {
                "paper_title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Direct zero-shot prompting",
            "name_full": "Direct single-pass zero-shot LLM prompting (no IRR interfaces)",
            "brief_description": "Baseline approach feeding question (and in some table/text-to-SQL cases, full structured data) into an LLM prompt in a single pass without iterative interface-based retrieval or evidence selection.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Davinci-003 (text-davinci-003)",
            "model_description": "OpenAI Davinci-003, GPT-3.5 family instruction-following LLM used in zero-shot prompting baseline.",
            "reasoning_method_name": "Direct single-pass prompting (zero-shot)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "The LLM is given the question (and possibly full linearized structured data) in a single prompt and asked to produce an answer or SQL in one generation step; there is no iterative retrieval or multiple alternative reasoning paths.",
            "task_name": "KGQA, TableQA, Text-to-SQL (benchmarks listed in the paper)",
            "task_description": "Same structured-data QA and semantic parsing tasks used in the paper.",
            "performance": "Davinci-003 direct zero-shot baseline performance (selected): KGQA WebQSP 48.3 Hits@1; MetaQA-2hop 25.3; TableQA WTQ 34.8 denotation; WikiSQL 49.1; Text-to-SQL Spider 68.8 execution accuracy.",
            "comparison_with_other_method": true,
            "performance_other_method": "Davinci-003 + IRR: WebQSP 71.9, MetaQA-2hop 59.5, WTQ 39.2, WikiSQL 51.8, Spider 69.5 (see paper Tables 1-3).",
            "key_findings": "Direct single-pass prompting provides a reasonable baseline but underperforms compared to the IRR iterative interface-augmented method, especially for multi-hop KGQA and many structured-data tasks where restricting retrieval and iteratively reasoning is helpful.",
            "counter_examples_or_negative_results": "For some tasks/datasets (e.g., TabFact for Davinci-003) the direct baseline performs comparably or even better than IRR in the paper's reported numbers (TabFact: baseline 80.7 vs IRR 76.5), showing IRR is not universally superior and that dataset/task characteristics matter.",
            "uuid": "e4955.2",
            "source_info": {
                "paper_title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Direct zero-shot prompting",
            "name_full": "Direct single-pass zero-shot LLM prompting (no IRR interfaces)",
            "brief_description": "Baseline approach feeding question (and in some table/text-to-SQL cases, full structured data) into an LLM prompt in a single pass without iterative interface-based retrieval or evidence selection.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "OpenAI ChatGPT (gpt-3.5-turbo), used as a zero-shot baseline prompting the model directly with question ± structured data.",
            "reasoning_method_name": "Direct single-pass prompting (zero-shot)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "The model receives the question and maybe linearized structured data in a single prompt and is expected to output the answer or SQL in one generation call; no iterative retrieval or evidence-selection interfaces are used.",
            "task_name": "KGQA, TableQA, Text-to-SQL",
            "task_description": "Same benchmarks as above (WebQSP, MetaQA, WTQ, WikiSQL, TabFact, Spider, Spider-SYN, SpiderRealistic).",
            "performance": "ChatGPT direct zero-shot baseline (selected): KGQA WebQSP 61.2 Hits@1; MetaQA-2hop 31.0; TableQA WTQ 43.3, WikiSQL 51.6, TabFact 82.9; Text-to-SQL Spider 70.1 execution acc.",
            "comparison_with_other_method": true,
            "performance_other_method": "ChatGPT + IRR: WebQSP 72.6, MetaQA-2hop 93.9, WTQ 48.4, WikiSQL 54.4, TabFact 87.1, Spider 74.8 (see paper Tables 1-3).",
            "key_findings": "ChatGPT's direct prompting baseline is strong on some tasks but IRR significantly improves ChatGPT especially on multi-hop KGQA and Text-to-SQL. The iterative, interface-guided evidence selection reduces distractors and boosts reasoning performance.",
            "counter_examples_or_negative_results": "Although IRR improves most metrics, differences across datasets and versions show non-uniform gains; for some tasks the baseline is already competitive and the absolute improvement is smaller. The paper also notes residual error categories (selection, reasoning, format, hallucination).",
            "uuid": "e4955.3",
            "source_info": {
                "paper_title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Don't generate, discriminate: A proposal for grounding language models to real-world environments",
            "rating": 1
        }
    ],
    "cost": 0.01488675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>StructGPT: A General Framework for Large Language Model to Reason over Structured Data</h1>
<p>Jinhao Jiang ${ }^{1,3}$, Kun Zhou ${ }^{2,3}$, Zican Dong ${ }^{1}$, Keming Ye ${ }^{4}$, Wayne Xin Zhao ${ }^{1,3 \dagger}$ and Ji-Rong Wen ${ }^{1,2,3}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China.<br>${ }^{2}$ School of Information, Renmin University of China.<br>${ }^{3}$ Beijing Key Laboratory of Big Data Management and Analysis Methods.<br>${ }^{4}$ University of Electronic Science and Technology of China.<br>jiangjinhao@ruc.edu.cn, batmanfly@gmail.com</p>
<h4>Abstract</h4>
<p>In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-thenReasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT. In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let LLMs concentrate on the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invoking-linearization-generation procedure to support LLMs in reasoning on the structured data with the help of the interfaces. By iterating this procedure with provided interfaces, our approach can gradually approach the target answers to a given query. Experiments conducted on three types of structured data show that StructGPT greatly improves the performance of LLMs, under the few-shot and zero-shot settings. Our codes and data are publicly available at https://github.com/RUCAIBox/StructGPT.</p>
<h2>1 Introduction</h2>
<p>Recently, large language models (LLMs) (Brown et al., 2020; Zhao et al., 2023) have made remarkable advancements in the NLP field. Existing work (Ouyang et al., 2022a; Zhang et al., 2022) has demonstrated that LLMs (e.g., ChatGPT or GPT4 (OpenAI, 2023)) have strong zero-shot capability to solve a broad range of tasks using specially designed prompts, without task-specific fine-tuning.</p>
<p>Despite the successes, recent work has also revealed that LLMs may generate unfaithful information in conflict with the factual knowledge (Li et al., 2023b), and also fall short of mastering domainspecific or real-time knowledge (Schick et al., 2023; Peng et al., 2023). A direct solution to the above</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>issues is to augment LLMs with external knowledge resources, so as to amend the incorrect generations. Among these resources, structured data (e.g., knowledge graphs and databases), has been widely used as the carrier of the required knowledge for LLMs. Unlike plain text, structured data is organized in a standardized format, conforming to some logical data model. For example, knowledge graphs (KGs) are often organized as fact triples that state the relations between head entities and tail entities, and data tables are organized in the form of column-indexed records by rows. However, as structured data has special data formats or schemas that LLMs have not seen during pretraining, they may be not fully grasped or understood by LLMs (Wei et al., 2021). A straightforward way to solve this problem is to linearize the structured data into a sentence that LLMs can well understand. While, the amount of structured data is often vast, making it infeasible to include all the data records in the input prompt.</p>
<p>Regarding the above challenges, we are inspired by the tool manipulation strategy for augmenting the abilities of LLMs (Schick et al., 2023; Nakano et al., 2021). Our basic idea is to incorporate specialized interfaces (e.g., extracting columns from tables) to manipulate the structured data records. With these interfaces, we can effectively reduce the search space of the data records, and more accurately identify the required evidence to fulfill specific tasks. In this way, LLMs can concentrate on reasoning based on the evidence obtained from the interfaces. To implement the interface-augmented approach, there remain two key problems, namely how to design suitable interfaces for specific tasks and how to utilize them for reasoning by LLMs, which are the focus of this work.</p>
<p>To design suitable interfaces, we regard multiple types of structured data as black-box systems, and design the interfaces to provide accurate, efficient data access and filtering for LLMs. For each</p>
<p>interface, its implementation is dependent on the characteristic of the structured data, while its functionality is general to all LLMs, with just a few arguments for specifying the data requirements. Based on these interfaces, we propose an Iterative Reading-then-Reasoning (IRR) framework for LLMs to utilize the interfaces to solve the tasks based on structured data, namely StructGPT. This framework considers two major functions to fulfill different tasks, namely collecting relevant evidence (reading) and inferring the answer or planning subsequent steps (reasoning). Specifically, we propose an invoking-linearization-generation procedure to support LLMs in reading and reasoning on the structured data with the help of the external interfaces. By iterating this procedure with provided interfaces, we can gradually approach the target answer to a given question.</p>
<p>To our knowledge, this is the first work that explores how to support LLMs in reasoning on multiple types of structured data (including tables, KGs, and DBs) in a unified paradigm. To evaluate the effectiveness of our approach, we conduct extensive experiments on a wide range of tasks (e.g., KGbased question answering (KGQA), Table-based question answering (TableQA), and DB-based Text-to-SQL). Experimental results on 8 datasets demonstrate that our approach can effectively enhance the reasoning performance of LLMs on structured data in zero-shot and few-shot settings, even comparable with competitive full-data supervised-tuning methods. For example, in KGQA, TableQA, and Text-to-SQL tasks, our approach yields an increase of $11.4 \%$ of Hits@1 on WebQSP, $4.2 \%$ of accuracy in TabFact, and $4.7 \%$ of execution accuracy in Spider respectively, compared to directly using ChatGPT in the zero-shot setting.</p>
<h2>2 Related Work</h2>
<p>Reasoning over Structured Data. Structured data (e.g., knowledge graphs, tables, and databases) is an important knowledge carrier for a variety of QA and reasoning tasks. Early work focuses on designing specific model architectures tailored for each type of structured data, such as graph neural networks (Sun et al., 2018), table Transformers (Herzig et al., 2020), and tree-structured decoder (Wang et al., 2020). While achieving remarkable performance, these approaches lack generality for various types of structured data and are hard to be transferred across different tasks. Re-
cently, with the success of pre-trained language models (PLMs) (e.g., T5 (Raffel et al., 2020), BART (Lewis et al., 2020)), several methods (Raffel et al., 2020; Khashabi et al., 2020) have adopted PLMs as the general encoder or solver for different structured data and tasks. Among them, UnifiedSKG (Xie et al., 2022) unifies a number of reasoning tasks over structured data into a text-to-text format, which concatenates the question and the linearized structured data as input, and then finetunes T5 to learn to generate the answer. However, UnifiedSKG also requires to tune the model parameters, and is unable to handle large-scale structured data under the limitation of the maximum input length. Instead, our method can utilize the LLM to perform reasoning on structured data without training, and also leverage the interfaces of structured data to better manipulate vast structured data.</p>
<p>LLMs for Structured Data. Benefitting from the strong few-shot and zero-shot capability, recent studies have leveraged LLMs to perform reasoning over structured data (Chen et al., 2023; Li et al., 2023a; Cheng et al., 2022; Rajkumar et al., 2022). Existing work can be roughly divided into two types. The first type of method linearizes the structured data into a sentence (e.g., table rows), and feeds it into the LLMs to generate the answer according to in-context exemplars (Cheng et al., 2022; Chen, 2023). For complex questions or structured data, they first decompose it into multiple simple and short ones and then perform linearization and generation (Ye et al., 2023). Another type of method leverages LLMs to evaluate the plausibility of the solution plan based on the knowledge base (Gu et al., 2023), or first generate a solution draft with in-context exemplars and then revise the draft grounding on the knowledge base (Li et al., 2023c). However, most of them only focus on a specific type of structured data, and are lack of generality across various data and tasks. In StructGPT, we provide a unified paradigm that is general to various structured data and downstream tasks.</p>
<h2>3 Preliminary</h2>
<p>In this section, we introduce the definition of structured data, which mainly consists of three commonly used types. Then we present the unified problem statement.</p>
<p>Structured Data. Structured data (e.g., data tables and knowledge graphs) refers to the data that</p>
<p>is in a standardized format, conforming to some logical data model (Xie et al., 2022; Chen et al., 2009). Due to the formal structure, it is easy and efficient to access and query structured data using formal languages (e.g., SQL and SPARQL for databases) or specific algorithms (e.g., triples search for knowledge graphs). In this work, we mainly focus on three types of structured data, namely knowledge graphs (KG), data tables (Table), and databases (DB), since they play an important role as the knowledge source in helping solve complex reasoning tasks, described as follows.</p>
<ul>
<li>Knowledge Graph. A knowledge graph (KG) consists of a number of triples to store the factual knowledge, denoted as $\mathcal{G}=\left{\left\langle e, r, e^{\prime}\right\rangle \mid e, e^{\prime} \in\right.$ $\mathcal{E}, r \in \mathcal{R}}$, where $\mathcal{E}$ and $\mathcal{R}$ denote the set of entities and relations, respectively. A triple $\left\langle e, r, e^{\prime}\right\rangle$ represents the fact that there is a relation $r$ between the head entity $e$ and the tail entity $e^{\prime}$.</li>
<li>Data Table. A data table $\mathcal{T}$ (table in short) contains multiple columns $\left{c_{i}\right}<em j="j">{i=1}^{C}$ and rows $\left{l</em>\right}<em j="j">{j=1}^{R}$, where each row $l</em>\right}}$ denotes a data record formatted by the attributes indexed by columns $\left{c_{i<em i_="i," j="j">{i=1}^{C}$, and $v</em>$ denotes the content in the cell corresponding to the position at column $i$ and row $j$.</li>
<li>Database. A database (DB) typically consists of $N$ data tables, denoted as $\mathcal{D}=\left{\mathcal{T}<em 2="2">{1}, \mathcal{T}</em>}, \ldots, \mathcal{T<em i="i">{N}\right}$. Besides the column names, the foreign keys across all tables are also available to link the data from two tables, denoted as $\left{\left(c</em>$ denote the $i$-th and $j$-th columns in the $k$-th and $h$-th tables, respectively.}^{(k)}, c_{j}^{(h)}\right)\right}$, where $c_{i}^{(k)}$ and $c_{j}^{(h)</li>
</ul>
<p>Problem Statement. This work mainly focuses on using LLMs to solve complex reasoning tasks based on structured data. Formally, it can be described as a question answering task: given a natural language question $q$ and an accessible structured data $\mathcal{S}$ (e.g., a knowledge graph, a table, or database), the LLM needs to extract useful evidence from $\mathcal{S}$ and then generates the expected result to answer the question $q$ based on the extracted evidence. According to the task requirement, the generated result can be either free-form answers in natural language or structured expressions (e.g., SQL statements) to be executed for obtaining the answer from $\mathcal{S}$. Since we consider three types of structured data (Section 4), our tasks can be instantiated as follows:</p>
<ul>
<li>KG based question answering (KGQA)</li>
<li>Table based question answering (TableQA)</li>
<li>DB based semantic parsing (Text-to-SQL)</li>
</ul>
<h2>4 Approach</h2>
<h3>4.1 Overview</h3>
<p>In this work, we assume that LLMs have to rely on the evidence contained in the structured data to solve the three tasks described in Section 3. An intuitive idea is to conduct a two-stage framework as prior studies on retrieval-augmented approaches (Izacard et al., 2022; Oguz et al., 2022), in which LLMs are employed to first collect sufficient evidence relating to the question and then figure out the answer by the LLMs. However, such an approach is not directly applicable to structured data. Although LLMs are capable of solving diverse tasks in natural language, they have limited capacities in accurately representing and understanding structured data, especially for their contained domain-specific knowledge (Moiseev et al., 2022; Emelin et al., 2022).</p>
<p>To address this difficulty, our solution is inspired by the use of specialized tools in solving complex tasks for LLMs (Nakano et al., 2021; Gao et al., 2022b; Schick et al., 2023). We noted that structured data is well organized and supports easy access via formal language or queries (called interface for generality). The basic idea of our approach is to disentangle the two processes of reading and reasoning for LLMs: we utilize the interface of structure data to implement accurate, efficient data access and filtering (obtaining the relevant evidence), and further utilize the reasoning ability of LLMs to figure out the final plan or result for the question (fulfilling the task). In this way, LLMs can concentrate on the reasoning process in answering the question, without considering the specialized approach to reading the structure data.</p>
<p>Specially, in our framework, we encapsulate the structure data as a black-box system, and provide specific interfaces for LLMs to access the contained data. Further, we propose an invoking-linearization-generation procedure that enables LLMs to read and extract useful evidence from structured data via the corresponding interface. By iterating the above procedure with provided interfaces, we can gradually obtain the answers by leveraging the superior reasoning abilities of LLMs.</p>
<h3>4.2 Interfaces for Structured Data</h3>
<p>Due to the standardized data formats, structured data is often equipped with efficient data management ways, e.g., SQL for the database. In our approach, we aim to provide LLMs with special-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of the proposed iterative reading-then-reasoning approach. We design specialized interfaces for reading structured data, and iterate the invoking-linearization-generation procedure to utilize LLMs for performing reasoning on the interfaces, until deriving the final answer or executable SQL.</p>
<p>ized interfaces, helping LLMs to <em>read</em> and <em>utilize</em> the structured data. Next, we present the specially designed interfaces for KG, table, and DB.</p>
<p><strong>Interfaces for Knowledge Graph.</strong> When performing complex reasoning on a KG, existing work (Sun et al., 2018) typically starts from a certain entity (about the question topic), and jumps along with the relations until reaching the answer. In this process, LLMs should be aware of the neighboring relations of the current entity, and the neighboring triples with certain relations to the current entity. Based on it, LLMs can select the relevant relations and triples from them to find the answer. For this purpose, we devise two functions for assisting LLMs to accomplish the above operations.</p>
<ul>
<li><strong>Extract_Neighbor_Relations (e):</strong> extracts all the neighboring relations of the entity <em>e</em>.</li>
<li><strong>Extract_Triples (e, {r}):</strong> extracts all the triples with the relation in {<em>r</em>} and head entity <em>e</em>.</li>
</ul>
<p><strong>Interfaces for Table.</strong> Given a data table, LLMs need to know its contained column names, and can access the content by row or column, enabling LLMs to extract its sub-table containing relevant columns and rows. Thus, we define three functions:</p>
<ul>
<li><strong>Extract_Column_Name (T):</strong> extracts all the column names of a table T.</li>
<li><strong>Extract_Columns (T, {c}):</strong> extracts the contents of columns from a table T by indices {<em>c</em>}.</li>
<li><strong>Extract_SubTable (T, {c}, {j}):</strong> extracts the sub-table specified by the column indices {<em>c</em>} and row indices {<em>j</em>} from a table T.</li>
</ul>
<p><strong>Interfaces for Database.</strong> Considering a simplified setting when querying the database, LLMs should be aware of all the contained tables and columns (by name) for relevant tables selection, and can also acquire the detailed columns and foreign keys from the selected tables to search for the answer. Thus, we devise two functions as follows:</p>
<ul>
<li><strong>Extract_Table&amp;Column_Name (D):</strong> extracts the names of all the tables and their contained columns from the database.</li>
<li><strong>Extract_Tables_Information ({T}):</strong> extracts the table names, column names, and foreign keys from a set of tables {T}.</li>
</ul>
<p>4.3 Reading and Reasoning with Interfaces</p>
<p>Based on the above interfaces, we propose a general invoking-linearization-generation procedure that can be iterated in multiple turns for utilizing LLMs to perform reading and reasoning on structured data. For each iteration, based on the currently collected data, we first invoke an interface to extract relevant evidence from structure data, then linearize it into a textual prompt, and finally feed the prompt into the LLM for generation (selecting useful data or predicting the answer).</p>
<p>Invoking an Interface. In this step, we aim to invoke an interface for extracting the relevant information from the structured data. According to the designed interfaces in Section 4.2, we construct the input based on the currently available data (e.g., entity and table), and then invoke the interface to obtain more detailed relevant information (e.g., neighboring relations and column names), which will be fed into LLMs for collecting useful information or generating the answer.</p>
<p>Information Linearization. Given the extracted information, we convert it into a textual sentence that can be understood by LLMs. For the information from KG (i.e., relations and triples), we concatenate them into a long sentence marked by specific separation and boundary symbols. For table and database, we leverage the same way to linearize the extracted table names or column names. While for contents in columns and rows, we follow existing work (Pasupat and Liang, 2015) that first converts them into triples, where head entities are the row indices, relations are column names, and tail entities are the content in the cell, e.g., "(row 1, year, 1896)" and "(row 1, city, Athens)". Then, for each row, we extract the row indices in the front and omit it in the triples, to compose a simplified sentence, e.g., "row 1: (year, 1896), (city, Athens)". For multiple rows, we concatenate them into a long sentence via a special separation symbol.</p>
<p>LLM for Generation. After linearization, we design two types of input prompts for LLMs to fulfill different purposes ${ }^{1}$ :</p>
<ul>
<li>The first type of prompts mostly adopts the following pattern: "Here are [Y]. Which [X] are most relevant to answer the question [Q]". It aims to elicit the ability of LLMs to select useful evidence</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(i.e., $[X]$ ) from linearized extracted information (i.e., $[Y]$ ), according to the question (i.e., $[Q]$ ).</p>
<ul>
<li>The second type of prompt follows the pattern: "Based on [Y], please generate [Z] for the question [Q]". It aims to predict the targeted results (i.e., $[Z]$ ) for the given question (i.e., $[Q]$ ) based on the linearized extracted information (i.e., $[Y]$ ). Note that the targeted results can be either the answer string or executable formal language (e.g., SQL) that can lead to the final answer.</li>
</ul>
<p>By iterating the above invoking-linearizationgeneration procedure on designed interfaces, LLMs can progressively capture more useful evidence for deriving the final answer.</p>
<h3>4.4 Instantiated Downstream Tasks</h3>
<p>In the following, we describe the instances of the above general workflow for the tasks described in Section 3, since they deal with very different structure data and vary in the task settings.</p>
<p>KG-based Question Answering (KGQA). This task aims to find the answer entities for the question based on the KG. Following existing work (Sun et al., 2018), we denote the mentioned entity in the given question $q$ as the topic entity $e_{T}$, and assume it has been linked to some specific entity on the KG through existing linking tools (e.g., Google Knowledge Graph Search API) or models (e.g., ELQ (Li et al., 2020)). Starting from $e_{T}$, we perform the invoking-linearization-generation procedure two times using the two interfaces in KG sequentially. First, we invoke the interface Extract_Neighbor_Relation( $e_{T}$ ) to extract the candidate one-hop relations, linearize them to compose the input prompt, and then leverage the LLM to select the useful relations ${r}$ according to the question. Then, based on ${r}$, we invoke the Extract_Triples $\left(e_{T},{r}\right)$ interface to collect the relevant triples for the head entity $e_{T}$ and relation in ${r}$, then linearize this information, and finally employ the LLM to select the most relevant triples, whose tail entities will be considered as the final answer. Besides, we can also consider the multihop KGQA task (Lan et al., 2021), where after selecting the triples of the current hop, the LLM should assess whether the current information is sufficient to answer the question. Then, LLMs will make according actions based on the assessment, i.e., stopping the iterations for producing the answer or continuing the iterations on next-hop tail entities from selected triples.</p>
<p>Table-based Question Answering (TableQA). For TableQA, we typically need to answer the question according to the content in the given table. We also perform the above procedure by using the three interfaces in turn. Concretely, first, we invoke Extract_Column_Name $(\mathcal{T})$ to extract all column names of a table, linearize them, and leverage LLMs to select the relevant ones ${c}$ according to the question. Then, we invoke Extract_Columns $(\mathcal{T},{c})$ to extract the contents of all relevant columns, and select the useful row indices ${j}$ by LLMs. Subsequently, we further invoke Extract_SubTable $(\mathcal{T},{c},{j})$ to generate the sub-table for the question. Based on the linearized sub-table, the LLM finally generates the answer to the question.</p>
<p>DB-based Semantic Parsing (Text-to-SQL). This task focuses on generating a SQL query that can be executed to obtain the required information from a database. To achieve this goal, first, we invoke Extract_Table\&amp;Column_Name $(\mathcal{D})$ to obtain all the table names and their column names in the DB, linearize them, and utilize the LLM to select the relevant table names. Then, we invoke Extract_Tables_Information $({\mathcal{T}})$ to obtain all the relevant information (i.e., column names and foreign keys) from these tables. Similarly, by linearizing this information and composing the input prompt, the LLM can generate an executable SQL for the given question.</p>
<h2>5 Experiment</h2>
<p>We conduct experiments on three complex reasoning tasks over structured data, i.e., KGQA, TableQA, and DB based text-to-SQL.</p>
<h3>5.1 Datasets</h3>
<p>For KG based QA (KGQA), we adopt two benchmark datasets, i.e., WebQuestionsSP (WebQSP) (Yih et al., 2016) and MetaQA (Zhang et al., 2018) for evaluation. The answer entities in WebQSP require up to 2-hop reasoning on the Freebase KG. In contrast, MetaQA contains questions in the movie domain, whose answer entities are up to 3 hops away from the topic entities on a movie KG (based on OMDb). According to the number of hops, it is split into three sub-datasets, i.e., MetaQA-1hop, MetaQA-2hop, and MetaQA-3hop.</p>
<p>For Table based QA (TableQA), we adopt three widely-used datasets, weakly-supervised WikiSQL (WikiSQL) (Zhong et al., 2017), WikiTable-</p>
<p>Questions (WTQ) (Pasupat and Liang, 2015), and TabFact (Chen et al., 2020). The first two are typical table-based question answering datasets, and the third one is a multiple-choice dataset that concentrates on table fact verification. WikiSQL requires filtering and aggregating information over the table content, and the WTQ demands more advanced reasoning capabilities (e.g., sorting). TabFact needs to judge whether the provided statement agrees with the facts stored in a table.</p>
<p>For DB based semantic parsing (Text-to-SQL), we adopt three public datasets, i.e., Spider (Yu et al., 2018), Spider-SYN (Gan et al., 2021), and SpiderRealistic (Deng et al., 2021). Spider is a typical Text-to-SQL dataset covering 20 databases with a set of 1034 evaluation samples. Spider-SYN and Spider-Realistic are two more challenging datasets derived from Spider. Concretely, Spider-SYN manually substitutes the synonyms in natural language questions, while Spider-Realistic removes the questions in the evaluation set that explicitly mention the required columns' names.</p>
<h3>5.2 Evaluation Metrics</h3>
<p>For KGQA, we employ Hits@1 which assesses whether the top-1 predicted answer is correct. In our approach, we focus on generating the most confident answer and then checking if the prediction hits any target. As LLMs may generate multiple answers, we also conducted a manual double-check finally (Tan et al., 2023), to judge if wrong answers are included. For TableQA, we adopt two evaluation metrics, namely denotation accuracy and accuracy. In WTQ and WikiSQL, denotation accuracy is employed to evaluate whether the predicted answer is the same as the gold answer based on set-level equivalence. In TabFact, we adopt accuracy to assess the correctness of the prediction. For Text-to-SQL, we adopt the execution accuracy (EX) to assess whether the execution results of the predicted SQL and the gold SQL are the same.</p>
<h3>5.3 Baselines</h3>
<p>We compare our method with competitive fulldata supervised-tuning baselines tailored to these tasks. Specifically, our method is a general iterative reading-then-reasoning (IRR) framework that can be used for different LLMs. And we test our IRR with two different LLMs, i.e., Davinci-003 (text-davinci-003 (Ouyang et al., 2022b)) and Chat-</p>
<p>GPT (i.e., gpt-3.5-turbo ${ }^{2}$ ), under zero-shot and few-shot settings ${ }^{3}$. Considering the evolution of the closed large language model, e.g., ChatGPT, we have further conducted supplementary experiments on three datasets (i.e., WebQSP, WTQ, and Spider) using the latest August version of ChatGPT. The results are presented in Appendix A. For KGQA, we select KV-Mem (Miller et al., 2016), GragtNet (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020), NSM (He et al., 2021), and UniKGQA (Jiang et al., 2023). For TableQA, we select MAPO (Liang et al., 2018), TAPAS (Herzig et al., 2020; Eisenschlos et al., 2020), UnifiedSKG (T5-3B) (Xie et al., 2022), TAPEX (Liu et al., 2022), and DATER (Ye et al., 2023). For Text-to-SQL, we select RATSQL+BERT $_{\text {Large }}$ (Wang et al., 2020), TKKLarge (Gao et al., 2022a), T5-3B+PICARD (Raffel et al., 2020), RASAT+PICARD (Qi et al., 2022), and RESDSQL-3B+NatSQL (Li et al., 2023a).</p>
<p>Additionally, we incorporate baselines that employ Davinci-003 and ChatGPT directly for achieving the aforementioned tasks in a zero-shot setting. To ensure a fair comparison, we utilize the same instruction prompt to evaluate them, ensuring that the only difference with our method is the usage of structured data. Specifically, in KGQA datasets, we follow existing work (Tan et al., 2023) that utilizes LLMs to answer the questions without using KG. In TableQA and Text-to-SQL, we feed the required information of tables with questions into LLMs (Liu et al., 2023c,a), without special treatment for the overlength problem.</p>
<h3>5.4 Results and Analysis</h3>
<p>We show the results on KGQA, TableQA, and Text-to-SQL tasks and analyze them respectively.</p>
<p>Evaluation on KGQA. Table 1 shows the results on KGQA datasets. First, LLMs can achieve performance comparable to the supervised learning model (i.e., 61.2 of ChatGPT v.s. 66.4 of GraftNet and 48.3 of Davinci-003 v.s. 46.7 of KV-Mem) on the WebQSP dataset, in a zero-shot setting without using KGs. It demonstrates that LLMs indeed grasp a certain amount of knowledge that can help them answer complex questions. However, on more difficult datasets that require multi-hop reasoning (e.g., MetaQA-2hop and MetaQA-3hop), the two LLMs perform not well. It indicates that</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Results of different methods for KGQA (Hits@1 in percent). We copy the results in the first block from He et al. (2021) and Jiang et al. (2023). The best results of each block are highlighted in bold.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>WQSP</th>
<th>MQA <br> 1hop</th>
<th>MQA <br> 2hop</th>
<th>MQA <br> 3hop</th>
</tr>
</thead>
<tbody>
<tr>
<td>KV-Mem</td>
<td>46.7</td>
<td>96.2</td>
<td>82.7</td>
<td>48.9</td>
</tr>
<tr>
<td>GraftNet</td>
<td>66.4</td>
<td>97.0</td>
<td>94.8</td>
<td>77.7</td>
</tr>
<tr>
<td>EmbedKGQA</td>
<td>66.6</td>
<td>97.5</td>
<td>98.8</td>
<td>94.8</td>
</tr>
<tr>
<td>NSM</td>
<td>68.7</td>
<td>97.1</td>
<td>$\mathbf{9 9 . 9}$</td>
<td>98.9</td>
</tr>
<tr>
<td>UniKGQA</td>
<td>$\mathbf{7 5 . 1}$</td>
<td>$\mathbf{9 7 . 5}$</td>
<td>99.0</td>
<td>$\mathbf{9 9 . 1}$</td>
</tr>
<tr>
<td>Davinci-003</td>
<td>48.3</td>
<td>52.1</td>
<td>25.3</td>
<td>42.5</td>
</tr>
<tr>
<td>+ IRR (ours)</td>
<td>71.9</td>
<td>94.4</td>
<td>59.5</td>
<td>70.2</td>
</tr>
<tr>
<td>+ IRR (ours, few-shot)</td>
<td>71.0</td>
<td>97.1</td>
<td>93.5</td>
<td>75.3</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>61.2</td>
<td>61.9</td>
<td>31.0</td>
<td>43.2</td>
</tr>
<tr>
<td>+ IRR (ours)</td>
<td>$\mathbf{7 2 . 6}$</td>
<td>94.2</td>
<td>93.9</td>
<td>80.2</td>
</tr>
<tr>
<td>+ IRR (ours, few-shot)</td>
<td>69.6</td>
<td>$\mathbf{9 7 . 1}$</td>
<td>$\mathbf{9 7 . 3}$</td>
<td>$\mathbf{8 7 . 0}$</td>
</tr>
</tbody>
</table>
<p>LLMs can not solely rely on their own knowledge to answer difficult questions, and their augmentation with KGs is necessary. In contrast, when incorporating our proposed method to access KG, the performance of Davinci-003 and ChatGPT can be both substantially improved, indicating the effectiveness of our proposed method for supporting LLMs reasoning over KG. By adding a few incontext exemplars (i.e., 15 for WQSP and 32 for MQA) to LLMs, we can further improve the model performance. In our approach, we devise interfaces for KG to efficiently read the relevant information, and leverage LLMs to extract useful parts and perform reasoning. We leverage the IRR procedure on devised interfaces sequentially, which can progressively capture more useful detailed evidence for finally obtaining the answer.</p>
<p>Evaluation on TableQA. Table 2 shows the results on three TableQA datasets. First, with the full table as the prompt, ChatGPT can also achieve comparable performance on WTQ and TabFact as full-data supervised-tuning methods, but performs not well on more difficult WikiSQL datasets. It also indicates that LLMs have the capability of understanding the knowledge within table data to some extent. Second, our proposed method can consistently improve the performance of two LLMs a lot in both three datasets. At the same time, when adding 32 in-context exemplars to the LLMs, they can obtain further performance improvements. It indicates the effectiveness of our proposed method in helping LLMs reasoning over Table. Our approach provides a more effective way for LLMs to</p>
<p>Table 2: Results of different methods for TableQA (denotation accuracy for WTQ and WikiSQL, accuracy for TabFact). We copy the results of TAPAS on TabFact from Eisenschlos et al. (2020), and others in the first block from their original papers. The best results of each block are highlighted in bold.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>WTQ</th>
<th>WikiSQL</th>
<th>TabFact</th>
</tr>
</thead>
<tbody>
<tr>
<td>MAPO</td>
<td>43.8</td>
<td>72.6</td>
<td>-</td>
</tr>
<tr>
<td>TAPAS</td>
<td>48.8</td>
<td>83.6</td>
<td>81.0</td>
</tr>
<tr>
<td>UnifiedSKG (T5-3B)</td>
<td>49.3</td>
<td>86.0</td>
<td>83.7</td>
</tr>
<tr>
<td>TAPEX</td>
<td>57.5</td>
<td>89.5</td>
<td>84.2</td>
</tr>
<tr>
<td>DATER</td>
<td>65.9</td>
<td>-</td>
<td>93.0</td>
</tr>
<tr>
<td>Davinci-003</td>
<td>34.8</td>
<td>49.1</td>
<td>80.7</td>
</tr>
<tr>
<td>+ IRR (ours)</td>
<td>39.2</td>
<td>51.8</td>
<td>76.5</td>
</tr>
<tr>
<td>+ IRR (ours, few-shot)</td>
<td>57.0</td>
<td>64.6</td>
<td>87.3</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>43.3</td>
<td>51.6</td>
<td>82.9</td>
</tr>
<tr>
<td>+ IRR (ours)</td>
<td>48.4</td>
<td>54.4</td>
<td>87.1</td>
</tr>
<tr>
<td>+ IRR (ours, few-shot)</td>
<td>52.2</td>
<td>65.6</td>
<td>87.6</td>
</tr>
</tbody>
</table>
<p>iteratively access and utilize the relevant information from the table, which reduces the influence of irrelevant and redundant information.</p>
<p>Evaluation on Text-to-SQL. Table 3 shows the results on DB-based datasets. First, with all the information from DB (table names, column names, and foreign keys) as the prompt, the LLMs have the capability of directly generating a suitable SQL query of the question, performing well on all three datasets. Whereas, the performance of LLMs is not better than competitive full-data supervisedtuning methods, showing the difficulty of this task. As our proposed method can extract relevant tables and columns, it also alleviates the influence of irrelevant information for LLMs to generate the SQL query. Simultaneously, with the assistance of 32 in-context exemplars, LLMs exhibit enhanced comprehension of the mapping between natural language questions and their corresponding SQL queries. The consistent performance improvements over the three datasets whenever in zero-shot or few-shot settings also indicate the effectiveness of our proposed method.</p>
<p>Case Study. We show an example of KGQA in Figure 2, to help understand the working process of our method. Given the question, the interfaces of the structured data are sequentially invoked to iteratively extract more useful and detailed information. In each iteration, we first invoke the Extract_Neighbor_Relations function to extract the neighboring relations (e.g., birthplace, residence, and education) of the topic entity "Harper Lee",</p>
<p>Table 3: Performance comparison of different methods for Text-to-SQL (execution accuracy in percent). We copy the results of RAT-SQL+BERT ${ }_{\text {Large }}$ and TKKLarge from Deng et al. (2021) and Gao et al. (2022a), respectively. And we copy the results of the other three methods in the first block from Liu et al. (2023b). The best results of each block are highlighted in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Spider</th>
<th style="text-align: center;">Spider- <br> SYN</th>
<th style="text-align: center;">Spider- <br> Realistic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RAT-SQL + BERT $_{\text {Large }}$</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: left;">TKK-Large</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">64.4</td>
</tr>
<tr>
<td style="text-align: left;">T5-3B + PICARD</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">RASAT + PICARD</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">71.9</td>
</tr>
<tr>
<td style="text-align: left;">RESDSQL-3B + NatSQL</td>
<td style="text-align: center;">$\mathbf{8 4 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Davinci-003</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">63.2</td>
</tr>
<tr>
<td style="text-align: left;">+ IRR (ours)</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">64.2</td>
</tr>
<tr>
<td style="text-align: left;">+ IRR (ours, few-shot)</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">63.4</td>
</tr>
<tr>
<td style="text-align: left;">+ IRR (ours)</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">70.3</td>
</tr>
<tr>
<td style="text-align: left;">+ IRR (ours, few-shot)</td>
<td style="text-align: center;">$\mathbf{7 7 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 0}$</td>
</tr>
</tbody>
</table>
<p>then linearize them and compose the input prompt. Here, we utilize the instruction (i.e., provide only one relevant relation that's present in the candidate) to elicit the LLM to generate the most relevant relation, i.e., education. Based on the selected relation, we further invoke the Extract_Triples function to extract the triples with the relation to the topic entity. After linearization, another instruction (i.e., you just need to provide only one answer entity), is adopted for guiding the LLM to generate the final answer, i.e., Monroe County High School. Besides, we show the representative examples of TableQA and Text-to-SQL in Appendix B.</p>
<p>Error Analysis. To systemically analyze the shortcomings of our approach, we first select three datasets (i.e., WebQSP, WTQ, and Spider) with different types of structured data, and randomly sample 100 error cases from each dataset. Then, we manually examine these failures and classify them into five categories:</p>
<ul>
<li>Selection Error: the relevant information has not been selected by the LLM.</li>
<li>Reasoning Error: given the extracted relevant information, the LLM fails to generate the groundtruth answer or SQL.</li>
<li>Generation Format Error: the generated answer is in an abnormal format that fails to be identified by our result parser.</li>
<li>Hallucination: the generated results are inconsistent with the extracted information.</li>
<li>Other Errors: other uncategorizable errors.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Case study of our method on WebQSP.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Proportions of different error types in three datasets over different types of structured data.</p>
<p>We show the statistics in Figure 3. First, for the three datasets, the distributions of occurring errors are different. In WikiSQL, the frequencies of generation format, selection, and reasoning errors are relatively uniform. Whereas, in WebQSP, the selection error is the major error type (74%), since the KGQA task requires selecting the most relevant one from thousands of relations, which is not easy work. In Spider, reasoning error occurs more (62%), since the Text-to-SQL task requires LLMs to generate a SQL that can be executed to obtain the answer, which is also hard for LLMs.</p>
<p>According to the error distributions, it is promising to refine the major error cases to specifically improve the performance on each dataset. Concretely, we can devise more high-quality prompts that elicit LLMs to carefully make decisions when selecting and reasoning on KGQA and Text-to-SQL tasks, respectively. Besides, we also consider adding more interfaces and iteration turns for decomposing the hard problem into multiple simple ones, to simplify the complex reasoning task for better performance. We will try the above solutions in our future work.</p>
<h2>6 Conclusion</h2>
<p>In this work, we proposed a general framework for improving the zero-shot reasoning ability of LLMs over structured data, namely StructGPT. In our approach, we first constructed the specialized interfaces that support accurate and efficient data access, and then proposed an invoking-linearization-generation procedure that leverages LLMs to read and perform reasoning based on the interface. By iterating the above procedure using the interfaces sequentially, LLMs can progressively capture more useful and detailed evidence and finally generate the answer. To verify the effectiveness of our approach, we implemented our approach on KG based QA, table based QA and DB based semantic parsing tasks. Experimental results on 8 datasets show that our approach can boost the zero-shot performance of LLMs by a large margin, and achieve comparable performance as full-data supervised-tuning methods. We also provide detailed error analysis to point out the weakness of our approach, for enlightening other researchers in related areas.</p>
<h2>7 Limitations</h2>
<p>Although StructGPT demonstrates remarkable performance across tasks over structured data, there are some limitations of our method. First, the two LLMs used in our model, i.e., ChatGPT and Davinci-003, have a strong capability of following instructions. Hence, more experiments are required to evaluate our method with in-context learning on other LLMs that perform poorly at instruction following. Similarly, we only evaluate question answering tasks based on structured data. Future work should include wider evaluation scenarios to evaluate the universality of our method, e.g., data-to-text and formal-language-to-text (Xie et al., 2022). Finally, since it is difficult to control the answer format during the generation process of LLMs in different datasets, there are several format errors in generated texts as shown in Section 5. Therefore, the performance of our method can be further improved by meticulously designing the prompt and answer parsing for different datasets.</p>
<h2>Acknowledgments</h2>
<p>This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027 and L233008. And this work is also partially supported by the Outstanding Innovative Talents Cultivation Funded Programs 2022 of Renmin University of China. Xin Zhao is the corresponding author.</p>
<h2>References</h2>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Wenhu Chen. 2023. Large language models are few(1)shot table reasoners. In Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 10901100. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020. Tabfact: A large-scale dataset for table-based fact verification. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.</p>
<p>Yi Chen, Wei Wang, Ziyang Liu, and Xuemin Lin. 2009. Keyword search on structured and semi-structured data. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2009, Providence, Rhode Island, USA, June 29 - July 2, 2009, pages 1005-1010. ACM.</p>
<p>Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. 2023. Exploring the potential of large language models (llms) in learning on graphs. CoRR, abs/2307.03393.</p>
<p>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022. Binding language models in symbolic languages. CoRR.</p>
<p>Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2021. Structure-grounded pretraining for text-to-sql. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June $6-11,2021$, pages $1337-1350$.</p>
<p>Julian Martin Eisenschlos, Syrine Krichene, and Thomas Müller. 2020. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, pages 281-296.</p>
<p>Denis Emelin, Daniele Bonadiman, Sawsan Alqahtani, Yi Zhang, and Saab Mansour. 2022. Injecting domain knowledge in language models for task-oriented dialogue systems. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11962-11974. Association for Computational Linguistics.</p>
<p>Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, and Pengsheng Huang. 2021. Towards robustness of text-tosql models against synonym substitution. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 2505-2515.</p>
<p>Chang Gao, Bowen Li, Wenxuan Zhang, Wai Lam, Binhua Li, Fei Huang, Luo Si, and Yongbin Li. 2022a. Towards generalizable and robust text-to-sql parsing. arXiv preprint arXiv:2210.12674.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022b. PAL: program-aided language models. CoRR.</p>
<p>Yu Gu, Xiang Deng, and Yu Su. 2023. Don't generate, discriminate: A proposal for grounding language models to real-world environments. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4928-4949. Association for Computational Linguistics.</p>
<p>Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. Improving multi-hop knowledge base question answering by learning intermediate supervision signals. In WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021, pages 553-561. ACM.</p>
<p>Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. 2020. Tapas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages $4320-4333$.</p>
<p>Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. CoRR, abs/2208.03299.</p>
<p>Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. 2023. Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, Findings of ACL.</p>
<p>Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. A survey on complex knowledge base question answering: Methods, challenges and solutions. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 44834491. ijcai.org.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 78717880 .</p>
<p>Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, and Wen-tau Yih. 2020. Efficient one-pass end-to-end entity linking for questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6433-6441. Association for Computational Linguistics.</p>
<p>Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. 2023a. Decoupling the skeleton parsing and schema linking for text-to-sql. arXiv preprint arXiv:2302.05965.</p>
<p>Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b. Halueval: A largescale hallucination evaluation benchmark for large language models. CoRR, abs/2305.11747.</p>
<p>Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu Chen. 2023c. Few-shot in-context learning for knowledge base question answering. CoRR.</p>
<p>Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V. Le, and Ni Lao. 2018. Memory augmented policy optimization for program synthesis and semantic parsing. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages $10015-10027$.</p>
<p>Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S. Yu. 2023a. A comprehensive evaluation of chatgpt's zeroshot text-to-sql capability. CoRR, abs/2303.13547.</p>
<p>Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S. Yu. 2023b. A comprehensive evaluation of chatgpt's zero-shot text-to-sql capability. CoRR, abs/2303.13547.</p>
<p>Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022. TAPEX: table pre-training via learning a neural SQL executor. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.</p>
<p>Qian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and Min Lin. 2023c. From zero to hero: Examining the power of symbolic tasks in instruction tuning. CoRR, abs/2304.07995.</p>
<p>Alexander H. Miller, Adam Fisch, Jesse Dodge, AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 1400-1409.</p>
<p>Fedor Moiseev, Zhe Dong, Enrique Alfonseca, and Martin Jaggi. 2022. SKILL: structured knowledge infusion for large language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 15811588. Association for Computational Linguistics.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: Browserassisted question-answering with human feedback. CoRR.</p>
<p>Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Sejr Schlichtkrull, Sonal Gupta, Yashar Mehdad, and Scott Yih. 2022. Unik-qa: Unified representations of structured and unstructured knowledge for opendomain question answering. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 1535-1546. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022a. Training language models to follow instructions with human feedback. In NeurIPS.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022b. Training language models to follow instructions with human feedback. In NeurIPS.</p>
<p>Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 14701480.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. CoRR, abs/2302.12813.</p>
<p>Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and Zhouhan Lin. 2022. Rasat: Integrating relational structures into pretrained seq2seq model for text-tosql. arXiv preprint arXiv:2205.06983.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022. Evaluating the text-to-sql capabilities of large language models. CoRR.</p>
<p>Apoorv Saxena, Aditay Tripathi, and Partha P. Talukdar. 2020. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4498-4507.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. CoRR.</p>
<p>Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W. Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 4231-4242.</p>
<p>Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. 2023. Evaluation of chatgpt as a question answering system for answering complex questions. CoRR, abs/2303.07992.</p>
<p>Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020. Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7567-7578.</p>
<p>Xiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew O. Arnold. 2021. Knowledge enhanced pretrained language models: A comprehensive survey. CoRR, abs/2110.08455.</p>
<p>Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,</p>
<p>Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 602-631.</p>
<p>Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning. CoRR.</p>
<p>Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers. The Association for Computer Linguistics.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 3911-3921.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068.</p>
<p>Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. 2018. Variational reasoning for question answering with knowledge graph. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 6069-6076.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR.</p>
<p>Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR.</p>
<p>Table 4: Results of different version of ChatGPT for WebQSP, WTQ, and Spider.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>WebQSP</th>
<th>WTQ</th>
<th>Spider</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT (June)</td>
<td>61.2</td>
<td>43.3</td>
<td>70.1</td>
</tr>
<tr>
<td>ChatGPT (June) + IRR</td>
<td>72.6</td>
<td>48.4</td>
<td>74.8</td>
</tr>
<tr>
<td>ChatGPT (August)</td>
<td>62.1</td>
<td>41.1</td>
<td>75.2</td>
</tr>
<tr>
<td>ChatGPT (August) + IRR</td>
<td>75.3</td>
<td>50.4</td>
<td>77.1</td>
</tr>
</tbody>
</table>
<h2>A Experiment With Latest Version of LLM</h2>
<p>We have noted that the ChatGPT is continuously evolving. Furthermore, we have conducted supplementary experiments on three datasets using the latest August version of LLM. The results are presented in the Table 4. It is noteworthy that ChatGPT indeed continuously evolves, as evidenced by its distinct performance compared to that of the June version. Although the evolved ChatGPT underperforms compared to the June version on the WTQ dataset, our approach can consistently further enhances the ChatGPT performance with the evolved version on all three tasks. It indicates the robustness of our proposed method.</p>
<h2>B Case Study</h2>
<p>Here, we select one representative example for each type of structured data and present the case study in Figure 4. For KG, we first invoke the Extract_Neighbor_Relations function to extract the neighboring relations (e.g., birthplace, residence, and education) of the topic entity "Harper Lee", then linearize them and compose the input prompt. In the prompt, we utilize the instruction (i.e., provide only one relevant relation that's present in the candidate) to elicit the LLM to generate the most relevant relation, i.e., education. Based on the selected relation, we further invoke the Extract_Triples function to extract the triples with the relation to the topic entity. After linearization, another instruction (i.e., you just need to provide only one answer entity), is adopted for guiding the LLM to generate the final answer, i.e., Monroe County High School.</p>
<p>For table, we first invoke the Extract_Column_Name function to extract the column names from the table for linearization, and then design the prompt (i.e., which columns are most relevant to answering the question?) for the LLM to select the useful columns, i.e., District and Incumbent. Then, by using the</p>
<p>Extract_Columns and Extract_SubTable functions and proper instructions, we elicit the LLM to select the useful row indices (i.e., item 8) and finally generate the answer (i.e., 19th).</p>
<p>For database, we also first invoke the Extract_Table\&amp;Column_Name to extract all the table names and column names, linearize them and utilize the instruction (i.e., which tables do you need to complete the SQLite SQL query?) to prompt the LLM. Then, based on the selected tables (i.e., Dogs and Breeds), we further invoke the Extract_Tables_Information function and prompt the LLM via an instruction (i.e., complete sqlite SQL query only with no explanation) to generate the SQL for the question, which can be executed to obtain the final answer.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Case of WebQSP (KGQA)</p>
<p>Question: In what district was the incumbent charles hawkins?</p>
<p>To answer ... first look at the available columns in the table: "District", "Incumbent", ...
Which columns are most relevant to answering the question? ...</p>
<h1>2007 Result</h1>
<p>In the table: "District", "Incumbent, ... In the table: "District, ..., 2007 Result"
Which columns are most relevant to answering the question? ...</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Generate</td>
<td style="text-align: center;">Columns: District, Incumbent.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Generate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Generate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://platform.openai.com/docs/models/gpt-3-5
${ }^{3}$ In our experiment, we use the June version of Davinci-003 and ChatGPT.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
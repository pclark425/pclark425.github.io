<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5433 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5433</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5433</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e" target="_blank">RARR: Researching and Revising What Language Models Say, Using Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes RARR (Retrofit Attribution using Research and Revision), a system that automatically finds attribution for the output of any text generation model, and post-edits the output to fix unsupported content while preserving the original output as much as possible.</p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5433.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5433.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RARR (PaLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrofit Attribution using Research and Revision (PaLM implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A research-and-revise pipeline that post-hoc attributes and minimally edits arbitrary language model outputs by (1) generating verification queries, (2) retrieving evidence, (3) running an agreement check per retrieved snippet, and (4) applying a targeted edit when disagreement is detected; implemented with few-shot prompting of PaLM for query generation, agreement and edit steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (540B) used as the few-shot prompting backbone for RARR components</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 540B, a large transformer language model used in few-shot mode to implement query generation (CQGen), agreement classification (chain-of-thought few-shot prompting), and edit generation (few-shot chain-of-thought editor).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Research-and-Revise (iterative generate-then-reflect / edit)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate a set of verification queries (CQGen) from the original LM output; for each query retrieve web evidence; for each (query, evidence) run an agreement model (PaLM few-shot, chain-of-thought) to decide if the current output agrees with the evidence; if disagreement, run an edit model (PaLM few-shot, identify span then minimally rewrite) to revise the text to align with evidence; repeat for each retrieved snippet and finally select up to M=5 evidence snippets for an attribution report.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Editing for Attribution on NQ (Natural Questions), SQA (StrategyQA), QReCC (conversational QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-form generations from LMs are post-edited to be attributable to web evidence while preserving original intent/structure; datasets: NQ (factoid statements), SQA (reasoning chains), QReCC (knowledge-intensive dialogs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>auto-AIS improved: NQ 45.6 -> 54.9; SQA 37.6 -> 45.1; QReCC 18.8 -> 29.4. Preservation (intent) ~90%+; Pres_Lev: NQ 89.6, SQA 89.9, QReCC 80.2. F1_AP (harmonic mean of attribution and preservation) reported: NQ 68.1, SQA 60.0, QReCC 43.1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Before editing (i.e., original LM outputs without RARR): auto-AIS values: NQ 45.6; SQA 37.6; QReCC 18.8. Retrieving evidence without editing yields much lower attribution (low teens to mid 30s reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: RARR increases automated attribution (auto-AIS) by up to ~9.3 points absolute on NQ (45.6->54.9), ~7.5 on SQA (37.6->45.1), and ~10.6 on QReCC (18.8->29.4); RARR preserves intent >90% of the time (much higher than baselines). Downstream impact: for NQ short-answer extraction, RARR improved short-answer accuracy by roughly 5% compared to original passages; qualitative examples show corrected entities and numerics while retaining original structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failures include edits driven by misleading or out-of-context retrievals (incorrectly adopting evidence that is not topical or temporally mismatched), not updating subsequent dependent reasoning steps after fixing an upstream fact (broken logical coherence in reasoning chains), tendency to preserve claims it cannot attribute (leaving some hallucinations intact), computational cost due to repeated large-LM prompting, and sensitivity to query generation strategy (sentence-as-query less robust than CQGen; removing agreement model causes over-editing and lower preservation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RARR: Researching and Revising What Language Models Say, Using Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5433.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5433.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RARR (GPT-3 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RARR implemented with GPT-3 few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the RARR pipeline where the query generation, agreement, and edit components are implemented using GPT-3 (text-davinci-003) instead of PaLM; reported as an open-source variant performing comparably or slightly better on some automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003 used for RARR components in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family (text-davinci-003), used in few-shot prompting mode for CQGen, agreement, and edit models in the RARR workflow; described as slightly tuned prompts for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Research-and-Revise (iterative generate-then-reflect / edit) - GPT-3 backed</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same pipeline as RARR: generate verification queries, retrieve evidence, run agreement checks and targeted edits with GPT-3 few-shot prompts; edits are rejected if edit distance is too large.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Editing for Attribution on NQ, SQA, QReCC (same evaluation suites as PaLM RARR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Post-editing LM outputs to be attributable on NQ (factoid), SQA (reasoning chains), and QReCC (dialogs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported (Table 4/5): NQ auto-AIS 44.3 -> 55.0 (Pres_Lev 90.6; F1_AP 68.5); SQA 38.6 -> 46.6 (Pres_Lev 89.3; F1_AP 61.2); QReCC 18.3 -> 28.6 (Pres_Lev 89.8; F1_AP 43.4). On passages originally from GPT-3, GPT-3 RARR achieved NQ auto-AIS 48.0 -> 59.3 and F1_AP 72.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Original GPT-3 passages before RARR (example): NQ auto-AIS 48.0 (before editing) in one reported split; retrieving evidence without editing gave substantially lower attribution measures (text reports low-teens to mid-30s depending on setup).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative improvements similar to PaLM RARR: attribution increases (e.g., NQ +~11 points auto-AIS in some settings) and high preservation scores; the authors report the GPT-3 based RARR slightly outperforms PaLM-based RARR on some datasets and release this as an open-source variant.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same class of failures as PaLM RARR: risk of adopting misleading evidence, not repairing downstream reasoning dependencies, and computational cost; performance depends on the few-shot prompt tuning and model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RARR: Researching and Revising What Language Models Say, Using Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5433.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5433.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaMDA Research</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LaMDA research-and-revise inference (LaMDA Research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LaMDA's multi-stage dialog response generation that first produces a base response, then generates search queries, and finally generates a revised response conditioned on retrieved evidence; used as a baseline research-and-revise system in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LaMDA: Language models for dialog applications</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (dialog model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LaMDA, a dialog-oriented large language model (Thoppilan et al., 2022), trained to produce base responses, generate search queries from the base response, and produce a revised response conditioned on retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LaMDA Research (generate base response -> generate queries -> revise)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A three-step process: (1) generate a base response, (2) generate search queries from the base response and retrieve evidence, (3) generate a revised response conditioned on the base response and retrieved evidence — effectively an integrated generate-then-revise workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Editing for Attribution on NQ, SQA, QReCC (used as a baseline by running its research stages conditioned on given x)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied LaMDA's two latter stages (query generation + revision) to existing model outputs by setting their base response to the input passage x, then running LaMDA's research to produce a revised output and evidence set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported attribution improvements (auto-AIS): For PaLM outputs on NQ, LaMDA auto-AIS improved 39.5 -> 49.9; on SQA 32.7 -> 43.2; on QReCC 16.4 -> 36.2. Preservation (intent) scores were substantially lower than RARR (LaMDA frequently changed style/intent).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base responses (before LaMDA Research) had lower attribution: e.g., NQ auto-AIS ~39.5 for the evaluated set; LaMDA Research increased attribution but at cost to preservation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: LaMDA Research increases auto-AIS by ~10 points on NQ and SQA and by ~20 points on QReCC in reported experiments. Qualitative: it can improve attribution but often alters style/intent (lower preservation), consistent with being designed for dialog rather than preserving arbitrary inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LaMDA Research tended to deviate from the original intent and linguistic style (lower preservation), was not optimized for preserving arbitrary input passages, and sometimes produced large stylistic or content changes; comparing apples-to-apples is limited because LaMDA was not trained for the specific preservation objective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RARR: Researching and Revising What Language Models Say, Using Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LaMDA: Language models for dialog applications <em>(Rating: 2)</em></li>
                <li>Evidence-based factual error correction <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5433",
    "paper_id": "paper-66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "RARR (PaLM)",
            "name_full": "Retrofit Attribution using Research and Revision (PaLM implementation)",
            "brief_description": "A research-and-revise pipeline that post-hoc attributes and minimally edits arbitrary language model outputs by (1) generating verification queries, (2) retrieving evidence, (3) running an agreement check per retrieved snippet, and (4) applying a targeted edit when disagreement is detected; implemented with few-shot prompting of PaLM for query generation, agreement and edit steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM (540B) used as the few-shot prompting backbone for RARR components",
            "model_description": "PaLM 540B, a large transformer language model used in few-shot mode to implement query generation (CQGen), agreement classification (chain-of-thought few-shot prompting), and edit generation (few-shot chain-of-thought editor).",
            "reflection_method_name": "Research-and-Revise (iterative generate-then-reflect / edit)",
            "reflection_method_description": "Generate a set of verification queries (CQGen) from the original LM output; for each query retrieve web evidence; for each (query, evidence) run an agreement model (PaLM few-shot, chain-of-thought) to decide if the current output agrees with the evidence; if disagreement, run an edit model (PaLM few-shot, identify span then minimally rewrite) to revise the text to align with evidence; repeat for each retrieved snippet and finally select up to M=5 evidence snippets for an attribution report.",
            "num_iterations": null,
            "task_name": "Editing for Attribution on NQ (Natural Questions), SQA (StrategyQA), QReCC (conversational QA)",
            "task_description": "Long-form generations from LMs are post-edited to be attributable to web evidence while preserving original intent/structure; datasets: NQ (factoid statements), SQA (reasoning chains), QReCC (knowledge-intensive dialogs).",
            "performance_with_reflection": "auto-AIS improved: NQ 45.6 -&gt; 54.9; SQA 37.6 -&gt; 45.1; QReCC 18.8 -&gt; 29.4. Preservation (intent) ~90%+; Pres_Lev: NQ 89.6, SQA 89.9, QReCC 80.2. F1_AP (harmonic mean of attribution and preservation) reported: NQ 68.1, SQA 60.0, QReCC 43.1.",
            "performance_without_reflection": "Before editing (i.e., original LM outputs without RARR): auto-AIS values: NQ 45.6; SQA 37.6; QReCC 18.8. Retrieving evidence without editing yields much lower attribution (low teens to mid 30s reported in text).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: RARR increases automated attribution (auto-AIS) by up to ~9.3 points absolute on NQ (45.6-&gt;54.9), ~7.5 on SQA (37.6-&gt;45.1), and ~10.6 on QReCC (18.8-&gt;29.4); RARR preserves intent &gt;90% of the time (much higher than baselines). Downstream impact: for NQ short-answer extraction, RARR improved short-answer accuracy by roughly 5% compared to original passages; qualitative examples show corrected entities and numerics while retaining original structure.",
            "limitations_or_failure_cases": "Failures include edits driven by misleading or out-of-context retrievals (incorrectly adopting evidence that is not topical or temporally mismatched), not updating subsequent dependent reasoning steps after fixing an upstream fact (broken logical coherence in reasoning chains), tendency to preserve claims it cannot attribute (leaving some hallucinations intact), computational cost due to repeated large-LM prompting, and sensitivity to query generation strategy (sentence-as-query less robust than CQGen; removing agreement model causes over-editing and lower preservation).",
            "uuid": "e5433.0",
            "source_info": {
                "paper_title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "RARR (GPT-3 variant)",
            "name_full": "RARR implemented with GPT-3 few-shot prompting",
            "brief_description": "A variant of the RARR pipeline where the query generation, agreement, and edit components are implemented using GPT-3 (text-davinci-003) instead of PaLM; reported as an open-source variant performing comparably or slightly better on some automated metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003 used for RARR components in ablation)",
            "model_description": "GPT-3 family (text-davinci-003), used in few-shot prompting mode for CQGen, agreement, and edit models in the RARR workflow; described as slightly tuned prompts for GPT-3.",
            "reflection_method_name": "Research-and-Revise (iterative generate-then-reflect / edit) - GPT-3 backed",
            "reflection_method_description": "Same pipeline as RARR: generate verification queries, retrieve evidence, run agreement checks and targeted edits with GPT-3 few-shot prompts; edits are rejected if edit distance is too large.",
            "num_iterations": null,
            "task_name": "Editing for Attribution on NQ, SQA, QReCC (same evaluation suites as PaLM RARR)",
            "task_description": "Post-editing LM outputs to be attributable on NQ (factoid), SQA (reasoning chains), and QReCC (dialogs).",
            "performance_with_reflection": "Reported (Table 4/5): NQ auto-AIS 44.3 -&gt; 55.0 (Pres_Lev 90.6; F1_AP 68.5); SQA 38.6 -&gt; 46.6 (Pres_Lev 89.3; F1_AP 61.2); QReCC 18.3 -&gt; 28.6 (Pres_Lev 89.8; F1_AP 43.4). On passages originally from GPT-3, GPT-3 RARR achieved NQ auto-AIS 48.0 -&gt; 59.3 and F1_AP 72.0.",
            "performance_without_reflection": "Original GPT-3 passages before RARR (example): NQ auto-AIS 48.0 (before editing) in one reported split; retrieving evidence without editing gave substantially lower attribution measures (text reports low-teens to mid-30s depending on setup).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative improvements similar to PaLM RARR: attribution increases (e.g., NQ +~11 points auto-AIS in some settings) and high preservation scores; the authors report the GPT-3 based RARR slightly outperforms PaLM-based RARR on some datasets and release this as an open-source variant.",
            "limitations_or_failure_cases": "Same class of failures as PaLM RARR: risk of adopting misleading evidence, not repairing downstream reasoning dependencies, and computational cost; performance depends on the few-shot prompt tuning and model scale.",
            "uuid": "e5433.1",
            "source_info": {
                "paper_title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LaMDA Research",
            "name_full": "LaMDA research-and-revise inference (LaMDA Research)",
            "brief_description": "LaMDA's multi-stage dialog response generation that first produces a base response, then generates search queries, and finally generates a revised response conditioned on retrieved evidence; used as a baseline research-and-revise system in the paper.",
            "citation_title": "LaMDA: Language models for dialog applications",
            "mention_or_use": "use",
            "model_name": "LaMDA (dialog model)",
            "model_description": "LaMDA, a dialog-oriented large language model (Thoppilan et al., 2022), trained to produce base responses, generate search queries from the base response, and produce a revised response conditioned on retrieved evidence.",
            "reflection_method_name": "LaMDA Research (generate base response -&gt; generate queries -&gt; revise)",
            "reflection_method_description": "A three-step process: (1) generate a base response, (2) generate search queries from the base response and retrieve evidence, (3) generate a revised response conditioned on the base response and retrieved evidence — effectively an integrated generate-then-revise workflow.",
            "num_iterations": null,
            "task_name": "Editing for Attribution on NQ, SQA, QReCC (used as a baseline by running its research stages conditioned on given x)",
            "task_description": "Applied LaMDA's two latter stages (query generation + revision) to existing model outputs by setting their base response to the input passage x, then running LaMDA's research to produce a revised output and evidence set.",
            "performance_with_reflection": "Reported attribution improvements (auto-AIS): For PaLM outputs on NQ, LaMDA auto-AIS improved 39.5 -&gt; 49.9; on SQA 32.7 -&gt; 43.2; on QReCC 16.4 -&gt; 36.2. Preservation (intent) scores were substantially lower than RARR (LaMDA frequently changed style/intent).",
            "performance_without_reflection": "Base responses (before LaMDA Research) had lower attribution: e.g., NQ auto-AIS ~39.5 for the evaluated set; LaMDA Research increased attribution but at cost to preservation.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: LaMDA Research increases auto-AIS by ~10 points on NQ and SQA and by ~20 points on QReCC in reported experiments. Qualitative: it can improve attribution but often alters style/intent (lower preservation), consistent with being designed for dialog rather than preserving arbitrary inputs.",
            "limitations_or_failure_cases": "LaMDA Research tended to deviate from the original intent and linguistic style (lower preservation), was not optimized for preserving arbitrary input passages, and sometimes produced large stylistic or content changes; comparing apples-to-apples is limited because LaMDA was not trained for the specific preservation objective.",
            "uuid": "e5433.2",
            "source_info": {
                "paper_title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LaMDA: Language models for dialog applications",
            "rating": 2
        },
        {
            "paper_title": "Evidence-based factual error correction",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 1
        }
    ],
    "cost": 0.015136999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RARR: Researching and Revising What Language Models Say, Using Language Models</h1>
<p>Luyu Gao ${ }^{1 \circ <em>}$ Zhuyun Dai ${ }^{2 </em>}$ Panupong Pasupat ${ }^{2 <em>}$ Anthony Chen ${ }^{3 \circ </em>}$ Arun Tejasvi Chaganty ${ }^{2 <em>}$ Yicheng Fan ${ }^{2 </em>}$ Vincent Y. Zhao ${ }^{2}$ Ni Lao ${ }^{2}$ Hongrae Lee ${ }^{2}$ Da-Cheng Juan ${ }^{2}$ Kelvin Guu ${ }^{2 *}$<br>${ }^{1}$ Carnegie Mellon University, ${ }^{2}$ Google Research, ${ }^{3}$ UC Irvine<br>luyug@cs.cmu.edu anthony.chen@uci.edu<br>{zhuyundai, ppasupat, arunchaganty, yichengfan, vzhao, nlao, hrlee, dacheng, kguu}@google.com</p>
<h4>Abstract</h4>
<p>Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Generative language models (LMs) and other text generation models are now the backbone of many AI systems. For example, large language models can perform multi-step reasoning (Nye et al., 2021; Wei et al., 2022), generate plans (Ahn et al., 2022), use tools and APIs (Shin et al., 2021; Thoppilan et al., 2022), and answer open-domain questions (Petroni et al., 2019; Roberts et al., 2020).</p>
<p>Despite these incredible advances, state-of-theart LMs still frequently produce biased, misleading,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The Editing for Attribution task. The input $x$ is a text passage produced by a generation model. Our Research \&amp; Revision model outputs an attribution report $A$ containing retrieved evidence snippets, along with a revision $y$ whose content can be attributed to the evidence in $A$ while preserving other properties of $x$ such as style or structure.
or unsupported content, colloquially called "hallucinations" (Maynez et al., 2020; Menick et al., 2022). To make LMs more trustworthy, we want to justify each generation by an attribution report (Rashkin et al., 2021; Bohnet et al., 2022) that contains supporting evidence from trusted sources (e.g., encyclopedia or articles) where appropriate.</p>
<p>Most existing LMs, such as those based on sequence-to-sequence architectures, lack a builtin mechanism for attribution. Even retrievalaugmented models (Guu et al., 2020; Lewis et al., 2020), which retrieve relevant documents and then condition on them to generate text, still do not guarantee attribution. Prior work has shown that</p>
<p>retrieval-augmented models generate text that either includes additional information outside the retrieved documents (Dziri et al., 2022), ignores the documents altogether (Krishna et al., 2021), or even contradicts the documents (Longpre et al., 2021). In fact, occasionally ignoring the retrievals can make the models more robust to bad retrievals (Khandelwal et al., 2020), illustrating that end-task performance and attribution are not always aligned.</p>
<p>Instead of constraining LMs to generate attributed text, we propose a model-agnostic approach to improve the attribution of any existing LM: Retrofit Attribution using Research and Revision (RARR). The approach is inspired by works on fact-checking ${ }^{2}$ where simple research-and-revise workflows are effective at attributing or correcting unattributed claims made by humans (Thorne et al., 2018; Schuster et al., 2021; Thorne and Vlachos, 2021). As shown in Figure 1, after generating text with the LM, RARR does research to retrieve relevant evidence, and then revises the text to make it consistent with the evidence while preserving qualities like style or structure, enabling the revised text to be seamlessly used in place of the original. RARR can be viewed as a retrievalaugmented model where retrieval happens after generation rather than before. This allows RARR to stand on the shoulders of giant LMs without having to modify them to support attribution.</p>
<p>In our effort to expand the scope of Research \&amp; Revision models to handle the output of arbitrary LMs, we make the following contributions. First, we formalize the Editing for Attribution task and propose new metrics that evaluate revision models not just on their ability to produce well-attributed revisions, but also on their ability to otherwise preserve original properties of the text. Second, we use these metrics to benchmark how existing revision models perform on various types of LM outputs such as knowledge-intensive statements, reasoning chains, and dialog responses. Finally, we find that existing revision models do not always generalize across many tasks (and were not originally intended to), and therefore propose a new research-and-revise model that leverages the power of few-shot prompting in large language models to robustly generalize across domains.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of RARR, which improves attribution for a text passage via Research \&amp; Revision. Given the input text passage, the research stage uses a query generator to raise questions about different aspects of the text. The retriever then searches for evidence to investigate each query. The revision stage first runs an agreement model to detect disagreement between the text and the evidence, then runs an edit model to revise the text if needed. Finally, $M$ evidence snippets are selected to form an attribution report.</p>
<h2>2 Task formulation</h2>
<p>We propose the task of Editing for Attribution as follows. As Figure 1 shows, the input to the system is a text passage $x$ produced by a generation model. The output is a revised text passage $y$ along with an attribution report $A$, which contains evidence snippets $e_{1}, \ldots, e_{M}$ that support the content in $y$. Optionally, the attribution report can contain additional information such as the alignment between evidence snippets and relevant parts in $y$.</p>
<p>We propose to measure the quality of the revised text $y$ and attribution report $A$ along two dimensions: (1) attribution: how much of the revised text $y$ can be attributed to the evidence in $A$, and (2) preservation: how much the revised text $y$ preserves aspects of the original text $x$.</p>
<h3>2.1 Measuring attribution</h3>
<p>Previously, Rashkin et al. (2021) proposed Attributable to Identified Sources (AIS), a human evaluation framework which considers a binary notion of attribution. Roughly speaking, a text passage $y$ is attributable to a set $A$ of evidence if a</p>
<p>generic hearer would affirm the statement "According to $A, y$ " under the context of $y$. A system either receives full credit (1.0) if all content in $y$ can be attributed to $A$, and no credit ( 0.0 ) otherwise.</p>
<p>We propose a more fine-grained, sentence-level extension of AIS. We ask annotators to give an AIS score for each sentence $s$ of $y$, and then report the average AIS score across all sentences:</p>
<p>$$
\operatorname{Attr}_{\mathrm{AIS}}(y, A)=\underset{s \in y}{\operatorname{avg}} \operatorname{AIS}(s, A)
$$</p>
<p>Since the AIS score is binary, this effectively measures the percentage of sentences in $y$ that are fully attributed to $A$. When judging each sentence, we also give annotators access to the surrounding sentences and other necessary context, such as the question that the text passage responded to. We also impose the maximum number of evidence snippets in the attribution report $A$ to make it concise enough for both the annotator and downstream users. By manually inspecting 30 examples from our benchmarks, we found $M=5$ snippets to be sufficient for full attribution.</p>
<p>During model development, we define an automated metric, auto-AIS (Attr auto ), that approximates human AIS judgments. We utilize the natural language inference (NLI) model from Honovich et al. (2022), which correlates well with AIS scores. For each sentence $s$ of $y$, and for each evidence snippet $e$ in $A$, let $\operatorname{NLI}(e, s)$ be the model probability of $e$ entailing $s$. We then define</p>
<p>$$
\operatorname{Attr}<em A="A" _in="\in" e="e">{\text {auto }}(y, A)=\underset{s \in y}{\operatorname{avg}} \max </em>(e, s)
$$} \operatorname{NLI</p>
<p>To improve accuracy, we decontextualize (Choi et al., 2021) each sentence based on the entire context of $y$ before computing the scores. See Appendix B for implementation details.</p>
<h3>2.2 Measuring preservation</h3>
<p>To measure preservation, we first ask annotators to decide if the revision preserves the text's original intent (completely, somewhat, or not at all — see Appendix C for exact rubrics). Like AIS evaluation, we give annotators the necessary surrounding context. We define the binary metric $\operatorname{Pres}_{\text {intent }}(x, y)$ to be 1.0 if the revision completely preserves the original intent, and 0.0 otherwise.</p>
<p>However, even if a revision preserves intent, it may still make superfluous modifications, such as reordering words, changing textual style, or including unnecessary additional information (Thorne
and Vlachos, 2021). Different tasks have different requirements for what should be preserved. Here, we desire a simple metric that can be readily computed for many tasks and that generally penalizes unnecessary changes. We thus define a metric based on the character-level Levenshtein edit distance (Levenshtein, 1965) between $x$ and $y$ :</p>
<p>$$
\operatorname{Pres}_{\mathrm{Lev}}(x, y)=\max \left(1-\frac{\operatorname{Lev}(x, y)}{\operatorname{length}(x)}, 0\right)
$$</p>
<p>This metric is 1.0 if $x$ and $y$ are the same, and 0.0 if $y$ completely overwrites all parts of $x$. Pres $_{\text {Lev }}$ is generally sensitive to any kind of change, but certainly does not capture all notions of preservation (e.g., preserving rhyme schemes or puns).</p>
<p>We want the revision to preserve the original intent while avoiding superfluous edits. To reflect this, we finally combine the two metrics as</p>
<p>$$
\operatorname{Pres}<em _intent="{intent" _text="\text">{\text {comb }}(x, y)=\operatorname{Pres}</em>(x, y)
$$}}(x, y) \cdot \operatorname{Pres}_{\text {Lev }</p>
<p>which is 0.0 if the revision changes the intent and equal to $\operatorname{Pres}<em _Lev="{Lev" _text="\text">{\text {Lev }}(x, y)$ otherwise. Since Pres intent requires human annotation, we use Pres $</em>$ as an automated metric for model development.}</p>
<h3>2.3 Discussion</h3>
<p>Optimizing for attribution alone cannot ensure a good revision: for example, an adversarial editor could ensure $100 \%$ attribution by simply replacing the input $x$ with the text of any arbitrary retrieved document, which is trivially attributable to itself. Ideally, we want to maximize both attribution and preservation, while navigating any tradoffs between the two. In our experiments, we report both metrics, as well as their harmonic mean ( $\mathrm{F} 1_{\mathrm{AP}}$, analogous to how recall and precision are combined in F1).</p>
<p>We emphasize that this evaluation scheme does not require any "gold" or "reference" edits (unlike many prior evaluations of text revision models), which are often only available for specialized domains. This enables us to broaden the scope to a much wider range of generation tasks.</p>
<h2>3 Approach</h2>
<p>We now present Retrofit Attribution using Research and Revision (RARR), a simple method for solving the Editing for Attribution task. As illustrated in Figure 2, given an input passage $x$, the research stage first generates a set of queries $\left{q_{1}, \ldots, q_{N}\right}$, each investigating one aspect of $x$ that potentially</p>
<p>(a) Query generation $x \rightarrow\left{q_{1}, \ldots, q_{N}\right}$ You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This is to prevent a buildup of mucus. It's called the nasal cycle.
To verify it,
a) I googled: Does your nose switch between nostrils?
b) I googled: How often does your nostrils switch?
c) I googled: Why does your nostril switch?
d) I googled: What is nasal cycle?
(b) Agreement model $(y, q, e) \rightarrow{0,1}$</p>
<p>You said: Your nose switches ... (same as above)...nasal cycle. I checked: How often do your nostrils switch?
I found this article: Although we don't usually notice it, during the nasal cycle one nostril becomes congested and thus contributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every 2 hours, according to a small 2016 study published in the journal PLOS One.
Your nose's switching time is about every 2 hours, not 45 minutes. This disagrees with what you said.
(c) Edit model $(y, q, e) \rightarrow$ new $y$</p>
<p>You said: Your nose switches ... (same as above)...nasal cycle. I checked: How often do your nostrils switch?
I found this article: Although we ... (same as above)...PLOS One. This suggests 45 minutes switch time in your statement is wrong. My fix: Your nose switches back and forth between nostrils. When you sleep, you switch about every 2 hours. This is to prevent a buildup of mucus. It's called the nasal cycle.</p>
<p>Figure 3: Examples of few-shot examples used to prompt the PaLM model (blue $=$ input; red $=$ output).
requires attribution. For each query $q_{i}$, it retrieves web documents and selects the best evidence snippets $\left{e_{i 1}, e_{i 2}, \ldots\right}$. The revision stage then revises the original text $x$ using the retrieval results $\left{\left(q_{1}, e_{11}\right), \ldots\right}$, yielding a revised text $y$.</p>
<p>Most components for RARR are implemented using few-shot prompting (Brown et al., 2020). We use PaLM (Chowdhery et al., 2022) as our language model. Figure 3 shows some few-shot examples we use, while Appendix D lists the full prompts.</p>
<h3>3.1 Research stage</h3>
<p>Query generation We perform comprehensive question generation (CQGen) which produces a sequence of questions covering all aspects of the passage $x$ that need to be verified and attributed. A similar strategy has been employed to train textplanning models (Narayan et al., 2022). A prompt with six human demonstrations was sufficient for PaLM to adequately learn the task. To increase diversity and coverage, we sample from our CQGen model three times and take the union of the resulting queries.</p>
<p>Evidence retrieval For each query from CQGen, we use Google Search to retrieve $K=5$ web pages. We extract candidate evidence snippets from each web page by running a sliding window of four sentences across the page, breaking at document headings. The evidence snippets for each query
are then ranked based on their relevance to the query. For this, we use an existing query-document relevance model trained following Ni et al. (2021), which computes a relevance score $S_{\text {relevance }}(q, e)$ between a query $q$ and an evidence snippet $e$. We then keep the top $J=1$ evidence for each query. The final retrieval result is $\left[\left(q_{1}, e_{11}\right), \ldots,\left(q_{1}, e_{1 J}\right)\right.$, $\left.\ldots,\left(q_{N}, e_{N 1}\right), \ldots,\left(q_{N}, e_{N J}\right)\right]$, where $e_{i j}$ denotes the $j^{\text {th }}$ evidence for the $i^{\text {th }}$ query, and $N$ denotes the total number of queries from CQGen (which can be different for each input $x$ ).</p>
<h3>3.2 Revision stage</h3>
<p>After retrieving evidence, certain parts of $x$ may now be properly attributed, but other parts remain unattributed and should be revised. As illustrated in Figure 2, the revision stage initializes the output $y=x$. Then for each retrieved $(q, e)=\left(q_{i}, e_{i j}\right)$, the agreement model checks if the evidence $e$ disagrees with the current output $y$ regarding the issue in query $q$. If a disagreement is detected, the edit model edits $y$ to agree with $e$; otherwise, it does nothing. The process continues until all retrievals are processed.</p>
<p>Agreement model The agreement model takes the partially edited passage $y$, a query $q$, and the evidence $e$ as input. It then decides whether both $y$ and $e$ imply the same answer to the question in $q$. This form of question-guided agreement was previously explored by Honovich et al. (2021). We implement this by few-shot prompting PaLM using a chain-of-thought style prompt (Wei et al., 2022), where we ask the model to explicitly state the implied answers for both $y$ and $e$ before producing its judgment about their agreement.</p>
<p>Edit model The edit model is run only if a disagreement is detected. The model takes $y, q$ and $e$ as input, and outputs a new version of $y$ that aims to agree with $e$ while otherwise minimally altering $y$. We again use few-shot prompting and chain-ofthought, where we ask the model to first identify a particular span in $y$ that needs to be edited before generating the revised $y$. This helps reduce the editor's deviation from the current $y .^{3}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.3 Attribution report</h3>
<p>Finally, we select at most $M=5$ evidence snippets to form an attribution report $A$. Note that during evidence retrieval and revision, we may have encountered and used more than $M$ snippets. Our goal is to find a subset of snippets that maximizes coverage over the potentially attributable points in the passage, as represented by the queries $q_{1}, \ldots, q_{N}$. We use the relevance model from Section 3.1 as a proxy for measuring how much an evidence $e$ covers the point raised by a query $q$. Then, we exhaustively search for $A \subseteq\left{e_{11}, \ldots, e_{N J}\right}$ of size at most $M$ that maximizes</p>
<p>$$
\operatorname{Cover}\left(A, q_{1: N}\right):=\sum_{i=1}^{N} \max <em _relevance="{relevance" _text="\text">{e \in A} S</em>, e\right)
$$}}\left(q_{i</p>
<h2>4 Related work</h2>
<p>Fact-checking Our research builds upon works to identify whether a claim is supported or refuted by the given evidence (Thorne et al., 2018; Wang, 2017; Karadzhov et al., 2017; Augenstein et al., 2019; Wadden et al., 2020). In real-world scenarios such as the one which RARR operates in, relevant evidence may not be provided, necessitating retrieval (Fan et al., 2020; Piktus et al., 2021).</p>
<p>Post-hoc editing for factuality Recent work has gone beyond checking the validity of a claim to correcting a piece of text to be factually consistent with a set of evidence via post-hoc editing (Shah et al., 2020; Thorne and Vlachos, 2021; Schuster et al., 2021; Balachandran et al., 2022; Cao et al., 2020; Iso et al., 2020). FRUIT (Logan IV et al., 2022) and PEER (Schick et al., 2022) both implement an editor that is fine-tuned on Wikipedia edit history with the goal of updating outdated information and collaborative writing respectively. Evidence-based Factual Error Correction (EFEC; Thorne and Vlachos, 2021) also implements a full research-and-revise workflow trained on Wikipedia passages (Thorne et al., 2018). A key differentiator of RARR is its ability to edit the output of any generation model without being restricted by the domain, task, or the need for training data.</p>
<p>Measuring attribution A key part of improving attribution is being able to quantify it. Apart from human evaluation (Rashkin et al., 2021), several automated evaluation methods have been proposed. Our work uses an entailment-based metric, which measures whether the referenced evidence entails</p>
<p>PaLM outputs on NQ (factoid statements)
Millie Inbetween is a British comedy television series. It premiered on 24 February 2014 on BBC One. The first series was produced by John Yorke and Phil Clymer.</p>
<p>PaLM outputs on SQA (reasoning chains)
The highest point of Mount Wycheproof is 70 metres. Edmund Hillary climbed Mount Everest, which is 8,848 metres. So Mount Wycheproof would be a breeze for Edmund Hillary.</p>
<p>LaMDA outputs on QReCC (knowledge-intensive dialogs)
When was Welsh social reformer Robert Owen born?
Robert Owen was born on 14 May 1771
$\left.\right}_{\text {context }}$
Did he have another job?
In 1810 he moved to Manchester and established a draper's shop.
Figure 4: Examples of input passages. For QReCC, prior dialog turns are also given as the context.
the output text (Bohnet et al., 2022; Kryscinski et al., 2020; Goyal and Durrett, 2021). A common alternative is to evaluate whether the output text contains the same factual information as the evidence; e.g., by checking if both yield the same answer to the same question (Wang et al., 2020). We use this notion of attribution in RARR's agreement model rather than for evaluation.</p>
<p>Retrieval-augmented models Models with a retrieval component have seen successes in question answering (Chen et al., 2017; Lee et al., 2019; Nakano et al., 2021), machine translation (Zhang et al., 2018), code generation (Hayati et al., 2018), language modeling (Khandelwal et al., 2020), and other knowledge-intensive tasks (Lewis et al., 2020). Their retrievals are not necessarily attributions (Dziri et al., 2022; Longpre et al., 2021) and typically are not used to revise an existing output. An exception is LaMDA (Thoppilan et al., 2022), a language model for dialog that performs revision by training on human annotations.</p>
<h2>5 Experiments</h2>
<h3>5.1 Evaluation setups</h3>
<p>RARR aspires to be a general-purpose method for improving the attribution of any text generation model in any text domain. We thus construct evaluation benchmarks by taking the task input from three diverse datasets, and prompting different generation models to produce long-form outputs which may contain "hallucinations," as demonstrated in Figure 4. These long-form outputs serve as input text passages to RARR. We generate 150 development and 150 test passages for each combination of generation model and source dataset.</p>
<p>Factoid statements We prompt PaLM 540B and GPT-3 text-davinci-002 to generate long-form answers to questions from the Natural Questions dev set (NQ; Kwiatkowski et al., 2019). The resulting passages are mostly coherent but often contain factual errors. This setup examines the ability to attribute a diverse range of factoid knowledge.</p>
<p>Reasoning chains Language models can generate reasoning chains to answer complex questions (Wei et al., 2022). We use PaLM and GPT-3 to generate reasoning chains for the StrategyQA train set (SQA; Geva et al., 2021). This setup tests whether the revision model can provide better attribution for intermediate steps of reasoning, while preserving the overall reasoning process.</p>
<p>Knowledge-intensive dialogs We consider the conversational QA task from the QReCC dev set (Anantha et al., 2021). Given the previous dialog turns, which are rounds of questions and answers $\left(Q_{1}, A_{1}, Q_{2}, A_{2}, \ldots, Q_{k}\right)$, we use LaMDA and GPT-3 to answer to the final question $Q_{k}$ conditioned on the dialog history. The answer tends to be context-dependent, featuring pronouns and implicit references. All dialog turns are given alongside the answer as inputs to the revision model.</p>
<h3>5.2 Models</h3>
<p>We compare RARR to several systems that have a research-and-revise workflow.</p>
<p>EFEC We consider EFEC (Thorne and Vlachos, 2021) as a representative fine-tuned editor. EFEC fine-tunes a T5-based model to revise text conditioned on multiple evidence snippets using both semi-supervised and fully-supervised approaches. We compare against their fully-supervised approach, which performed best in their experiments. EFEC uses a neural retrieval model (Karpukhin et al., 2020) to retrieve from Wikipedia; however, not all passages in our experiments are supported by Wikipedia articles. To more fairly compare the editing capabilities of EFEC, we instead use the evidence retrieved by our research stages (CQGen and web search). Note that the EFEC editor conditions on multiple pieces of evidence at once, while our editor iteratively conditions on one at a time.</p>
<p>LaMDA LaMDA (Thoppilan et al., 2022) generates responses in three steps: 1) generate a "base response"; 2) generate search queries from the base response; 3) generate a "revised response" conditioned on the base response and retrieved evidence.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Attribution</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Preservation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{F1}_{\mathrm{AP}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">auto-AIS</td>
<td style="text-align: center;">AIS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">intent</td>
<td style="text-align: center;">Lev</td>
<td style="text-align: center;">comb</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM outputs on NQ</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">EFEC</td>
<td style="text-align: center;">$45.6 \rightarrow 64.3$</td>
<td style="text-align: center;">$35.4 \rightarrow 48.3$</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">$39.5 \rightarrow 49.9$</td>
<td style="text-align: center;">$18.3 \rightarrow 30.4$</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.9</td>
</tr>
<tr>
<td style="text-align: center;">RARR</td>
<td style="text-align: center;">$45.6 \rightarrow 54.9$</td>
<td style="text-align: center;">$35.4 \rightarrow 43.4$</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">PaLM outputs on SQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">EFEC</td>
<td style="text-align: center;">$37.8 \rightarrow 58.6$</td>
<td style="text-align: center;">$24.5 \rightarrow 51.7$</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7.1</td>
</tr>
<tr>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">$32.7 \rightarrow 43.2$</td>
<td style="text-align: center;">$15.8 \rightarrow 27.0$</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30.0</td>
</tr>
<tr>
<td style="text-align: center;">RARR</td>
<td style="text-align: center;">$37.6 \rightarrow 45.1$</td>
<td style="text-align: center;">$24.5 \rightarrow 31.5$</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: center;">LaMDA outputs on QReCC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">EFEC</td>
<td style="text-align: center;">$19.1 \rightarrow 47.4$</td>
<td style="text-align: center;">$13.2 \rightarrow 48.7$</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">$16.4 \rightarrow 36.2$</td>
<td style="text-align: center;">$16.0 \rightarrow 27.1$</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: center;">RARR</td>
<td style="text-align: center;">$18.8 \rightarrow 29.4$</td>
<td style="text-align: center;">$13.2 \rightarrow 28.3$</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">41.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation results. For attribution, we report the AIS scores of the texts both before and after editing (before $\rightarrow$ after). For preservation, we report intent preservation Pres $<em _Lev="{Lev" _text="\text">{\text {intent }}$, Levenshtein similarity Pres $</em>$, and the combined Pres $}<em _AIS="{AIS" _text="\text">{\text {comb }}$. We summarize Attr $</em>$ and Pres $}<em _mathrm_AP="\mathrm{AP">{\text {comb }}$ using their harmonic mean ( $\mathrm{F} 1</em>$ ).}</p>
<p>To apply LaMDA on a given text $x$, we simply set the base response in step 1 to $x$, and then run steps 2 and 3 (we call these latter two stages "LaMDA Research"). LaMDA was trained as a dialog system, and always expects a dialog context where the user speaks first. So, for non-dialog tasks, we insert an artificial user utterance as dialog history: "Tell me something interesting." For the attribution report, we take all evidence documents retrieved by LaMDA during its research process.</p>
<p>RARR Our model uses few-shot prompting on PaLM 540B for query generation, the agreement model, and the edit model. We use the same prompts for all tasks except when the context comes from a dialog, where we slightly modify the prompts to use the dialog context (e.g., CQGen now maps dialog context $+x$ to queries). The queryevidence relevance model $S_{\text {relevance }}$ is a pretrained T5-large model (Raffel et al., 2020) fine-tuned following Ni et al. (2021) on MS MARCO (Nguyen et al., 2016). See Appendix D for the few-shot prompting strategies and more modeling details.</p>
<h3>5.3 Results</h3>
<p>For the main experiments, we report results on passages generated by PaLM and LaMDA. Results on GPT-3 passages show similar trends (Appendix A). Table 1 and Figure 5 show attribution and preservation results for each model and dataset. We also report $\mathrm{F} 1_{\mathrm{AP}}$, the harmonic mean of the two metrics, which is shown as level curves in Figure 5.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Attribution and preservation scores. Dashed lines indicate the highest attribution score obtained by any of the models before editing: points above the line have better attribution after revision. The contours are F1AP level curves: points along a contour have equivalent F1AP. Different models make very different trade-offs between attribution and preservation. Only RARR has a robust F1AP across all tasks.</p>
<p>RARR significantly improves attribution while preserving most of the original text. In terms of F1AP, RARR is the only method that performs robustly across all three datasets, and significantly outperforms prior methods on NQ and SQA.</p>
<p>We found that RARR is the only method that preserves the original intent of x over 90% of the time — EFEC and LaMDA only manage to preserve the original intent 6–40% of the time. We also see that editing is crucial to improve attribution: if we only retrieve evidence to support the original response x without editing, attribution ranges from the low 10s to mid 30s. After editing, RARR can increase attribution by up to 13% absolute, while changing only 10–20% of the text.</p>
<p>As noted in Section 2, one can sacrifice preservation for higher attribution. EFEC is able to obtain strong F1AP on QReCC by making larger changes to the text in exchange for a higher attribution score. However, it occupies a very different point from RARR on the attribution-preservation trade-off curve, as visualized in Figure 5.</p>
<h2>6 Analysis</h2>
<h3>6.1 Qualitative analysis</h3>
<p><strong>Human oracle</strong> To understand the remaining headroom in our task, we ask: <em>what is the minimal amount of editing needed to make a text passage fully attributed?</em> The answer would depend on the quality of the LM that generated the text as well as the task difficulty. As an approximation, we manually edited 30 examples in our NQ benchmark until we judged them to be 100% attributable. We achieved a preservation score of 88%, which (when combined with 100% attribution) translates to 93.6 F1AP, indicating a significant headroom.</p>
<table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
<tr>
<td>x: Justice Ashok Kumar Mathur headed the 7th central pay commission in India. It was created in 2014 and submitted its report in 2016.</td>
</tr>
<tr>
<td><strong>Attribution:</strong> 50%</td>
</tr>
<tr>
<td><strong>Preservation:</strong> 100%</td>
</tr>
<tr>
<td>EFEC: The 7th central pay commission in India was created in 2014.</td>
</tr>
<tr>
<td><strong>Attribution:</strong> 100%</td>
</tr>
<tr>
<td><strong>Preservation:</strong> 0%</td>
</tr>
<tr>
<td>LaMDA: I heard the 7th CPC made recommendations for increasing the minimum salary pay from Rs 7066 to 18k per month for new central government employees.</td>
</tr>
<tr>
<td><strong>Attribution:</strong> 0%</td>
</tr>
<tr>
<td><strong>Preservation:</strong> 0%</td>
</tr>
<tr>
<td>RARR: Justice Ashok Kumar Mathur headed the 7th central pay commission in India. It was created in 2014 and submitted its report in 2015.</td>
</tr>
<tr>
<td><strong>Attribution:</strong> 100%</td>
</tr>
<tr>
<td><strong>Preservation:</strong> 100%</td>
</tr>
<tr>
<td>evidence: The 7th Central Pay Commission (Chair: Justice A. K. Mathur) submitted its report on November 19, 2015. The Commission had been appointed in February 2014, to look at remuneration for central government employees. …</td>
</tr>
</tbody>
</table>
<p>Figure 6: Example model outputs and human judgment of their attribution and preservation scores. EFEC reduces the passage x into a single sentence. LaMDA changes the writing style. RARR preserves the structure of the input passage. We show one evidence retrieved by RARR to help explain the example.</p>
<p><strong>Analyzing the baselines</strong> As exemplified in Figure 6, EFEC frequently attempts to summarize the entire passage into one sentence, or drops later sentences. This is likely due to EFEC's training data, which was limited to single sentences. This behavior generally increases the attribution score, because it is usually easier to make one sentence fully attributable than many sentences. However, in datasets where the claim contains multiple sentences (NQ and SQA), such a behavior yields low preservation scores, and also results in outputs that are less informative. We expect that EFEC could perform much better if its training data were augmented to include multiple sentences. LaMDA Research achieves similar attribution scores to RARR. But as mentioned in Section 5.2, the intent and linguistic style of the output tend to deviate from the input, resulting in lower preservation scores (Figure 6). We emphasize that this is not a purely apples-to-apples comparison since LaMDA was not optimized for preservation. Overall, these experiments are mainly meant to illustrate that prior models were simply not designed for the task of <em>Editing for Attribution</em>, rather than to mark RARR as the best method.</p>
<p><strong>Analyzing RARR</strong> For the research stage, the question generation model had comprehensive coverage: a manual inspection of 40 examples shows &gt; 80% with questions that fully cover all aspects of the input text. The retriever was strongest at re-</p>
<p>(a) Correctly revising an entity
y: If She Knew What She Wants was written by Henry Roth.
$e$ : [en.wikipedia.org] "If She Knew What She Wants" is a song written by American singer-songwriter Jules Shear and introduced on ...
$y^{\prime}$ : If She Knew What She Wants was written by Jules Shear.
(b) Correctly revising a number
y: God Save the Queen became the British national anthem in 1745. ...
$e$ : [www.britannica.com] The oldest national anthem is Great Britain's "God Save the Queen," which was described as a national anthem in 1825. ...
$y^{\prime}$ : God Save the Queen became the British national anthem in 1825. ...
(The year 1745 was when the song was first performed.)
(c) Performing a necessary larger revision
y: "It's My Party" is a song written and composed by American singersongwriter and producer Walter Gold. The song was published in 1962 and first recorded by American singer-songwriter, producer and arranger Lesley Gore.
$e$ : [secondhandsongs.com] Song: It's My Party written by John Gluck, Wally Gold, Seymour Gottlieb, Herbert Weiner.
$y^{\prime}$ : "It's My Party" is a song written and composed by John Gluck, Wally Gold, Seymour Gottlieb and Herb Weiner. The song was published in 1963 and first recorded by American singer Lesley Gore.
(d) Confused by misleading evidence
y: In the Heat of the Night is a police procedural crime drama TV series produced by MGM Television. It aired on NBC from September 8, 1988 to May 3, 1992. ...
$e$ : [merv.com] Watch In the Heat of the Night on MeTV.
$y^{\prime}$ : In the Heat of the Night is a police procedural crime drama TV series produced by MGM Television. It aired on MeTV from September 8, 1988 to May 3, 1992. ...
(The show did actually air on NBC in the 1980s, but the model fails to realize that MeTV is a re-run much later in time.)
(e) Improved attribution, but reasoning doesn't adapt to new info
(Q: Does Homer Simpson need two hands worth of fingers to count to 5?)
y: Homer Simpson has 5 fingers and 2 hands. Therefore, he does not need two hands worth of fingers to count to 5 . He only need one hand to count to 5 .
$e$ : [refinery29.com] Other iconic animated characters, such as Homer Simpson and his brood, . . . only have four fingers, too.
$y^{\prime}$ : Homer Simpson has 4 fingers and 2 hands. Therefore, he does not need two hands worth of fingers to count to 5 . He only need one hand to count to 5 . (Having only 4 fingers per hand, he does need two hands to count to 5.)</p>
<p>Figure 7: Example revisions from RARR, both good and bad. $y=$ partially edited passage; $e=$ evidence; $y^{\prime}$ $=$ passage after editing with $e$.
searching content involving distinct entities (e.g., a movie, a major event, or a person). In contrast, we found significant headroom for better attribution of statements involving generic objects and more abstract claims (e.g. "Video games require electricity."- since this is obvious to most humans, retrieved articles from the web tend to address related but different topics). We suspect that a significant amount of attribution headroom on our benchmarks would benefit from a better research stage.</p>
<p>For the revision stage, RARR was able to revise many unattributed claims, especially those involving entities and numbers (Figures 7a and 7b). It can also perform larger revisions when necessary (Figure 7c). Moreover, RARR abstains from editing when the claim is already well-attributed: on NQ, among the inputs with near-perfect attribution (preedit $\mathrm{Attr}_{\mathrm{AIS}}&gt;0.9$ ), RARR does not make an edit in $90 \%$ of the cases. However, the system also has several shortcomings. Some erroneous edits arise from misleading irrelevant evidence (Figure 7d). We also observed an interesting challenge when revising reasoning chains, where the model suc-
cessfully revised an incorrect claim, but did not revise subsequent reasoning steps that depend on the earlier claim (Figure 7e). In this case, further editing to improve logical coherence could help.</p>
<h3>6.2 Ablations</h3>
<p>Ablating query generation RARR uses generated questions as search queries for evidence retrieval. We consider two natural alternatives: using the entire input passage as a single search query, or using each sentence as a search query. For the former, we retrieve $J=3$ evidence snippets to make the amount a closer match to other methods.</p>
<p>The results are in Table 2. Using the entire input passage as the query gives poor results, as the retrieved evidence tends to not focus on potentially unattributed parts in the passage. Using sentences as queries gives results closer to the full CQGen, but a closer analysis reveals two caveats.</p>
<p>First, sentences-as-queries are more effective when such sentences "mimic" content on the Web, and are less effective otherwise. In Table 3, we test this by excluding all of Wikipedia from web search results (since many PaLM outputs for NQ have a Wikipedia style). The attribution performance of sentences-as-queries drops significantly, while CQGen is more robust.</p>
<p>Second, sentence-as-queries tends to retrieve passages that may encourage confirmation bias. Consider the example "Georgia is called the Peach State, but California actually produces the most peaches." Retrieval using sentences-as-queries found an article echoing that California produces the most peaches, while CQGen generated the more impartial query "Which state produces the most peaches?" and found a newer article saying that South Carolina replaced California as the top peach producer. In this case, RARR using CQGen needs to sacrifice more preservation score to edit the text, leading to a lower $\mathrm{F} 1_{\mathrm{AP}}$ score. This underscores that attribution alone cannot measure "correctness" since not all evidence is up-to-date or reliable.</p>
<p>Ablating agreement model We try removing the agreement model, which effectively forces the model to revise the passage based on every retrieved evidence. The results are shown in Table 2. As expected, more revision leads to less preservation score and spurious changes to the text passage, as demonstrated in Figure 8.</p>
<p>Impact on downstream task performance We have measured preservation using the metric de-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PaLM outputs on NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaLM outputs on SQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LaMDA outputs on QReCC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
</tr>
<tr>
<td style="text-align: left;">Full RARR</td>
<td style="text-align: center;">$45.6 \rightarrow 54.9$</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">$\mathbf{6 8 . 1}$</td>
<td style="text-align: center;">$37.6 \rightarrow 45.1$</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$18.8 \rightarrow 29.4$</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">$\mathbf{4 3 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">no agreement model</td>
<td style="text-align: center;">$45.6 \rightarrow 50.6$</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">$37.8 \rightarrow 46.9$</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$18.8 \rightarrow 28.8$</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">query $=$ input</td>
<td style="text-align: center;">$45.4 \rightarrow 47.2$</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">$39.4 \rightarrow 30.3$</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">$19.7 \rightarrow 20.6$</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">34.0</td>
</tr>
<tr>
<td style="text-align: left;">query $=$ sentence</td>
<td style="text-align: center;">$49.1 \rightarrow 52.1$</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">$43.7 \rightarrow 44.3$</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">$\mathbf{6 1 . 2}$</td>
<td style="text-align: center;">$19.0 \rightarrow 19.6$</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">32.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Ablation results. We report the automatic metrics: Attr $<em _Lev="{Lev" _text="\text">{\text {auto }}$, Pres $</em>$ as RARR, it is less robust to corpus shifts and tends to retrieve passages that may encourage confirmation bias.}}$, and harmonic mean between the two ( $\mathrm{F} 1_{\mathrm{AP}}$ ). We show auto-AIS scores both before and after editing (before $\rightarrow$ edit), with respect to the attribution report $A$ produced by the model. Even though sentence-as-queries may achieve similar $\mathrm{F} 1_{\mathrm{AP}</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NQ F1AP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SQA F1AP</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">orig</td>
<td style="text-align: center;">no wiki</td>
<td style="text-align: center;">orig</td>
<td style="text-align: center;">no wiki</td>
</tr>
<tr>
<td style="text-align: left;">Full RARR</td>
<td style="text-align: center;">$\mathbf{6 8 . 1}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 3}$</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$\mathbf{5 7 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">query $=$ sentence</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">$\mathbf{6 1 . 2}$</td>
<td style="text-align: center;">56.7</td>
</tr>
</tbody>
</table>
<p>Table 3: The impact of excluding Wikipedia from the retrieval corpus. CQGen (full RARR) is more robust to Wikipedia's absence, while using sentences-as-queries suffers a bigger drop in performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$x$ : The Crown-of-thorns starfish is native to the Great Barrier Reef. . . The <br> starfish was introduced to the Great-Barrier-Reef by ocean currents.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$c$ : [invasivespeciesinfo.gov] Ballast water is one of the major pathways for <br> the introduction of nonindigenous marine species. .</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$y$ : The Crown-of-thorns starfish is native to the Great Barrier Reef. . . The <br> starfish was introduced to the Great-Barrier-Reef by ballast water.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 8: Disabling the agreement model leads to over-edits. Here, the evidence $c$ does not explicitly disagree with $x$, but without an agreement model to detect this, the edit model makes an unsupported change.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 9: Downstream task performance on NQ and SQA. RARR's revisions lead to better answer accuracy on NQ. No models improved answer accuracy on SQA.
fined in Section 2.2. However, another measure of preservation is whether the revised text can still be used to perform the task that it was originally generated for. Following EFEC, we quantitatively evaluate this on short answer tasks NQ and SQA, and we summarize the result in Figure 9.</p>
<p>For NQ, each original text $x$ is a long-form response to a factoid question. To determine whether the revised text $y$ still serves this purpose, we feed the factoid question and $y$ back into PaLM and prompt it to extract a short answer from $y$. We find
that RARR not only preserves the short answer accuracy but actually improves it by roughly $5 \%$.</p>
<p>For SQA, each original text is a reasoning chain that helps to answer a yes/no question. We feed the SQA question and $y$ back into PaLM and prompt it to output a yes/no answer, and evaluate answer accuracy. Here, we find that increasing attribution comes at a slight cost in downstream task performance: answer accuracy drops modestly for all revision models (up to $2.6 \%$ ). We suspect that this may be due to noisy retrievals, which sometimes provide misleading evidence (exemplified in Figure 7d). Furthermore, even though revisions can address factoid errors in the passage (e.g., "Homer Simpson has 5 fingers" from Figure 7e), RARR currently does not try to modify subsequent reasoning steps which may no longer be logically entailed (e.g., "He only needs one hand to count to 5").</p>
<h2>7 Conclusion</h2>
<p>Language models have developed increasingly good "procedural" knowledge of what should be discussed and how it should be presented, but often struggle to memorize "factoid" knowledge and produce unsubstantiated claims. We proposed RARR, a framework for revising such claims to make them attributable to the researched evidence. From experiments on text passages generated by different models on various domains, we showed that RARR can revise the passages to improve attribution while preserving other desirable properties such as writing style or structure. Furthermore, RARR sits on top of existing generation models without needing to re-design or re-train LMs.</p>
<p>Major headroom still remains, as discussed in Section 6 and the Limitations section. We hope our analysis of RARR would help with developing new approaches for integrating attribution to LMs.</p>
<h2>8 Limitations</h2>
<p>Limitations of our task definition Depending on the application, attribution and preservation may not deserve equal weight. For instance, if there are multiple acceptable options for the output, such as in a dialog system, we might trade-off preservation for attribution, similar to how LaMDA behaves in our experiments.</p>
<p>Our evaluation metrics also do not measure all aspects of attribution. For instance, some sentences are self-evident and do not require attribution (e.g., "I agree.") but would be penalized in our evaluation. It is also necessary to note that linguistic assertions have varying scope: for example, there is a difference between "Frozen is a scary movie" and "I got scared watching Frozen" - while expressing a similar sentiment, the former makes a more general statement that many would disagree with, while the latter is scoped to the speaker's own experience. In some applications, one could even argue that the latter case does not require attribution, since the speaker is their own source-of-truth. In addition to varying scope, utterances can also make assertions with varying levels of directness. For example, according to standard linguistics, "John ate some of the cookies" yields the implicature that John did not eat all of the cookies, even though it is not logically entailed. This raises the question of which implicatures or implied assertions should be detected and attributed, which should be explored in future work. For more nuances, we refer to Rashkin et al. (2021).</p>
<p>For preservation, we wish to explore other properties that should be preserved, such as discourse or logical coherence. Additionally, if the input text passage is completely misguided or flawed, it can be difficult to revise the text without significant changes, which would be heavily penalized by the current metrics.</p>
<p>Limitations of our model While we aspire to improve attribution for arbitrary text, it is clear that RARR is not yet fully general. For example, the current implementation of RARR would not be well-prepared to edit poetry (where preserving rhyme matters) or long documents, primarily because we do not provide examples of such inputs in our few-shot LLM prompts. However, we do believe that future developers may be able to quickly adapt RARR to such tasks by simply changing the prompts. Second, RARR tends to preserve rather
than delete claims that it cannot attribute. Some of these claims genuinely do not require attribution, but others are hallucination and should be removed. Judging whether a claim requires attribution can be subjective and challenging. Finally, our model is computationally costly, since it is based on prompting a large language model. One potential solution is to leverage recent synthetic data generation recipes to train a smaller model (Lee et al., 2021; Schick et al., 2022).</p>
<h2>9 Ethical considerations</h2>
<p>Partial attribution When RARR is not 100\% successful in making text consistent with retrieved evidence, the revised text will be partially attributed. One could identify unattributed parts using either the automated attribution score (Attr $_{\text {AIS }}$ ) or the relevance scores used to generate the attribution report (Section 3.3). Such information should be presented to avoid misleading readers into thinking that the entire revision is attributed.</p>
<p>Evidence trustworthiness RARR seeks to improve attribution for the output of any generative model. However, even if RARR can attribute content to a particular source, the user must still consider whether the source itself is trustworthy. Even for sources that are traditionally considered "authoritative" (such as an encyclopedia), there may still be factual inaccuracies or biases. This work does not address the question of whether a source is trustworthy, or the related topic of misinformation. While we do not provide a means for judging trustworthiness, the design of RARR does allow for the research stage to restrict its search over a userspecified corpus, based on what the user deems trustworthy.</p>
<p>Conflicting evidence There is also the possibility that some content may be simultaneously supported by certain sources, while contradicted by others. This can easily occur for content involving subjective or imprecise claims. The current implementation and evaluation for RARR does not explicitly address this issue - we adopted a "permissive" definition of attribution, where we consider content to be attributed if there exists any source that supports it. For some applications, a more restrictive definition that requires both existence of supporting sources and absence of contradicting sources would be needed.</p>
<h2>Acknowledgments</h2>
<p>We wish to thank Raphael Hoffmann, Slav Petrov, Dipanjan Das, Michael Collins, Iftekhar Naim, Kristina Toutanova, William Cohen, Sundeep Tirumalareddy, Samer Hassan, Quoc Le and Heng-Tze Cheng for their research mentorship, feedback and support. We are grateful to Hao Zhou and Petr Pilar for helping us experiment with LaMDA and motivating our dialog experiments. We also wish to thank Tal Schuster for pointing us to relevant work in the fact checking literature, and helping us reproduce it. We thank Vitaly Nikolaev, David Reitter and Roee Aharoni for helping us use AIS and auto-AIS. We also wish to thank Jianmo Ni and Honglei Zhuang for developing the query-evidence relevance model we use, Daniel Andor for developing the sentence decontextualization model we use, and Ran Tian for the initial prototype of CQGen. Finally, we thank Kathy Meier-Hellstern, Philip Parham and Diane Korngiebel for their thoughtful feedback on ethical considerations.</p>
<h2>Contributions</h2>
<p>Luyu Gao: Designed RARR's few-shot prompting strategies and implemented the first PaLM-based prototype. Analyzed results, and advised on the design of human and automatic evaluation.</p>
<p>Zhuyun Dai: Proposed the evaluation setup of editing long-form generations from PaLM/LaMDA on various QA datasets. Hosted and mentored Luyu Gao (student researcher) in prototyping RARR. Implemented the final models, designed overall experiments, and obtained main results and ablations (together with Ice Pasupat). Contributed many parts of the writing.</p>
<p>Ice Pasupat: Implemented the final models, designed overall experiments, and obtained main results and ablations (together with Zhuyun Dai). Automated experimental infrastructure, conducted error analyses, and oversaw many parts of the paper writing.</p>
<p>Anthony Chen: Developed the automatic evaluation for attribution and preservation and worked with Arun Chaganty to design human evaluation. Developed the open-source implementation (GPT3 RARR), made improvements to prompts, and helped with writing.</p>
<p>Arun Chaganty: Led and implemented all human evaluation. Proposed the two-dimensional attribution + preservation metric (together with</p>
<p>Kelvin Guu). Advised on model design and contributed many parts of the writing.</p>
<p>Yicheng Fan: Worked with Kelvin Guu to develop the first prototype of RARR. Proposed multiple retrieval strategies and implemented the EFEC baseline.</p>
<p>Vincent Zhao: Co-hosted and mentored Luyu Gao (student researcher) in prototyping RARR. Enabled bulk inference for PaLM. Proposed the downstream task evaluation.</p>
<p>Ni Lao: Research mentorship, advising and contributed many parts of the writing.</p>
<p>Hongrae Lee: Research mentorship and advising. Helped integrate RARR with Google Search and evaluate LaMDA.</p>
<p>Da-Cheng Juan: Research mentorship and early design discussions.</p>
<p>Kelvin Guu: Proposed the original research-andrevise concept, implemented the first prototype, initiated the project and involved all collaborators. Implemented baselines (together with Yicheng Fan). Research mentorship, oversaw project coordination and paper writing.</p>
<h2>References</h2>
<p>Michael Ahn et al. 2022. Do as I can, not as I say: Grounding language in robotic affordances. ArXiv, abs/2204.01691.</p>
<p>Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen G. Pulman, and Srinivas Chappidi. 2021. Open-domain question answering goes conversational via question rewriting. In NAACL.</p>
<p>Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. 2019. MultiFC: A real-world multi-domain dataset for evidencebased fact checking of claims. In EMNLP.</p>
<p>Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, and Yulia Tsvetkov. 2022. Correcting diverse factual errors in abstractive summarization via postediting and language model infilling. In EMNLP.</p>
<p>Bernd Bohnet, Vinh Quang Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. $A r X i v$.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS.</p>
<p>Mengyao Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correction for abstractive summarization models. In EMNLP.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In $A C L$.</p>
<p>Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sentences stand-alone. TACL, 9:447-461.</p>
<p>Aakanksha Chowdhery et al. 2022. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311.</p>
<p>Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. 2022. On the origin of hallucinations in conversational models: Is it the datasets or the models? In NAACL.</p>
<p>Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. TACL, 9:391-409.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In $A C L$.</p>
<p>Angela Fan, Aleksandra Piktus, Fabio Petroni, Guillaume Wenzek, Marzieh Saeidi, Andreas Vlachos, Antoine Bordes, and Sebastian Riedel. 2020. Generating fact checking briefs. In EMNLP.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. TACL, 9:346-361.</p>
<p>Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In NAACL.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrievalaugmented language model pre-training. In ICML.</p>
<p>Shirley Anugrah Hayati, Raphaël Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, and Graham Neubig. 2018. Retrieval-based neural code generation. In EMNLP.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In $I C L R$.</p>
<p>Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Y. Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. In Workshop on Documentgrounded Dialogue and Conversational Question Answering.</p>
<p>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. $\mathrm{Q}^{2}$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In EMNLP.</p>
<p>Hayate Iso, Chao Qiao, and Hang Li. 2020. Fact-based text editing. In $A C L$.</p>
<p>Georgi Karadzhov, Preslav Nakov, Lluís Màrquez, Alberto Barrón-Cedeño, and Ivan Koychev. 2017. Fully automated fact checking using external sources. In RANLP.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. In EMNLP.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In $I C L R$.</p>
<p>Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTaiL: A textual entailment dataset from science question answering. In AAAI.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In NAACL.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In EMNLP.</p>
<p>Tom Kwiatkowski et al. 2019. Natural Questions: A benchmark for question answering research. TACL, 7:453-466.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In $A C L$.</p>
<p>Kenton Lee, Kelvin Guu, Luheng He, Timothy Dozat, and Hyung Won Chung. 2021. Neural data augmentation via example extrapolation. ArXiv, abs/2102.01335.</p>
<p>Vladimir I. Levenshtein. 1965. Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics. Doklady, 10:707-710.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive NLP tasks. In NeurIPS.</p>
<p>Robert L Logan IV, Alexandre Passos, Sameer Singh, and Ming-Wei Chang. 2022. FRUIT: Faithfully reflecting updated information in text. In NAACL.</p>
<p>Shayne Longpre, Kartik Kumar Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In EMNLP.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020. On faithfulness and factuality in abstractive summarization. In $A C L$.</p>
<p>Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, and Nathan McAleese. 2022. Teaching language models to support answers with verified quotes. ArXiv, abs/2203.11147.</p>
<p>Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332.</p>
<p>Shashi Narayan, Joshua Maynez, Reinald Kim Amplayo, Kuzman Ganchev, Annie Louis, Fantine Huot, Dipanjan Das, and Mirella Lapata. 2022. Conditional generation with a question-answering blueprint. ArXiv, abs/2207.00397.</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In CoCo@NIPS.</p>
<p>Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2021. Large dual encoders are generalizable retrievers. ArXiv, abs/2112.07899.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? In EMNLP.</p>
<p>Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Ouguz, Edouard Grave, Wen tau Yih, and Sebastian Riedel. 2021. The web is your oyster - knowledge-intensive nlp against a very large web corpus. ArXiv, abs/2112.09924.</p>
<p>Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 21.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. ArXiv, abs/2112.12870.</p>
<p>Adam Roberts, Colin Raffel, and Noam M. Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In EMNLP.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. PEER: A collaborative language model. ArXiv, abs/2208.11663.</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In NAACL.</p>
<p>Darsh J. Shah, Tal Schuster, and Regina Barzilay. 2020. Automatic fact-guided sentence modification. In AAAI.</p>
<p>Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. In EMNLP.</p>
<p>Romal Thoppilan et al. 2022. LaMDA: Language models for dialog applications. ArXiv, abs/2201.08239.</p>
<p>James Thorne and Andreas Vlachos. 2021. Evidencebased factual error correction. In $A C L$.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and verification. In NAACL.</p>
<p>David Wadden, Kyle Lo, Lucy Lu Wang, Shanchuan Lin, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In EMNLP.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In $A C L$.</p>
<p>William Yang Wang. 2017. "Liar, liar pants on fire": A new benchmark dataset for fake news detection. In ACL.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.</p>
<p>Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura. 2018. Guiding neural machine translation with retrieved translation pieces. In NAACL.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In NAACL.</p>
<h2>A Additional experiments and analysis</h2>
<p>Model variance The main experiments in Section 5 are based on a single run. We ran automated evaluation on 3 random runs of RARR, using PaLM outputs on NQ as input passages. The standard deviations of $\mathrm{Attr}<em _mathrm_Lev="\mathrm{Lev">{\text {auto }}, \operatorname{Pres}</em>$ are 1.2, 0.5, and 1.0 respectively.}}$, and $\mathrm{F1}_{\mathrm{AP}</p>
<p>Impact of the retriever choice We tried using Microsoft Bing in place of Google Search, with near identical results ( $&lt;1 \%$ difference).</p>
<p>Impact of model scale Many components in RARR work by few-shot prompting PaLM, a large 540B parameter LM. To assess the benefit of LM scaling, we replaced PaLM 540B with a smaller 62B parameter PaLM. As shown in Table 4, we found that 540B outperforms 62B by a large margin, suggesting that RARR could potentially further improve with even more scaling. We also experimented with keeping the editor stage at 540B while shrinking the query generation stage to 64B - this yielded a relatively small performance drop, suggesting that model scaling is more important for the editor.</p>
<p>Impact of model type Few-shot prompting has proven to be effective for many recent large language models. We try replacing the query generation model, agreement model, and edit model with GPT-3 text-davinci-003. The few-shot prompts were slightly tuned to fit the GPT-3 model. Table 4 shows the results, which are slightly better than RARR implemented with PaLM 540B on all three
datasets. We will release this open-source version of RARR that uses GPT-3 as the backbone.</p>
<p>Results on GPT-3 passages Table 5 shows automated evaluation results on passages generated by GPT-3. The results follow the same trend as the results on PaLM and LaMDA passages.</p>
<p>Challenging domains We report results on tasks where attribution was particularly hard, and significant future work is needed.</p>
<p>We considered news article summaries produced by summarization models from SummEval (Fabbri et al., 2021) (e.g., "John Doe was left homeless when the storms hit Staten Island, New York ..."). Results are shown in Table 6. First, we note that the before-edit auto-AIS scores for all models are low. These news article summaries are often about less widely known people and events, which is challenging for retrievers, leading to low attribution. For example, our query generator may ask "where does John Doe live" but get results for a different John Doe. EFEC and LaMDA also face this issue, but instead trade preservation for attribution and rewrite the text to a different topic. This result suggests that using web search with standard question generation methods may fail to capture important context from the input, and is not sufficient for the attribution task.</p>
<p>We also considered long-form explanations generated by PaLM for the ELI5 dataset (Fan et al., 2019) (Table 6). ELI5 was collected from online forums, so many answers tend to have subjective opinions instead of specific entities and facts (e.g., "How do our brains interpret scary music? To me, scary music often sounds a little bit like a person ... "), and are thus difficult to attribute. Sometimes the whole output is based on a false premise and needs to be completely rewritten, in which case RARR cannot satisfactorily edit due to our revision threshold (Section 3.2).</p>
<p>Finally, we considered technical explanations to questions from the MMLU dataset (Hendrycks et al., 2021) which covers diverse subjects from social science, humanities, STEM, and others. ${ }^{4}$ An example input looks like "Every time you remove an edge from a complete graph, you divide it into two connected components. So, a complete graph with 13 vertices must have 12 connected components." Results are shown in Table 7. RARR im-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PaLM outputs on NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaLM outputs on SQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LaMDA outputs on QReCC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
</tr>
<tr>
<td style="text-align: left;">Full RARR</td>
<td style="text-align: center;">$45.6 \rightarrow 54.9$</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">$37.6 \rightarrow 45.1$</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$18.8 \rightarrow 29.4$</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: left;">qgen 62B, editor 540B</td>
<td style="text-align: center;">$45.9 \rightarrow 54.6$</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">$37.0 \rightarrow 40.5$</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">$15.8 \rightarrow 28.4$</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">41.4</td>
</tr>
<tr>
<td style="text-align: left;">qgen 62B, editor 62B</td>
<td style="text-align: center;">$45.9 \rightarrow 49.9$</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">$37.0 \rightarrow 38.3$</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">$15.8 \rightarrow 21.9$</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">$44.3 \rightarrow 55.0$</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">$\mathbf{6 8 . 5}$</td>
<td style="text-align: center;">$38.6 \rightarrow 46.6$</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">$\mathbf{6 1 . 2}$</td>
<td style="text-align: center;">$18.3 \rightarrow 28.6$</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">$\mathbf{4 3 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Additional ablation results. We report the automatic metrics: Attr $<em _Lev="{Lev" _text="\text">{\text {auto }}$, Pres $</em>$ ). We show auto-AIS scores both before and after editing (before $\rightarrow$ edit), with respect to the attribution report $A$ produced by the model.}}$, and harmonic mean between the two ( $\mathrm{F} 1_{\mathrm{AP}</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT-3 outputs on NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-3 outputs on SQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-3 outputs on QReCC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
</tr>
<tr>
<td style="text-align: left;">EFEC</td>
<td style="text-align: center;">$48.3 \rightarrow 66.8$</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">$32.6 \rightarrow 50.6$</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">$26.4 \rightarrow 53.1$</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: left;">LaMDA</td>
<td style="text-align: center;">$36.2 \rightarrow 61.1$</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">$22.3 \rightarrow 27.3$</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">$19.0 \rightarrow 33.9$</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: left;">PaLM RARR</td>
<td style="text-align: center;">$48.3 \rightarrow 57.2$</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">$32.6 \rightarrow 36.3$</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">$26.4 \rightarrow 31.1$</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">$\mathbf{4 5 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 RARR</td>
<td style="text-align: center;">$48.0 \rightarrow 59.3$</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">$\mathbf{7 2 . 0}$</td>
<td style="text-align: center;">$34.7 \rightarrow 37.0$</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">$\mathbf{5 2 . 8}$</td>
<td style="text-align: center;">$23.2 \rightarrow 25.3$</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">39.5</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on passages from GPT-3. We report the automatic metrics: Attr $<em _Lev="{Lev" _text="\text">{\text {auto }}$, Pres $</em>$ ). We show auto-AIS scores both before and after editing (before $\rightarrow$ edit), with respect to the attribution report $A$ produced by the model. The results show similar trends as the results on passages from PaLM and LaMDA in Table 1.}}$, and harmonic mean between the two ( $\mathrm{F} 1_{\mathrm{AP}</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Attr $_{\text {auto }}$</th>
<th style="text-align: center;">Pres $_{\text {Lev }}$</th>
<th style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SummEval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">EFEC</td>
<td style="text-align: center;">$17.9 \rightarrow 34.6$</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">LaMDA</td>
<td style="text-align: center;">$10.3 \rightarrow 28.8$</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: left;">RARR</td>
<td style="text-align: center;">$18.3 \rightarrow 16.9$</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">28.6</td>
</tr>
<tr>
<td style="text-align: left;">ELI5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">EFEC</td>
<td style="text-align: center;">$18.2 \rightarrow 41.2$</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">LaMDA</td>
<td style="text-align: center;">$19.9 \rightarrow 40.1$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: left;">RARR</td>
<td style="text-align: center;">$18.5 \rightarrow 18.9$</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">31.7</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on ELI5 and SummEval.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">RARR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MMLU Category</td>
<td style="text-align: center;">Attr $_{\text {auto }}$</td>
<td style="text-align: center;">Pres $_{\text {Lev }}$</td>
<td style="text-align: center;">$\mathrm{F} 1_{\mathrm{AP}}$</td>
</tr>
<tr>
<td style="text-align: left;">Humanities</td>
<td style="text-align: center;">$26.6 \rightarrow 29.6$</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: left;">Social Sciences</td>
<td style="text-align: center;">$35.5 \rightarrow 40.7$</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">56.5</td>
</tr>
<tr>
<td style="text-align: left;">STEM</td>
<td style="text-align: center;">$37.8 \rightarrow 41.5$</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">57.4</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: center;">$36.9 \rightarrow 41.7$</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">57.6</td>
</tr>
</tbody>
</table>
<p>Table 7: RARR results on MMLU.
proves attribution of the explanations on all four categories of MMLU, although the increases are relatively small. We also found that RARR's performance is low on examples with mathematical reasoning, as these are beyond the capability of the edit model with our current prompt.</p>
<h2>B Details on automated evaluation</h2>
<p>Sentence splitting When computing the attribution score, we use spaCy en_core_web_sm v3.0.0a1 to segment the text passage into sentences.
(More recent models gave similar results.) While each sentence may contain multiple claims that could be attributed independently, there is currently no linguistic consensus on what constitutes a claim. Instead of depending on a particular definition of claims, we use sentences as claims for simplicity and reproducibility. The same segmentation is also used for human evaluation.</p>
<p>Decontextualization We decontextualize each sentence in the text passage before computing the attribution score. We use the model from Choi et al. (2021), which is a T5 model fine-tuned to map the input "[HEAD] [SEP] context and passage [start] sentence [end]" to the output "[OPCODE] decontextualized sentence", where the OPCODE can be "done" (success), "un" (unnecessary), or "imp" (impossible). We feed the passage's context (questions for NQ and SQA; dialog context for QRECC) along with the passage itself to the input. We use beam search with beam size 8 and discard any result whose number of tokens differ by more than 4 .</p>
<p>NLI model We obtained a newer version of the end-to-end NLI model from the authors of Honovich et al. (2022), which was trained on MNLI, SNLI, FEVER, PAWS, SciTail and VitaminC (Williams et al., 2018; Bowman et al., 2015; Thorne et al., 2018; Zhang et al., 2019; Khot et al., 2018; Schuster et al., 2021). The model is a T5</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 10: Violin plot illustrating the strong correlation between human AIS and auto-AIS labels on our NQ benchmark. Pearson correlation is $0.74(\mathrm{~N}=450) . y$ axis is auto-AIS score, the two violins correspond to a human label of 0 or 1 .
model fine-tuned to map the input "premise: evidence hypothesis: claim sentence" to either " 1 " (entailed) or " 0 " (not entailed). As suggested by the authors, we use the probability of producing " 1 " as the entailment score.</p>
<p>Comparing human and automated evaluation We conducted correlation studies between human and automatic metrics and found strong Pearson correlation (attribution $=0.74$; preservation $=0.62$ ). We visualize the correlation between human and automated attribution scores on NQ and SQA in Figure 10. We found that the AIS scores from human correlate well with auto-AIS scores, with some bias for non-attributed sentences to be judged as attributed by auto-AIS.</p>
<h2>C Details on human evaluation</h2>
<p>To end-goal of RARR is to improve the attribution of generation models through post-editing while preserving the original intent. Attribution and preservation are both subjective properties that may change with even small edits. In the main paper, we present two automatic metrics to conveniently gauge these properties, but rely on a human evaluation as the gold standard. In this section, we describe how we conducted the human evaluation and what instructions and examples annotators were provided.</p>
<p>Rater recruitment and training We engaged with a vendor supplier of full-time crowd workers to recruit human annotators for our task. Annotators were asked to review the instructions below and were provided direct feedback on their
responses during the pilot annotation runs. We had 3 annotators rate each example in the pilot phase to measure inter-annotator agreement, and had a single rater annotate each example afterwards.</p>
<h2>C. 1 Instructions: Overview</h2>
<p>In this task you will evaluate the quality of text generated by a system (the "passage") based on how well it represents information from multiple pieces of "evidence".</p>
<p>We will be using two categories to evaluate the quality of the passage: Attribution and Intent Similarity. You will evaluate these categories in succession. In some tasks, you will only evaluate Attribution. The task interface will guide you through the flow; you can also see the overall task flow in the diagram below.</p>
<p>Note: The passage may appear very fluent and well-formed, but still contain slight inaccuracies that are not easy to discern at first glance. Pay close attention to the text. Read it carefully as you would when proofreading.</p>
<h2>C. 2 Instructions: Attribution</h2>
<p>In this step, you will evaluate how much of the passage is attributable to one or more pieces of evidence (Figure 11).</p>
<p>In the interface, the passage of text and the context in which it was generated is shown on the left, and each piece of evidence is shown on the right. You will use all three (context, passage, evidence) to answer the following question for each sentence in the passage: Is all of the information provided by this sentence fully supported by at least one piece of evidence?</p>
<p>Determining the information provided by the sentence. Three points are key when determining information provided by the sentence:</p>
<ol>
<li>The context and the other sentences of the passage are often critical in understanding the information provided by the sentence.</li>
<li>The context should only be used to understand the information provided by the sentence.</li>
<li>The evidence should be completely ignored for this step.</li>
</ol>
<p>Consider the following example:
Context: who plays doug williams in days of our lives</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 11: Screenshot of interface to annotate attribution at the sentence level. annotators were asked to mark sentences as being fully attributable or not fully attributable by clicking each sentence, and rating each piece of evidence as being useful or not in helping determine attribution of the passage. Annotators were also presented with the context of the generation.</p>
<h1>Attribution Evaluation Instructions</h1>
<p>CONTEXT
who pays medical bills in great britain where does the money come from to pay these bills</p>
<p>PASSAGE A
Britain's National Health Service (NHS) is paid for through general taxation and national insurance. In 2017/18, the NHS budget was $£ 176.5$ bn.</p>
<p>Given the same context above, how similar is the intent conveyed by Passage A and Passage B? Please ignore any differences in details.</p>
<p>Not at all similar Passage A is about a significantly different topic than Passage B.</p>
<p>Somewhat similar Passage A is about the same topic as Passage B, but differs substantially level of detail or style of presentation. They may differ in factual details.</p>
<p>Very similar Passage A is about the same topic as Passage B, with a similar level of detail and style of presentation. They may differ in factual details.</p>
<p>Previous
Next
Submit Comment</p>
<p>Figure 12: Screenshot of the preservation interface. Annotators are asked to read compare two passages and rate how similar the intent conveyed by the two passages is.</p>
<p>Passage: In the American daytime drama Days of Our Lives, Doug Williams and Julie Williams are portrayed by Bill Hayes and Susan Seaforth Hayes.</p>
<p>In the above example, the meaning of the passage is clear even without seeing the query. But consider another example:</p>
<p>Context: who plays doug williams in days of our lives
Passage: he is played by Bill Hayes
Passage (interpreted): Doug Williams is played by Bill Hayes in days of our lives</p>
<p>In this case the pronoun "he" depends on the context, but it is clear that the intended meaning of the passage can be reasonably interpreted as "Doug Williams is played by Bill Hayes in days of our lives". This interpretation is the "information provided by the passage".</p>
<p>Pronouns such as he/she/it/they etc. are one case where context is needed to figure out the intended meaning of the system response. Here's another example (given with paraphrases of the information highlighted below):</p>
<p>Context: when is the last time the us lost basketball at the olympics
Passage: The last time they lost was in 2004, when Argentina defeated the US $\overline{89}-\overline{79}$. Most recently, they won gold in 2016.</p>
<p>Passage (interpreted): The last time the United States lost basketball at the Olympics was in 2004.</p>
<p>The context should only be used to determine the information provided by the passage; at times, the passage may be about a slightly different topic than the context, for example:</p>
<p>Context: the south west wind blows across nigeria between
Passage: The Harmattan is a dry and dusty northeasterly trade wind that blows across West Africa from December to March. It is very dusty because it blows across the Sahara.</p>
<p>Here, the passage talks about a northeasterly wind, while the context asks about a south-west wind, but the passage can be fully understood.</p>
<p>In general, use your best judgment to determine the information provided by the passage. If the passage is hard to understand and you are unsure what the intended meaning of the passage is, mark the sentences as not attributed and enter a comment with an explanation. As one example, take the following:</p>
<p>Context: how many NBA championships did Michael Jordan win?
Passage: it is the best team in the NBA
Determining if the information accurately represents the evidence. Two points are key when determining whether the information accurately represents the evidence: When interpreting a piece of evidence, use only the title and text of that specific evidence. Completely ignore the context, passage and all other evidence. Check all the information in a sentence. If only some information is supported by the evidence, mark the sentence as not fully attributable.</p>
<p>Consider the following example:
Context: when did reba mcentire record back to god
Passage: Back to God was released by McEntire in 2017.
Evidence: "Back to God" is a song performed by American singer, Reba McEntire. It was released as the second single from her 2017 album, Sing it Now: Songs of Faith \&amp; Hope, on January 20, 2017.</p>
<p>In the above example, it is reasonable to conclude that the evidence supports all the information in the passage, and we can mark the passage as being fully attributable. But consider another example:</p>
<p>Context: who won the womens 2017 ncaa basketball tournament
Passage: South Carolina Gamecocks won the 2017 NCAA Women's Division I Basketball Tournament.
Evidence: The South Carolina Gamecocks defeated the Mississippi State Bulldogs, 67-55, to claim their first-ever national championship.</p>
<p>In this case, while the evidence also mentions the "South Carolina Gamecocks", it isn't clear that the national championship being mentioned is indeed the 2017 NCAA Women's Division I Basketball</p>
<p>Tournament. The passage should be marked as not attributable.</p>
<p>Finally, when the passage contains multiple sentences, evaluate whether each sentence can be fully attributed to one or more pieces of evidence-it is possible for one sentence to be attributed while another is not. For example:</p>
<p>Context: who won the womens 2017 ncaa basketball tournament
Passage: South Carolina Gamecocks won the 2017 NCAA Women's Division I Basketball Tournament. The final score is 67-55, The championship game was held in Dallas, Texas.
Evidence 1: The South Carolina Gamecocks defeated the Mississippi State Bulldogs, 67-55, to claim their first-ever national championship.
Evidence 2: The 2017 NCAA Women's Division I Basketball Tournament was played from Friday, March 17 to Sunday, April 2, 2017, with the Final Four played at the American Airlines Center in Dallas, Texas on March 31 and April 2 .</p>
<p>The first two sentences cannot be attributed to either evidence for the same reason as the previous example, but the last sentence is fully supported by Evidence 2 and should be marked as attributed.</p>
<p>In general, you should use your best judgment in determining whether all of the information provided by the passage is "an accurate representation of information in at least one evidence". See Table 8 for additional examples.</p>
<p>We give the following final notes of guidance:</p>
<ul>
<li>Marking evidence as useful. When reviewing each piece of evidence, mark it as useful if it helps you judge the attributability of any sentence, and mark it not useful if not. In the above example Evidence 1 is not useful because it didn't contain enough context to actually help you assess if the passage was attributable, but Evidence 2 was useful.</li>
<li>Contradicting evidence. Mark a sentence as being attributed if any piece of evidence supports it: if two pieces of evidence contradict each other, but one of them supports the passage, mark the sentence as fully attributable.</li>
<li>More on the concept of "accurate representation". We take as inspiration the journalist's conception of "accurate representation". For example, take this excerpt on Accuracy in the NPR Ethics Handbook: "When quoting or paraphrasing anyone . . . consider whether the source would agree with the interpretation..." In other words, if you had written the source document, consider whether you would view the system response as an accurate representation of information in that source document.</li>
</ul>
<h2>C. 3 Instructions: Intent Similarity</h2>
<p>In this step, you will evaluate how much similar the passage is to another passage (Figure 12).</p>
<p>In the interface, the passage A and passage B are both text generated by a system-given the same context in which it was generated. You will use all three (context, passage A, passage B) to answer the following question: How similar is the intent expressed by Passage A and Passage B? Please ignore any differences in details.</p>
<p>Two points are key when determining whether the two passages convey the same intent:</p>
<ol>
<li>Judge the similarity solely based on the similarity in the type and quantity of information provided by each passage.</li>
<li>Ignore any differences in factual details between the two passages.</li>
</ol>
<p>Consider the following examples:
Context: who pays medical bills in great britain where does the money come from to pay these bills
Passage A: Britain's National Health Service (NHS) is paid for through general taxation and national insurance. In 2017/18, the NHS budget was $£ 176.5 \mathrm{bn}$. Passage B: Britain's National Health Service (NHS) is paid for through general taxation. In 2017/18, the NHS budget was $£ 118$ bn.
Rating: Very similar. Passage A is about the same topic as Passage B, with a similar level of detail and style of presentation. They may differ in factual details.</p>
<p>The above example should be rated "very similar" because both passages include information about (1) how the NHS is paid for, and (2) what its budget</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Context + Passage</th>
<th style="text-align: center;">Evidences</th>
<th style="text-align: center;">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Context: who played morticia in the addams family tv show <br> The Addams Family is an American animated sitcom TV series. It was first aired on NBC on September 24, 1973. Carolyn Jones played the role of Morticia.</td>
<td style="text-align: center;">1/ The Addams Family (1973 TV series): The Addams Family is an American animated sitcom adaptation of the Charles Addams comic. The series was produced in 1973 and was rebroadcast the following season. <br> 2/ The Addams Family (TV Series 1964-1966): When The Addams Family went off the air in 1966, network executives in charge of children's programming for NBC brought them back in 1973 for their own Saturday Morning cartoon show featuring the voices of Carolyn Jones from the original series.</td>
<td style="text-align: center;">While the evidence supports the show being aired in 1973, it doesn't specify the exact date (September 24, 1973). <br> Similarly, while the evidence mentions Carolyn Jones as being a voice actor, it doesn't say she played the role of Mortica.</td>
</tr>
<tr>
<td style="text-align: center;">Context: when will the la sagrada familia be finished <br> The La Sagrada Familia is a large Roman Catholic church in Barcelona. It is designed by Antoni Gaudi. It started construction in 1882, and the construction is still going on. The estimated date to finish is 2026.</td>
<td style="text-align: center;">1/ Sagrada Família - Wikipedia: The Basílica i Temple Expiatori de la Sagrada Família is a church in the Eixample district of Barcelona, Catalonia, Spain, and is currently the largest unfinished Roman Catholic church. <br> 2/ Find Out Sagrada Família's Expected Finish Date: Visiting the breathtaking Sagrada Família today also means witnessing the slow progress towards the completion of the project. Sagrada Família is now expected to be completed in 2026, the centenary of Gaudi's death. It's a reasonable inference that La Sagrada Família is the same as Sagrada Familia, even though the names differ slightly.</td>
<td style="text-align: center;">While Evidence 2 mentions Gaudi, it isn't clear this is a reference to Antoni Gaudi and further doesn't say that he designed the church.</td>
</tr>
</tbody>
</table>
<p>Table 8: Additional examples for annotating attribution.
in 2017/18 was, though they differ in their actual answers to these questions.</p>
<p>Context: who is the owner of reading football club
Passage A: Reading's owner is Yongge Dai. Yongge Dai is also the president of Chinese company Dai Yongge Real Estate. Yongge's son, Dai Xiu Li, is Reading's vice-president.
Passage B: Reading's owner is Dai Yongge. Yongge's brother and sister pair behind the Reading FC takeover-Dai Yongge and Dai Xiu Li-has made their fortune through a massive property empire. Mr Dai, has been the chairman of Renhe Commercial since 1999, which is an organisation owned by his sister behind a vast network of underground shopping centres in China.
Rating: Somewhat similar. Passage A is about the same topic as Passage B, but differs substantially in level of detail or style of presentation. They may differ in factual details.</p>
<p>The above example should be rated "somewhat similar" because both passages are still about the same topic-Reading's owner- but differ substantially in the information they discuss: Passage A includes information about (1a) who Reading's owner is, (2a) which company they are the pres-
ident of and (3a) who their vice-president is. In contrast, while Passage B shares information about (1a), it also includes information about (2b) how the Reading owner made their fortune, (3b) their company position and how long they held it for and (4b) what the company also owns.</p>
<p>Context: what is the numbers of total elected member of indian parliment in present time
Passage A: The total number of elected members of the Lok Sabha is 543.
Passage B: The total number of elected members of the Rajya Sabha is 238.
Rating: Not at all similar. Passage A is about a significantly different topic than Passage B.</p>
<p>Even though the passages look very similar, the above example should be rated "not at all similar" because the two passages are about significantly different topics: "the Lok Sabha" vs "the Rajya Sabha".</p>
<h2>D Details on the model</h2>
<p>Few-shot prompting with LLMs We implement many sub-tasks within RARR using fewshot prompting of LLMs (also known as in-context learning (Brown et al., 2020)) as follows:</p>
<ol>
<li>For each sub-task, we manually author a small number of training examples:</li>
</ol>
<p>(input ${ }<em j="j">{j}$, output $</em>$ ) for $j=1, \ldots, J$, where $J$ ranges between 5 and 10 and where both the input and output are strings.
2. We form the following prompt: input $<em 1="1">{1} \diamond$ output $</em> \oplus$ input $<em 2="2">{2} \diamond$ output $</em> \oplus \ldots \oplus$ input $<em J="J">{J} \diamond$ output $</em> \oplus$ new_input, where $\diamond$ denotes a newline character and $\oplus$ denotes a double newline character.
3. To perform inference on a new input, we condition the LLM on the prompt and sample continuations of the prompt up until the next double newline character.</p>
<p>All of our prompts are included in Figures 13, 14, and 15. The contextual version used for QReCC are in Figures 16, 17, and 18.</p>
<p>Model statistics We implemented most parts of RARR with the PALM model which has 540B parameters. We prompted PALM without any training or finetuning. We used a TPU v2-128 to run inference with PALM.</p>
<p>We manually wrote our prompts by eye-balling quality on a dozen of examples from a separate validation set. We tune our hyperparameters on the validation set as well. We used sampling temperature 0.7 for all generation tasks. For each input text, we sample 3 question generations, and for each question we retrieve 5 results. For agreement gate and editing, we only sample 1 generation. We reject an editing if the edit distance is more than 50 characters or more than half of the original text length.</p>
<h1>E Details on the dataset</h1>
<p>As explained in Section 5.1, we generated 150 development and 150 test passages for each of the 6 combinations of dataset and model: (NQ, PaLM), (SQA, PaLM), (QReCC, LaMDA), (NQ, GPT-3), (SQA, GPT-3), (QReCC, GPT-3). Figures 19, 20, 21 , and 22 are the few-shot prompts used to generate the passages.</p>
<p>Following the corresponding datasets, all generated passages are in English. The authors have manually looked through most of the data and found no personal identifiers.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ MMLU has questions from 57 subjects; we took 10 random question from each topic and generated answer explanations by prompting PALM 540B.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
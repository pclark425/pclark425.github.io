<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1469 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1469</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1469</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-270688227</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2024.emnlp-main.138.pdf" target="_blank">Direct Multi-Turn Preference Optimization for Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ScienceWorld: Is your agent smarter than a 5th grader? <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning text and embodied environments for interactive learning <em>(Rating: 2)</em></li>
                <li>Autoact: Automatic agent learning from scratch via self-planning <em>(Rating: 1)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1469",
    "paper_id": "paper-270688227",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ScienceWorld: Is your agent smarter than a 5th grader?",
            "rating": 2,
            "sanitized_title": "scienceworld_is_your_agent_smarter_than_a_5th_grader"
        },
        {
            "paper_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "Autoact: Automatic agent learning from scratch via self-planning",
            "rating": 1,
            "sanitized_title": "autoact_automatic_agent_learning_from_scratch_via_selfplanning"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        }
    ],
    "cost": 0.006555,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Direct Multi-Turn Preference Optimization for Language Agents</p>
<p>Wentao Shi shiwentao123@mail.ustc.edu 
University of Science and Technology
China</p>
<p>Mengqi Yuan yuanmengqi@mail.ustc.edu 
University of Science and Technology
China</p>
<p>Junkang Wu 
University of Science and Technology
China</p>
<p>Qifan Wang 
MetaAI</p>
<p>Fuli Feng fulifeng93@gmail.com 
University of Science and Technology
China</p>
<p>Direct Multi-Turn Preference Optimization for Language Agents
65B1436E4CAE0ACF39510236FD3DDEB5Preference Trajectories Data
Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents.Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives.However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function.Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories.In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations.Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.</p>
<p>Introduction</p>
<p>Developing generalist agents capable of solving complex tasks has been a central goal in the artificial intelligence community (Reed et al., 2022;Team et al., 2024).Recently, Language agents (Yao et al., 2022b) emerge as a prominent research direction, leveraging the considerable potential of Large Language Models to address intricate tasks involving instruction following (Ouyang et al., 2022), action planning (Huang et al., 2022), and tool utilization (Schick et al., 2024).Nevertheless, the substantial disparity between the pretraining task of Large Language Models and the requirements of agent tasks suggests significant potential for future advancements in language agent capabilities.</p>
<p>Behavioral Cloning (BC) (Pomerleau, 1991) is a frequently employed approach to bridge the do- main gap by fine-tuning LLMs through expert agent trajectories.Recent endeavors in BC (Chen et al., 2023;Zeng et al., 2023;Yin et al., 2023) involve the Supervised Fine-tuning of LLMs on optimal state-action pairs.Although these methods enable swift adaptation of LLMs to agent tasks, BC is notably susceptible to compounding errorsminor errors of the learner accumulate along interactions between the agent and environment, leading to performance deterioration in non-deterministic environments (Ross et al., 2011).</p>
<p>In alleviating compounding errors, Direct Preference Optimization (Rafailov et al., 2024b) has demonstrated remarkable success in the single-turn preference alignment task due to its simple implementation and robustness.DPO optimizes RL objectives by maximizing the likelihood of preferred responses over dis-preferred responses, mitigating the need for continuous interaction with the environment and the training instability commonly associated with traditional RL algorithms (Christianos et al., 2023;Liang et al., 2024).Although there has been an initial endeavor to apply the DPO loss on LLMs for agent tasks (Song et al., 2024), it encounters suboptimal performance, as it is tailored specifically for the single-turn bandit setting and is ill-suited for multi-turn agent tasks.This work aims to develop a robust loss function capable of directly optimizing RL objectives in multi-turn scenarios.The crux of this pursuit involves eliminating the partition function in the Bradley-Terry (BT) model (Bradley and Terry, 1952;Christiano et al., 2017).This entails ensuring the partition function's independence from the current state and neutralizing the impact of the length disparity between preferred and dis-preferred trajectories.To achieve this, we substitute the policy constraint with the state-action occupancy measure (SAOM) (Johnson et al., 2000) constraint in the RL objective and introduce length normalization into the BT model.These adjustments culminate in the development of a new and simple loss function DMPO for multi-turn agent tasks.As shown in Figure 1, DMPO directly optimizes the RL objective by maximizing the likelihood of preferred ("win") trajectory over dis-preferred ("lose") trajectory.Notably, the SAOM constraint has advantages in mitigating compounding errors compared to the policy constraint (Xu et al., 2020;Ghasemipour et al., 2020).Furthermore, the derivation offers a theoretical rationale for the efficacy of the length normalization technique in DPO loss (Meng et al., 2024).</p>
<p>To summarize, our contributions are threefold:</p>
<p>• We introduce a new loss function called DMPO, which directly optimizes RL objectives in multiturn scenarios, thereby mitigating the compounding errors associated with BC methods.</p>
<p>• We provide a theoretical explanation for the efficacy of the length normalization technique, illustrating how it cancels out the partition function in the BT model and improves performance.</p>
<p>• Extensive experiments on three multi-turn agent task datasets validate the effectiveness and the superiority of the DMPO loss function.</p>
<p>Related Work</p>
<p>In this section, we first introduce the in-context learning methods and fine-tuning methods of language agents and then review the literature in preference-based RL.</p>
<p>In-Context Learning Inspired by the superior in-context learning capabilities of LLMs (Achiam et al., 2023), researchers have designed various instruction prompts for LLMs, equipped with memory modules (Zhang et al., 2024), toolkits (Qu et al., 2024), and various workflows (Sumers et al., 2023), to build language agents for various real-world domains.ReAct (Yao et al., 2022b) incorporates CoT reasoning (Wei et al., 2022) into action generation.Reflexion (Shinn et al., 2024) and PROMST (Chen et al., 2024) refine the prompt using environment feedback.However, these in-context learning methods fail to fully exploit the potential of LLMs, since most LLMs are not specifically trained for agent tasks.This work focuses on adapting the LLMs to agent tasks through fine-tuning.</p>
<p>Agent Tuning Recent studies, including Fire-Act (Chen et al., 2023), AgentTuning (Zeng et al., 2023), Lumos (Yin et al., 2023), MIMIR (Deng et al., 2024), AUTOACT (Qiao et al., 2024), and α-UMi (Shen et al., 2024) supervised fine-tuning LLMs with self-instruct or expert trajectories.However, such BC approaches suffer from compounding errors when interacting with dynamic environments.Taking a step further, Pangu (Christianos et al., 2023) and CMAT (Liang et al., 2024) utilize RL technologies to further fine-tune the LLMs, which may result in a complex and unstable training procedure.To simplify the procedure, ETO (Song et al., 2024) and EMMA (Yang et al., 2024) directly employ the DPO loss (Rafailov et al., 2024b) to optimize the RL objective for the agent task.Nevertheless, the DPO loss is designed for single-turn bandit settings and is ill-suited for multiturn scenarios.Along this line, this work extends the DPO loss in multi-turn scenarios and derives the DMPO loss.</p>
<p>Preference-Based RL In multi-turn scenarios, preference-based RL typically starts by explicitly learning a reward function from preference data and then optimizing it (Fürnkranz et al., 2012;Christiano et al., 2017;Hejna III and Sadigh, 2023;Shin et al., 2021).However, this two-stage learning process presents challenges regarding training efficiency and instability.This work instead presents a single-stage policy learning approach using DMPO loss that directly optimizes a policy to satisfy preferences.While IPL (Hejna and Sadigh, 2024) and CPL (Hejna et al., 2023) share a similar idea with our work in eliminating the reward learning stage, their loss functions are limited to trajectory pairs of equal length, significantly restricting their applicability.</p>
<p>Preliminary</p>
<p>In this section, we present multi-turn agent task formulation and briefly introduce Direct Preference Optimization (DPO) loss.</p>
<p>Task Description</p>
<p>The agent task can be formulated as a Markov decision process (MDP).A MDP is a 5-tuple (S, A, T , R, γ), where S denotes the state space, A denotes action space, T denotes dynamic transition function S × A → S, R denotes reward function S × A → [0, 1], and γ ∈ [0, 1) is the discount factor.The goal for the agent is to choose actions at each time step that maximize the expected future discounted reward E</p>
<p>T −1 t=0 γ t r(s t , a t ) , where T is the trajectory length.</p>
<p>In the language agent setting (Christianos et al., 2023), the state space and action space are both subsets of the language space.For the initial state s 0 ∈ S, it contains the task instruction and prompt.At each time step t, LLMs generate action a t according to the policy π θ (a t |s t ) with the parameter θ.Then the environment will return dynamic feedback o t and transport the state into s t+1 .Note that the new state s t+1 is just a simple combination of s t , a t , and o t , and the trajectory τ = (s 0 , a 0 , s 1 , a 1 , • • • , s T , a T ).</p>
<p>Direct Preference Optimization</p>
<p>The aim of the DPO loss is to directly optimize RL objectives with KL divergence constraints on the policy function:
max π θ E τ [ T −1 t=0 γ t r(s t , a t )] − βD KL [π θ (a t |s t )||π ref (a t |s t )], (1)
where E is the expectation function, D KL [•||•] denotes the KL divergence between two distributions, π ref denotes a reference policy, and the β is a parameter controlling the deviation from the base reference policy π ref .The DPO loss is tailored for the single-turn preference alignment setting, where the trajectory length (T ) is limited to 1.</p>
<p>Notably, the reward function is learned through the Bradley-Terry (BT) model (Bradley and Terry, 1952;Christiano et al., 2017):
p(a w 0 ≻ a l 0 |s 0 ) = exp(r(s 0 , a w 0 )) exp(r(s 0 , a w 0 )) + exp(r(s 0 , a l 0 )) ,(2)
which gives the probability that the "win" action a w 0 is preferred to the "lose" action a l 0 given the state s 0 .</p>
<p>Then DPO leverages the established closed-form solution for the single-turn formulation of the reinforcement learning problem in Eq (1) presented in (Ziebart et al., 2008;Ziebart, 2010):
π * (a|s) = 1 Z(s) π ref (a|s)e r(s,a) ,(3)
where π * denotes the optimal policy and Z(s) denotes the partition function that normalizes it.We can easily rearrange Eq (3) and substitute it into Eq (2) to get the BT model over policy:
p(a w 0 ≻ a l 0 |s 0 ) = σ β log π θ (a w 0 |s 0 ) π ref (a w 0 |s 0 ) − β log π θ (a l 0 |s 0 ) π ref (a l 0 |s 0 ) ,(4)
where the partition function Z(s) is canceled from the BT model and σ is the sigmoid function.The DPO loss obtains the optimal policy π * θ by maximizing the likelihood:
L DP O = −E (s 0 ,a w 0 ,a l 0 )∼D log p(a w 0 ≻ a l 0 |s 0 ) ,(5)
where D represents the preference dataset.Nonetheless, such concise and elegant derivations are only suitable for single-turn preference optimization tasks.As shown in Eq (3), the partition function Z(s) is dependent on the current state s, which precludes its cancellation under the policy constraint in the multi-turn setting.</p>
<p>Method</p>
<p>In this section, we will outline the definition and benefits of the state-action occupancy measure.Subsequently, we will introduce two adjustments to derive the DMPO loss.Finally, we will delve deeper into the analysis of the DMPO loss.</p>
<p>State-Action Occupancy Measure</p>
<p>The discounted state-action occupancy measure d π (s, a) of a policy π describes the distribution of state-action pairs that an agent visits in the space with policy π: where P(•) denotes the probability and the coefficient
d π (s, a) = 1 − γ 1 − γ T T −1 t=0 γ t P(s t = s, a t = a|π),(6)(1 − γ)/(1 − γ T ) is used to normalize the probability distribution.
First, we will provide an intuitive explanation of how the SAOM constraint can reduce the compounding error.In imitation learning, the conventional SFT learning objective aims to minimize the KL divergence between the expert policy and the current policy:
min π θ E (s,a)∼d E [D KL (π E (a|s)||π θ (a|s)] = − max π θ E (s,a)∼d E [log(π θ (a|s)],(7)
where π E is the expert policy and d E is the SAOM with policy π E .As shown in Figure 2, the trajectories learned under policy constraints are susceptible to significant compounding error.This vulnerability stems from the fact that expert datasets are unable to comprehensively cover all possible states.Consequently, the SFT loss leads the model to choose random actions in states that are not represented in the expert datasets.As a result, the model gradually deviates from the expert trajectories after the initial error, illustrating the phenomenon known as compounding error.</p>
<p>To alleviate the compounding error, subsequent imitation learning research such as (Abbeel and Ng, 2004;Ghasemipour et al., 2020;Ho and Ermon, 2016) employ the SAOM constraint:
min π θ E (s,a)∼d E [D (•) (d π θ (a|s)||d π E (a|s))], (8)
where different approaches utilize different distribution distance measures D (•) .The strength of SAOM constraint lies in its ability to steer action selection towards distributions that closely mimic expert state-action pairs, especially in unexplored states within the expert datasets.Illustrated in Figure 2, at state s 2 , policy constraints lead the model to choose actions uniformly, whereas SAOM constraints aim to lead the model toward actions that bring the next state back onto the expert trajectory.This effectively mitigates compounding errors and enhances the cumulative reward.</p>
<p>DMPO</p>
<p>Inspired by imitation learning, we substitute the policy constraint with the SAOM constraint in Eq (1) and get the following RL objective:
max π θ E (s,a)∼d π θ (s,a) [r(s, a)] − βD KL [d π θ (s, a)||d π ref (s, a)], (9)
where π ref represents the reference policy.Similar to (Rafailov et al., 2024b), it is straightforward to show that the optimal solution to the RL objective in Eq (9) takes the form:
d π * (s, a) = 1 Z d π ref (s, a) exp( 1 β r(s, a)), (10)
where π * represents the optimal policy, Z is the partition function that normalizes the probability.</p>
<p>It's noteworthy that as d π (s, a) is a function of (s, a) pairs, normalizing it results in the partition functions Z being independent of the current state s.Consequently, Z remains constant for all (s, a) pairs, providing us with the opportunity to eliminate them.Easily, we can rearrange Eq (10) into:
r(s, a) = β log d π * (s, a) d π ref (s, a) + β log Z. (11)
Similar to Eq (2), we learn the reward function for multi-turn scenarios through the BT model:
p(τ w ≻ τ l |s 0 ) = σ Tw−1 t=0 γ t r(s w t , a w t ) − T l −1 t=0 γ t r(s l t , a l t ) ,(12)
where τ w and τ l represent the "win" and "lose" trajectories respectively, T w and T l represent the "win" and "loss" trajectory length respectively.However, since T w ̸ = T l , the partition function Z cannot be canceled directly in Eq (12).</p>
<p>To overcome this obstacle, we introduce the length normalization technique to Eq (12):
p(τ w ≻ τ l |s 0 ) = σ 1 − γ 1 − γ Tw Tw−1 t=0 γ t r(s w t , a w t ) − 1 − γ 1 − γ T l T l −1 t=0 γ t r(s l t , a l t ) . (13)
In this way, we can eliminate the partition function Z in Eq (13) by substituting the reward function r(s, a) in Eq (11).Then we maximize the likelihood and obtain:
L DM P O = −E (s 0 ,τ w ,τ l )∼D log σ 1 − γ 1 − γ Tw Tw−1 t=0 βγ t log d π θ (s w t , a w t ) d π ref (s w t , a w t ) − 1 − γ 1 − γ T l T l −1 t=0 βγ t log d π θ (s l t , a l t ) d π ref (s l t , a l t ) ,(14)
where the d π (s t , a t ) can be further written as:
d π (s = s w t , a = a w t ) = γ t • P (s 0 )• t−1 k=0 π(a w k |s w k )P (s w k+1 |s w k , a w k ),(15)
where P (s 0 ) represents the probability of the initial state s 0 and P (s k+1 |s k , a k ) denotes the transition functions.In general, obtaining the SAOM d π (s t , a t ) is challenging because we do not know the transition function P (s k+1 |s k , a k ) in dynamic environments.However, in Eq (14) we simply calculate the ratio between the current SAOM d π θ (s t , a t ) and the reference SAOM d π ref (s t , a t ).</p>
<p>It is important to note that the transition function remains consistent for both, allowing for cancellation.By substituting the Eq (15) into Eq (14), we can obtain the DMPO loss function:
L DM P O = −E (s 0 ,τ w ,τ l )∼D log σ Tw−1 t=0 βϕ(t, T w ) log π θ (a w t |s w t ) π ref (a w t |s w t ) − T l −1 t=0 βϕ(t, T l ) log π θ (a l t |s l t ) π ref (a l t |s l t ) ,(16)
where the discount function
ϕ(t, T ) = (1 − γ T −t )/(1 − γ T ).
It's noteworthy that DMPO reweights state-action pairs at various steps using a discount function ϕ(t, T ).</p>
<p>In-Depth Analysis</p>
<p>In this subsection, we will explore the advantages of the DMPO loss and present some lemmas and observations.</p>
<p>Corollary 4.0.1.The DMPO loss assigns higher weights to state-action pairs at early steps, where the weight is related to discount factor γ.</p>
<p>Proof.To prove the lemma, we analyze the gradient of the loss function L DM P O according to θ:
∇ θ L DM P O = −E (s 0 ,τ w ,τ l )∼D σ[Φ(τ l )−Φ(τ w )] Tw−1 t=0 βϕ(t, T w )∇ θ log π θ (a w t |s w t ) − T l −1 t=0 βϕ(t, T l )∇ θ log π θ (a l t |s l t ) ,(17)
where function Φ(τ ) =</p>
<p>T −1 t=0 βϕ(t, T ) log π θ (at|st) π ref (at|st) and ϕ(t, T ) = (1 − γ T −t )/(1 − γ T ).The discount function ϕ(t, T ) decreases as t increases and is related to the discounted factor γ.This completes the proof.</p>
<p>Corollary 4.0.2.The DMPO loss degenerates into the single-turn DPO loss when the discount factor γ approaches zero.</p>
<p>Proof.When γ equals 0, the function ϕ(t, T ) is 1 at t = 0, and 0 otherwise, which is equivalent to a single-turn DPO loss.</p>
<p>Based on the analysis above, we have the following observations: Observation 4.0.1.Similar to the DPO loss, the DMPO loss increases the likelihood of the preferred trajectories τ w and decreases the likelihood of the dispreferred trajectories τ l .</p>
<p>Observation 4.0.2.If the reward Φ(τ l ) of dispreferred trajectory is estimated higher by the policy π θ , the weight σ[Φ(τ l ) − Φ(τ w )] will be larger.</p>
<p>Length</p>
<p>Normalization Explanation In SimPO (Meng et al., 2024), the effectiveness of the length normalization technique was empirically demonstrated.However, a theoretical explanation was not provided.Our derivation shows that it assists in eliminating the partition function.Without length normalization in Eq (13), a length-dependent bias term arises in the BT model, degrading model performance as the disparity in trajectory lengths between preferred and dispreferred samples increases.</p>
<p>Further Discussion</p>
<p>As discussed in Section 4.2, the optimal solution to the RL objective in Eq (9) takes the form shown in Eq (10).However, it is contended that achieving the optimal solution may not always be feasible when dealing with an arbitrary reward function r(s, a) within the context of a language agent setting.This limitation arises due to the definition of the new state s t+1 as a composite of s t , a t , and o t , which introduces an inherent constraint on the transition function between states.</p>
<p>In general, in multi-turn dynamic environments, no loss function can rigorously optimize the RL objective, and the DMPO loss serves as a good approximation.In many cases, the DMPO loss can precisely optimize the RL objective in Eq (9).</p>
<p>Experiments</p>
<p>In this section, we conduct extensive experiments on three agent tasks to demonstrate the effectiveness of the proposed DMPO loss function.Our experiments aim to address the following questions:</p>
<p>• RQ1: Can the DMPO loss function exhibit robustness to noisy training trajectories data and mitigate compounding errors?</p>
<p>• RQ2: How does the DMPO loss function perform compared to other baselines?</p>
<p>• RQ3: What is the impact of the discount factor γ and the trajectory length on the DMPO loss?</p>
<p>Experiment Setting</p>
<p>Datasets Following prior work (Song et al., 2024), we conduct experiments on three representative agent datasets, including WebShop (Yao et al., 2022a), ScienceWorld (Wang et al., 2022), and ALFWorld (Shridhar et al., 2020b).</p>
<p>• WebShop is a simulated shopping website environment where agents find and purchase products according to specifications provided in a natural language instruction.The final reward r ∈ [0, 1] is calculated based on how closely the purchased products match the specified criteria.</p>
<p>• ScienceWorld is an interactive text environment that tests agents' scientific reasoning abilities in elementary science experiments with 10 task types.The final reward r ∈ [0, 1] is computed based on the number of subgoals the agent successfully accomplishes within each task.</p>
<p>• ALFWorld is a simulated text-based environment that enables agents to complete embodied household tasks from the ALFRED benchmark (Shridhar et al., 2020a).The final binary rewards signify the completion status of the task.</p>
<p>All three environments can be formally described as MDP and conducted by language agents.The statistical details of our datasets are outlined in Table 1.Following (Song et al., 2024), in addition to the in-distribution "seen" test sets, both ScienceWorld and ALFWorld include "unseen" test sets that include out-of-distribution tasks.These additional test sets enable us to evaluate the generalization capabilities of different agents.</p>
<p>Training Setting We assess the robustness and effectiveness of the DMPO loss function by employing two distinct training scenarios: Noisy setting and Clean setting.Following (Song et al., 2024), we adopt the experts' trajectories as the "win" trajectories to form preference trajectory data in both noisy setting and clean setting.Initially, we utilize the LLMs, which have been fine-tuned with expert trajectories, to generate new trajectories on the training set.We observe that the LLMs have a tendency to generate trajectories with repeated actions or meaningless words.In the noisy setting, these noisy trajectories are used as "lose" trajectories for preference data.Conversely, in the Clean setting, we eliminate the noisy trajectories and employ the remaining ones as "lose" trajectories for preference data.</p>
<p>Parameter Settings In this work, we utilize two different base models Llama-2-7B-Chat (Touvron et al., 2023) and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) to build language agents.Following (Song et al., 2024), we utilize the AdamW optimizer.When supervised fine-tuning the base models to get the reference model, we set the batch size to 64.The learning rate is selected from {1e-5, 2e-5, 3e-5} with 3% warm up and a cosine scheduler.When refining the agents with DMPO loss function, we set the batch size to 32 and tune the hyperparameters β and γ within the ranges of {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 } and {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99} respectively.We conduct all experiments on 8 NVIDIA A100 GPUs.</p>
<p>Evaluation Setting Following (Song et al., 2024), we evaluate all methods using the ReActstyle interaction format (Yao et al., 2022b), which generates both reasoning traces and actions in an in- terleaved manner.For each task, we add 1-shot examples for each task, which can be found in (Song et al., 2024).Unless otherwise stated, we set the decoding generate temperature as 0.0.</p>
<p>Noisy Setting Results (RQ1)</p>
<p>In the noisy setting, we utilize the noisy trajectories as "lose" trajectories for preference data to investigate the robustness of the DMPO loss function.</p>
<p>As shown in Table 2, we evaluate the DMPO loss function with two different base models on two representative agent tasks and observe that:</p>
<p>• In all Unseen test sets and most Seen test sets for both base models, the DMPO loss function outperforms the DPO loss function.This superiority stems from DMPO assigning greater importance to initial state-action pairs, prioritizing high-quality expert actions from the early stages, and reducing the influence of noisy "lose" actions in later stages.This mitigates the influence of noise, endowing the model with enhanced generalization capabilities.Meanwhile, the DPO loss is not appropriate for multi-turn settings and cannot cancel out the partition function in the BT model, thereby resulting in its inferior performance.</p>
<p>• The performance of Mistral-7B-Instruct-v0.2 is significantly better than that of Llama-2-7B-Chat on Scienceworld and AlfWorld.This observation suggests a positive correlation between the effectiveness of the base model and its performance enhancement after fine-tuning for agent tasks using the DMPO loss function.</p>
<p>Clean Setting Results (RQ2)</p>
<p>In clean setting, we filter out the noisy trajectories and select high-quality trajectories as the "lose" trajectories for preference data, enabling us to utilize the DMPO loss function fully.</p>
<p>Baselines Following (Song et al., 2024), we compare our models trained by DMPO loss function with the following representative baselines.1) Base: default LLM without tuning.2) SFT: LLM fine-tuned through supervised learning on expert trajectories.3) Best-of-N: This approach involves using an SFT-based agent for sampling and selecting the trajectory with the highest reward out of N samples.Here, N is specified as 10. 4) RFT (Rejection sampling Fine-Tuning) (Yuan et al., 2023): This approach augments the expert trajectory dataset by incorporating successful trajectories and subsequently trains the agent on the augmented dataset.5) PPO (Proximal Policy Optimization) (Schulman et al., 2017) directly optimize RL objectives to maximize the cumulative rewards.6) ETO (Exploration-based Trajectory Optimization) (Song et al., 2024) iteratively explores the environment to enhance the training preference data and utilizes DPO loss to learn from preference data.</p>
<p>Results</p>
<p>Based on the Llama-2-7B-Chat model, we show the comparison results under clean setting in Table 3. Notably, we observe that:</p>
<p>• All fine-tuning methods significantly outperform the base model on both datasets, with improvements of at least 49%.On Webshop, they even surpass the performance of advanced closed-source LLMs.This underscores the significant gap be- tween the pre-training tasks of LLMs and the agent tasks.By fine-tuning LLMs, language agents exhibit substantial potential for improvement.</p>
<p>• The model trained using DMPO loss achieved optimal performance on both datasets, highlighting the effectiveness of DMPO loss in learning from preference data.The improvement over the SFT model suggests that DMPO reduces the compounding errors, resulting in higher rewards.</p>
<p>• The model trained using DMPO loss exhibits substantial performance improvements compared to the noisy setting, achieving an average increase of 5.2% on Webshop and 11.3% on Scienceworld.This highlights the importance of selecting highquality "lose" trajectories in constructing preference data, as opting for such trajectories yields superior performance.</p>
<p>Ablation Study (RQ3)</p>
<p>Hyperparamter Analysis To verify the impact of reweight function ϕ(t, T ) in Eq (17), we tune the the hyperparameter γ on WebShop and present the results in Figure 3.Our findings reveal that both base models achieve optimal performance with a smaller γ in the noisy setting and a larger γ in the clean setting.According to Eq (17), a smaller γ implies that the DMPO loss assigns reduced weight to the state-action pairs in later steps.This indicates that DMPO can balance the impact of noise by adjusting the parameter γ.When faced with noisy "loss" trajectories, selecting a smaller γ can help alleviate noise impact.Conversely, when dealing with high-quality "loss" trajectories, a larger gamma can be selected to better learn strategies from the state-action pairs in later steps.</p>
<p>Length Analysis To examine the impact of trajectory length on model performance, we conducted an experiment by categorizing the noisy trajectories into three groups based on their maximum length.We ensure that the number of preference data in each group is the same.As shown in Figure 4, we observe that the performance of the model trained with DPO loss function decreases rapidly as the length of noisy "loss" trajectories increases.In contrast, the model trained with the DMPO loss function exhibits robustness against noisy "loss" trajectory length.This is attributed to the length normalization employed in the DMPO loss, which mitigates the influence of inconsistent lengths between "win" and "lose" trajectories.</p>
<p>Conclusion</p>
<p>In this work, we propose a simple and robust loss function DMPO loss, which directly optimizes the RL objective for multi-turn agent tasks.By substituting the policy constraint with the SAOM constraint and introducing the length normalization into BT model, we eliminate the partition function in the BT model and derive the DMPO loss function.The SAOM constraint has played a pivotal role in mitigating compounding errors.Meanwhile, this derivation offers a theoretical rationale for the efficacy of the length normalization technique.Extensive experiments on three agent datasets demonstrate the effectiveness of DMPO loss, highlighting its capability to reduce compounding errors and its resilience to trajectory length disparity.</p>
<p>Limitation</p>
<p>This paper primarily focuses on issues when finetuning LLMs on the agent tasks and derives a simple and robust loss function.However, our study has several limitations: 1) We solely concentrate on turn-wise task formulation which results in sparse rewards for LLMs.Exploring token-wise task formulation as suggested in (Rafailov et al., 2024a) would be a valuable avenue for future investigation.</p>
<p>2) The experiments in this work are conducted using 7B-sized models on simulated datasets.Future experiments on larger models and datasets can provide stronger validation of our conclusions.</p>
<p>Ethical Considerations</p>
<p>In this paper, we present a new DMPO loss function for refining LLMs in agent tasks, without bringing forth additional ethical dilemmas.We utilize publicly accessible data while conscientiously steering clear of sensitive information.Additionally, the use of LLMs could perpetuate unnoticed societal biases.We suggest thorough risk assessments and advise users to be mindful of the potential risks linked to model deployment.</p>
<p>A Case Study</p>
<p>In this section, we compare the performance of DPO and DMPO using an example from WebShop.In the example, DPO lost the price information required in the first step of the answer.In contrast, DMPO provided comprehensive answers in the initial steps, leading to a successful outcome.</p>
<p>Case</p>
<p>Figure 1 :
1
Figure 1: Illustration of DMPO loss, which directly optimizes the RL objective by maximizing the likelihood of the preferred trajectory over the dispreferred trajectory.</p>
<p>Figure 2 :
2
Figure 2: Illustration of expert trajectories and trajectories learned under the constraints of policy and stateaction occupancy measure.</p>
<p>Figure 3 :
3
Figure 3: The effect of hyperparameter γ on the relative performance of the model trained with DMPO loss on the WebShop dataset in both noisy and clean settings.</p>
<p>Figure 4 :
4
Figure 4: The effect of "loss" trajectories length on the performance of the model trained with DPO and DMPO loss in the noisy setting on ScienceWorld.The base model is Mistral-7B-Instruct-v0.2.</p>
<p>Table 2 :
2
Noisy setting: The average reward of different base LLMs on three agent datasets."Seen" denotes in-distribution test sets, while "Unseen" denotes out-of-distribution test sets.The results are averaged with three distinct random seeds.The best results for each base model are highlighted in bold.
MethodWebShopScienceWorld Seen UnseenALFWorld SeenUnseenLlama-2-7B-Chat + DPO Llama-2-7B-Chat + DMPO0.641 ± 0.002 0.666 ± 0.0070.601 ± 0.004 0.619 ± 0.0030.576 ± 0.001 0.584 ± 0.0050.474±0.004 0.433 ± 0.0040.540 ± 0.005 0.550 ± 0.004Mistral-7B-Instructv0.2 + DPO Mistral-7B-Instructv0.2 + DMPO0.637 ± 0.007 0.643 ± 0.0080.700 ± 0.003 0.708 ± 0.0150.629 ± 0.008 0.651 ± 0.0040.745 ± 0.004 0.742 ± 0.0120.883 ± 0.004 0.888 ± 0.000MethodWebShopScienceWorld Seen UnseenGPT-4<em>0.6320.6480.644GPT-3.5-Turbo</em>0.6240.1650.130Base<em>0.1790.3800.310Best-of-N</em>0.6380.7020.576RFT<em>0.6360.7160.543PPO</em>0.6420.5940.517SFT0.6310.5680.560ETO0.698±0.0030.685±0.0040.611±0.003DMPO0.701±0.0030.724±0.0050.617±0.002</p>
<p>Table 3 :
3
(Song et al., 2024)average reward of different methods on two agent datasets based on Llama-2-7B-Chat.The best results of tuning methods are highlighted in bold.*Results are taken from(Song et al., 2024).</p>
<p>Study of WebShop DPO Task Instruction: I want green modern velvet dining chairs for the dining room, and price lower than 200.00 dollars Thought 1: I should search for green modern velvet dining chairs first.Action 1: search[green modern velvet dining chairs] Observation 1: <searching results> Thought 2: The chair with the code B08TW9LTLM seems to be a green modern velvet dining chair within the price range.I should click on it to get more information.Action 2: click[B08TW9LTLM] Observation 2: <product details> Thought 3: The item has options for different quantities and colors.I should select the quantity that matches my needs.I want green modern velvet dining chairs for the dining room, and price lower than 200.00 dollars Thought 1: I need to find green modern velvet dining chairs that are less than $200.00,so I'll start by searching for that specific description.Action 1: search[green modern velvet dining chairs] Observation 1: <searching results> Thought 2: There are several options, but many are over my budget.The ZHENGHAO Modern Velvet Dining Chairs Set of 2 for $173.99 seems to fit my criteria, even though it's slightly over my budget.I'll click on it to see more details.Action 2: click[B07R9KFCKY] Observation 2: <product details> Thought 3: This set of chairs is available in emerald green, which is the color I want.I'll select this color option.Action 3: click[emerald green] Observation 3: <searching results> Thought 4: Now I need to decide if I want a set of 2 or a set of 4. Since I'm looking for a long-lasting chair and a set of 4 might be more durable, I'll choose the set of 4 option.Action 4: click[set of 4] Observation 4: <searching results> Thought 5: I've selected the color and the set of chairs I want, and the price is slightly over my budget.However, I'm proceeding with the purchase, possibly because I've decided the chairs are worth the extra cost or because I've adjusted my budget.
DMPOAction 3: click[2] Observation 3: <product details> Thought 4: Now I should click on the Buy Now button to purchase the item. Action 4: click[Buy Now] Reward: 0.33 # The selected item does not meet the price requirement.Task Instruction: Action 5: click[buy now] Reward: 1.0
Figure 5: Case study of WebShop.</p>
<p>AcknowledgmentsThis work is supported by the National Natural Science Foundation of China (62272437).This research was also supported by the advanced computing resources provided by the Supercomputing Center of the USTC.B MT-BenchIn this section, we evaluate and compare the models trained with DMPO vs DPO on various datasets using MT-bench(Zheng et al., 2023), and the results are presented in Table4.The analysis of win rates presented in the table indicates that DMPO consistently outperforms DPO across all training datasets on the MT-bench.Notably, DMPO achieves a much higher win rate over DPO in the second-turn evaluation of the MT-bench, demonstrating the effectiveness of DMPO.
Apprenticeship learning via inverse reinforcement learning. Pieter Abbeel, Andrew Y Ng, Proceedings of the twenty-first international conference on Machine learning. the twenty-first international conference on Machine learning20041</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. Allan Ralph, Milton E Bradley, Terry, Biometrika. 393/41952</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, arXiv:2310.05915Fireact: Toward language agent fine-tuning. 2023arXiv preprint</p>
<p>Prompt optimization in multi-step tasks (promst): Integrating human feedback and preference alignment. Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan, arXiv:2402.087022024arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730</p>
<p>Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, arXiv:2312.14878Pangu-agent: A fine-tunable generalist agent with structured reasoning. 2023arXiv preprint</p>
<p>Mimir: A streamlined platform for personalized agent tuning in domain expertise. Chunyuan Deng, Xiangru Tang, Yilun Zhao, Hanming Wang, Haoran Wang, Wangchunshu Zhou, Arman Cohan, Mark Gerstein, arXiv:2404.042852024arXiv preprint</p>
<p>Preference-based reinforcement learning: a formal framework and a policy iteration algorithm. Johannes Fürnkranz, Eyke Hüllermeier, Weiwei Cheng, Sang-Hyeun Park, Machine learning. 892012</p>
<p>A divergence minimization perspective on imitation learning methods. Seyed Kamyar, Seyed Ghasemipour, Richard Zemel, Shixiang Gu, Conference on robot learning. PMLR2020</p>
<p>Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, Bradley Knox, Dorsa Sadigh, arXiv:2310.13639Contrastive prefence learning: Learning from human feedback without rl. 2023arXiv preprint</p>
<p>Inverse preference learning: Preference-based rl without a reward function. Joey Hejna, Dorsa Sadigh, Advances in Neural Information Processing Systems. 202436</p>
<p>Fewshot preference learning for human-in-the-loop rl. Donald Joseph, Hejna Iii, Dorsa Sadigh, Conference on Robot Learning. PMLR2023</p>
<p>Generative adversarial imitation learning. Jonathan Ho, Stefano Ermon, Advances in neural information processing systems. 201629</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR2022</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2023Mistral 7b. CoRR, abs/2310.06825</p>
<p>Reinforcement learning: An introduction. Jeffrey D Johnson, Jinghong Li, Zengshi Chen, Neurocomputing. R.S. sutton, A.G. barto351-42000. 1998MIT press</p>
<p>Cmat: A multi-agent collaboration tuning framework for enhancing small language models. Xuechen Liang, Meiling Tao, Tianyu Shi, Yiting Xie, arXiv:2404.016632024arXiv preprint</p>
<p>Simpo: Simple preference optimization with a reference-free reward. Yu Meng, Mengzhou Xia, Danqi Chen, arXiv:2405.147342024Preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Efficient training of artificial neural networks for autonomous navigation. A Dean, Pomerleau, Neural computation. 311991</p>
<p>Autoact: Automatic agent learning from scratch via self-planning. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen, arXiv:2401.052682024arXiv preprint</p>
<p>Tool learning with large language models: A survey. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen, arXiv:2405.179352024Preprint</p>
<p>Rafael Rafailov, Joey Hejna, Ryan Park, Chelsea Finn, arXiv:2404.12358From r to q*: Your language model is secretly a q-function. 2024aarXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 2024b36</p>
<p>Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. 2022. A generalist agent. Scott E Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Trans. Mach. Learn. Res. 2022</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, the fourteenth international conference on artificial intelligence and statistics2011. 202436Advances in Neural Information Processing Systems</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, Fei Huang, arXiv:2401.07324Small llms are weak tool learners: A multi-llm agent. 2024arXiv preprint</p>
<p>Offline preference-based apprenticeship learning. Daniel Shin, Daniel S Brown, Anca D Dragan, arXiv:2107.092512021arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020a</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020barXiv preprint</p>
<p>Trial and error: Exploration-based trajectory optimization for llm agents. Yifan Song, Xiang Da Yin, Jie Yue, Sujian Huang, Bill Li, Lin Yuchen, arXiv:2403.025022024arXiv preprint</p>
<p>Shunyu Theodore R Sumers, Karthik Yao, Thomas L Narasimhan, Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>. Maria Sima Team, Arun Abi Raad, Catarina Ahuja, Frederic Barros, Andrew Besse, Adrian Bolt, Bethanie Bolton, Gavin Brownfield, Max Buttimore, Sarah Cant, Chakera, C Y Stephanie, Jeff Chan, Adrian Clune, Vikki Collister, Alex Copeman, Ishita Cullum, Dario Dasgupta, Julia De Cesare, Yani Di Trapani, Emma Donchev, Martin Dunleavy, Ryan Engelcke, Frankie Faulkner, Charles Garcia, Zhitao Gbadamosi, Lucy Gong, Kshitij Gonzalez, Karol Gupta, Arne Olav Gregor, Tim Hallingstad, Sam Harley, Felix Haves, Ed Hill, Drew A Hirst, Jony Hudson, Steph Hudson, Danilo J Hughes-Fitt, Mimi Rezende, Laura Jasarevic, Nan Rosemary Kampis, Thomas Ke, Junkyung Keck, Oscar Kim, Kavya Knagg, Andrew K Kopparapu, Shane Lampinen, Alexander Legg, Marjorie Lerchner, Yulan Limont, Maria Liu, Joseph Loks-Thompson, Kathryn Martin Marino, Loic Cussons, Siobhan Matthey, Piermaria Mcloughlin, Hamza Mendolicchio, Anna Merzic, Alexandre Mitenkova, Valéria Moufarek, Yanko Gitahy Oliveira, Hannah Oliveira, Openshaw, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David P. Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. SawyerNathaniel WongRenke Pan; Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam, Denis Teplyashin, Tayfun Terzi, Davide Vercelli; Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams; Sarah Yorkand Nick Young. 2024. Scaling instructable agents across many simulated worlds. CoRR, abs/2404.10179</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Cristian Canton Blecher, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel Khabsa, Artem Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Iliyan Mihaylov ; Zheng Yan, Yuchen Zarov, Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Error bounds of imitating policies and environments. Tian Xu, Ziniu Li, Yang Yu, 2020In NeurIPS</p>
<p>Embodied multi-modal agent trained by an llm from a parallel textworld. Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022barXiv preprint</p>
<p>Lumos: Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, arXiv:2311.056572023arXiv preprint</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, CoRR, abs/2308.018252023</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, arXiv:2306.056852023Preprint</p>
<p>Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. Brian D Ziebart, 2010USACarnegie Mellon UniversityPh.D. thesis</p>
<p>Maximum entropy inverse reinforcement learning. Andrew L Brian D Ziebart, Andrew Maas, Anind K Bagnell, Dey, Aaai. Chicago, IL, USA20088</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5594 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5594</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5594</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-270675818</p>
                <p><strong>Paper Title:</strong> Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model</p>
                <p><strong>Paper Abstract:</strong> : The compaction and structuring of system logs facilitate and expedite anomaly and cyberattack detection processes using machine-learning techniques, while simultaneously reducing alert fatigue caused by false positives. In this work, we implemented an innovative algorithm that employs hierarchical codes based on the semantics of natural language, enabling the generation of a significantly reduced log that preserves the semantics of the original. This method uses codes that reflect the specificity of the topic and its position within a higher hierarchical structure. By applying this catalog to the analysis of logs from the Hadoop Distributed File System (HDFS), we achieved a concise summary with non-repetitive themes, significantly speeding up log analysis and resulting in a substantial reduction in log size while maintaining high semantic similarity. The resulting log has been validated for anomaly detection using the “bert-base-uncased” model and compared with six other methods: PCA, IM, LogCluster, SVM, DeepLog, and LogRobust. The reduced log achieved very similar values in precision, recall, and F1-score metrics, but drastically reduced processing time.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5594.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5594.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT_20M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>bert-base-uncased applied to HDFS (20M) for anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This work applies the pretrained 'bert-base-uncased' model to detect anomalies in HDFS system-log sequences and to evaluate semantic preservation after a hierarchical-code based log compaction; embeddings and the model are used for anomaly detection and similarity analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>bert-base-uncased</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT: a pretrained bidirectional Transformer encoder (the 'bert-base-uncased' checkpoint), pretrained on large English corpora; used here as the language-model backbone to produce embeddings and to perform anomaly-detection modeling on log sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Applied BERT as the anomaly-detection model on tokenized log lines (used embeddings and model inference/further modeling for classification of anomalous sequences); also used cosine similarity on BERT embeddings to measure semantic similarity between original and compacted logs. The paper implies model fine-tuning/inference on labeled anomaly data (standard precision/recall/F1 evaluation), though exact fine-tuning recipe is not fully detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>structured/semi-structured system logs (HDFS) treated as sequences of tokens/messages</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>log-sequence anomalies / anomalous events in system logs (rare or abnormal log templates/sequences indicating system problems)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS (two variants: HDFS_2k and HDFS_20M; the anomaly detection evaluation reported on HDFS_20M_HC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision 0.88, Recall 0.86, F1-Score 0.83 (reported for BERT_20M on the processed 20M HDFS log). Also reported cosine similarity between original and compacted embeddings: 0.9724.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to PCA, Invariant Mining (IM), LogCluster, SVM, DeepLog, and LogRobust: BERT_20M achieved lower precision/F1 than top methods such as LogCluster and DeepLog (which report higher precision/recall/F1 in the paper), but outperformed SVM and PCA on some metrics. Specifically, some baselines reported precision of 1.00 (PCA, IM, LogCluster, DeepLog) and higher F1 (LogCluster 0.96, DeepLog 0.98) while BERT_20M had F1 0.83.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Did not achieve top F1/precision compared to some specialized methods (LogCluster, DeepLog). The paper's hierarchical-cataloging process can leave some tokens uncategorized (catalog coverage limits), relies on English WordNet nouns (limited to English noun vocabulary / single language), and compaction increases lexical density which raised mean per-batch processing time in some cases. The paper does not report AUROC or other thresholds, and exact fine-tuning details are not fully specified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5594.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5594.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models (LLMs) for anomaly detection (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references the emerging use of large pre-trained language models / transformer-based LLMs to detect anomalies in system logs by learning language patterns and semantics in log sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Models / Transformer-based LMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General family: transformer-based pretrained LMs (bidirectional and autoregressive variants) trained on large corpora; able to encode semantic and contextual information from text and log sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Mentioned approaches include fine-tuning pretrained LMs on normal log data and using them to detect deviations in log sequences (self-supervised or supervised fine-tuning, representation learning and sequence modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>system logs treated as sequences / textual events</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic anomalies in log sequences, unusual patterns or rare/unseen events</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Discussed qualitatively as offering superior semantic understanding vs older approaches (e.g., RNN/LSTM, autoencoders) and as complementary to hierarchical compaction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>General limitations mentioned: resource and performance costs scale with token volume; necessity of pre-processing/log compaction to reduce cost; transferability and robustness require careful evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5594.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5594.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT (Log Anomaly Detection via BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited prior work that proposes a self-supervised BERT-based framework for log-anomaly detection, extracting contextualized semantic representations of log events to detect anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Log Anomaly Detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogBERT (BERT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-supervised BERT-based framework adapted to log data to learn contextual representations of log events for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Self-supervised BERT-style training on logs to build representations used to flag anomalous log events or sequences (representation + downstream detection).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log sequences / event sequences</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>anomalous log sequences / events</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5594.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5594.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogFiT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogFiT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work (Almodovar et al.) that fine-tunes a BERT-based language model on normal log data to detect anomalies by recognizing linguistic patterns in logs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Log Anomaly Detection using Fine-Tuned Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>fine-tuned BERT (LogFiT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT checkpoint fine-tuned on log corpora to model typical log language patterns for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Fine-tuning a pretrained language model on normal logs so deviations from learned patterns indicate anomalies (supervised or self-supervised fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>system logs / log sequences</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic and sequence anomalies in logs</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5594.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5594.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogEvent2vec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogEvent2vec</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited approach that maps log events to vector representations (event-to-vector) to enable anomaly detection on large-scale logs, using representation learning techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogEvent2vec: LogEvent-to-Vector Based Anomaly Detection for Large-Scale Logs in Internet of Things</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogEvent2vec (representation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A technique converting log events into vector embeddings for downstream anomaly detection; focuses on scalability for large logs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Representation learning to embed log events (not necessarily a large pretrained language model) and detect anomalies via vector-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>large-scale logs / event sequences</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>outliers or abnormal event embeddings / sequence anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5594.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5594.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogEncoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogEncoder (Log-based Contrastive Representation Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work exploring contrastive representation learning for log events to improve anomaly-detection representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Log-Based Contrastive Representation Learning for Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogEncoder (contrastive representation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive learning framework that produces semantic vectors of log events for anomaly detection, improving separability of normal vs anomalous patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Contrastive representation learning on logs to produce embeddings used for anomaly detection (unsupervised or self-supervised).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log sequences / structured log templates</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic/sequence anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5594.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5594.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robust/Transferable LMs (Ott et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study that emphasizes robustness and transferability of pre-trained language-model-based anomaly detectors across environments and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pre-trained language models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Study focuses on applying pretrained LMs for log anomaly detection, analyzing robustness/transferability across datasets and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Using pretrained language models (fine-tuning or feature extraction) for anomaly detection, with an emphasis on transferability/robustness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log sequences / event sequences</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>anomalous log sequences; cross-dataset anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Log Anomaly Detection via BERT <em>(Rating: 2)</em></li>
                <li>Log Anomaly Detection using Fine-Tuned Language Models <em>(Rating: 2)</em></li>
                <li>Log-Based Contrastive Representation Learning for Anomaly Detection <em>(Rating: 2)</em></li>
                <li>LogEvent2vec: LogEvent-to-Vector Based Anomaly Detection for Large-Scale Logs in Internet of Things <em>(Rating: 2)</em></li>
                <li>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models <em>(Rating: 2)</em></li>
                <li>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning <em>(Rating: 1)</em></li>
                <li>Mining invariants from console logs for system problem detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5594",
    "paper_id": "paper-270675818",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "BERT_20M",
            "name_full": "bert-base-uncased applied to HDFS (20M) for anomaly detection",
            "brief_description": "This work applies the pretrained 'bert-base-uncased' model to detect anomalies in HDFS system-log sequences and to evaluate semantic preservation after a hierarchical-code based log compaction; embeddings and the model are used for anomaly detection and similarity analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "bert-base-uncased",
            "model_description": "BERT: a pretrained bidirectional Transformer encoder (the 'bert-base-uncased' checkpoint), pretrained on large English corpora; used here as the language-model backbone to produce embeddings and to perform anomaly-detection modeling on log sequences.",
            "model_size": null,
            "anomaly_detection_method": "Applied BERT as the anomaly-detection model on tokenized log lines (used embeddings and model inference/further modeling for classification of anomalous sequences); also used cosine similarity on BERT embeddings to measure semantic similarity between original and compacted logs. The paper implies model fine-tuning/inference on labeled anomaly data (standard precision/recall/F1 evaluation), though exact fine-tuning recipe is not fully detailed.",
            "data_type": "structured/semi-structured system logs (HDFS) treated as sequences of tokens/messages",
            "anomaly_type": "log-sequence anomalies / anomalous events in system logs (rare or abnormal log templates/sequences indicating system problems)",
            "dataset_name": "HDFS (two variants: HDFS_2k and HDFS_20M; the anomaly detection evaluation reported on HDFS_20M_HC)",
            "performance_metrics": "Precision 0.88, Recall 0.86, F1-Score 0.83 (reported for BERT_20M on the processed 20M HDFS log). Also reported cosine similarity between original and compacted embeddings: 0.9724.",
            "baseline_comparison": "Compared to PCA, Invariant Mining (IM), LogCluster, SVM, DeepLog, and LogRobust: BERT_20M achieved lower precision/F1 than top methods such as LogCluster and DeepLog (which report higher precision/recall/F1 in the paper), but outperformed SVM and PCA on some metrics. Specifically, some baselines reported precision of 1.00 (PCA, IM, LogCluster, DeepLog) and higher F1 (LogCluster 0.96, DeepLog 0.98) while BERT_20M had F1 0.83.",
            "limitations_or_failure_cases": "Did not achieve top F1/precision compared to some specialized methods (LogCluster, DeepLog). The paper's hierarchical-cataloging process can leave some tokens uncategorized (catalog coverage limits), relies on English WordNet nouns (limited to English noun vocabulary / single language), and compaction increases lexical density which raised mean per-batch processing time in some cases. The paper does not report AUROC or other thresholds, and exact fine-tuning details are not fully specified.",
            "uuid": "e5594.0",
            "source_info": {
                "paper_title": "Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLMs (general mention)",
            "name_full": "Large Language Models (LLMs) for anomaly detection (general)",
            "brief_description": "The paper references the emerging use of large pre-trained language models / transformer-based LLMs to detect anomalies in system logs by learning language patterns and semantics in log sequences.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large Language Models / Transformer-based LMs",
            "model_description": "General family: transformer-based pretrained LMs (bidirectional and autoregressive variants) trained on large corpora; able to encode semantic and contextual information from text and log sequences.",
            "model_size": null,
            "anomaly_detection_method": "Mentioned approaches include fine-tuning pretrained LMs on normal log data and using them to detect deviations in log sequences (self-supervised or supervised fine-tuning, representation learning and sequence modeling).",
            "data_type": "system logs treated as sequences / textual events",
            "anomaly_type": "semantic anomalies in log sequences, unusual patterns or rare/unseen events",
            "dataset_name": null,
            "performance_metrics": "",
            "baseline_comparison": "Discussed qualitatively as offering superior semantic understanding vs older approaches (e.g., RNN/LSTM, autoencoders) and as complementary to hierarchical compaction.",
            "limitations_or_failure_cases": "General limitations mentioned: resource and performance costs scale with token volume; necessity of pre-processing/log compaction to reduce cost; transferability and robustness require careful evaluation.",
            "uuid": "e5594.1",
            "source_info": {
                "paper_title": "Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT (Log Anomaly Detection via BERT)",
            "brief_description": "A cited prior work that proposes a self-supervised BERT-based framework for log-anomaly detection, extracting contextualized semantic representations of log events to detect anomalies.",
            "citation_title": "Log Anomaly Detection via BERT",
            "mention_or_use": "mention",
            "model_name": "LogBERT (BERT-based)",
            "model_description": "Self-supervised BERT-based framework adapted to log data to learn contextual representations of log events for anomaly detection.",
            "model_size": null,
            "anomaly_detection_method": "Self-supervised BERT-style training on logs to build representations used to flag anomalous log events or sequences (representation + downstream detection).",
            "data_type": "log sequences / event sequences",
            "anomaly_type": "anomalous log sequences / events",
            "dataset_name": null,
            "performance_metrics": "",
            "baseline_comparison": "",
            "limitations_or_failure_cases": "",
            "uuid": "e5594.2",
            "source_info": {
                "paper_title": "Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LogFiT",
            "name_full": "LogFiT",
            "brief_description": "A cited work (Almodovar et al.) that fine-tunes a BERT-based language model on normal log data to detect anomalies by recognizing linguistic patterns in logs.",
            "citation_title": "Log Anomaly Detection using Fine-Tuned Language Models",
            "mention_or_use": "mention",
            "model_name": "fine-tuned BERT (LogFiT)",
            "model_description": "BERT checkpoint fine-tuned on log corpora to model typical log language patterns for anomaly detection.",
            "model_size": null,
            "anomaly_detection_method": "Fine-tuning a pretrained language model on normal logs so deviations from learned patterns indicate anomalies (supervised or self-supervised fine-tuning).",
            "data_type": "system logs / log sequences",
            "anomaly_type": "semantic and sequence anomalies in logs",
            "dataset_name": null,
            "performance_metrics": "",
            "baseline_comparison": "",
            "limitations_or_failure_cases": "",
            "uuid": "e5594.3",
            "source_info": {
                "paper_title": "Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LogEvent2vec",
            "name_full": "LogEvent2vec",
            "brief_description": "A cited approach that maps log events to vector representations (event-to-vector) to enable anomaly detection on large-scale logs, using representation learning techniques.",
            "citation_title": "LogEvent2vec: LogEvent-to-Vector Based Anomaly Detection for Large-Scale Logs in Internet of Things",
            "mention_or_use": "mention",
            "model_name": "LogEvent2vec (representation learning)",
            "model_description": "A technique converting log events into vector embeddings for downstream anomaly detection; focuses on scalability for large logs.",
            "model_size": null,
            "anomaly_detection_method": "Representation learning to embed log events (not necessarily a large pretrained language model) and detect anomalies via vector-based methods.",
            "data_type": "large-scale logs / event sequences",
            "anomaly_type": "outliers or abnormal event embeddings / sequence anomalies",
            "dataset_name": null,
            "performance_metrics": "",
            "baseline_comparison": "",
            "limitations_or_failure_cases": "",
            "uuid": "e5594.4",
            "source_info": {
                "paper_title": "Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LogEncoder",
            "name_full": "LogEncoder (Log-based Contrastive Representation Learning)",
            "brief_description": "A cited work exploring contrastive representation learning for log events to improve anomaly-detection representations.",
            "citation_title": "Log-Based Contrastive Representation Learning for Anomaly Detection",
            "mention_or_use": "mention",
            "model_name": "LogEncoder (contrastive representation model)",
            "model_description": "Contrastive learning framework that produces semantic vectors of log events for anomaly detection, improving separability of normal vs anomalous patterns.",
            "model_size": null,
            "anomaly_detection_method": "Contrastive representation learning on logs to produce embeddings used for anomaly detection (unsupervised or self-supervised).",
            "data_type": "log sequences / structured log templates",
            "anomaly_type": "semantic/sequence anomalies",
            "dataset_name": null,
            "performance_metrics": "",
            "baseline_comparison": "",
            "limitations_or_failure_cases": "",
            "uuid": "e5594.5",
            "source_info": {
                "paper_title": "Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Robust/Transferable LMs (Ott et al.)",
            "name_full": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
            "brief_description": "A cited study that emphasizes robustness and transferability of pre-trained language-model-based anomaly detectors across environments and datasets.",
            "citation_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
            "mention_or_use": "mention",
            "model_name": "pre-trained language models (general)",
            "model_description": "Study focuses on applying pretrained LMs for log anomaly detection, analyzing robustness/transferability across datasets and settings.",
            "model_size": null,
            "anomaly_detection_method": "Using pretrained language models (fine-tuning or feature extraction) for anomaly detection, with an emphasis on transferability/robustness evaluation.",
            "data_type": "log sequences / event sequences",
            "anomaly_type": "anomalous log sequences; cross-dataset anomalies",
            "dataset_name": null,
            "performance_metrics": "",
            "baseline_comparison": "",
            "limitations_or_failure_cases": "",
            "uuid": "e5594.6",
            "source_info": {
                "paper_title": "Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Log Anomaly Detection via BERT",
            "rating": 2,
            "sanitized_title": "log_anomaly_detection_via_bert"
        },
        {
            "paper_title": "Log Anomaly Detection using Fine-Tuned Language Models",
            "rating": 2,
            "sanitized_title": "log_anomaly_detection_using_finetuned_language_models"
        },
        {
            "paper_title": "Log-Based Contrastive Representation Learning for Anomaly Detection",
            "rating": 2,
            "sanitized_title": "logbased_contrastive_representation_learning_for_anomaly_detection"
        },
        {
            "paper_title": "LogEvent2vec: LogEvent-to-Vector Based Anomaly Detection for Large-Scale Logs in Internet of Things",
            "rating": 2,
            "sanitized_title": "logevent2vec_logeventtovector_based_anomaly_detection_for_largescale_logs_in_internet_of_things"
        },
        {
            "paper_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
            "rating": 2,
            "sanitized_title": "robust_and_transferable_anomaly_detection_in_log_data_using_pretrained_language_models"
        },
        {
            "paper_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "rating": 1,
            "sanitized_title": "deeplog_anomaly_detection_and_diagnosis_from_system_logs_through_deep_learning"
        },
        {
            "paper_title": "Mining invariants from console logs for system problem detection",
            "rating": 1,
            "sanitized_title": "mining_invariants_from_console_logs_for_system_problem_detection"
        }
    ],
    "cost": 0.010515,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model
21 June 2024</p>
<p>Sohag Kabir 
Department of Computer Science and Information Technologies
University of A Coruña
15071 ACoruñaSpain</p>
<p>Ibrahim Ghafir 
Department of Computer Science and Information Technologies
University of A Coruña
15071 ACoruñaSpain</p>
<p>Clara Corbelle c.corbelle@udc.es 
Department of Computer Science and Information Technologies
University of A Coruña
15071 ACoruñaSpain</p>
<p>Victor Carneiro victor.carneiro@udc.es 
Department of Computer Science and Information Technologies
University of A Coruña
15071 ACoruñaSpain</p>
<p>Fidel Cacheda fidel.cacheda@udc.es 
Department of Computer Science and Information Technologies
University of A Coruña
15071 ACoruñaSpain</p>
<p>Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a BERT Model
21 June 2024A1BD760518E3C65881B2B27BFD2BD1BE10.3390/app14135388Received: 16 May 2024 Revised: 7 June 2024 Accepted: 18 June 2024Corbelle, C.Carneiro, V.Cacheda, F. Semantic Hierarchical Classification Applied to Anomaly Detection Using System Logs with a system logsanomaly detectionBERT modelhierarchical codessemantic similarity
The compaction and structuring of system logs facilitate and expedite anomaly and cyberattack detection processes using machine-learning techniques, while simultaneously reducing alert fatigue caused by false positives.In this work, we implemented an innovative algorithm that employs hierarchical codes based on the semantics of natural language, enabling the generation of a significantly reduced log that preserves the semantics of the original.This method uses codes that reflect the specificity of the topic and its position within a higher hierarchical structure.By applying this catalog to the analysis of logs from the Hadoop Distributed File System (HDFS), we achieved a concise summary with non-repetitive themes, significantly speeding up log analysis and resulting in a substantial reduction in log size while maintaining high semantic similarity.The resulting log has been validated for anomaly detection using the "bert-base-uncased" model and compared with six other methods: PCA, IM, LogCluster, SVM, DeepLog, and LogRobust.The reduced log achieved very similar values in precision, recall, and F1-score metrics, but drastically reduced processing time.</p>
<p>Introduction</p>
<p>The growth of communication networks and computer systems has led to an increase in cyberattacks and consequently the number of incidents recorded through the information residing in their logs [1].Peripheral nodes where events occur send their logs to central nodes responsible for analyzing them and detecting anomalies that may cause unexpected system behavior.The exponential increase in log size slows the process, increases the number of false positives, and consequently generates alert fatigue when cybersecurity professionals are overwhelmed by a high volume of security alerts, reducing their ability to react and detect anomalies [2].Logs generated by a system often have a high number of repeated or similar messages.</p>
<p>Modern anomaly detection systems based on logs utilize various artificial intelligence techniques, the most commonly used being those based on machine-learning algorithms [3] or natural language processing like Large Language Models [4].These techniques consist of three phases: log parsing, feature extraction, and anomaly detection using a trained model.In log parsing, the log is converted from its semi-structured format into a format that can be analyzed by the machine.With feature extraction, the goal is to extract the most important information from the data.The third phase, model execution, has resource needs and performance proportional to the volume or number of tokens processed.</p>
<p>The literature includes various works that have addressed the task of log compaction, and, as in our case, have encountered various challenges.Logs can have different origins, which implies a certain diversity of words in the messages, complicating their analysis [5].There are no dictionaries that associate all the words in a log's messages with their natural language meanings so that this log's semantics can be automatically obtained and used in its analysis [6].The hierarchies of themes currently used refer to very restricted thematic areas and have few levels in their hierarchical tree [7,8].Nor is there a numerical encoding of the themes of the words that simply stores complete semantic information of each word of the log and its messages [9].</p>
<p>Our work processes a system log and converts it into a much smaller log with the same semantics but without repeated messages.In this way, we significantly reduce the log size, automatically decreasing the time and resources needed for its processing with an AI model, as well as reducing the number of false positives.</p>
<p>To carry out this task, we implemented an algorithm that develops a hierarchical classification of the themes of the words in a log and assigns each theme a numerical code, called a hierarchical code.Each of these codes represents the theme of a node in the tree and the themes of its superior nodes.A hierarchical code is formed by the concatenation of digits that identify each of the nodes in the branch of the tree that descends from the root node to the node in question.Thus, the coding performed allows a code to reflect the specificity of its theme in the hierarchy.The constructed tree has more than 15 levels and contains all names in English, which facilitates its applicability to various logs.From the themes of the hierarchical tree and their codes, we generate a catalog of words ordered alphabetically, each associated with its possible hierarchical codes, also considering the existence of synonyms.We automatically use the catalog to obtain the themes of all the messages from a system log and then compare and analyze the themes of different messages of the log among themselves to obtain a concise summary with non-repeated themes.</p>
<p>The rest of this article is organized as follows.Section 2 introduces previous work related to our proposal.The algorithm we developed is shown in Section 3, where we explain the thematic classifier tree, the themes and codes catalog we created, and the log analysis using hierarchical codes.Next, in Section 4, we present the experimentation performed, describing the logs used, their evaluation, and the analysis of results.Finally, Section 5 summarizes the conclusions and possible future work.</p>
<p>Related Work</p>
<p>Hierarchical thematic classification in system logs offers a structured and efficient methodology for reducing data size and improving anomaly detection by considering the hierarchical structure of themes and the underlying semantic relationships in system logs.Hierarchical thematic classification in a system log can be an effective strategy for reducing its size and preparing it for fine-tuning Large Language Models (LLM) in anomaly detection, which are starting to yield promising results [4].This technique involves organizing themes in a hierarchical structure that reflects the relationship between them, which can facilitate the identification of patterns and more precise anomaly detection [10,11].</p>
<p>LLMs have emerged as powerful tools in the field of cybersecurity, particularly for anomaly detection.By leveraging extensive training on diverse datasets, LLMs can understand and generate human-like text, enabling them to identify unusual patterns and potential threats within network traffic and system logs [12].</p>
<p>By applying hierarchical classification to system logs, thematic trees can be used to analyze and categorize log messages more efficiently [13].This methodology can be especially useful when dealing with large datasets as it allows for a more compact and structured representation of the information contained in system logs [14].</p>
<p>Moreover, the use of hierarchical classification models can improve the accuracy of anomaly detection by considering the latent structure of semantic relationships in the data [15,16].These approaches allow for a more effective capture of contextual and thematic information in system logs, which can be crucial for accurately identifying anomalous behavior [17,18].</p>
<p>Log analysis for anomaly detection has seen significant advances with the integration of BERT (Bidirectional Encoder Representations from Transformers) models.BERT is a cutting-edge natural language processing model developed by Google using a bidirectional transformer architecture to understand the context of words from both directions, enabling deeper language comprehension [19].Pre-trained on a large corpus, it can be fine-tuned for specific tasks.Wang et al. [20] introduced a method that combines BERT and Variational AutoEncoders (VAE) to extract semantic and statistical features from log sequences for anomaly detection.Similarly, Almodovar et al. [21] proposed LogFiT, which fine-tunes a BERT-based language model to recognize linguistic patterns in normal log data.Guo et al. [22] further improved anomaly detection with LogBERT, a self-supervised framework based on BERT, addressing the limitations of RNN-based models.Additionally, recent studies like LogEvent2vec by Wang et al. [23] and LogEncoder by Qi et al. [24] have explored contrastive representation learning based on logs and vector transformations for anomaly detection, respectively.These approaches leverage the power of BERT to enhance the accuracy of anomaly detection.Furthermore, Lv et al. [25] proposed ConAnomaly, a model that uses log sequence encoders and LSTM networks for anomaly detection, demonstrating the versatility of log analysis techniques.</p>
<p>Fält et al.'s research [3] emphasizes the importance of learning normal system behavior for accurate anomaly detection, aligning with the methodology of leveraging BERT models to better understand log sequences.Moreover, Ott et al. [26] highlighted the robustness and transferability of anomaly detection models using pre-trained language models, which is crucial for real-world applications.</p>
<p>Our work can be considered complementary to previous studies, as it can be applied to any log analysis technique to enhance performance, particularly in terms of the response time as it will significantly reduce the log size.To validate the semantic similarity of the log records generated by our algorithm in anomaly detection, we focus on the use of Large Language Models (LLMs) because the introduction of the Transformers model [27], which laid the groundwork for LLMs, provides superior performance compared to other models, such as those based on neural networks like LSTM (Long Short-Term Memory) or autoencoders.</p>
<p>Hierarchical Classification</p>
<p>This work uses a thematic hierarchical classifier tree and a thematic catalog in which each theme is associated with one or more numeric codes deduced from the hierarchical tree.The themes in the catalog were represented by words that can exist in the messages; specifically, in this work, we focused on English nouns appearing in WordNet [28].For the system log analysis process, the words of each message were read, searched for in the catalog, and their thematic codes were extracted.The analysis of these hierarchical codes allowed obtaining a semantically equivalent but much more compact log.</p>
<p>In the initial phase, a thematic classification tree is constructed, and a catalog of hierarchical codes is created with the themes and their relationships.Subsequently, an algorithm is developed that processes the original log to identify similar or duplicated themes, thereby compacting the input records while maintaining high semantic similarity.</p>
<p>The steps of this algorithm are detailed below, whose novelty lies in the use of a catalog of hierarchical codes that reflect both their thematic specificity and their position within the higher hierarchy.Unlike traditional methods that simply reduce redundancy, our algorithm preserves the semantics of the logs using a hierarchical coding system based on natural language semantics.This ensures that essential information is not lost during the compaction process.Structuring and compacting logs more effectively significantly reduces fatigue from false alerts, enhancing the efficiency of the cyberattack and anomaly detection process.</p>
<p>Thematic Classifier Tree</p>
<p>The first step was to build a thematic classifier tree in which each node represents a specialization of its parent node, forming a structure where descendant nodes encompass subtopics of their predecessors.Each node is labeled with a code that increases in length as it advances toward more specific nodes.</p>
<p>To define the themes of the hierarchy, we used the WordNet database, specifically the index.posand data.posfiles (where "pos" indicates the grammatical category: noun, verb, adjective, or adverb).For example, index.nouncontains an alphabetical list of nouns, each followed by offset codes representing different meanings.In data.noun,these codes, ordered incrementally, identify a set of synonyms for each meaning with a description at the end.Each entry also indicates the code of its hypernym (@) and hyponyms (~), facilitating the deduction of the thematic hierarchy.In our prototype, each word or group of words was treated as a unique theme where the offset code at the beginning of each entry in data.nounindicates the byte offset from the beginning of the file to the corresponding set of synonyms.</p>
<p>Catalog of Words from a Message</p>
<p>The catalog we created is a dictionary of words to which we associated the codes of the most specific themes they belong to, as shown in Figure 1.To generate the hierarchical codes, our algorithm reads the data.nounfile starting from the root node and descending to each child or hyponym successively, creating in this process the hierarchical codes assigned to each node.For each level descended, more digits are added to the hierarchical code being created.As a result, a file is obtained that associates each hierarchical code with an offset code.</p>
<p>Figure 2 shows a fragment of the generated "codes" file where the offset code appears in the first column and the hierarchical code in the second column.The algorithm creates the hierarchical codes using character strings grouped into blocks of three digits (from 000 to 999), allowing up to 1000 possibilities per node, reflecting a theoretical maximum of 1000 children per theme.For example, Node Theme 0.1.2.2 has the hierarchical code 000001002002, indicating its position within Theme 0.1.2,which in turn has the code 00000002.The root node is identified with the code 000, simplifying the representation of the thematic tree in data_noun and facilitating future expansions.</p>
<p>The hierarchical codes we create are not a number of bytes, like the offset codes, but rather codes that represent the meaning of a word and all its hypernyms in the thematic hierarchy, which will be useful in thematic analysis.By looking at Figure 2, we can deduce that the more digits two hierarchical codes have in common starting from the left, the more similar the themes they represent are, although the length of both codes must also be considered.</p>
<p>In the process illustrated in Figure 3, the "generatewords" algorithm transforms the index.nounfile into a new file called namesjer.In this new file, the original offset codes of index.nounare replaced by hierarchical codes.Thus, each line of the namesjer file begins with a word followed by hierarchical codes representing the themes associated with that word, forming a structured catalog of themes and their corresponding hierarchical codes.</p>
<p>Log Analysis Using Thematic Hierarchical Codes</p>
<p>The next phase involved analyzing log messages to identify common themes and eliminate duplicates.The process consisted of searching the words of each message in the catalog, extracting their hierarchical codes, and comparing them between messages.Codes that shared a common prefix up to a certain threshold indicated a similar theme.This threshold was defined as a percentage of matching digits of the shortest code, allowing the determination of thematic specificity and relationship between messages.</p>
<p>If we consider L as the set of all lines in the log file, each line l i is a vector of words or tokens w i = w i1 w i2 . . .w in , and w ij is the j-th word in the i-th line, and n i is the number of words in line i.We have a dictionary D that associates words w with codes c represented as a mapping D : W → C, where W is the set of all possible words and C is the set of all possible codes.The transformation of each line l i is performed using the substitution function f , which is applied to each word w ij in w i .</p>
<p>The function f (w) is defined as:
f (w) = D(w) if w ∈ dom(D) ∅ if w / ∈ dom(D)
where dom(D) represents the domain of definition of the dictionary D, i.e., the set of words or tokens for which D has an associated code.We apply f to each word in w i to obtain a new vector of codes c i = f (w i1 ) f (w i2 ) . . .f (w in ), and we apply g to each line l i of the original file, consisting of the vector w i , to obtain a new output file:
g(w i ) = w i c i = v i
The transformed log file is represented by a new set of vectors v i , where each line contains the vector w i from the original file followed by the vector c i resulting from applying the transformation to each line l i of the original file.</p>
<p>We can describe the complete transformation process of the log file as follows:
v i = w i 1w i 2 . . . w i n f (w i 1) f (w i 2) . . . f (w i n) ∀ w i ∈ L
To compensate for differences between the base dictionary and the words used in the logs, we adapted the words in the messages to identify themes rather than exact matches.The adaptations included treating the words separated by dots or that start with uppercase letters as different, even if they are joined.All uppercase letters were transformed to lowercase, and substrings such as URLs, domains, and multimedia file extensions were removed.Digits, control characters, punctuation marks (except the apostrophe that appears in the base dictionary), and functional words stored in a file called 'paldescart' were also discarded.</p>
<p>Additionally, accents were removed to align with the base dictionary, which lacks accented words.If the hierarchical codes of a word with "-ing" or "-ed" endings were not found, they were replaced by "-ion", and if still not found, alternatives were tried by removing the last, penultimate, or antepenultimate character.If no matches were found, it was checked if the word was in plural, consulting the 'plurirreg' file for irregular plurals and adjusting the ending as necessary to search for the singular corresponding hierarchical code.</p>
<p>Obtaining the Summary Log</p>
<p>The 'datacod' file contains rows that include both the original log message and the hierarchical codes that reflect its meaning.We implemented an algorithm that processes this file to obtain a much more compact log than the original, representing each template of the original log with a single message.The process is divided into three main modules:</p>
<p>First, the log entry is stripped of rows with repeated hierarchical codes.Each log row includes at the end the hierarchical codes of the message that encapsulate the themes of that message.By removing messages with duplicate themes and keeping only one per theme, templates and their original semantics are preserved, significantly reducing the log size and increasing its efficiency.</p>
<p>Let L be the set of all lines in the 'datacod' file, where each line l i is an element of L, i.e., l i ∈ L. Each line l i is a vector v i = (w i c i ), where w i = (w i1 , w i2 , . . ., w in ) and c i = (c i1 , c i2 , . . ., c im ), with w ix being a token or word and c ix a hierarchical code.Let n i be the number of lines with identical hierarchical codes up to line i, i.e.:
n i = i ∑ j=1 1 | c i = c j
The transformation of the 'datacod' file to one without lines with identical codes is performed using the function f , which is applied to each vector v i such that:
f (v i ) = w i c i if n i = 1 ∅ if n i &gt; 1
The resulting log file is reduced to a set of vectors {v i }, where each v i is the result of applying the function f to each line l i of the original file.We can describe the complete process on the 'datacod' file as:
{v i |v i = f (v i ) ∀ v i ∈ L}.
In the second step, rows with similar meaning codes are removed from the resulting log.To determine which rows are similar, the themes of all messages are compared using a specificity threshold, which is a percentage of the shortest code's length among the two codes to be compared.Another threshold is set to determine the percentage of codes of the same theme needed to consider that two messages belong to similar themes.</p>
<p>Finally, the last step starts from the log resulting from the previous module and removes all hierarchical codes, leaving only the messages.The final log presents one message per existing template in the original log, simplifying the representation and improving the system's efficiency.</p>
<p>Experimental Analysis</p>
<p>In this phase, we evaluated the performance of the original log files and those processed with the algorithms described in the previous section.To this end, we used the BERT model (Bidirectional Encoder Representations from Transformers) [29].Bidirectional Encoder Representations from Transformers (BERT) is based on a deep bidirectional architecture to understand the context of a word based on its surrounding words, leading to superior performance in various NLP tasks.This model pre-trains on vast amounts of text data to capture linguistic nuances, which are then fine-tuned for specific applications.BERT, with its advanced contextual understanding and ability to process large volumes of textual data, presents a promising solution for enhancing anomaly detection using system logs.</p>
<p>The specific checkpoint called 'bert-base-uncased' refers to a model configuration pre-trained using a vast corpus of English text without distinguishing between uppercase and lowercase letters.</p>
<p>For the experimental evaluation of our system, we implemented a set of tests using distributed file system logs (HDFS).For the analysis, we selected two log files: one containing a smaller set of 2000 messages and the other significantly larger with 149,477 messages and an approximate size of 20.4 MB.These data volumes were chosen to provide a clear perspective on the system's performance under different processing loads.</p>
<p>The format of the HDFS logs we handled follows a standardized schema that facilitates automatic analysis.Each log entry includes several comma-delimited fields: the date and time of the event (including minutes and seconds), a unique identifier for the process that generated the message, the message's severity level, the system component the message refers to, and the textual content of the message itself.This structured format is essential for the proper functioning of our program, as it allows efficient segmentation and categorization of information for subsequent processing and analysis.</p>
<p>Log Evaluation</p>
<p>For the project experimentation, we implemented software that calculated certain statistical parameters of the original HDFS logs, the 2 K messages one, and the 20 MB one, as well as the two logs processed with the semantic compaction algorithms developed in this work.</p>
<p>The results for the 2 K log are shown in Table 1.Analyzing the original 2 K log (HDFS_2k) and its corresponding processed log (HDFS_2k_HC), we can state that the total number of lines decreases by 99.20%.Similarly, the total number of tokens in the entire log decreases by 99.25%.The total number of hierarchical codes representing the themes of each log also decreases by 99.29%.However, the number of different hierarchical codes in the log remains the same in both the original and the processed logs (147).This allows us to conclude that the reduction in the file size and its elements has not affected its semantics and that the processed log retains the same themes as the original log.</p>
<p>Additionally, we focused on the distribution of rows in the original log that repeat in themes and correspond to a single row in the processed log.In the original log, there are repeated rows in 1984 cases, but there are no repeated rows in the processed log.This demonstrates that the processed log retains all the meaning with the minimum size.We performed the same statistical tests again for the original 20 MB log (HDFS_20M) and its processed log (HDFS_20M_HC) in Table 2, and the conclusions were similar.In this case, the compaction is greater, as both the number of lines of the original log, the tokens, and the total number of hierarchical codes are reduced by 99.99%, maintaining the number of different hierarchical codes.The total number of repeated rows in the processed file is also zero, as was the case with the original 2 K log. Figure 4 shows a comparison that considers the number of tokens per line and the number of hierarchical codes per line for the original 2 K log.It is observed that for each line, the number of codes is higher than the number of tokens.This happens because each token can have one or several hierarchical codes since its meaning responds to more than one theme in the classifier tree due to the possibility of synonyms.It can be seen in the figure that generally, the more tokens a line has, the more hierarchical codes it will have.A similar comparison was made with the original 20 MB log, as shown in Figure 5, and the results are similar to those of the smaller original log, i.e., the number of codes per line is higher than the number of tokens, and this is maintained with some uniformity throughout the log.It should be noted that some tokens might not have an associated hierarchical code in the catalog.We say that tokens are categorized if they have associated hierarchical codes.Therefore, we also counted the number of categorized tokens per line, as shown in Figure 6 for the processed 2 K log and in Figure 7 for the processed 20 MB log.The proportion of categorized tokens to all tokens is significant, allowing the processed file to retain the necessary semantics of the log.With fewer rows, the processed logs show less uniformity in the relationship between tokens per line and codes per line, which would relate to greater thematic diversity in the lines, as there are no repeated rows in the processed logs.</p>
<p>It should be noted that some tokens might not have an associated hierarchical code in the catalog.We say that tokens are categorized if they have associated hierarchical codes.Therefore, we also counted the number of categorized tokens per line, as shown in Figure 6 for the processed 2 K log and in Figure 7 for the processed 20 MB log.The proportion of categorized tokens to all tokens is significant, allowing the processed file to retain the necessary semantics of the log.With fewer rows, the processed logs show less uniformity in the relationship between tokens per line and codes per line, which would relate to greater thematic diversity in the lines, as there are no repeated rows in the processed logs.For log processing, we used the 'transformers' and 'datasets' libraries available in Python, using the PyTorch library for optimization and efficiency, a widely used opensource machine learning library for deep learning applications.</p>
<p>First, with a batch size of 8, we iterate through the log file lines, obtaining the following tokenization statistics:  It should be noted that some tokens might not have an associated hierarchical code in the catalog.We say that tokens are categorized if they have associated hierarchical codes.Therefore, we also counted the number of categorized tokens per line, as shown in Figure 6 for the processed 2 K log and in Figure 7 for the processed 20 MB log.The proportion of categorized tokens to all tokens is significant, allowing the processed file to retain the necessary semantics of the log.With fewer rows, the processed logs show less uniformity in the relationship between tokens per line and codes per line, which would relate to greater thematic diversity in the lines, as there are no repeated rows in the processed logs.For log processing, we used the 'transformers' and 'datasets' libraries available in Python, using the PyTorch library for optimization and efficiency, a widely used opensource machine learning library for deep learning applications.</p>
<p>First, with a batch size of 8, we iterate through the log file lines, obtaining the following tokenization statistics:  For log processing, we used the 'transformers' and 'datasets' libraries available in Python, using the PyTorch library for optimization and efficiency, a widely used opensource machine learning library for deep learning applications.</p>
<p>First, with a batch size of 8, we iterate through the log file lines, obtaining the following tokenization statistics: For each log line, we store the processed tokens per line and the processing time for each line.</p>
<p>Table 3 presents a quantitative analysis of four datasets derived from the Hadoop Distributed File System (HDFS), differentiated by their data volume.The suffix "HC" refers to the log version processed with the hierarchical classification method described earlier.</p>
<p>The "totalLines" attribute refers to the total number of lines per dataset, with "HDFS_2k" and "HDFS_2k_HC" containing 2000 and 16 lines, respectively, and "HDFS_20M" and "HDFS_20M_HC" containing 14,947,770 and 22 lines, respectively.The "tokensCount" indicates the total number of tokens, being substantially higher in the larger datasets.The mean ("tokensMeanBatch") and standard deviation ("tokensStdBatch") of tokens per batch provide a measure of centrality and dispersion, respectively, in the distribution of the number of tokens per batch within each dataset.Notably, the "HC" subsets show a higher mean of tokens per batch, indicating higher lexical density or information concentration.Table 4 compares the performance metrics of two main datasets "HDFS_2k" and "HDFS_20M" along with their corresponding "HC" subsets.The "lineCount" metric indicates the number of lines in each dataset, providing context for interpreting the subsequent metrics."Tokenized Time" indicates the time in seconds required to tokenize the datasets, with the "HC" subsets requiring less time due to their smaller size."Modelized Time" denotes the time needed to process the 'bert-base-uncased' model, with the "HDFS_20M" set requiring significantly more time, consistent with its larger size.Finally, "Mean Batch Time" represents the average time taken per batch, which remains relatively constant across all cases, suggesting that the time per batch is independent of the dataset's total size and that the "HC" subsets have characteristics that require more processing time per batch, such as greater data complexity.</p>
<p>Analysis of Results</p>
<p>As seen in the previous section and as might be evident, reducing the log size significantly improves its processing.When evaluating how similar the original and processed logs are using hierarchical classification, we focus on creating a similarity matrix between the lines of each analyzed set, both original and processed, and between them.</p>
<p>Figure 8 shows two heatmaps of similarity matrices for HDFS log records.In the graph on the left, the matrix for the "HDFS_2k vs. HDFS_2k" dataset shows uniformity in color, suggesting high similarity between the log lines, implying little variation between them, as expected in unprocessed log records where repetitive messages are common.On the other hand, the graph on the right shows the matrix "HDFS_2k_HC vs. HDFS_2k_HC" corresponding to a processed dataset of high cardinality.The more varied colors in this matrix indicate more significant differences in similarity between the log lines, suggesting that processing has highlighted or preserved the most unique lines or anomalies, leading to greater diversity in the log content.The presence of dark red blocks indicates pairs of lines with high similarity, while lighter colors represent lower similarity.This reflects the typical purpose of a high-cardinality process, which is to identify and isolate the less frequent or more distinct events within a large dataset.The predominance of red throughout the map suggests that despite the processing, there is notable consistency or significant similarity between the corresponding lines of both logs.The lack of distinct blue blocks also indicates that there are no lines between the datasets that are entirely different.The processing aimed to preserve essential information while reducing redundancy; this heatmap suggests that a considerable degree of the original information has been retained in the processed version.</p>
<p>To further compare the similarity between the original and processed log, we used the cosine similarity function from the PyTorch deep learning library to calculate the similarity between two sets of embeddings.The torch.nn.functional .cosine_similarityfunction calculates the cosine similarity between two tensors, which in this context are embeddings corresponding to the two tokenized log files.Cosine similarity measures the cosine of the angle between two vectors in a multidimensional space, providing an indicator of their relative orientation but not their magnitude.</p>
<p>Cosine similarity between two vectors is calculated using the following formula:
cosine similarity(A, B) = A • B</p>
<p>∥A∥∥B∥</p>
<p>In the case of vectors with only positive components, as is common in text representations and many other applications, the cosine similarity will be between 0 and 1.</p>
<p>Using cosine similarity between the two datasets, we obtain a value of 0.9724, indicating that the vectors represented in these tensors are very similar in terms of their orientation in vector space.In other words, the embeddings of both tensors have very similar directions, even if their magnitudes (i.e., the length of the vectors) might be different.</p>
<p>Since cosine similarity focuses on orientation and not magnitude, this high value suggests that the data represented by these embeddings share many features or patterns in the context being modeled by the vector space.Given that these embeddings result from running the 'bert-base-uncased' model on both the original and processed log files, it can be inferred that they share a similar context or semantic meaning.</p>
<p>In the previous section, we verified the significant reduction in processing time of the 2 K and 20 M logs, achieving reductions of 760.8% and 995.8%, respectively, in tokenization time, and 989.5% and 999.8%, respectively, in BERT model training time.</p>
<p>Finally, we analyzed the results of applying the BERT model to the processed 20 M log (HDFS_20M_HC) for anomaly detection, comparing it with other methods present in the literature.These methods are briefly described below:</p>
<p>• PCA [13]: this work converts logs into vectors and then uses the Principal Component Analysis (PCA) algorithm to divide them into normal space and anomaly space.</p>
<p>•</p>
<p>Invariant Mining [30]: this paper proposes an Invariant Mining (IM) method to extract invariants from log vectors, which are labeled as anomalous logs.</p>
<p>•</p>
<p>LogCluster [31]: LogCluster proposes an approach that clusters logs for easier identification of issues based on logs and uses a knowledge base to reduce redundancy by pre-examining log sequences.• SVM [32]: this work represents log sequences as vectors and uses a Support Vector Machine (SVM) as its classification algorithm.• DeepLog [33]: DeepLog is a deep neural network model that uses LSTM to model a system log as a natural language sequence.• LogRobust [34]: LogRobust extracts semantic information from log events and represents them as semantic vectors.It uses a Bi-LSTM attention-based model to detect anomalous log sequences.</p>
<p>To carry out this comparison, we used traditional evaluation metrics: precision, recall, and F1-Score.</p>
<p>As shown in Table 5, our model (BERT_20M) has a precision of 88%, a recall of 86%, and an F1-Score of 83%.Comparing these results with those obtained by other methods, we observe that BERT_20M has a precision of 88%, which is lower than the best methods such as PCA, IM, LogCluster, and DeepLog, which reach 100% precision.However, it outperforms SVM, which has the worst precision with 62%.In terms of recall, BERT_20M obtains 86%, which is lower than the recall of LogCluster (96%) and LogRobust (91%), but significantly better than PCA, which has the worst recall with 67%.Regarding the F1-Score, BERT_20M achieves a value of 0.83.Although it is lower than LogCluster (0.96) and DeepLog (0.98), it surpasses PCA (0.70), which has the lowest F1-Score.</p>
<p>Although the BERT_20M model does not achieve the best metrics compared to Log-Cluster and DeepLog, it offers considerably better performance than PCA and SVM.</p>
<p>Conclusions and Future Research</p>
<p>In this work, we initially transcribed the thematic contents inherent to system logs using a numerical coding scheme that assigns unique identifiers to each topic, modeling thematic specificity and its contextual interconnections.Subsequently, we performed an extraction and analytical synthesis operation on the thematic vectors corresponding to the entries in an HDFS system log, allowing the obtainment of a much-condensed log, reducing the number of lines, tokens, and thematic codes by 99%, and maintaining semantic similarity.</p>
<p>The flexibility of the methodological approach suggests a cross-sectional adaptability for its application in HDFS log files and other computer system architectures.The expansion of this prototype to a thematic plurality could be carried out by integrating Wordnet World instead of WordNet, which would expand the lexical index's linguistic spectrum to multiple languages.</p>
<p>A comparative study of the pre-and post-processing datasets was conducted using hierarchical thematic classification models.The intrinsic similarity preservation of the generated embeddings was evidenced, demonstrating a high cosine similarity coefficient (0.9724), thus corroborating the retention of a shared core of meaning and semantic context between the two data instances.</p>
<p>This methodology was subjected to an empirical analysis of its effectiveness and efficiency using a 'bert-base-uncased' model.The quantitative evaluation focused on traditional precision, recall, and F1-score metrics, demonstrating that the application of this compaction technique not only maintains semantic similarity but also allows anomaly detection similar to other methods used in the literature.</p>
<p>In conclusion, the implementation of hierarchical thematic classification in system log collection systems is revealed as an efficient vector for data compression without compromising semantic integrity.This compression proves to be a key component in monitoring infrastructure, optimizing both alert management in Security Information and Event Management (SIEM) systems and anomaly detection through deep learning language models (LLMs), mitigating operational overload and alert fatigue.</p>
<p>Figure 1 .
1
Figure 1.Generation of hierarchical codes.</p>
<p>Figure 2 .
2
Figure 2. Fragment of the codes file.</p>
<p>Figure 3 .
3
Figure 3. Creation of the world catalog and hierarchical codes.</p>
<p>Figure 4 .
4
Figure 4. Statistics of original 2 K log.</p>
<p>Figure 5 .
5
Figure 5. Statistics of original 20 MB log.</p>
<p>Figure 6 .
6
Figure 6.Statistics of processed 2 K log.</p>
<p>Figure 7 .
7
Figure 7. Statistics of processed 20 MB log.</p>
<p>'lineCount' : Number of lines in the dataset 'batchSize' : Number of lines in each batch 'tokensCount' : Total number of tokens in the dataset 'tokensMeanBatch': Mean number of tokens in each batch</p>
<p>Figure 6 .
6
Figure 6.Statistics of processed 2 K log.</p>
<p>Figure 6 .
6
Figure 6.Statistics of processed 2 K log.</p>
<p>Figure 7 .
7
Figure 7. Statistics of processed 20 MB log.</p>
<p>'lineCount' : Number of lines in the dataset 'batchSize' : Number of lines in each batch 'tokensCount' : Total number of tokens in the dataset 'tokensMeanBatch': Mean number of tokens in each batch</p>
<p>Figure 7 .
7
Figure 7. Statistics of processed 20 MB log.</p>
<p>'lineCount' : Number of lines in the dataset 'batchSize' : Number of lines in each batch 'tokensCount' : Total number of tokens in the dataset 'tokensMeanBatch': Mean number of tokens in each batch 'tokensStdBatch' : Standard deviation of tokens between batches 'tokenizedTime' : Total tokenization time of the dataset 'modelizedTime' : Total modeling time of the dataset 'meanBatchTime' : Mean execution time of a batch 'tokensBatchList': List with the number of tokens in each batch 'timeBatchList' : List with the processing time per batch }</p>
<p>Figure 8 .
8
Figure 8. Internal similarity of original and processed log.</p>
<p>Figure 9
9
Figure9presents a heatmap showing a range of similarities between the lines of the original "HDFS_2k" dataset and its processed version "HDFS_2k_HC".The dominant red tones indicate high similarity, while blue represents lower similarity.</p>
<p>Figure 9 .
9
Figure 9. Similarity between the original and processed log.</p>
<p>Table 1 .
1
Statistics of 2 K Log.
LogTotal LinesTotal Num TokensTotal CodesTotal Codes DifTotal RepetitionHDFS_2k HDFS_2k_HC2000 1614,890 11164,065 456147 1471984 0</p>
<p>Table 2 .
2
Statistics of 20 MB log.
LogTotal LinesTotal Num TokensTotal CodesTotal Codes DifTotal RepetitionHDFS_20M HDFS_20M_HC149,477 221,147,745 1884,894,020 732213 213149,455 0</p>
<p>Table 3 .
3
Tokens per line.
LogTotal LinesTokens CountTokens Mean BatchTokens Std BatchHDFS_2k HDFS_2k_HC HDFS_20M HDFS_20M_HC2000 16 149,477 22152,960 1344 11,043,195 1944611 672 591 648234 16 54 28</p>
<p>Table 4 .
4
Processing times per line.
LogTotal LinesTokenized TimeModelized TimeMean Batch TimeHDFS_2k HDFS_2k_HC HDFS_20M HDFS_20M_HC2000 16 149,477 220.8816 0.2109 50.0624 0.208654.0701 0.5704 4039.0378 0.83970.2156 0.2842 0.2155 0.2791</p>
<p>Table 5 .
5
Evaluation of HDFS_20M_HC log.
ModelPrecisionRecallF1-ScorePCA IM LogCluster SVM DeepLog LogRobust BERT_20M1.00 1.00 1.00 0.62 1.00 0.81 0.880.67 0.83 0.96 0.73 0.78 0.91 0.860.70 0.80 0.96 0.73 0.98 0.91 0.83
Funding: This research received no external funding.Institutional Review Board Statement: Not applicable.Informed Consent Statement: Not applicable.Data Availability Statement:The raw data supporting the conclusions of this article will be made available by the authors on request.Conflicts of Interest:The authors declare no conflicts of interest.
T S Ustun, S M S Hussain, A Ulutas, A Onen, M M Roomi, D Mashima, 10.3390/sym13050826Machine Learning-Based Intrusion Detection for Achieving Cybersecurity in Smart Grids Using IEC 61850 GOOSE Messages. Symmetry 2021. 13</p>
<p>Breaking Alert Fatigue: AI-Assisted SIEM Framework for Effective Incident Response. T Ban, T Takahashi, S Ndichu, D Inoue, 10.3390/app13116610Appl. Sci. 132023</p>
<p>Machine Learning Based Anomaly Detection of Log Files Using Ensemble Learning and Self-Attention. M Fält, S Forsström, T Zhang, 10.1109/ICSRS53853.2021.9660694Proceedings of the 2021 5th International Conference on System Reliability and Safety (ICSRS). the 2021 5th International Conference on System Reliability and Safety (ICSRS)Palermo, ItalyNovember 2021</p>
<p>Transformer-based LLMs in Cybersecurity: An in-depth Study on Log Anomaly Detection and Conversational Defense Mechanisms. P Balasubramanian, J Seby, P Kostakos, 10.1109/BigData59044.2023.10386976Proceedings of the 2023 IEEE International Conference on Big Data (BigData). the 2023 IEEE International Conference on Big Data (BigData)Sorrento, Italy18 December 2023</p>
<p>UniParser: A Unified Log Parser for Heterogeneous Log Data. Y Liu, X Zhang, S He, H Zhang, L Li, Y Kang, Y Xu, M Ma, Q Lin, Y Dang, 10.1145/3485447.3511993Proceedings of the ACM Web Conference. the ACM Web ConferenceLyon, FranceApril 2022</p>
<p>Taming the logs-Vocabularies for semantic security analysis. A Ekelhart, E Kiesling, K Kurniawan, 10.1016/j.procs.2018.09.011Procedia Comput. Sci. 1372018</p>
<p>A hierarchical prototype-based approach for classification. X Gu, W Ding, 10.1016/j.ins.2019.07.077Inf. Sci. Int. J. 5052019</p>
<p>A survey of hierarchical classification across different application domains. C N Silla, A A Freitas, 10.1007/s10618-010-0175-9Data Min. Knowl. Discov. 222011</p>
<p>Model for data codification in hierarchical classifications with application to the biodiversity domain. A Altaf, D Mohamed, Z Soumia, E Badia, Z Jamal, 10.1016/j.jksuci.2019.08.010J. King Saud Univ. Comput. Inf. Sci. 342022</p>
<p>A hierarchical topic analysis tool to facilitate digital humanities research. C.-M Chen, S.-Y Ho, C Chang, 10.1108/AJIM-11-2021-0325Aslib J. Inf. Manag. 752022</p>
<p>Latent tree models for hierarchical topic detection. P Chen, N L Zhang, T Liu, L K M Poon, Z Chen, F Khawar, 10.1016/j.artint.2017.06.004Artif. Intell. 2502017</p>
<p>J Zhang, H Bu, H Wen, Y Chen, L Li, H Zhu, 10.48550/arXiv.2405.03644arXiv:2405.03644When llms meet cybersecurity: A systematic literature review. arXiv 2024. </p>
<p>Detecting large-scale system problems by mining console logs. W Xu, L Huang, A Fox, D Patterson, M I Jordan, 10.1145/1629575.1629587Proceedings of the ACM SIGOPS 22nd Symposium on OPERATING Systems Principles. the ACM SIGOPS 22nd Symposium on OPERATING Systems PrinciplesBig Sky, MT, USAOctober 2009</p>
<p>Temporal analysis of a very large topically categorized Web query log. S M Beitzel, E C Jensen, A Chowdhury, O Frieder, D Grossman, 10.1002/asi.20464J. Am. Soc. Inf. Sci. Technol. 582007</p>
<p>A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification. R Li, C Lin, M Collinson, X Li, G Chen, 10.18653/v1/K19-1036Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Hong Kong, China3-4 November 2019</p>
<p>Improving the Accuracy in Sentiment Classification in the Light of Modelling the Latent Semantic Relations. Information. N Rizun, Y Taranenko, W Waloszek, 10.3390/info912030720189307</p>
<p>Log Anomaly Detection Based on Hierarchical Graph Neural Network and Label Contrastive Coding. Y Fang, Z Zhao, Y Xu, Z Liu, 10.32604/cmc.2023.033124Comput. Mater. Contin. 742022</p>
<p>LogEvent2vec: LogEvent-to-Vector Based Anomaly Detection for Large-Scale Logs in Internet of Things. J Wang, Y Tang, S He, C Zhao, P K Sharma, O Alfarraj, A Tolba, 10.3390/s20092451Sensors. 2024512020</p>
<p>A primer in BERTology: What we know about how BERT works. A Rogers, O Kovaleva, A Rumshisky, 10.1162/tacl_a_00349Trans. Assoc. Comput. Linguist. 82021</p>
<p>Log Sequence Anomaly Detection Method Based on Contrastive Adversarial Training and Dual Feature Extraction. Q Wang, X Zhang, X Wang, Z Cao, 10.3390/e24010069Entropy. 242022</p>
<p>Log Anomaly Detection using Fine-Tuned Language Models. C Almodovar, F Sabrina, S Karimi, S Azad, Logfit, 10.36227/techrxiv.22290982.v1IEEE Trans. Netw. Serv. Manag. 212024</p>
<p>H Guo, S Yuan, X Wu, Logbert, 10.48550/arXiv.2103.04475arXiv:2103.04475Log Anomaly Detection via BERT. arXiv 2021. </p>
<p>Deep Learning for Anomaly Detection. R Wang, K Nie, T Wang, Y Yang, B Long, 10.1145/3336191.3371876Proceedings of the 13th International Conference on Web Search and Data Mining. the 13th International Conference on Web Search and Data MiningHouston, TX, USAFebruary 2020</p>
<p>Log-Based Contrastive Representation Learning for Anomaly Detection. J Qi, Z Luan, S Huang, C Fung, H Yang, H Li, D Zhu, D Qian, Logencoder, 10.1109/TNSM.2023.3239522IEEE Trans. Netw. Serv. Manag. 202023</p>
<p>ConAnomaly: Content-Based Anomaly Detection for System Logs. D Lv, N Luktarhan, Y Chen, 10.3390/s21186125Sensors. 2161252021</p>
<p>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models. H Ott, J Bogatinovski, A Acker, S Nedelkoski, O Kao, 10.1109/CloudIntelligence52565.2021.00013Proceedings of the 2021 IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence). the 2021 IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence)Madrid, Spain29 May 2021</p>
<p>N Parmar, A Vaswani, J Uszkoreit, L Kaiser, N Shazeer, A Ku, D Image Tran, Transformer, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholm, Sweden10-15 July 2018. 1 September 2023</p>
<p>Introduction to WordNet: An On-line Lexical Database*. G A Miller, R Beckwith, C Fellbaum, D Gross, K J Miller, 10.1093/ijl/3.4.235Int. J. Lexicogr. 31990</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, 10.48550/arXiv.1810.04805arXiv:1810.04805Pre-training of Deep Bidirectional Transformers for Language Understanding. 2019</p>
<p>Mining invariants from console logs for system problem detection. J.-G Lou, Q Fu, S Yang, Y Xu, J Li, Proceedings of the 2010 USENIX Annual Technical Conference (USENIX ATC 10). the 2010 USENIX Annual Technical Conference (USENIX ATC 10)Boston, MA, USAJune 2010. 12 October 2023</p>
<p>Log clustering based problem identification for online service systems. Q Lin, H Zhang, J.-G Lou, Y Zhang, X Chen, 10.1145/2889160.2889232Proceedings of the 38th International Conference on Software Engineering Companion. the 38th International Conference on Software Engineering CompanionAustin, TX, USAMay 2016</p>
<p>Towards Automated Log Parsing for Large-Scale Log Data Analysis. P He, J Zhu, S He, J Li, M R Lyu, 10.1109/TDSC.2017.2762673IEEE Trans. Dependable Secur. Comput. 152018</p>
<p>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. M Du, F Li, G Zheng, V Srikumar, 10.1145/3133956.3134015Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. the 2017 ACM SIGSAC Conference on Computer and Communications SecurityDallas, TX, USA30 October-3 November 2017</p>
<p>Robust log-based anomaly detection on unstable log data. X Zhang, Y Xu, Q Lin, B Qiao, H Zhang, Y Dang, C Xie, X Yang, Q Cheng, Z Li, 10.1145/3338906.3338931Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringTallinn, Estonia26-30 August 2019</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
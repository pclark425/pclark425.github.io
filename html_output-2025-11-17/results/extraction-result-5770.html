<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5770 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5770</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5770</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-713b604fb9cdd6631074cbd6bf36db029031992e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/713b604fb9cdd6631074cbd6bf36db029031992e" target="_blank">Large Language Models are Zero Shot Hypothesis Proposers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The findings of this paper strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Paper Abstract:</strong> Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5770.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5770.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BHP-hypothesis-proposer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Hypothesis Proposer (BHP dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper constructs a temporally split biomedical Background-and-Hypothesis-Pairs (BHP) dataset and systematically evaluates multiple LLMs (zero-shot, few-shot, fine-tuned) for their ability to distill generalizable scientific hypotheses/principles from literature, and explores a multi-agent LLM framework with tool use to improve hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>multiple (gpt-3.5-turbo, Llama-2-70b-chat, WizardLM-13B/70B, PMC-LLaMA-13B, MedAlpaca-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A collection of API and open-source instructed LLMs evaluated in the study: OpenAI gpt-3.5-turbo (API-based) and several Llama-family based instructed models (Vicuna, Llama-2-chat, WizardLM variants, PMC-LLaMA, MedAlpaca). Models vary from ~13B to 70B parameters and include domain-adapted and fine-tuned variants.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Distill novel, generalizable scientific hypotheses/principles (qualitative laws) from biomedical scholarly literature, tested under strict temporal visibility (zero-shot on unseen August 2023 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedicine (cardiology examples are used; general biomedical literature across many topics in the BHP corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Build BHP dataset split by publication date (train before Jan 2023; unseen test = Aug 2023). Prompt LLMs in zero-shot and 5-shot (random and similarity-retrieved) formats, fine-tune selected open models (WizardLM-13B) on background→hypothesis pairs, and run a multi-agent pipeline (Analyst/Engineer/Scientist/Critic) with optional tool use (PubMed retrieval) implemented via ReAct or OpenAI function-calling. Evaluate with word-overlap metrics (BLEU/ROUGE), LLM-based scoring (ChatGPT ratings for novelty/relevance/significance/verifiability on 0–3 scale), and human expert evaluation (3 annotators on 100 unseen examples).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Natural-language scientific hypotheses/principles (e.g., proposed mechanistic or diagnostic rules such as 'combination of α_short and α_long can serve as early MI biomarkers' or 'telemonitoring and wearable rhythm monitoring improve patient outcomes by enabling earlier detection and intervention').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE-L (word-overlap); ChatGPT-based 4-dim scoring (novelty, relevance, significance, verifiability; 0–3); human expert ratings on same 4 dimensions; Pearson/Spearman correlations between ChatGPT and human scores; coherences with word-overlap metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key findings: (1) LLMs can generate untrained but literature-validated hypotheses on unseen papers (zero-shot hypothesis proposals). (2) Zero-shot generation often yields higher novelty than few-shot; few-shot increases verifiability but reduces novelty. (3) Instruction fine-tuning (WizardLM-13B SFT) improves word-overlap and some metrics but can reduce novelty. (4) Tool use (PubMed retrieval via ReAct or function-calling) provided minimal consistent gains; ReAct performed worse than direct function-calling. (5) Multi-agent role decomposition increased novelty and overall average evaluation scores, suggesting collaborative uncertainty enhances creative hypothesis generation. Quantitatively, ChatGPT average eval scores for top models ranged ~1.8–2.2 (avg across 4 dims) on the unseen set; human–ChatGPT correlations >0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop for dataset filtering and for final evaluation: three biomedical experts rated 100 unseen examples; humans optionally included in the multi-agent loop but primary experiments were automated.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>BHP (Background-and-Hypothesis-Pairs): ~2,700 seen pairs (published before Jan 2023; used for training/validation) and 200 unseen test pairs sampled from papers published Aug 2023, constructed via literature selection + ChatGPT summarization + human filtering following a Self-Instruct pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Traceability of pretraining data (hard to guarantee model hasn't seen related content), word-overlap metrics (BLEU/ROUGE) are inadequate for hypothesis evaluation, LLM hallucination and factual errors remain risks, tool-assisted retrieval sometimes harms novelty or confuses models, uncertainty trade-off: increasing uncertainty helps novelty but may harm verifiability; multi-agent/tool pipelines require careful design.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Example distilled rules/hypotheses include: (a) power-law rheology markers (α_short/α_long and E2/E3/Etotal) as stage-specific myocardial infarction biomarkers; (b) hypothesis that telemonitoring and wearables improve cardiac outcomes by enabling earlier detection/intervention (produced via multi-agent rounds); several model outputs matched golden hypotheses extracted from Aug 2023 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5770.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5770.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>API-based instructed LLM used in zero-shot and few-shot modes both as a hypothesis generator and as an annotator for automatic evaluation; produced novel, verifiable biomedical hypotheses in zero-shot runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>OpenAI's GPT-3.5 Turbo (chat-optimized), API-accessible, released 2022/12; general-domain instruction-following model used here as a black-box hypothesis proposer and evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate scientific hypotheses from biomedical backgrounds (zero-shot and few-shot) and serve as an automatic annotator for novelty/relevance/significance/verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature (BHP corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Zero-shot prompting and 5-shot in-context examples (both random and similarity-retrieved), evaluated on the unseen Aug 2023 test set; also used as ChatGPT-based evaluator producing per-dimension scores and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Natural-language biomedical hypotheses (diagnostic/mechanistic assertions about biomarkers and disease progression).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU/ROUGE, ChatGPT 4-dim scores (0–3), human annotation correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Produced high-novelty hypotheses in zero-shot; ChatGPT evaluation avg ~1.90 on unseen set (paper reports comparable performance to top open models); human–ChatGPT correlation high (Pearson ~0.87). Few-shot increased verifiability scores but decreased novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human experts used for independent evaluation and dataset filtering; GPT-3.5 also used to provide automatic annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>BHP unseen test (200 Aug 2023 examples) and seen/train splits for few-shot example selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Opaque training data (traceability), few-shot examples can reduce generative novelty, potential for overfitting to in-context examples, and possible hallucinations in factual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Zero-shot outputs included MI biomarker hypotheses (α_short/α_long as early MI markers) matching gold hypotheses from test literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5770.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5770.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-70b-chat (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source chat-tuned Llama-2 model (70B parameters) evaluated in zero-shot and few-shot settings for hypothesis generation; showed strong zero-shot novelty and competitive average evaluation scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama-2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Meta's Llama-2 family chat-tuned 70B-parameter model (released 2023), commonly used as an open-source, instruction-following baseline for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Zero-shot/few-shot generation of biomedical hypotheses and participation in multi-agent experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature (BHP corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Zero-shot and 5-shot prompts (random and similarity-retrieved); included in multi-agent role experiments without/with tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Biomedical hypotheses and diagnostic/biomechanical principles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, ChatGPT 4-dim scoring, human eval correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Llama-2-70b-chat achieved one of the higher average ChatGPT evaluation scores (unseen avg ≈2.04) with strong novelty in zero-shot; few-shot sometimes increased BLEU/ROUGE but could reduce novelty. Participated effectively in multi-agent setups where role decomposition improved outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human experts for dataset creation and evaluation; optional human-in-the-loop in multi-agent pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>BHP (seen/unseen splits).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same traceability concerns; few-shot prompts can constrain generative novelty; tool use required careful control (excluded papers published after Jan 2023 from retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Proposed that E2/E3/Etotal metrics could be used across MI stages and speculated on extracellular matrix mechanisms underlying the two-stage power-law rheology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5770.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5770.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardLM-13B-V1.2 (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardLM-13B-V1.2 (fine-tuned Llama-2 13B, supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter Llama-2 based model fine-tuned on instruction datasets and further supervised-fine-tuned on the BHP background→hypothesis pairs; used to study effects of instruction tuning on hypothesis distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>WizardLM-13B-V1.2 (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A 13B-parameter Llama-2 derivative (WizardLM) instruction-tuned model; in this work it was fine-tuned on the constructed BHP dataset (full-parameter fine-tuning: 3 epochs, batch size 8, seq len 2048, lr=3e-5, early stopping).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Improve hypothesis generation via supervised fine-tuning on background→hypothesis pairs, assessing trade-offs between verifiability and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature (BHP corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Full-parameter fine-tuning on training BHP pairs, evaluation on seen and unseen splits; compared SFT vs zero-shot/few-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Extracted natural-language hypotheses and domain-adapted principles from biomedical papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU/ROUGE, ChatGPT 4-dim scoring, human evaluation correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuning improved BLEU/ROUGE and some verifiability (word-overlap) measures and yielded competitive ChatGPT/human evals, but tended to lower novelty relative to zero-shot outputs; demonstrates domain adaptation can improve match to known hypotheses but can constrain creative generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Humans used for dataset generation, filtering low-quality pairs, and for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>BHP training split (2,500 training, 200 validation) and unseen test (200 pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuning reduces generative uncertainty leading to lower novelty; risk of overfitting to training hypotheses; evaluation still limited by inadequate automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>WizardLM-13B-SFT produced combined α_short/α_long MI biomarker hypotheses and other stage-specific diagnostic rules, aligning closely with gold hypotheses from test literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5770.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5770.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PMC-LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PMC-LLaMA-13B (medical-domain LLaMA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medical-domain adapted 13B Llama-based model trained on PMC and medical sources; evaluated to probe the impact of domain adaptation on hypothesis/distillation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PMC-LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A 13B-parameter Llama-based model further trained/adapted on PubMed Central (PMC) and other medical corpora to improve domain knowledge in medicine.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Assess whether domain-adapted LLMs better distill biomedical hypotheses/principles from literature compared to general-domain LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Medicine / biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Zero-shot and few-shot evaluation on BHP; compared word-overlap and ChatGPT/human evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Medical hypotheses and diagnostic interpretations (e.g., biomarker role suggestions and mechanistic claims).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU/ROUGE, ChatGPT 4-dim scoring, human eval correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Domain adaptation (PMC-LLaMA) improved word-overlap metrics (BLEU/ROUGE) but had only small/variable effects on ChatGPT-based hypothesis-quality scores; suggests domain adaptation helps recall known phrasing but not necessarily creative hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human evaluation used; dataset construction and filtering involved human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Medical corpora used to pretrain/adapt PMC-LLaMA; evaluated on BHP unseen set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Domain adaptation biases models toward known literature formulations (increasing verifiability/overlap) and can reduce novelty; discrepancy between word-overlap gains and human/ChatGPT judgments indicates metric mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>PMC-LLaMA generated hypotheses similar in wording to gold hypotheses but sometimes with reduced novelty compared to zero-shot outputs from general LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5770.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5770.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-MultiAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Multi-Agent Collaborative Hypothesis Generation Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system where specialized LLM agents (Analyst, Engineer, Scientist, Critic) iteratively collaborate (optionally with PubMed/tool retrieval) to distill hypotheses/principles from literature; designed to emulate scientific discussion and increase generative uncertainty for creative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>various (roles instantiated by LLMs such as Llama-2 and gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Role-based instantiation of LLMs: Analyst extracts keywords/structure; Engineer retrieves/organizes external evidence; Scientist synthesizes hypotheses; Critic evaluates and proposes refinements. Implemented with ReAct or OpenAI function-calling for tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Improve quality and creativity of distilled qualitative laws (hypotheses/principles) by structured multi-agent interaction and targeted retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature (BHP) and potentially general scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Assign LLM instances to roles; iterate Analyst→Engineer→Scientist→Critic cycles; optional tool use via PubMed search; compare single-agent runs vs multi-agent outputs on BLEU/ROUGE and ChatGPT/human 4-dim scores.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Iteratively refined, multi-agent-derived scientific hypotheses and mechanistic principles (expressed in natural language).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU/ROUGE, ChatGPT evaluation (novelty/relevance/significance/verifiability), human evaluation on sampled outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Multi-agent collaboration increased average ChatGPT evaluation scores and novelty (multi-agent w/o tools avg ≈2.09; with tools ≈2.07 reported) relative to single-agent baselines; tool use alone had minimal benefit, but tools combined with multi-agent roles provided modest additional improvements. ReAct implementation underperformed relative to direct function-calling in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Humans used for evaluation; multi-agent framework is automated but allows optional human-in-the-loop refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>BHP unseen test for evaluation; PubMed used as external retrieval source (search filtered to exclude post-Jan 2023 publications).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tool-guided retrieval is brittle: agents struggled to identify and synthesize useful evidence in some runs; ReAct chain of thought + action sometimes worsened outputs; designing agent roles and inter-agent prompts is nontrivial and task-sensitive; increased computational/coordination overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>A multi-agent produced a refined hypothesis that telemonitoring and wearable devices improve cardiac outcomes via earlier detection and tailored interventions, after 6 iterative rounds of Analyst/Scientist/Critic interplay; several multi-agent outputs showed higher novelty while retaining verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent autonomous scientific research capabilities of large language models. <em>(Rating: 2)</em></li>
                <li>Learning to generate novel scientific directions with contextualized literature-based discovery. <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery. <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5770",
    "paper_id": "paper-713b604fb9cdd6631074cbd6bf36db029031992e",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "BHP-hypothesis-proposer",
            "name_full": "Zero-shot Hypothesis Proposer (BHP dataset)",
            "brief_description": "This paper constructs a temporally split biomedical Background-and-Hypothesis-Pairs (BHP) dataset and systematically evaluates multiple LLMs (zero-shot, few-shot, fine-tuned) for their ability to distill generalizable scientific hypotheses/principles from literature, and explores a multi-agent LLM framework with tool use to improve hypothesis generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "multiple (gpt-3.5-turbo, Llama-2-70b-chat, WizardLM-13B/70B, PMC-LLaMA-13B, MedAlpaca-13B)",
            "llm_model_description": "A collection of API and open-source instructed LLMs evaluated in the study: OpenAI gpt-3.5-turbo (API-based) and several Llama-family based instructed models (Vicuna, Llama-2-chat, WizardLM variants, PMC-LLaMA, MedAlpaca). Models vary from ~13B to 70B parameters and include domain-adapted and fine-tuned variants.",
            "task_goal": "Distill novel, generalizable scientific hypotheses/principles (qualitative laws) from biomedical scholarly literature, tested under strict temporal visibility (zero-shot on unseen August 2023 papers).",
            "domain": "Biomedicine (cardiology examples are used; general biomedical literature across many topics in the BHP corpus).",
            "methodology": "Build BHP dataset split by publication date (train before Jan 2023; unseen test = Aug 2023). Prompt LLMs in zero-shot and 5-shot (random and similarity-retrieved) formats, fine-tune selected open models (WizardLM-13B) on background→hypothesis pairs, and run a multi-agent pipeline (Analyst/Engineer/Scientist/Critic) with optional tool use (PubMed retrieval) implemented via ReAct or OpenAI function-calling. Evaluate with word-overlap metrics (BLEU/ROUGE), LLM-based scoring (ChatGPT ratings for novelty/relevance/significance/verifiability on 0–3 scale), and human expert evaluation (3 annotators on 100 unseen examples).",
            "type_of_qualitative_law": "Natural-language scientific hypotheses/principles (e.g., proposed mechanistic or diagnostic rules such as 'combination of α_short and α_long can serve as early MI biomarkers' or 'telemonitoring and wearable rhythm monitoring improve patient outcomes by enabling earlier detection and intervention').",
            "evaluation_metrics": "BLEU, ROUGE-L (word-overlap); ChatGPT-based 4-dim scoring (novelty, relevance, significance, verifiability; 0–3); human expert ratings on same 4 dimensions; Pearson/Spearman correlations between ChatGPT and human scores; coherences with word-overlap metrics reported.",
            "results_summary": "Key findings: (1) LLMs can generate untrained but literature-validated hypotheses on unseen papers (zero-shot hypothesis proposals). (2) Zero-shot generation often yields higher novelty than few-shot; few-shot increases verifiability but reduces novelty. (3) Instruction fine-tuning (WizardLM-13B SFT) improves word-overlap and some metrics but can reduce novelty. (4) Tool use (PubMed retrieval via ReAct or function-calling) provided minimal consistent gains; ReAct performed worse than direct function-calling. (5) Multi-agent role decomposition increased novelty and overall average evaluation scores, suggesting collaborative uncertainty enhances creative hypothesis generation. Quantitatively, ChatGPT average eval scores for top models ranged ~1.8–2.2 (avg across 4 dims) on the unseen set; human–ChatGPT correlations &gt;0.7.",
            "human_involvement": "Human-in-the-loop for dataset filtering and for final evaluation: three biomedical experts rated 100 unseen examples; humans optionally included in the multi-agent loop but primary experiments were automated.",
            "dataset_or_corpus": "BHP (Background-and-Hypothesis-Pairs): ~2,700 seen pairs (published before Jan 2023; used for training/validation) and 200 unseen test pairs sampled from papers published Aug 2023, constructed via literature selection + ChatGPT summarization + human filtering following a Self-Instruct pipeline.",
            "limitations_or_challenges": "Traceability of pretraining data (hard to guarantee model hasn't seen related content), word-overlap metrics (BLEU/ROUGE) are inadequate for hypothesis evaluation, LLM hallucination and factual errors remain risks, tool-assisted retrieval sometimes harms novelty or confuses models, uncertainty trade-off: increasing uncertainty helps novelty but may harm verifiability; multi-agent/tool pipelines require careful design.",
            "notable_examples": "Example distilled rules/hypotheses include: (a) power-law rheology markers (α_short/α_long and E2/E3/Etotal) as stage-specific myocardial infarction biomarkers; (b) hypothesis that telemonitoring and wearables improve cardiac outcomes by enabling earlier detection/intervention (produced via multi-agent rounds); several model outputs matched golden hypotheses extracted from Aug 2023 papers.",
            "uuid": "e5770.0",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "GPT-3.5 Turbo (OpenAI)",
            "brief_description": "API-based instructed LLM used in zero-shot and few-shot modes both as a hypothesis generator and as an annotator for automatic evaluation; produced novel, verifiable biomedical hypotheses in zero-shot runs.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "gpt-3.5-turbo",
            "llm_model_description": "OpenAI's GPT-3.5 Turbo (chat-optimized), API-accessible, released 2022/12; general-domain instruction-following model used here as a black-box hypothesis proposer and evaluator.",
            "task_goal": "Generate scientific hypotheses from biomedical backgrounds (zero-shot and few-shot) and serve as an automatic annotator for novelty/relevance/significance/verifiability.",
            "domain": "Biomedical literature (BHP corpus).",
            "methodology": "Zero-shot prompting and 5-shot in-context examples (both random and similarity-retrieved), evaluated on the unseen Aug 2023 test set; also used as ChatGPT-based evaluator producing per-dimension scores and explanations.",
            "type_of_qualitative_law": "Natural-language biomedical hypotheses (diagnostic/mechanistic assertions about biomarkers and disease progression).",
            "evaluation_metrics": "BLEU/ROUGE, ChatGPT 4-dim scores (0–3), human annotation correlation.",
            "results_summary": "Produced high-novelty hypotheses in zero-shot; ChatGPT evaluation avg ~1.90 on unseen set (paper reports comparable performance to top open models); human–ChatGPT correlation high (Pearson ~0.87). Few-shot increased verifiability scores but decreased novelty.",
            "human_involvement": "Human experts used for independent evaluation and dataset filtering; GPT-3.5 also used to provide automatic annotations.",
            "dataset_or_corpus": "BHP unseen test (200 Aug 2023 examples) and seen/train splits for few-shot example selection.",
            "limitations_or_challenges": "Opaque training data (traceability), few-shot examples can reduce generative novelty, potential for overfitting to in-context examples, and possible hallucinations in factual detail.",
            "notable_examples": "Zero-shot outputs included MI biomarker hypotheses (α_short/α_long as early MI markers) matching gold hypotheses from test literature.",
            "uuid": "e5770.1",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Llama-2-70b-chat",
            "name_full": "Llama-2-70b-chat (Meta)",
            "brief_description": "A large open-source chat-tuned Llama-2 model (70B parameters) evaluated in zero-shot and few-shot settings for hypothesis generation; showed strong zero-shot novelty and competitive average evaluation scores.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "Llama-2-70b-chat",
            "llm_model_description": "Meta's Llama-2 family chat-tuned 70B-parameter model (released 2023), commonly used as an open-source, instruction-following baseline for downstream tasks.",
            "task_goal": "Zero-shot/few-shot generation of biomedical hypotheses and participation in multi-agent experiments.",
            "domain": "Biomedical literature (BHP corpus).",
            "methodology": "Zero-shot and 5-shot prompts (random and similarity-retrieved); included in multi-agent role experiments without/with tool use.",
            "type_of_qualitative_law": "Biomedical hypotheses and diagnostic/biomechanical principles.",
            "evaluation_metrics": "BLEU, ROUGE, ChatGPT 4-dim scoring, human eval correlations.",
            "results_summary": "Llama-2-70b-chat achieved one of the higher average ChatGPT evaluation scores (unseen avg ≈2.04) with strong novelty in zero-shot; few-shot sometimes increased BLEU/ROUGE but could reduce novelty. Participated effectively in multi-agent setups where role decomposition improved outputs.",
            "human_involvement": "Human experts for dataset creation and evaluation; optional human-in-the-loop in multi-agent pipeline.",
            "dataset_or_corpus": "BHP (seen/unseen splits).",
            "limitations_or_challenges": "Same traceability concerns; few-shot prompts can constrain generative novelty; tool use required careful control (excluded papers published after Jan 2023 from retrieval).",
            "notable_examples": "Proposed that E2/E3/Etotal metrics could be used across MI stages and speculated on extracellular matrix mechanisms underlying the two-stage power-law rheology.",
            "uuid": "e5770.2",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "WizardLM-13B-V1.2 (SFT)",
            "name_full": "WizardLM-13B-V1.2 (fine-tuned Llama-2 13B, supervised fine-tuning)",
            "brief_description": "A 13B-parameter Llama-2 based model fine-tuned on instruction datasets and further supervised-fine-tuned on the BHP background→hypothesis pairs; used to study effects of instruction tuning on hypothesis distillation.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "WizardLM-13B-V1.2 (SFT)",
            "llm_model_description": "A 13B-parameter Llama-2 derivative (WizardLM) instruction-tuned model; in this work it was fine-tuned on the constructed BHP dataset (full-parameter fine-tuning: 3 epochs, batch size 8, seq len 2048, lr=3e-5, early stopping).",
            "task_goal": "Improve hypothesis generation via supervised fine-tuning on background→hypothesis pairs, assessing trade-offs between verifiability and novelty.",
            "domain": "Biomedical literature (BHP corpus).",
            "methodology": "Full-parameter fine-tuning on training BHP pairs, evaluation on seen and unseen splits; compared SFT vs zero-shot/few-shot baselines.",
            "type_of_qualitative_law": "Extracted natural-language hypotheses and domain-adapted principles from biomedical papers.",
            "evaluation_metrics": "BLEU/ROUGE, ChatGPT 4-dim scoring, human evaluation correlations.",
            "results_summary": "Fine-tuning improved BLEU/ROUGE and some verifiability (word-overlap) measures and yielded competitive ChatGPT/human evals, but tended to lower novelty relative to zero-shot outputs; demonstrates domain adaptation can improve match to known hypotheses but can constrain creative generalization.",
            "human_involvement": "Humans used for dataset generation, filtering low-quality pairs, and for evaluation.",
            "dataset_or_corpus": "BHP training split (2,500 training, 200 validation) and unseen test (200 pairs).",
            "limitations_or_challenges": "Fine-tuning reduces generative uncertainty leading to lower novelty; risk of overfitting to training hypotheses; evaluation still limited by inadequate automatic metrics.",
            "notable_examples": "WizardLM-13B-SFT produced combined α_short/α_long MI biomarker hypotheses and other stage-specific diagnostic rules, aligning closely with gold hypotheses from test literature.",
            "uuid": "e5770.3",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PMC-LLaMA-13B",
            "name_full": "PMC-LLaMA-13B (medical-domain LLaMA variant)",
            "brief_description": "A medical-domain adapted 13B Llama-based model trained on PMC and medical sources; evaluated to probe the impact of domain adaptation on hypothesis/distillation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "PMC-LLaMA-13B",
            "llm_model_description": "A 13B-parameter Llama-based model further trained/adapted on PubMed Central (PMC) and other medical corpora to improve domain knowledge in medicine.",
            "task_goal": "Assess whether domain-adapted LLMs better distill biomedical hypotheses/principles from literature compared to general-domain LLMs.",
            "domain": "Medicine / biomedical literature.",
            "methodology": "Zero-shot and few-shot evaluation on BHP; compared word-overlap and ChatGPT/human evaluation metrics.",
            "type_of_qualitative_law": "Medical hypotheses and diagnostic interpretations (e.g., biomarker role suggestions and mechanistic claims).",
            "evaluation_metrics": "BLEU/ROUGE, ChatGPT 4-dim scoring, human eval correlation.",
            "results_summary": "Domain adaptation (PMC-LLaMA) improved word-overlap metrics (BLEU/ROUGE) but had only small/variable effects on ChatGPT-based hypothesis-quality scores; suggests domain adaptation helps recall known phrasing but not necessarily creative hypothesis generation.",
            "human_involvement": "Human evaluation used; dataset construction and filtering involved human oversight.",
            "dataset_or_corpus": "Medical corpora used to pretrain/adapt PMC-LLaMA; evaluated on BHP unseen set.",
            "limitations_or_challenges": "Domain adaptation biases models toward known literature formulations (increasing verifiability/overlap) and can reduce novelty; discrepancy between word-overlap gains and human/ChatGPT judgments indicates metric mismatch.",
            "notable_examples": "PMC-LLaMA generated hypotheses similar in wording to gold hypotheses but sometimes with reduced novelty compared to zero-shot outputs from general LLMs.",
            "uuid": "e5770.4",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM-MultiAgent",
            "name_full": "LLM-based Multi-Agent Collaborative Hypothesis Generation Framework",
            "brief_description": "A multi-agent system where specialized LLM agents (Analyst, Engineer, Scientist, Critic) iteratively collaborate (optionally with PubMed/tool retrieval) to distill hypotheses/principles from literature; designed to emulate scientific discussion and increase generative uncertainty for creative outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "various (roles instantiated by LLMs such as Llama-2 and gpt-3.5-turbo)",
            "llm_model_description": "Role-based instantiation of LLMs: Analyst extracts keywords/structure; Engineer retrieves/organizes external evidence; Scientist synthesizes hypotheses; Critic evaluates and proposes refinements. Implemented with ReAct or OpenAI function-calling for tool use.",
            "task_goal": "Improve quality and creativity of distilled qualitative laws (hypotheses/principles) by structured multi-agent interaction and targeted retrieval.",
            "domain": "Biomedical literature (BHP) and potentially general scientific literature.",
            "methodology": "Assign LLM instances to roles; iterate Analyst→Engineer→Scientist→Critic cycles; optional tool use via PubMed search; compare single-agent runs vs multi-agent outputs on BLEU/ROUGE and ChatGPT/human 4-dim scores.",
            "type_of_qualitative_law": "Iteratively refined, multi-agent-derived scientific hypotheses and mechanistic principles (expressed in natural language).",
            "evaluation_metrics": "BLEU/ROUGE, ChatGPT evaluation (novelty/relevance/significance/verifiability), human evaluation on sampled outputs.",
            "results_summary": "Multi-agent collaboration increased average ChatGPT evaluation scores and novelty (multi-agent w/o tools avg ≈2.09; with tools ≈2.07 reported) relative to single-agent baselines; tool use alone had minimal benefit, but tools combined with multi-agent roles provided modest additional improvements. ReAct implementation underperformed relative to direct function-calling in these experiments.",
            "human_involvement": "Humans used for evaluation; multi-agent framework is automated but allows optional human-in-the-loop refinement.",
            "dataset_or_corpus": "BHP unseen test for evaluation; PubMed used as external retrieval source (search filtered to exclude post-Jan 2023 publications).",
            "limitations_or_challenges": "Tool-guided retrieval is brittle: agents struggled to identify and synthesize useful evidence in some runs; ReAct chain of thought + action sometimes worsened outputs; designing agent roles and inter-agent prompts is nontrivial and task-sensitive; increased computational/coordination overhead.",
            "notable_examples": "A multi-agent produced a refined hypothesis that telemonitoring and wearable devices improve cardiac outcomes via earlier detection and tailored interventions, after 6 iterative rounds of Analyst/Scientist/Critic interplay; several multi-agent outputs showed higher novelty while retaining verifiability.",
            "uuid": "e5770.5",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models.",
            "rating": 2,
            "sanitized_title": "emergent_autonomous_scientific_research_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Learning to generate novel scientific directions with contextualized literature-based discovery.",
            "rating": 2,
            "sanitized_title": "learning_to_generate_novel_scientific_directions_with_contextualized_literaturebased_discovery"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery.",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Galactica: A large language model for science.",
            "rating": 1,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        }
    ],
    "cost": 0.017567,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Zero Shot Hypothesis Proposers</h1>
<p>Biqing Qi ${ }^{1,2,3 <em>}$<br>qibiqing7@gmail.com<br>Kaiyan Zhang ${ }^{1 </em>}$<br>zhang-ky22@mails.tsinghua.edu.cn<br>Haoxiang Li ${ }^{1}$<br>kai Tian ${ }^{1}$<br>Sihang Zeng ${ }^{4}$<br>hx-li20@mails.tsinghua.edu.cn tk23@mails.tsinghua.edu.cn zengsh@uw.edu<br>Zhang-Ren Chen ${ }^{5}$ Jin-Fang Hu ${ }^{5}$！ Bowen Zhou ${ }^{1,2}$！<br>chenzhangren@ncu.edu.cn hujinfang333@126.com zhoubowen@tsinghua.edu.cn<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Frontis.AI ${ }^{3}$ Harbin Institute of Technology<br>${ }^{4}$ University of Washington ${ }^{5}$ The First Affiliated Hospital of Nanchang University</p>
<h4>Abstract</h4>
<p>Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
<h2>1 Introduction</h2>
<p>"When nothing is sure, everything is possible." - Margaret Drabble</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The pursuit of knowledge discovery stands as a cornerstone of human progress, driving innovation, and shaping our understanding of the world [30, 28]. However, in recent times, the process of knowledge discovery has encountered formidable challenges, characterized by serendipity and sluggishness. As the volume of data and literature continues to expand at an unprecedented rate, the ability to distill high-value insights and gain profound understanding from this wealth of information has become increasingly daunting[28]. Silos of information have erected themselves between disciplines, impeding the crucial cross-pollination of ideas and insights that could propel discovery to new heights. Yet, amidst these challenges, there is a glimmer of hope. The advent of large-scale models (LLMs), possessing the capacity to harness a vast reservoir of world knowledge and span multiple domains, holds promise in revolutionizing the landscape of knowledge discovery. These models present an opportunity to break down the barriers between disciplines, enabling researchers to traverse the expansive sea of information with ease and efficiency. Central to the process of knowledge discovery lies the formulation of sound hypotheses [42, 31, 1, 38]. However, a glaring gap persists in the arsenal of tools available to formally explore and evaluate hypotheses. While literature is replete with discussions on validation, it often overlooks the critical aspect of generating novel hypotheses.</p>
<p>In light of these challenges and opportunities, this paper delves into the current state of knowledge discovery, examining the hurdles posed by information explosion and disciplinary isolation. It explores the potential transformative role of LLMs in bridging these gaps, ultimately emphasizing the pivotal role of hypothesis generation in the knowledge discovery process. Furthermore, it highlights the pressing need for tools and methodologies to facilitate hypothesis generation, thus propelling knowledge discovery into a new era of efficiency and innovation [13]. Currently, both ChatGPT and GPT-4 undergo extensive pre-training on vast datasets and possess the capability of continuous updates. However, ensuring strict traceability of data sources becomes a challenging task, limiting our ability to explore zero-shot hypothesis generation.</p>
<p>The past literatures have explored scenarios of problem discovery, yet rigorous experimental designs to investigate whether LLMs can effectively propose genuine problems under zero-shot conditions remain lacking. To tackle this issue, we assemble a dataset of biomedicine literature spanning from January 2000 to September 2023. This dataset is partitioned into training and testing sets, with the training set exclusively containing literature published before January 2023. We construct an unseen test set using literature from August 2023 and ensure that the evaluated LLMs have been trained on corpora before that date. Additionally, we devise a multi-intelligent collaborative framework that incorporates search tools and role-playing to delve deeper into and uncover the potential for hypothesis generation.</p>
<p>Through experiments and analyses as shown in Figure 1, we draw the following findings: 1) LLMs surprisingly generate hypotheses that are untrained yet validated when tested against literature. 2) Increasing uncertainty levels can benefit by diversifying candidate generation and potentially enhancing zero-shot hypothesis generation capabilities. For instance, introducing heightened uncertainty through collaborative multi-agent approaches significantly improves the model’s ability to generalize in zero-shot scenarios. However, integrating subsequent few-shot enhancements and using additional tools may reduce the model’s proficiency in generating hypotheses. This phenomenon is likely due to the reduction of uncertainty, limiting the model’s space for hypothesis generation. Consequently, it lacks consistent positive effects, underscoring the need for careful consideration of the type of external knowledge employed. The above findings also support the notion: "When nothing is sure, everything is possible." Specifically, our contributions are as follows:</p>
<p>1) To rigorously validate the zero-shot and few-shot hypothesis generation potential of LLMs, we construct temporal biomedical instruction data and devised novel and effective experiments for</p>
<p>comprehensive analysis and evaluation. To the best of our knowledge, this is the first work that formally designs experiments to investigate the zero shot hypothesis generation capacity of LLMs.
2) Through validation across different models and various scenario dimensions, we surprisingly find that LLMs possess rudimentary higher-order knowledge reasoning capabilities and can propose new hypothesis statements. This provides new empirical insights and pathways for knowledge discovery.
3) For a comprehensive review of the generated hypotheses, we design metrics across four dimensions for both ChatGPT-based and human evaluations. The correlation scores between ChatGPT evaluations and manual results indicate that LLMs also play a significant role in hypothesis evaluations.
4) To efficiently explore and further harness the capability of hypothesis generation, we introduce a multi-agent system based on LLMs. Through efficient collaboration among multiple models and tool utilization, we analyze the factors influencing hypothesis generation by LLMs.</p>
<h1>2 Process of Scientific Discovery</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The iterative experimental loop of scientific discovery: observations and data accumulated from past experiments are analyzed and used to generate new hypotheses, and in turn new experiments that will yield new data to continue to cycle. In this paper, we mainly focus on investigating whether LLMs have the zero shot generalization ability to generate new hypotheses.</p>
<p>Scientific discovery involves key components, each crucial for advancing our understanding of the natural world: data analysis, hypothesis formulation, experiment design, execution, and observation and reflection [13] as shown in Figure 2.</p>
<p>1) Data Analysis: Foundational in the scientific process, it entails collecting and examining data to discern patterns and anomalies, extracting insights through statistical techniques and visualization. It initiates scientific inquiry, guiding further exploration. 2) Generating Hypotheses: Among these components, hypothesis formulation is pivotal. It entails crafting informed guesses to explain observed phenomena. Hypotheses serve as guiding frameworks, directing and focusing research by articulating specific relationships and outcomes for experimental exploration. 3) Experiment Design: Once a hypothesis is set, designing experiments becomes essential to rigorously test its validity. This involves defining variables, specifying control groups, and outlining methods and procedures. Well-designed experiments ensure objective hypothesis testing and yield meaningful, informative results. 4) Experiment Execution: Meticulous execution of designed experiments and data collection are critical. Researchers adhere precisely to experimental protocols, recording observations, measurements, and unexpected findings. Integrity in execution ensures reliable, reproducible outcomes. 5) Accumulating Observations: After experiments, scientists engage in observation and reflection. They analyze collected data to determine if results support or refute the initial hypothesis. If unsupported, hypotheses may be revised or new ones formulated based on findings. Observation and reflection permit iterative refinement of scientific understanding.
Hypothesis Pioneers Pathways: Guiding Knowledge Discovery. While all components are essential, hypothesis formulation holds a unique position. It drives the scientific endeavor, guiding research question selection, experiment design, and data analysis. Well-constructed hypotheses not only provide direction but also lay the foundation for meaningful scientific discoveries by posing rigorously testable questions. Hypothesis formulation serves as the intellectual anchor steering scientific investigation and ultimately advancing knowledge.</p>
<p>3 Can LLMs Truly Generate Zero-Shot Hypotheses?</p>
<p>In this section, we outline the methodology employed for a thorough assessment of LLMs’ capacity to generate hypotheses under zero-shot conditions. To accomplish this, we begin by defining the problem of hypothesis generation in zero-shot settings. Next, we elucidate the process of dataset construction within the biomedical domain. Finally, we undertake comprehensive experiments to evaluate various instructed models across multiple dimensions, aiming to explore the factors influencing the ability of LLMs to propose improved hypotheses.</p>
<h3>3.1 Problem Definition</h3>
<p>Following the scientific discovery process outlined in Section 2, hypothesis generation typically occurs after thorough literature analysis and examination of specific phenomena. To enhance evaluation effectiveness, we formalize this process as a text completion task. Given dataset $D$, an instruction $I$, and text pairs $(X_{i},Y_{i})<em i="i">{i=1}^{n}$ containing background knowledge and corresponding hypotheses, extracted from medical papers, our objective is to assess model $M$ by having it generate hypotheses based on the task instruction and background knowledge, i.e., $M(I,X</em>$. The objective function is formulated as:})=Y_{i}$, for each $i$ $\in{1,..,,n</p>
<p>$y^{*}=\underset{y_{1},...,y_{n}}{\arg\max}\prod_{t=1}^{n}{P(y_{t}|y_{1},...,y_{t-1},I,X)}.$</p>
<h3>3.2 Dataset Construction</h3>
<p>In this section, we detail the process of constructing datasets and ensuring the robustness of our evaluation. Prevalent LLMs, like Llama and ChatGPT, face challenges in tracing the origin of their knowledge due to continuous self-updating. To address this, we propose a novel approach to assess LLMs’ hypothesis generation. Recognizing their potential impact on public domain data, we reconstruct a new biomedical literature dataset based on publication dates.</p>
<p>As depicted in Figure 3, we designated the year 2023 as the cut-off point. Our training dataset comprises literature published before January 2023, while the test dataset comprises literature published after January 2023, forming pairs of data with background knowledge and hypothesis proposals. Due to the emergence of more advanced LLMs, our evaluations focus exclusively on the unseen test set, featuring literature published in August 2023. We selected instructed models fine-tuned before August 2023 for both evaluation and fine-tuning testing. In our experimental setup, we implemented stringent measures to ensure the models had no prior exposure to the test data, affirming the validity of our experiments. We strictly follow the standard pipeline as outlined in Self-Instruct [32] for our data generation process, encompassing four key steps: 1) Compose the paper set based on the topic and content of the literature. 2) Utilize chatgpt-turbo-3.5 to summarize the literature knowledge. 3) Generate background knowledge-assume pairs. 4) Filter low-quality data. 5) Split the dataset according to publication time.</p>
<h3>3.3 Datast Analysis</h3>
<p>In this section, we provide a comprehensive overview of the constructed dataset, encompassing details about the data acquisition strategy, dataset size, visibility control measures, distribution by year and month, as well as topic distribution.</p>
<p>We have created two datasets to maintain control over the visibility of hypotheses: 1) Seen dataset This dataset comprises 2700 background and hypothesis pairs sourced from literature published before January 2023. This dataset was partitioned into training (2500) and validation (200) subsets (as well as seen test set). It is consistent with the corpus that the LLMs have been exposed to. 2) Unseen dataset The unseen dataset consists of 200 pairs extracted from papers published in August 2023, which the LLMs have not encountered during training and are used for testing purposes.</p>
<p>We also provide publication date and topic distribution of constructed dataset in Appendix B.1.</p>
<h1>3.4 Experiment Setup</h1>
<p>In this section, we introduce experimental settings for hypothesis generation and evaluation.
Models For a fair comparison, we exclusively evaluate LLMs trained on corpora before March 2023 to ensure the test set remains unseen. We consider three categories of models in total: 1) API-based LLMs: this is mainly ChatGPT. 2) General domain instructed LLMs: These models consist of open-source models that have undergone fine-tuning based on Llama using general domain instructions. We primarily choose the top-tier models based on their performance rankings on the Alpaca Eval Leaderboard ${ }^{3}$. 3) Specific domain instructed LLMs: These include PMC-LLaMA [35], and MedAlpaca [10]. These models are trained on a variety of sources in medicine domain, such as medical books, PMC papers, medical dialogs, and others. We provide detailed meta-information for various models, including their training data sources and publication dates, in Appendix B.2.
Prompts To ensure a consistent output format across different models, we create prompts in two formats: zero-shot and few-shot examples. In our experiments, we adopt a 5-shot format, selecting examples from the training set before January 2023 using both randomly sampled and similarity retrieval methods. We provide illustrations of zero-shot and few-shot prompts in Appendix E.
Finetuning To assess the hypothesis generation capability beyond zero-shot scenarios, we identify the top-performing open-source models through few-shot evaluation. We proceed to fine-tune the full parameters of WizardLM-13B-V1.2 with the background and hypothesis pairs. The fine-tuning process consists of three epochs, employing a batch size of 8 , a maximum sequence length of 2048 tokens, and a learning rate set at $3 \mathrm{e}-5$. We implement early stopping and select the best checkpoints based on their performance on the seen test dataset.
Evaluation Given the disparities between the hypothesis generation task and traditional text generation tasks liking machine translation and summarization, with the former being more challenging and often involving uncertainty that extends beyond established ground truth, we approach our evaluation from two primary perspectives: conducting evaluations with and without golden hypotheses. In evaluations with golden hypotheses, we employ standard text generation metrics, including BLEU and ROUGE in evaluate library ${ }^{4}$, to assess word overlap between the generated outputs and the ground truth. The vastness of the hypothesis space renders it difficult to comprehensively assess the quality of generated hypotheses using word overlap metrics alone. To provide a more comprehensive evaluation of the generated hypotheses from multiple facets, we have thoughtfully devised four metrics: novelty, relevance, significance, and verifiability. Inspired by recent research that highlights ChatGPT as proficient annotators [8, 16], demonstrating a strong correlation with human ratings, we employ ChatGPT for further evaluation. In detail, we request ChatGPT to evaluate both the generated scientific hypotheses and the provided background across these aspects. The scoring scale ranges from 0 to 3 , where a higher score indicates superior results. Additionally, we solicit ChatGPT to furnish a step-by-step explanation to substantiate the assigned score. Moreover, we conduct human evaluation based on the four metrics for the top-tier models identified in the automatic evaluation in Section 3.5, and we provide a detailed description of this process in Section 3.6.</p>
<h3>3.5 Experiment Results</h3>
<p>This section presents the results of hypothesis generation across various models in both zero-shot and few-shot settings. We primarily analyze the results from two perspectives: the impact of the zero-shot setting and the influence of introducing external knowledge on hypothesis generation.</p>
<h3>3.5.1 Impact of zero-shot settings</h3>
<p>The results presented in Table 1 demonstrate the significant impact of zero-shot settings in improving hypothesis generation, particularly in terms of fostering high novelty. We analyze these results from two key perspectives as following.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results of various LLMs: We assess instructed models using zero-shot and few-shot format prompts to generate constrained outputs. To provide a comprehensive assessment, we calculate the average scores for novelty, relevance, significance, and verifiability, denoted as Avg. Results marked with an asterisk (*) indicate that the few-shot prompts are constructed by retrieving samples from the training set that are similar to the background of inputs. To facilitate better comparison, we highlight the highest and sub-high score with both bold and underline formatting under each category.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>Seen</th>
<th></th>
<th></th>
<th></th>
<th>Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>BLEU</td>
<td>ROUGE</td>
<td>BLEU</td>
<td>ROUGE</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbot(8-shot)</td>
<td>13.93</td>
<td>25.32</td>
<td>15.52</td>
<td>26.48</td>
<td>1.42</td>
<td>2.63</td>
<td>1.58</td>
<td>1.97</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)</td>
<td>16.47</td>
<td>27.07</td>
<td>16.49</td>
<td>26.96</td>
<td>1.22</td>
<td>2.57</td>
<td>1.84</td>
<td>2.03</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)*</td>
<td>17.33</td>
<td>27.28</td>
<td>17.71</td>
<td>27.53</td>
<td>1.02</td>
<td>2.61</td>
<td>1.85</td>
<td>2.36</td>
</tr>
<tr>
<td>General</td>
<td>Vicuna-33b-v1.3(0-shot)</td>
<td>13.97</td>
<td>24.43</td>
<td>13.66</td>
<td>23.43</td>
<td>1.67</td>
<td>2.55</td>
<td>2.04</td>
<td>1.84</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)</td>
<td>11.23</td>
<td>22.54</td>
<td>11.49</td>
<td>22.68</td>
<td>1.60</td>
<td>2.40</td>
<td>1.67</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)*</td>
<td>12.78</td>
<td>24.11</td>
<td>13.12</td>
<td>23.66</td>
<td>1.19</td>
<td>2.71</td>
<td>2.00</td>
<td>2.17</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>10.95</td>
<td>21.56</td>
<td>11.44</td>
<td>22.04</td>
<td>1.86</td>
<td>2.41</td>
<td>1.91</td>
<td>1.98</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>8.17</td>
<td>21.09</td>
<td>7.63</td>
<td>20.70</td>
<td>1.95</td>
<td>2.38</td>
<td>2.06</td>
<td>2.22</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>8.40</td>
<td>21.65</td>
<td>9.66</td>
<td>22.43</td>
<td>1.43</td>
<td>2.50</td>
<td>1.94</td>
<td>2.15</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(0-shot)</td>
<td>11.91</td>
<td>23.35</td>
<td>12.03</td>
<td>23.55</td>
<td>1.62</td>
<td>2.55</td>
<td>1.90</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)</td>
<td>14.00</td>
<td>24.30</td>
<td>13.82</td>
<td>24.38</td>
<td>1.33</td>
<td>2.54</td>
<td>1.81</td>
<td>2.23</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)*</td>
<td>14.96</td>
<td>25.66</td>
<td>15.26</td>
<td>25.78</td>
<td>1.06</td>
<td>2.64</td>
<td>1.73</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>13.45</td>
<td>24.12</td>
<td>14.25</td>
<td>25.05</td>
<td>1.57</td>
<td>2.45</td>
<td>1.74</td>
<td>1.89</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>14.04</td>
<td>24.59</td>
<td>13.78</td>
<td>24.28</td>
<td>1.17</td>
<td>2.61</td>
<td>2.12</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)*</td>
<td>14.46</td>
<td>24.78</td>
<td>15.26</td>
<td>25.56</td>
<td>0.97</td>
<td>2.67</td>
<td>1.85</td>
<td>1.99</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(0-shot)</td>
<td>8.79</td>
<td>22.71</td>
<td>8.38</td>
<td>21.48</td>
<td>1.58</td>
<td>2.51</td>
<td>1.70</td>
<td>2.05</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)</td>
<td>12.46</td>
<td>23.60</td>
<td>12.58</td>
<td>24.21</td>
<td>1.06</td>
<td>2.64</td>
<td>2.09</td>
<td>2.20</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)*</td>
<td>12.37</td>
<td>23.93</td>
<td>12.88</td>
<td>24.78</td>
<td>1.16</td>
<td>2.76</td>
<td>2.10</td>
<td>2.23</td>
</tr>
<tr>
<td>Medicine</td>
<td>MedAlpaca-13B(0-shot)</td>
<td>6.10</td>
<td>22.07</td>
<td>5.82</td>
<td>20.49</td>
<td>0.55</td>
<td>1.17</td>
<td>1.17</td>
<td>1.06</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)</td>
<td>0.99</td>
<td>3.84</td>
<td>1.08</td>
<td>3.84</td>
<td>0.98</td>
<td>1.32</td>
<td>1.32</td>
<td>1.49</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)*</td>
<td>4.60</td>
<td>9.36</td>
<td>4.50</td>
<td>9.07</td>
<td>1.09</td>
<td>1.40</td>
<td>1.20</td>
<td>1.53</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>22.89</td>
<td>40.36</td>
<td>22.37</td>
<td>40.45</td>
<td>0.76</td>
<td>1.94</td>
<td>1.42</td>
<td>1.52</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)</td>
<td>1.36</td>
<td>4.83</td>
<td>1.41</td>
<td>4.78</td>
<td>1.13</td>
<td>1.45</td>
<td>1.26</td>
<td>0.88</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>6.21</td>
<td>12.39</td>
<td>6.16</td>
<td>12.13</td>
<td>1.73</td>
<td>2.17</td>
<td>1.88</td>
<td>2.09</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13B-V1.2</td>
<td>19.13</td>
<td>27.35</td>
<td>19.73</td>
<td>27.58</td>
<td>0.97</td>
<td>2.55</td>
<td>1.38</td>
<td>2.26</td>
</tr>
</tbody>
</table>
<p>Zero-shot Outperforms Few-shot. Our findings indicate that, for extra large models like Llama-2-70b-chat and WizardLM-70B-V1.0, zero-shot performance surpasses that of the fewshot setting, where few-shot examples are obtained by randomly sampling. This suggests that the capacity of hypothesis generation is limited by the inclusion of few-shot examples, and models exhibit stronger abilities in a zero-shot setting.
Outperforming the Unseen Compared to the Seen Test Set. Despite the visibility of literature published before 2022 in the pre-training corpus of most LLMs, we have categorized the test set into "seen" and "unseen." Typically, LLMs may excel in the "seen" test set due to the potential memorization of hypotheses present in the pre-training corpus, resulting in higher performance compared to the "unseen" test set. However, our results indicate that LLMs tend to perform better on the "unseen" test set. We speculate that this is because the complexity of hypothesis generation may hinder LLMs from effectively leveraging the dark knowledge in their parameters.</p>
<h1>3.5.2 Influence of external knowledge</h1>
<p>Based on the results, we observe that the introduction of external knowledge, such as few-shot examples, domain adaptation, and instruction fine-tuning, does not consistently enhance the ability of hypothesis proposing.
Few-Shot Examples Enhance Verifiability but Decrease Novelty. In comparison to zero-shot settings, models using few-shot prompts benefit from the provided examples, resulting in very high matching rates. Regarding word overlap metrics, including BLEU and ROUGE, most models, especially WizardLM series models, and Openchat-v3.2-super, show improved performance when provided with in-context examples, with retrieved examples being particularly beneficial for their generations. However, it's important to note that these few-shot prompts significantly increase verifiability while simultaneously leading to lower levels of novelty compared to zero-shot results.
Randomly Sampled Few-Shot Examples vs. Similarity Retrieval. Given that randomly sampled in-context examples often differ from the provided background in terms of topics or domains, this can potentially confuse LLMs and lead to decreased performance. In our pursuit of further exploration into the hypothesis generation capabilities of LLMs, we retrieve examples from the training dataset based on their similarity to the given background. The results indicate that similarity retrieval can further enhance performance.</p>
<p>Instruction Tuning Enhances LLM Performance. Following fine-tuning on a dataset comprising background and hypothesis pairs, WizardLM-13B-V1.2 attains superior performance, surpassing even gpt-3.5-turbo and WizardLM-70B-V1.0. This finding underscores that domain adaptation remains a valuable approach to enhance the hypothesis generation capabilities of LLMs. It not only leads to greater resource efficiency but also supports the promotion of privacy in a localized context.
Impact of Domain Adaptation on Hypothesis Generation. We have also conducted an analysis of the influence of fine-tuning for domain adaptation on hypothesis generation. In this comparison, we utilize instructed models adapted to the field of medicine. The results obtained from MedAplaca and PMC-LLaMA indicate that domain adaptation can significantly improve word overlap performance. However, the metrics derived from ChatGPT suggest that domain adaptation has only a minimal effect on hypothesis generation. This discrepancy between word overlap metrics and ChatGPT's evaluation highlights the need for more comprehensive and unified metrics in the context of hypothesis generation tasks.</p>
<h1>3.6 Human Evaluation and Case Study</h1>
<p>In this section, we conduct a human evaluation to assess the generated hypotheses and calculate coherence scores to compare them with ChatGPT evaluation scores, guiding further evaluation efforts.</p>
<h3>3.6.1 Evaluation Settings</h3>
<p>Evaluation Metrics To comprehensively evaluate the generations manually and simultaneously assess the quality of ChatGPT's evaluations, we continue to utilize the four metrics outlined in Section 3.4, which encompass novelty, relevance, significance, and verifiability. The range of each metric remains from 0 to 3 , with higher values indicating better performance. Additionally, we calculate coherence scores between human evaluations and ChatGPT evaluations.
Selection of Models Given the constraints associated with the cost of human evaluation, our primary objective is to assess whether LLMs can produce valuable hypotheses, rather than striving for state-of-the-art performance. As a result, we exclusively perform human evaluation on the outputs generated by the LLM that ranks highest in performance based on automatic metrics and ChatGPT evaluation. Furthermore, we aim to encompass a variety of prompts and models in our evaluation. The final models selected for human evaluation are detailed in Table 2.
Evaluation Details We randomly selected 100 examples from the unseen test set and had three evaluators with a biomedical background assign scores for each metric to each model.</p>
<h3>3.6.2 Evaluation Results</h3>
<p>As depicted in Table 2, the human evaluations exhibit a strong correlation with ChatGPT's evaluations, with Pearson and Spearman coefficients exceeding 0.7 for all models. These results strongly support our earlier findings regarding the influence of zero-shot learning and external knowledge. This reinforces our assertion that large language models can effectively propose hypotheses and significantly contribute to scientific discovery. For additional insights, we present correlation coefficients between word overlap scores and manual scores in the Appendix C, revealing lower coherence and highlighting the need for more advanced evaluation metrics.
We also conduct a case study that showcases the hypotheses generated by various models and includes examples of step-by-step evaluations by ChatGPT. Details can be found in Appendix D.</p>
<h2>4 Can agent collaboration enhance LLMs' zero-shot generalization?</h2>
<p>In this section, we will strive to enhance the ability of LLMs in hypothesis generation through multi-agent collaboration and the use of tools. Our objective is to improve hypothesis efficiency by employing multi-agent collaboration, simulating real-world research scenarios. To begin, we introduce the conceptual system of multi-agent collaboration for hypothesis generation, drawing inspiration from scientific research. Subsequently, we present the role design and the tools use in this context. Finally, we present preliminary validated results of the multi-agent system using our proposed BHP dataset.</p>
<p>Table 2: This table presents the results of human evaluation. The Avg Coefficient are used to assess the correlation between the average scores obtained from ChatGPT and those from human evaluation.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>ChatGPT</th>
<th>Human Eval</th>
<th></th>
<th></th>
<th></th>
<th>Avg Coefficient</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Eval.Avg</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
<td>Avg</td>
<td>Pearson</td>
<td>Spearman</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbo(0-shot)</td>
<td>1.90</td>
<td>1.54</td>
<td>2.69</td>
<td>1.77</td>
<td>2.08</td>
<td>2.02</td>
<td>0.87</td>
<td>0.78</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbo(5-shot)*</td>
<td>1.96</td>
<td>1.31</td>
<td>2.62</td>
<td>2.08</td>
<td>2.62</td>
<td>2.15</td>
<td>0.80</td>
<td>0.78</td>
</tr>
<tr>
<td>General</td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>2.04</td>
<td>1.77</td>
<td>2.23</td>
<td>1.92</td>
<td>1.92</td>
<td>1.96</td>
<td>0.89</td>
<td>0.84</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>2.20</td>
<td>2.15</td>
<td>2.77</td>
<td>2.08</td>
<td>2.31</td>
<td>2.33</td>
<td>0.96</td>
<td>0.90</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>2.01</td>
<td>1.38</td>
<td>2.62</td>
<td>2.31</td>
<td>2.00</td>
<td>2.08</td>
<td>0.97</td>
<td>0.94</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>1.91</td>
<td>1.38</td>
<td>2.31</td>
<td>1.54</td>
<td>2.00</td>
<td>1.81</td>
<td>0.90</td>
<td>0.75</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>2.01</td>
<td>1.15</td>
<td>2.69</td>
<td>2.46</td>
<td>1.77</td>
<td>2.02</td>
<td>0.85</td>
<td>0.89</td>
</tr>
<tr>
<td>Medicine</td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>1.41</td>
<td>1.00</td>
<td>2.62</td>
<td>1.92</td>
<td>2.00</td>
<td>1.88</td>
<td>0.73</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>1.97</td>
<td>1.85</td>
<td>2.23</td>
<td>1.92</td>
<td>1.69</td>
<td>1.92</td>
<td>0.95</td>
<td>0.94</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13BV1.2</td>
<td>1.79</td>
<td>0.85</td>
<td>2.77</td>
<td>1.23</td>
<td>2.23</td>
<td>1.77</td>
<td>0.83</td>
<td>0.85</td>
</tr>
</tbody>
</table>
<h1>4.1 Multi-agent Framework</h1>
<p>Inspired by the structured methodology detailed in Section 2, we introduce a comprehensive framework tailored for hypothesis formulation. This framework encapsulates a multi-agent system where each agent assumes a distinct role, mirroring the collaborative nature of scientific endeavors. Through a symbiotic and iterative process, these agents collaborate to craft hypotheses that are not only grounded in existing knowledge but also pave the way for novel insights. By emulating the essence of scientific discovery, our framework strives to produce hypotheses that are both innovative and scientifically robust. As depicted in Figure 4, we have partitioned the framework into five components, encompassing four automated agents and the option for human involvement within the loop.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: The conceptual system of multi-agent collaboration for hypothesis generation. The overall prototyping process is illustrated below, allowing users to choose optional involvement. We offer core role descriptions of multi-agents and the fully automated system above.</p>
<p>Role Design In our proposed multi-agent framework, each component plays a distinct and pivotal role. The Analyst serves as the foundation, meticulously extracting and defining core elements from the research background. Its primary objective is to interpret the literature, distilling it into keywords or topics that subsequently guide the Engineer's search efforts. The Engineer, leveraging these keywords, embarks on a mission to retrieve and organize pertinent information. They meticulously plan and execute detailed searches, ensuring that the findings are compiled in a structured manner. This organized materials then lands in the domain of the Scientist, whose objective is to weave together the Engineer's findings with the original research background. Through careful interpretation, the Scientist crafts a hypothesis that is both grounded in existing knowledge and offers a fresh perspective. However, before this hypothesis is finalized, it undergoes scrutiny by the Critic. The Critic's role</p>
<p>is paramount in ensuring the hypothesis's robustness, coherence, and novelty. They evaluate the hypothesis against the backdrop of the research background, ensuring it stands up to academic rigor. Feedback from the Critic, if necessary, loops back to refine the hypothesis or prompts the Analyst for further insights, creating a cyclical and iterative process of refinement.</p>
<p>Tool Use To explore external knowledge beyond the inherent dark knowledge within LLMs, we integrate the Engineer agent with search engines, mainly PubMed ${ }^{5}$. Similarly, to control the visibility of the unseen test dataset, we filter and exclude literature published after January 2023 from the search results. We carry out tool use experiments using ReAct [40] and OpenAI function calling. ReAct is a method that extends the concept of Chain of Thought (CoT) [34], involving thinking before taking action and subsequently making observations based on feedback from the environment. In our experiments, we instruct the LLMs to initially contemplate the provided background information and then make a decision regarding whether to utilize tools. Upon receiving feedback from the tools, the LLMs are expected to identify supporting evidence in the results or potentially make further tool requests. The LLMs are responsible for concluding the hypothesis generation process and summarizing the hypotheses independently. In the case of OpenAI function calling, we directly specify tools for publication searching and transmit them to OpenAI APIs. This process is roughly implemented through fine-tuning, as described in ToolFormer [24].</p>
<h1>4.2 Experiment Results</h1>
<p>Our primary focus is to investigate the impact of tool use and multi-agent collaboration on hypothesis generation. We present the experimental results in Table 3. Based on the results, we summarize our findings from two perspectives: tool use and role-playing.</p>
<p>Influence of Tool Use Based on our results, we observe that tool use has minimal impact on improving the hypothesis generation ability of LLMs. This observation aligns with the findings presented in the previous sections regarding the analysis of external knowledge. Notably, the ReAct-based method performs worse than OpenAI function calling. It is also evident that LLMs struggle to identify useful information and exhibit weaknesses in the thought-action-observation process, even when utilizing the official interface from OpenAI. Hypothesis generation is indeed a challenging task that necessitates iterative discussions and the exchange of ideas among various individuals.</p>
<p>Multi-Agent Collaboration In addition to tool use, our findings suggest that the division of labor and interaction among multi-agents can significantly enhance the model's capability to propose hypotheses by introducing uncertainty. This mirrors the dynamics of real-world scientific research, where hypotheses are formulated through iterative discussions and refutations. Additionally, it is worth noting that tool use can further enhance the performance of the multi-agent framework.</p>
<p>Table 3: Results of individual agents and multi-agent systems, both with and without the use of tools, on the unseen test dataset. The results demonstrate that both multi-agent systems and the utilization of tools enhance the ability of LLMs in hypothesis generation. Among the various types of models, both $2^{a}$ and $2^{b}$ are evaluated with tool use. The difference between them lies in their implementations: ReAct [40] and OpenAI function calling ${ }^{6}$, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Influence Factor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Automatic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4 Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-agent</td>
<td style="text-align: center;">Tool use</td>
<td style="text-align: center;">BLUE</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">Novelty</td>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">Significance</td>
<td style="text-align: center;">Verifiability</td>
<td style="text-align: center;">Avg</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.52</td>
<td style="text-align: center;">26.48</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">1.92</td>
</tr>
<tr>
<td style="text-align: center;">$2^{a}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">14.94</td>
<td style="text-align: center;">24.16</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: center;">$2^{b}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">15.87</td>
<td style="text-align: center;">24.94</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">1.49</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.71</td>
<td style="text-align: center;">22.11</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.10</td>
<td style="text-align: center;">2.09</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">22.04</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.07</td>
</tr>
</tbody>
</table>
<h2>5 Conclusion</h2>
<p>From the hypothesis-proposer perspective, we investigated LLMs' zero-shot generalisation ability in scientific research. Specifically, we first build a comprehensive corpus based on biomedical literature,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>split by publication date, including background knowledge and hypothesis pairs. This corpus is then used as a basis for fine-tuning LLMs, leading to the generation of the LLM. To further analysis and enhance the capabilities of the hypothesis proposer, we introduce a LLM-based multi-agent collaboration system. Experimental results show that fine-tuned LLMs of various sizes can propose new hypotheses that did not appear in the training data but can be confirmed by the test literature, with performance comparable to ChatGPT and in some cases even better. Notably, our study revealed that introducing uncertainty into processes and operations enhances zero-shot generalization capabilities. These findings confirm the potential of LLMs to propose new hypotheses and offers hope for future unlocked scientific discovery. In future work, we will focus on optimizing models and generating hypotheses guided by effective uncertainty assessment metrics.</p>
<h1>Acknowledgements</h1>
<p>We extend our gratitude to the anonymous reviewers for their insightful feedback.</p>
<h2>References</h2>
<p>[1] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.
[2] Andres M. Bran, Sam Cox, Andrew D. White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools, June 2023. arXiv:2304.05376.
[3] Boxi Cao, Hongyu Lin, Xianpei Han, and Le Sun. The Life Cycle of Knowledge in Big Language Models: A Survey, March 2023. arXiv:2303.07616 [cs].
[4] Zhuo Chang, Jing Zhang, Yilun Liu, Huajian Gao, and Guang-Kui Xu. New Mechanical Markers for Tracking the Progression of Myocardial Infarction. Nano Letters, 23(16):7350-7357, August 2023.
[5] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking Before Action: Gather Information in Embodied Decision Making with Language Models, May 2023. arXiv:2305.15695 [cs].
[6] Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Empowering Private Tutoring by Chaining Large Language Models, September 2023. arXiv:2309.08112 [cs] version: 1.
[7] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback, May 2023. arXiv:2305.10142 [cs].
[8] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, July 2023. arXiv:2303.15056 [cs].
[9] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. MindAgent: Emergent Gaming Interaction, September 2023. arXiv:2309.09971 [cs] version: 1.
[10] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno K Bressem. Medalpaca-an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023.
[11] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework, August 2023. arXiv:2308.00352 [cs].
[12] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory, June 2023. arXiv:2306.03901 [cs].</p>
<p>[13] Moksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. Digital Discovery, 2(3):557-577, 2023.
[14] Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, and Jie Fu. Think Before You Act: Decision Transformers with Internal Working Memory, May 2023. 0 citations (Semantic Scholar/arXiv) [2023-05-30] arXiv:2305.16338 [cs].
[15] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society, March 2023. arXiv:2303.17760 [cs].
[16] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, May 2023. arXiv:2303.16634 [cs].
[17] Philipp Maas, Frank Carey, Chris Wheeler, Edward Saatchi, Pete Billington, and Jessica Yaffa Shamash. SHOW-1 and Showrunner Agents in Multi-Agent Simulations. arXiv preprint, 2023.
[18] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, June 2022. arXiv:2112.09332.
[19] Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: Tool Augmented Language Models, May 2022. arXiv:2205.12255 [cs].
[20] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative Agents: Interactive Simulacra of Human Behavior, August 2023. arXiv:2304.03442 [cs].
[21] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large Language Model Connected with Massive APIs, May 2023. arXiv:2305.15334 [cs].
[22] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative Agents for Software Development, August 2023. arXiv:2307.07924 [cs].
[23] Vipula Rawte, Amit Sheth, and Amitava Das. A Survey of Hallucination in Large Foundation Models, September 2023. arXiv:2309.05922 [cs].
[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools, February 2023. arXiv:2302.04761.
[25] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, May 2023. arXiv:2303.17580 [cs].
[26] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, June 2023. arXiv:2303.11366 [cs].
[27] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs, June 2023. arXiv:2306.06624 [cs].
[28] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[29] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the Planning Abilities of Large Language Models - A Critical Investigation, May 2023. arXiv:2305.15771 [cs].</p>
<p>[30] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, 2023.
[31] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Learning to generate novel scientific directions with contextualized literature-based discovery. arXiv preprint arXiv:2305.14259, 2023.
[32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January 2023. arXiv:2201.11903 [cs].
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
[35] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmcllama: Towards building open-source language models for medicine, 2023.
[36] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The Rise and Potential of Large Language Model Based Agents: A Survey, September 2023. arXiv:2309.07864 [cs].
[37] Hui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions, June 2023. arXiv:2306.02224.
[38] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023.
[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, May 2023. arXiv:2305.10601 [cs].
[40] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models, March 2023. arXiv:2210.03629.
[41] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models, September 2023. arXiv:2309.01219 [cs].
[42] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. arXiv preprint arXiv:2302.14233, 2023.
[43] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory, June 2023. arXiv:2305.17144 [cs].
[44] Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. Pad: Program-aided distillation specializes large models in reasoning. arXiv preprint arXiv:2305.13888, 2023.</p>
<h1>A Related Works</h1>
<h2>A. 1 Data-Driven Scientific Discovery</h2>
<p>Data-driven knowledge discovery research within LLM is relatively limited, with the current focus primarily on dataset construction and task-driven design. In this context, [42] proposed a dataset for investigating the transition from goals to discoveries. However, it should be noted that accurate discoveries within this dataset are not recent. [31] introduced a method for automatically collecting and constructing publication data, along with a proposal for a hypothesis generation approach in the natural language processing (NLP) domain. However, this method requires prior human knowledge, explicit context, and is not an automated process. It's worth noting that their data was constructed from literature before 2021 from the ACL collection, implying that the information may already exist in open-source models like chatGPT and LLAMA. Furthermore, [31] focused on integrating computational tools in the field of chemistry, primarily analyzing the capabilities of LLMs in using integrated tools but neglecting the ability for zero-shot generalization in chemistry reactions. [1] delved more into the abilities of LLMs regarding planning and conducting experiments but did not consider proposing new hypotheses. [38] introduced a new task for open-domain hypothesis induction and created a dataset comprising 50 articles from social science journals. Additionally, they developed a multi-module system for exploring feedback mechanisms. However, all of the above-mentioned literature lacks strict guarantees on the visibility of test data to models, thereby limiting our exploration of the zero-shot generalization capability of LLMs through learning from existing knowledge to propose new hypothesis. Unlike existing works, we have designed datasets based on publication dates, which can easily ensure a strict independence between test data and LLMs.</p>
<h2>A. 2 LLM-driven Autonomous Agents</h2>
<p>Large language models demonstrate exceptional capabilities in tasks such as question answering, program coding, and instruction following. However, they still confront significant challenges related to factual hallucination [41, 23], knowledge outdated [3], and interactions with real-world. To address these challenges, recent research has explored enhancing LLMs by incorporating tools such as search engines [18, 19], calculators [24], code interpreter [44], RESTful APIs [27, 21] and others. The integration of LLMs with tool use, also known as LLM-driven autonomous agents (LAAs), has attracted substantial public attention. These agents are equipped with reasoning [33, 39], planning [25, 29], decision-making [37, 14, 5], and long-term memory capabilities [43, 12], and they are constructed upon the foundation of LLMs. LAAs can autonomously plan sub-goals for complex tasks, execute actions, obtain feedback from the environment, and adjust their behaviors to adapt [40, 36, 26]. LAAs have demonstrated significant potential in addressing complex real-world tasks, including software development [22, 11], drama creation [17], course design [6], chemistry experiments [2] and more. Furthermore, multi-agent collaboration plays a significant role in LAA applications, allowing agents to collaborate and interact to solve problems through various roleplaying scenarios [20, 7, 9, 15]. To the best of our knowledge, there is still a dearth of exploration regarding the use of agents, particularly multi-agents, for scientific discovery. In this paper, our objective is to undertake a preliminary effort to enhance the hypothesis proposing capability of LLMs by harnessing tools and multiple agents, along with conducting an analysis of influencing factors.</p>
<h2>B Implementation Details</h2>
<p>In this section, we delve into further implementation details of our experiments, including information about the constructed dataset and open-source models.</p>
<h2>B. 1 Details of Dataset</h2>
<p>Distribution of Training and Test Sets. We present the publication dates and topic distributions of the various datasets for comparison, as illustrated in Figure 5, where we utilize Nomic Atlas ${ }^{7}$ to visualize the topic distribution of abstracts in both the training and test datasets.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Distribution of the background and hypothesis pairs (BHP) dataset: In the left panel, we present the publication distribution by year for the training and seen test datasets, indicating a steady increase year by year until January 2023. In the center panel, we depict the publication distribution by month for the unseen test dataset, which was sampled from August 2023 and emphasizes the latter part of the month. The right panel displays the distribution of keywords in abstracts from the training, seen test, and unseen test datasets, represented by blue, yellow, and red, respectively.</p>
<h1>B. 2 Details of Models</h1>
<p>We present the meta-information of the open-source models used in our experiments, as shown in Table 4. We have gathered data regarding their pre-training, supervised learning corpus, and release dates to ensure the non-visibility of the unseen test data.</p>
<p>Table 4: To further ensure the non-visibility of the test data, we provide an overview of the related literature corpus within the training set of various LLMs, accompanied by their respective publication dates. The data marked with $\left(^{*}\right)$ is the data generated by people talking to ChatGPT. Our date marking is consistent with ChatGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">SFT Data (Y/M)</th>
<th style="text-align: center;">Released</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo (0-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo (5-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-4*</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Vicuna-33b-v1.3</td>
<td style="text-align: center;">Llama-1</td>
<td style="text-align: center;">ShareGPT (Unknown)</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-7b-chat</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-13b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">openchat-v3.2-super</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Sharegpt4 Dataset (2023/06)</td>
<td style="text-align: center;">2023/09</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">MedAlpaca-13B</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/03)</td>
<td style="text-align: center;">2023/03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatDoctor*</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B</td>
<td style="text-align: center;">Llama-2*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/08*</td>
</tr>
</tbody>
</table>
<h2>C Additional Results</h2>
<p>We have included additional results from human evaluations in Table 5, primarily focusing on correlation scores between word overlap metrics and manual evaluations. Note that we continue to use the same samples used in human evaluation to compute BLEU and ROUGE-L for a fair comparison. We calculate the Pearson and Spearman coefficients between each automatic metric and the average human score. These results reveal that word overlap metrics, such as BLEU and ROUGE-L, exhibit notably lower correlation with manual scores. While BLEU and ROUGE-L may have a high correlation with relevance metrics, they are weak in providing a comprehensive evaluation of the generations. Conversely, evaluations conducted by ChatGPT demonstrate higher correlation with human evaluations, as illustrated in Table 2. However, there is still a significant need to explore advanced metrics, particularly automated ones, in the context of scientific discovery.</p>
<p>Table 5: The table illustrates the correlations between automatic metrics and human evaluations. We annotate the Pearson and Spearman scores after each correlation score, denoting them as $r$ and $\rho$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Word Overlap</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU $(r / \rho)$</td>
<td style="text-align: center;">ROUGE-L $(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
</tr>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo(0-shot)</td>
<td style="text-align: center;">16.59(0.03/0.01)</td>
<td style="text-align: center;">29.87(-0.04/-0.05)</td>
<td style="text-align: center;">1.90(0.87/0.78)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo(5-shot)*</td>
<td style="text-align: center;">14.99(-0.09/0.12)</td>
<td style="text-align: center;">27.51(-0.33/-0.35)</td>
<td style="text-align: center;">1.96(0.80/0.78)</td>
<td style="text-align: center;">2.15(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Llama-2-70b-chat(0-shot)</td>
<td style="text-align: center;">9.64(-0.21/-0.20)</td>
<td style="text-align: center;">22.17(-0.31/-0.28)</td>
<td style="text-align: center;">2.04(0.89/0.84)</td>
<td style="text-align: center;">1.96(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)</td>
<td style="text-align: center;">9.42(-0.58/-0.65)</td>
<td style="text-align: center;">20.59(-0.47/-0.42)</td>
<td style="text-align: center;">2.20(0.96/0.90)</td>
<td style="text-align: center;">2.33(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)*</td>
<td style="text-align: center;">9.60(-0.16/-0.10)</td>
<td style="text-align: center;">19.99(-0.15/-0.17)</td>
<td style="text-align: center;">2.01(0.97/0.94)</td>
<td style="text-align: center;">2.08(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(0-shot)</td>
<td style="text-align: center;">11.42(0.21/0.36)</td>
<td style="text-align: center;">24.11(0.29/0.49)</td>
<td style="text-align: center;">1.91(0.90/0.75)</td>
<td style="text-align: center;">1.81(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(5-shot)</td>
<td style="text-align: center;">9.86(-0.28/-0.37)</td>
<td style="text-align: center;">23.52(-0.17/-0.24)</td>
<td style="text-align: center;">2.01(0.85/0.89)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">PMC-LLaMA-13B(0-shot)</td>
<td style="text-align: center;">8.19(0.32/0.39)</td>
<td style="text-align: center;">21.85(0.18/0.27)</td>
<td style="text-align: center;">1.41(0.73/0.73)</td>
<td style="text-align: center;">1.88(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B(5-shot)*</td>
<td style="text-align: center;">5.52(0.06/-0.01)</td>
<td style="text-align: center;">13.64(0.26/0.23)</td>
<td style="text-align: center;">1.97(0.95/0.94)</td>
<td style="text-align: center;">1.92(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">21.48(-0.00/0.00)</td>
<td style="text-align: center;">27.83(0.17/0.27)</td>
<td style="text-align: center;">1.79(0.83/0.85)</td>
<td style="text-align: center;">1.77(1.00/1.00)</td>
</tr>
</tbody>
</table>
<h1>D Case Study</h1>
<p>In this section, we present several generated hypotheses from various models and provide examples of the evaluation process, step by step, using ChatGPT.</p>
<h2>D. 1 Generated Hypothesis</h2>
<p>We compare the generated hypotheses of different LLMs selected in human evaluation. The selected medicine literature was published in August 2023 [4], which proposed the power law index as an early marker of myocardial infarction. As shown in Table 6, some responses like gpt-3.5-turbo (0-shot) propose this hypothesis from zero, based only on the background. This indicates that LLMs have great potential in proposing highly novel and verifiable hypotheses and could play a significant role in scientific discovery. Furthermore, we showcase the proposed hypotheses in a multi-agent collaboration involving three roles: Analyst, Scientist, and Critic, working together to generate hypotheses across 6 rounds, as shown in Table 7.</p>
<p>Table 6: This table illustrates the hypothesis proposals from different LLMs.</p>
<h2>InSTRUCTED LLMS CASE</h2>
<h2>Background:</h2>
<p>(1) In the early stage of MI, $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples with an AUC $&gt;0.7$.
(2) In the mid-stage and late-stage of MI, E2, E3, and Etotal exhibit excellent discriminating performance with an AUC $&gt;0.6$.
(3) All cardiac tissues display a universal two-stage power-law rheology at different time scales, independent of their disease status.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The sequential order of MI-related changes in cardiac tissues starts from the cytoplasmic level and progresses to the cellular and tissue levels.
(2) Pathological changes at subcellular, cellular, and tissue levels are accompanied by mechanical alterations.
(3) The proposed mechanical markers have diagnostic potential for assessing the progression of MI and studying other cardiovascular diseases.</p>
<h2>gpt-3.5-turbo (0-shot):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long can be used as a reliable biomarker for early detection and differentiation of myocardial infarction (MI) from healthy controls.
(2) E2, E3, and Etotal can serve as effective biomarkers for distinguishing between mid-stage and late-stage MI patients with high accuracy.
(3) The universal two-stage power-law rheology observed in cardiac tissues may play a crucial role in maintaining tissue integrity and function, regardless of the presence or absence of disease.</p>
<h1>gpt-3.5-turbo (5-shot):</h1>
<p>(1) The levels of $\alpha$ short and $\alpha$ long may serve as potential biomarkers for early detection and diagnosis of myocardial infarction (MI).
(2) E2, E3, and Etotal may have utility as biomarkers for assessing the progression and severity of MI in the mid and late stages.
(3) The universal two-stage power-law rheology observed in cardiac tissues suggests a fundamental biomechanical principle underlying tissue behavior, regardless of disease status.</p>
<h2>Llama-2-70b-chat (0-shot):</h2>
<p>(1) E2, E3, and Etotal can also track and distinguish MI samples from healthy control samples in the early stage of MI, with an AUC $&gt;0.7$, similar to $\alpha$ short and $\alpha$ long.
(2) The universal two-stage power-law rheology of cardiac tissues is related to the alteration of the extracellular matrix and the degradation of collagen in the early stage of MI, which can be detected by E2, E3, and Etotal.
(3) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI is affected by the changes in the levels of collagen and proteoglycans in the extracellular matrix, which can be evaluated by measuring the mechanical properties of cardiac tissues using a uniaxial tensiometer.</p>
<h2>Llama-2-70b-chat (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI is attributed to their ability to capture the subtle changes in cardiac tissue mechanics caused by the onset of fibrosis.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the mid-stage and late-stage of MI due to their sensitivity to changes in collagen content and cross-linking, which are hallmarks of fibrosis progression.
(3) The universal two-stage power-law rheology observed in all cardiac tissues, independent of disease status, suggests a common mechanism of mechanical regulation that can be exploited for early disease detection and monitoring.</p>
<h2>Llama-2-70b-chat (5-shot)*:</h2>
<p>(1) The $\alpha$ short and $\alpha$ long parameters can detect changes in myocardial stiffness in the early stages of myocardial infarction (MI).
(2) E2, E3, and Etotal can differentiate between healthy controls and MI samples in the mid and late stages.
(3) The universal two-stage power-law rheology in cardiac tissues may be useful in developing novel therapeutic approaches for MI.</p>
<h2>WizardLM-70B-V1.0 (0-shot):</h2>
<p>(1) $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples in all stages of MI with an AUC $&gt;0.7$.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the early stage of MI with an AUC $&gt;0.6$.
(3) The two-stage power-law rheology observed in all cardiac tissues is related to the progression of MI.</p>
<h2>WizardLM-70B-V1.0 (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI may be attributed to their sensitivity to early molecular changes in the myocardium.
(2) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI may be due to their ability to reflect the progression of myocardial damage and remodeling.</p>
<p>(3) The universal two-stage power-law rheology observed in all cardiac tissues may be a fundamental property of cardiac tissue biomechanics, potentially useful for developing diagnostic and therapeutic strategies for MI.</p>
<h1>PMC-LLaMA-13B (0-shot):</h1>
<p>(1) $\alpha$ short and $\alpha$ long are effective biomarkers for distinguishing MI samples from healthy control samples both in the early stage and the late stage of MI.
(2) E2 and E3 (Etotal) are likely to serve as effective biomarkers for discriminating mid-stage or late-stage MI samples from healthy control samples.
(3) Cardiac tissues display a universal two-stage power-law rheology that is independent of their disease status.</p>
<h2>PMC-LLaMA-13B (5-shot):</h2>
<p>(1) Further comprehensive validation of the proposed framework is needed to elucidate the pathological changes in MI patients.
(2) Consideration of multiple markers is important.
(3) The influence of various external factors should be incorporated for accurate interpretation of the mechanical markers.</p>
<h2>WizardLM-13B-V1.2 (SFT):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long features can provide a more robust identification of MI samples.
(2) E2, E3, and Etotal features can serve as independent predictors of MI.
(3) The universal two-stage power-law rheology observed in all cardiac tissues may have diagnostic implications.</p>
<p>Table 7: This table illustrates a case from multi-agent collaboration on hypothesis proposing.</p>
<h2>Multi-Agent Case</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>Dialogue process of multi-agent collaboration.</h1>
<p>(Round 1) Analyst: Based on the provided research background, I have identified the following keywords and topics:</p>
<p>Keywords:</p>
<ul>
<li>Digital electrophysiology</li>
<li>Telemonitoring</li>
<li>Cardiac implantable electronic devices (CIEDs)</li>
<li>Wearable devices</li>
<li>Cardiac rhythm monitoring</li>
<li>Photoplethysmography</li>
</ul>
<p>Topics:</p>
<ol>
<li>Advancements in digital electrophysiology and telemonitoring of CIEDs</li>
<li>Benefits of telemonitoring in improving patient outcomes</li>
<li>Accuracy and reliability of wearable devices for cardiac rhythm monitoring using photoplethysmography</li>
</ol>
<p>Preliminary insights:</p>
<ul>
<li>The field of digital electrophysiology has seen significant development in the past 25 years, particularly in the areas of telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring.</li>
<li>Telemonitoring of CIEDs has been shown to have positive impacts, such as reducing the time to detect clinically actionable events and improving patient outcomes.</li>
<li>Wearable devices, including smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</li>
</ul>
<p>Based on these insights, it can be hypothesized that the advancements in telemonitoring and wearable devices have led to improved cardiac rhythm monitoring and patient outcomes. However, further analysis and collaboration with the Scientist and Critic are needed to refine and validate this hypothesis.
(Round 2) Scientist: Based on the insights provided by the Analyst and the research background, I propose the following hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes and more accurate detection of clinically actionable events.</p>
<p>This hypothesis suggests that the development and implementation of telemonitoring technologies for CIEDs, along with the increasing accuracy and reliability of wearable devices using photoplethysmography, have positively impacted patient outcomes by reducing the time to detect clinically actionable events. By continuously monitoring cardiac rhythm, these advancements have the potential to provide timely interventions and improve overall patient care.</p>
<p>The hypothesis is grounded in the existing literature, which highlights the benefits of telemonitoring in improving patient outcomes and the increasing accuracy of wearable devices for cardiac rhythm monitoring. However, further collaboration with the Critic is necessary to evaluate the novelty, coherence, and scientific validity of the hypothesis.</p>
<p>(Round 3) Critic: The hypothesis proposed by the Scientist is well-aligned with the research background and the insights provided by the Analyst. It effectively captures the essence of the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring. The hypothesis also highlights the potential positive impact on patient outcomes and the detection of clinically actionable events.</p>
<p>However, there are a few areas that could be further refined or expanded upon in the hypothesis:</p>
<ol>
<li><strong>Specific outcomes</strong>: The hypothesis mentions ""improved patient outcomes," but it would be beneficial to specify the specific outcomes that are expected to improve. For example, are we referring to reduced hospitalizations, improved quality of life, or decreased mortality rates?</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The hypothesis briefly mentions the potential for timely interventions and improved patient care, but it would be helpful to elaborate on the specific mechanisms through which the advancements in telemonitoring and wearable devices are expected to achieve these outcomes. Are there specific interventions or treatments that can be administered based on the data collected from these devices?</li>
<li>${ }^{<em> </em>}$ Scientific validity**: While the hypothesis is grounded in the existing literature, it would be valuable to provide some references or studies that support the claims made in the hypothesis. This would enhance the scientific validity of the hypothesis and demonstrate that it is not just speculative but based on empirical evidence.</li>
</ol>
<p>To refine the hypothesis, I would recommend the Scientist to consider incorporating these suggestions and providing more specific details and references to support the claims made. This will strengthen the coherence and scientific validity of the hypothesis.</p>
<p>Analyst, based on the feedback provided, please re-evaluate the research background and provide any additional insights or interpretations that could further refine the hypothesis.
(Round 4) Analyst: Based on the feedback from the Critic, I have re-evaluated the research background and identified additional insights:</p>
<ol>
<li><strong>Specific outcomes</strong>: The research background mentions that telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes. These outcomes could include early detection of arrhythmias, reduced hospitalizations due to cardiac events, improved quality of life, and decreased mortality rates. It would be beneficial to specify one or more of these specific outcomes in the hypothesis.</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The advancements in telemonitoring and wearable devices can potentially lead to improved patient outcomes through several mechanisms. For example, continuous monitoring of cardiac rhythm can enable early detection of abnormal rhythms, allowing for timely interventions such as medication adjustments or electrical cardioversion. Additionally, the data collected from these devices can provide valuable insights for personalized treatment plans and remote patient management. Including these specific mechanisms in the hypothesis would enhance its coherence.</li>
<li>${ }^{<em> </em>}$ Scientific validity**: To strengthen the scientific validity of the hypothesis, it would be valuable to reference specific studies or research that support the claims made. For example, citing studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events or improving patient outcomes would provide empirical evidence to support the hypothesis.</li>
</ol>
<p>Incorporating these additional insights and suggestions, the refined hypothesis could be:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis takes into account the specific outcomes, mechanisms, and scientific validity, as suggested by the Critic. It provides a more comprehensive and well-supported statement that captures the essence of the research background and the collaborative insights.
(Round 5) Scientist: Based on the additional insights and suggestions provided by the Analyst and the Critic, I propose the following refined hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis incorporates the specific outcomes mentioned in the research background, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. It also expands on the mechanisms through which these advancements are expected to achieve these outcomes, including early detection of abnormal rhythms, timely interventions, and personalized treatment plans. Additionally, the hypothesis acknowledges the importance of scientific validity by referencing studies that support the claims made.</p>
<p>The refined hypothesis is both innovative and scientifically grounded, as it builds upon the existing literature and incorporates the collaborative insights provided by the Analyst and the Critic. It provides a logical and well-supported statement that can be further evaluated and tested.
(Round 6) Critic: The refined hypothesis proposed by the Scientist is well-structured and addresses the feedback provided by the Critic. It incorporates specific outcomes, mechanisms, and scientific validity, making it a comprehensive and scientifically robust statement.</p>
<p>Final Answer:
(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>D. 2 Evaluation Examples</h1>
<p>We provide evaluation results conducted by ChatGPT in Table 8. ChatGPT offers a step-by-step explanation for each metric, ultimately providing independent scores in integer format.</p>
<p>Table 8: This table illustrates a case for hypothesis evaluation using ChatGPT.</p>
<h2>CASE OF ChatGPT as ANNOTATOR</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h2>Evaluation Results</h2>
<p>Novelty:
Explanation: The generated scientific hypothesis builds upon the background information by stating that the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes. It further suggests that these advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis goes beyond the background by proposing specific mechanisms through which the advancements in telemonitoring and wearable devices contribute to improved patient outcomes. It also mentions the supporting evidence from studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. Overall, the hypothesis provides new insights and expands upon the existing knowledge in the field.
Score: 2</p>
<h2>Relevance:</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/nomic-ai/nomic&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-461 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-461</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-461</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-43922261</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1805.08949v1.pdf" target="_blank">Learning to mine aligned code and natural language pairs from stack overflow</a></p>
                <p><strong>Paper Abstract:</strong> For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e461.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e461.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Title-Code Misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-Language Question Title vs. Answer Code Misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch where the SO question title (intent) does not align exactly with code present in answers because code blocks often contain context, imports, variable setup, or unrelated examples rather than the implementation of the stated intent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stack Overflow NL–Code Mining Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A logistic-regression classifier that ranks candidate line-contiguous code snippets from SO answers against the question title (treated as the NL intent), using hand-crafted structural features and neural correspondence features.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title (how-to intent)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Answer code snippets (full code blocks and all contiguous line sub-snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete / imprecise NL-to-code alignment</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Question titles are used as proxies for intent but many code blocks in answers include import statements, variable definitions, printouts, interpreter output, or unrelated demonstrations; thus a direct heuristic pairing (e.g., title -> accepted answer full block) often pairs the NL intent with code that does not implement that intent.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>candidate selection / pairing stage (intent ↔ candidate snippet matching)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual annotation of gold-standard intent/snippet pairs and comparison of mined pairs to gold labels (error analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Precision–Recall and ROC curves vs. annotated gold set (5-fold cross-validation); precision at given recall levels reported; baseline precisions (e.g., Random 0.10 Python, 0.06 Java) used to quantify noise</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Heuristic pairings (title→full accepted-answer block) miss true fine-grained snippets and introduce noise; paper shows their full method increases recall while improving precision over baselines — e.g., able to extract up to an order of magnitude more aligned pairs at no loss in accuracy, or halve errors at constant extraction size (reported qualitatively in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High: in annotated data, ~70% of Python and ~46% of Java cases had the best-aligned snippet that was not a full code block (i.e., sub-block needed); many SO answers contain context lines.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Simplifying assumption that question title equals intent, heterogeneous content in SO answer blocks (context vs. implementation), and prior heuristic mining strategies that only consider full blocks</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Generate all line-contiguous sub-snippets as candidates; train a classifier with structural features that detect imports/assignments/context and correspondence features to score intent↔snippet semantics; use manual small-scale annotation to supervise classifier</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: combined Structural+Correspondence classifier substantially outperforms baselines (higher precision at given recall and higher AUC); structural features alone already outperform heuristics; exact numeric improvements shown in PR/ROC plots in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / NL–code mining (program synthesis / code retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to mine aligned code and natural language pairs from stack overflow', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e461.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e461.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granularity Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Code Block vs. Sub-Snippet Granularity Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior heuristics assume the entire code block answers the intent, but many correct implementations are substrings (line-contiguous fragments) within larger code blocks, leading to missed or noisy pairs when mining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stack Overflow NL–Code Mining Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Same classifier; candidate generation enumerates every contiguous set of lines within answer code blocks to enable sub-block matching.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title (how-to intent)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Answer code snippets (full blocks vs. contiguous sub-snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>granularity mismatch / incomplete selection (full-block bias)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Heuristic methods selecting whole code blocks miss correct sub-snippets; structural-only models tend to select full blocks or the last line (because features like FullBlock and EndOfBlock have high weights), causing false positives (e.g., print statements or pass lines) and reducing fine-grained alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>candidate-generation and ranking (snippet granularity selection)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual annotation showing gold snippets often are sub-blocks + analysis of model selections (which positions are favored)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantify proportion of gold snippets that are not full blocks (table statistics); compare precision/recall when allowing sub-snippets vs. full-block-only baselines</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Allowing sub-snippets dramatically increases coverage (recall) and enables extraction of many correct pairs that full-block heuristics miss; structural-only models still bias to full-blocks and thus underperform on fine-grained retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Frequent: ~70% of Python gold snippets and ~46% of Java gold snippets were not full code blocks (paper Table 1), showing high prevalence of this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Prior mining heuristics and some structural features implicitly prefer whole blocks; SO answers commonly include combination of context + multiple implementations in same block</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Enumerate all contiguous line sub-snippets; train classifier on annotated sub-snippet labels; combine structural and statistical correspondence features to choose correct granularity</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective as reported: full model improves recall and precision vs. heuristics; structural features help capture block-level signals while correspondence helps select smaller correct fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / NL–code mining</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to mine aligned code and natural language pairs from stack overflow', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e461.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e461.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context-Token Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context Tokens (imports / assignments) Causing False Correspondences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NL↔code correspondence models assign high probability to snippets that contain tokens (e.g., library names in imports) strongly associated with intents, even when those tokens are contextual and not the implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Correspondence Component (attentional encoder–decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Neural MT style encoder–decoder (with attention) trained on heuristically paired full accepted-answer single-block pairs to estimate P(S|I) and P(I|S); outputs log-probabilities used as features.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title (intent)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Answer code block (often starting with import statements)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>feature/representation bias leading to false positives (context vs. implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Correspondence model trained on full blocks often sees import/library names (e.g., 'import datetime' ↔ 'current time') that correlate strongly with intent words, causing high P(I|S) even when the snippet is only context and not the actual implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>correspondence model training data and scoring (feature computation)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Ablation and error analysis: observing cases where correspondence scores are high but structural features indicate context; qualitative examples and model failure cases discussed</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Compare performance of Correspondence-only vs. Structural-only vs. Full models; analyze error cases (qualitative) and precision/recall differences</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Correspondence-only features underperform structural ones for filtering contextual statements; however, when combined, correspondence helps fine-grained selection — alone it yields false positives due to context-token bias.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified as single percentage, but noted as a recurring failure mode in qualitative analysis and a reason correspondence-only underperforms, especially for Python where imports correlate strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Noisy training data: correspondence model trained on full accepted-answer blocks (many include imports); strong lexical correlates between library names and intents</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Combine correspondence features with structural features that detect imports/assignments and penalize context; normalize scores per-page; include structural binary flags like ContainsImport</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: combination reduces false positives due to context tokens and yields highest overall performance in PR/ROC; paper reports combined model outperforms either feature set alone.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / machine translation for NL–code correspondence</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to mine aligned code and natural language pairs from stack overflow', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e461.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e461.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Annotation Coverage Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incomplete or Noisy Gold Annotation Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual annotation of gold intent↔snippet pairs is difficult and incomplete, leading to evaluation and training weaknesses (missing correct snippets, annotation errors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Annotation Protocol and Gold-Standard Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Manually curated labels (intent/context/snippet) for a limited set of SO questions (527 Python and 330 Java annotations) used both as gold labels for classifier training and for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title and optionally re-written intent</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Annotated snippet lines (may be sub-snippets or full blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>annotation incompleteness / labeling error</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Annotators sometimes miss valid alternative implementations in the same post, mislabel long/complex solutions, or fail to detect inline blocks; annotation protocol refinement was necessary and residual errors remain (e.g., some mined snippets judged false were correct—annotation error).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>gold-label creation / evaluation data</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual inspection and error analysis of 'false positives' where model output seems correct but gold marks it incorrect; authors acknowledge annotation threats to validity</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counts of annotated items (527 Python, 330 Java); removal of 'not sure' and unparsable snippets; qualitative examples of annotation errors cited</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Annotation error can bias model training and evaluation (e.g., legitimate mined snippets labeled as incorrect lower apparent precision); small annotated set may hinder generalization (threat to validity).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Non-trivial: authors explicitly list multiple annotation threats and cite concrete examples (e.g., S1 for I9 labeled wrong due to annotation error); exact rate not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguity in intent/code mapping, difficulty distinguishing unusual-but-correct solutions from incorrect ones, limited annotation resources</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Iteratively refine annotation protocol and interface; have experienced annotator re-annotate final set; discard pilot annotations; provide guidance and 'not sure' option</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Improves consistency but does not eliminate errors; authors still observe residual annotation errors and list them as threats to validity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / dataset curation for NL–code tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to mine aligned code and natural language pairs from stack overflow', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e461.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e461.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Incomplete Snippet Error</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mined Snippets Missing Required Context or Definitions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extracted sub-snippets sometimes omit intermediate statements (e.g., definitions of helper variables/functions) required for the snippet to be semantically or operationally complete.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stack Overflow NL–Code Mining Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Classifier that may select concise fragments that appear to implement intent but actually lack supporting context (e.g., missing variable initialization or helper definitions).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title (intent)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Extracted contiguous sub-snippets</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete implementation extraction (missing dependencies)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The miner can select a fragment that looks like the implementation but is missing earlier lines necessary for correctness (e.g., keys_to_keep definition omitted), causing the snippet to be non-functional or misleading as an isolated training example.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>snippet extraction / granularity selection</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Qualitative error analysis comparing mined snippets to gold; manual inspection of top-ranked snippets</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported as qualitative failure cases; specific example S3 for I3 (Python) flagged as missing keys_to_keep definition</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to mined pairs that are not directly usable for downstream tasks (synthesis, execution) and can degrade models trained on them if dependencies are not available; quantitative effect not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in error analysis; not given as a global percentage but listed as one of four main error sources</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Sub-snippet selection without dependency analysis; lack of static/dynamic analysis to ensure self-containedness</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Potential mitigations (discussed as future work): detect and include necessary context lines (imports, variable/function definitions) or enforce syntactic/semantic closure (e.g., include surrounding definitions) when extracting snippets</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in paper; authors list this as an open failure case requiring future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / dataset extraction for code synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to mine aligned code and natural language pairs from stack overflow', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e461.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e461.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spurious/Counterexample Cases</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High Correspondence for Spurious or Counterexample Code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cases where code shown in answers as counterexamples or related-but-incorrect implementations still receive high correspondence scores and are selected as matches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Correspondence Component + Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The correspondence model (trained on large heuristic pairs) learns lexical/semantic associations that can make counterexample or related implementation snippets appear good matches for the intent.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title (intent)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Answer code snippets including counterexamples or related implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>semantic misinterpretation / counterexample confusion</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Examples in answers that are explicitly counterexamples or alternative but undesired implementations (e.g., code demonstrating what not to do) may contain tokens strongly associated with the intent and thus receive high P(I|S)/P(S|I), leading the miner to select them incorrectly.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>scoring stage (correspondence probabilities used as features)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Error analysis of top predictions (human inspection of outputs vs. original SO post context)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative examples provided (e.g., S1 for I4), noting that correspondence features alone cannot detect that a snippet is a counterexample</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Generates false positives; correspondence features alone underperform and need to be combined with structural/contextual cues to avoid selecting counterexamples.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not numerically quantified; identified as a recurrent failure mode in qualitative analysis</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Correspondence model lacks discourse-level awareness (cannot detect negation/explicit counterexample markers) and was trained on noisy data lacking such labels</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Combine correspondence with structural/contextual features and potentially train models to detect negative/counterexample contexts; require page-level normalization and additional features</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Combination reduces but does not eliminate such errors; explicit mitigation not fully implemented in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / NL–code mining</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to mine aligned code and natural language pairs from stack overflow', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e461.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e461.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structural Feature Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Heuristics Induce Positional or Size Biases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hand-crafted structural features (e.g., EndOfBlock, NumLines buckets) induce learned biases (e.g., favoring last line or full-block) that cause systematic mis-selections of non-implementing lines like print or pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Structural Feature Set for Snippet Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A set of language-independent binary and bucketed features (FullBlock, StartOfBlock, ContainsImport, NumLinesX, EndsOfBlock, etc.) used by the logistic classifier to identify candidate snippets likely to be implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow question title (intent)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Answer code snippets (analyzed for structure/position)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>heuristic-induced bias causing false positives</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because some structural features correlate with correct answers in training (e.g., many correct answers are whole blocks or last-line implementations), the learned classifier can give undue weight to positional features and consequently prefer non-implementing last lines (print statements) or full blocks even when not appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>feature design and classifier weighting stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Analysis of top-100 ranked candidates showing Structural model's selections (all full blocks in top-100 for Structural), and inspection of model weights (e.g., high weight for LineNum=1 and EndsCodeBlock)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Compare composition of top-ranked results across models (Structural vs. Correspondence vs. Full); PR/ROC comparisons and qualitative inspection</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Structural-only model performs reasonably well overall (especially for Java where full-block answers are common) but lacks fine granularity and makes systematic selection errors when partial-snippet is needed, reducing top-k precision for those cases.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed pattern: out of top-100 for Structural, all were full code blocks (contrast: only 21 for Correspondence), indicating a strong prevalence of this bias in Structural-only model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Simplicity and sparsity of hand-crafted features combined with limited annotated training data reinforcing coarse heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Combine with correspondence features that can detect semantic fit and penalize contextual lines (imports/assignments); add combination features (e.g., ¬StartWithAssign + NumLines1) to refine selection</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Combination of Structural+Correspondence alleviates some bias and improves overall performance, as shown by improved ROC/AUC and PR curves.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / NL–code mining</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to mine aligned code and natural language pairs from stack overflow', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow <em>(Rating: 2)</em></li>
                <li>From query to usable code: an analysis of Stack Overflow code snippets <em>(Rating: 2)</em></li>
                <li>AutoComment: Mining question and answer sites for automatic comment generation <em>(Rating: 1)</em></li>
                <li>A Syntactic Neural Model for General-Purpose Code Generation <em>(Rating: 1)</em></li>
                <li>Summarizing Source Code using a Neural Attention Model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-461",
    "paper_id": "paper-43922261",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Title-Code Misalignment",
            "name_full": "Natural-Language Question Title vs. Answer Code Misalignment",
            "brief_description": "Mismatch where the SO question title (intent) does not align exactly with code present in answers because code blocks often contain context, imports, variable setup, or unrelated examples rather than the implementation of the stated intent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Stack Overflow NL–Code Mining Classifier",
            "system_description": "A logistic-regression classifier that ranks candidate line-contiguous code snippets from SO answers against the question title (treated as the NL intent), using hand-crafted structural features and neural correspondence features.",
            "nl_description_type": "Stack Overflow question title (how-to intent)",
            "code_implementation_type": "Answer code snippets (full code blocks and all contiguous line sub-snippets)",
            "gap_type": "incomplete / imprecise NL-to-code alignment",
            "gap_description": "Question titles are used as proxies for intent but many code blocks in answers include import statements, variable definitions, printouts, interpreter output, or unrelated demonstrations; thus a direct heuristic pairing (e.g., title -&gt; accepted answer full block) often pairs the NL intent with code that does not implement that intent.",
            "gap_location": "candidate selection / pairing stage (intent ↔ candidate snippet matching)",
            "detection_method": "Manual annotation of gold-standard intent/snippet pairs and comparison of mined pairs to gold labels (error analysis)",
            "measurement_method": "Precision–Recall and ROC curves vs. annotated gold set (5-fold cross-validation); precision at given recall levels reported; baseline precisions (e.g., Random 0.10 Python, 0.06 Java) used to quantify noise",
            "impact_on_results": "Heuristic pairings (title→full accepted-answer block) miss true fine-grained snippets and introduce noise; paper shows their full method increases recall while improving precision over baselines — e.g., able to extract up to an order of magnitude more aligned pairs at no loss in accuracy, or halve errors at constant extraction size (reported qualitatively in paper).",
            "frequency_or_prevalence": "High: in annotated data, ~70% of Python and ~46% of Java cases had the best-aligned snippet that was not a full code block (i.e., sub-block needed); many SO answers contain context lines.",
            "root_cause": "Simplifying assumption that question title equals intent, heterogeneous content in SO answer blocks (context vs. implementation), and prior heuristic mining strategies that only consider full blocks",
            "mitigation_approach": "Generate all line-contiguous sub-snippets as candidates; train a classifier with structural features that detect imports/assignments/context and correspondence features to score intent↔snippet semantics; use manual small-scale annotation to supervise classifier",
            "mitigation_effectiveness": "Effective: combined Structural+Correspondence classifier substantially outperforms baselines (higher precision at given recall and higher AUC); structural features alone already outperform heuristics; exact numeric improvements shown in PR/ROC plots in paper.",
            "domain_or_field": "Software engineering / NL–code mining (program synthesis / code retrieval)",
            "reproducibility_impact": true,
            "uuid": "e461.0",
            "source_info": {
                "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Granularity Mismatch",
            "name_full": "Full Code Block vs. Sub-Snippet Granularity Mismatch",
            "brief_description": "Prior heuristics assume the entire code block answers the intent, but many correct implementations are substrings (line-contiguous fragments) within larger code blocks, leading to missed or noisy pairs when mining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Stack Overflow NL–Code Mining Classifier",
            "system_description": "Same classifier; candidate generation enumerates every contiguous set of lines within answer code blocks to enable sub-block matching.",
            "nl_description_type": "Stack Overflow question title (how-to intent)",
            "code_implementation_type": "Answer code snippets (full blocks vs. contiguous sub-snippets)",
            "gap_type": "granularity mismatch / incomplete selection (full-block bias)",
            "gap_description": "Heuristic methods selecting whole code blocks miss correct sub-snippets; structural-only models tend to select full blocks or the last line (because features like FullBlock and EndOfBlock have high weights), causing false positives (e.g., print statements or pass lines) and reducing fine-grained alignment.",
            "gap_location": "candidate-generation and ranking (snippet granularity selection)",
            "detection_method": "Manual annotation showing gold snippets often are sub-blocks + analysis of model selections (which positions are favored)",
            "measurement_method": "Quantify proportion of gold snippets that are not full blocks (table statistics); compare precision/recall when allowing sub-snippets vs. full-block-only baselines",
            "impact_on_results": "Allowing sub-snippets dramatically increases coverage (recall) and enables extraction of many correct pairs that full-block heuristics miss; structural-only models still bias to full-blocks and thus underperform on fine-grained retrieval.",
            "frequency_or_prevalence": "Frequent: ~70% of Python gold snippets and ~46% of Java gold snippets were not full code blocks (paper Table 1), showing high prevalence of this gap.",
            "root_cause": "Prior mining heuristics and some structural features implicitly prefer whole blocks; SO answers commonly include combination of context + multiple implementations in same block",
            "mitigation_approach": "Enumerate all contiguous line sub-snippets; train classifier on annotated sub-snippet labels; combine structural and statistical correspondence features to choose correct granularity",
            "mitigation_effectiveness": "Effective as reported: full model improves recall and precision vs. heuristics; structural features help capture block-level signals while correspondence helps select smaller correct fragments.",
            "domain_or_field": "Software engineering / NL–code mining",
            "reproducibility_impact": true,
            "uuid": "e461.1",
            "source_info": {
                "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Context-Token Bias",
            "name_full": "Context Tokens (imports / assignments) Causing False Correspondences",
            "brief_description": "NL↔code correspondence models assign high probability to snippets that contain tokens (e.g., library names in imports) strongly associated with intents, even when those tokens are contextual and not the implementation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Neural Correspondence Component (attentional encoder–decoder)",
            "system_description": "Neural MT style encoder–decoder (with attention) trained on heuristically paired full accepted-answer single-block pairs to estimate P(S|I) and P(I|S); outputs log-probabilities used as features.",
            "nl_description_type": "Stack Overflow question title (intent)",
            "code_implementation_type": "Answer code block (often starting with import statements)",
            "gap_type": "feature/representation bias leading to false positives (context vs. implementation)",
            "gap_description": "Correspondence model trained on full blocks often sees import/library names (e.g., 'import datetime' ↔ 'current time') that correlate strongly with intent words, causing high P(I|S) even when the snippet is only context and not the actual implementation.",
            "gap_location": "correspondence model training data and scoring (feature computation)",
            "detection_method": "Ablation and error analysis: observing cases where correspondence scores are high but structural features indicate context; qualitative examples and model failure cases discussed",
            "measurement_method": "Compare performance of Correspondence-only vs. Structural-only vs. Full models; analyze error cases (qualitative) and precision/recall differences",
            "impact_on_results": "Correspondence-only features underperform structural ones for filtering contextual statements; however, when combined, correspondence helps fine-grained selection — alone it yields false positives due to context-token bias.",
            "frequency_or_prevalence": "Not quantified as single percentage, but noted as a recurring failure mode in qualitative analysis and a reason correspondence-only underperforms, especially for Python where imports correlate strongly.",
            "root_cause": "Noisy training data: correspondence model trained on full accepted-answer blocks (many include imports); strong lexical correlates between library names and intents",
            "mitigation_approach": "Combine correspondence features with structural features that detect imports/assignments and penalize context; normalize scores per-page; include structural binary flags like ContainsImport",
            "mitigation_effectiveness": "Partially effective: combination reduces false positives due to context tokens and yields highest overall performance in PR/ROC; paper reports combined model outperforms either feature set alone.",
            "domain_or_field": "Software engineering / machine translation for NL–code correspondence",
            "reproducibility_impact": true,
            "uuid": "e461.2",
            "source_info": {
                "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Annotation Coverage Gap",
            "name_full": "Incomplete or Noisy Gold Annotation Coverage",
            "brief_description": "Manual annotation of gold intent↔snippet pairs is difficult and incomplete, leading to evaluation and training weaknesses (missing correct snippets, annotation errors).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Annotation Protocol and Gold-Standard Dataset",
            "system_description": "Manually curated labels (intent/context/snippet) for a limited set of SO questions (527 Python and 330 Java annotations) used both as gold labels for classifier training and for evaluation.",
            "nl_description_type": "Stack Overflow question title and optionally re-written intent",
            "code_implementation_type": "Annotated snippet lines (may be sub-snippets or full blocks)",
            "gap_type": "annotation incompleteness / labeling error",
            "gap_description": "Annotators sometimes miss valid alternative implementations in the same post, mislabel long/complex solutions, or fail to detect inline blocks; annotation protocol refinement was necessary and residual errors remain (e.g., some mined snippets judged false were correct—annotation error).",
            "gap_location": "gold-label creation / evaluation data",
            "detection_method": "Manual inspection and error analysis of 'false positives' where model output seems correct but gold marks it incorrect; authors acknowledge annotation threats to validity",
            "measurement_method": "Counts of annotated items (527 Python, 330 Java); removal of 'not sure' and unparsable snippets; qualitative examples of annotation errors cited",
            "impact_on_results": "Annotation error can bias model training and evaluation (e.g., legitimate mined snippets labeled as incorrect lower apparent precision); small annotated set may hinder generalization (threat to validity).",
            "frequency_or_prevalence": "Non-trivial: authors explicitly list multiple annotation threats and cite concrete examples (e.g., S1 for I9 labeled wrong due to annotation error); exact rate not quantified.",
            "root_cause": "Ambiguity in intent/code mapping, difficulty distinguishing unusual-but-correct solutions from incorrect ones, limited annotation resources",
            "mitigation_approach": "Iteratively refine annotation protocol and interface; have experienced annotator re-annotate final set; discard pilot annotations; provide guidance and 'not sure' option",
            "mitigation_effectiveness": "Improves consistency but does not eliminate errors; authors still observe residual annotation errors and list them as threats to validity.",
            "domain_or_field": "Software engineering / dataset curation for NL–code tasks",
            "reproducibility_impact": true,
            "uuid": "e461.3",
            "source_info": {
                "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Incomplete Snippet Error",
            "name_full": "Mined Snippets Missing Required Context or Definitions",
            "brief_description": "Extracted sub-snippets sometimes omit intermediate statements (e.g., definitions of helper variables/functions) required for the snippet to be semantically or operationally complete.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Stack Overflow NL–Code Mining Classifier",
            "system_description": "Classifier that may select concise fragments that appear to implement intent but actually lack supporting context (e.g., missing variable initialization or helper definitions).",
            "nl_description_type": "Stack Overflow question title (intent)",
            "code_implementation_type": "Extracted contiguous sub-snippets",
            "gap_type": "incomplete implementation extraction (missing dependencies)",
            "gap_description": "The miner can select a fragment that looks like the implementation but is missing earlier lines necessary for correctness (e.g., keys_to_keep definition omitted), causing the snippet to be non-functional or misleading as an isolated training example.",
            "gap_location": "snippet extraction / granularity selection",
            "detection_method": "Qualitative error analysis comparing mined snippets to gold; manual inspection of top-ranked snippets",
            "measurement_method": "Reported as qualitative failure cases; specific example S3 for I3 (Python) flagged as missing keys_to_keep definition",
            "impact_on_results": "Leads to mined pairs that are not directly usable for downstream tasks (synthesis, execution) and can degrade models trained on them if dependencies are not available; quantitative effect not numerically reported.",
            "frequency_or_prevalence": "Observed in error analysis; not given as a global percentage but listed as one of four main error sources",
            "root_cause": "Sub-snippet selection without dependency analysis; lack of static/dynamic analysis to ensure self-containedness",
            "mitigation_approach": "Potential mitigations (discussed as future work): detect and include necessary context lines (imports, variable/function definitions) or enforce syntactic/semantic closure (e.g., include surrounding definitions) when extracting snippets",
            "mitigation_effectiveness": "Not evaluated in paper; authors list this as an open failure case requiring future work.",
            "domain_or_field": "Software engineering / dataset extraction for code synthesis",
            "reproducibility_impact": true,
            "uuid": "e461.4",
            "source_info": {
                "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Spurious/Counterexample Cases",
            "name_full": "High Correspondence for Spurious or Counterexample Code",
            "brief_description": "Cases where code shown in answers as counterexamples or related-but-incorrect implementations still receive high correspondence scores and are selected as matches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Neural Correspondence Component + Classifier",
            "system_description": "The correspondence model (trained on large heuristic pairs) learns lexical/semantic associations that can make counterexample or related implementation snippets appear good matches for the intent.",
            "nl_description_type": "Stack Overflow question title (intent)",
            "code_implementation_type": "Answer code snippets including counterexamples or related implementations",
            "gap_type": "semantic misinterpretation / counterexample confusion",
            "gap_description": "Examples in answers that are explicitly counterexamples or alternative but undesired implementations (e.g., code demonstrating what not to do) may contain tokens strongly associated with the intent and thus receive high P(I|S)/P(S|I), leading the miner to select them incorrectly.",
            "gap_location": "scoring stage (correspondence probabilities used as features)",
            "detection_method": "Error analysis of top predictions (human inspection of outputs vs. original SO post context)",
            "measurement_method": "Qualitative examples provided (e.g., S1 for I4), noting that correspondence features alone cannot detect that a snippet is a counterexample",
            "impact_on_results": "Generates false positives; correspondence features alone underperform and need to be combined with structural/contextual cues to avoid selecting counterexamples.",
            "frequency_or_prevalence": "Not numerically quantified; identified as a recurrent failure mode in qualitative analysis",
            "root_cause": "Correspondence model lacks discourse-level awareness (cannot detect negation/explicit counterexample markers) and was trained on noisy data lacking such labels",
            "mitigation_approach": "Combine correspondence with structural/contextual features and potentially train models to detect negative/counterexample contexts; require page-level normalization and additional features",
            "mitigation_effectiveness": "Combination reduces but does not eliminate such errors; explicit mitigation not fully implemented in paper.",
            "domain_or_field": "Software engineering / NL–code mining",
            "reproducibility_impact": true,
            "uuid": "e461.5",
            "source_info": {
                "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Structural Feature Bias",
            "name_full": "Structural Heuristics Induce Positional or Size Biases",
            "brief_description": "Hand-crafted structural features (e.g., EndOfBlock, NumLines buckets) induce learned biases (e.g., favoring last line or full-block) that cause systematic mis-selections of non-implementing lines like print or pass.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Structural Feature Set for Snippet Filtering",
            "system_description": "A set of language-independent binary and bucketed features (FullBlock, StartOfBlock, ContainsImport, NumLinesX, EndsOfBlock, etc.) used by the logistic classifier to identify candidate snippets likely to be implementations.",
            "nl_description_type": "Stack Overflow question title (intent)",
            "code_implementation_type": "Answer code snippets (analyzed for structure/position)",
            "gap_type": "heuristic-induced bias causing false positives",
            "gap_description": "Because some structural features correlate with correct answers in training (e.g., many correct answers are whole blocks or last-line implementations), the learned classifier can give undue weight to positional features and consequently prefer non-implementing last lines (print statements) or full blocks even when not appropriate.",
            "gap_location": "feature design and classifier weighting stage",
            "detection_method": "Analysis of top-100 ranked candidates showing Structural model's selections (all full blocks in top-100 for Structural), and inspection of model weights (e.g., high weight for LineNum=1 and EndsCodeBlock)",
            "measurement_method": "Compare composition of top-ranked results across models (Structural vs. Correspondence vs. Full); PR/ROC comparisons and qualitative inspection",
            "impact_on_results": "Structural-only model performs reasonably well overall (especially for Java where full-block answers are common) but lacks fine granularity and makes systematic selection errors when partial-snippet is needed, reducing top-k precision for those cases.",
            "frequency_or_prevalence": "Observed pattern: out of top-100 for Structural, all were full code blocks (contrast: only 21 for Correspondence), indicating a strong prevalence of this bias in Structural-only model outputs.",
            "root_cause": "Simplicity and sparsity of hand-crafted features combined with limited annotated training data reinforcing coarse heuristics",
            "mitigation_approach": "Combine with correspondence features that can detect semantic fit and penalize contextual lines (imports/assignments); add combination features (e.g., ¬StartWithAssign + NumLines1) to refine selection",
            "mitigation_effectiveness": "Combination of Structural+Correspondence alleviates some bias and improves overall performance, as shown by improved ROC/AUC and PR curves.",
            "domain_or_field": "Software engineering / NL–code mining",
            "reproducibility_impact": true,
            "uuid": "e461.6",
            "source_info": {
                "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
                "publication_date_yy_mm": "2018-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow",
            "rating": 2,
            "sanitized_title": "staqc_a_systematically_mined_questioncode_dataset_from_stack_overflow"
        },
        {
            "paper_title": "From query to usable code: an analysis of Stack Overflow code snippets",
            "rating": 2,
            "sanitized_title": "from_query_to_usable_code_an_analysis_of_stack_overflow_code_snippets"
        },
        {
            "paper_title": "AutoComment: Mining question and answer sites for automatic comment generation",
            "rating": 1,
            "sanitized_title": "autocomment_mining_question_and_answer_sites_for_automatic_comment_generation"
        },
        {
            "paper_title": "A Syntactic Neural Model for General-Purpose Code Generation",
            "rating": 1,
            "sanitized_title": "a_syntactic_neural_model_for_generalpurpose_code_generation"
        },
        {
            "paper_title": "Summarizing Source Code using a Neural Attention Model",
            "rating": 1,
            "sanitized_title": "summarizing_source_code_using_a_neural_attention_model"
        }
    ],
    "cost": 0.01577825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow</p>
<p>Pengcheng Yin pcyin@cs.cmu.edu 
Carnegie Mellon University
USA</p>
<p>Bowen Deng bdeng1@cs.cmu.edu 
Carnegie Mellon University
USA</p>
<p>Edgar Chen edgarc@cs.cmu.edu 
Carnegie Mellon University
USA</p>
<p>Bogdan Vasilescu bogdanv@cs.cmu.edu 
Carnegie Mellon University
USA</p>
<p>Graham Neubig gneubig@cs.cmu.edu 
Carnegie Mellon University
USA</p>
<p>Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow
10.1145/3196398.3196408
For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.</p>
<p>INTRODUCTION</p>
<p>Recent years have witnessed the burgeoning of a new suite of developer assistance tools based on natural language processing (NLP) techniques, for code completion [9], source code summarization [2], automatic documentation of source code [44], deobfuscation [16,34,40], cross-language porting [27,28], code retrieval [3,42] and even code synthesis from natural language [7,21,32,47].</p>
<p>Besides the creativity and diligence of the researchers involved, these recent success stories can be attributed to two properties of software source code. First, it is highly repetitive [8,10], therefore * PY and BD contributed equally to this work. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. predictable in a statistical sense. This statistical predictability enabled researchers to expand from models of source code and natural language (NL) created using hand-crafted rules, which have a long history [23], to data-driven models that have proven flexible, relatively easy-to-create, and often more effective than corresponding hand-crafted precursors [13,27]. Second, source code is available in large amounts, thanks to the proliferation of open source software in general, and the popularity of open access, "Big Code" repositories like GitHub and Stack Overflow (SO); these platforms host tens of millions of code repositories and programming-related questions and answers, respectively, and are ripe with data that can, and is, being used to train such models [34]. However, the statistical models that power many such applications are only as useful as the data they are trained on, i.e., garbage in, garbage out [35]. For a particular class of applications, such as source code retrieval given a NL query [42], source code summarization in NL [15], and source code synthesis from NL [33,47], all of which involve correspondence between NL utterances and code, it is essential to have access to high volume, high quality, parallel data, in which NL and source code align closely to each other.</p>
<p>While one can hope to mine such data from Big Code repositories like SO, straightforward mining approaches may also extract quite a bit of noise. We illustrate the challenges associated with arXiv:1805.08949v1 [cs.CL] 23 May 2018 mining aligned (parallel) pairs of NL and code from SO with the example of a Python question in Figure 1. Given a NL query (or intent), e.g., "removing duplicates in lists", and the goal of finding its matching source code snippets among the different answers, prior work used either a straightforward mining approach that simply picks all code blocks that appear in the answers [3], or one that picks all code blocks from answers that are highly ranked or accepted [15,44]. 1 However, it is not necessarily the case that every code block accurately reflects the intent. Nor is it that the entire code block is answering the question; some parts may simply describe the context, such as variable definitions (Context 1) or import statements (Context 2), while other parts might be entirely irrelevant (e.g., the latter part of the first code block).</p>
<p>There is an inherent trade-off here between scale and data quality. On the one hand, when mining pairs of NL and code from SO, one could devise filters using features of the SO questions, answers, and the specific programming language (e.g., only consider accepted answers with a single code block or with high vote counts, or filtering out print statements in Python, much like one thrust of prior work [15,44]); fine-tuning heuristics may achieve high pair quality, but this inherently reduces the size of the mined data set and it may also be very language-specific. On the other hand, extracting all available code blocks, much like the other thrust of prior work [3], scales better but adds noise (and still cannot handle cases where the "best" code snippets are smaller than a full code block). Ideally, a mining approach to extract parallel pairs would handle these tricky cases and would operate at scale, extracting many high-quality pairs. To date, none of the prior work approaches satisfies both requirements of high quality and large quantity.</p>
<p>In this paper, we propose a novel technique that fills this gap (see Figure 2 for an overview). Our key idea is to treat the problem as a classification problem: given an NL intent (e.g., the SO question title) and all contiguous code fragments extracted from all answers of that question as candidate matches (for each answer code block, we consider all line-contiguous fragments as candidates, e.g., for a 3-line code block 1-2-3, we consider fragments consisting of lines 1, 2, 3, 1-2, 2-3, and 1-2-3), we use a data-driven classifier to decide if a candidate aligns well with the NL intent. Our model uses two kinds of information to evaluate candidates: (1) structural features, which are hand-crafted but largely language-independent, and try to estimate whether a candidate code fragment is valid syntactically, and (2) correspondence features, automatically learned, which try to estimate whether the NL and code correspond to each other semantically. Specifically, for the latter we use a model inspired by recent developments in neural network models for machine translation [4], which can calculate bidirectional conditional probabilities of the code given the NL and vice-versa. We evaluate our method on two small labeled data sets of Python and Java code that we created from SO. We show that our approach can extract significantly more, and significantly more accurate code snippets in both languages than previous baseline approaches. We also demonstrate that the classifier is still effective even when trained on Python then used to extract snippets for Java, and vice-versa, which demonstrates potential for generalizability to other programming languages without laborious annotation of correct NL-code pairs. 1 There is at most one accepted answer per question; see green check symbol in Fig 1. Our approach strikes a good balance between training effort, scale, and accuracy: the correspondence features can be trained without human intervention on readily available data from SO; the structural features are simple and easy to apply to new programming languages; and the classifier requires minimal amounts of manually labeled data (we only used 152 Python and 102 Java manually-annotated SO question threads in total). Even so, compared to the heuristic techniques from prior work [3,15,44], our approach is able to extract up to an order of magnitude more aligned pairs with no loss in accuracy, or reduce errors by more than half while holding the number of extracted pairs constant.</p>
<p>Specifically, we make the following contributions: • We propose a novel technique for extracting aligned NL-code pairs from SO posts, based on a classifier that combines snippet structural features, readily extractable, with bidirectional conditional probabilities, estimated using a state-of-the-art neural network model for machine translation. • We propose a protocol and tooling infrastructure for generating labeled training data. • We evaluate our technique on two data sets for Python and Java and discuss performance, potential for generalizability to other languages, and lessons learned. • All annotated data, the code for the annotation interface and the mining algorithm are available at http://conala-corpus.github.io.</p>
<p>PROBLEM SETTING</p>
<p>Stack Overflow (SO) is the most popular Q&amp;A site for programming related questions, home to millions of users. An example of the SO interface is shown in Figure 1, with a question (in the upper half) and a number of answers by different SO users. Questions can be about anything programming-related, including features of the programming language or best practices. Notably, many questions are of the "how to" variety, i.e., questions that ask how to achieve a particular goal such as "sorting a list", "merging two dictionaries", or "removing duplicates in lists" (as shown in the example); for example, around 36% of the Python-tagged questions are in this category, as discussed later in Section 3.2. These how-to questions are the type that we focus on in this work, since they are likely to have corresponding snippets and they mimic NL-to-code (or vice versa) queries that users might naturally make in the applications we seek to enable, e.g., code retrieval and synthesis. Specifically, we focus on extracting triples of three specific elements of the content included in SO posts:</p>
<p>• Intent: A description in English of what the questioner wants to do; usually corresponds to some portion of the post title. • Context: A piece of code that does not implement the intent, but is necessary setup, e.g., import statements, variable definitions. • Snippet: A piece of code that actually implements the intent.</p>
<p>An example of these three elements is shown in Figure 1. Several interesting points can be gleamed from this example. First, and most important, we can see that not all snippets in the post are implementing the original poster's intent: only two of four highlighted are actual examples of how to remove duplicates in lists, the other two highlighted are context, and others still are examples of interpreter output. If one is to train, e.g., a data-driven system for code synthesis from NL, or code retrieval using NL, only the snippets, or portions of snippets, that actually implement the user intent should be used. Thus, we need a mining approach that can distinguish which segments of code are actually legitimate implementations, and which can be ignored. Second, we can see that there are often several alternative implementations with different trade-offs (e.g., the first example is simpler in that it doesn't require additional modules to be imported first). One would like to be able to extract all of these alternatives, e.g., to present them to users in the case of code retrieval 2 or, in the case of code summarization, see if any occur in the code one is attempting to summarize.</p>
<p>These aspects are challenging even for human annotators, as we illustrate next.</p>
<p>MANUAL ANNOTATION</p>
<p>To better understand the challenges with automatically mining aligned NL-code snippet pairs from SO posts, we manually annotated a set of labeled NL-code pairs. These also serve as the goldstandard data set for training and evaluation. Here we describe our annotation method and criteria, salient statistics about the data collected, and challenges faced during annotation.</p>
<p>For each target programming language, we first obtained all questions from the official SO data dump 3 dated March 2017 by filtering questions tagged with that language. We then generated the set of questions to annotate by: (1) including all top-100 questions ranked by view count; and (2) sampling 1,000 questions from the probability distribution generated by their view counts on SO; we choose this method assuming that more highly-viewed questions are more important to consider as we are more likely to come across them in actual applications. While each question may have any number of answers, we choose to only annotate the top-3 highestscoring answers to prevent annotators from potentially spending a long time on a single question.</p>
<p>Annotation Protocol and Interface</p>
<p>Consistently annotating the intent, context, and snippet for a variety of posts is not an easy task, and in order to do so we developed and iteratively refined a web annotation interface and a protocol with detailed annotation criteria and instructions.</p>
<p>The annotation interface allows users to select and label parts of SO posts as (I)intent, (C)ontext, and (S)nippet using shortcut keys, as well as rewrite the intent to better match the code (e.g., adding variable names from the snippet into the original intent), in consideration of potential future applications that may require more precisely aligned NL-code data; in the following experiments we solely consider the intent and snippet, and reserve examination of the context and re-written intent for future work.</p>
<p>Multiple NL-code pairs that are part of the same post can be annotated this way. There is also a "not applicable" button that allows users to skip posts that are not of the "how to" variety, and a "not sure" button, which can be used when the annotator is uncertain. The annotation criteria were developed by having all authors attempt to perform annotations of sample data, gradually adding notes of the difficult-to-annotate cases to a shared document. We completed several pilot annotations for a sample of Python questions, iteratively discussing among the research team the annotation criteria and the difficult-to-annotate cases after each, before finalizing the annotation protocol. We repeated the process for Java posts. Once we converged on the final annotation standards in both languages, we discarded all pilot annotations, and one of the authors (a graduate-level NLP researcher and experienced programmer) re-annotated the entire data set according to this protocol.</p>
<p>While we cannot reflect all difficult cases here for lack of space, below is a representative sample from the Python instructions:</p>
<p>• Intents: Annotate the command form when possible (e.g., "how do I merge dictionaries" will be annotated as "merge dictionaries"). Extraneous words such as "in Python" can be ignored. Intents will almost always be in the title of the post, but intents expressed elsewhere that are different from the title can also be annotated. • Context: Contexts are a set of statements that do not directly reflect the annotated intent, but may be necessary in order to get the code to run, and include import statements, variable definitions, and anything else that is necessary to make sure that the code executes. When no context exists in the post this field can be left blank. • Snippet: Try to annotate full lines when possible. Some special tokens such as "&gt;&gt;&gt;", "print", and "In[...]" that appear at the beginning of lines due to copy-pasting can be included. When the required code is encapsulated in a function, the function definition can be skipped. • Re-written intent: Try to be accurate, but try to make the minimal number of changes to the original intent. Try to reflect all of the free variables in the snippet to be conducive to future automatic matching of these free variables to the corresponding position in code. When referencing string literals or numbers, try to write exactly as written in the code, and surround variables with a grave accent "'".</p>
<p>Annotation Outcome</p>
<p>We annotated a total of 418 Python questions and 200 Java questions. Of those, 152 in Python and 102 in Java were judged as annotatable (i.e., the "how-to" style questions described in Section 2), resulting in 577 Python and 354 Java annotations. We then removed the annotations marked as "not sure" and all unparsable code snippets. 4 In the end we generated 527 Python and 330 Java annotations, respectively. Table 1 lists basic statistics of the annotations. Notably, compared to Python, Java code snippets are longer (13.2 vs. 30.6 tokens per snippet), and more likely to be full code blocks (30.7% vs. 53.6%). That is, in close to 70% of cases for Python, the code snippet best-aligned with the NL intent expressed in the question title was not a full code block (SO uses special HTML tags to highlight code blocks, recall the example in Figure 1) from one of the answers, but rather a subset of it; similarly, the best-aligned Java snippets were not full code blocks in almost half the cases. This confirms the importance of mining code snippets beyond the level of entire code blocks, a limitation of prior approaches. Overall, we found the annotation process to be non-trivial, which raises several noteworthy threats to validity: (1) it can be difficult for annotators to distinguish between incorrect solutions and unusual or bad solutions that are nonetheless correct; (2) in cases where a single SO question elicits many correct answers with many implementations and code blocks, annotators may not always label all of them; (3) long and complex solutions may be mis-annotated; and (4) inline code blocks are harder to recognize than stand-alone code blocks, increasing the risk of annotators missing some. We made a best effort to minimize the impact of these threats by carefully designing and iteratively refining our annotation protocol.</p>
<p>MINING METHOD</p>
<p>In this section, we describe our mining method (see Figure 2 for an overview). As mentioned in Section 2, we frame the problem as a classification problem. First, for every "how to" SO question we consider its title as the intent and extract all contiguous lines from across all code blocks in the question's answers (including those we might manually annotate as context; inline code snippets are excluded) as candidate implementations of the intent, as long as we could parse the candidate snippets. 4 There are some cases where the title is not strictly equal to the intent, which go beyond the scope of this paper; for the purpose of learning the model we assume the title is representative. This step generates, for every SO question considered, a set of pairs (intent I , candidate snippet S). For example, the second answer in Figure 1, containing a threeline-long code block, would generate six line-contiguous candidate snippets, corresponding to lines 1, 2, 3, 1-2, 2-3, and 1-2-3. Our candidate snippet generation approach, though clearly not the only possible approach (1) is simple and language-independent, (2) is informed by our manual annotations, and (3) it gives good coverage of all possible candidate snippets.</p>
<p>Then, our task is, given a candidate pair (I , S), to assign a label y representing whether or not the snippet S reflects the intent I ; we define y to equal 1 if the pair matches and -1 otherwise. Our general approach to making this binary decision is to use machine learning to train a classifier that predicts, for every pair (I , S), the probability that S accurately implements I , i.e., P(y = 1|I , S), based on a number of features (Sections 4.1 and 4.2). As is usual in supervised learning, our system first requires an offline training phase that learns the parameters (i.e., feature weights) of the classifier, for which we use the annotated data described above (Section 3). This way, we can 4 We use the built-in ast parser module for Python, and JavaParser for Java.  Figure 2: Overview of our approach.</p>
<p>apply our system to an SO page of interest, and compute P(y = 1|I , S) for each possible intent/candidate snippet pair mined from the SO page. We choose logistic regression as our classifier, as implemented in the scikit-learn Python package.</p>
<p>As human annotation to generate training data is costly, our goal is to keep the amount of manually labeled training data to a minimum, such that scaling our approach to other programming languages in the future can be feasible. Therefore, to ease the burden on the classifier in the face of limited training data, we combined two types of features: hand-crafted structural features of the code snippets (Section 4.1) and machine learned correspondence features that predict whether intents and code snippets correspond to eachother semantically (Section 4.2). Our intuition, again informed by the manual annotation, was that "good" and "bad" pairs can often be distinguished based on simple hand-crafted features; these features could eventually be learned (as opposed to hand-crafted), but this would require more labeled training data, which is relatively expensive to create.</p>
<p>Hand-crafted Code Structure Features</p>
<p>The structural features are intended to distinguish whether we can reasonably expect that a particular piece of code implements an intent. We aimed for these features to be both informative and generally applicable to a wide range of programming languages. These features include the following:</p>
<p>• FullBlock, StartOfBlock, EndOfBlock: A code block may represent a single cohesive solution. By taking only a piece of a code block, we may risk acquiring only a partial solution, and thus we use a binary feature to inform the classifier of whether it is looking at a whole code block or not. On the other hand, as shown in Figure 1, many code blocks contain some amount of context before the snippet, or other extraneous information, e.g., print statements. To consider these, we also add binary features indicating that a snippet is at the start or end of its code block. • ContainsImport, StartsWithAssignment, IsValue: Additionally, some statements are highly indicative of a statement being context or extraneous. For example, import statements are highly indicative of a particular line being context instead of the snippet itself, and thus we add a binary feature indicating whether an import statement is included. Similarly, variable assignments are often context, not the implementation itself, and thus we add another feature indicating whether the snippet starts with a variable assignment. Finally, we observed that in SO (particularly for Python), it was common to have single lines in the code block that consisted of only a variable or value, often as an attempt to print these values to the interactive terminal. • AcceptedAns, PostRank1, PostRank2, PostRank3: The quality of the post itself is also indicative of whether the answer is likely to be valid or not. Thus, we add several features indicating whether the snippet appeared in a post that was the accepted answer or not, and also the rank of the post within the various answers for a particular question. • OnlyBlock: Posts with only a single code block are more likely to have that snippet be a complete implementation of the intent, so we added another feature indicating when the extracted snippet is the only one in the post. • NumLinesX: Snippets implementing the intent also tend to be concise, so we added features indicating the number of lines in the snippet, bucketed into X = 1, 2, 3, 4-5, 6-10, 11-15, &gt;15. • Combination Features: Some features can be logically combined to express more complex concepts. E.g., AcceptedAns + OnlyBlock + WholeBlock can express the strategy of selecting whole blocks from accepted answers with only one block, as used in previous work [15,44]. We use this feature and two other combination features: specifically ¬StartWithAssign + EndOfBlock and ¬StartWithAssign + NumLines1.</p>
<p>Unsupervised Correspondence Features</p>
<p>While all of the features in the previous section help us determine which code snippets are likely to implement some intent, they say nothing about whether the code snippet actually implements the particular intent I that is currently under consideration. Of course considering this correspondence is crucial to accurately mining intent-snippet pairs, but how to evaluate this correspondence computationally is non-trivial, as there are very few hard and fast rules that indicate whether an intent and snippet are expressing similar meaning. Thus, in an attempt to capture this correspondence, we take an indirect approach that uses a potentially-noisy (i.e., not manually validated) but easy-to-construct data set to train a probabilistic model to approximately capture these correspondences, then incorporate the predictions of this noisily trained model as features into our classifier.</p>
<p>Training data of correspondence features: Apart from our manuallyannotated data set, we collected a relatively large set of intentsnippet pairs using simple heuristic rules for learning the correspondence features. The data set is created by pairing the question titles and code blocks from all SO posts, where (1) the code block comes from an SO answer that was accepted by the original poster, and (2) there is only one code block in this answer. Of course, many of these code blocks will be noisy in the sense that they contain extraneous information (such as extra import statements or variable definitions, etc.), or not directly implement the intent at all, but they will still be of use for learning which NL expressions in the intent tend to occur with which types of source code.  Learning a model of correspondence: Given the training data above, we need to create a model of the correspondence between the intent I and snippet S. To this end, we build a statistical model of the bidirectional probability of the intent given the snippet P(I | S), and the probability of the snippet given the intent P(S | I ).</p>
<p>Specifically, we follow previous work that has noted that models from machine translation (MT; [19]) are useful for learning the correspondences between natural language and code for the purposes of code summarization [15,30], code synthesis from natural language [21], and code retrieval [3]. In particular, we use a model based on neural MT [4,17], a method for MT based on neural networks that is well-suited for this task of learning correspondences for a variety of reasons, which we outline below after covering the basics. To take the example of using a neural MT model that attempts to generate an intent I given a snippet S, these models work by incrementally generating each word of the intent i 1 , i 2 , . . . , i |I | one word at a time (the exact same process can be performed in the reverse direction to generate a snippet S given intent I ). For example, if our intent is "download and save an http file", the model would first predict and generate "download", then "and", then "save", etc. This is done in a probabilistic way by calculating the probability of the first word given the snippet P(i 1 | S) and outputting the word in the vocabulary that maximizes this probability, then calculating the probability of the second word given the first word and the snippet P(i 2 | S, i 1 ) and similarly outputting the word with the highest probability, etc. Incidentally, if we already know a particular intent I and want to calculate its probability given a particular snippet S (for example to use as features in our classifier), we can also do so by calculating the probability of each word in the intent and multiplying them together as follows: P(I | S) = P(i 1 | S)P(i 2 | S, i 1 )P(i 3 | S, i 1 , i 2 ) . . .</p>
<p>(1) So how do neural MT models calculate this probability? We will explain a basic outline of a basic model called the encoder-decoder model [39], and refer readers to references for details [4,25,39]. The encoder-decoder model, as shown in Figure 3, works in two stages: First, it encodes the input (in this case S) into a hidden vector of continuous numbers h using an encoding function h |S | = encode(S).</p>
<p>This function generally works in two steps: looking up a vector of numbers representing each word (often called "word embeddings" or "word vectors"), then incrementally adding information about these embeddings one word at a time using a particular variety of network called a recurrent neural network (RNN). To take the specific example shown in the figure, at the first time step, we would look up a word embedding vector for the first word "mylist", e 1 = e mylist and then perform a calculation such as the one below to calculate the hidden vector for the first time step:
h 1 = tanh(W enc,e e 1 + b enc ),(3)
where W enc,e and b enc are a matrix and vector that are parameters of the model, and tanh(·) is the hyperbolic tangent function used to "squish" the values to be between -1 and 1. In the next time step, we would do the same for the symbol ". ", using its embedding e 2 = e . , and in the calculation from the second step onward we also use the result of the previous calculation (in this case h 2 ): h 1 = tanh(W enc,h h 1 + W enc,e e 2 + b enc ).</p>
<p>(4) By using the hidden vector from the previous time step, the RNN is able to "remember" features of the previously occurring words within this vector, and by repeating this process until the end of the input sequence, it (theoretically) has the ability to remember the entire content of the input within this vector.</p>
<p>Once we have encoded the entire source input, we can use this encoded vector to predict the first word of the output. This is done by multiplying the vector h with another weight matrix to calculate a score д for each word in the output vocabulary:
д 1 = W pred h |S | + b pred .(5)
We then predict the actual probability of the first word in the output sentence, for example "find", by using the softmax function, which exponentiates all of the scores in the output vocabulary and then normalizes these scores so that they add up to one:
P(i 1 = "find") = exp(д find ) д exp(д) .(6)
We use a neural MT model with this basic architecture, with the addition of a feature called attention, which, put simply, allows the model to "focus" on particular words in the source snippet S when generating the intent I . The details of attention are beyond the scope of this paper, but interested readers can reference [4,22].</p>
<p>Why attentional neural MT models?: Attention-based neural MT models are well-suited to the task of learning correspondences between natural language intents and code snippets for a number of reasons. First, they are a purely probabilistic model capable of calculating P(S | I ) and P(I | S), which allows them to easily be incorporated as features in our classifier, as described in the following paragraph. Second, they are powerful models that can learn correspondences on a variety of levels; from simple phenomena such as direct word-by-word matches [12], to soft paraphrases [36], to weak correspondences between keywords and large documents for information retrieval [14]. Finally, they have demonstrated success in a number of NL-code related tasks [2,3,21,47], which indicates that they could be useful as part of our mining approach as well.</p>
<p>Incorporating correspondence probabilities as features: For each intent I and candidate snippet S, we calculate the probabilities P(S | I ) and P(I | S), and add them as features to our classifier, as we did with the hand-crafted structural features in Section 4.1.</p>
<p>• SGivenI, IGivenS: Our first set of features are the logarithm of the probabilities mentioned above: log P(S | I ) and log P(I | S). 5 Intuitively, these probabilities will be indicative of S and I being a good match because if they are not, the probabilities will be low. If the snippet and the intent are not a match at all, both features will have a low value. If the snippet and intent are partial matches, but either the snippet S or intent I contain extraneous information that cannot be predicted from the counterpart, then SGivenI and IGivenS will have low values respectively. • ProbMax, ProbMin: We also represent the max and min of log P(S | I ) and log P(I | S). In particular, the ProbMin feature is intuitively helpful because pairs where the probability in either direction is low are likely not good pairs, and this feature will be low in the case where either probability is low. • NormalizedSGivenI, NormalizedIGivenS: In addition, intuitively we might want the best matching NL-code pairs within a particular SO page. In order to capture this intuition, we also normalize the scores over all posts within a particular page so that their mean is zero and standard deviation is one (often called the z-score). In this way, the pairs with the best scores within a page will get a score that is significantly higher than zero, while the less good scores will get a score close to or below zero.</p>
<p>EVALUATION</p>
<p>In this section we evaluate our proposed mining approach. We first describe the experimental setting in Section 5.1 before addressing the following research questions:</p>
<p>(1) How does our mining method compare with existing approaches across different programming languages? (Section 5.2) (2) How do the structural and correspondence features impact the system's performance? (Section 5.2) (3) Given that annotation of data for each language is laborious, is it possible to use a classifier learned on one programming language to perform mining on other languages? (Section 5.3) (4) What are the qualitative features of the NL-code pairs that our method succeeds or fails at extracting? (Section 5.4)</p>
<p>We show that our method clearly outperforms existing approaches and shows potential for reuse without retraining, we uncover tradeoffs between performance and training complexity, and we discuss limitations, which can inform future work.</p>
<p>Experimental Settings</p>
<p>We conduct experimental evaluation on two programming languages: Python and Java. These languages were chosen due to their large differences in syntax and verbosity, which have been shown to effect characteristics of code snippets on SO [45].</p>
<p>Learning unsupervised features: We start by filtering the SO questions in the Stack Exchange data dump 3 by tag (Python and Java), and we use an existing classifier [15] to identify the how-to style questions. The classifier is a support vector machine trained by bootstrapping from 100 labeled questions, and achieves over 75% accuracy as reported in [15]. We then extract intent/snippet pairs from all these questions as described in Section 4.2, collecting 33,946 pairs for Python and 37,882 for Java. Next we split the data set into training and validation sets with a ratio of 9:1, keeping the 90% for training. Statistics of the data set are listed in Table 2. 6 We implement our neural correspondence model using the DyNet neural network toolkit [26]. The dimensionality of word embedding and RNN hidden states is 256 and 512. We use dropout [38], a standard method to prevent overfitting, on the input of the last softmax layer over target words (p = 0.5), and recurrent dropout [11] on RNNs (p = 0.2). We train the network using the widely used optimization method Adam [18]. To evaluate the neural network, we use the remaining 10% of pairs left aside for testing, retaining the model with the highest likelihood on the validation set.</p>
<p>Evaluating the mining model: For the logistic regression classifier, which uses the structural and correspondence features described above, the latter computed by the previous neural network, we use our annotated intent/snippet data (Section 3.2) 7 during 5-fold cross validation. Recall, our code mining model takes as input a SO question (i.e., intent reflected by the question title) with its answers, and outputs a ranked list of candidate intent/snippet pairs (with probability scores). For evaluation, we first rank all candidate intent/snippet pairs for all questions, and then compare the ranked list with gold-standard annotations. We present the results using standard precision-recall (PR) and Receiver Operating Characteristic (ROC) curves. In short, a PR curve shows the precision w.r.t. recall for the top-k predictions in the ranked list, with k from 1 to the number of candidates. A ROC curve plots the true positive rates w.r.t. false positive rates in similar fashion. We also compute the Area Under the Curve (AUC) scores for all ROC curves.</p>
<p>Baselines: As baselines for our model (denoted as Full), we implement three approaches reflecting prior work and sensible heuristics:</p>
<p>AcceptOnly is the state-of-the art from prior work [15,44]; it selects the whole code snippet in the accepted answers containing exactly one code snippet. All denotes the baseline method that exhaustively selects all full code blocks in the top-3 answers in a post. Random is the baseline that randomly selects from all consecutive code segment candidates.</p>
<p>Similarly to our model, we enforce the constraint that all mined code snippets given by the baseline approaches should be parseable. Additionally, to study the impact of hand-crafted Structural versus learned Correspondence features, we also trained versions of our model with either of the two types of features only.</p>
<p>Results and Discussion</p>
<p>Our main results are depicted in Figure 4. First, we can see that the precision of the random baseline is only 0.10 for Python and 0.06 for Java. This indicates that only one in 10-17 candidate code snippets is judged to validly correspond to the intent, reflecting the difficulty of the task. The AcceptOnly and All baselines perform significantly better, with precision of 0.5 or 0.6 at recall 0.05-0.1 and 0.3-0.4 respectively, indicating that previous heuristic methods using full code blocks are significantly better than random, but still have a long way to go to extract broad-coverage and accurate NL-code pairs (particularly in the case of Python). 8 Next, turning to the full system, we can see that the method with the full feature set significantly outperforms all baselines (Figures 4b and 4d): much better recall (precision) at the same level of precision (recall) as the heuristic approaches. The increase in precision suggests the importance of intelligently selecting NL-code pairs using informative features, and the increase in recall suggests the importance of considering segments of code within code blocks, instead of simply selecting the full code block as in prior work.</p>
<p>Comparing different types of features (Structural v.s. Correspondence), we find that with structural features alone our model already significantly outperforms baseline approaches; and these features are particularly effective for Java. On the other hand, interestingly the correspondence features alone provide less competitive results. Still, the structural and correspondence features seem to be complementary, with the combination of the two feature sets further significantly improving performance, particularly on Python. A closer examination of the results generated the following insights.</p>
<p>Why do correspondence features underperform? While these features effectively filter totally unrelated snippets, they still have a difficult time excluding related contextual statements, e.g., imports, assignments. This is because (1) the snippets used for training correspondence features are full code blocks (as in §4.2), usually starting with import statements; and (2) the library names in import statements often have strong correspondence with the intents (e.g., "How to get current time in Python?" and import datetime), yielding high correspondence probabilities.</p>
<p>What are the trends and error cases for structural features? Like the baseline methods, Structural tends to give priority to full code blocks; out of the top-100 ranked candidates for Structural, all were full code blocks (in contrast to only 21 for Correspondence). Because selecting code blocks is a reasonably strong baseline, and because the model has access to other strongly-indicative binary features that can be used to further prioritize its choices, it is able to achieve reasonable precision-recall scores only utilizing these features. However, unsurprisingly, it lacks fine granularity in terms of pinpointing exact code segments that correspond to the intents; when it tries to select partial code segments, the results are likely to be irrelevant to the intent. As an example, we find that Structural tends to select the last line of code at each code block, since the learned weights for LineNum=1 and EndsCodeBlock features are high, even though these often consist of auxiliary print statement or even simply pass (for Python).</p>
<p>What is the effect of the combination? When combining Structural and Correspondence features together, the full model has the ability to use the knowledge of the Structural model extract full code blocks or ignore imports, leading to high performance in the beginning stages. Then, in the latter and more difficult cases, it is able to more effectively cherry-pick smaller snippets based on their correspondence properties, which is reflected in the increased accuracy on the right side of the ROC and precision-recall curves.</p>
<p>How do the trends differ between programming languages? Compared with the baseline approaches AcceptOnly and All, our full model performs significantly better on Python. We hypothesize that this is because learning correspondences between intent/snippet pairs for Java is more challenging. Empirically, Python code snippets are much shorter, and the average number of tokens for predicted code snippets on Python and Java is 11.6 and 42.4, respectively. Meanwhile, since Java code snippets are more verbose and contain significantly more boilerplate (e.g., class/function definitions, type declaration, exception handling, etc.), estimating correspondence scores using neural networks is more challenging.</p>
<p>Also note that the Structural model performs much better on Java than on Python. This is due to the fact that Java annotations are more likely to be full code blocks (see Table 1), which can be easily captured by our designed features like FullBlock. Nevertheless, adding correspondence features is clearly helpful for the harder cases for both programming languages. For instance, from the ROC curve in Figure 4c, our full model achieves higher true positive rates compared with Structural, registering higher AUC scores.</p>
<p>Must We Annotate Each Language?</p>
<p>As discussed in §3, collecting high-quality intent/snippet annotations to train the code mining model for a programming language can be costly and time-consuming. An intriguing research question is how we could transfer the learned code mining model from one programming language and use it for mining intent/snippet data for another language. To test this, we train a code mining model using the annotated intent/snippet data on language A, and evaluate using the annotated data on language B. 9 This is feasible since almost all of the features used in our system is language-agnostic. 10 Also note values of a specific feature might have different ranges for different languages. As an example, the average value of SGivenI feature for Python and Java is -23.43 and -47.64, respectively. To mitigate this issue, we normalize all feature values to zero mean and unit variance before training the logistic regression classifier. Figures 5a and 5b show the precision-recall curves for applying Java (Python) mining model on Python (Java) data. We report results for both the Structural model and our full model, and compare with the original models trained on the target programming language. Unsurprisingly, the original full model tuned on the target language still performs the best. Nevertheless, we observe that the performance gap between the original full model and the transferred one is surprisingly small. Notably, we find that overall the transferred full model (Full-Java) performs second best on Python, even outperforming the original Structural model. These results are encouraging, in that they suggest that it is likely feasible to train a single code mining classifier and then apply it to different programming languages, even those for which we do not have any annotated intent/snippet data.</p>
<p>Successful and Failed Examples</p>
<p>As illustration, we showcase successful and failed examples of our proposed approach, for Python in Table 3 and for Java in Table 4. Given a SO question (intent), we show the top-3 most probable code snippets. First, we find our model can correctly identify code snippets for various types of intents, even when the target snippets are not full code blocks. I 1 and I 6 demonstrate that our model can leave contextual information like variable definitions in the original SO posts and only retain the actual implementation of the intent. 11 I 2 , I 3 and I 7 are more interesting: in the original SO post, there could be multiple possible solutions in the same code block (I 2 and I 7 ), or the gold-standard snippets are located inside larger code structures like a for loop (S 2 for I 3 ). Our model learns to "break down" the solutions in single code block into multiple snippets, and extract the actual implementation from large code chunks.</p>
<p>We also identify four sources of errors: • Incomplete code: Some code snippets are incomplete, and the model fails to include intermediate statements (e.g., definitions of custom variables or functions) that are part of the implementation. For instance, S 3 for I 3 misses the definition of the keys_to_keep, which is the set of keys excluding the key to remove. • Including auxiliary info: Sometimes the model fails to exclude auxiliary code segments like the extra context definition (e.g., S 1 for I 8 ) and print function. This is especially true for Java, where full code blocks are likely to be correct snippets, and the model tends to bias towards larger code chunks. 11 We refer readers to the original SO page for reference.</p>
<p>• Spurious cases: We identify two "spurious" cases where our correspondence feature often do not suffice. (1) Counter examples: the S 1 for I 4 is mentioned in the original post as a counter example, but the values of correspondence features are still high since append() is highly related to "append it to another list" in the intent.</p>
<p>(2) Related implementation: I 5 shows an example where the model has difficulty distinguishing between the actual snippets and related implementations. • Annotation error: We find cases where our annotation is incomplete. For instance, S 1 for I 9 should be correct. As discussed in Section 3, guaranteeing coverage in the annotation process is non-trivial, and we leave this as a challenge for future work.</p>
<p>RELATED WORK</p>
<p>A number of previous works have proposed methods for mining intent-snippet pairs for purposes of code summarization, search, or synthesis. We can view these methods from several perspectives:</p>
<p>Data Sources: First, what data sources do they use to mine their data? Our work falls in the line of mining intent-snippet pairs from SO (e.g., [15,44,46,48]), while there has been research on mining from other data sources such as API documentation [5,6,24], code comments [43], specialized sites [32], parameter/method/class names [1,37], and developer mailing lists [31]. It is likely that it could be adapted to work with other sources, requiring only changes in the definition of our structural features to incorporate insights into the data source at hand.</p>
<p>Methodologies:</p>
<p>Second, what is the methodology used therein, and can it scale to our task of gathering large-scale data across a number of languages and domains? Several prior work approaches used heuristics to extract aligned intent-snippet pairs [6,44,48]). Our approach also contains an heuristic component. However, as evidenced by our experiments here, our method is more effective at extracting accurate intent-snippet pairs. Some work on code search has been performed by retrieving candidate code snippets given an intent based on weighted keyword matches and other features [29,42]. These methods similarly aim to learn correspondences between natural language queries and returned code, but they are tailored specifically for performing code search, apply a more rudimentary feature set (e.g., they do not employ neural network-based correspondence features) than we  do, and will generally not handle sub-code-block sized contexts, which proved important in our work.</p>
<p>We note that concurrent to this work, [46] also explored the problem of mining intent/code pairs from SO, identifying candidate code blocks of an intent using information from both the contextual texts and the code in an SO answer. Our approach, however, considers more fine-grained, sub-code-block sized candidates, aiming to recover code solutions that exactly answer the intent.</p>
<p>Finally, some work has asked programmers to manually write NL descriptions for code [20,30], or vice-versa [41]. This allows for the generation of high-quality data, but is time consuming and does not scale beyond limited domains.</p>
<p>THREATS TO VALIDITY</p>
<p>Besides threats related to the manual labeling (Section 3.2), we note the following overall threats to the validity of our approach:</p>
<p>Annotation Error: Our code mining approach is based on learning from a small amount of annotated data, and errors in annotation may impact the performance of the system (see Sections 3 and 5.4).</p>
<p>Data Set Volume: Our annotated data set contains mainly highranked SO questions, and is relatively small (with a few hundreds of examples for each language), which could penitentially hinder the generalization ability of the system on lower-ranked questions.  Meanwhile, we used cross-validation for evaluation, while evaluating our mining method on full-scale SO data would be ideal but challenging.</p>
<p>CONCLUSIONS</p>
<p>In this paper, we described a novel method for extracting aligned code/natural language pairs from the Q&amp;A website Stack Overflow. The method is based on learning from a small number of annotated examples, using highly informative features that capture structural aspects of the code snippet and the correspondence between it and the original natural language query. Experiments on Python and Java demonstrate that this approach allows for more accurate and more exhaustive extraction of NL-code pairs than prior work. We foresee the main impact of this paper lying in the resources it would provide when applied to the full Stack Overflow data: the NL-code pairs extracted would likely be of higher quality and larger scale. Given that high-quality parallel NL-code data sets are currently a significant bottleneck in the development of new data-driven software engineering tools, we hope that such a resource will move the field forward. In addition, while our method is relatively effective compared to previous work, there is still significant work to be done on improving mining algorithms to deal with current failure cases, such as those described in Section 5.4. Our annotated data set and evaluation tools, publicly available, may provide an impetus towards further research in this area.</p>
<p>Figure 1 :
1MSR '18, May 28-29, 2018, Gothenburg, Sweden © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5716-6/18/05. . . $15.00 https://doi.org/10.1145/3196398.Excerpt from a SO post showing two answers, and the corresponding NL intent and code pairs.</p>
<p>Figure 3 :
3An example of neural MT encoder-decoder framework used in calculating correspondence scores.</p>
<p>Figure 4 :
4Evaluation Results on Mining Python (a)(b) and Java (c)(d)</p>
<p>Figure 5 :
5Precision-Recall Curves for Transfer Learning on Java → Python (a) and Python → Java (b)</p>
<p>I 4 :
4Python: take the content of a list and append it to another list URL: https://stackoverflow.com/questions/8177079/ Top Predictions:S 1 list2.append(list1) ✗ S 2 list2.extend(list1) ✓ S 3 list1.extend(mylog) ✓I 5 : Converting integer to string in Python? URL: https://stackoverflow.com/questions/961632/ Top Predictions: S 1 int('10') ✗ S 2 str(10); int('10') ✗ S 3 a.<strong>str</strong>() ✓</p>
<p>I 6 :
6How to convert List<Integer> to int[] in Java? URL: https://stackoverflow.com/questions/960431/ Top Predictions: S 1 int[] array = list.stream().mapToInt(i −&gt; i).toArray(); ✓ S 2 int[] intArray2 = ArrayUtils.toPrimitive(myList.toArray( NO_INTS)); ✗ S 3 int[] intArray = ArrayUtils.toPrimitive(myList.toArray(new Integer[myList.size()])); ✓ I 7 : How do I compare strings in Java? URL: https://stackoverflow.com/questions/513832/ Top Predictions: S 1 new String("test").equals("test"); ✓ S 2 Objects.equals(null, "test"); ✓ S 3 nullString1.equals(nullString2); ✓I 8 : How do I set the colour of a label (coloured text) in Java? URL: https://stackoverflow.com/questions/2966334/ Top Predictions: S 1 JLabel title = new JLabel("I love stackoverflow!", JLabel. CENTER); title.setForeground(Color.white); ✗ S 2 frame.add(new JLabel("<html>Text color: <font color='red'> red</font></html>")); ✓ S 3 label.setForeground(Color.red); ✓ I 9 : Generating a Random Number between 1 and 10 Java URL: https://stackoverflow.com/questions/20389890/ Top Prediction:(only show one for space reason) S 1 public static int randInt(int min, int max) { Random rand = new Random(); int randomNum = rand.nextInt((max − min) + 1) + min; return randomNum; } ✗ (annotation error)</p>
<p>Table 1 :
1Details of the labeled data set.Lang. #Annot. #Ques. #Answer 
Posts </p>
<h1>Code</h1>
<p>Blocks </p>
<p>Avg. Code 
Length </p>
<p>%Full 
Blocks </p>
<p>%Annot. 
with Context 
Python 527 
142 
412 
736 
13.2 
30.7% 36.8% 
Java 
330 
100 
297 
434 
30.6 
53.6% 42.4% </p>
<p>Filter "How to" questionsSmall sample </p>
<p>Manual labeling 
Intent-Snippet pairs 
Gold 
standard </p>
<p>Training 
data </p>
<p>Classifier </p>
<p>Correspondence 
features (RNN ) </p>
<p>Structural 
features </p>
<p>Training </p>
<p>Classification </p>
<p>Question + 
Answers </p>
<p>Candidate snippets </p>
<p>p = 0.8 </p>
<p>p = 0.2 </p>
<p>p = 0.5 </p>
<p>Ranked list </p>
<p>Table 2 :
2Details of the NL-code data used for learning unsupervised correspondence features.Lang. 
Training Data 
(NL/Code Pairs) </p>
<p>Validation 
Data </p>
<p>Intents 
Code </p>
<p>Avg. 
Length </p>
<p>Vocabulary 
Size </p>
<p>Avg. 
Length </p>
<p>Vocabulary 
Size 
Python 
33,946 
3,773 
11.9 
12,746 
65.4 
30,286 
Java 
37,882 
4,208 
11.6 
13,775 
65.7 
29,526 </p>
<p>Table 3 :
3Examples of Mined Python CodeI 1 : Remove specific characters from a string in python URL: https://stackoverflow.com/questions/3939361/ Top Predictions: S 1 string.replace('1', '') ✓ S 2 line = line.translate(None, '!@#$') ✓ S 3 line = re.sub('[!@#$]', '', line) ✓ I 2 : Get Last Day of the Month in Python URL: https://stackoverflow.com/questions/42950/ Top Predictions: S 1 calendar.monthrange(year, month)[1] ✓ S 2 calendar.monthrange(2100, 2) ✓ S 3 (datetime.date(2000, 2, 1) − datetime.timedelta(days=1))✓ I 3 : Delete a dictionary item if the key exists URL: https://stackoverflow.com/questions/15411107/ Top Predictions: S 1 mydict.pop('key', None) ✓ S 2 del mydict[key] ✓ S 3 new_dict = {k: mydict[k] for k in keys_to_keep}✗</p>
<p>Table 4 :
4Examples of Mined Java Code
Ideally one would also like to present a description of the trade-offs, but mining this information is a challenge beyond the scope of this work.3 Available online at https://archive.org/details/stackexchange
We take the logarithm of the probabilities because the actual probability values tend to become very small for very long sequences (e.g., 10 −50 to 10 −100 ), while the logarithm is in a more manageable range (e.g., −50 to −100).
Note that this data may contain some of the posts included in the cross-validation test set with which we evaluate our model later. However, even if it does, we are not using the annotations themselves in the training of the correspondence features, so this does not pose a problem with our experimental setting.7 Recall that our annotated data contains only how-to style questions, and therefore question filtering is not required. When applying our mining method to the full SO data, we could use the how-to question classifier in[15].
Interestingly, AcceptOnly and All have similar precision, which might be due to two facts. First, we enforce all candidate snippets to be syntactically correct, which rules out erroneous candidates like input/output examples. Second, we use the top 3 answers for each question, which usually have relatively high quality.
We still train the correspondence model using the target language unlabeled data.10 The only one that was not applicable to both languages was the SingleValue feature for Python, which helps rule out code that contains only a single value. We omit this feature in the cross-lingual experiments.</p>
<p>Suggesting Accurate Method and Class Names. Miltiadis Allamanis, T Earl, Christian Barr, Charles Bird, Sutton, Joint Meeting on Foundations of Software Engineering (ESEC/FSE). ACMMiltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Sug- gesting Accurate Method and Class Names. In Joint Meeting on Foundations of Software Engineering (ESEC/FSE). ACM, 38-49.</p>
<p>A Convolutional Attention Network for Extreme Summarization of Source Code. Miltiadis Allamanis, Hao Peng, Charles Sutton, arXiv:1602.03001arXiv preprintMiltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional Attention Network for Extreme Summarization of Source Code. arXiv preprint arXiv:1602.03001 (2016).</p>
<p>Bimodal Modelling of Source Code and Natural Language. Miltiadis Allamanis, Daniel Tarlow, D Andrew, Yi Gordon, Wei, International Conference on Machine Learning (ICML). Miltiadis Allamanis, Daniel Tarlow, Andrew D Gordon, and Yi Wei. 2015. Bimodal Modelling of Source Code and Natural Language. In International Conference on Machine Learning (ICML). 2123-2132.</p>
<p>Neural Machine Translation by Jointly Learning to Align and Translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, International Conference on Learning Representations. ICLRDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Representations (ICLR).</p>
<p>A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation. Antonio Valerio, Miceli Barone, Rico Sennrich, arXiv:1707.02275arXiv preprintAntonio Valerio Miceli Barone and Rico Sennrich. 2017. A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documenta- tion and Code Generation. arXiv preprint arXiv:1707.02275 (2017).</p>
<p>Sniff: A Search Engine for Java Using Free-form Queries. Sudeep Shaunak Chatterjee, Koushik Juvekar, Sen, International Conference on Fundamental Approaches to Software Engineering. SpringerShaunak Chatterjee, Sudeep Juvekar, and Koushik Sen. 2009. Sniff: A Search En- gine for Java Using Free-form Queries. In International Conference on Fundamental Approaches to Software Engineering. Springer, 385-400.</p>
<p>Program Synthesis using Natural Language. Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, International Conference on Software Engineering (ICSE). ACMSubhajit Roy, and othersAditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, Subhajit Roy, and others. 2016. Program Synthesis using Natural Language. In International Conference on Software Engineering (ICSE). ACM, 345- 356.</p>
<p>New Initiative: the Naturalness of Software. Premkumar Devanbu, International Conference on Software Engineering (ICSE). IEEE2Premkumar Devanbu. 2015. New Initiative: the Naturalness of Software. In International Conference on Software Engineering (ICSE), Vol. 2. IEEE, 543-546.</p>
<p>CACHECA: A Cache Language Model based Code Suggestion Tool. Christine Franks, Zhaopeng Tu, Premkumar Devanbu, Vincent Hellendoorn, International Conference on Software Engineering (ICSE). IEEE2Christine Franks, Zhaopeng Tu, Premkumar Devanbu, and Vincent Hellendoorn. 2015. CACHECA: A Cache Language Model based Code Suggestion Tool. In International Conference on Software Engineering (ICSE), Vol. 2. IEEE, 705-708.</p>
<p>A Study of the Uniqueness of Source Code. Mark Gabel, Zhendong Su, International Symposium on Foundations of Software Engineering (FSE). ACMMark Gabel and Zhendong Su. 2010. A Study of the Uniqueness of Source Code. In International Symposium on Foundations of Software Engineering (FSE). ACM, 147-156.</p>
<p>A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. Yarin Gal, Zoubin Ghahramani, Annual Conference on Neural Information Processing Systems (NIPS). Yarin Gal and Zoubin Ghahramani. 2016. A Theoretically Grounded Applica- tion of Dropout in Recurrent Neural Networks. In Annual Conference on Neural Information Processing Systems (NIPS). 1019-1027.</p>
<p>Incorporating Copying Mechanism in Sequence-to-Sequence Learning. Jiatao Gu, Zhengdong Lu, Hang Li, O K Victor, Li, Annual Meeting of the Association for Computational Linguistics (ACL). ACL. Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating Copying Mechanism in Sequence-to-Sequence Learning. In Annual Meeting of the Association for Computational Linguistics (ACL). ACL, 1631-1640.</p>
<p>On the naturalness of software. Abram Hindle, T Earl, Mark Barr, Zhendong Gabel, Premkumar Su, Devanbu, Commun. ACM. 59Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. 2016. On the naturalness of software. Commun. ACM 59, 5 (2016), 122-131.</p>
<p>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, Larry Heck, International Conference on Information and Knowledge Management (CIKM). ACMPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning Deep Structured Semantic Models for Web Search using Clickthrough Data. In International Conference on Information and Knowledge Management (CIKM). ACM, 2333-2338.</p>
<p>Summarizing Source Code using a Neural Attention Model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, Annual Meeting of the Association for Computational Linguistics (ACL). ACL. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a Neural Attention Model. In Annual Meeting of the Association for Computational Linguistics (ACL). ACL, 2073-2083.</p>
<p>Meaningful Variable Names for Decompiled Code: A Machine Translation Approach. Alan Jaffe, Jeremy Lacomis, Edward J Schwartz, Claire Le Goues, Bogdan Vasilescu, International Conference on Program Comprehension (ICPC). ACMAlan Jaffe, Jeremy Lacomis, Edward J. Schwartz, Claire Le Goues, and Bogdan Vasilescu. 2018. Meaningful Variable Names for Decompiled Code: A Machine Translation Approach. In International Conference on Program Comprehension (ICPC). ACM.</p>
<p>Recurrent Continuous Translation Models. Nal Kalchbrenner, Phil Blunsom, Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL. Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL, 1700-1709.</p>
<p>Adam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, CoRR abs/1412.6980Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti- mization. CoRR abs/1412.6980 (2014). http://arxiv.org/abs/1412.6980</p>
<p>Statistical Machine Translation. Philipp Koehn, Cambridge PressPhilipp Koehn. 2010. Statistical Machine Translation. Cambridge Press.</p>
<p>NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System. Chenglong Xi Victoria Lin, Luke Wang, Michael D Zettlemoyer, Ernst, International Conference on Language Resources and Evaluation (LREC). Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D Ernst. 2018. NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System. International Conference on Language Resources and Evaluation (LREC) (2018).</p>
<p>Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge. Nicholas Locascio, Karthik Narasimhan, Eduardo De Leon, Nate Kushman, Regina Barzilay, Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL. Nicholas Locascio, Karthik Narasimhan, Eduardo De Leon, Nate Kushman, and Regina Barzilay. 2016. Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge. In Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL, 1918-1923.</p>
<p>Effective Approaches to Attention-based Neural Machine Translation. Thang Luong, Hieu Pham, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL. Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Ap- proaches to Attention-based Neural Machine Translation. In Conference on Em- pirical Methods in Natural Language Processing (EMNLP). ACL, 1412-1421.</p>
<p>Natural Language Programming: Styles, Strategies, and Contrasts. A Lance, Miller, IBM Systems Journal. 20Lance A Miller. 1981. Natural Language Programming: Styles, Strategies, and Contrasts. IBM Systems Journal 20, 2 (1981), 184-215.</p>
<p>Natural Language Models for Predicting Programming Comments. Dana Movshovitz, -Attias William W Cohen, Annual Meeting of the Association for Computational Linguistics (ACL). ACL. Dana Movshovitz-Attias and William W Cohen. 2013. Natural Language Models for Predicting Programming Comments. In Annual Meeting of the Association for Computational Linguistics (ACL). ACL, 35-40.</p>
<p>Graham Neubig, arXiv:1703.01619Neural Machine Translation and Sequence-to-Sequence Models: A Tutorial. arXiv preprintGraham Neubig. 2017. Neural Machine Translation and Sequence-to-Sequence Models: A Tutorial. arXiv preprint arXiv:1703.01619 (2017).</p>
<p>Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, arXiv:1701.03980Swabha Swayamdipta, and Pengcheng Yin. 2017. DyNet: The Dynamic Neural Network Toolkit. arXiv preprintGraham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, and Pengcheng Yin. 2017. DyNet: The Dynamic Neural Network Toolkit. arXiv preprint arXiv:1701.03980 (2017).</p>
<p>Statistical Learning Approach for Mining API Usage Mappings for Code Migration. Anh Tuan Nguyen, Anh Hoan, Tung Thanh Nguyen, Tien N Nguyen, Nguyen, International Conference on Automated Software Engineering (ASE). Anh Tuan Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. 2014. Statistical Learning Approach for Mining API Usage Mappings for Code Migration. In International Conference on Automated Software Engineering (ASE).</p>
<p>Lexical Statistical Machine Translation for Language Migration. Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen, Joint Meeting on Foundations of Software Engineering (ESEC/FSE). ACMAnh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. 2013. Lexical Statistical Machine Translation for Language Migration. In Joint Meeting on Foundations of Software Engineering (ESEC/FSE). ACM, 651-654.</p>
<p>Learning to Rank Code Examples for Code Search Engines. Haoran Niu, Iman Keivanloo, Ying Zou, Empirical Software Engineering. Haoran Niu, Iman Keivanloo, and Ying Zou. 2016. Learning to Rank Code Examples for Code Search Engines. Empirical Software Engineering (2016), 1-33.</p>
<p>Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation. Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, International Conference on Automated Software Engineering (ASE). IEEEYusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation. In International Confer- ence on Automated Software Engineering (ASE). IEEE, 574-584.</p>
<p>Mining Source Code Descriptions from Developer Communications. Sebastiano Panichella, Jairo Aponte, Massimiliano Di Penta, Andrian Marcus, Gerardo Canfora, International Conference on Program Comprehension (ICPC). Sebastiano Panichella, Jairo Aponte, Massimiliano Di Penta, Andrian Marcus, and Gerardo Canfora. 2012. Mining Source Code Descriptions from Developer Communications. In International Conference on Program Comprehension (ICPC).</p>
<p>. IEEE. IEEE, 63-72.</p>
<p>Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes. Chris Quirk, Raymond Mooney, Michel Galley, Annual Meeting of the Association for Computational Linguistics (ACL. Chris Quirk, Raymond Mooney, and Michel Galley. 2015. Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes. In Annual Meeting of the Association for Computational Linguistics (ACL). 878-888.</p>
<p>Abstract Syntax Networks for Code Generation and Semantic Parsing. Maxim Rabinovich, Mitchell Stern, Dan Klein, Proceedings of the 55th. the 55thMaxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract Syntax Net- works for Code Generation and Semantic Parsing. In Proceedings of the 55th</p>
<p>Annual Meeting of the Association for Computational Linguistics. Vancouver, CanadaAssociation for Computational Linguistics1Long Papers)Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, 1139- 1149. http://aclweb.org/anthology/P17-1105</p>
<p>Predicting Program Properties from "Big Code. Veselin Raychev, Martin Vechev, Andreas Krause, ACM Symposium on Principles of Programming Languages (POPL). ACMVeselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting Program Properties from "Big Code". In ACM Symposium on Principles of Programming Languages (POPL). ACM, 111-124.</p>
<p>Get Another Label? Improving Data Quality and Data Mining using Multiple, Noisy Labelers. S Victor, Foster Sheng, Panagiotis G Provost, Ipeirotis, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACMVictor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. 2008. Get Another Label? Improving Data Quality and Data Mining using Multiple, Noisy Labelers. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 614-622.</p>
<p>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. Richard Socher, H Eric, Jeffrey Huang, Pennin, D Christopher, Andrew Y Manning, Ng, Advances in Neural Information Processing Systems (NIPS). Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems (NIPS). 801-809.</p>
<p>Generating Parameter Comments and Integrating with Method Summaries. Giriprasad Sridhara, Lori Pollock, K Vijay-Shanker, International Conference on Program Comprehension (ICPC). IEEEGiriprasad Sridhara, Lori Pollock, and K Vijay-Shanker. 2011. Generating Pa- rameter Comments and Integrating with Method Summaries. In International Conference on Program Comprehension (ICPC). IEEE, 71-80.</p>
<p>Dropout: a Simple Way to Prevent Neural Networks from Overfitting. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Journal of Machine Learning Research. 15Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research 15, 1 (2014), 1929-1958.</p>
<p>Sequence to Sequence Learning with Neural Networks. Ilya Sutskever, Oriol Vinyals, Quoc Vv Le, Advances in Neural Information Processing Systems (NIPS). Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to Sequence Learning with Neural Networks. In Advances in Neural Information Processing Systems (NIPS). 3104-3112.</p>
<p>Recovering Clear, Natural Identifiers from Obfuscated JavaScript Names. Bogdan Vasilescu, Casey Casalnuovo, Premkumar Devanbu, Joint Meeting on the Foundations of Software Engineering (ESEC/FSE). ACMto appearBogdan Vasilescu, Casey Casalnuovo, and Premkumar Devanbu. 2017. Recovering Clear, Natural Identifiers from Obfuscated JavaScript Names. In Joint Meeting on the Foundations of Software Engineering (ESEC/FSE). ACM. to appear.</p>
<p>Building a Semantic Parser Overnight. Yushi Wang, Jonathan Berant, Percy Liang, Annual Meeting of the Association for Computational Linguistics (ACL). ACL. Yushi Wang, Jonathan Berant, and Percy Liang. 2015. Building a Semantic Parser Overnight. In Annual Meeting of the Association for Computational Linguistics (ACL). ACL, 1332-1342.</p>
<p>Building Bing Developer Assistant. Yi Wei, Nirupama Chandrasekaran, Sumit Gulwani, Youssef Hamadi, . MSR-TR-2015-36Microsoft ResearchTechnical ReportYi Wei, Nirupama Chandrasekaran, Sumit Gulwani, and Youssef Hamadi. 2015. Building Bing Developer Assistant. Technical Report. MSR-TR-2015-36, Microsoft Research.</p>
<p>CloCom: Mining Existing Source Code for Automatic Comment Generation. Edmund Wong, Taiyue Liu, Lin Tan, International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEEEdmund Wong, Taiyue Liu, and Lin Tan. 2015. CloCom: Mining Existing Source Code for Automatic Comment Generation. In International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEE, 380-389.</p>
<p>AutoComment: Mining question and answer sites for automatic comment generation. Edmund Wong, Jinqiu Yang, Lin Tan, International Conference on Automated Software Engineering (ASE). IEEEEdmund Wong, Jinqiu Yang, and Lin Tan. 2013. AutoComment: Mining question and answer sites for automatic comment generation. In International Conference on Automated Software Engineering (ASE). IEEE, 562-567.</p>
<p>From query to usable code: an analysis of Stack Overflow code snippets. Di Yang, Aftab Hussain, Cristina Videira Lopes, Working Conference on Mining Software Repositories (MSR). ACMDi Yang, Aftab Hussain, and Cristina Videira Lopes. 2016. From query to usable code: an analysis of Stack Overflow code snippets. In Working Conference on Mining Software Repositories (MSR). ACM, 391-402.</p>
<p>StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow. Ziyu Yao, Daniel S Weld, Wei-Peng Chen, Huan Sun, WWW 2018: The 2018 Web Conference. Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. 2018. StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow. In WWW 2018: The 2018 Web Conference.</p>
<p>A Syntactic Neural Model for General-Purpose Code Generation. Pengcheng Yin, Graham Neubig, Meeting of the Association for Computational Linguistics (ACL). Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General- Purpose Code Generation. In Meeting of the Association for Computational Lin- guistics (ACL).</p>
<p>Example overflow: Using Social Media for Code Recommendation. Alexey Zagalsky, Ohad Barzilay, Amiram Yehudai, International Workshop on Recommendation Systems for Software Engineering (RSSE). IEEE PressAlexey Zagalsky, Ohad Barzilay, and Amiram Yehudai. 2012. Example overflow: Using Social Media for Code Recommendation. In International Workshop on Recommendation Systems for Software Engineering (RSSE). IEEE Press, 38-42.</p>            </div>
        </div>

    </div>
</body>
</html>
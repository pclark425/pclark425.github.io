<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3878 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3878</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3878</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-b146be9e80c66a6e062a1525693311fac65ae19e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b146be9e80c66a6e062a1525693311fac65ae19e" target="_blank">MatSciBERT: A materials domain language model for text mining and information extraction</a></p>
                <p><strong>Paper Venue:</strong> npj Computational Materials</p>
                <p><strong>Paper TL;DR:</strong> It is shown that MatSciBERT outperforms SciBERT, a language model trained on science corpus, and establish state-of-the-art results on three downstream tasks, named entity recognition, relation classification, and abstract classification.</p>
                <p><strong>Paper Abstract:</strong> A large amount of materials science knowledge is generated and stored as text published in peer-reviewed scientific literature. While recent developments in natural language processing, such as Bidirectional Encoder Representations from Transformers (BERT) models, provide promising information extraction tools, these models may yield suboptimal results when applied on materials domain since they are not trained in materials science specific notations and jargons. Here, we present a materials-aware language model, namely, MatSciBERT, trained on a large corpus of peer-reviewed materials science publications. We show that MatSciBERT outperforms SciBERT, a language model trained on science corpus, and establish state-of-the-art results on three downstream tasks, named entity recognition, relation classification, and abstract classification. We make the pre-trained weights of MatSciBERT publicly accessible for accelerated materials discovery and information extraction from materials science texts.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3878.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3878.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatSciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MatSciBERT (Materials Domain BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A materials-domain adapted BERT-style language model: SciBERT-initialized then further pre-trained on a ~285M-word materials science corpus to improve information extraction (NER, relation classification, document classification) for materials literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>MatSciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A domain-adapted transformer language model created by initializing from SciBERT weights and continuing masked-language-model pretraining on a large materials-science corpus (dynamic whole-word masking, no NSP, full-length sequences, RoBERTa-style recipe). It is then finetuned on downstream supervised tasks (NER, relation classification, abstract classification) to extract structured knowledge from materials papers.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Material Science Corpus (MSC): ~153,978 papers downloaded from Elsevier ScienceDirect across four broad material families (inorganic glasses & ceramics, bulk metallic glasses, alloys, cement & concrete), totaling ~284,518,744 tokens (~285M words). Corpus includes both full texts and abstracts, unicode-normalized and tokenized with SciBERT uncased vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>No separate natural-language meta-query engine described; tasks are specified as standard downstream inputs: tokenized sentences for NER, sentence+entity-span pairs for relation classification, and abstracts for document classification. Topic-selection for corpus construction used keyword queries to Crossref + SciBERT classifiers finetuned on 500 manually labeled abstracts to select relevant papers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Pretrain–finetune paradigm: continued masked language model pretraining (dynamic whole-word masking, 15% masking) on the MSC to adapt representations to materials domain, followed by supervised finetuning on downstream tasks (LM-Linear, LM-CRF, LM-BiLSTM-CRF for NER; entity-marker based finetuning for relation classification). The model is used to extract entities/relations and generate contextual embeddings that serve as distilled knowledge used for downstream extraction/synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Model weights and embeddings; task-specific outputs include BIO-tagged named entities (NER), directed relation labels between entity spans (relation classification), and binary/ multiclass document labels (abstract classification). Also used to extract named entities from image captions (lists of entity strings with types) enabling structured catalogs of materials concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Pretraining monitored by validation perplexity (final ppl = 3.112). Downstream evaluation: entity-level exact-match using CoNLL script; Micro-F1 and Macro-F1 for NER and relation classification; binary F1 for abstract classification. Results averaged across seeds and cross-validation splits where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Pretraining produced validation perplexity 3.112. MatSciBERT outperforms SciBERT on all tested downstream tasks: e.g., on the Matscholar dataset LM-CRF Micro-F1 = 87.54% (MatSciBERT) vs 86.31% (SciBERT); on glass vs non-glass classification F1 = 93.57% vs 91.04%; relation classification Macro-F1 = 87.87% vs 87.22%. For SOFC-Slot NER, MatSciBERT (LM-BiLSTM-CRF) improved Macro-F1 by ~4.2 points and Micro-F1 by ~2.1 points over SciBERT; for SOFC dataset improvements are ~1.9 Macro-F1 and ~1.9 Micro-F1. MatSciBERT consistently yields higher per-entity F1 for materials-related entity types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Corpus is substantially smaller than general SciBERT/BERT corpora (~285M words vs ~3.17B for SciBERT), so coverage is narrower. Pretraining required substantial compute (2x NVIDIA V100 32GB GPUs for 10 days). Limited availability of annotated materials-domain datasets constrains supervised finetuning and evaluation diversity. The work does not implement higher-level literature synthesis strategies (e.g., multi-document abstractive theory distillation, retrieval-augmented generation) or explicit reasoning chains; potential issues such as hallucination or incorrect cross-document synthesis are not addressed experimentally. Reliance on Elsevier-accessible papers may bias corpus coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared directly to SciBERT (used both as initialization and as baseline), MatSciBERT yields consistent improvements across tasks and surpasses previously reported state-of-the-art numbers on some materials datasets (e.g., SOFC-Slot and SOFC). No human expert comparative study (for synthesis/theory distillation) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MatSciBERT: A materials domain language model for text mining and information extraction', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3878.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3878.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBERT (Scientific BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-style language model pre-trained on a large corpus of scientific text (predominantly biomedical + computer science) used as the baseline and initialization point for MatSciBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SciBERT</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A domain-specific BERT variant pre-trained on scientific and biomedical corpora to provide contextual embeddings for scientific text. In this paper SciBERT is used as the initialization for MatSciBERT and as a baseline LM for downstream task comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Described in the paper as a scientific corpus consisting of ~82% biomedical and ~18% computer science papers (full size and exact counts not given in this manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Uses tokenized text as input; finetuned for tasks the same way as MatSciBERT (NER, relation classification, classification).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Pretrain–finetune paradigm (standard BERT-style masked language modeling pretraining followed by supervised finetuning on downstream tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Contextual embeddings and task-specific outputs (BIO tags, relation labels, document labels) when finetuned.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Serves as the primary baseline for quantitative comparisons on NER, relation classification and document classification tasks using Micro/Macro-F1 and binary F1 metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Provided baseline performance numbers in the tables (e.g., Matscholar LM-CRF Micro-F1 = 86.31%; glass classification F1 = 91.04%) which MatSciBERT improves upon. Specific per-dataset numbers are reported in the manuscript tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not trained on materials-domain text; vocabulary and representations may lack materials-specific tokens/jargon and therefore yield suboptimal performance on fine-grained materials extraction tasks compared to a domain-adapted LM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Used as the main baseline; MatSciBERT shows consistent improvements over SciBERT across tested tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MatSciBERT: A materials domain language model for text mining and information extraction', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3878.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3878.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioBERT (Biomedical BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical domain-adapted BERT model mentioned as an example of domain adaptation (BERT re-pretrained on biomedical corpora) but not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An instance of the pretrain–finetune paradigm where general BERT weights are further pretrained on biomedical text to improve downstream biomedical NLP tasks; cited here as motivation for domain adaptation to materials.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Biomedical clinical/academic corpora (details not provided in this manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not applicable in this manuscript (mentioned for context).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Continued masked-language-model pretraining on domain corpus, followed by supervised finetuning for tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Contextual embeddings and finetuned task outputs (not used here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not evaluated in this paper; cited as prior art motivating domain-specific LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as evidence that domain-specific re-pretraining helps downstream tasks in specialized domains (biomedical). No quantitative results reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this manuscript; mentioned only as prior example of domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not compared experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MatSciBERT: A materials domain language model for text mining and information extraction', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3878.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3878.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDataExtractor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemDataExtractor (chemical NLP pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An NLP pipeline/toolkit used in materials/chemistry literature mining to create structured databases from scientific text (e.g., battery materials, Curie/Néel temperatures, synthesis routes).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChemDataExtractor</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A rule-and-model based chemistry text-mining pipeline that extracts entities and relations from scientific articles (tables, captions, text) to build structured datasets. It is not a large transformer LLM but is cited as an example of successful literature mining applied to materials-related domains.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied in cited works to various materials and chemistry literature collections (examples in text include battery materials, magnetic Curie/Néel temperatures, inorganic synthesis routes). Exact corpus sizes not specified in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Typically uses pattern rules, chemical parsers and parsing heuristics applied to raw text/XML and table extractions (not described as natural-language prompting here).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Rule-based information extraction + NLP pipelines (tokenization, parsing, chemical entity recognition), not an LLM-driven distillation method.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured databases/tables of extracted chemical/material entities and properties suitable for downstream ML.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Prior cited studies validated extracted databases by using them to train ML models and by manual verification (details in cited works, not in this manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as enabling creation of datasets used for downstream ML (battery materials, Curie/Néel temperatures, synthesis routes), demonstrating utility of NLP pipelines for literature-to-structured-data conversion. No new results presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not an LLM; may be limited by rule coverage and ability to generalize to varied writing styles; dataset annotation scarcity in materials domain remains a bottleneck as noted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Cited works used it to build databases used for ML; explicit baseline comparisons not provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MatSciBERT: A materials domain language model for text mining and information extraction', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3878.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3878.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Artificial Chemist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Artificial Chemist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system referenced that takes precursor information and generates synthetic routes for optoelectronic semiconductors with targeted band gaps — an example of automated synthesis planning combining chemical parsing and algorithmic generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Artificial Chemist</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Described as a system that consumes precursor information and outputs synthetic routes for target material properties; cited as an example of NLP/ML-driven automation in materials/chemistry, but not an LLM-based literature synthesizer within this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not detailed in this paper; described conceptually as using precursor inputs and domain knowledge to generate synthesis routes.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Structured precursor specifications serve as input; not described as free-text prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified here; presented as an example of algorithmic synthesis planning rather than LLM-driven multi-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Synthetic routes (procedural outputs) for manufacturing optoelectronic semiconductors targeting band gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described in this paper (referenced work would contain details).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an application that demonstrates potential of NLP/ML to aid materials discovery and synthesis planning; no quantitative results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not discussed in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MatSciBERT: A materials domain language model for text mining and information extraction', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3878.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3878.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSCAR4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OSCAR4 (Chemical entity recognition tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical parser/toolcapable of identifying entities and chemicals from text, cited as an enabling technology for materials-domain information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>OSCAR4</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A chemical named-entity recognition/parsing tool that detects chemical entities in text; cited as an example of domain-specific parsers used in materials informatics pipelines rather than an LLM-based synthesis system.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Used on chemistry/materials text corpora in prior work (details not given here).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Pattern/parsing based extraction from text; not given as natural-language query input in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Rule-based/ML-based chemical entity recognition (not a transformer LLM in the context described here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Identified chemical entities/tokens for downstream use in databases and ML pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not specified in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited to illustrate available tools for extracting entities from scientific text; no experimental details or metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not an LLM; limited to chemical entity recognition and may not capture wider contextual or cross-document synthesis capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not compared in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MatSciBERT: A materials domain language model for text mining and information extraction', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3878",
    "paper_id": "paper-b146be9e80c66a6e062a1525693311fac65ae19e",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "MatSciBERT",
            "name_full": "MatSciBERT (Materials Domain BERT)",
            "brief_description": "A materials-domain adapted BERT-style language model: SciBERT-initialized then further pre-trained on a ~285M-word materials science corpus to improve information extraction (NER, relation classification, document classification) for materials literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "MatSciBERT",
            "system_or_method_description": "A domain-adapted transformer language model created by initializing from SciBERT weights and continuing masked-language-model pretraining on a large materials-science corpus (dynamic whole-word masking, no NSP, full-length sequences, RoBERTa-style recipe). It is then finetuned on downstream supervised tasks (NER, relation classification, abstract classification) to extract structured knowledge from materials papers.",
            "input_corpus_description": "Material Science Corpus (MSC): ~153,978 papers downloaded from Elsevier ScienceDirect across four broad material families (inorganic glasses & ceramics, bulk metallic glasses, alloys, cement & concrete), totaling ~284,518,744 tokens (~285M words). Corpus includes both full texts and abstracts, unicode-normalized and tokenized with SciBERT uncased vocabulary.",
            "topic_or_query_specification": "No separate natural-language meta-query engine described; tasks are specified as standard downstream inputs: tokenized sentences for NER, sentence+entity-span pairs for relation classification, and abstracts for document classification. Topic-selection for corpus construction used keyword queries to Crossref + SciBERT classifiers finetuned on 500 manually labeled abstracts to select relevant papers.",
            "distillation_method": "Pretrain–finetune paradigm: continued masked language model pretraining (dynamic whole-word masking, 15% masking) on the MSC to adapt representations to materials domain, followed by supervised finetuning on downstream tasks (LM-Linear, LM-CRF, LM-BiLSTM-CRF for NER; entity-marker based finetuning for relation classification). The model is used to extract entities/relations and generate contextual embeddings that serve as distilled knowledge used for downstream extraction/synthesis tasks.",
            "output_type_and_format": "Model weights and embeddings; task-specific outputs include BIO-tagged named entities (NER), directed relation labels between entity spans (relation classification), and binary/ multiclass document labels (abstract classification). Also used to extract named entities from image captions (lists of entity strings with types) enabling structured catalogs of materials concepts.",
            "evaluation_or_validation_method": "Pretraining monitored by validation perplexity (final ppl = 3.112). Downstream evaluation: entity-level exact-match using CoNLL script; Micro-F1 and Macro-F1 for NER and relation classification; binary F1 for abstract classification. Results averaged across seeds and cross-validation splits where applicable.",
            "results_summary": "Pretraining produced validation perplexity 3.112. MatSciBERT outperforms SciBERT on all tested downstream tasks: e.g., on the Matscholar dataset LM-CRF Micro-F1 = 87.54% (MatSciBERT) vs 86.31% (SciBERT); on glass vs non-glass classification F1 = 93.57% vs 91.04%; relation classification Macro-F1 = 87.87% vs 87.22%. For SOFC-Slot NER, MatSciBERT (LM-BiLSTM-CRF) improved Macro-F1 by ~4.2 points and Micro-F1 by ~2.1 points over SciBERT; for SOFC dataset improvements are ~1.9 Macro-F1 and ~1.9 Micro-F1. MatSciBERT consistently yields higher per-entity F1 for materials-related entity types.",
            "limitations_or_challenges": "Corpus is substantially smaller than general SciBERT/BERT corpora (~285M words vs ~3.17B for SciBERT), so coverage is narrower. Pretraining required substantial compute (2x NVIDIA V100 32GB GPUs for 10 days). Limited availability of annotated materials-domain datasets constrains supervised finetuning and evaluation diversity. The work does not implement higher-level literature synthesis strategies (e.g., multi-document abstractive theory distillation, retrieval-augmented generation) or explicit reasoning chains; potential issues such as hallucination or incorrect cross-document synthesis are not addressed experimentally. Reliance on Elsevier-accessible papers may bias corpus coverage.",
            "comparison_to_baselines_or_humans": "Compared directly to SciBERT (used both as initialization and as baseline), MatSciBERT yields consistent improvements across tasks and surpasses previously reported state-of-the-art numbers on some materials datasets (e.g., SOFC-Slot and SOFC). No human expert comparative study (for synthesis/theory distillation) is reported.",
            "uuid": "e3878.0",
            "source_info": {
                "paper_title": "MatSciBERT: A materials domain language model for text mining and information extraction",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "SciBERT",
            "name_full": "SciBERT (Scientific BERT)",
            "brief_description": "A BERT-style language model pre-trained on a large corpus of scientific text (predominantly biomedical + computer science) used as the baseline and initialization point for MatSciBERT.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "SciBERT",
            "system_or_method_description": "A domain-specific BERT variant pre-trained on scientific and biomedical corpora to provide contextual embeddings for scientific text. In this paper SciBERT is used as the initialization for MatSciBERT and as a baseline LM for downstream task comparisons.",
            "input_corpus_description": "Described in the paper as a scientific corpus consisting of ~82% biomedical and ~18% computer science papers (full size and exact counts not given in this manuscript).",
            "topic_or_query_specification": "Uses tokenized text as input; finetuned for tasks the same way as MatSciBERT (NER, relation classification, classification).",
            "distillation_method": "Pretrain–finetune paradigm (standard BERT-style masked language modeling pretraining followed by supervised finetuning on downstream tasks).",
            "output_type_and_format": "Contextual embeddings and task-specific outputs (BIO tags, relation labels, document labels) when finetuned.",
            "evaluation_or_validation_method": "Serves as the primary baseline for quantitative comparisons on NER, relation classification and document classification tasks using Micro/Macro-F1 and binary F1 metrics.",
            "results_summary": "Provided baseline performance numbers in the tables (e.g., Matscholar LM-CRF Micro-F1 = 86.31%; glass classification F1 = 91.04%) which MatSciBERT improves upon. Specific per-dataset numbers are reported in the manuscript tables.",
            "limitations_or_challenges": "Not trained on materials-domain text; vocabulary and representations may lack materials-specific tokens/jargon and therefore yield suboptimal performance on fine-grained materials extraction tasks compared to a domain-adapted LM.",
            "comparison_to_baselines_or_humans": "Used as the main baseline; MatSciBERT shows consistent improvements over SciBERT across tested tasks.",
            "uuid": "e3878.1",
            "source_info": {
                "paper_title": "MatSciBERT: A materials domain language model for text mining and information extraction",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "BioBERT",
            "name_full": "BioBERT (Biomedical BERT)",
            "brief_description": "A biomedical domain-adapted BERT model mentioned as an example of domain adaptation (BERT re-pretrained on biomedical corpora) but not used experimentally in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "BioBERT",
            "system_or_method_description": "An instance of the pretrain–finetune paradigm where general BERT weights are further pretrained on biomedical text to improve downstream biomedical NLP tasks; cited here as motivation for domain adaptation to materials.",
            "input_corpus_description": "Biomedical clinical/academic corpora (details not provided in this manuscript).",
            "topic_or_query_specification": "Not applicable in this manuscript (mentioned for context).",
            "distillation_method": "Continued masked-language-model pretraining on domain corpus, followed by supervised finetuning for tasks.",
            "output_type_and_format": "Contextual embeddings and finetuned task outputs (not used here).",
            "evaluation_or_validation_method": "Not evaluated in this paper; cited as prior art motivating domain-specific LMs.",
            "results_summary": "Mentioned as evidence that domain-specific re-pretraining helps downstream tasks in specialized domains (biomedical). No quantitative results reported in this paper.",
            "limitations_or_challenges": "Not discussed in this manuscript; mentioned only as prior example of domain adaptation.",
            "comparison_to_baselines_or_humans": "Not compared experimentally in this paper.",
            "uuid": "e3878.2",
            "source_info": {
                "paper_title": "MatSciBERT: A materials domain language model for text mining and information extraction",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "ChemDataExtractor",
            "name_full": "ChemDataExtractor (chemical NLP pipeline)",
            "brief_description": "An NLP pipeline/toolkit used in materials/chemistry literature mining to create structured databases from scientific text (e.g., battery materials, Curie/Néel temperatures, synthesis routes).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "ChemDataExtractor",
            "system_or_method_description": "A rule-and-model based chemistry text-mining pipeline that extracts entities and relations from scientific articles (tables, captions, text) to build structured datasets. It is not a large transformer LLM but is cited as an example of successful literature mining applied to materials-related domains.",
            "input_corpus_description": "Applied in cited works to various materials and chemistry literature collections (examples in text include battery materials, magnetic Curie/Néel temperatures, inorganic synthesis routes). Exact corpus sizes not specified in this manuscript.",
            "topic_or_query_specification": "Typically uses pattern rules, chemical parsers and parsing heuristics applied to raw text/XML and table extractions (not described as natural-language prompting here).",
            "distillation_method": "Rule-based information extraction + NLP pipelines (tokenization, parsing, chemical entity recognition), not an LLM-driven distillation method.",
            "output_type_and_format": "Structured databases/tables of extracted chemical/material entities and properties suitable for downstream ML.",
            "evaluation_or_validation_method": "Prior cited studies validated extracted databases by using them to train ML models and by manual verification (details in cited works, not in this manuscript).",
            "results_summary": "Cited as enabling creation of datasets used for downstream ML (battery materials, Curie/Néel temperatures, synthesis routes), demonstrating utility of NLP pipelines for literature-to-structured-data conversion. No new results presented here.",
            "limitations_or_challenges": "Not an LLM; may be limited by rule coverage and ability to generalize to varied writing styles; dataset annotation scarcity in materials domain remains a bottleneck as noted.",
            "comparison_to_baselines_or_humans": "Cited works used it to build databases used for ML; explicit baseline comparisons not provided in this manuscript.",
            "uuid": "e3878.3",
            "source_info": {
                "paper_title": "MatSciBERT: A materials domain language model for text mining and information extraction",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Artificial Chemist",
            "name_full": "Artificial Chemist",
            "brief_description": "A system referenced that takes precursor information and generates synthetic routes for optoelectronic semiconductors with targeted band gaps — an example of automated synthesis planning combining chemical parsing and algorithmic generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Artificial Chemist",
            "system_or_method_description": "Described as a system that consumes precursor information and outputs synthetic routes for target material properties; cited as an example of NLP/ML-driven automation in materials/chemistry, but not an LLM-based literature synthesizer within this manuscript.",
            "input_corpus_description": "Not detailed in this paper; described conceptually as using precursor inputs and domain knowledge to generate synthesis routes.",
            "topic_or_query_specification": "Structured precursor specifications serve as input; not described as free-text prompting.",
            "distillation_method": "Not specified here; presented as an example of algorithmic synthesis planning rather than LLM-driven multi-document synthesis.",
            "output_type_and_format": "Synthetic routes (procedural outputs) for manufacturing optoelectronic semiconductors targeting band gaps.",
            "evaluation_or_validation_method": "Not described in this paper (referenced work would contain details).",
            "results_summary": "Mentioned as an application that demonstrates potential of NLP/ML to aid materials discovery and synthesis planning; no quantitative results provided here.",
            "limitations_or_challenges": "Not discussed in this manuscript.",
            "comparison_to_baselines_or_humans": "Not discussed in this manuscript.",
            "uuid": "e3878.4",
            "source_info": {
                "paper_title": "MatSciBERT: A materials domain language model for text mining and information extraction",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "OSCAR4",
            "name_full": "OSCAR4 (Chemical entity recognition tool)",
            "brief_description": "A chemical parser/toolcapable of identifying entities and chemicals from text, cited as an enabling technology for materials-domain information extraction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "OSCAR4",
            "system_or_method_description": "A chemical named-entity recognition/parsing tool that detects chemical entities in text; cited as an example of domain-specific parsers used in materials informatics pipelines rather than an LLM-based synthesis system.",
            "input_corpus_description": "Used on chemistry/materials text corpora in prior work (details not given here).",
            "topic_or_query_specification": "Pattern/parsing based extraction from text; not given as natural-language query input in this manuscript.",
            "distillation_method": "Rule-based/ML-based chemical entity recognition (not a transformer LLM in the context described here).",
            "output_type_and_format": "Identified chemical entities/tokens for downstream use in databases and ML pipelines.",
            "evaluation_or_validation_method": "Not specified in this manuscript.",
            "results_summary": "Cited to illustrate available tools for extracting entities from scientific text; no experimental details or metrics provided in this paper.",
            "limitations_or_challenges": "Not an LLM; limited to chemical entity recognition and may not capture wider contextual or cross-document synthesis capabilities.",
            "comparison_to_baselines_or_humans": "Not compared in this manuscript.",
            "uuid": "e3878.5",
            "source_info": {
                "paper_title": "MatSciBERT: A materials domain language model for text mining and information extraction",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01787925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MatSciBERT: A Materials Domain Language Model for Text Mining and Information Extraction</h1>
<p>Tanishq Gupta ${ }^{1}$, Mohd Zaki ${ }^{2}$, N. M. Anoop Krishnan ${ }^{2,3, <em>}$, Mausam ${ }^{3,4, </em>}$<br>${ }^{1}$ Deparment of Mathematics, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India 110016<br>${ }^{2}$ Deparment of Civil Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India 110016<br>${ }^{3}$ School of Artificial Intelligence, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016<br>${ }^{4}$ Deparment of Computer Science and Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India 110016<br>*Corresponding authors: N. M. A. Krishnan (krishnan@iitd.ac.in), Mausam<br>mausam@iitd.ac.in)</p>
<h4>Abstract</h4>
<p>An overwhelmingly large amount of knowledge in the materials domain is generated and stored as text published in peer-reviewed scientific literature. Recent developments in natural language processing, such as bidirectional encoder representations from transformers (BERT) models, provide promising tools to extract information from these texts. However, direct application of these models in the materials domain may yield suboptimal results as the models themselves may not be trained on notations and jargon that are specific to the domain. Here, we present a materials-aware language model, namely, MatSciBERT, which is trained on a large corpus of scientific literature published in the materials domain. We further evaluate the performance of MatSciBERT on three downstream tasks, namely, abstract classification, named entity recognition, and relation extraction, on different materials datasets. We show that MatSciBERT outperforms SciBERT, a language model trained on science corpus, on all the tasks. Further, we discuss some of the applications of MatSciBERT in the materials domain for extracting information, which can, in turn, contribute to materials discovery or optimization. Finally, to make the work accessible to the larger materials community, we make the pretrained and finetuned weights and the models of MatSciBERT freely accessible.</p>
<h2>1 Introduction</h2>
<p>The naming of human civilization after the materials used in those periods signifies their importance in our daily lives. Historically starting from the stone, bronze and iron ages, we have now come to the times when we are using a variety of alloys like magnesium ${ }^{1}$ and aluminum ${ }^{2}$ alloys in aerospace and automobiles, titanium alloys in biocompatible implants ${ }^{3}$, glasses for optical and communication devices ${ }^{4}$, and concrete for construction activities ${ }^{5}$. Despite technological advancement in experimental and computation domains, discovering new materials and bringing them to market is still a time-consuming process that may span decades ${ }^{6,7}$. To accelerate this process, we need to understand the properties of existing elements and their compounds which are the building block of materials ${ }^{8-13}$. Textbooks, scientific publications, reports, handbooks, websites etc., serve as a large data repository that can be mined for obtaining the already existing desired information ${ }^{14,15}$. However, it is a humanly impossible task to go through all the available literature and extract relevant information. Further, advancements in machine learning (ML) and natural language processing (NLP) have</p>
<p>enabled researchers to automate the information extraction from the text. Although it is a challenging task, since most of the information in scientific journals is unstructured, for example, experimental procedures are reported in the form of paragraphs and image captions having findings of experiments observed using graphs and different microscopy and diffraction techniques.</p>
<p>In the material science domain, researchers have used NLP tools to automate database creation for ML applications. One such example is ChemDataExtractor ${ }^{16}$, an NLP pipeline used to create databases of battery materials ${ }^{17}$, Curie and Néel temperatures of magnetic materials ${ }^{18}$, and inorganic material synthesis routes ${ }^{19}$, thus demonstrating NLP applications' ability to understand the chemical and material science entities from the scientific text. In other related works, researchers have used NLP to collect the composition and dissolution rate of calcium aluminosilicate glassy materials from tables published in research articles and, hence, predict the property using ML algorithms ${ }^{20}$. Similarly, using the table extraction tools, the researchers have used ML on a dataset created through NLP assisted automated extraction of zeolite synthesis routes to synthesize germanium containing zeolites. In glass science, researchers have used latent-Dirichlet allocation to classify the literature into 15 broad categories. Further, using caption cluster plots, researchers have made it possible to find papers having specific elements, results from different experimental techniques and are related to broad categories of glass science ${ }^{15}$. The use of NLP to extract process and testing parameters of oxide glasses followed by including the extracted parameters as input parameters has improved the prediction of the Vickers hardness ${ }^{21}$. This increased use of NLP and ML in the material science domain implies the need for a universal NLP tool that can understand the domain-specific entities and provide convenient solutions for such automated extractions.</p>
<p>A comprehensive review by Olivetti et al. (2019) describes several ways in which NLP can benefit the material science community ${ }^{22}$. Providing insights into chemical parsing tools like OSCAR4 ${ }^{23}$ capable of identifying entities and chemicals from text, Artificial Chemist ${ }^{24}$, which takes the input of precursor information and generates synthetic routes to manufacture optoelectronic semiconductors with targeted band gaps, robotic system for making thin films to produce cleaner and sustainable energy solutions ${ }^{25}$, and identification of more than 80 million material science domain-specific named entities, researches have prompted the accelerated discovery of novel materials for different applications through the combination of ML and NLP techniques.</p>
<p>For using text in NLP applications, non-neural methods are based on n-grams such as Brown et al. (1992) ${ }^{27}$, structural learning framework by Ando and Zhang (2005) ${ }^{28}$, or structural correspondence learning by Blitzer et al. (2006) ${ }^{29}$, but these are no longer the state of the art. Neural pre-trained embeddings like word2vec ${ }^{30,31}$ and GloVe ${ }^{32}$ are quite popular, but lack material science knowledge, and do not produce contextual embeddings -- a word embedding in the context of the accompanying text. Recent progress in NLP has led to the development of a novel computational paradigm in which a large, pre-trained language model (LM) is finetuned for domain specific tasks. Research has consistently shown that this pretrain-finetune paradigm leads to the best overall task performance. Statistically, LMs are probability distributions for a sequence of words such that for a given set of words, it assigns probability to each word ${ }^{26}$. Recently, due to availability of large amounts of text and high computing power, researchers are able to pre-train these large neural language models. For example, Bidirectional Encoder Representations from Transformers (BERT) ${ }^{33}$ is trained on BookCorpus ${ }^{34}$ and English Wikipedia, resulting in a state-of-the-art performance on multiple NLP tasks like question answering and entity recognition, to name a few.</p>
<p>Although researchers have shown the domain adaptation capability of word2vec and BERT in the field of biological sciences as BioWordVec ${ }^{35}$ and BioBERT ${ }^{36}$, other domain-specific BERTs like SciBERT ${ }^{37}$ trained on scientific and biomedical corpus ${ }^{38}$, clinicalBERT ${ }^{39}$ trained on 2 million clinical notes in MIMIC-III v1.4 database ${ }^{40}$, mBERT ${ }^{41}$ for multilingual machine translations tasks, patentBERT ${ }^{42}$ for patent classification and FinBERT for financial tasks ${ }^{43}$, there is a lack of material science aware LM which can accelerate the research in the field by further adapting to downstream tasks. This has been indeed cited as a major challenge in several previous works ${ }^{14,22}$. Therefore, in this work, we train material science domain-specific BERT and achieve state of the art results on domain-specific tasks as listed below, details of which are described in the Results and discussion section of the paper.
a. NER on SOFC, SOFC Slot dataset by Friedrich et al. (2020) ${ }^{44}$ and Matscholar dataset by Weston et al. (2019) ${ }^{14}$
b. Glass vs Non-Glass classification ${ }^{15}$
c. Relation Classification on MSPT corpus ${ }^{45}$</p>
<p>The present work, thus, bridges the gap in the availability of a materials domain language mode allowing researchers to automate information extraction, knowledge graph completion and hence accelerate the discovery of novel materials.</p>
<h1>2 Methodology</h1>
<p>Figure 1 shows the graphical summary of the methodology adopted in this work encompassing creating the material science corpus, training the MatSciBERT, and evaluating on different downstream tasks.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The methodology for training MatSciBERT followed by evaluation on downstream tasks.</p>
<h3>2.1 Dataset collection and preparation</h3>
<p>In the training of an LM in a generalizable way, a considerable amount of dataset is required. For example, BERT ${ }^{33}$ was pre-trained on BookCorpus ${ }^{34}$ and English Wikipedia, containing a total of 3.3 billion words. SciBERT ${ }^{37}$, an LM trained on scientific literature, was pre-trained</p>
<p>using a corpus consisting of $82 \%$ papers from the broad biomedical domain and $18 \%$ papers from the computer science domain. However, we note that none of these LMs includes text related to the materials domain. Here, we consider materials science literature from four broad categories, namely, inorganic glasses and ceramics, metallic glasses, cement and concrete, and alloys, to cover the materials domain in a representative fashion.</p>
<p>The first step in retrieving the research papers is to query search from the Crossref metadata database ${ }^{7}$. This resulted in a list of more than 1 M articles. Although Crossref gives the search results from different journals and publishers, we downloaded papers only from the Elsevier Science Direct database using their sanctioned API ${ }^{8}$. Note that the Elsevier API returns the research articles in XML format; hence, we wrote a custom XML parser for extracting the text. Occasionally, there were papers having only abstract and not full-text depending upon the journal and publication date. Therefore, we have included all the sections of the paper when available and abstracts otherwise. For glass science related papers, the details are given in our previous work ${ }^{15}$. For concrete and alloys, we first downloaded many research papers for each material category using several queries such as 'cement', 'interfacial transition zone', 'magnesium alloy', and 'magnesium alloy composite materials' to name a few.</p>
<p>Since all the downloaded papers did not belong to a particular class of materials, we manually annotated 500 papers based on their abstracts, whether they were relevant to the field of interest or not. Further, we finetuned SciBERT classifiers ${ }^{37,49}$, one for each category of material, on these labelled abstracts for identifying relevant papers among the downloaded 1 M articles. We consider these selected papers from each category of materials for training the language model. A detailed description of the Material Science Corpus (MSC) is given in the Results and Discussion section of the paper. Finally, we divided this corpus into training and validation, with $85 \%$ being used to train the language models and the remaining $15 \%$ to see their performance on unseen text.</p>
<p>Note that the texts in the scientific literature may have several symbols, including some random characters. To address these anomalies, we also performed Unicode normalization of MSC to:
A. get rid of random Unicode characters like $\boxplus 1, \exists, \boxed{1}$, and
B. map different Unicode characters having similar meaning and appearance to either a single standard character or a sequence of standard characters.</p>
<p>For example, \% gets mapped to $\%,&gt;$ to $&gt;, \gg$ to $&gt;&gt;&gt;$, " and $=$ to $=, 3 / 4$ to $3 / 4$, to name a few. First, we normalized the corpus using BertNormalizer from the tokenizers library by Hugging Face ${ }^{47,50}$. Next, we created a list containing mappings of the Unicode characters appearing in the MSC. We mapped random characters to space so that they do not interfere during pretraining. It's important to note that we also perform this normalization step on every dataset before passing it through MatSciBERT. This dataset was then used to pretrain the MatSciBERT.</p>
<h1>2.2 Pre-training of MatSciBERT</h1>
<p>We pre-train MatSciBERT on MSC as detailed in the last sub-section. Pre-training an LM from scratch requires significant computational power and a large dataset. To address this issue, we initialize MatSciBERT with weights from SciBERT and perform tokenization using the SciBERT uncased vocabulary. This has the additional advantage that existing models relying on SciBERT, which is pretrained on biomedical and computer science, can be interchangeably used with MatSciBERT, making both the LMs compatible with each other. Further, the</p>
<p>vocabulary existing in the scientific literature as constructed by SciBERT may be used to represent the new words in the materials domain.</p>
<p>To pre-train MatSciBERT, we employ the improved training recipe suggested by Liu et al. (2019) used to train the RoBERTa ${ }^{51}$. Specifically, the following simple modifications were adopted for MatSciBERT pre-training: (i) Dynamic whole word masking, (ii) Removing the NSP loss from the training objective, (iii) Training on full-length sequences, (iv) Using larger batch sizes. Following these modifications, we pre-train MatSciBERT on the MSC with a maximum sequence length of 512 tokens for 10 days on 2 NVIDIA V100 32GB GPUs with a batch size of 256 sequences per GPU. We use dynamic whole-word masking as the training objective. It involves doing masking at the word level instead of masking at the WordPiece level, as discussed in the new release of BERT pre-training code ${ }^{52}$ by Google. Each time a sequence is sampled, we randomly mask $15 \%$ of the words and let the model predict each masked WordPiece token independently. To ensure training on full length sentences, we allow input sentences to contain segments of more than one document and separate documents using the [SEP] token. We use the AdamW optimizer with $\beta_{1}=0.9, \beta_{2}=0.98, \varepsilon=1 \mathrm{e}^{-6}$, weight decay $=1 \mathrm{e}^{-2}$ and linear decay schedule for learning rate with warmup ratio $=4.8 \%$ and peak learning rate $=1 \mathrm{e}^{-4}$. Pre-training code is written using PyTorch and Transformers library and is available at our GitHub repository for this work.</p>
<h1>2.3 Downstream Tasks</h1>
<p>Once the LM is pre-trained, we finetune it on various supervised downstream tasks. This is done so as to adapt the model to specific tasks as well as to learn the task-specific randomly initialized weights present in the output layer. We evaluate the performance of MatSciBERT on the following three downstream NLP tasks:
(i) Named Entity Recognition
(ii) Text/Abstract Classification
(iii)Relation Classification</p>
<h3>2.3.1 Tasks Description</h3>
<p>(i) Named Entity Recognition (NER) involves identifying domain-specific named entities in a given sentence. Entities are encoded using the BIO scheme to account for multi-token entities ${ }^{53}$. Dataset for the NER task includes various sentences, with each sentence being split into multiple tokens. Gold labels are provided for each token. More formally, Let $\mathrm{E}=\left{\mathrm{e}<em _mathrm_k="\mathrm{k">{1}, \ldots\right.$ $\mathrm{e}</em>}}}$ be the set of k entity types for a given dataset. If $\left[\mathrm{x<em _mathrm_n="\mathrm{n">{1}, \ldots \mathrm{x}</em>}}\right]$ are tokens of a sentence and $\left[\mathrm{y<em _mathrm_n="\mathrm{n">{1}, \ldots \mathrm{y}</em>}}\right]$ are labels for these tokens, then each $\mathrm{y<em 1="1">{\mathrm{i}} \in \mathrm{L}=\left{\mathrm{B}-\mathrm{e}</em>}, \mathrm{I}-\mathrm{e<em _mathrm_k="\mathrm{k">{1}, \ldots \mathrm{~B}-\mathrm{e}</em>}}, \mathrm{I}-\mathrm{e<em 1="1">{\mathrm{k}}, \mathrm{O}\right}$.
(ii) Input for the Relation Classification ${ }^{54}$ task consists of a sentence and an ordered pair of entity spans in that sentence. Output is a label denoting the directed relationship between the two entities. The two entity spans can be represented as $\mathrm{s}</em>)$, where i and j denote the starting and ending index of the first entity and similarly k and 1 denote the starting and ending index of the second entity in the input statement. Here $i \leq j, k \leq l$ and $(j&lt;$ $k$ or $l&lt;i)$. The last constraint guarantees that the two entities do not overlap with each other. The output label belongs to L , where L is a fixed set of relation types.
(iii) In the Text/Abstract Classification task, we are given an abstract of a research paper, and we have to classify whether the abstract is relevant to a given field or not.}=(\mathrm{i}, \mathrm{j})$ and $\mathrm{s}_{2}=(\mathrm{k}, \mathrm{l</p>
<h3>2.3.2 Datasets</h3>
<p>We use the following three Material Science based NER datasets to evaluate the performance of MatSciBERT against SciBERT:</p>
<ol>
<li>Matscholar NER dataset ${ }^{14,55}$ by Weston et al. (2019): This dataset is publicly available and contains 7 different entity types. Training, validation and test set consists of 440, 511 and 546 sentences, respectively. Entity types present in this dataset are inorganic material (MAT), symmetry/phase label (SPL), sample descriptor (DSC), material property (PRO), material application (APL), synthesis method (SMT) and characterization method (CMT).</li>
<li>Solid Oxide Fuel Cells - Entity Mention Extraction (SOFC) dataset ${ }^{44}$ by Friedrich et al. (2020): This dataset consists of 45 open-access scholarly articles annotated by domain experts. Four different entity types have been annotated by the authors, namely Material, Experiment, Value and Device. There are 611, 92 and 173 sentences in the training, validation and test sets.</li>
<li>Solid Oxide Fuel Cells - Slot Filling (SOFC-Slot) dataset ${ }^{44}$ by Friedrich et al. (2020): This is the same as the above dataset except that entity types are more fine-grained. There are 16 different entity types, namely Anode Material, Cathode Material, Conductivity, Current Density, Degradation Rate, Device, Electrolyte Material, Fuel Used, Interlayer Material, Open Circuit Voltage, Power Density, Resistance, Support Material, Time of Operation, Voltage and Working Temperature. Two additional entity types: Experiment Evoking Word and Thickness, are used for training the models.</li>
</ol>
<p>For relation classification, we use the Materials Synthesis Procedures dataset ${ }^{45}$ by Mysore et al. (2019). This dataset consists of 230 synthesis procedures annotated as graphs where nodes represent the participants of synthesis steps, and edges specify the relationships between the nodes. The average length of a synthesis procedure is 9 sentences, and 26 tokens are present in each sentence on average. The dataset consists of 16 relation labels. The relation labels have been divided into three categories by the authors:
a. Operation-Argument relations: Recipe target, Solvent material, Atmospheric material, Recipe precursor, Participant material, Apparatus of, Condition of
b. Non-Operation Entity relations: Descriptor of, Number of, Amount of, Apparatus-attrof, Brand of, Core of, Property of, Type of
c. Operation-Operation relations: Next operation</p>
<p>The train, validation, and test set consists of 150,30 and 50 annotated material synthesis procedures, respectively.</p>
<p>The dataset for classifying research papers related to glass science or not on the basis of their abstracts has been taken from Venugopal et al. (2021) ${ }^{15}$. The authors have manually labelled 1500 abstracts as glass and non-glass. These abstracts belong to different fields of glass science like bioactive glasses, rare earth glasses, glass ceramics, thin film studies, and optical, dielectric, and thermal properties of glasses, to name a few. We divide the abstracts into a train-validation-test split of 3:1:1.</p>
<h1>2.4 Modelling</h1>
<h3>2.4.1 Named Entity Recognition</h3>
<p>We use the BERT contextual output embedding of the first WordPiece of every token to classify the tokens among $|\mathrm{L}|$ classes. We model the NER task using three architectures: LMLinear, LM-CRF and LM-BiLSTM-CRF. Here, LM can be replaced by any BERT-based transformer model. We take LM to be SciBERT and MatSciBERT in this work.</p>
<ol>
<li>LM-Linear: The output embedding of the WordPieces are passed through a linear layer with softmax activation. We use the BERT Token Classifier implementation of transformers library ${ }^{50}$.</li>
<li>LM-CRF: We replace the final softmax activation of the LM-Linear architecture with a CRF layer ${ }^{56}$ so that the model can learn to label the tokens belonging to the same entity mentioned and also learn the transition scores between different entity types. We use the CRF implementation of pytorch-crf library ${ }^{57}$.</li>
<li>LM-BiLSTM-CRF: Bidirectional Long Short Term Memory ${ }^{58}$ is added in between the LM and CRF layer. BERT embeddings of all the WordPieces are passed through a stacked BiLSTM. The output of BiLSTM is finally fed to the CRF layer to make predictions.</li>
</ol>
<h1>2.4.2 Relation Classification</h1>
<p>We use the Entity Markers-Entity Start architecture ${ }^{54}$ proposed by Soares et al. for modelling of the relation classification task. Here, we surround the entity spans within the sentence with some special WordPieces. We wrap the first and second entities with [E1], [\E1] and [E2], [E2] respectively. We concatenate the output embeddings of [E1] and [E2] and then pass it through a linear layer with softmax activation. We use the Standard Cross Entropy loss function for the training of the linear layer and finetuning of the language model.</p>
<h3>2.4.3 Text/Abstract Classification</h3>
<p>We use the output embedding of the CLS token to encode the entire text/abstract. We pass this embedding through a simple classifier to make predictions. We use the BERT Sentence Classifier implementation of the transformers library ${ }^{50}$.</p>
<h3>2.4.4 Hyperparameters</h3>
<p>We use a linear decay schedule for the learning rate with a warmup ratio of 0.1 . To ensure sufficient training of randomly initialized non-BERT layers, we set different learning rates for the BERT part and non-BERT part. We set the peak learning rate of the non-BERT part to 3e4 and choose the peak learning rate of the BERT part from [2e-5, 3e-5, 5e-5], whichever results in a maximum validation score averaged across 3 seeds. We use a batch size of 16 and AdamW optimizer for all the architectures. For LM-BiLSTM-CRF architecture, we use a 2-layer stacked BiLSTM with a hidden dimension of 300 and dropout of 0.2 in between the layers. We perform finetuning for 15, 20 and 40 epochs for Matscholar, SOFC and SOFC Slot datasets, respectively, as initial experiments exhibited little or no improvement after the specified number of epochs. All the weights of any given architecture are updated during finetuning, i.e., we do not freeze any of the weights. We make the code for finetuning and different architectures publicly available. We refer readers to the code for further details about the hyperparameters.</p>
<h3>2.4.5 Evaluation Metrics</h3>
<p>We evaluate the NER task based on entity-level exact matches. We use the CoNLL evaluation script (https://github.com/spyysalo/conlleval.py) after verifying its correctness. For NER and Relation Classification tasks, we use Micro-F1 and Macro-F1 as the primary evaluation metrics. We use binary F1-score to evaluate the performance of the Text/Abstract classification task.</p>
<h2>3 Results and discussion</h2>
<h3>3.1 Dataset</h3>
<p>Textual datasets are an integral part of the training of an LM. There exist many general purpose corpora like BookCorpus and EnglishWikipedia ${ }^{33,34}$, and domain specific corpora like biomedical corpus ${ }^{38}$, and clinical database ${ }^{40}$, to name a few. However, none of these corpora is suitable for the materials domain. Therefore, with the aim of providing a materials specific LM, we first create a corpus spanning four important material science families of inorganic glasses, metallic glasses, alloys, and cement and concrete. It should be noted that although these broad categories are mentioned, several other categories of materials, including twodimensional materials, were also present in the corpus. Specifically, we have selected $\sim 150 \mathrm{~K}$ papers out of $\sim 1 \mathrm{M}$ papers downloaded from the Elsevier Science Direct Database. The steps to create the corpus are provided in the Methodology section. The details about the number of papers and words for each family are given in Table 1.</p>
<p>The material science corpus developed for this work has $\sim 285 \mathrm{M}$ words, which is nearly $9 \%$ of the number of words used to pre-train SciBERT ( 3.17 B words) and BERT ( 3.3 B words). From Table 1, one can observe that $40 \%$ of the words are from research papers related to inorganic glasses and ceramics, and $20 \%$ each from bulk metallic glasses (BMG), alloys, and cement. Note that although the number of research papers for "cement and concrete" is more than "inorganic glasses and ceramics", the latter has higher words. This is because of the presence of a greater number of full text documents retrieved associated with the latter category. The average paper length for this corpus is $\sim 1848$ words, which is two-thirds of the average paper length of 2769 words for the SciBERT corpus. The lower average paper length can be attributed to two things: (a) In general, material science papers are shorter than biomedical papers. We verified this by computing the average paper length of full text material science papers. The number came out to be 2366 . (b) There are papers without full text also in our corpus. In that case, we have used the abstracts of such papers to arrive at the final corpus.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus details</th>
<th style="text-align: center;"># (Papers)</th>
<th style="text-align: center;"># (Words)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Inorganic glasses and ceramics</td>
<td style="text-align: center;">42,186</td>
<td style="text-align: center;">$112,560,687$</td>
</tr>
<tr>
<td style="text-align: center;">Bulk metallic glasses</td>
<td style="text-align: center;">21,093</td>
<td style="text-align: center;">$59,338,072$</td>
</tr>
<tr>
<td style="text-align: center;">Alloys</td>
<td style="text-align: center;">21,093</td>
<td style="text-align: center;">$55,683,291$</td>
</tr>
<tr>
<td style="text-align: center;">Cement and concrete</td>
<td style="text-align: center;">69,606</td>
<td style="text-align: center;">$56,936,694$</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">153,978</td>
<td style="text-align: center;">284,518,744</td>
</tr>
</tbody>
</table>
<p>Table 1. Numbers of papers and tokens taken for training the language model</p>
<h1>3.2 Pre-training of MatSciBERT</h1>
<p>Since training from scratch is a computationally intensive task, we resort to the way BioBERT was pre-trained ${ }^{36}$, that is, we initialize MatSciBERT weights with that of some suitable LM and then pre-train it on MSC. To determine the appropriate initial weights for MatSciBERT, we trained an uncased WordPiece ${ }^{46}$ vocabulary based on the MSC using the tokenizers library ${ }^{47}$. The overlap of MSC vocabulary is $53.64 \%$ with the uncased SciBERT ${ }^{37}$ vocabulary and $38.90 \%$ with the uncased BERT vocabulary. Because of the larger overlap with the</p>
<p>vocabulary of SciBERT, we tokenize our corpus using the SciBERT vocabulary and initialize the MatSciBERT weights with that of SciBERT as made publicly available by Beltagy et al. $(2019)^{37}$. The details of the pre-training procedure are provided in the Methodology section.</p>
<p>Figure 2 shows the variation of perplexity (ppl) on the validation data with the number of days for which pre-training was done. The model achieved a final perplexity of 3.112 on the validation set. The final pre-trained LM was then used for evaluation on different material science domain-specific downstream tasks, details of which are described in the subsequent sections. The performance of the LM on the downstream tasks was compared with that of SciBERT to evaluate the effectiveness of MatSciBERT to learn the materials specific information.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Perplexity vs number of days taken for pre-training
In order to understand the effect of pre-training on the model performance, a materials domain specific downstream task, NER on SOFC-slot, was performed on the model at regular intervals of pre-training. To this extent, the pre-trained model was finetuned on the training set of the SOFC-slot dataset. The choice of the SOFC-slot dataset was based on the fact that the dataset was comprised of fine-grained materials specific information. Thus, this dataset is appropriate to distinguish the performance of SciBERT from the materials-aware LMs. The performance of these finetuned models was evaluated on the test set. LM-CRF architecture was used for the analysis since LM-CRF consistently gives the best performance for the downstream task, as shown later in this work. Figure 4 shows the Macro-F1 averaged across 3 seeds for the SOFCSlot test set. LM-CRF architecture varies with the number of days for which MatSciBERT was pre-trained. The increasing trend of the graph shows the importance of pre-training for longer durations.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Macro-F1 (test set) vs number of days elapsed for MatSciBERT training</p>
<h1>3.3 Downstream tasks</h1>
<h3>3.3.1 Named Entity Recognition</h3>
<p>Here, we present the results on the three material science NER datasets as described in the methodology section. To the best of our knowledge, the best Macro-F1 on solid oxide fuel cells (SOFC) and SOFC-Slot datasets are $81.50 \%$ and $62.60 \%$, respectively, as reported by Friedrich et al. (2020), who introduced the dataset ${ }^{44}$. We run the experiments on the exact same train-validation-test splits as done by Friedrich et al. for a fair comparison of results. Moreover, since the authors reported results averaged over 17 entities (the extra entity is "Thickness") for the SOFC-Slot dataset, we also report the results taking the "Thickness" entity into account.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Architecture</th>
<th style="text-align: center;">Macro - F1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Micro - F1</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{L M}=$ <br> MatSciBERT</td>
<td style="text-align: center;">$\mathbf{L M}=\mathbf{S c i B E R T}$</td>
<td style="text-align: center;">$\mathbf{L M}=$ <br> MatSciBERT</td>
<td style="text-align: center;">$\mathbf{L M}=\mathbf{S c i B E R T}$</td>
</tr>
<tr>
<td style="text-align: left;">LM - Linear</td>
<td style="text-align: center;">$63.63 \%(67.40 \%)$</td>
<td style="text-align: center;">$58.64 \%(64.58 \%)$</td>
<td style="text-align: center;">$71.03 \%(69.64 \%)$</td>
<td style="text-align: center;">$67.85 \%(67.58 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LM - CRF</td>
<td style="text-align: center;">$65.00 \%(69.84 \%)$</td>
<td style="text-align: center;">$59.07 \%(68.31 \%)$</td>
<td style="text-align: center;">$72.06 \%(72.14 \%)$</td>
<td style="text-align: center;">$69.68 \%(70.15 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LM - <br> BiLSTM-CRF</td>
<td style="text-align: center;">$65.92 \%(69.88 \%)$</td>
<td style="text-align: center;">$61.68 \%(68.44 \%)$</td>
<td style="text-align: center;">$72.72 \%(72.55 \%)$</td>
<td style="text-align: center;">$70.66 \%(70.56 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 2. Test set results for SOFC-Slot averaged over 3 seeds and 5 cross-validation splits. Values in the parenthesis show the results on the validation set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Architecture</th>
<th style="text-align: center;">Macro - F1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Micro - F1</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{L M}=$ <br> MatSciBERT</td>
<td style="text-align: center;">$\mathbf{L M}=\mathbf{S c i B E R T}$</td>
<td style="text-align: center;">$\mathbf{L M}=$ <br> MatSciBERT</td>
<td style="text-align: center;">$\mathbf{L M}=\mathbf{S c i B E R T}$</td>
</tr>
<tr>
<td style="text-align: left;">LM-Linear</td>
<td style="text-align: center;">$81.56 \%(81.10 \%)$</td>
<td style="text-align: center;">$79.91 \%(80.91 \%)$</td>
<td style="text-align: center;">$83.69 \%(81.77 \%)$</td>
<td style="text-align: center;">$81.93 \%(81.61 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LM-CRF</td>
<td style="text-align: center;">$81.92 \%(82.53 \%)$</td>
<td style="text-align: center;">$81.07 \%(82.04 \%)$</td>
<td style="text-align: center;">$84.39 \%(83.22 \%)$</td>
<td style="text-align: center;">$83.46 \%(83.03 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LM-BiLSTM- <br> CRF</td>
<td style="text-align: center;">$82.07 \%(82.20 \%)$</td>
<td style="text-align: center;">$80.12 \%(81.92 \%)$</td>
<td style="text-align: center;">$84.37 \%(82.93 \%)$</td>
<td style="text-align: center;">$82.48 \%(82.78 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 3. Test set results for SOFC averaged over 3 seeds, and 5 cross-validation splits. Values in the parenthesis shows the results on the validation set.</p>
<p>Table 2 and Table 3 shows the results for the NER tasks on the SOFC-Slot and SOFC datasets, respectively, by SciBERT and MatSciBERT. We observe that LM-CRF always performs better than LM-Linear. This can be attributed to the fact that the CRF layer can model the BIO tags accurately. We obtained an improvement of $\sim 4.2$ Macro F1 and $\sim 2.1$ Micro F1 on the SOFCSlot test set while using the LM-BiLSTM-CRF architecture. For the SOFC test dataset, MatSciBERT-BiLSTM-CRF performs better than SciBERT-BiLSTM-CRF by $\sim 1.9$ Macro F1 and $\sim 1.9$ Micro F1. Similar improvements can be seen for other architectures as well. These MatSciBERT results also surpass the current best results on SOFC-Slot and SOFC datasets by $\sim 3.3$ and $\sim 0.6$ Macro-F1, respectively.</p>
<p>It is worth noting that the SOFC-slot dataset consists of 17 entities and hence has more finegrained information regarding the materials. On the other hand, SOFC-slot has only four entities representing coarse-grained information. We notice that the performance of MatSciBERT on SOFC-slot is significantly better than that of SciBERT. To further evaluate this aspect, we analyzed the F1-score of both SciBERT and MatSciBERT on all the 17 entities of the SOFC-slot data individually, as shown in Table 4. Interestingly, we observe that for all the materials related entities, namely anode material, cathode material, electrolyte material, interlayer material, and support material, MatSciBERT performs better than SciBERT. In addition, for materials related properties such as conductivity and degradation rate, MatSciBERT is able to significantly outperform SciBERT. This suggests that MatSciBERT is indeed able to capitalize on the additional information learned from the MSC to deliver better performance on complex problems specific to the materials domain.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity Type</th>
<th style="text-align: center;">MatSciBERT <br> F1-score</th>
<th style="text-align: center;">SciBERT F1- <br> score</th>
<th style="text-align: left;">Entity Type</th>
<th style="text-align: center;">MatSciBERT <br> F1-score</th>
<th style="text-align: center;">SciBERT F1- <br> score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Anode Material</td>
<td style="text-align: center;">$41.72 \%$</td>
<td style="text-align: center;">$38.44 \%$</td>
<td style="text-align: left;">Interlayer <br> Material</td>
<td style="text-align: center;">$35.26 \%$</td>
<td style="text-align: center;">$28.40 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Cathode <br> Material</td>
<td style="text-align: center;">$49.39 \%$</td>
<td style="text-align: center;">$44.44 \%$</td>
<td style="text-align: left;">Open Circuit <br> Voltage</td>
<td style="text-align: center;">$67.39 \%$</td>
<td style="text-align: center;">$66.50 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Conductivity</td>
<td style="text-align: center;">$92.10 \%$</td>
<td style="text-align: center;">$89.55 \%$</td>
<td style="text-align: left;">Power Density</td>
<td style="text-align: center;">$97.36 \%$</td>
<td style="text-align: center;">$96.54 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Current <br> Density</td>
<td style="text-align: center;">$90.89 \%$</td>
<td style="text-align: center;">$92.39 \%$</td>
<td style="text-align: left;">Resistance</td>
<td style="text-align: center;">$83.73 \%$</td>
<td style="text-align: center;">$83.53 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Degradation <br> Rate</td>
<td style="text-align: center;">$42.40 \%$</td>
<td style="text-align: center;">$25.45 \%$</td>
<td style="text-align: left;">Support <br> Material</td>
<td style="text-align: center;">$49.63 \%$</td>
<td style="text-align: center;">$47.78 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Device</td>
<td style="text-align: center;">$69.37 \%$</td>
<td style="text-align: center;">$68.52 \%$</td>
<td style="text-align: left;">Thickness</td>
<td style="text-align: center;">$77.26 \%$</td>
<td style="text-align: center;">$76.20 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Electrolyte <br> Material</td>
<td style="text-align: center;">$54.86 \%$</td>
<td style="text-align: center;">$45.69 \%$</td>
<td style="text-align: left;">Time of <br> Operation</td>
<td style="text-align: center;">$67.98 \%$</td>
<td style="text-align: center;">$67.09 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Fuel Used</td>
<td style="text-align: center;">$66.18 \%$</td>
<td style="text-align: center;">$69.43 \%$</td>
<td style="text-align: left;">Voltage</td>
<td style="text-align: center;">$69.34 \%$</td>
<td style="text-align: center;">$68.27 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;">Working <br> Temperature</td>
<td style="text-align: center;">$90.93 \%$</td>
<td style="text-align: center;">$89.66 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4. Comparison of entity-level F1-score for MatSciBERT and SciBERT on validation sets of SOFC-slots.</p>
<p>Now, we present the results for the Matscholar dataset ${ }^{14}$ in Table 5. For the Matscholar dataset too, MatSciBERT outperforms SciBERT as well as the current best results, as can been seen in the case of LM-CRF architecture. The authors obtained a Micro-F1 of $87.09 \%$ on the validation set and $87.04 \%$ on the test set. We observe that the best model LM-CRF with MatSciBERT has Micro-F1 values of $89.33 \%$ and $87.54 \%$, both better than the state-of-theart.</p>
<p>In order to demonstrate the performance of MatSciBERT, we demonstrate an example from the validation set of the dataset. Figure 4 shows a sentence from a manuscript related to alloys. The italicized entities are colored on the basis of their true labels. From the predictions, it was observed that both MatSciBERT and SciBERT misclassified the phrase "dislocation cell-like</p>
<p>structure" as "PRO", which was labelled as "O" or "Other" in the dataset. Further, "ND" and "TD" were labelled as "PRO" by SciBERT but MatSciBERT was able to correctly identify these entities as "O". Overall, we observe that MatSciBERT is able to identify the context in the text related to materials, from which the entities are correctly tagged. Similarly, In Figure 5, we show the results obtained using SciBERT, where it has labelled "YSZ" (also called yttriastabilized zirconia) as an electrolyte material that has a true label of "B-interlayer_material". However, MatSciBERT was able to correctly identify the label of "YSZ". Note that these are some arbitrary examples selected to demonstrate the performances of both SciBERT and MatSciBERT and are not necessarily representative in nature.</p>
<h1>Ground truth and model predictions</h1>
<p>By $86 \%$ cold rolling, acicular a ${ }^{\circ}$ martensite microstructures change into extremely refined dislocation cell-like structure with an average size of 60 nm , accompanied with the development of cold rolling texture in which the basal plane normal is tilted from the plate normal direction (ND) toward transverse direction (TD) at angles of $\pm 49$ deg. for Ti-8\% V alloy and $\pm 46$ deg. for (Ti-8 mass\% V)-4 mass\% Sn alloy.</p>
<h2>Labels</h2>
<p>Synthesis method (SMT), Symmetry/phase label (SPL), Property (PRO), Material (MAT), Descriptor (DSC)</p>
<p>Figure 4. Visualising results on the Matscholar NER dataset. The italicized colored text correspond to true labels. The underlined colored text represents the mistakes made by SciBERT. The normal font represents the mistakes made by MatSciBERT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Architecture</th>
<th style="text-align: center;">Micro - F1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Macro - F1</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{L M}=$ <br> MatSciBERT</td>
<td style="text-align: center;">$\mathbf{L M}=\mathbf{S c i B E R T}$</td>
<td style="text-align: center;">$\mathbf{L M}=$ <br> MatSciBERT</td>
<td style="text-align: center;">$\mathbf{L M}=\mathbf{S c i B E R T}$</td>
</tr>
<tr>
<td style="text-align: left;">LM - Linear</td>
<td style="text-align: center;">$86.23 \%(88.26 \%)$</td>
<td style="text-align: center;">$84.81 \%(87.78 \%)$</td>
<td style="text-align: center;">$85.56 \%(86.63 \%)$</td>
<td style="text-align: center;">$83.80 \%(86.05 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LM - CRF</td>
<td style="text-align: center;">$87.54 \%(89.33 \%)$</td>
<td style="text-align: center;">$86.31 \%(88.77 \%)$</td>
<td style="text-align: center;">$86.30 \%(88.75 \%)$</td>
<td style="text-align: center;">$85.04 \%(88.07 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LM - <br> BiLSTM-CRF</td>
<td style="text-align: center;">$86.84 \%(89.27 \%)$</td>
<td style="text-align: center;">$86.38 \%(88.71 \%)$</td>
<td style="text-align: center;">$85.66 \%(88.87 \%)$</td>
<td style="text-align: center;">$85.66 \%(87.66 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 5. Test set results for Matscholar averaged over 3 seeds. Values in the parenthesis shows the results on the validation set.</p>
<h2>Prediction of SciBERT:</h2>
<p>While the peak power density of the cell ( cell 3 ) with an YSZ (BElectrolyte material)blocking layer reached approximately $35 \mathrm{~mW} / \mathrm{cm} 2$ , that of the single - layered GDC - based cell ( cell 1) showed a much lesser power density below approximately $0.01 \mathrm{~mW} / \mathrm{cm} 2$, as shown in Figure 5a,b.</p>
<p>Figure 5. Visualising prediction on SOFC slot dataset.</p>
<h1>3.3.2 Text/Abstract Classification</h1>
<p>Here, we consider the ability of LMs to classify a manuscript into glass vs non-glass topics based on an in-house dataset ${ }^{15}$. This is a binary classification problem, with the input being the abstract of a manuscript. Table 6 shows the comparison of F1-scores achieved by MatSciBERT and SciBERT. It can be clearly seen that MatSciBERT outperforms SciBERT by $\sim 2.5$ F1-score on both validation and test sets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MatSciBERT</th>
<th style="text-align: left;">SciBERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">F1-Score</td>
<td style="text-align: left;">$93.57 \%(93.04 \%)$</td>
<td style="text-align: left;">$91.04 \%(90.51 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 6. Test set results for glass vs non-glass dataset averaged over 3 seeds. Values in the parenthesis represent the results on the validation set.</p>
<h3>3.3.3 Relation Classification</h3>
<p>Table 7 shows the results for the relation classification task performed on the Materials Synthesis Procedures dataset ${ }^{45}$. MatSciBERT obtains minor improvement over SciBERT for both the metrics. To the best of our knowledge, this dataset has been evaluated in only one research article ${ }^{48}$. However, our results are not comparable with them because of two reasons:
a) They assume access to the entity types for making predictions while we do not,
b) We make predictions over 16 relation classes, while they include empty relation class as well.
Even in this task, we observe that MatSciBERT performs better than SciBERT consistently, although with a lower margin.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MatSciBERT</th>
<th style="text-align: left;">SciBERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Macro-F1</td>
<td style="text-align: left;">$87.87 \%(87.99 \%)$</td>
<td style="text-align: left;">$87.22 \%(87.21 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Micro-F1</td>
<td style="text-align: left;">$91.26 \%(91.40 \%)$</td>
<td style="text-align: left;">$91.04 \%(91.03 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 7. Test set results for Materials Synthesis Procedures dataset averaged over 3 seeds. Values in the parenthesis represent the results on the validation set.</p>
<h3>3.4 Applications in Materials Domain</h3>
<p>Now, we discuss some of the potential areas of application of MatSciBERT in materials science. These areas can range from the simple topic-based classification of research papers to discovering novel materials or novel applications for existing materials. We demonstrate some of these applications below.
(i) Document classification: A large number of manuscripts have been published on materials related topics, and the numbers are increasing exponentially. Identifying manuscripts related to a given topic is a challenging task. Traditionally, these tasks are carried out employing approaches such as term frequency-inverse document frequency (TFIDF) along with latent Dirichlet allocation (LDA). However, these approaches directly vectorize a word and are not context-sensitive. For instance, in the phrases "flat glass", "glass transition temperature", "tea glass", the word "glass" is used in a very different sense. MatSciBERT, being based on BERT, will be able to extract the contextual meaning of the embeddings. Thus, MatSciBERT will be</p>
<p>able to effectively classify the topics thereby enabling improved topic classification. This is evident from the binary classification results presented earlier, where we observe that the F1score obtained was found to be significantly higher than the results obtained using simple logistic regression based on TF-IDF. This approach can be extended to a larger set of abstracts for unsupervised topic modelling, enabling improved identification of documents relevant to specific topics.
(ii) Information extraction from images: Images hold a large amount of information regarding the structure and properties of materials. A proxy to identify relevant images would be to go through the captions of all the images. However, each caption may contain multiple entities and identifying the relevant keywords might be a challenging task. To this extent, MatSciBERT finetuned on NER can be an extremely useful tool for extracting information from figure captions.</p>
<p>Here, we extracted entities from the figure captions used by Venugopal et al. (2021) ${ }^{15}$ using MatSciBERT finetuned on the Matscholar NER dataset. Specifically, entities were extracted from $\sim 1,10,000$ image captions on topics related to inorganic glasses. Using MatSciBERT, we obtained 87318 entities as DSC (sample descriptor), 10633 entities under APL (application), 145324 as MAT (inorganic material), 76898 as PRO (material property), 73241 as CMT (characterization method), 33426 as SMT (synthesis method), and 2676 as SPL (symmetry/phase label). Figure 6 shows the top 10 extracted entities under the seven categories proposed in the Matscholar dataset. The top entities associated with each of the categories are coating (application), XRD (characterization), glass (sample descriptor, inorganic material), composition (material property), heat (synthesis method), and hexagonal (symmetry/phase). Further details associated with each category can also be obtained from these named entities. It should be noted that each caption may be associated with more than one entity. These entities can then be used to obtain relevant images for specific queries such as "XRD measurements of glasses used for coating" or "emission spectra of doped glasses" or "SEM images of bioglasses with Ag ", to name a few.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. Bar graph of top-10 entities under the following categories (a) APL=Application, (b) CMT=Characterization method, (c) DSC=Sample descriptor, (d) MAT=Inorganic material, (e) PRO=Material Property, (f) SMT=Synthesis method, and (g) SPL=Symmetry/phase label</p>
<p>Further, Table 8 shows some of the selected captions from the image captions along with the corresponding manual annotation by Venugopal et al. (2021) ${ }^{15}$. These tags assigned to each caption in Venugopal et al. (2021) ${ }^{15}$ were carried out by human experts. Note that only one word was assigned per image caption in the previous. Using the MatSciBERT NER model, we show that multiple entities are extracted for the selected 5 captions. This illustrates the large amount of information that can be captured using the LM proposed in this work.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Caption</th>
<th style="text-align: center;">Label by <br> Venugopal et al. <br> $\mathbf{( 2 0 2 1 )}^{15}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The comparison of XRD patterns of glass ceramic heat treated at</td>
<td style="text-align: center;">Reflection</td>
</tr>
<tr>
<td style="text-align: center;">$725 \sim \infty$ C for 5 h and rhombohedral Ba4Gd3F17. The</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">superstructure reflections are marked with ,óa. Inset: enlarged sections</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">of XRD patterns.</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">HRTEM image and the corresponding FFT pattern taken from asdeposited sample $\mathrm{B}(80 / 1)$ (a) and annealed sample $\mathrm{D}(30 / 1)$ (b); identifying rutile TiO2 crystal grains.</th>
<th style="text-align: center;">FFT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The illustrative schemes: a) The bonding of hexagonal ZnO nanocrystals to the glass surface. b) The structure of multi-layers coatings.</td>
<td style="text-align: center;">Crystal</td>
</tr>
<tr>
<td style="text-align: center;">(a) XRD patterns of the glass-ceramics sintered different holding times; (b) intensity of $\mu$ - and $\alpha$-cordierite peaks count at (101) and (110) plane respectively as a function of sintering holding time.</td>
<td style="text-align: center;">XRD</td>
</tr>
<tr>
<td style="text-align: center;">Photoluminescence spectra of PbBr -based layered perovskites with an organic layer of naphthalene-linked ammonium molecules. Profiles: (a) 1 ; (b) 2 ; (c) 3 ; (d) 4 ; (e) 5 .</td>
<td style="text-align: center;">Luminescence</td>
</tr>
<tr>
<td style="text-align: center;">Table 8. Comparing the results of MatSciBERT NER with manually assigned labels ${ }^{15}$. Application (APL). Characterization method (CMT). Descriptor (DSC). Material (MAT). Property (PRO). Synthesis method (SMT). Symmetry/phase label (SPL)</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(iii) Other applications such as relation classification and question answering: MatSciBERT can also be applied for addressing several other issues such as relation classification and question answering. The relation classification task demonstrated in the present manuscript can provide key information regarding several aspects in materials science which are followed in a sequence. These would include synthesis and testing protocols, and measurement sequences. This information can be further used to discover an optimal pathway for material synthesis or a new pathway. In addition, such approaches can also be used to obtain the effect of different testing and environmental conditions, along with the relevant parameters, on the measured property of materials. This could be especially important for those properties such as hardness or fracture toughness, which are highly sensitive to sample preparation protocols, testing conditions, and the equipment used. Thus, the LM can enable the extraction of information regarding synthesis and testing conditions that are otherwise buried in the text. Similarly, it can also enable question-answering when trained on a dataset.</p>
<p>At this juncture, it is worth noting there are very few annotated datasets available for the material corpus. This is in contrast to the biomedical corpus, where several annotated datasets are available for different downstream tasks such as relation extraction, question-answering and NER. While the development of materials science specific language model can significantly accelerate the NLP-related applications in materials, the development of annotated datasets is equally important for accelerating materials discovery.</p>
<h1>4 Conclusion</h1>
<p>Altogether, we developed a materials-aware language model, namely, MatSciBERT, that is trained a materials science corpora of journals. The LM, trained from the initial weights of SciBERT, exploits the knowledge on computer science and biomedical corpora (on which the original SciBERT was pretrained) along with the additional information on materials science. We test the performance of MatSciBERT on several downstream tasks such as document classification, NER, and relation classification. We demonstrate that MatSciBERT exhibits superior performance on all the datasets tested in comparison to SciBERT. Finally, we discuss</p>
<p>some of the applications through which MatSciBERT can enable accelerated information extraction from the materials science text corpora.</p>
<h1>References</h1>
<p>1I. J. Polmear, Materials Science and Technology, 1994, 10, 1-16.</p>
<p>2T. Dursun and C. Soutis, Materials \&amp; Design (1980-2015), 2014, 56, 862-871.
3M. Niinomi, Materials Science and Engineering: A, 1998, 243, 231-236.
4J. S. Sanghera and I. D. Aggarwal, Journal of Non-Crystalline Solids, 1999, 256-257, 6-16.
5A. M. Neville and J. J. Brooks, Concrete technology, Longman Scientific \&amp; Technical England, 1987.
6N. Science and T. C. (US), Materials genome initiative for global competitiveness, Executive Office of the President, National Science and Technology Council, 2011.
7A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder and K. A. Persson, APL Materials, 2013, 1, 011002.
8A. Zunger, Nat Rev Chem, 2018, 2, 1-16.
9C. Chen, Y. Zuo, W. Ye, X. Li, Z. Deng and S. P. Ong, Advanced Energy Materials, 2020, 10, 1903242 .
10 J. J. de Pablo, N. E. Jackson, M. A. Webb, L.-Q. Chen, J. E. Moore, D. Morgan, R. Jacobs, T. Pollock, D. G. Schlom, E. S. Toberer, J. Analytis, I. Dabo, D. M. DeLongchamp, G. A. Fiete, G. M. Grason, G. Hautier, Y. Mo, K. Rajan, E. J. Reed, E. Rodriguez, V. Stevanovic, J. Suntivich, K. Thornton and J.-C. Zhao, npj Comput Mater, 2019, 5, 1-23.
11 R. L. Greenaway and K. E. Jelfs, Advanced Materials, 2021, 33, 2004831.
12 Ravinder, V. Venugopal, S. Bishnoi, S. Singh, M. Zaki, H. S. Grover, M. Bauchy, M. Agarwal and N. M. A. Krishnan, International Journal of Applied Glass Science, 2021, 12, 277-292.
13 E. D. Zanotto and F. A. B. Coutinho, Journal of Non-Crystalline Solids, 2004, 347, 285-288.
14 L. Weston, V. Tshitoyan, J. Dagdelen, O. Kononova, A. Trewartha, K. A. Persson, G. Ceder and A. Jain, J. Chem. Inf. Model., 2019, 59, 3692-3702.
15 V. Venugopal, S. Sahoo, M. Zaki, M. Agarwal, N. N. Gosvami and N. M. A. Krishnan, Patterns, 2021, 100290.
16 M. C. Swain and J. M. Cole, J. Chem. Inf. Model., 2016, 56, 1894-1904.
17 S. Huang and J. M. Cole, Scientific Data, 2020, 7, 260.
18 C. J. Court and J. M. Cole, Sci Data, 2018, 5, 180111.
19 O. Kononova, H. Huo, T. He, Z. Rong, T. Botari, W. Sun, V. Tshitoyan and G. Ceder, Scientific Data, 2019, 6, 203.
20 H. Uvegi, Z. Jensen, T. N. Hoang, B. Traynor, T. Aytaş, R. T. Goodwin and E. A. Olivetti, Journal of the American Ceramic Society, , DOI:https://doi.org/10.1111/jace.17631.
21 M. Zaki, Jayadeva and N. M. A. Krishnan, Chemical Engineering and Processing Process Intensification, 2021, 108607.
22 E. A. Olivetti, J. M. Cole, E. Kim, O. Kononova, G. Ceder, T. Y.-J. Han and A. M. Hiszpanski, Applied Physics Reviews, 2020, 7, 041317.
23 D. M. Jessop, S. E. Adams, E. L. Willighagen, L. Hawizy and P. Murray-Rust, Journal of Cheminformatics, 2011, 3, 41.
24 R. W. Epps, M. S. Bowen, A. A. Volk, K. Abdel-Latif, S. Han, K. G. Reyes, A. Amassian and M. Abolhasani, Advanced Materials, 2020, 32, 2001626.
25 B. P. MacLeod, F. G. L. Parlane, T. D. Morrissey, F. Häse, L. M. Roch, K. E. Dettelbach, R. Moreira, L. P. E. Yunker, M. B. Rooney, J. R. Deeth, V. Lai, G. J. Ng, H.</p>
<p>Situ, R. H. Zhang, M. S. Elliott, T. H. Haley, D. J. Dvorak, A. Aspuru-Guzik, J. E. Hein and C. P. Berlinguette, Science Advances, 2020, 6, eaaz8867.</p>
<p>26 C. Manning and H. Schutze, Foundations of Statistical Natural Language Processing, MIT Press, 1999.
27 P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai and R. L. Mercer, Computational Linguistics, 1992, 18, 467-480.
28 R. K. Ando and T. Zhang, Journal of Machine Learning Research, 2005, 6, 18171853 .
29 J. Blitzer, R. McDonald and F. Pereira, in Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Sydney, Australia, 2006, pp. 120-128.
30 T. Mikolov, K. Chen, G. Corrado and J. Dean, arXiv:1301.3781 [cs].
31 T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado and J. Dean, in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2013, vol. 26.
32 J. Pennington, R. Socher and C. Manning, in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Doha, Qatar, 2014, pp. 1532-1543.
33 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, arXiv:1810.04805 [cs].
34 Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler, in 2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 19-27.
35 Y. Zhang, Q. Chen, Z. Yang, H. Lin and Z. Lu, Sci Data, 2019, 6, 52.
36 J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So and J. Kang, Bioinformatics, 2019, btz682.
37 I. Beltagy, K. Lo and A. Cohan, arXiv:1903.10676 [cs].
38 W. Ammar, D. Groeneveld, C. Bhagavatula, I. Beltagy, M. Crawford, D. Downey, J. Dunkelberger, A. Elgohary, S. Feldman, V. Ha, R. Kinney, S. Kohlmeier, K. Lo, T. Murray, H.-H. Ooi, M. Peters, J. Power, S. Skjonsberg, L. Wang, C. Wilhelm, Z. Yuan, M. van Zuylen and O. Etzioni, in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), Association for Computational Linguistics, New Orleans Louisiana, 2018, pp. 84-91.
39 E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Naumann and M. McDermott, in Proceedings of the 2nd Clinical Natural Language Processing Workshop, Association for Computational Linguistics, Minneapolis, Minnesota, USA, 2019, pp. 7278 .
40 A. E. W. Johnson, T. J. Pollard, L. Shen, L. H. Lehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi and R. G. Mark, Sci Data, 2016, 3, 160035.
41 J. Libovický, R. Rosa and A. Fraser, arXiv:1911.03310 [cs].
42 J.-S. Lee and J. Hsiang, arXiv:1906.02124 [cs, stat].
43 D. Araci, arXiv:1908.10063 [cs].
44 A. Friedrich, H. Adel, F. Tomazic, J. Hingerl, R. Benteau, A. Marusczyk and L. Lange, in Proceedings of the 58th annual meeting of the association for computational linguistics, Association for Computational Linguistics, Online, 2020, pp. 1255-1268.
45 S. Mysore, Z. Jensen, E. Kim, K. Huang, H.-S. Chang, E. Strubell, J. Flanigan, A. McCallum and E. Olivetti, in Proceedings of the 13th linguistic annotation workshop, Association for Computational Linguistics, Florence, Italy, 2019, pp. 56-64.
46 Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Ł. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes and J. Dean,</p>
<p>arXiv:1609.08144 [cs].
47 Hugging Face, https://github.com/huggingface, (accessed 1 September 2021).
48 D. Swarup, A. Bajaj, S. Mysore, T. O’Gorman, R. Das and A. McCallum, in Findings of the association for computational linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020, pp. 3010-3017.
49 allenai/scibert_scivocab_uncased $\cdot$ Hugging Face, https://huggingface.co/allenai/scibert_scivocab_uncased, (accessed 5 September 2021).
50 T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest and A. M. Rush, in Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations, Association for Computational Linguistics, Online, 2020, pp. 38-45.
51 Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer and V. Stoyanov, arXiv:1907.11692 [cs].
52 BERT, Google Research, 2021.
53 E. F. T. K. Sang and S. Buchholz, arXiv:cs/0009008.
54 L. Baldini Soares, N. FitzGerald, J. Ling and T. Kwiatkowski, in Proceedings of the 57th annual meeting of the association for computational linguistics, Association for Computational Linguistics, Florence, Italy, 2019, pp. 2895-2905.
55 2019.
56 J. Lafferty, A. McCallum and F. C. N. Pereira, 10.
57 pytorch-crf - pytorch-crf 0.7.2 documentation, https://pytorch-
crf.readthedocs.io/en/stable/, (accessed 5 September 2021).
58 Z. Huang, W. Xu and K. Yu, arXiv:1508.01991 [cs].</p>            </div>
        </div>

    </div>
</body>
</html>
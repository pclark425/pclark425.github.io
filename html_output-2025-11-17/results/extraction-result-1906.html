<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1906 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1906</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1906</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-281315107</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.11417v1.pdf" target="_blank">Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</a></p>
                <p><strong>Paper Abstract:</strong> Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1906.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1906.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language-action (VLA) architecture used as a baseline VLA in this paper, initialized from pretrained vision-language/backbone components and finetuned on robot action data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-languageaction model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language-action model that combines pretrained visual backbones and a language model to autoregressively predict robot actions; in this paper it is initialized from pretrained VLM components (DINO + SigLIP + Llama) and used as a baseline VLA architecture processing RGB images and language commands to output short-horizon continuous robot trajectories (encoded as tokens in some variants).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language / multimodal pretraining (pretrained VLM components: visual backbones + language model)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Initialized from VLM components trained on web-scale image-text data and robust visual backbones (paper cites DINO, SigLIP, and Llama); pretraining data is described generally as image-text pairs with object descriptions, spatial relationships and web-scale visual-language pairs (the paper does not list the exact pretraining corpora for OpenVLA in detail).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (short-horizon trajectory prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Robotic manipulation tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear, PickKnife, PickCarrot, etc.) where policy maps RGB observations and a fixed natural-language command to H-step low-level actions a_t = (Δx, Δy, Δz, ϕ, θ, ψ, g). Evaluations performed in simulation (SimplerEnv Visual Matching and Visual Variant Aggregation) and on a real ViperX 300s robot; action space is continuous 7-DoF end-effector translation/rotation plus binary gripper state.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper discusses semantic alignment: baseline OpenVLA is initialized from VLMs (visual + language) so there is substantial overlap in object / spatial concept representation, but direct fine-tuning on robotic action data disrupts pretrained representations; the authors explicitly aim to improve alignment via string-based action tokenization and co-training with spatial/affordance vision-language data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Baseline OpenVLA (initialized from pretrained VLMs) average success rate on SimplerEnv Visual Matching: 35.03% (Table II). Language-robustness (PickCan original): 36.70% success; paraphrased: 12.12% (Table IV).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Paper does not provide a direct OpenVLA run with no language / no VLM pretraining (random init) for comparison; the reported baseline OpenVLA is VLM-initialized. Authors do report that direct fine-tuning on robot data degrades pretrained visual representations but do not report numeric performance for a vision-only or randomly initialized OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit quantitative sample-efficiency numbers comparing language-pretrained vs non-pretrained models are provided. The paper states co-training and preserving pretrained features help in low-data regimes, but offers no numeric sample-count comparisons (e.g., episodes or demonstrations-to-performance).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map analysis is reported for OpenVLA. The paper does not present attention visualizations or per-token attention diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes — the paper visualizes visual encoder features with t-SNE on CIFAR-10 before/after VLA training and reports that baseline OpenVLA features after direct fine-tuning show degraded class separability, whereas representations preserved via the authors' recipe have tighter, more separable clusters and higher linear-probe classification scores (qualitative + linear-probe numbers shown in Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect evidence: authors show that representing continuous actions as character sequences (string tokenizer) and co-training with vision-language data improves task success and language robustness (substantial increases in success rates and paraphrase robustness), which the authors interpret as better grounding of action outputs into pretrained language/visual representations. There is no direct probing that ties specific verbs to particular visual affordances or motor primitives beyond improved behavioral metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Partial: t-SNE and linear-probe analyses indicate that higher-level semantic clusters (class separability) in visual representations are better preserved with the proposed recipe; no explicit analysis separating low-level vs high-level feature layers beyond the dual-encoder freezing ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer improves when (1) a partially-frozen dual visual encoder is used (one frozen 'anchor' encoder + one trainable encoder), (2) actions are tokenized as strings to align with language pretraining, and (3) co-training mixes robot demos with vision-language datasets emphasizing spatial reasoning/affordances. Transfer degrades under direct finetuning (representation collapse), and naive co-training without the unified tokenizer can be suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Authors evaluate out-of-distribution visuals (background masking/randomization, distractors) and unseen paraphrased instructions. Results show large performance drops for baselines on OOD visuals and paraphrases, while models using the preservation recipe maintain much higher success rates; the paper does not provide a detailed per-object in-vs-out comparison with exact numeric deltas per object type beyond distractor examples.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot language generalization: authors generate synonymous paraphrases (via GPT-4) and evaluate zero-shot generalization to these unseen instructions. Baseline OpenVLA: large performance drop on paraphrases (PickCan 36.70% -> 12.12%); OpenVLA+ (full method) largely preserves performance zero-shot (PickCan 90.32% orig, 84.52% paraphrase), demonstrating strong zero-shot robustness to instruction paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Yes — ablations examine freezing vs finetuning: the partially-frozen dual-encoder (freeze one copy of visual encoder, finetune the other) substantially improves performance (OpenVLA: 35.03% -> 55.55% with only dual-encoder), indicating that particular encoder components (frozen vs trainable copy) are important for preserving pretrained features while allowing task adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — direct fine-tuning of pretrained VLMs on robot action data causes representation degradation and large performance drops under visual or language perturbations (described qualitatively and shown in t-SNE and drop in OOD evaluations). Naive co-training across highly heterogeneous objectives is also reported to sometimes degrade performance unless combined with their tokenizer + partial freezing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct numeric head-to-head comparison to vision-only pretraining (e.g., ImageNet-only) is reported in controlled experiments. The paper discusses visual backbones (DINO) but does not present an explicit vision-only vs vision-language pretraining ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No detailed analysis of training dynamics over time (e.g., early vs late training phase metrics) is provided beyond before/after representation visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No formal dimensionality / intrinsic-dimension measurements (PCA explained variance, ID) are reported; analysis limited to t-SNE visualizations and linear-probe classification numbers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1906.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1906.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0 (pi-zero)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action flow model used as a representative VLA baseline for general robot control and finetuning experiments in the paper, initialized from the PaliGemma VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>π 0 : A vision-language-action flow model for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0 (pi-zero)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A flow-matching-based vision-language-action model that predicts robot actions (originally using a flow-matching action head); in this paper π0 is used as a baseline VLA initialized from PaliGemma and evaluated both in its original form and with the authors' string tokenizer + partially-frozen dual encoder + co-training modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (initialized from the PaliGemma VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>PaliGemma is cited as a versatile 3B VLM for transfer (paper references it); general characterization in this paper: VLM pretraining uses large-scale image-text corpora with semantic and spatial information. The paper does not enumerate PaliGemma's exact pretraining corpora here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (short-horizon trajectory prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same robotic manipulation tasks as OpenVLA: PickCan, OpenDrawer, CloseDrawer, MoveNear, etc., mapping RGB observations and language commands to H-step continuous actions (Δx, Δy, Δz, rotations, gripper), evaluated in SimplerEnv simulation and on real robot; original π0 uses a flow-matching head for actions (replaced by string tokenizer for π0+ experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper notes π0 initialized from a VLM (PaliGemma) so it benefits from vision-language pretraining; however, directly finetuning action heads can break alignment. The authors' approach aims to preserve alignment via dual encoder and string action tokenization, which they show improves generalization for π0 as well.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Baseline π0 (with instruction augmentation) average in-table: 62.64% (Table II). With the authors' full method (π0+), average reported around 69.19% (Table II), an improvement of ~7 percentage points according to the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No explicit π0 run with no language pretraining or random init is reported; baseline π0 is VLM-initialized and sometimes trained with instruction augmentation as noted by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit numeric sample-efficiency comparison is provided for π0 between language-pretrained and non-pretrained variants. The paper argues their recipe helps in low-data regimes but does not quantify sample counts for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention visualization or analysis is reported specifically for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>The paper includes embedding/t-SNE analyses for the visual encoder generally (applies to both architectures) showing representations degrade with direct finetuning and are better preserved under their method; π0+ is reported to have improved separability and downstream VQA/representation metrics (Figure 4 and Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect evidence via behavioral improvements: replacing π0's flow-matching head with the string tokenizer and co-training yields higher success rates and improved robustness to paraphrased instructions and OOD visuals, interpreted as improved grounding between language-conditioned outputs and visual affordances. No fine-grained probe explicitly mapping verbs to motor patterns is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit per-layer hierarchical analysis for π0; visual representation analyses (t-SNE) indicate preservation of higher-level semantic structure is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>π0 benefits from (a) partially-frozen dual-encoder, (b) string-based tokenizer replacing flow-matching head to align action outputs with language pretraining, and (c) co-training on vision-language spatial datasets; naive finetuning or instruction-augmentation-only setups perform worse under perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper reports OOD evaluations (visual variant aggregation, distractors) and paraphrase robustness for π0 and π0+, showing π0+ performs better under OOD conditions (specific numbers for paraphrase robustness: for PickCan π0 w/ Inst. Aug.: 84.33% orig -> 42.20% paraphrase; π0+: 83.40% orig -> 55.40% paraphrase), indicating better robustness to unseen language variants; direct per-object in-vs-out numeric breakdown is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot language generalization to paraphrased instructions is evaluated: π0 without the authors' recipe shows large drops to paraphrases, while π0+ retains more performance zero-shot (see paraphrase numbers above), demonstrating improved zero-shot instruction robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Yes — authors ablate the action head (flow-matching vs string tokenizer) and show swapping to string tokenizer for π0+ improves transfer/generalization; also dual-encoder freezing benefits are discussed across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Direct fine-tuning of the pretrained VLM within π0 leads to representation degradation and poor OOD generalization; likewise naive co-training without the unified tokenizer can be suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No explicit controlled comparison to purely vision-only pretraining is provided for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal dynamics (training-phase) analysis beyond before/after representation visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality metrics reported for π0 representations (only t-SNE/linear-probe).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1906.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1906.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA + (OpenVLA with Dual Encoder + String Tokenizer + Co-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The OpenVLA model augmented with the paper's proposed recipe: a partially-frozen dual visual encoder, string-based tokenizer for continuous action-as-text, and co-training with vision-language spatial/affordance datasets to preserve pretrained representations and improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA+ (OpenVLA with D + S + C)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenVLA architecture modified with (1) partially-frozen dual visual encoders (one frozen anchor, one trainable copy), (2) string-based tokenizer that renders continuous action values as character sequences predicted autoregressively, and (3) co-training mixing robot demonstration data with vision-language datasets emphasizing spatial reasoning and affordances; processes RGB and language to autoregress tokens representing actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>initialized from pretrained VLM/backbone components (vision-language + visual backbones + language model) and then co-trained; the core approach leverages vision-language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Initial VLM components (DINO, SigLIP, Llama) originate from web-scale image-text and robust visual pretraining; co-training uses vision-language datasets chosen to emphasize spatial reasoning and affordances (e.g., LLaVA Visual Instruct CC3M, VQASynth-Spatial, OneVision, RoboPoint) combined with robot demonstration datasets (RT-1 / OXE / Bridge) in a 50/50 sampling strategy per batch.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (same set as OpenVLA baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>PickCan, OpenDrawer, CloseDrawer, MoveNear, PickKnife, PickCarrot and other short-horizon manipulation tasks in SimplerEnv (simulation) and on a real ViperX 300s robot; continuous 7-DoF action space (Δx, Δy, Δz, rotations, gripper) represented as strings with four decimals; co-training also includes VQA/spatial datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicitly optimized: authors design the string tokenizer to align continuous action outputs to the language token space so pretrained language semantics can be reused for action prediction; co-training uses datasets with spatial/affordance content to increase overlap between pretraining semantics and robotic tasks. Empirically this yields large gains in language and visual robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>OpenVLA+ (D+S+C, full method) average success rate on SimplerEnv Visual Matching: 78.46% (Table II). Language robustness on PickCan: orig 90.32%, paraphrase 84.52% (Table IV). Visual OOD robustness metrics: OpenVLA+ and OpenVLA+SC significantly outperform baselines across masked-background and visual-variant aggregation (exact per-task numbers in Table III/figures; overall qualitative improvement emphasized). Real-world trials: OpenVLA+ outperforms baseline OpenVLA across tasks in Table V and Figure 6 (exact per-task numbers for real robot referenced but not enumerated in text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported — the comparison is primarily between baseline OpenVLA (VLM-initialized and finetuned) and OpenVLA+ (the preservation recipe). The OpenVLA baseline is the VLM-initialized reference (35.03% avg).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit numeric sample-efficiency curves or demonstrations-to-performance counts are provided; authors claim improvements in low-data regimes and reduced overfitting but do not quantify sample savings in episodes or demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map visualizations are provided for OpenVLA+; authors rely on behavioral metrics and embedding visualizations instead.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes — t-SNE visualizations on CIFAR-10 show that representations after training with OpenVLA+ are tighter and more linearly separable than those from direct finetuning; linear-probe classification numbers reported are higher for OpenVLA+ indicating preserved semantic structure (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Evidence is behavioral and representational: (1) string-based tokenizer enables actions to be generated in the same token space as language, (2) OpenVLA+ shows large gains in paraphrase robustness and visual OOD tests (e.g., PickCan paraphrase performance 84.52% vs baseline 12.12%), and (3) co-training with spatial/affordance vision-language datasets improves generalization — together these indicate stronger grounding between language semantics and action outputs, though no direct mechanistic probe mapping specific verbs to motor commands is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Authors show improved preservation of higher-level semantic clusters (better linear-probe performance) for OpenVLA+; they do not provide explicit layer-level breakdown showing which layers encode low-level vs high-level features most improved.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>OpenVLA+ transfers well when: (a) the frozen encoder preserves pretrained semantic/visual features, (b) action outputs are represented as string tokens aligned with the language vocabulary, and (c) training mixes robot demos with vision-language datasets rich in spatial/affordance content. Transfer is weaker under direct fine-tuning without these elements and when instruction paraphrases or visual perturbations are introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Evaluations include OOD visuals and distractors; OpenVLA+ shows much smaller performance drops when objects/distractors or backgrounds are changed compared to baseline. Exact per-object in-vs-out numeric breakdowns are not exhaustively tabulated beyond example tasks and aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>OpenVLA+ demonstrates strong zero-shot robustness to paraphrased instructions (see Table IV: PickCan orig 90.32%, paraphrase 84.52%; MoveNear orig 77.60%, paraphrase 78.19%), indicating effective zero-shot language-conditioned transfer without having seen paraphrases during training.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Ablations show each component's contribution: partially-frozen dual encoder alone improves OpenVLA from 35.03% -> 55.55%; string tokenizer alone raises to 50.25%; co-training combined with string tokenizer yields 78.17%; full D+S+C yields 78.46% — indicating layer/component-level interventions (freezing one encoder copy, exchanging action head) materially affect transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Baseline evidence: direct finetuning on robotic action data (without the preservation recipe) causes representation collapse and poor OOD performance; naïve co-training without the unified tokenizer is sometimes suboptimal. No case where OpenVLA+ degrades performance relative to simpler baselines is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No explicit controlled experiment comparing OpenVLA+ to a vision-only (ImageNet/self-supervised-only) pretrained variant is provided; improvements are reported relative to VLM-initialized baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit timeline/dynamics of representation change during finetuning is quantified beyond before/after t-SNE visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No formal dimensionality or intrinsic-dimension analysis; representational comparisons are via t-SNE and linear-probe performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1906.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1906.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0 + (π0 with Dual Encoder + String Tokenizer + Co-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The π0 baseline adapted with the paper's preservation recipe (partially-frozen dual encoder, string-based action tokenizer, and vision-language co-training) to improve robustness and generalization for robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0+ (π0 with D + S + C)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>π0 architecture modified by replacing its original flow-matching action head with the string-based tokenizer, adding a partially-frozen dual visual encoder (one frozen, one trainable), and co-training on mixed batches of robot demonstrations and vision-language datasets emphasizing spatial reasoning and affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>initialized from a pretrained VLM (PaliGemma) and then co-trained; leverages vision-language pretraining for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Base PaliGemma VLM pretraining (large image-text corpora) provides initial multimodal knowledge; co-training uses vision-language datasets (LLaVA Visual Instruct CC3M, VQASynth-Spatial, OneVision, RoboPoint) plus robot demonstration datasets (RT-1/Bridge/OXE) to preserve and adapt pretrained features.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (same as π0/OpenVLA tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Short-horizon manipulation tasks in SimplerEnv and on a real robot; continuous 7-DoF action space represented as strings with four-decimal precision for Δx,y,z and rotations plus gripper bit; evaluated on tasks including PickCan, OpenDrawer, MoveNear, PickKnife, PickCarrot with OOD visuals and paraphrased instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Authors explicitly seek to improve semantic alignment between pretrained VLM representations and robotic actions via the string tokenizer (aligning actions into language token space) and co-training on spatial/affordance datasets; empirically π0+ shows improved alignment as evidenced by higher task success and language robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>π0+ average success on Visual Matching around 69.19% (Table II) vs π0 baseline ~62.64%; π0+ shows improved paraphrase robustness (e.g., PickCan orig 83.40%, paraphrase 55.40% vs baseline with instruction augmentation 84.33% -> 42.20%). The paper reports consistent improvements in OOD visual robustness and VQA benchmarks for π0+ relative to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported; baseline π0 in the paper is VLM-initialized and is used as the primary point of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit per-sample learning curves or demonstrations-to-performance comparisons; claims about better behavior in low-data regimes are qualitative.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis provided for π0+.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes — overall embedding analyses (t-SNE and linear-probe) show that representations under π0+ retain better semantic structure and yield higher accuracy on VQA benchmarks relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral and representational evidence: changing the action head to string tokenizer and co-training improves π0's robustness to paraphrases and visual variants, suggesting stronger grounding between language tokens and the produced motor/action outputs; direct mechanistic probes tying specific verb semantics to motor primitives are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit layer-wise hierarchical feature breakdown; improvements are demonstrated in higher-level semantic separability and downstream reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Same as OpenVLA+: partial freezing to preserve pretrained visual features, string tokenization to align action outputs to language tokens, and co-training with spatially relevant vision-language datasets improve transfer; naive finetuning or replacing the tokenizer with flow-matching reduces robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Evaluated via OOD visuals/distractors and paraphrased instructions; π0+ shows less degradation on novel visual variants and unseen paraphrases compared to the baseline. The paper provides per-task paraphrase numbers (see Table IV) but not exhaustive per-object tables.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>π0+ demonstrates improved zero-shot robustness to paraphrased instructions compared to the baseline (see paraphrase metrics above); no few-shot numeric claims are given.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Ablations performed include swapping the flow-matching head for the string tokenizer and using a dual encoder with partial freezing; these ablations show measurable improvements, indicating specific components/layers are responsible for transfer gains.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Direct finetuning without preservation strategies leads to representation collapse and degraded OOD performance; naive co-training may also be detrimental without the unified string tokenization and partial freezing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No explicit vision-only pretraining comparison is reported for π0+ in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No time-resolved analysis of representational drift during training is provided beyond before/after comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality or intrinsic-dimension statistics reported; embedding analysis is qualitative (t-SNE) plus linear-probe performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>π 0 : A vision-language-action flow model for general robot control. <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-languageaction model. <em>(Rating: 2)</em></li>
                <li>Rt-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>Magma: A foundation model for multimodal ai agents. <em>(Rating: 1)</em></li>
                <li>Robopoint: A vision-language model for spatial affordance prediction in robotics. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1906",
    "paper_id": "paper-281315107",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source vision-language-action (VLA) architecture used as a baseline VLA in this paper, initialized from pretrained vision-language/backbone components and finetuned on robot action data.",
            "citation_title": "Openvla: An open-source vision-languageaction model.",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "A vision-language-action model that combines pretrained visual backbones and a language model to autoregressively predict robot actions; in this paper it is initialized from pretrained VLM components (DINO + SigLIP + Llama) and used as a baseline VLA architecture processing RGB images and language commands to output short-horizon continuous robot trajectories (encoded as tokens in some variants).",
            "pretraining_type": "vision-language / multimodal pretraining (pretrained VLM components: visual backbones + language model)",
            "pretraining_data_description": "Initialized from VLM components trained on web-scale image-text data and robust visual backbones (paper cites DINO, SigLIP, and Llama); pretraining data is described generally as image-text pairs with object descriptions, spatial relationships and web-scale visual-language pairs (the paper does not list the exact pretraining corpora for OpenVLA in detail).",
            "target_task_name": "robotic manipulation (short-horizon trajectory prediction)",
            "target_task_description": "Robotic manipulation tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear, PickKnife, PickCarrot, etc.) where policy maps RGB observations and a fixed natural-language command to H-step low-level actions a_t = (Δx, Δy, Δz, ϕ, θ, ψ, g). Evaluations performed in simulation (SimplerEnv Visual Matching and Visual Variant Aggregation) and on a real ViperX 300s robot; action space is continuous 7-DoF end-effector translation/rotation plus binary gripper state.",
            "semantic_alignment": "Paper discusses semantic alignment: baseline OpenVLA is initialized from VLMs (visual + language) so there is substantial overlap in object / spatial concept representation, but direct fine-tuning on robotic action data disrupts pretrained representations; the authors explicitly aim to improve alignment via string-based action tokenization and co-training with spatial/affordance vision-language data.",
            "performance_with_language_pretraining": "Baseline OpenVLA (initialized from pretrained VLMs) average success rate on SimplerEnv Visual Matching: 35.03% (Table II). Language-robustness (PickCan original): 36.70% success; paraphrased: 12.12% (Table IV).",
            "performance_without_language_pretraining": "Paper does not provide a direct OpenVLA run with no language / no VLM pretraining (random init) for comparison; the reported baseline OpenVLA is VLM-initialized. Authors do report that direct fine-tuning on robot data degrades pretrained visual representations but do not report numeric performance for a vision-only or randomly initialized OpenVLA.",
            "sample_efficiency_comparison": "No explicit quantitative sample-efficiency numbers comparing language-pretrained vs non-pretrained models are provided. The paper states co-training and preserving pretrained features help in low-data regimes, but offers no numeric sample-count comparisons (e.g., episodes or demonstrations-to-performance).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention-map analysis is reported for OpenVLA. The paper does not present attention visualizations or per-token attention diagnostics.",
            "embedding_space_analysis": "Yes — the paper visualizes visual encoder features with t-SNE on CIFAR-10 before/after VLA training and reports that baseline OpenVLA features after direct fine-tuning show degraded class separability, whereas representations preserved via the authors' recipe have tighter, more separable clusters and higher linear-probe classification scores (qualitative + linear-probe numbers shown in Figure 4).",
            "action_grounding_evidence": "Indirect evidence: authors show that representing continuous actions as character sequences (string tokenizer) and co-training with vision-language data improves task success and language robustness (substantial increases in success rates and paraphrase robustness), which the authors interpret as better grounding of action outputs into pretrained language/visual representations. There is no direct probing that ties specific verbs to particular visual affordances or motor primitives beyond improved behavioral metrics.",
            "hierarchical_features_evidence": "Partial: t-SNE and linear-probe analyses indicate that higher-level semantic clusters (class separability) in visual representations are better preserved with the proposed recipe; no explicit analysis separating low-level vs high-level feature layers beyond the dual-encoder freezing ablations.",
            "transfer_conditions": "Transfer improves when (1) a partially-frozen dual visual encoder is used (one frozen 'anchor' encoder + one trainable encoder), (2) actions are tokenized as strings to align with language pretraining, and (3) co-training mixes robot demos with vision-language datasets emphasizing spatial reasoning/affordances. Transfer degrades under direct finetuning (representation collapse), and naive co-training without the unified tokenizer can be suboptimal.",
            "novel_vs_familiar_objects": "Authors evaluate out-of-distribution visuals (background masking/randomization, distractors) and unseen paraphrased instructions. Results show large performance drops for baselines on OOD visuals and paraphrases, while models using the preservation recipe maintain much higher success rates; the paper does not provide a detailed per-object in-vs-out comparison with exact numeric deltas per object type beyond distractor examples.",
            "zero_shot_or_few_shot": "Zero-shot language generalization: authors generate synonymous paraphrases (via GPT-4) and evaluate zero-shot generalization to these unseen instructions. Baseline OpenVLA: large performance drop on paraphrases (PickCan 36.70% -&gt; 12.12%); OpenVLA+ (full method) largely preserves performance zero-shot (PickCan 90.32% orig, 84.52% paraphrase), demonstrating strong zero-shot robustness to instruction paraphrases.",
            "layer_analysis": "Yes — ablations examine freezing vs finetuning: the partially-frozen dual-encoder (freeze one copy of visual encoder, finetune the other) substantially improves performance (OpenVLA: 35.03% -&gt; 55.55% with only dual-encoder), indicating that particular encoder components (frozen vs trainable copy) are important for preserving pretrained features while allowing task adaptation.",
            "negative_transfer_evidence": "Yes — direct fine-tuning of pretrained VLMs on robot action data causes representation degradation and large performance drops under visual or language perturbations (described qualitatively and shown in t-SNE and drop in OOD evaluations). Naive co-training across highly heterogeneous objectives is also reported to sometimes degrade performance unless combined with their tokenizer + partial freezing.",
            "comparison_to_vision_only": "No direct numeric head-to-head comparison to vision-only pretraining (e.g., ImageNet-only) is reported in controlled experiments. The paper discusses visual backbones (DINO) but does not present an explicit vision-only vs vision-language pretraining ablation.",
            "temporal_dynamics": "No detailed analysis of training dynamics over time (e.g., early vs late training phase metrics) is provided beyond before/after representation visualizations.",
            "dimensionality_analysis": "No formal dimensionality / intrinsic-dimension measurements (PCA explained variance, ID) are reported; analysis limited to t-SNE visualizations and linear-probe classification numbers.",
            "uuid": "e1906.0"
        },
        {
            "name_short": "π0",
            "name_full": "π0 (pi-zero)",
            "brief_description": "A vision-language-action flow model used as a representative VLA baseline for general robot control and finetuning experiments in the paper, initialized from the PaliGemma VLM.",
            "citation_title": "π 0 : A vision-language-action flow model for general robot control.",
            "mention_or_use": "use",
            "model_name": "π0 (pi-zero)",
            "model_description": "A flow-matching-based vision-language-action model that predicts robot actions (originally using a flow-matching action head); in this paper π0 is used as a baseline VLA initialized from PaliGemma and evaluated both in its original form and with the authors' string tokenizer + partially-frozen dual encoder + co-training modifications.",
            "pretraining_type": "vision-language pretraining (initialized from the PaliGemma VLM)",
            "pretraining_data_description": "PaliGemma is cited as a versatile 3B VLM for transfer (paper references it); general characterization in this paper: VLM pretraining uses large-scale image-text corpora with semantic and spatial information. The paper does not enumerate PaliGemma's exact pretraining corpora here.",
            "target_task_name": "robotic manipulation (short-horizon trajectory prediction)",
            "target_task_description": "Same robotic manipulation tasks as OpenVLA: PickCan, OpenDrawer, CloseDrawer, MoveNear, etc., mapping RGB observations and language commands to H-step continuous actions (Δx, Δy, Δz, rotations, gripper), evaluated in SimplerEnv simulation and on real robot; original π0 uses a flow-matching head for actions (replaced by string tokenizer for π0+ experiments).",
            "semantic_alignment": "Paper notes π0 initialized from a VLM (PaliGemma) so it benefits from vision-language pretraining; however, directly finetuning action heads can break alignment. The authors' approach aims to preserve alignment via dual encoder and string action tokenization, which they show improves generalization for π0 as well.",
            "performance_with_language_pretraining": "Baseline π0 (with instruction augmentation) average in-table: 62.64% (Table II). With the authors' full method (π0+), average reported around 69.19% (Table II), an improvement of ~7 percentage points according to the text.",
            "performance_without_language_pretraining": "No explicit π0 run with no language pretraining or random init is reported; baseline π0 is VLM-initialized and sometimes trained with instruction augmentation as noted by the authors.",
            "sample_efficiency_comparison": "No explicit numeric sample-efficiency comparison is provided for π0 between language-pretrained and non-pretrained variants. The paper argues their recipe helps in low-data regimes but does not quantify sample counts for π0.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention visualization or analysis is reported specifically for π0.",
            "embedding_space_analysis": "The paper includes embedding/t-SNE analyses for the visual encoder generally (applies to both architectures) showing representations degrade with direct finetuning and are better preserved under their method; π0+ is reported to have improved separability and downstream VQA/representation metrics (Figure 4 and Figure 5).",
            "action_grounding_evidence": "Indirect evidence via behavioral improvements: replacing π0's flow-matching head with the string tokenizer and co-training yields higher success rates and improved robustness to paraphrased instructions and OOD visuals, interpreted as improved grounding between language-conditioned outputs and visual affordances. No fine-grained probe explicitly mapping verbs to motor patterns is presented.",
            "hierarchical_features_evidence": "No explicit per-layer hierarchical analysis for π0; visual representation analyses (t-SNE) indicate preservation of higher-level semantic structure is beneficial.",
            "transfer_conditions": "π0 benefits from (a) partially-frozen dual-encoder, (b) string-based tokenizer replacing flow-matching head to align action outputs with language pretraining, and (c) co-training on vision-language spatial datasets; naive finetuning or instruction-augmentation-only setups perform worse under perturbations.",
            "novel_vs_familiar_objects": "Paper reports OOD evaluations (visual variant aggregation, distractors) and paraphrase robustness for π0 and π0+, showing π0+ performs better under OOD conditions (specific numbers for paraphrase robustness: for PickCan π0 w/ Inst. Aug.: 84.33% orig -&gt; 42.20% paraphrase; π0+: 83.40% orig -&gt; 55.40% paraphrase), indicating better robustness to unseen language variants; direct per-object in-vs-out numeric breakdown is not provided.",
            "zero_shot_or_few_shot": "Zero-shot language generalization to paraphrased instructions is evaluated: π0 without the authors' recipe shows large drops to paraphrases, while π0+ retains more performance zero-shot (see paraphrase numbers above), demonstrating improved zero-shot instruction robustness.",
            "layer_analysis": "Yes — authors ablate the action head (flow-matching vs string tokenizer) and show swapping to string tokenizer for π0+ improves transfer/generalization; also dual-encoder freezing benefits are discussed across architectures.",
            "negative_transfer_evidence": "Direct fine-tuning of the pretrained VLM within π0 leads to representation degradation and poor OOD generalization; likewise naive co-training without the unified tokenizer can be suboptimal.",
            "comparison_to_vision_only": "No explicit controlled comparison to purely vision-only pretraining is provided for π0.",
            "temporal_dynamics": "No temporal dynamics (training-phase) analysis beyond before/after representation visualizations.",
            "dimensionality_analysis": "No explicit dimensionality metrics reported for π0 representations (only t-SNE/linear-probe).",
            "uuid": "e1906.1"
        },
        {
            "name_short": "OpenVLA+",
            "name_full": "OpenVLA + (OpenVLA with Dual Encoder + String Tokenizer + Co-training)",
            "brief_description": "The OpenVLA model augmented with the paper's proposed recipe: a partially-frozen dual visual encoder, string-based tokenizer for continuous action-as-text, and co-training with vision-language spatial/affordance datasets to preserve pretrained representations and improve generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenVLA+ (OpenVLA with D + S + C)",
            "model_description": "OpenVLA architecture modified with (1) partially-frozen dual visual encoders (one frozen anchor, one trainable copy), (2) string-based tokenizer that renders continuous action values as character sequences predicted autoregressively, and (3) co-training mixing robot demonstration data with vision-language datasets emphasizing spatial reasoning and affordances; processes RGB and language to autoregress tokens representing actions.",
            "pretraining_type": "initialized from pretrained VLM/backbone components (vision-language + visual backbones + language model) and then co-trained; the core approach leverages vision-language pretraining.",
            "pretraining_data_description": "Initial VLM components (DINO, SigLIP, Llama) originate from web-scale image-text and robust visual pretraining; co-training uses vision-language datasets chosen to emphasize spatial reasoning and affordances (e.g., LLaVA Visual Instruct CC3M, VQASynth-Spatial, OneVision, RoboPoint) combined with robot demonstration datasets (RT-1 / OXE / Bridge) in a 50/50 sampling strategy per batch.",
            "target_task_name": "robotic manipulation (same set as OpenVLA baseline)",
            "target_task_description": "PickCan, OpenDrawer, CloseDrawer, MoveNear, PickKnife, PickCarrot and other short-horizon manipulation tasks in SimplerEnv (simulation) and on a real ViperX 300s robot; continuous 7-DoF action space (Δx, Δy, Δz, rotations, gripper) represented as strings with four decimals; co-training also includes VQA/spatial datasets.",
            "semantic_alignment": "Explicitly optimized: authors design the string tokenizer to align continuous action outputs to the language token space so pretrained language semantics can be reused for action prediction; co-training uses datasets with spatial/affordance content to increase overlap between pretraining semantics and robotic tasks. Empirically this yields large gains in language and visual robustness.",
            "performance_with_language_pretraining": "OpenVLA+ (D+S+C, full method) average success rate on SimplerEnv Visual Matching: 78.46% (Table II). Language robustness on PickCan: orig 90.32%, paraphrase 84.52% (Table IV). Visual OOD robustness metrics: OpenVLA+ and OpenVLA+SC significantly outperform baselines across masked-background and visual-variant aggregation (exact per-task numbers in Table III/figures; overall qualitative improvement emphasized). Real-world trials: OpenVLA+ outperforms baseline OpenVLA across tasks in Table V and Figure 6 (exact per-task numbers for real robot referenced but not enumerated in text excerpt).",
            "performance_without_language_pretraining": "Not reported — the comparison is primarily between baseline OpenVLA (VLM-initialized and finetuned) and OpenVLA+ (the preservation recipe). The OpenVLA baseline is the VLM-initialized reference (35.03% avg).",
            "sample_efficiency_comparison": "No explicit numeric sample-efficiency curves or demonstrations-to-performance counts are provided; authors claim improvements in low-data regimes and reduced overfitting but do not quantify sample savings in episodes or demonstrations.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention-map visualizations are provided for OpenVLA+; authors rely on behavioral metrics and embedding visualizations instead.",
            "embedding_space_analysis": "Yes — t-SNE visualizations on CIFAR-10 show that representations after training with OpenVLA+ are tighter and more linearly separable than those from direct finetuning; linear-probe classification numbers reported are higher for OpenVLA+ indicating preserved semantic structure (Figure 4).",
            "action_grounding_evidence": "Evidence is behavioral and representational: (1) string-based tokenizer enables actions to be generated in the same token space as language, (2) OpenVLA+ shows large gains in paraphrase robustness and visual OOD tests (e.g., PickCan paraphrase performance 84.52% vs baseline 12.12%), and (3) co-training with spatial/affordance vision-language datasets improves generalization — together these indicate stronger grounding between language semantics and action outputs, though no direct mechanistic probe mapping specific verbs to motor commands is provided.",
            "hierarchical_features_evidence": "Authors show improved preservation of higher-level semantic clusters (better linear-probe performance) for OpenVLA+; they do not provide explicit layer-level breakdown showing which layers encode low-level vs high-level features most improved.",
            "transfer_conditions": "OpenVLA+ transfers well when: (a) the frozen encoder preserves pretrained semantic/visual features, (b) action outputs are represented as string tokens aligned with the language vocabulary, and (c) training mixes robot demos with vision-language datasets rich in spatial/affordance content. Transfer is weaker under direct fine-tuning without these elements and when instruction paraphrases or visual perturbations are introduced.",
            "novel_vs_familiar_objects": "Evaluations include OOD visuals and distractors; OpenVLA+ shows much smaller performance drops when objects/distractors or backgrounds are changed compared to baseline. Exact per-object in-vs-out numeric breakdowns are not exhaustively tabulated beyond example tasks and aggregate metrics.",
            "zero_shot_or_few_shot": "OpenVLA+ demonstrates strong zero-shot robustness to paraphrased instructions (see Table IV: PickCan orig 90.32%, paraphrase 84.52%; MoveNear orig 77.60%, paraphrase 78.19%), indicating effective zero-shot language-conditioned transfer without having seen paraphrases during training.",
            "layer_analysis": "Ablations show each component's contribution: partially-frozen dual encoder alone improves OpenVLA from 35.03% -&gt; 55.55%; string tokenizer alone raises to 50.25%; co-training combined with string tokenizer yields 78.17%; full D+S+C yields 78.46% — indicating layer/component-level interventions (freezing one encoder copy, exchanging action head) materially affect transfer.",
            "negative_transfer_evidence": "Baseline evidence: direct finetuning on robotic action data (without the preservation recipe) causes representation collapse and poor OOD performance; naïve co-training without the unified tokenizer is sometimes suboptimal. No case where OpenVLA+ degrades performance relative to simpler baselines is reported.",
            "comparison_to_vision_only": "No explicit controlled experiment comparing OpenVLA+ to a vision-only (ImageNet/self-supervised-only) pretrained variant is provided; improvements are reported relative to VLM-initialized baselines.",
            "temporal_dynamics": "No explicit timeline/dynamics of representation change during finetuning is quantified beyond before/after t-SNE visualizations.",
            "dimensionality_analysis": "No formal dimensionality or intrinsic-dimension analysis; representational comparisons are via t-SNE and linear-probe performance.",
            "uuid": "e1906.2"
        },
        {
            "name_short": "π0+",
            "name_full": "π0 + (π0 with Dual Encoder + String Tokenizer + Co-training)",
            "brief_description": "The π0 baseline adapted with the paper's preservation recipe (partially-frozen dual encoder, string-based action tokenizer, and vision-language co-training) to improve robustness and generalization for robot manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "π0+ (π0 with D + S + C)",
            "model_description": "π0 architecture modified by replacing its original flow-matching action head with the string-based tokenizer, adding a partially-frozen dual visual encoder (one frozen, one trainable), and co-training on mixed batches of robot demonstrations and vision-language datasets emphasizing spatial reasoning and affordances.",
            "pretraining_type": "initialized from a pretrained VLM (PaliGemma) and then co-trained; leverages vision-language pretraining for transfer.",
            "pretraining_data_description": "Base PaliGemma VLM pretraining (large image-text corpora) provides initial multimodal knowledge; co-training uses vision-language datasets (LLaVA Visual Instruct CC3M, VQASynth-Spatial, OneVision, RoboPoint) plus robot demonstration datasets (RT-1/Bridge/OXE) to preserve and adapt pretrained features.",
            "target_task_name": "robotic manipulation (same as π0/OpenVLA tasks)",
            "target_task_description": "Short-horizon manipulation tasks in SimplerEnv and on a real robot; continuous 7-DoF action space represented as strings with four-decimal precision for Δx,y,z and rotations plus gripper bit; evaluated on tasks including PickCan, OpenDrawer, MoveNear, PickKnife, PickCarrot with OOD visuals and paraphrased instructions.",
            "semantic_alignment": "Authors explicitly seek to improve semantic alignment between pretrained VLM representations and robotic actions via the string tokenizer (aligning actions into language token space) and co-training on spatial/affordance datasets; empirically π0+ shows improved alignment as evidenced by higher task success and language robustness.",
            "performance_with_language_pretraining": "π0+ average success on Visual Matching around 69.19% (Table II) vs π0 baseline ~62.64%; π0+ shows improved paraphrase robustness (e.g., PickCan orig 83.40%, paraphrase 55.40% vs baseline with instruction augmentation 84.33% -&gt; 42.20%). The paper reports consistent improvements in OOD visual robustness and VQA benchmarks for π0+ relative to baseline.",
            "performance_without_language_pretraining": "Not reported; baseline π0 in the paper is VLM-initialized and is used as the primary point of comparison.",
            "sample_efficiency_comparison": "No explicit per-sample learning curves or demonstrations-to-performance comparisons; claims about better behavior in low-data regimes are qualitative.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis provided for π0+.",
            "embedding_space_analysis": "Yes — overall embedding analyses (t-SNE and linear-probe) show that representations under π0+ retain better semantic structure and yield higher accuracy on VQA benchmarks relative to baselines.",
            "action_grounding_evidence": "Behavioral and representational evidence: changing the action head to string tokenizer and co-training improves π0's robustness to paraphrases and visual variants, suggesting stronger grounding between language tokens and the produced motor/action outputs; direct mechanistic probes tying specific verb semantics to motor primitives are not provided.",
            "hierarchical_features_evidence": "No explicit layer-wise hierarchical feature breakdown; improvements are demonstrated in higher-level semantic separability and downstream reasoning tasks.",
            "transfer_conditions": "Same as OpenVLA+: partial freezing to preserve pretrained visual features, string tokenization to align action outputs to language tokens, and co-training with spatially relevant vision-language datasets improve transfer; naive finetuning or replacing the tokenizer with flow-matching reduces robustness.",
            "novel_vs_familiar_objects": "Evaluated via OOD visuals/distractors and paraphrased instructions; π0+ shows less degradation on novel visual variants and unseen paraphrases compared to the baseline. The paper provides per-task paraphrase numbers (see Table IV) but not exhaustive per-object tables.",
            "zero_shot_or_few_shot": "π0+ demonstrates improved zero-shot robustness to paraphrased instructions compared to the baseline (see paraphrase metrics above); no few-shot numeric claims are given.",
            "layer_analysis": "Ablations performed include swapping the flow-matching head for the string tokenizer and using a dual encoder with partial freezing; these ablations show measurable improvements, indicating specific components/layers are responsible for transfer gains.",
            "negative_transfer_evidence": "Direct finetuning without preservation strategies leads to representation collapse and degraded OOD performance; naive co-training may also be detrimental without the unified string tokenization and partial freezing.",
            "comparison_to_vision_only": "No explicit vision-only pretraining comparison is reported for π0+ in this paper.",
            "temporal_dynamics": "No time-resolved analysis of representational drift during training is provided beyond before/after comparisons.",
            "dimensionality_analysis": "No dimensionality or intrinsic-dimension statistics reported; embedding analysis is qualitative (t-SNE) plus linear-probe performance.",
            "uuid": "e1906.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "π 0 : A vision-language-action flow model for general robot control.",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-languageaction model.",
            "rating": 2
        },
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": "Magma: A foundation model for multimodal ai agents.",
            "rating": 1
        },
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction in robotics.",
            "rating": 2
        }
    ],
    "cost": 0.018723749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations
17 Sep 2025</p>
<p>Shresth Grover 
Akshay Gopalkrishnan 
Bo Ai 
Henrik I Christensen 
Hao Su 
Xuanlin Li 
U C San Diego 
Hillbot 
Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations
17 Sep 2025546455478AD6EB085F1CB5775884F4CFarXiv:2509.11417v2[cs.RO]
Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments.However, direct fine-tuning on robot data often disrupts these representations and limits generalization.We present a framework that better preserves pretrained features while adapting them for robot manipulation.Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances.Evaluations in simulation and on real robot show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.</p>
<p>I. INTRODUCTION</p>
<p>Building general-purpose robots that generalize across tasks and environments is a long-standing goal in robotics.While foundation models in vision and language generalize well from internet-scale data [1], the lack of large-scale, actionlabeled robot data makes comparable generalization in robotic manipulation challenging [2].Efforts on vision-languageaction (VLA) models seek to improve robotic generalization by leveraging the rich representations of pretrained visionlanguage models (VLMs) and finetuning them on robot data (e.g., [3], [4]).This allows us to leverage advances in VLMs for robotic tasks, presenting a promising pathway toward manipulation policies that generalize across environments, tasks, and embodiments.</p>
<p>However, directly finetuning pretrained VLMs on robot data leads to significant representation degradation.Our preliminary experiments show that, with an existing training recipe [3], [4], background changes and small instruction paraphrases can cause large drops in performance (Figure 1), suggesting that finetuning disrupts the structures of pretrained visual and language representations.Recent work has proposed cotraining on both vision-language and robotic data [5], [6] to mitigate this issue.However, we find that naive co-training on both objectives does not lead to the best performance, as the two sources of data differ substantially in structure, limiting the ability for the robot action prediction process to reuse vision-language representations.These findings point to an important question: what is the training recipe that preserves the powerful, general representations of pretrained VLMs to facilitate generalization of downstream VLAs?</p>
<p>To address this challenge, we explore three design choices.First, we propose mixing frozen and finetuned visual encoders to preserve pretrained VLM representations while keeping high model flexibility to adapt well to robotic tasks.Second, we propose a language-aligned action tokenizer that casts numerical robot actions into character sequences, thereby enabling maximal reuse of pretrained language representations and allowing actions to be refined step by step during generation.Finally, leveraging this unified string-based output space, we co-train the model on both robot and visionlanguage datasets that emphasize spatial affordances and reasoning.The training recipe is general and can be applied to different existing VLA model architectures.</p>
<p>We evaluate these designs in both simulation and realworld settings and find consistent improvements over baseline VLAs.Our approach yields stronger generalization to out-ofdistribution visuals, such as backgrounds, table textures, and distractor objects, as well as to varied language commands, leading to higher task success rates.These results provide practical insights into how pretrained VLMs can be effectively grounded in robotic action, bringing us closer to reliable generalist robot manipulation policies.</p>
<p>II. RELATED WORK</p>
<p>Vision-Language Models (VLMs).VLMs learn unified representations across vision and language modalities through large-scale pretraining on web-scale image-text data.Recent VLMs such as PaliGemma [7], Qwen2.5-VL[8], InternVL3 [9], and many others [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20] combine strong language models with pretrained visual encoders to achieve generalization across diverse tasks such as image captioning, visualquestion answering, and spatial reasoning.Visual backbones like DINOv2 [21] and SigLIP [22] provide robust spatial and semantic grounding, while language models such as Gemma [23], LLaMA [24], and many others [17], [25], [26], [27], [28], [29], [30] enable instruction following and compositional reasoning.While these models show impressive zero-shot and few-shot capabilities, they are primarily optimized for passive perception and reasoning tasks.Adapting them for embodied agents introduces challenges in preserving spatial grounding capabilities, visual robustness, and cross-modal alignment when transitioning from static vision-language data to dynamic robotic control.</p>
<p>Vision-Language-Action Models (VLAs).VLAs [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41]   (c) Success rates for both OpenVLA and π 0 decrease even with slightly-rephrased instructions, revealing language overfitting.While text augmentation helps, the performance still lags significantly behind our proposed approach.</p>
<p>adapt pretrained VLMs for robotic control by fine-tuning them on action-labeled datasets.A widely adopted approach in works like OpenVLA and Magma [4], [42] remaps rarely used language tokens to bins of discretized actions, discarding their pretrained semantics and breaking alignment with the original vision-language representations.Fine-tuning exclusively on robotic data also leads to degradation of visual representations, reducing robustness to distractors, viewpoint shifts, or rephrased instructions.Several methods attempt to mitigate this via input augmentation [43], [44], spatially enriched encoders [21], [22], [45], [46], or auxiliary cues like end-effector traces [47], [48], but their training data is still limited to robotic domains.Recent works have also explored co-training on both vision-language and action data [5], [6], [31], but it is found that robotic action prediction and vision-language reasoning often involve conflicting training objectives, and naïvely co-training on both data types can degrade performance in each domain [5].In this work, we address these co-training issues with a partially frozen visual encoder designed to better preserve pretrained robust visual representations during fine-tuning, and an action tokenizer that reuses pretrained embeddings aligned better with the vision-language training objective.</p>
<p>III. METHOD</p>
<p>The robotic manipulation problem can be modeled as a partially observable Markov decision process (POMDP) [49], [50].At the beginning of each episode, the agent receives a natural language command c ∈ C, which remains fixed for the duration of the task.At each timestep t, the agent observes o t ∈ O, typically an RGB image, and predicts a short-horizon trajectory τ t = (a t , . . ., a t+H−1 ) consisting of H low-level actions.Each action a t ∈ A is parameterized as
a t = (∆x, ∆y, ∆z, ϕ, θ, ψ, g),
where (∆x, ∆y, ∆z) and (ϕ, θ, ψ) denote the end-effector's translation and rotation, and g ∈ {0, 1} indicates the gripper state (open or closed).The policy π thus maps observations and commands to action trajectories, optimized to minimize a task-specific objective, τt = arg min τt L(τ t , τ * t ), where τ * t is the expert trajectory.Directly fine-tuning pretrained VLMs for this objective often results in representation collapse and limited generalization, as shown in Fig. 1.To address this, we introduce three components that can be integrated into the training process of many different VLA models.</p>
<p>A. Partially-Frozen Dual Encoder Architecture</p>
<p>We first study whether using a partially-frozen encoder for VLAs could help retain pretrained representations.Instead of completely freezing visual encoders, which prior work [4] has shown to degrade performance, we propose to use two siamese encoders: a frozen one that serves as an "anchor" to retain the robust, semantically rich representations from VLM pretraining, and a finetuned one that keeps the full flexibility to be specialized to robotic actions prediction.Mathematically,
z t = ϕ frozen (o t ) ∥ ϕ train (o t ) ,
where ϕ frozen denotes the frozen encoder, ϕ train the trainable encoder, and ∥ vector concatenation.The combined latent representation z t is then passed to a language-conditioned action tokenizer ψ to generate an action token sequence,
a t = ψ(z t , c),
In practice, when the base VLA model provides only a</p>
<p>[Δx, Δy, Δz, ɸ, , , g] "0" "." "2" "0" "5"</p>
<p>Frozen Encoder  single vision encoder, we duplicate it and freeze one copy (e.g., [3]); if it already includes two, we freeze one while adapting the other (e.g., [4]).</p>
<p>B. String-Based Tokenizer</p>
<p>To maximally reuse pretrained language representations, we render each action dimension as a character sequence and predict it with the same autoregressive objective used in language pretraining.For example, the action component ∆x = 0.0312 is tokenized into:
0 . 0 3 1 2
Each box denotes a single character token from the language vocabulary.Representing numerical values as strings allows a single head to be co-trained on different prediction objectives (i.e., action for robot data and non-action prediction for vision-language datasets), and makes spatial information in pretrained language representations potentially more useful in adapting to robot action prediction, thereby improving generalization.Autoregressive string generation also refines actions step by step, yielding more precise predictions.We train our model with 4 decimals action.</p>
<p>Although string-based action tokenization along with the dual encoder together can increase model inference time by 0.5×-1.3×(depending on the model), we later show that the benefits substantially outweigh this cost.</p>
<p>C. Feature Regularization via Co-training</p>
<p>Learning solely from robot datasets often leads to overfitting, particularly in low-data regimes common in robotics.</p>
<p>To counter this, we co-train VLAs using a mixture of robot data and vision-language datasets (Table I), enabled by our shared string-based language and action representation.We utilize a mixture of spatial reasoning, spatial affordance, and</p>
<p>Type Source Description</p>
<p>Robot OXE [37] Real robot demonstrations</p>
<p>Vision-Language Reasoning</p>
<p>LLaVA Visual Instruct CC3M [51] Image captioning and object grounding VQASynth-Spatial [52] Spatial reasoning via imagelanguage queries LLaVA OneVision [11] OCR, chart, and multimodal question answering RoboPoint [53] Spatial grounding of language into pixel locations general vision-language reasoning data.For every training batch, we sample 50% from each type to balance the gradient.</p>
<p>We hypothesize that such a joint training strategy prevents catastrophic forgetting and enhances generalization.</p>
<p>IV. EXPERIMENTS</p>
<p>This section seeks to answer the following questions: Q1.Are the proposed designs effective in learning robot manipulation policies?Q2.Does our approach effectively preserve pretrained VLM representations, thereby improving generalization and robustness to novel visuals and instructions?Q3.How well does the learned policy transfer to real-world environments?We investigate Q1-Q2 in simulation and offline visionlanguage benchmark datasets for scalable and controlled study, and validate Q3 on real-world robotic platforms.</p>
<p>For simulation, we use SimplerEnv [54], which provides evaluation results that correlate well with real-world behaviors.To systematically study Q1-Q2, we evaluate under two SimplerEnv setups.The Visual Matching setup contains visuals closely matching the VLA training data, serving as</p>
<p>A. Analysis of Different Design Choices</p>
<p>Evaluation setup.We first perform SimplerEnv -Visual Matching evaluation to filter our design choices, as running the Visual Variant Aggregation evaluation is significantly more costly.We evaluate on four representative manipulation tasks, PickCan, OpenDrawer, CloseDrawer, and MoveNear, each over 300 episodes.Model setup.We utilize two representative VLA architectures: OpenVLA and π 0 , and apply our method from Sec. III.For π 0 , which uses a flow-matching action head, we retain the original head for the baseline and replace it with string-based tokenizer for our method.All models, both baselines and ours, are initialized from their respective pretrained VLMs (DINO + SigLIP + Llama for OpenVLA; PaliGemma for π 0 )</p>
<p>We then finetune the models on the RT-1 dataset [37], [55], optionally with co-training and our method.</p>
<p>Results.We present the Visual Matching evaluation results in Table II.Overall, models trained with our recipe, OpenVLA + and π 0 + , outperform OpenVLA and π 0 by nearly 40% and 7% respectively, showcasing the effectiveness of our method.Effect of partially-frozen dual vision encoder.Utilizing our partially-frozen dual encoder alone improves OpenVLA performance from 35.03% to 55.55%.When combined with our string tokenizer and co-training approaches, the performance further rises to 78.46%, though the gain over only using String Tokenizer + co-training is marginal (78.17% vs. 78.46%).However, the benefits of dual encoder will become clear when we later assess the robustness and generalization of VLAs using vision and language perturbations.</p>
<p>Effect of string tokenizer and vision-language co-training.</p>
<p>Adding string tokenizer to OpenVLA alone raises the success rate from 35.03% to 50.25%.Moreover, when vision-language co-training is utilized, the model trained with string tokenizer performs significantly better than the one without (78.17%vs. 51.05%).This result indicates that unifying the action and language output space makes it easier to harness pretrained features, while not compromising the model's ability to adapt to precise robot actions.We also analyze the effect of data composition in co-training, showing that our selected datasets are well aligned with robotic tasks and consistently yield positive performance gains (Figure 3).</p>
<p>B. Analysis of Representation Generalization and Robustness</p>
<p>Evaluation setup.We assess whether the learned visual and language representations in our models are rich, robust, and thus enable generalizable robot manipulation.We first evaluate visual robustness in two setups: (i) background masking, where background pixels are masked to test visual sensitivity, and (ii) visual variant aggregation in SimplerEnv [54], where task environments are visually randomized to assess generalization across environmental visual appearances.We also visualize learned representations on standard vision datasets for a qualitative analysis.For language robustness, we use GPT-4 to generate synonymous instructions (e.g., "grasp the can" and "get the can" for PickCan), and evaluate zero-shot generalization to these unseen language variants.In addition, we evaluate our models on standard VQA benchmarks to analyze their representation quality through their general reasoning capabilities.Visual robustness.Both OpenVLA + and π 0 + demonstrate substantial improvements in visual robustness across both background masking and visual variant aggregation.Table III shows our methods outperform their respective baseline models on average across four robotic manipulation tasks.Visualizing learned visual representations.We extract learned representations on the CIFAR-10 dataset before and after VLA training, and visualize the low-dimensional embeddings obtained with t-SNE (Figure 4).Embeddings of baseline VLAs typically exhibit unclear boundaries between TABLE III: Out-of-distribution (OOD) visual generalization evaluation in SimplerEnv."Masked Background" refers to Visual Matching evaluation but with background removed (illustrated in Fig. 1(a))."Visual Variant Aggregation" follows the original evaluation protocol in SimplerEnv, randomizing backgrounds, table textures, lightings, distractors, and camera poses for testing OOD generalization.We also report OpenVLA+SC in this table to show that OpenVLA + has better robustness, despite performing very similarly for in-distribution evaluation in Tab.II.OpenVLA + and π 0 + achieve significantly higher performance across all tasks, showing that our training recipe helps retain pretrained representations of VLM backbones.augmentation.This shows that our method achieves stronger instruction generalization by leveraging pretrained representations through our string-based action tokenizer, effectively inheriting the language understanding and spatial reasoning capabilities of upstream VLMs.</p>
<p>Reasoning abilities.To further analyze the representation quality of our models, we present results on common VLM benchmarks like Text-VQA [56], POPE [57], GQA [58], VizWiz [59], and VSR [60] and report the accuracy metric in  Distractors are irrelevant objects that may mislead the policy, such as similar items (e.g., differently colored plates) or dissimilar ones (e.g., knife, carrot, cloth), and are used to assess the robustness of the policies.Our models consistently outperform baselines in their presence.</p>
<p>Fig. 6: Qualitative results on the PickKnife and PickCarrot tasks.Our models have stronger robustness to distractors.Fig. 5.These benchmarks include many challenging reasoning questions that place high demands on the underlying representations.We find that baseline OpenVLA and π 0 perform poorly, whereas OpenVLA + and π 0 + achieve substantially higher accuracy, demonstrating that our training recipe more effectively preserves the reasoning capabilities of pretrained VLM representations within VLAs.</p>
<p>C. Real-World Evaluation</p>
<p>Evaluation setup.We initialized VLAs from their respective pretrained VLMs and finetune the policies on the Bridge dataset.We deploy both baselines and our method on the ViperX 300s2 robot to evaluate real-world performance.Our experimental configuration utilizes a Logitech C920 Webcam for visual input [61] and dual GTX 1080 Ti GPUs3 .We evaluate models on both in-distribution and out-of-distribution (OOD) instructions, along with potentially unseen distractors, to comprehensively analyze model capabilities.We conduct 25 trials per task and per model.For a 7-DoF action, the inference time is about 1.53s for baseline OpenVLA, 2.30s for OpenVLA + , 0.73s for baseline π 0 , and 1.70s for π 0 + .Results. Figure 6 and Table V illustrate the results.We find that our models consistently outperform baseline VLAs on all manipulation tasks.In particular, we observe that base models such as OpenVLA and π 0 tend to misinterpret the task under distractors, often reaching for the closest object in the scene rather than the instructed target, e.g., picking a carrot instead of a knife in PickKnife, or failing to pick the correct carrot among other similar items.In contrast, our approach demonstrates a more robust understanding of the task, successfully completing goal-conditioned actions even in the presence of semantically or spatially similar distractors, and achieving substantially better performance.</p>
<p>V. CONCLUSION</p>
<p>In this paper, we proposed a set of approaches that improve the robustness and generalization of vision-language-action (VLA) models by better preserving the robust representation structures from pretrained vision-language models (VLMs).These include a dual-visual-encoder design that mixes pretrained and finetuned features, aligning robotic action with language output via a string-based tokenizer to better transfer pretrained knowledge, and a balanced co-training approach on both robot and vision-language reasoning data.Experiments in both simulation and the real world demonstrate that our designs enable better learning of robot manipulation policies, better generalization and robustness to novel visuals and instructions, and better performance in the real-world.</p>
<p>Fig. 1 :
1
Fig. 1: Motivating experiments.(a) Performance for both OpenVLA (top) and π 0 (down) is significantly impacted by background variations (e.g., background masking and randomization), indicating visual overfitting.(b) t-SNE plots of visual encoders show that pretrained visual representations deteriorate when VLA models are trained by directly finetuning VLMs on action data.(c)Success rates for both OpenVLA and π 0 decrease even with slightly-rephrased instructions, revealing language overfitting.While text augmentation helps, the performance still lags significantly behind our proposed approach.</p>
<p>Fig. 2 :
2
Fig. 2: Method overview.Our approach improves VLA generalization through three designs: (a) Co-training: Jointly training on robotic and vision-language datasets that emphasize spatial affordance and reasoning helps preserve pretrained representations for generalization.(b) Partially-frozen visual encoders: One encoder is frozen to retain robust pretrained features from VLMs, whereas the other keeps the full flexibility to specialize for the robot tasks.(c) String tokenizer: Robotic actions are expressed as digit-based strings to maximally reuse pretrained language representations and to unify prediction targets across non-robotic and robotic task domains for co-training.</p>
<p>Fig. 3 :
3
Fig. 3: Co-training data composition ablation on Sim-plerEnv (Visual Matching).With our unified string-based tokenizer, combining diverse datasets for co-training consistently improves robotic task performance.</p>
<p>Fig. 4 :
4
Fig.4: t-SNE visualizations of vision encoder features on CIFAR-10.We compare (i) original visual backbone from VLM before VLA training, (ii) after direct VLA fine-tuning on robot data, and (iii) after our approaches in Sec.III.Numbers indicate linear-probe classification performance on CIFAR-10 using the corresponding features.For both OpenVLA and π 0 , our approach yields noticeably tighter, more well-separated class clusters that lead to better linear-probe performance, indicating better preservation of semantic structures in pretrained visual representations.</p>
<p>Fig. 5 :
5
Fig. 5: Evaluating VLAs on five VQA benchmarks.OpenVLA + and π 0 + achieve significantly higher performance across all tasks, showing that our training recipe helps retain pretrained representations of VLM backbones.</p>
<p>TABLE I :
I
Co-training datasets.</p>
<p>TABLE II :
II
Ablation on SimplerEnv (Visual Matching).The Visual Matching evaluation setting contains visuals highly similar to the robot action dataset used for VLA training.D, S, and C denote dual encoder, string tokenizer, and co-training, respectively.OpenVLA + and π 0 + denote the corresponding models trained with all of D, S, and C.
ModelPick CanOpen DrawerClose DrawerMove NearAvgOpenVLA36.7016.7026.7060.03 35.03OpenVLA+D65.3333.9359.2063.75 55.55OpenVLA+S78.001.0065.0057.00 50.25OpenVLA+C66.6764.4111.1162.00 51.05OpenVLA+DS90.0044.0072.1070.00 69.03OpenVLA+SC78.3462.0092.0080.34 78.17OpenVLA +90.3253.5092.4077.60 78.46π0 w/ Inst. Aug. 1 84.3330.4057.2978.54 62.64π + 083.4044.3564.3784.66 69.19
an in-distribution evaluation.The Visual Variant Aggregation setup introduces backgrounds, lightings, table textures, distractor objects, and camera poses out of the distribution of the robot data (OOD), serving as a generalization test.</p>
<p>TABLE IV :
IV
Language robustness evaluation.Evaluating policies under original and paraphrased instructions."w/Inst.Aug."indicates training with instruction augmentations.We also report OpenVLA+SC in this table to show that OpenVLA + has better robustness, despite performing very similarly for in-distribution evaluation in Tab.II.
ModelPickCanMoveNearOrig.Para.Orig.Para.OpenVLA36.70 12.12 60.03 42.07OpenVLA w/ Inst. Aug. 30.56 38.89 56.49 50.00OpenVLA+SC78.34 62.52 80.34 82.10OpenVLA +90.32 84.52 77.60 78.19π 0 w/ Inst. Aug.84.33 42.20 78.52 58.24π 0 +83.40 55.40 84.66 63.33different classes, while representations from our models aremuch more linearly separable, yielding much higher linear-probe classification performance.Language robustness. Table IV compares three settings: (1)baseline VLA models, (2) baselines trained with paraphrased-instruction augmentation, and (3) our models. Unlike (2),where the paraphrased instructions used for evaluation aredisjoint from those seen during training, both (1) and (3)have not seen any paraphrased instructions. The same setof paraphrased instructions is used consistently across allevaluations. Despite receiving no language augmentations,our models substantially outperform baselines trained with</p>
<p>TABLE V :
V
Real-world evaluation results.</p>
<p>Throughout the paper, we trained π 0 baselines with instruction augmentation, as omitting it led to substantially poorer performance; our OpenVLA + and π 0 + models are trained without such augmentation. This setup is thus favorable to the baselines in comparison.
Although a WidowX setup was not available, we adapted the ViperX control stack to closely match that of the WidowX.
More advanced GPUs were not available in the room where we conducted the experiments. However, this should not affect our findings.</p>
<p>On the opportunities and risks of foundation models. R Bommasani, ArXiv. 2021</p>
<p>Good old-fashioned engineering can close the 100,000-year "data gap" in robotics. K Goldberg, 10.1126/scirobotics.aea7390Science Robotics. 101052025</p>
<p>π 0 : A vision-language-action flow model for general robot control. K Black, 10.48550/arXiv.2410.24164arXiv:2410.24164CoRR. 2410.24164, 2024</p>
<p>Openvla: An open-source vision-languageaction model. M J Kim, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Magma: A foundation model for multimodal ai agents. J Yang, arXiv:2502.131302025arXiv preprint</p>
<p>A vision-language-action model with open-world generalization. P Intelligence, arXiv:2504.16054[cs.LG]2025</p>
<p>L Beyer, arXiv:2407.07726Paligemma: A versatile 3b vlm for transfer. 2024arXiv preprint</p>
<p>S Bai, 10.48550/arXiv.2502.13923arXiv:2502.13923Qwen2.5-vl technical report. 2502.13923, 2025</p>
<p>Internvl3: Exploring advanced training and testtime recipes for open-source multimodal models. J Zhu, arXiv:2504.10479[cs.CV]2025</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. P Wang, arXiv:2409.121912024arXiv preprint</p>
<p>Llava-onevision: Easy visual task transfer. B Li, arXiv:2408.033262024arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. J Li, International conference on machine learning. PMLR202319742</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, arXiv:2304.105922023arXiv preprint</p>
<p>Prismatic vlms: Investigating the design space of visually-conditioned language models. S Karamcheti, Forty-first International Conference on Machine Learning. 2024</p>
<p>Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. M Deitke, 10.48550/arXiv.2409.17146CoRR. 2409.17146, 2024</p>
<p>VILA: on pre-training for visual language models. J Lin, 10.1109/CVPR52733.2024.02520IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024. Seattle, WA, USAJune 16-22, 2024, IEEE, 202426689</p>
<p>Phi-3 technical report: A highly capable language model locally on your phone. M I Abdin, 10.48550/arXiv.2404.14219CoRR. 2024</p>
<p>Llava-onevision: Easy visual task transfer. B Li, 10.48550/arXiv.2408.03326arXiv:2408.03326CoRR. 2408.03326, 2024</p>
<p>Cogvlm2: Visual language models for image and video understanding. W Hong, 10.48550/arXiv.2408.16500arXiv:2408.165002408.16500, 2024CoRR</p>
<p>Minicpm-v: A GPT-4V level MLLM on your phone. Y Yao, 10.48550/ARXIV.2408.01800arXiv:2408.018002024</p>
<p>. 10.48550/arXiv.2408.01800</p>
<p>Dinov2: Learning robust visual features without supervision. M Oquab, Trans. Mach. Learn. Res. 20242024</p>
<p>Sigmoid loss for language image pre-training. X Zhai, 10.1109/ICCV51070.2023.01100IEEE/CVF International Conference on Computer Vision, ICCV 2023. Paris, FranceOctober 1-6, 2023, IEEE, 202311952</p>
<p>Gemma: Open models based on gemini research and technology. T Mesnard, 10.48550/ARXIV.2403.08295arXiv: 2403CoRR. 2024</p>
<p>. Online, 10.48550/arXiv.2403.08295</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, arXiv:2307.092882023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, J. Mach. Learn. Res. 242023</p>
<p>-Ai Deepseek, 10.48550/arXiv.2412.19437arXiv:2412.19437Deepseek-v3 technical report. CoRR2412.19437, 2024. 19437</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. -Ai Deepseek, 10.48550/arXiv.2501.12948arXiv:2501.12948CoRR. 2501.12948, 2025</p>
<p>Textbooks are all you need. S Gunasekar, 10.48550/arXiv.2306.11644arXiv:2306.11644CoRR. 2306.11644, 2023</p>
<p>Openai, 10.48550/arXiv.2303.08774arXiv:2303.08774GPT-4 technical report. CoRR2023</p>
<p>Mixtral of experts. A Q Jiang, 10.48550/arXiv.2401.04088arXiv:2401.04088CoRR. 2024</p>
<p>Hamster: Hierarchical action models for openworld robot manipulation. Y Li, arXiv:2502.054852025arXiv preprint</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. J Wen, arXiv:2502.058552025arXiv preprint</p>
<p>Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. C Cui, arXiv:2505.039122025arXiv preprint</p>
<p>Gr-2: A generative video-languageaction model with web-scale knowledge for robot manipulation. C.-L Cheang, arXiv:2410.061582024arXiv preprint</p>
<p>Octo: An open-source generalist robot policy. D Ghosh, 10.15607/RSS.2024.XX.090Robotics: Science and Systems XX. D Kulic, Delft, The NetherlandsJuly 15-19, 2024. 2024</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, ser. Proceedings of Machine Learning Research. D Liu, J Kulic, Ichnowski, Auckland, New ZealandPMLRCoRL 2022, 14-18 December 2022. 2022205Conference on Robot Learning</p>
<p>Open x-embodiment: Robotic learning datasets and RT-X models : Open x-embodiment collaboration. A O'neill, 10.1109/ICRA57147.2024.10611477IEEE International Conference on Robotics and Automation. Yokohama, JapanMay 13-17, 2024, IEEE, 2024</p>
<p>RVT-2: learning precise manipulation from few demonstrations. A Goyal, 10.15607/RSS.2024.XX.055Robotics: Science and Systems XX. D Kulic, Delft, The NetherlandsJuly 15-19, 2024. 2024</p>
<p>VIMA: general robot manipulation with multimodal prompts. Y Jiang, 10.48550/arXiv.2210.03094arXiv:2210.030942022CoRR</p>
<p>Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. J Liu, arXiv:2503.106312025arXiv preprint</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. J Bjorck, arXiv:2503.147342025arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, Conference on Robot Learning. PMLR2023</p>
<p>Robotic skill acquisition via instruction augmentation with vision-language models. T Xiao, 10.15607/RSS.2023.XIX.029Robotics: Science and Systems XIX. K E Bekris, Daegu, Republic of KoreaJuly 10-14, 2023. 2023</p>
<p>Scaling robot learning with semantically imagined experience. T Yu, 10.15607/RSS.2023.XIX.027Robotics: Science and Systems XIX. K E Bekris, Daegu, Republic of KoreaJuly 10-14, 2023. 2023</p>
<p>Learning transferable visual models from natural language supervision. A Radford, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. M Meila, T Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139ser. Proceedings of Machine Learning Research</p>
<p>Theia: Distilling diverse vision foundation models for robot learning. J Shang, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. J Gu, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, AustriaMay 7-11, 2024, OpenReview.net, 2024</p>
<p>LLARVA: vision-action instruction tuning enhances robot learning. D Niu, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Deep visual navigation under partial observability. B Ai, IEEE International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artif. Intell. 1011-21998</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202414465</p>
<p>Robopoint: A vision-language model for spatial affordance prediction in robotics. W Yuan, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Evaluating real-world robot manipulation policies in simulation. X Li, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, arXiv:2212.068172022arXiv preprint</p>
<p>Towards vqa models that can read. A Singh, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Evaluating object hallucination in large visionlanguage models. Y Li, arXiv:2305.103552023arXiv preprint</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. D A Hudson, C D Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Vizwiz: Nearly real-time answers to visual questions. J P Bigham, Proceedings of the 23nd annual ACM symposium on User interface software and technology. the 23nd annual ACM symposium on User interface software and technology2010</p>
<p>Visual instruction tuning. H Liu, Advances in neural information processing systems. 202336</p>
<p>Bridgedata V2: A dataset for robot learning at scale. H R Walke, ser. Proceedings of Machine Learning Research. J Tan, M Toussaint, K Darvish, Atlanta, GA, USAPMLRCoRL 2023, 6-9 November 2023. 2023229Conference on Robot Learning</p>            </div>
        </div>

    </div>
</body>
</html>
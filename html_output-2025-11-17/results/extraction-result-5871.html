<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5871 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5871</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5871</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-257833697</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.17511v1.pdf" target="_blank">On pitfalls (and advantages) of sophisticated large language models</a></p>
                <p><strong>Paper Abstract:</strong> Natural language processing based on large language models (LLMs) is a booming field of AI research. After neural networks have proven to outperform humans in games and practical domains based on pattern recognition, we might stand now at a road junction where artificial entities might eventually enter the realm of human communication. However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism. This also concerns the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it makes a massive spread of misinformation possible.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5871.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5871.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaTensor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaTensor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI system (DeepMind) reported to have discovered novel matrix multiplication algorithms; cited in this paper as an example of neural networks producing novel algorithmic/mathematical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering novel algorithms with AlphaTensor</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaTensor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not described in detail in this paper; cited only as an AI that discovered novel matrix multiplication algorithms (see Fawzi et al. 2022 for architecture and training details).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Algorithms / theoretical computer science (matrix multiplication)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>mathematical algorithm (matrix multiplication algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td>Reported only as 'novel matrix multiplication algorithms' (no explicit equation or relationship given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not reported in this paper (see original AlphaTensor paper for evaluation details).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This paper only cites AlphaTensor as an example of an AI discovering novel algorithms; no experimental details or quantitative metrics are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper with respect to AlphaTensor beyond the single citation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5871.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5871.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model trained for scientific content that was intended to generate plausible scientific text; cited here as an example of an LLM applied to scientific corpora but criticized for producing plausible-sounding yet nonsensical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not detailed in this paper beyond the name and citation (Taylor et al. 2022); presented as an LLM targeted at scientific text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Scientific literature / scholarly writing generation</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not reported in this paper; Galactica is discussed here only with regard to its tendency to generate plausible but incorrect scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an LLM built for scientific text that was taken offline after criticism for producing vaguely plausible but ultimately nonsensical academic papers; no claim in this paper that Galactica distilled quantitative laws from corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes Galactica produced plausible-sounding but nonsensical academic papers; implies reliability issues for scientific knowledge extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5871.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5871.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system for protein structure prediction cited here as an example of impressive scientific-domain success of neural networks, not as a system that distills quantitative laws from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not described in detail in this paper; cited as an example of a neural network that achieved high accuracy on protein structure prediction (see Jumper et al. 2021 for full details).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Structural biology / protein structure prediction</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not discussed in this paper (original AlphaFold paper contains evaluation against known protein structures).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an example of domain-specific neural network success; this paper does not claim AlphaFold extracts quantitative laws from scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed here beyond being an example of domain-specific success versus LLM limitations in language/truth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Discovering novel algorithms with AlphaTensor <em>(Rating: 2)</em></li>
                <li>Galactica: A Large Language Model for Science <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with AlphaFold <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5871",
    "paper_id": "paper-257833697",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [
        {
            "name_short": "AlphaTensor",
            "name_full": "AlphaTensor",
            "brief_description": "An AI system (DeepMind) reported to have discovered novel matrix multiplication algorithms; cited in this paper as an example of neural networks producing novel algorithmic/mathematical outputs.",
            "citation_title": "Discovering novel algorithms with AlphaTensor",
            "mention_or_use": "mention",
            "model_name": "AlphaTensor",
            "model_description": "Not described in detail in this paper; cited only as an AI that discovered novel matrix multiplication algorithms (see Fawzi et al. 2022 for architecture and training details).",
            "task_domain": "Algorithms / theoretical computer science (matrix multiplication)",
            "input_corpus_description": null,
            "distillation_method": null,
            "quantitative_law_type": "mathematical algorithm (matrix multiplication algorithm)",
            "example_law_extracted": "Reported only as 'novel matrix multiplication algorithms' (no explicit equation or relationship given in this paper).",
            "evaluation_method": "Not reported in this paper (see original AlphaTensor paper for evaluation details).",
            "results_summary": "This paper only cites AlphaTensor as an example of an AI discovering novel algorithms; no experimental details or quantitative metrics are provided here.",
            "limitations_challenges": "Not discussed in this paper with respect to AlphaTensor beyond the single citation.",
            "comparison_to_baselines": "Not provided in this paper.",
            "uuid": "e5871.0"
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A Large Language Model for Science",
            "brief_description": "A large language model trained for scientific content that was intended to generate plausible scientific text; cited here as an example of an LLM applied to scientific corpora but criticized for producing plausible-sounding yet nonsensical outputs.",
            "citation_title": "Galactica: A Large Language Model for Science",
            "mention_or_use": "mention",
            "model_name": "Galactica",
            "model_description": "Not detailed in this paper beyond the name and citation (Taylor et al. 2022); presented as an LLM targeted at scientific text generation.",
            "task_domain": "Scientific literature / scholarly writing generation",
            "input_corpus_description": null,
            "distillation_method": null,
            "quantitative_law_type": null,
            "example_law_extracted": null,
            "evaluation_method": "Not reported in this paper; Galactica is discussed here only with regard to its tendency to generate plausible but incorrect scientific text.",
            "results_summary": "Cited as an LLM built for scientific text that was taken offline after criticism for producing vaguely plausible but ultimately nonsensical academic papers; no claim in this paper that Galactica distilled quantitative laws from corpora.",
            "limitations_challenges": "Paper notes Galactica produced plausible-sounding but nonsensical academic papers; implies reliability issues for scientific knowledge extraction.",
            "comparison_to_baselines": "Not provided in this paper.",
            "uuid": "e5871.1"
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold",
            "brief_description": "A deep learning system for protein structure prediction cited here as an example of impressive scientific-domain success of neural networks, not as a system that distills quantitative laws from papers.",
            "citation_title": "Highly accurate protein structure prediction with AlphaFold",
            "mention_or_use": "mention",
            "model_name": "AlphaFold",
            "model_description": "Not described in detail in this paper; cited as an example of a neural network that achieved high accuracy on protein structure prediction (see Jumper et al. 2021 for full details).",
            "task_domain": "Structural biology / protein structure prediction",
            "input_corpus_description": null,
            "distillation_method": null,
            "quantitative_law_type": null,
            "example_law_extracted": null,
            "evaluation_method": "Not discussed in this paper (original AlphaFold paper contains evaluation against known protein structures).",
            "results_summary": "Mentioned as an example of domain-specific neural network success; this paper does not claim AlphaFold extracts quantitative laws from scientific literature.",
            "limitations_challenges": "Not discussed here beyond being an example of domain-specific success versus LLM limitations in language/truth.",
            "comparison_to_baselines": "Not provided in this paper.",
            "uuid": "e5871.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Discovering novel algorithms with AlphaTensor",
            "rating": 2,
            "sanitized_title": "discovering_novel_algorithms_with_alphatensor"
        },
        {
            "paper_title": "Galactica: A Large Language Model for Science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold",
            "rating": 1,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        }
    ],
    "cost": 0.01011625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On pitfalls (and advantages) of sophisticated Large Language Models</p>
<p>Anna Strasser 
Faculty of Philosophy
Ludwig-Maximilians-Universität München</p>
<p>DenkWerkstatt Berlin</p>
<p>On pitfalls (and advantages) of sophisticated Large Language Models
1Large Language Modelshuman-machine discrimination abilitiesethical consequencesprivacy rightshuman counterfeitsoverreliancemisinformation
Natural language processing based on large language models (LLMs) is a booming field of AI research. After neural networks have proven to outperform humans in games and practical domains based on pattern recognition, we might stand now at a road junction where artificial entities might eventually enter the realm of human communication. However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism. This also concerns the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it makes a massive spread of misinformation possible.</p>
<p>Introduction</p>
<p>Natural language processing based on large language models (LLMs), such as BERT, Eleuther, ChatGPT, GPT-3, LaMDA, and PALM, is a booming field of AI research. In general, artificial systems based on neural networks and deep learning have achieved impressive success in many domains based on pattern recognition, like speech recognition, lipreading, and game-playing. Examples in this area include LipNet (Assael et al. 2016), a program for lung cancer screening (Ardila et al. 2019), AlphaFold -predicting protein structure (Jumper et al. 2021), and AlphaTensor -discovering novel matrix multiplication algorithms (Fawzi et al. 2022), DeepBlue (Campbell 2002), AlphaGo and other gameplaying programs (Brown &amp; Sandholm 2019;Silver et al. 2016Silver et al. , 2018. With respect to language, we have observed impressive progress in automatic translation (DeepL) and computer code generation (GitHub Copilot) over the past few years, all relying on LLMs and deep learning. Remarkably, LLMs are able to produce grammatically correct linguistic outputs with fluency, often similar to that of a human. Many of their outputs can hardly be distinguished from linguistic outputs originally created by humans, even though some demonstrate a lack of common sense and embarrassingly expose the models. 1 After neural networks have proven to outperform humans in games and practical domains relying on pattern recognition, we potentially now stand at a road junction where artificial entities could eventually enter the realm of human communication. However, this comes with serious risks that we should consider with caution. The widespread use of LLMs, along with expected advances in their development, will make it increasingly difficult to distinguish between human-written and machine-generated text. This fact alone is already going to create all sorts of new challenges. For instance, it will be difficult to prove human authorship beyond doubt, and it will be equally difficult to unmask with certainty machine-generated text that is fraudulently passed off as self-written text. Furthermore, text generated by LLMs has the potential to be abused for a new kind of plagiarism, fraud, and the spread of misinformation. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Precisely because they are so good at mimicking human linguistic performance, there is a risk that they will be used to mass-produce misinformation (Marcus 2022(Marcus , 2023. With the help of LLMs, it will be even easier than before to create an infinite amount of text for troll farms and fake websites, which in turn will lead to a decline in the level of trustworthiness on the Internet. Even though the outputs of LLMs often sound very convincing, they are not reliably truthful. Critical voices say that, for example, ChatGPT is a 'bullshit generator' (McQuillan 2023). One could go so far as to conjecture that with the further development of such language models, a new class of weapons is emerging that can have devastating effects in the war for truth (cp. Guardian editorial 2023). In this chapter, I focus on the consequences current and further advanced LLMs might have. I start with an overview of how difficult it is already now to distinguish between machine-generated and human-made text (2. Difficult to distinguish). Thereafter, I discuss various ethical consequences arising from this indistinguishability. Specifically, I address challenges related to difficult-to-prove human authorship, examine new forms of plagiarism, and critically evaluate copyright and privacy issues related to the model construction. I also consider the possibilities of counterfeiting people and spreading misinformation and toxic language (3. Ethical consequences). In the next section, I discuss possible ways by which society might deal with the inherent risks of LLMs. This concerns not only a possible adaptation of our legal basis but also the technical possibilities available for implementing the legislation (4. How to handle the epistemological crisis). After addressing at length the potential risks associated with the increased use of LLMs, I turn to the question of the extent to which there might also be helpful applications arising from LLMs that could be used in everyday life (5. LLMs as thinking tools).</p>
<p>Background</p>
<p>Since research related to LLMs is a fairly young and rapidly evolving field of research, it is difficult to provide an overview of the state of the art that is not already outdated at the time of publication, as there is a constantly growing body of new publications. Research papers, as well as opinion papers from computer science and philosophy and various publications in the media, served as the basis for this chapter. These are, for example, technical papers introducing specific LLMs such as BERT (Devlin et al. 2018;Rogers et al. 2020), Eleuther (https://www.eleuther.ai), GPT-3 (Brown et al. 2020), LaMDA (Thoppilan et al. 2022), and PALM (Chowdhery et al. 2022) as well as meta-reviews on benchmarks used to assess the performance of language models (Michael et al. 2022, Srivastava et al. 2022). Since the initial release of GPT-3 on June 11, 2020, LLMs have attracted the interest of other disciplines, such as linguistics, cognitive science, philosophy, and others, and have also received considerable public attention (Mahowald et al. 2023;Marcus &amp; Davis 2020;Heaven 2020;Simonite 2020;Weinberg (ed.) 2020). For example, Mahowald and colleagues (2023) investigate the capabilities of LLMs, distinguishing between formal competence (the knowledge of linguistic rules) and functional competence, which refers to understanding and using language in the world. They conclude that LLMs are close to mastering formal competence but fail at functional competence tasks. Public perception is initially blown away by the impressive achievements of LLMs, and also from scientific communities, the degree of impressiveness is immense as it is shown, e.g., by the title of an opinion piece by Will Douglas Heaven (2020): "OpenAI's new language generator GPT-3 is shockingly good -and completely mindless." In June 2022, Google's LaMDA model made international headlines when Google engineer Blake Lemoine said he became convinced that LaMDA was sentient, prompting a flood of papers (Bryson 2022;Frankish 2022;Hofstadter 2022;Klein 2022;Roberts 2022;Shanahan 2023;Tiku 2022). Although the majority is not inclined to ascribe sentience to LLMs, there is a trend to use philosophically loaded terms, such as "knowing," "believing," "comprehending," and "thinking" when describing these systems. The extent to which this can be a justified use of these terms is open to debate. Similarly, ChatGPT, launched in November 2022, evoked a long-lasting echo in the media and the academic communities (Chiang 2023;Krakauer &amp; Mitchell 2022;Lock 2022;Roose 2022;Thorp 2023;Wolfram 2023). For example, Derek Thompson (2022) mentioned ChatGPT in The Atlantic magazine's "Breakthroughs of the Year" as part of "the generative-AI eruption" that "may change our mind about how we work, how we think, and what human creativity really is." In the beginning, the enthusiastic voices received a lot of attention, but now the critical voices seem also to gain more consideration (Guardian editorial 2023;Hofstadter 2022;Marcus 2020Marcus , 2023Marcus &amp; Davis 2020McQuillan 2023).</p>
<p>Hard to distinguish</p>
<p>Just ten years ago, people didn't give much thought to how to distinguish machine-generated text from human-generated text. The differences were so obvious back then, and it didn't seem like that would change quickly. Back then, the differences were so obvious, and it didn't look like that that this would change anytime soon. However, with the advent of more and more upscaled LLMs, this is becoming a serious problem. Today, in social media, e-customer service, and advertising, we are increasingly exposed to machine-generated content that can easily be mistaken for human-generated content. Neither humans nor sophisticated detection software can distinguish with certainty between humangenerated and machine-generated text. In empirical research, this indistinguishability, along with the tendency of humans to anthropomorphize, is sometimes even exploited when experimental protocols with artificial agents are used to test hypotheses about human social cognitive mechanisms (Strasser 2022;Wykowska et al. 2016). And it is already a concern for teachers that they will not be able to distinguish their students' self-written essays from machine-generated ones (Herman 2022;Hutson 2022;Huang 2023;Marche 2022;Peritz 2022Sparrow 2022.</p>
<p>Human discrimination abilities</p>
<p>The more advanced LLMs are, the more difficult it becomes for humans to distinguish between machine-generated and human-made text. Besides various rather informal assessments (Rajnerowicz 2022;Sinapayen 2023;Vota 2020), there are three studies using rigorous psychological methods to test human's ability to distinguish between machine-generated and human-made text (Clark et al. 2021;Brown et al. 2020;Schwitzgebel et al. 2023). By demonstrating a difference between the two basic models of Open-AI (GPT-2 and GPT-3), Clark and colleagues (2021) were able to show that the more advanced the LLMs, the more difficult the distinction becomes. They collected short human-generated texts in three domains: stories, news articles, and recipes, and used the two base models to generate texts within the same domains. The participants (6 groups covering the three domains for each model) were then presented with five selected texts and asked to judge whether these texts were likely to have been generated by humans or by machines. Results for the older model, GPT-2, showed that participants were able to accurately distinguish between GPT-2-generated and human-generated texts 58% of the time, significantly above the chance rate of 50%. In contrast, accuracy in discriminating between the newer model (GPT-3) and human-generated text was only 50%, not significantly different from chance. Even additional training in follow-up experiments failed to increase accuracy to above 57% in any domain. These results show that scaling up the models makes it more difficult to distinguish between human-made and machinegenerated text, and it is expected that the results of such an experiment will point even more clearly in this direction when GPT-4 is on the market. Brown et al. (2020) focused on the domain of news articles and found similar results, indicating a moderately good discrimination rate for smaller (older) language models and near-chance performance with the largest version of GPT-3. In the study I conducted with Eric and David Schwitzgebel (Schwitzgebel et al. 2023), we fine-tuned the Davinci model of GPT-3 on the corpus of the well-known philosopher Daniel Dennett (Strasser et al. 2023) and tested three groups of participants (ordinary naïve participants, philosophical blog readers, and experts of Dennett's work). Our results showed that only the discrimination abilities of blog readers and experts were significantly above the chance rate of 20% 2 (blog readers 48%, experts 51%), even though lower than we hypothesized. In comparison, ordinary participants were near the chance rate of 20%. Given the expected improvement of future models, this suggests that probably even expertise in a domain will soon no longer provide a reliably way to distinguish machine-generated text from human-made text. Already now, experts on Dennett could, on average, only identify Dennett's answer half the time when presented with his answer alongside four answers from our fine-tuned language model. 3 These empirical results clearly show that human-machine discrimination abilities with respect to the linguistic output of LLMs are no longer a reliable criterion for identifying the linguistic outputs of LLMs beyond doubt.</p>
<p>Discrimination with the help of detection software</p>
<p>One might think that if humans are unable to recognize the machine-generated text as machinegenerated text, it should at least be possible to tell the difference beyond doubt using detection software. But at least with respect to the current state of research, even detection software cannot distinguish with 100% certainty between machine-generated and human-made text. Here, we seem to be at the beginning of an arms race between fraudsters and fraud detection. Eric Mitchell and colleagues (2023) proposed a method called DetectGPT for deciding if a text passage was generated by a particular source model, for example, GPT-3. This method is based on the idea that if a text was generated by GPT-3, then this text has a high probability according to GPT-3, while humanwritten text does not have such a high probability from the point of view of GPT-3 (a nice explanation of this method can be found in the blog of Melanie Mitchell (2023)). Tests with several large language models showed that their method was able to distinguish between human-written and LLM-generated text in over 95% of the cases. However, 95% is not 100%, and also it is critical to note that the number of possible specific LLMs is constantly increasing, and since this method is specific to a particular model, the number of models that need to be tested may present a problem. One could argue that pretty much any LLM uses a similar neural network architecture and is trained with comparable training data. But, after all, one cannot rule out the possibility that there will be other LLMs in the future. Moreover, LLM users can manually set a preferred probability, to what extent this poses difficulties for this method would need to be tested. It is important to keep in mind that current detectors for LLM-generated text commit two types of errors: false-negative (machine-generated text falsely judged to be written by humans) 4 and falsepositive errors (human-generated text falsely judged to be machine-generated). False positives can be very harmful to humans, as I will describe in the next section. As long as we cannot exclude that such detectors falsely accuse humans of cheating, they should be used with caution and with the knowledge that their judgment could be false. This no longer indubitable distinctness leads to various other difficulties, which I will address in more detail below.</p>
<p>Ethical consequences</p>
<p>Due to the ever-increasing indistinguishability between machine-generated and human-generated texts, various ethical problems arise, especially in the age of electronic transfer. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism, but concerns also the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it enables the massive spread of misinformation. Such consequences are certainly also supported by the hype that has taken place on social media. Investigating the first reactions to the release of LLMs like GPT-3 or ChatGPT, the voices expressing their astonishment and deep impression seem to be in the majority. However, right from the start, several scholars demonstrated how easy it is to expose LLMs (Marcus &amp; Davis 2020). It seems that the critical voices have become more prominent only recently (Guardian Editorial 2023), even though the enthusiastic voices have not dried out. But even independent of a possible overestimation of the factual capabilities of LLMs, in the future, we will have to deal with ethical issues that arise primarily from the fact that we can no longer clearly distinguish machine-generated outputs from humanwritten outputs.</p>
<p>How to verify authorship</p>
<p>Unless authors are directly monitored in the process of writing their texts, and it is ruled out that they can use LLMs in the writing process, it will no longer be possible to infer a human author beyond doubt on a textual basis. It could be that the accusation of passing off a machine-generated text as one's own can no longer be dispelled in the last instance. This means that it could become difficult to prove oneself beyond doubt as the author of a text when submitting a paper. Assuming that also in the future, neither humans nor detection software can reliably distinguish between machine-generated and human-written text, we have to be prepared to deal with false positives and false negatives when trying to recognize a human-written text. From that point of view, it is conceivable that an author submitting a paper could be falsely accused of having delivered a machine-generated text, and conversely, it is equally possible for a machine-generated text could pass as human-generated. As LLMs continue to advance, it may even become common to submit newspaper articles or even articles to scientific journals in which much of the content is produced by machines. For sure, it is expected that students could make use of LLMs to let them produce text for their essays, and their teachers will not be able to recognize whether the students delivered self-made texts (Herman 2022;Hutson 2022;Huang 2023;Marche 2022;Peritz 2022;Sparrow 2022). Universities might turn back to in-person exams to reassure the authorship of their students. However, this is not possible in all cases where authorship matters, especially with respect to the mass of electronically distributed texts. How new chains of trust can be established will be a challenge for future societies.</p>
<p>New forms of plagiarism</p>
<p>Since language models create novel sentences -they are not parrots -it is unlikely that their output will lead to any results when using standard plagiarism checkers. Using plagiarism checkers, similarity thresholds below 10%-15% are considered ordinary for non-plagiarized work (Mahian et al. 2017). Schwitzgebel and colleagues (2023) investigated whether fine-tuning GPT-3 on Dennett's works might have led the model to be overtrained 5 so that it simply parroted sentences or multi-word strings of texts from Dennett's corpus. To verify that the fine-tuned model was indeed producing new texts, we used the Turnitin plagiarism checker to check for "plagiarism" between the machine-generated outputs and the Turnitin corpus supplemented with the works that were used as the training data. Turnitin reported an overall similarity of 5 % between the GPT-3 generated answers and the comparison corpora, and none of the passages were flagged as similar to the training corpus. Also, the search for matching text strings between the GPT-3 responses and the training corpus revealed no matches, except for some stock phrases favored by analytic philosophers. Thus, the machinegenerated text does not simply plagiarize its training data word for word but generates novel -albeit stylistically and philosophically similar -content. Nevertheless, it is at least arguable whether outputs of fine-tuned language models should be considered plagiarism because they sort of 'borrow' ideas from their training data with which they were fine-tuned without acknowledging the original author.</p>
<p>Violation of copyright rights and privacy</p>
<p>Another issue concerns the intellectual property of the persons who wrote the text with which LLMs are trained. For example, OpenAI's GPT-3 was trained on hundreds of billions of words of text (499 billion tokens) 6 from Common Crawl, WebText, books, and Wikipedia (Brown et al. 2020). And especially fine-tuned models like the one we fine-tuned on the corpus of Daniel Dennett have to put up with the question of whether this is a fair use of someone else's intellectual property to use their works in creating an LLM without asking permission. For this reason, we asked Daniel Dennett for the explicit permission before fine-tuning our LLM on his texts, and we agreed that he has the final say when it comes to the question of who may use this language model and which outputs are published.</p>
<p>It goes without saying that we always explicitly label the outputs of our model as machine-generated output. However, in the case of already deceased individuals, it is not possible to ask for permission (for a review regarding the potential use of personal data of deceased persons, see Nakagawa &amp; Orita 2022). To date, copyright laws regarding the use of copyrighted text as training data for fine-tuning language models have not yet been clarified (see Government UK consultations 2021). And the question of whether output from LLMs could be protected by copyright has not yet been conclusively resolved. According to a report in The Verge (Robertson 2022), the U.S. Copyright Office recently rejected a request to grant copyright to a work of art to an AI because, in their view, it was a necessary standard for protection that the work of art contain at least elements of "human authorship."</p>
<p>Counterfeits of people</p>
<p>Furthermore, it is a matter of fact that people leave behind a lot of private data that are not secured at all. LLMs can be fine-tuned on all kinds of additional training data. This could concern any information a person has shared on social media. For example, one could use all kinds of data available about someone's life, e.g., in social media, blogs, websites, online stores, and search engines, to create a digital replica that could convincingly imitate some of that person's behavior (Karpus &amp; Strasser under review). A striking illustration of such a replica, albeit a fictional one, can be found in the episode "Be right back" of the television series Black Mirror (Brooker 2013). This episode inspired Eugenia Kuyda to feed all the saved online conversations she had with a deceased friend into an AI-powered system to create a chatbot version of her friend. Subsequently, a public application called The Replika was created (see https://replika.com; Murphy 2019). The extent to which existing LLMs, tools, and chatbots based on more or less private data violate copyright rights and rights concerning privacy seems to me to be an open question that we should urgently address. Even if the science fiction story depicted in the Black Mirror episode is still too futuristic in many aspects in the context of today's technology, it is worth considering how revealing the data we leave behind on social media, search engines, online stores, and other platforms already is about our character traits concerning our likes, dislikes, desires and other features of our personality. Besides the scandals involving Cambridge Analytica in the UK which demonstrated the richness of the data available about our lives on social media platforms, there is an insightful art project by the Berlinbased artist collective Laokoon (https://www.madetomeasure.online/en). Their investigative project 'Made to measure' explored the question of how far one can get in constructing a doppelganger of a person using data available online about that person's life. Using anonymized data of a person's Google search history from five years of that person's life, they retraced the life of that person and re-enacted that life in a film. When confronting the data donor, who was contacted after processing the data with the help of a personalized Instagram message, it turned out that the reconstruction of this person's life was amazingly accurate in many aspects (for a more detailed description, see Karpus &amp; Strasser under review). If digital replicas start to speak on behalf of the person out of which data they were constructed, they could be considered as a counterfeit of this person. In other words, if outputs of LLMs were presented as a quotation or paraphrase of positions of existing persons, this would constitute counterfeiting (Dennett as interviewed in Cukier 2022). With the help of increasingly sophisticated language models, we will be able to create fakes of people that are difficult to distinguish from their originals. Just as we are able to create counterfeit money, it is conceivable that in virtual communication, for example, one can create the appearance of interacting with a real person that turns out to be a fake. Such deep fakes do not have to be restricted to linguistic output; they can also be enriched with visual and auditory imitations. In Germany, for example, a mayor was made to believe that she was interacting with Vitali Klitschko in a zoom-call (Hoppenstedt 2022). Another example, again from the field of art, that illustrates the potential of counterfeiting is the art project "Chomsky vs. Chomsky" (Rodriguez 2022). It is important to note that while it was made clear from the outset that this is an artifact and not the real Chomsky, this project nevertheless shows how disturbing a digital replica can be. This art project presents a virtual version of Noam Chomsky -a location-based, Mixed Reality (MR) experience that draws not only on Chomsky's texts but also on recorded lectures. Thereby, this project offers the experience of asking questions orally in virtual reality and receiving an audio response whose sound is almost indistinguishable from the recordings of the real Chomsky. No doubt, it should be against the law to present a conversational AI without making it clear that it is an AI and not a human. If an LLM counterfeits people, the creator and the users of this LLM are guilty of a crime. However, until now, we do not have the legal basis to prosecute such crimes.</p>
<p>Spread of misinformation, nonsense and toxic language</p>
<p>Stepping back from the emotional and overwhelming judgments shared in social media, it is an important and serious question to investigate to what extent we can trust machine-generated assertions. The danger of mistakenly trusting GPT-3 is particularly evident in the health sector. We should be clear about the fact that within this domain, we are nowhere near any application where GPT-3 could provide reliable help in any sense. It lacks the scientific and medical expertise that would make it useful for any medical Q&amp;A, as it can be very wrong, and this is not viable in healthcare. For example, in a test where GPT-3 responded to mental health problems, the AI advised a simulated patient to commit suicide (Daws 2020). Nevertheless, despite the warning from OpenAI it is probably to be expected that such applications will be developed. It is a matter of fact that LLMs based on a transformer architecture with a statistical self-attention mechanism -machines that calculate the probability of words appearing in the context of other words -have severe limitations regarding reliability. This becomes evident, for example, when LLMs make self-contradictory statements. It is quite possible that if an LLM receives the same question as a prompt several times, it will respond with very different answers which are contradicting each other. In contrast, a standard calculator will always give the same answer, for example, it will always 'claim' that 2+2 equals 4, whereas LLMs are able to 'claim' that 2+2 equals 4 in one instance and that 2+2 equals 5 in another. For this reason, it is important that a balanced assessment of the performance of LLMs should not rely solely on cherry-picked, mind-boggling outputs. Moreover, the better the models become, the more difficult it becomes to distinguish machinegenerated linguistic outputs from human-made utterances, and at the same time, the risk of misuse increases. For example, LLMs can play a weighty role in spreading misinformation (Marcus 2022). Since LLMs are inherently unreliable, they do make severe mistakes in reasoning and facts. This unreliability is due in part to the fact that LLMs build models of word sequences based on how humans use language rather than models describing how the world works. Although it can be concluded from this that many machine-generated linguistic results are correct because human language often reflects facts in the world. But at the same time, it follows that the accuracy of LLM's statements is, to some extent, a matter of chance because, unlike humans, machines do not use language to refer to the world. An example that can illustrate the production of misinformation and nonsense is the brief presence of Galactica. This LLM was created to write plausible-sounding academic papers. However, it was taken offline a few days after its release due to harsh criticism amounting to the claim that Galactica produces vaguely-plausible-sounding-but-ultimately-nonsensical academic papers (Al-Sibai 2022; Taylor et al. 2022). Likewise, I suppose that it is to be expected that applications that aim to assist search applications like ChatGPT with respect to Bing will soon be taken offline (Rogers 2023). Lately, the most discussed LLM has been ChatGPT. Unfortunately, ChatGPT has a tendency to hallucinate -it produces statements that sound plausible but are simply false. It is able to invent references of papers that were never been written, it can make up historical dates, and it commits severe failures regarding the solution of logical problems. Furthermore, LLMs lack what we would call 'common sense' in the human case. It is a serious problem that LLMs can produce sentences that are simply not true (for a repository of errors made by LLMs, see Davis et al. 2023. To date, LLMs have no reliable mechanisms for verifying the truth of their statements, and this is effectively a springboard for the mass production of misinformation. For example, Gary Marcus (2023) reports that the independent researcher Shawn Oakley has shown how easy it is to get ChatGPT to produce misinformation that it even backs up with fictional studies. This is especially troubling as ChatGPT adopts an authoritative tone. Another problem with LLMs is that their outputs depend on their training data. This means that if they are not constantly retrained, they quickly become obsolete in terms of up-to-date information. ChatGPT lacks 'knowledge' of events that occurred after 2021. They can make prophecy-like statements about events that may have happened in the meantime, but these statements lack any relation to our reality. Because of their limited reliability, LLMs require human supervision, but this leads to another critical ethical issue. OpenAI strives to filter out toxic content (e.g., sexual abuse, violence, racism, sexism, etc.), which is a good goal in principle, but the implementation is not perfect and is also highly questionable. To flag toxic data produced by LLMs, one needs humans, and this work is traumatic in nature. According to a TIME investigation, OpenAI used outsourced Kenyan laborers earning less than $2 per hour to make ChatGPT less toxic (Perrigo 2023). Apart from the fact that this practice is ethically questionable, it is also not ultimately successful from a technical point of view.</p>
<p>How to handle the epistemological crisis</p>
<p>Due to all potential deep fakes, there is an epistemological crisis to be expected, and people will need to look out for what they take as representing a real person. Avoiding that we get too suspicious and paranoid, we need laws for how AIs present themselves, and we will probably have to develop new strategies for identifying our counterparts as humans. One helpful measure would be to create a legal basis for requiring machine-generated output to be labeled as machine-generated text as a matter of principle. This is addressed, for example, by a recent proposal, the so-called AI-act of the European Commission (2021), which requires labeling for anything that might be mistaken for human interaction. Such regulations could help mitigate the risk that LLMs will be used to contribute to a huge spread of misinformation. One way to label machine-generated text could be accomplished through the use of digital watermarks (Wigger 2022). Kirchenbauer and colleagues (2023) have suggested that one way to do this would be to require the creators of LLMs to add a watermark signal to each generated text passage that cannot be easily removed by simply modifying the text, and to provide open-source software for watermark detection. This sounds good at first glance, but one cannot assume that all LLM creators will adhere to it, and of course, it is also possible to fool watermark detectors. Again, it is likely that an arms race will develop here between fraudsters and those who want to mark LLM's outputs recognizably. And as described above, so far, there is no completely reliable method for detecting AI-generated text. So, bans cannot be enforced proactively, which means that one has to rely on human help. For example, a conference has banned the use of machine-generated text in submissions. However, in the end, only submissions that are deemed suspicious to other scientists were examined (Vincent 2022). It seems as if we are not prepared for the emergence of such disruptive and novel technologies.</p>
<p>LLMs as thinking tools</p>
<p>All of this is not to say that positive applications of LLMs are not conceivable. Many applications that we use on a daily basis are based on models that have been trained to predict the next word or words in a text based on the preceding words, e.g., it is part of the technology that predicts the next word you want to type on your mobile phone allowing you to complete the message faster. Likewise, the increased quality of translation software is indeed a helpful tool, although it is still advisable not to release the translation without human review. In other domains, such as game-playing AIs, neural network architectures have led to success (Campbell 2002;Brown &amp; Sandholm 2019;Silver et al. 2016Silver et al. , 2018. And impressive results can also be pointed to in scientific fields, such as a program for lung cancer screening (Ardila et al. 2019), or AlphaFold, that is predicting protein structure (Jumper et al. 2021), and AlphaTensor, that is discovering novel matrix multiplication algorithms (Fawzi et al. 2022). However, it is important to keep in mind that the performance of these neural networks is tied to clearly definable goals. The output generated by machines can be evaluated and verified on the basis of clear criteria. A victory in a chess game is clearly defined, and the individual steps are also subject to a set of rules. However, when we move into the realm of communication with human language, it is not always clear whether an answer to a question and its justifications meet our requirements for comprehension, rational thinking, and the like. Considering the potential of large language models and assuming that technology will continue to advance, it is conceivable that language models will soon produce results interesting enough to serve as a valuable resource for human researchers. In other domains, computer programs are able to generate music in the style of a particular composer (Hadjeres et al. 2017;Daly 2021;Elgammal 2021) or create all kinds of images (DALL-E). Even if not all outputs are reliable or interesting, selected outputs seem to have significant musical or artistic value. A composer or artist could produce many outputs, select the most promising, edit them slightly, and present them as original works. As a matter of fact, there was already a case where an AI-produced picture did win a competition (Metz 2022). In this way, language models could become thinking tools that people use. However, it is important that the users are experts in their domains and remain able to validate the outputs. For example, a researcher could fine-tune a language model with certain corpora and then generate outputs they can use as inspiration for further ideas. However, when using language models as thinking tools, one must be careful not to rely too heavily on them as deep learning networks always have reliability issues (Alshemali &amp; Kalita 2020;Bosio et al. 2019). A user with insufficient expertise might mistakenly assume that all results from a large language model fine-tuned to an author's work reflect the author's actual views (Bender et al. 2021;Wedinger et al. 2021). The use of such language models will not be able to replace reading the original works (see Steven &amp; Iziev 2022), but they may eventually become a helpful tool for humans creating text. According to a whitepaper published by Lionbridge (2023), ChatGPT can help with translation, terminology, style guides, content classification, post-editing, content analysis, and creating working code. However, it can only help -one can never rely on an LLM to say true things or know what is right or wrong, outputs of all LLMs are unreliable. Therefore, it remains up to humans to decide what makes sense and what is true or false. AI can be used to improve, polish, edit, or write texts, but it will still be up to humans to judge their value.
 For examples of typical mistakes, see ChatGPT/LLM error tracker (Davies et al. 2023).
Chance rate is at 20% because we used a five-alternative forced choice task.
In other domains, such as deep fake detection for audio(Groh et al. 2021;Müller et al. 2022) or differentiating humanmade artwork from AI-generated artwork(Gangadharbatla 2022) also rather weak human discrimination capabilities were found. 4 The first version of GPTzero (https://gptzero.me), for example, did evaluate the 40 machine-generated answers used in the experiment of Eric Schwitzgebel and colleagues as human-like.
Being overtrained is an issue regarding neural networks. However, running four epochs of fine-tuning is a standard recommendation from OpenAI, and in most applications, four epochs of training do not result in overtraining(Brownlee 2019). 6 A token is a sequence of commonly co-occurring characters, with approximately ¾ of an English word per token on average.</p>
<p>Facebook Takes Down AI That Churns Out Fake Academic Papers After Widespread Criticism. The Byte. N Al-Sibai, Al-Sibai, N. (2022). Facebook Takes Down AI That Churns Out Fake Academic Papers After Widespread Criticism. The Byte. https://futurism.com/the-byte/facebook-takes-down-galactica-ai</p>
<p>GitHub deepmind / alphatensor. Alphatensor , AlphaTensor. GitHub deepmind / alphatensor. https://github.com/deepmind/alphatensor</p>
<p>Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems. B Alshemali, J Kalita, 10.1016/j.knosys.2019.105210191105210Alshemali, B. &amp; Kalita, J. (2020). Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191, 105210. doi:10.1016/j.knosys.2019.105210</p>
<p>End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. D Ardila, A P Kiraly, S Bharadwaj, B Choi, J J Reicher, L Peng, D Tse, M Etemadi, W Ye, G Corrado, D P Naidich, S Shetty, 10.1038/s41591-019-0447-xNature medicine. 256Ardila, D., Kiraly, A. P., Bharadwaj, S., Choi, B., Reicher, J. J., Peng, L., Tse, D., Etemadi, M., Ye, W., Corrado, G., Naidich, D. P., &amp; Shetty, S. (2019). End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nature medicine, 25(6), 954-961. doi: 10.1038/s41591-019- 0447-x</p>
<p>LipNet: Sentence-level Lipreading. Y Assael, B Shillingford, S Whiteson, N Freitas, 10.48550/arXiv.1611.01599Assael, Y., Shillingford, B., Whiteson, S., &amp; Freitas, N. (2016). LipNet: Sentence-level Lipreading. doi:10.48550/arXiv.1611.01599</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, doi.org/10.1145/3442188.3445922BERT:OfficialGitHubFAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610-623. doi.org/10.1145/3442188.3445922 BERT: Official GitHub repository. https://github.com/google-research/bert</p>
<p>A Reliability Analysis of a Deep Neural Network. A Bosio, P Bernardi, Ruospo, E Sanchez, 10.1109/LATW.2019.8704548IEEE Latin American Test Symposium (LATS). Bosio, A., Bernardi, P., Ruospo, &amp; Sanchez, E. (2019). A Reliability Analysis of a Deep Neural Network. 2019 IEEE Latin American Test Symposium (LATS), 1-6. doi:10.1109/LATW.2019.8704548</p>
<p>Black Mirror: Be Right Back. C Brooker, Episode 1) [movieBrooker, C. (2013). Black Mirror: Be Right Back (Season 2, Episode 1) [movie].</p>
<p>Contributor on AI or human lyrics: Could you tell which is which?. Zeppotron, E Brown, Zeppotron. Brown, E. (2020). Contributor on AI or human lyrics: Could you tell which is which? https://www.zdnet.com/article/ai-or-human-lyrics-could-you-tell-which-is-which</p>
<p>. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ...</p>
<p>Language models are few-shot learners. D Amodei, 10.48550/arXiv.2005.14165Advances in neural information processing systems. 33Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901. doi:10.48550/arXiv.2005.14165</p>
<p>Superhuman AI for multiplayer poker. N Brown, T Sandholm, 10.1126/science.aay2400Science. 365Brown, N. &amp; Sandholm, T. (2019). Superhuman AI for multiplayer poker. Science. 365. doi:10.1126/science.aay2400</p>
<p>A gentle introduction to early stopping to avoid overtraining neural networks. Machine learning mastery. J Brownlee, One Day, AI Will Seem as Human as Anyone. What ThenBrownlee, J. (2019). A gentle introduction to early stopping to avoid overtraining neural networks. Machine learning mastery. https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural- network-models Bryson (2022). One Day, AI Will Seem as Human as Anyone. What Then. Wired. https://www.wired.com/story/lamda-sentience-psychology-ethics-policy</p>
<p>Deep blue. M Campbell, A J HoaneJr, F H Hsu, Artificial intelligence. 1341-2Campbell, M., Hoane Jr, A. J., &amp; Hsu, F. H. (2002). Deep blue. Artificial intelligence, 134(1-2), 57-83.</p>
<p>ChatGPT Is a Blurry JPEG of the Web. T Chiang, Chiang, T. (2023) ChatGPT Is a Blurry JPEG of the Web. The New Yorker. https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web</p>
<p>Scaling Language Modeling with Pathways. A Chowdhery, S Narang, J Devlin, Palm, Google AI Blog. 2022. Available from. Chowdhery A, Narang S, Devlin J. PaLM: Scaling Language Modeling with Pathways. Google AI Blog. 2022. Available from: https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html</p>
<p>Babbage: Could artificial intelligence become sentient? The Economist. K Cukier, Cukier, K. (2022). Babbage: Could artificial intelligence become sentient? The Economist. https://shows.acast.com/theeconomistbabbage/episodes/babbage-could-artificial-intelligence-become- sentient DALL-E. https://openai.com/blog/dall-e/</p>
<p>AI software writes new Nirvana and Amy Winehouse songs to raise awareness for mental health support. R Daly, Daly, R. (2021). AI software writes new Nirvana and Amy Winehouse songs to raise awareness for mental health support. NME. https://www.nme.com/news/music/ai-software-writes-new-nirvana-amy- winehouse-songs-raise-awareness-mental-health-support-2913524</p>
<p>. E Davis, J Hendler, W Hsu, E Leivada, G Marcus, M Witbrock, V Shwartz, M Ma, Davis, E., Hendler, J., Hsu, W., Leivada, E., Marcus, G., Witbrock, M., Shwartz, V., &amp; Ma, M. (2023).</p>
<p>. / Chatgpt, Llm, Tracker, ChatGPT/LLM error tracker. https://researchrabbit.typeform.com/llmerrors?typeform- source=garymarcus.substack.com</p>
<p>Medical chatbot using OpenAI's GPT-3 told a fake patient to kill themselves. R Daws, Daws, R. (2020). Medical chatbot using OpenAI's GPT-3 told a fake patient to kill themselves. AI News. https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill- themselves/</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, 10.48550/arXiv.1810.04805Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. doi:10.48550/arXiv.1810.04805</p>
<p>. A I Eleuther, Eleuther AI. https://www.eleuther.ai</p>
<p>How a team of musicologists and computer scientists completed Beethoven's unfinished 10th symphony. The Conversation. A Elgammal, Elgammal, A. (2021). How a team of musicologists and computer scientists completed Beethoven's unfinished 10th symphony. The Conversation. https://theconversation.com/how-a-team-of-musicologists-and- computer-scientists-completed-beethovens-unfinished-10th-symphony-168160</p>
<p>AI-act. Proposal for a regulation of the European parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts. European Commission (21.4.2021European Commission (21.4.2021). AI-act. Proposal for a regulation of the European parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts. https://artificialintelligenceact.eu/the-act/</p>
<p>Discovering novel algorithms with AlphaTensor. A Fawzi, Frankish, K. (2022). Some thoughts on LLMs. Blog post at The tricks of the Mind. Fawzi, A. et al. (2022). Discovering novel algorithms with AlphaTensor. https://www.deepmind.com/blog/discovering-novel-algorithms-with- alphatensor?utm_campaign=AlphaTensor&amp;utm_medium=bitly&amp;utm_source=Twitter+Organic Frankish, K. (2022). Some thoughts on LLMs. Blog post at The tricks of the Mind (2 Nov) https://www.keithfrankish.com/blog/some-thoughts-on-llms</p>
<p>The Role of AI Attribution Knowledge in the Evaluation of Artwork. H Gangadharbatla, 10.1177/0276237421994697Empirical Studies of the Arts. 402Gangadharbatla, H. (2022). The Role of AI Attribution Knowledge in the Evaluation of Artwork. Empirical Studies of the Arts, 40(2), 125-142. https://doi.org/10.1177/0276237421994697</p>
<p>Artificial intelligence call for views: copyright and related rights. Github Copilot, GitHub Copilot. https://docs.github.com/en/copilot Government UK consultations (2021). Artificial intelligence call for views: copyright and related rights. https://www.gov.uk/government/consultations/artificial-intelligence-and-intellectual-property-call-for- views/artificial-intelligence-call-for-views-copyright-and-related-rights GPT-3. https://github.com/openai/gpt-3</p>
<p>Deepfake Detection by Human Crowds, Machines, and Machine-Informed Crowds. M Groh, Z Epstein, R Firestone &amp; Picard, Proceedings of the National Academy of Sciences. 1119Groh, M. Epstein, Z., Firestone &amp; Picard, R. (2021). Deepfake Detection by Human Crowds, Machines, and Machine-Informed Crowds. Proceedings of the National Academy of Sciences, 119(1).</p>
<p>The Guardian view on ChatGPT search: exploiting wishful thinking. The Guardian. G Pachet, F Nielsen, F , Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningCMP=share_btn_link Hadjeres,Guardian editorial (10 Feb 2023). The Guardian view on ChatGPT search: exploiting wishful thinking. The Guardian. https://www.theguardian.com/commentisfree/2023/feb/10/the-guardian-view-on-chatgpt- search-exploiting-wishful-thinking?CMP=share_btn_link Hadjeres, G., Pachet, F., &amp; Nielsen, F. (2017). DeepBach: a steerable model for Bach chorales generation. Proceedings of the 34th International Conference on Machine Learning, 1362-1371.</p>
<p>Open AI's new language generator GPT-3 is shockingly good -and completely mindless. W Heaven, MIT Technological Review. Heaven, W. (2020). Open AI's new language generator GPT-3 is shockingly good -and completely mindless. MIT Technological Review. https://www.technologyreview.com/2020/07/20/1005454/openai-machine- learning-language-generator-gpt-3-nlp/</p>
<p>The end of high school English. D Herman, Herman, D. (2022). The end of high school English. The Atlantic. https://www.theatlantic.com/technology/archive/2022/12/openai-chatgpt-writing-high-school-english- essay/672412</p>
<p>Artificial neural networks today are not conscious, according to Douglas Hofstadter. D Hofstadter, Hofstadter, D. (2022, June 9). Artificial neural networks today are not conscious, according to Douglas Hofstadter. The Economist. https://www.economist.com/by-invitation/2022/06/09/artificial-neural- networks-today-are-not-conscious-according-to-douglas-hofstadter</p>
<p>Russische Komiker zeigen Ausschnitt von Giffey-Gespräch mit Fake-Klitschko. M Hoppenstedt, Hoppenstedt, M. (2022, August 11). Russische Komiker zeigen Ausschnitt von Giffey-Gespräch mit Fake- Klitschko. SPIEGEL. https://www.spiegel.de/netzwelt/web/franziska-giffey-russische-komiker-zeigen- ausschnitt-von-gespraech-mit-fake-klitschko-a-527ab090-2979-4e70-a81c-08c661c0ef62</p>
<p>Could AI help you to write your next paper?. M Hutson, 10.1038/d41586-022-03479-wNature. 611Hutson, M. (2022). Could AI help you to write your next paper? Nature 611, 192-193. doi:10.1038/d41586- 022-03479-w</p>
<p>Alarmed by A.I. chatbots, universities start revamping how they teach. The New York Times. K Huang, Huang, K. (2023). Alarmed by A.I. chatbots, universities start revamping how they teach. The New York Times. https://www.nytimes.com/2023/01/16/technology/chatgpt-artificial-intelligence-universities.html</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, D Hassabis, 10.1038/s41586-021-03819-2Nature. 5967873Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Žídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., … Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), 583-589. doi: 10.1038/s41586-021-03819-2</p>
<p>under review). Persons and their digital replicas. J Karpus, A Strasser, Karpus, J. &amp; Strasser, A. (under review). Persons and their digital replicas.</p>
<p>A Watermark for Large Language Models. J Kirchenbauer, J Geiping, Y Wen, J Katz, I Miers, T Goldstein, 10.48550/arXiv.2301.10226Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., &amp; Goldstein, T. (2023). A Watermark for Large Language Models. doi:10.48550/arXiv.2301.10226</p>
<p>This is a weirder moment than you think. The New York Times. E Klein, AI won an art contest, and artists are furiousKlein, E. (2022, June 19). This is a weirder moment than you think. The New York Times. https://www.nytimes.com/2022/06/19/opinion/its-not-the-future-we-cant-see.htmlMetz, R. (2022, September 3). AI won an art contest, and artists are furious. CNN Business. https://edition.cnn.com/2022/09/03/tech/ai-art-fair-winner-controversy/index.html</p>
<p>The Debate Over Understanding in AI's Large Language Model. D Krakauer, M Mitchell, 10.48550/arXiv.2210.13966Krakauer, D. &amp; Mitchell, M. (2022). The Debate Over Understanding in AI's Large Language Model. doi:10.48550/arXiv.2210.13966</p>
<p>What ChatGPT gets right and wrong and why it's probably a game-changer for the localization industry. Lionbridge, Lionbridge (2023). What ChatGPT gets right and wrong and why it's probably a game-changer for the localization industry. https://www.lionbridge.com/content/dam/lionbridge/pages/whitepapers/whitepaper-what-chatgpt- gets-right-and-wrong/chatgpt-whitepaper-english.pdf</p>
<p>What is AI chatbot phenomenon ChatGPT and could it replace humans? The Guardian. Samantha Lock, Lock, Samantha (2022). What is AI chatbot phenomenon ChatGPT and could it replace humans? The Guardian. https://www.theguardian.com/technology/2022/dec/05/what-is-ai-chatbot-phenomenon-chatgpt-and- could-it-replace-humans</p>
<p>Measurement of similarity in academic contexts. O Mahian, M Treutwein, P Estellé, S Wongwises, D Wen, G Lorenzini, A Sahin, 10.3390/publications5030018Publications. 53Mahian, O., Treutwein, M., Estellé, P., Wongwises, S., Wen, D., Lorenzini, G., … Sahin, A. (2017). Measurement of similarity in academic contexts. Publications, 5(3), 18, doi:10.3390/publications5030018</p>
<p>Dissociating language and thought in large language models: a cognitive perspective. K Mahowald, A A Ivanova, I A Blank, N Kanwisher, J B Tenenbaum, E Fedorenko, 10.48550/arXiv.2301.06627Mahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., &amp; Fedorenko, E. (2023). Dissociating language and thought in large language models: a cognitive perspective. doi:10.48550/arXiv.2301.06627.</p>
<p>Will ChatGPT kill the student essay? The Atlantic. S Marche, Marche, S. (2022). Will ChatGPT kill the student essay? The Atlantic. https://www.theatlantic.com/technology/archive/2022/12/chatgpt-ai-writing-college-student- essays/672371/</p>
<p>GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about. G Marcus, E Davis, MIT Technology Review. Marcus, G. &amp; Davis, E. (2020). GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about". MIT Technology Review.</p>
<p>Large language models like ChatGPT say the darnedest things. Blog post at The Road to AI We Can Trust. G Marcus, E Davis, Marcus, G., &amp; Davis, E. (2023). Large language models like ChatGPT say the darnedest things. Blog post at The Road to AI We Can Trust (Jan 10). https://garymarcus.substack.com/p/large-language-models-like-chatgpt</p>
<p>AI platforms like ChatGPT are easy to use but also potentially dangerous. G Marcus, Scientific American. Marcus, G. (2022). AI platforms like ChatGPT are easy to use but also potentially dangerous. Scientific American. https://www.scientificamerican.com/article/ai-platforms-like-chatgpt-are-easy-to-use-but-also- potentially-dangerous</p>
<p>Inside the Heart of ChatGPT's Darkness. Blog post at The Road to AI We Can Trust. G Marcus, Marcus, G. (2023). Inside the Heart of ChatGPT's Darkness. Blog post at The Road to AI We Can Trust (Feb 11) https://garymarcus.substack.com/p/inside-the-heart-of-chatgpts- darkness?utm_source=substack&amp;utm_medium=email</p>
<p>ChatGPT Is a Bullshit Generator Waging Class War. D Mcquillan, McQuillan, D. (2023). ChatGPT Is a Bullshit Generator Waging Class War. Vice. https://www.vice.com/en/article/akex34/chatgpt-is-a-bullshit-generator-waging-class-war</p>
<p>What do NLP researchers believe? Results of the NLP community metasurvey. J Michael, A Holtzman, A Parrish, A Mueller, A Wang, A Chen, . . Bowman, S R , 10.48550/arXiv.2208.12852Michael, J., Holtzman, A., Parrish, A., Mueller, A., Wang, A., Chen, A., ... &amp; Bowman, S. R. (2022). What do NLP researchers believe? Results of the NLP community metasurvey. doi:10.48550/arXiv.2208.12852</p>
<p>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. E Mitchell, Y Lee, A Khazatsky, C D Manning, C Finn, 10.48550/arXiv.2301.11305Mitchell, E., Lee, Y., Khazatsky, A., Manning, C.D., &amp; Finn, C. (2023). DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. doi:10.48550/arXiv.2301.11305</p>
<p>Human Perception of Audio Deepfakes. N Müller, K Pizzi, J Williams, 10.1145/3552466.3556531Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia (DDAM '22). the 1st International Workshop on Deepfake Detection for Audio Multimedia (DDAM '22)New York, NY, USAAssociation for Computing MachineryMüller, N., Pizzi, K. &amp; Williams, J. (2022). Human Perception of Audio Deepfakes. In Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia (DDAM '22). Association for Computing Machinery, New York, NY, USA, 85-91. https://doi.org/10.1145/3552466.3556531</p>
<p>This app is trying to replicate you. M Murphy, Murphy, M. (2019). This app is trying to replicate you. Quartz. https://qz.com/1698337/replika-this-app-is- trying-to-replicate-you/</p>
<p>Using deceased people's personal data. H Nakagawa, A Orita, 10.1007/s00146-022-01549-1AI &amp; SocietyNakagawa, H. &amp; Orita, A. (2022). Using deceased people's personal data. AI &amp; Society. doi:10.1007/s00146- 022-01549-1.</p>
<p>Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic. The Times. B Perrigo, Perrigo, B. (2023). Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic. The Times. https://time.com/6247678/openai-chatgpt-kenya-workers</p>
<p>Sept 06). A.I. Is Making It Easier Than Ever for Students to Cheat. A Peritz, Peritz, A. (2022, Sept 06). A.I. Is Making It Easier Than Ever for Students to Cheat. Slate. https://slate.com/technology/2022/09/ai-students-writing-cheating-sudowrite.html</p>
<p>K Rajnerowicz, ·Human vs. AI Test: Can We Tell the Difference Anymore? Statistics &amp; Tech Data Library. Rajnerowicz, K. (2022).·Human vs. AI Test: Can We Tell the Difference Anymore? Statistics &amp; Tech Data Library. https://www.tidio.com/blog/ai-test</p>
<p>Is Google's LaMDA artificial intelligence sentient? Wrong question. M Roberts, The Washington Post. Roberts, M. (2022). Is Google's LaMDA artificial intelligence sentient? Wrong question. The Washington Post. https://www.washingtonpost.com/opinions/2022/06/14/google-lamda-artificial-intelligence-sentient- wrong-question</p>
<p>The US Copyright Office says an AI can't copyright its art. The Verge. A Robertson, Robertson, A. (2022). The US Copyright Office says an AI can't copyright its art. The Verge. https://www.theverge.com/2022/2/21/22944335/us-copyright-office-reject-ai-generated-art-recent- entrance-to-paradise</p>
<p>Chomsky vs. S Rodriguez, Rodriguez, S. (2022). Chomsky vs. Chomsky. http://opendoclab.mit.edu/presents/ch-vs-ch-prologue-sandra- rodriguez</p>
<p>The new Bing is acting all weird and creepy -but the human response is way scarier. A Rogers, Rogers, A. (2023). The new Bing is acting all weird and creepy -but the human response is way scarier. Insider. https://www.businessinsider.com/weird-bing-chatbot-google-chatgpt-alive-conscious-sentient- ethics-2023-2</p>
<p>A Primer in BERTology: What we know about how BERT works. A Rogers, O Kovaleva, A Rumshisky, 10.48550/arXiv.2002.12327Rogers, A., Kovaleva, O., Rumshisky, A. (2020). A Primer in BERTology: What we know about how BERT works. doi:10.48550/arXiv.2002.12327</p>
<p>The Brilliance and Weirdness of ChatGPT. The New York Times. Kevin Roose, Roose, Kevin (December 5, 2022). The Brilliance and Weirdness of ChatGPT. The New York Times.</p>
<p>GPT-3 Can Talk Like the Philosopher Daniel Dennett Without Parroting His Words. Blog post at The Splintered Mind. E Schwitzgebel, Schwitzgebel, E. (2022). GPT-3 Can Talk Like the Philosopher Daniel Dennett Without Parroting His Words. Blog post at The Splintered Mind (Nov 3).</p>
<p>Creating a Large Language Model of a Philosopher. E Schwitzgebel, 10.48550/arXiv.2302.01339preprintSchwitzgebel, E. et al. (preprint). Creating a Large Language Model of a Philosopher. doi:10.48550/arXiv.2302.01339</p>
<p>Talking About Large Language Models. M Shanahan, 10.48550/arXiv.2212.03551Shanahan, M. (2023). Talking About Large Language Models. doi:10.48550/arXiv.2212.03551.</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, 10.1038/nature16961Nature. 529Silver, D., Huang, A. et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529, 484-489. doi:10.1038/nature16961</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, T Hubert, 10.1126/science.aar6404Science. 3626419Silver, D., Hubert, T. et al. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362 (6419), 1140-1144. doi: 10.1126/science.aar6404</p>
<p>Did a Person Write This Headline. T Simonite, Simonite, T. (2020). Did a Person Write This Headline, or a Machine? Wired. https://www.wired.com/story/ai- text-generator-gpt-3-learning-language-fitfully</p>
<p>L Sinapayen, Telling Apart AI and Humans #3: Text and humor. Sinapayen, L. (2023). Telling Apart AI and Humans #3: Text and humor https://towardsdatascience.com/telling- apart-ai-and-humans-3-text-and-humor-c13e345f4629</p>
<p>Full-on robot writing': the artificial intelligence challenge facing universities. J Sparrow, Sparrow, J. (2022, Nov 1).'Full-on robot writing': the artificial intelligence challenge facing universities. Guardian.https://www.theguardian.com/australia-news/2022/nov/19/full-on-robot-writing-the-artificial- intelligence-challenge-facing-universities</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A Shoeb, A Abid, A Fisch, . . Shaham, U , abs/2206.04615ArXiv. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A., Abid, A., Fisch, A.,..., &amp; Shaham, U. (2022). Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615.</p>
<p>A.I. Is Mastering Language. Should We Trust What It Says? The New York Times. J Steven, N Iziev, Steven, J., &amp; Iziev, N. (2022, April 15). A.I. Is Mastering Language. Should We Trust What It Says? The New York Times. https://www.nytimes.com/2022/04/15/magazine/ai-language.html</p>
<p>How far can we get in creating a digital replica of a philosopher?. A Strasser, M Crosby, E Schwitzgebel, 10.3233/FAIA220637Social Robots in Social Institutions. Proceedings of Robophilosophy 2022. Series Frontiers of AI and Its Applications. R. Hakli, P. Mäkelä, J. SeibtAmsterdamIOS Press366Strasser, A., Crosby, M. &amp; Schwitzgebel, E. (2023). How far can we get in creating a digital replica of a philosopher? In R. Hakli, P. Mäkelä, J. Seibt (eds.), Social Robots in Social Institutions. Proceedings of Robophilosophy 2022. Series Frontiers of AI and Its Applications, 366, 371-380. IOS Press, Amsterdam. doi:10.3233/FAIA220637</p>
<p>From tool use to social interactions. A Strasser, 10.1515/9783839462652-004Social robotics and the good life. Bielefeld: transcript Verlag. J. Loh &amp; W. LohStrasser, A. (2022). From tool use to social interactions. In J. Loh &amp; W. Loh (Ed.), Social robotics and the good life. Bielefeld: transcript Verlag. doi:10.1515/9783839462652-004</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A S Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, 10.48550/arXiv.2211.09085Galactica: A Large Language Model for Science. Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A.S., Saravia, E., Poulton, A., Kerkez, V., &amp; Stojnic, R. (2022). Galactica: A Large Language Model for Science. doi:10.48550/arXiv.2211.09085</p>
<p>The Google engineer who thinks the company's AI has come to life. T Tiku, The Washington Post. Tiku, T. (2022, June 11). The Google engineer who thinks the company's AI has come to life. The Washington Post. https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine</p>
<p>Breakthroughs of the Year. The Atlantic. D Thompson, Thompson, D. (2022). Breakthroughs of the Year. The Atlantic. https://www.theatlantic.com/newsletters/archive/2022/12/technology-medicine-law-ai-10- breakthroughs-2022/672390</p>
<p>LaMDA-Language Models for Dialog Applications. Thoppilan, 10.48550/arXiv.2201.08239Thoppilan et al (2022). LaMDA-Language Models for Dialog Applications. doi:10.48550/arXiv.2201.08239</p>
<p>ChatGPT is fun, but not an author. H Thorp, 10.1126/science.adg7879Science. 3796630Thorp, H. (2023). ChatGPT is fun, but not an author. Science, 379(6630), 313-313. doi:10.1126/science.adg7879</p>
<p>Top AI conference bans use of ChatGPT and AI language tools to write academic papers. The Verge. J Vincent, Vincent, J. (2022). Top AI conference bans use of ChatGPT and AI language tools to write academic papers. The Verge. https://www.theverge.com/2023/1/5/23540291/chatgpt-ai-writing-tool-banned-writing- academic-icml-paper</p>
<p>Bot or Not: Can You Tell What is Human or Machine Written Text?. W Vota, Vota, W. (2020). Bot or Not: Can You Tell What is Human or Machine Written Text? https://www.ictworks.org/bot-or-not-human-machine-written/#.Y9VO9hN_oRU</p>
<p>Ethical and social risks of harm from Language Models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P Huang, I Gabriel, 10.48550/arXiv.2112.04359Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P., …, Gabriel, I. (2021). Ethical and social risks of harm from Language Models. doi:10.48550/arXiv.2112.04359</p>
<p>Philosophers On GPT-3 (updated with replies by GPT-3). Daily Nous. J Weinberg, Weinberg, J. (ed.) (2020). Philosophers On GPT-3 (updated with replies by GPT-3). Daily Nous. https://dailynous.com/2020/07/30/philosophers-gpt-3</p>
<p>OpenAI's attempts to watermark AI text hit limits. K Wiggers, Wiggers, K. (2022). OpenAI's attempts to watermark AI text hit limits. TechCrunch. https://techcrunch.com/2022/12/10/openais-attempts-to-watermark-ai-text-hit-limits</p>
<p>What Is ChatGPT Doing … and Why Does It Work. S Wolfram, Wolfram, S. (2023). What Is ChatGPT Doing … and Why Does It Work. https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work</p>
<p>Embodied artificial agents for understanding human social cognition. A Wykowska, T Chaminade, G Cheng, 10.1098/rstb.2015.0375Philosophical transactions of the Royal Society of London. Series B, Biological sciences. 371Wykowska, A.; Chaminade, T.; Cheng, G. (2016). Embodied artificial agents for understanding human social cognition. Philosophical transactions of the Royal Society of London. Series B, Biological sciences 371 (1693), 20150375. doi:10.1098/rstb.2015.0375</p>            </div>
        </div>

    </div>
</body>
</html>
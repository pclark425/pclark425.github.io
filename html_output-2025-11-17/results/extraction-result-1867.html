<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1867 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1867</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1867</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-35.html">extraction-schema-35</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <p><strong>Paper ID:</strong> paper-279057001</p>
                <p><strong>Paper Title:</strong> Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization</p>
                <p><strong>Paper Abstract:</strong> In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1867.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1867.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADMET-AI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADMET-AI (Swanson et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A machine-learning platform used to compute multiple ADMET-related molecular property scores (e.g., hERG, CARC, MUT, DILI, HIA, AMP, QED) for large chemical libraries; used in this paper to annotate molecules and to evaluate optimization outcomes as proxy ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Admet-ai: a machine learning admet platform for evaluation of large-scale chemical libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ADMET-AI property prediction tool</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>drug discovery / cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Predicts multiple pharmacologically relevant molecular properties (hERG inhibition, carcinogenicity, mutagenicity, drug-induced liver injury, human intestinal absorption (HIA), PAMPA/AMP, QED, etc.) using data-driven ML models; these predicted scalar scores are used to (a) label molecules as 'near-optimal' or 'sub-optimal' versus thresholds Θ_p and (b) compute evaluation metrics (Success Rate, RI, SR_Θ).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>data-driven ML</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Experimental wet-lab assays corresponding to each property (e.g., PAMPA/AMP permeability assays, in vitro hERG patch-clamp assays, mutagenicity/carcinogenicity bioassays, clinical DILI reports). The paper did not perform wet-lab experimental validation; experimental datasets are noted to be scarce (e.g., <2,000 measured BBBP values publically available).</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Not reported quantitatively in this paper (authors state ADMET-AI 'demonstrate strong alignment with experimentally measured values' and are 'top-performing' on TDC benchmarks, but no R²/MAE/FP/FN numbers vs experiments are provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Described qualitatively as 'rank among the top-performing predictors in the Therapeutics Data Commons (TDC) benchmark' (no numeric CV accuracy/R² reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Used to annotate both training (pairs from Chen et al., 2021 dataset: 256K pairs) and test pools (250K ZINC molecules sampled); the paper evaluates model generalization on in-distribution (IND) and out-of-distribution (OOD) tasks but does not quantify predictor reliability vs experimental values as a function of extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Not discussed for ADMET-AI predictions in relation to experimental breakthroughs; ADMET-AI is treated as a practical surrogate enabling large-scale dataset construction (incremental utility emphasized).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td>No uncertainty estimates or calibration metrics for ADMET-AI predictions are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>No bias correction or recalibration methods against experimental labels are described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td>The paper notes experimental datasets are limited and therefore empirical predictors are necessary; no temporal trend analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Authors note that experimentally validated measurements are severely limited for many properties (e.g., BBBP) and that predictors are therefore used as practical surrogates; domain factors (scarcity of wet-lab data) affect reliability but no numeric effect size provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td>ADMET-AI was used for most properties; DRD2 and PlogP are computed using the official implementations from You et al. (2018) (separate predictor) — the paper does not present systematic head-to-head numerical comparisons of these predictors vs experiments within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>ADMET-AI scores were computed for the dataset used to build C-MuMOInstruct: training source contains 256K molecule pairs (used to select up to 100 pairs per task), C-MuMOInstruct contains 28,266 tasks and 331,586 molecules referenced in Table 1; test pool: 250K sampled ZINC molecules; evaluation uses 500 test molecules per of 10 representative property combinations (5,000 test cases).</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>Paper notes experimental assays are scarce/expensive and justifies use of computational predictors as necessary for scalable dataset creation; no detailed cost numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td>None reported where ADMET-AI predictions were validated experimentally within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Paper explicitly states these predictors are not experimentally validated in this work and acknowledges potential inaccuracies and that they may not always reflect exact experimental data; recommends future work to incorporate experimentally validated datasets or wet-lab feedback.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1867.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1867.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>You2018-DRD2/PlogP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Official implementation from You et al. (2018) used for DRD2 and Penalized LogP (PlogP) prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An implementation (from You et al., 2018) used to compute DRD2 inhibition scores and PlogP values for molecules; these predictions are used as part of the multi-property annotations and evaluation criteria in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>You et al. (2018) DRD2 and PlogP predictors (official implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>drug discovery / cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Computational predictors for DRD2 inhibition probability/score and Penalized LogP (PlogP) (a composite proxy for lipophilicity, synthetic accessibility and ring complexity). These predicted scalar scores are used to determine whether properties meet pharmaceutically relevant thresholds Θ_p and to compute optimization success.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>data-driven ML / empirical computational implementation (as provided by prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Experimental DRD2 binding assays (e.g., binding affinity assays) and experimentally measured logP values; not performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Not reported in this paper; no R²/MAE or FP/FN rates vs experimental DRD2 or measured logP are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Not numerically reported here; paper states these implementations are 'well-established' and used by prior works but does not provide cross-validation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Used across IND and OOD evaluation tasks; however, no systematic analysis of predictor degradation with chemical novelty vs experimental truth is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>No characterization relative to known vs novel discoveries with respect to experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td>No calibration or uncertainty estimates provided in this paper for these predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>No recalibration to experimental data is described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Paper notes DRD2 and PlogP are standard, well-studied properties and thus practical to predict computationally; however, no numeric statements about prediction difficulty vs other properties.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td>These predictors are used alongside ADMET-AI predictors for other properties; the paper does not provide per-predictor error correlations or comparative reliability statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Predictions computed for the same dataset sizes as ADMET-AI: 256K molecule pairs for training pair selection, and evaluation across ~5,000 test inputs (500 per of 10 property combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>Not discussed beyond the general note that computational predictors enable scalable dataset creation while experimental data are scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td>None reported in-paper with experimental confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>The paper cautions that these predictors, while standard, are not experimental ground truth and may introduce inaccuracies; experimental validation is recommended for downstream use.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1867.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1867.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C-MuMOInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>C-MuMOInstruct (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuning dataset (28,266 tasks) designed for controllable multi-property, multi-objective molecule optimization: tasks are labeled using computational property predictors and property-specific thresholds; evaluation uses computed property scores as surrogate ground-truth to compute Success Rate and other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>C-MuMOInstruct dataset and evaluation pipeline (proxy-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>drug discovery / machine learning for chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Task success and model performance are judged using computationally predicted molecular properties (ADMET-AI and You et al. predictors). Primary surrogate evaluation metrics: Success Rate (SR) — proportion of inputs with at least one generated candidate satisfying all property constraints (improve sub-optimal properties by ≥ Δ_p and maintain near-optimal within Δ_p), Strict Success Rate (SR_Θ) — stricter variant requiring properties to exceed pharmaceutically relevant thresholds Θ_p, Relative Improvement (RI) — average relative gain across sub-optimal properties; Similarity (Sim), Validity, Novelty, Synthetic Accessibility Score (SAS), Average Property Score (APS) are also computed from predicted property values.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>data-driven ML (ensemble of property predictors used as surrogates)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Wet-lab experimental assays (not conducted in this paper); authors explicitly state the evaluation is based on computational predictors and lack experimental ground-truth validation, and recommend future wet-lab validation or incorporation of experimental datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>No direct proxy-vs-experiment gap metrics provided. The paper provides many quantitative comparisons between models using proxy metrics (e.g., SR, RI, SR_Θ), but does not quantify predictor agreement with experimental measurements (no R²/MAE vs wet-lab).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Detailed proxy-based model performance is reported; examples include: GeLLM 4 O-C models achieve up to 126% higher SR and 143% higher RI than baselines on IND tasks; generalist GeLLM 4 O-C-P(10) Mistral reported average SR of 63% across OOD tasks; many per-task SR, RI, Sim values are tabulated across IND and OOD (see Tables 3, 4, A2, A3...).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>The dataset and evaluation explicitly separate IND (in-distribution) and OOD (out-of-distribution) tasks; evaluation reports that generalist models maintain strong 0-shot generalization to OOD tasks (e.g., outperform baselines by 27% on average for OOD, and by 35% SR / 76% RI in a particular comparison). Quantitative novelty: test construction used 500 molecules for each of 10 representative property combinations (5,000 test molecules); OOD tasks number 68 tasks vs 51 IND tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>While the paper reports model performance differences between IND and OOD settings (generalist models degrade less than baselines), it does not present any quantitative analysis of how the proxy-to-experimental gap changes with novelty/extrapolation distance.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>The paper frames improvements as enabling more realistic, controllable multi-property optimization (incremental methodological advance); it reports larger relative gains on challenging, specialized tasks (e.g., up to 126% SR increase) but does not classify discoveries as incremental vs transformational with respect to experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td>No per-prediction uncertainty estimates or calibration analyses are provided for the surrogate property predictions or for LLM-generated candidates relative to predictor uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>No bias-correction or multi-fidelity calibration to experimental measurements is implemented; authors note this as a limitation and suggest future incorporation of wet-lab feedback or experimental datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td>Not discussed beyond noting current predictors are well-established and necessary due to scarcity of experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Authors note domain-specific constraints: some properties (e.g., BBBP) have very limited experimental measurements (<2,000 public values), and multi-property trade-offs and conflicting objectives make single-step optimization difficult; these factors limit how well computational proxies might reflect true experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td>The evaluation pipeline uses multiple predictors (ADMET-AI for many properties and You et al. implementations for DRD2/PlogP); the paper reports model-level outcomes aggregated over these predicted properties but does not provide a per-predictor error-correlation analysis or compare predictors' agreement patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>C-MuMOInstruct: 28,266 tasks; constructed from 256K molecule pairs (source Chen et al., 2021); training examples limited to at most 100 pairs per task; evaluation test pool: 250K ZINC molecules sampled, from which 500 molecules per of 10 representative property combinations were selected (5,000 test samples total across the 10 combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>Paper states computational prediction enables scalable dataset creation and evaluation at far lower cost than experimental assays; experimental data scarcity and high cost motivate use of computational surrogates, but no explicit dollar/time costs are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td>The paper reports that under the stricter SR_Θ metric (requiring properties to exceed Θ_p), the performance gap between GeLLM 4 O-Cs and baselines becomes even more pronounced (e.g., up to 218% SR and 313% RI improvements), suggesting some cases where proxy-driven optimization reaches high predicted thresholds; however, no experimental confirmation of these exceptional proxy successes is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Paper explicitly lists reliance on computational predictors as a limitation: predictors may introduce inaccuracies and may not reflect experimental data, and single-step optimization may be insufficient to reach pharmaceutically meaningful thresholds; authors call for future work to incorporate wet-lab validation or intermediate reward signals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1867.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1867.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeLLM-4O-C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeLLM 4 O-C family (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of instruction-tuned large language models (specialist and generalist variants) trained on C-MuMOInstruct to perform controllable multi-property, multi-objective molecule optimization; evaluated using computational property predictors as surrogate ground-truth and metrics like Success Rate (SR) and Relative Improvement (RI).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>GeLLM 4 O-C instruction-tuned large language models (specialist GeLLM 4 O-C-N and generalist GeLLM 4 O-C-P(N))</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>drug discovery / molecular design with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Model outputs (SMILES strings) are judged by predicted property scores (from ADMET-AI and You et al. predictors) to compute Success Rate (SR: fraction of inputs for which at least one generated candidate satisfies all predicted property constraints), Strict Success Rate SR_Θ (requires predicted scores exceed pharmaceutically relevant thresholds Θ_p), Relative Improvement (RI) averaged across predicted sub-optimal properties, Similarity (Tanimoto) between input and generated molecules, Validity, Novelty, Synthetic Accessibility Score (SAS), and Average Property Score (APS). These computed values are treated as surrogate validation for optimization success.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>data-driven ML (LLM outputs evaluated by data-driven property predictors as surrogate objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>No experimental (wet-lab) validation of generated molecules is performed within this paper; ground truth would be in vitro / in vivo assays corresponding to the properties (e.g., hERG assay, PAMPA, DRD2 binding), but these were not conducted here.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>No numerical proxy-vs-experiment gap metrics are provided; all quantitative performance comparisons are between computational models using the same surrogate predictors (e.g., SR, RI) rather than against experimental measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Extensively reported proxy-based performance numbers. Representative figures: GeLLM 4 O-Cs 'consistently outperform strong baselines' with up to 126% higher SR and 143% higher RI on IND tasks; generalist GeLLM 4 O-C-P(10) Mistral reported average SR of 63% across OOD tasks; under stricter SR_Θ criteria generalist models outperform best baseline by up to 218% SR and 313% RI (see Tables 3, 4, A2). Per-task SR values reported across IND and OOD (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Paper evaluates both IND and OOD tasks (51 IND, 68 OOD tasks in evaluation set) and also tests generalization to unseen instructions; generalist models show strong zero-shot generalization to OOD and unseen instructions (e.g., outperform baselines by ~27% average on OOD tasks and up to 35% SR / 76% RI in certain comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>While model performance differences between IND and OOD are quantified (generalists retain performance better than baselines), there is no analysis of how the surrogate-to-experimental gap changes with novelty or extrapolation distance.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Authors describe GeLLM 4 O-C as advancing the capability to perform realistic, controllable multi-property optimizations (incremental method advance); they report large relative improvements on challenging tasks but do not claim experimentally validated transformational discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td>The LLMs do not produce per-molecule calibrated uncertainty estimates with respect to experimental outcomes in this work; no calibration analysis versus experimental labels is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>No methods (e.g., multifidelity learning or recalibration) are used to correct systematic bias between computational predictors and experimental measurements in this study; authors highlight this gap as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td>Paper compares variants (specialist vs generalist; different pre-trained LLM backbones like Mistral vs Llama) and reports relative performance gains but does not analyze temporal trends in proxy vs experimental agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Paper notes that properties differ in data availability and difficulty (e.g., BBBP experimental data scarce), and that multitask training can introduce conflicts degrading specialization for some tasks; these domain factors influence model performance as measured via proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td>Performance is aggregated across multiple predicted properties; per-property APS (average property score) is reported in tables (e.g., APS per property columns), but the paper does not analyze cross-predictor error correlations with experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>GeLLM 4 O-C models trained on C-MuMOInstruct constructed from up to 256K molecule pairs with limits of 100 pairs per task; evaluation on 5,000 test inputs (500 per of 10 property combinations) with up to 20 candidate molecules generated per input for beam-supporting models.</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>Paper motivates computational evaluation on grounds of cost and data scarcity for wet-lab experiments; generating and scoring thousands of candidates via predictors is presented as computationally tractable compared to experimental validation; explicit costs not given.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td>Under the stricter SR_Θ metric requiring predicted properties to exceed pharmaceutically relevant thresholds, GeLLM models show particularly large relative gains over baselines (e.g., up to 218% SR improvement), representing exceptional proxy-level successes; no real-world experimental corroboration provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Authors explicitly list the lack of experimental validation as a key limitation, note single-step modifications may be insufficient to reach true pharmaceutically meaningful experimental thresholds, and recommend future integration of experimental feedback or iterative refinement mechanisms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Admet-ai: a machine learning admet platform for evaluation of large-scale chemical libraries. <em>(Rating: 2)</em></li>
                <li>A deep generative model for molecule optimization via one fragment modification <em>(Rating: 2)</em></li>
                <li>Machine learning in preclinical drug discovery <em>(Rating: 2)</em></li>
                <li>MuMOInstruct <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1867",
    "paper_id": "paper-279057001",
    "extraction_schema_id": "extraction-schema-35",
    "extracted_data": [
        {
            "name_short": "ADMET-AI",
            "name_full": "ADMET-AI (Swanson et al., 2024)",
            "brief_description": "A machine-learning platform used to compute multiple ADMET-related molecular property scores (e.g., hERG, CARC, MUT, DILI, HIA, AMP, QED) for large chemical libraries; used in this paper to annotate molecules and to evaluate optimization outcomes as proxy ground-truth.",
            "citation_title": "Admet-ai: a machine learning admet platform for evaluation of large-scale chemical libraries.",
            "mention_or_use": "use",
            "system_or_method_name": "ADMET-AI property prediction tool",
            "domain": "drug discovery / cheminformatics",
            "proxy_metric_description": "Predicts multiple pharmacologically relevant molecular properties (hERG inhibition, carcinogenicity, mutagenicity, drug-induced liver injury, human intestinal absorption (HIA), PAMPA/AMP, QED, etc.) using data-driven ML models; these predicted scalar scores are used to (a) label molecules as 'near-optimal' or 'sub-optimal' versus thresholds Θ_p and (b) compute evaluation metrics (Success Rate, RI, SR_Θ).",
            "proxy_type": "data-driven ML",
            "ground_truth_description": "Experimental wet-lab assays corresponding to each property (e.g., PAMPA/AMP permeability assays, in vitro hERG patch-clamp assays, mutagenicity/carcinogenicity bioassays, clinical DILI reports). The paper did not perform wet-lab experimental validation; experimental datasets are noted to be scarce (e.g., &lt;2,000 measured BBBP values publically available).",
            "quantitative_gap_measure": "Not reported quantitatively in this paper (authors state ADMET-AI 'demonstrate strong alignment with experimentally measured values' and are 'top-performing' on TDC benchmarks, but no R²/MAE/FP/FN numbers vs experiments are provided here).",
            "proxy_performance": "Described qualitatively as 'rank among the top-performing predictors in the Therapeutics Data Commons (TDC) benchmark' (no numeric CV accuracy/R² reported in this paper).",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Used to annotate both training (pairs from Chen et al., 2021 dataset: 256K pairs) and test pools (250K ZINC molecules sampled); the paper evaluates model generalization on in-distribution (IND) and out-of-distribution (OOD) tasks but does not quantify predictor reliability vs experimental values as a function of extrapolation.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "Not discussed for ADMET-AI predictions in relation to experimental breakthroughs; ADMET-AI is treated as a practical surrogate enabling large-scale dataset construction (incremental utility emphasized).",
            "calibration_or_uncertainty": "No uncertainty estimates or calibration metrics for ADMET-AI predictions are reported in this paper.",
            "bias_correction_methods": "No bias correction or recalibration methods against experimental labels are described in this paper.",
            "temporal_or_maturity_effects": "The paper notes experimental datasets are limited and therefore empirical predictors are necessary; no temporal trend analysis is provided.",
            "domain_specific_factors": "Authors note that experimentally validated measurements are severely limited for many properties (e.g., BBBP) and that predictors are therefore used as practical surrogates; domain factors (scarcity of wet-lab data) affect reliability but no numeric effect size provided.",
            "multiple_proxy_comparison": "ADMET-AI was used for most properties; DRD2 and PlogP are computed using the official implementations from You et al. (2018) (separate predictor) — the paper does not present systematic head-to-head numerical comparisons of these predictors vs experiments within this work.",
            "sample_size": "ADMET-AI scores were computed for the dataset used to build C-MuMOInstruct: training source contains 256K molecule pairs (used to select up to 100 pairs per task), C-MuMOInstruct contains 28,266 tasks and 331,586 molecules referenced in Table 1; test pool: 250K sampled ZINC molecules; evaluation uses 500 test molecules per of 10 representative property combinations (5,000 test cases).",
            "cost_or_resource_discussion": "Paper notes experimental assays are scarce/expensive and justifies use of computational predictors as necessary for scalable dataset creation; no detailed cost numbers are provided.",
            "exceptional_cases": "None reported where ADMET-AI predictions were validated experimentally within this paper.",
            "limitations_discussion": "Paper explicitly states these predictors are not experimentally validated in this work and acknowledges potential inaccuracies and that they may not always reflect exact experimental data; recommends future work to incorporate experimentally validated datasets or wet-lab feedback.",
            "uuid": "e1867.0"
        },
        {
            "name_short": "You2018-DRD2/PlogP",
            "name_full": "Official implementation from You et al. (2018) used for DRD2 and Penalized LogP (PlogP) prediction",
            "brief_description": "An implementation (from You et al., 2018) used to compute DRD2 inhibition scores and PlogP values for molecules; these predictions are used as part of the multi-property annotations and evaluation criteria in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "You et al. (2018) DRD2 and PlogP predictors (official implementation)",
            "domain": "drug discovery / cheminformatics",
            "proxy_metric_description": "Computational predictors for DRD2 inhibition probability/score and Penalized LogP (PlogP) (a composite proxy for lipophilicity, synthetic accessibility and ring complexity). These predicted scalar scores are used to determine whether properties meet pharmaceutically relevant thresholds Θ_p and to compute optimization success.",
            "proxy_type": "data-driven ML / empirical computational implementation (as provided by prior work)",
            "ground_truth_description": "Experimental DRD2 binding assays (e.g., binding affinity assays) and experimentally measured logP values; not performed in this paper.",
            "quantitative_gap_measure": "Not reported in this paper; no R²/MAE or FP/FN rates vs experimental DRD2 or measured logP are provided here.",
            "proxy_performance": "Not numerically reported here; paper states these implementations are 'well-established' and used by prior works but does not provide cross-validation numbers.",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Used across IND and OOD evaluation tasks; however, no systematic analysis of predictor degradation with chemical novelty vs experimental truth is provided.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "No characterization relative to known vs novel discoveries with respect to experimental validation.",
            "calibration_or_uncertainty": "No calibration or uncertainty estimates provided in this paper for these predictors.",
            "bias_correction_methods": "No recalibration to experimental data is described in this paper.",
            "temporal_or_maturity_effects": "Not discussed.",
            "domain_specific_factors": "Paper notes DRD2 and PlogP are standard, well-studied properties and thus practical to predict computationally; however, no numeric statements about prediction difficulty vs other properties.",
            "multiple_proxy_comparison": "These predictors are used alongside ADMET-AI predictors for other properties; the paper does not provide per-predictor error correlations or comparative reliability statistics.",
            "sample_size": "Predictions computed for the same dataset sizes as ADMET-AI: 256K molecule pairs for training pair selection, and evaluation across ~5,000 test inputs (500 per of 10 property combinations).",
            "cost_or_resource_discussion": "Not discussed beyond the general note that computational predictors enable scalable dataset creation while experimental data are scarce.",
            "exceptional_cases": "None reported in-paper with experimental confirmation.",
            "limitations_discussion": "The paper cautions that these predictors, while standard, are not experimental ground truth and may introduce inaccuracies; experimental validation is recommended for downstream use.",
            "uuid": "e1867.1"
        },
        {
            "name_short": "C-MuMOInstruct",
            "name_full": "C-MuMOInstruct (this paper)",
            "brief_description": "A large instruction-tuning dataset (28,266 tasks) designed for controllable multi-property, multi-objective molecule optimization: tasks are labeled using computational property predictors and property-specific thresholds; evaluation uses computed property scores as surrogate ground-truth to compute Success Rate and other metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "C-MuMOInstruct dataset and evaluation pipeline (proxy-based evaluation)",
            "domain": "drug discovery / machine learning for chemistry",
            "proxy_metric_description": "Task success and model performance are judged using computationally predicted molecular properties (ADMET-AI and You et al. predictors). Primary surrogate evaluation metrics: Success Rate (SR) — proportion of inputs with at least one generated candidate satisfying all property constraints (improve sub-optimal properties by ≥ Δ_p and maintain near-optimal within Δ_p), Strict Success Rate (SR_Θ) — stricter variant requiring properties to exceed pharmaceutically relevant thresholds Θ_p, Relative Improvement (RI) — average relative gain across sub-optimal properties; Similarity (Sim), Validity, Novelty, Synthetic Accessibility Score (SAS), Average Property Score (APS) are also computed from predicted property values.",
            "proxy_type": "data-driven ML (ensemble of property predictors used as surrogates)",
            "ground_truth_description": "Wet-lab experimental assays (not conducted in this paper); authors explicitly state the evaluation is based on computational predictors and lack experimental ground-truth validation, and recommend future wet-lab validation or incorporation of experimental datasets.",
            "quantitative_gap_measure": "No direct proxy-vs-experiment gap metrics provided. The paper provides many quantitative comparisons between models using proxy metrics (e.g., SR, RI, SR_Θ), but does not quantify predictor agreement with experimental measurements (no R²/MAE vs wet-lab).",
            "proxy_performance": "Detailed proxy-based model performance is reported; examples include: GeLLM 4 O-C models achieve up to 126% higher SR and 143% higher RI than baselines on IND tasks; generalist GeLLM 4 O-C-P(10) Mistral reported average SR of 63% across OOD tasks; many per-task SR, RI, Sim values are tabulated across IND and OOD (see Tables 3, 4, A2, A3...).",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "The dataset and evaluation explicitly separate IND (in-distribution) and OOD (out-of-distribution) tasks; evaluation reports that generalist models maintain strong 0-shot generalization to OOD tasks (e.g., outperform baselines by 27% on average for OOD, and by 35% SR / 76% RI in a particular comparison). Quantitative novelty: test construction used 500 molecules for each of 10 representative property combinations (5,000 test molecules); OOD tasks number 68 tasks vs 51 IND tasks.",
            "gap_varies_with_novelty": false,
            "gap_variation_details": "While the paper reports model performance differences between IND and OOD settings (generalist models degrade less than baselines), it does not present any quantitative analysis of how the proxy-to-experimental gap changes with novelty/extrapolation distance.",
            "incremental_vs_transformational": "The paper frames improvements as enabling more realistic, controllable multi-property optimization (incremental methodological advance); it reports larger relative gains on challenging, specialized tasks (e.g., up to 126% SR increase) but does not classify discoveries as incremental vs transformational with respect to experimental validation.",
            "calibration_or_uncertainty": "No per-prediction uncertainty estimates or calibration analyses are provided for the surrogate property predictions or for LLM-generated candidates relative to predictor uncertainty.",
            "bias_correction_methods": "No bias-correction or multi-fidelity calibration to experimental measurements is implemented; authors note this as a limitation and suggest future incorporation of wet-lab feedback or experimental datasets.",
            "temporal_or_maturity_effects": "Not discussed beyond noting current predictors are well-established and necessary due to scarcity of experimental data.",
            "domain_specific_factors": "Authors note domain-specific constraints: some properties (e.g., BBBP) have very limited experimental measurements (&lt;2,000 public values), and multi-property trade-offs and conflicting objectives make single-step optimization difficult; these factors limit how well computational proxies might reflect true experimental outcomes.",
            "multiple_proxy_comparison": "The evaluation pipeline uses multiple predictors (ADMET-AI for many properties and You et al. implementations for DRD2/PlogP); the paper reports model-level outcomes aggregated over these predicted properties but does not provide a per-predictor error-correlation analysis or compare predictors' agreement patterns.",
            "sample_size": "C-MuMOInstruct: 28,266 tasks; constructed from 256K molecule pairs (source Chen et al., 2021); training examples limited to at most 100 pairs per task; evaluation test pool: 250K ZINC molecules sampled, from which 500 molecules per of 10 representative property combinations were selected (5,000 test samples total across the 10 combinations).",
            "cost_or_resource_discussion": "Paper states computational prediction enables scalable dataset creation and evaluation at far lower cost than experimental assays; experimental data scarcity and high cost motivate use of computational surrogates, but no explicit dollar/time costs are provided.",
            "exceptional_cases": "The paper reports that under the stricter SR_Θ metric (requiring properties to exceed Θ_p), the performance gap between GeLLM 4 O-Cs and baselines becomes even more pronounced (e.g., up to 218% SR and 313% RI improvements), suggesting some cases where proxy-driven optimization reaches high predicted thresholds; however, no experimental confirmation of these exceptional proxy successes is provided.",
            "limitations_discussion": "Paper explicitly lists reliance on computational predictors as a limitation: predictors may introduce inaccuracies and may not reflect experimental data, and single-step optimization may be insufficient to reach pharmaceutically meaningful thresholds; authors call for future work to incorporate wet-lab validation or intermediate reward signals.",
            "uuid": "e1867.2"
        },
        {
            "name_short": "GeLLM-4O-C",
            "name_full": "GeLLM 4 O-C family (this paper)",
            "brief_description": "A family of instruction-tuned large language models (specialist and generalist variants) trained on C-MuMOInstruct to perform controllable multi-property, multi-objective molecule optimization; evaluated using computational property predictors as surrogate ground-truth and metrics like Success Rate (SR) and Relative Improvement (RI).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "GeLLM 4 O-C instruction-tuned large language models (specialist GeLLM 4 O-C-N and generalist GeLLM 4 O-C-P(N))",
            "domain": "drug discovery / molecular design with LLMs",
            "proxy_metric_description": "Model outputs (SMILES strings) are judged by predicted property scores (from ADMET-AI and You et al. predictors) to compute Success Rate (SR: fraction of inputs for which at least one generated candidate satisfies all predicted property constraints), Strict Success Rate SR_Θ (requires predicted scores exceed pharmaceutically relevant thresholds Θ_p), Relative Improvement (RI) averaged across predicted sub-optimal properties, Similarity (Tanimoto) between input and generated molecules, Validity, Novelty, Synthetic Accessibility Score (SAS), and Average Property Score (APS). These computed values are treated as surrogate validation for optimization success.",
            "proxy_type": "data-driven ML (LLM outputs evaluated by data-driven property predictors as surrogate objectives)",
            "ground_truth_description": "No experimental (wet-lab) validation of generated molecules is performed within this paper; ground truth would be in vitro / in vivo assays corresponding to the properties (e.g., hERG assay, PAMPA, DRD2 binding), but these were not conducted here.",
            "quantitative_gap_measure": "No numerical proxy-vs-experiment gap metrics are provided; all quantitative performance comparisons are between computational models using the same surrogate predictors (e.g., SR, RI) rather than against experimental measurements.",
            "proxy_performance": "Extensively reported proxy-based performance numbers. Representative figures: GeLLM 4 O-Cs 'consistently outperform strong baselines' with up to 126% higher SR and 143% higher RI on IND tasks; generalist GeLLM 4 O-C-P(10) Mistral reported average SR of 63% across OOD tasks; under stricter SR_Θ criteria generalist models outperform best baseline by up to 218% SR and 313% RI (see Tables 3, 4, A2). Per-task SR values reported across IND and OOD (see tables).",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Paper evaluates both IND and OOD tasks (51 IND, 68 OOD tasks in evaluation set) and also tests generalization to unseen instructions; generalist models show strong zero-shot generalization to OOD and unseen instructions (e.g., outperform baselines by ~27% average on OOD tasks and up to 35% SR / 76% RI in certain comparisons).",
            "gap_varies_with_novelty": false,
            "gap_variation_details": "While model performance differences between IND and OOD are quantified (generalists retain performance better than baselines), there is no analysis of how the surrogate-to-experimental gap changes with novelty or extrapolation distance.",
            "incremental_vs_transformational": "Authors describe GeLLM 4 O-C as advancing the capability to perform realistic, controllable multi-property optimizations (incremental method advance); they report large relative improvements on challenging tasks but do not claim experimentally validated transformational discoveries.",
            "calibration_or_uncertainty": "The LLMs do not produce per-molecule calibrated uncertainty estimates with respect to experimental outcomes in this work; no calibration analysis versus experimental labels is reported.",
            "bias_correction_methods": "No methods (e.g., multifidelity learning or recalibration) are used to correct systematic bias between computational predictors and experimental measurements in this study; authors highlight this gap as future work.",
            "temporal_or_maturity_effects": "Paper compares variants (specialist vs generalist; different pre-trained LLM backbones like Mistral vs Llama) and reports relative performance gains but does not analyze temporal trends in proxy vs experimental agreement.",
            "domain_specific_factors": "Paper notes that properties differ in data availability and difficulty (e.g., BBBP experimental data scarce), and that multitask training can introduce conflicts degrading specialization for some tasks; these domain factors influence model performance as measured via proxies.",
            "multiple_proxy_comparison": "Performance is aggregated across multiple predicted properties; per-property APS (average property score) is reported in tables (e.g., APS per property columns), but the paper does not analyze cross-predictor error correlations with experimental data.",
            "sample_size": "GeLLM 4 O-C models trained on C-MuMOInstruct constructed from up to 256K molecule pairs with limits of 100 pairs per task; evaluation on 5,000 test inputs (500 per of 10 property combinations) with up to 20 candidate molecules generated per input for beam-supporting models.",
            "cost_or_resource_discussion": "Paper motivates computational evaluation on grounds of cost and data scarcity for wet-lab experiments; generating and scoring thousands of candidates via predictors is presented as computationally tractable compared to experimental validation; explicit costs not given.",
            "exceptional_cases": "Under the stricter SR_Θ metric requiring predicted properties to exceed pharmaceutically relevant thresholds, GeLLM models show particularly large relative gains over baselines (e.g., up to 218% SR improvement), representing exceptional proxy-level successes; no real-world experimental corroboration provided.",
            "limitations_discussion": "Authors explicitly list the lack of experimental validation as a key limitation, note single-step modifications may be insufficient to reach true pharmaceutically meaningful experimental thresholds, and recommend future integration of experimental feedback or iterative refinement mechanisms.",
            "uuid": "e1867.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Admet-ai: a machine learning admet platform for evaluation of large-scale chemical libraries.",
            "rating": 2
        },
        {
            "paper_title": "A deep generative model for molecule optimization via one fragment modification",
            "rating": 2
        },
        {
            "paper_title": "Machine learning in preclinical drug discovery",
            "rating": 2
        },
        {
            "paper_title": "MuMOInstruct",
            "rating": 1
        }
    ],
    "cost": 0.020178,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization
29 May 2025</p>
<p>Vishal Dey 
Department of Computer Science and Engineering
The Ohio State University
USA</p>
<p>Xiao Hu 
Department of Computer Science and Engineering
The Ohio State University
USA</p>
<p>Xia Ning 
Department of Computer Science and Engineering
The Ohio State University
USA</p>
<p>Translational Data Analytics Institute
The Ohio State University
USA</p>
<p>Department of Biomedical Informatics
The Ohio State University
USA</p>
<p>College of Pharmacy
The Ohio State University
USA</p>
<p>Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization
29 May 202535A0068E0A05897C8F87E25659353D43arXiv:2505.23987v1[cs.LG]
In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria.However, existing computational approaches and instructiontuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability.To address this, we introduce C-MuMOInstruct, the first instructiontuning dataset focused on multi-property optimization with explicit, property-specific objectives.Leveraging C-MuMOInstruct, we develop GeLLM 4 O-Cs, a series of instructiontuned LLMs that can perform targeted propertyspecific optimization.Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLM 4 O-Cs consistently outperform strong baselines, achieving up to 126% higher success rate.Notably, GeLLM 4 O-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions.This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives.</p>
<p>Introduction</p>
<p>Developing a new drug is a time-consuming and expensive process, requiring over a decade and $2 billions (Sertkaya et al., 2024).A key stage in this process is lead optimization (Nicolaou and Brown, 2013), where "hit" molecules -exhibiting promising early-stage bioactivity against drug targets -are optimized for multiple molecular properties (Nicolotti et al., 2011) critical for pharmaceutical success.In practice, this stage often requires improving specific properties up to a pharmaceutically significant level, while maintaining already desirable ones within acceptable bounds.We refer to this setting as controllable multi-property, multi-objective optimization (C-MuMO), allowing for property-specific objectives, and thus greater control over the optimization.</p>
<p>Such controllable optimization requires navigating complex trade-offs among multiple properties that are often competing or even conflicting (Niu et al., 2024).For instance, optimizing an oral antipsychotic drug requires sufficiently high bloodbrain barrier permeability (BBBP) (Pollak et al., 2018) and dopamine receptor D2 (DRD2) inhibition (Seeman, 2001) to access the central nervous system (CNS) and block dopamine receptors in the CNS (Seeman et al., 1976).Meanwhile, properties related to toxicity, such as Potassium (K + ) channel inhibition must be lowered, since excessive inhibition of K + channels in the brain (Shepard et al., 2007) can cause fatal cardiac arrythmias (Sanguinetti and Tristani-Firouzi, 2006).Additionally, properties supporting oral bioavailability, such as intestinal absorption, must be maintained if they already meet desirable levels.These trade-offs highlight the need for property-specific objectives to mimic realistic optimization tasks.</p>
<p>Most existing computational approaches (Gao et al., 2022;Jensen, 2019;You et al., 2018;Blaschke et al., 2020) cannot handle tasks with multiple objectives.Furthermore, existing approaches for multi-objective optimization (Sun et al., 2022;Kim et al., 2024;Wu et al., 2024) rely on manually curated reward functions and careful task-specific tuning -limiting their scalability and applicability to diverse tasks in practice.We refer readers to Appendix A for a detailed review of existing approaches.Recently, instruction-tuned LLMs (Dey et al., 2025), demonstrated strong performance on diverse multi-property optimization tasks.However, they only tackle tasks where all properties should be improved simultaneously.This setting fails to capture the nuanced property-specific objectives prevalent in realistic lead optimization.</p>
<p>To address these critical limitations, we in- troduce C-MuMOInstruct, the first high-quality instruction-tuning dataset designed for C-MuMO tasks involving up to 10 molecular properties.Unlike prior datasets that require all properties to improve, C-MuMOInstruct explicitly incorporates controllable property-specific objectives -specifying which properties must be improved up to a user-defined property-specific threshold, and which must be maintained within acceptable bounds.This design better reflects real-world lead optimization, where some properties reach pharmaceutically significant levels in early stages, while others require multiple iterations for further improvement.Built on C-MuMOInstruct, we introduce a family of Generalizable Large Language Models for Multi-property, Multi-Objective Controllable optimization, GeLLM 4 O-C, by instruction-tuning general-purpose LLMs.GeLLM 4 O-C is trained to handle tasks requiring selective improvement of specific properties while maintaining already desirable ones.We develop both specialist and generalist variants.Each specialist GeLLM 4 O-C is trained on a single property combination with multiple controllable multi-objective tasks.Generalist GeLLM 4 O-C is trained across diverse multiproperty combinations and multiple controllable objectives within each combination, enabling crosstask knowledge transfer.This enables a single foundational model to handle novel and diverse C-MuMO tasks without task-specific fine-tuning.</p>
<p>We evaluate our GeLLM 4 O-C models with strong general-purpose LLMs and foundational LLMs for chemistry across 5 in-distribution (IND) and 5 outof-distribution (OOD) tasks.Our results reveal several key findings: (1) All GeLLM 4 O-Cs substantially outperform state-of-the-art baselines on all IND and OOD tasks, with gains of up to 126% over the (3) Generalist GeLLM 4 O-Cs demonstrate remarkable 0-shot generalization to OOD tasks, outperforming strong baselines by 27% on average.</p>
<p>To the best of our knowledge, C-MuMOInstruct is the first large scale, high-quality instructiontuning dataset specifically focused on controllable, multi-objective optimization with up to 10 properties.Generalist GeLLM 4 O-Cs tuned on C-MuMOInstruct demonstrate strong generalization abilities, which highlights their strong potential to tackle unseen, diverse C-MuMO tasks prevalent in realistic drug design scenarios.Figure 1 presents the overall framework of GeLLM 4 O-C.Dataset, models, and code are accessible through https://github.com/ninglab/GeLLMO-C.</p>
<p>C-MuMOInstruct</p>
<p>In this paper, we introduce C-MuMOInstruct, which provides control over each property objective in multi-property optimization tasks, unlike existing datasets such as MuMOInstruct.This enables models tuned on C-MuMOInstruct to improve specific properties up to a user-defined level, while maintaining others at already desirable levels -a crucial capability that distinguishes C-MuMOInstruct from existing datasets.These key differences are highlighted in Table 1.</p>
<p>Problem Definition: A C-MuMO task is to modify a hit molecule M x into an improved lead molecule M y , via structural modifications on M x , guided by property-specific objectives -controlling which properties to be improved and the extent of such improvement.Given P molecular properties, we define a pharmaceutically relevant level, Θ p , for each property p ∈ P, Accordingly, p is considered near-optimal if its score in M x -denoted as p(M x ) -is more desirable than Θ p (represented as p(M x ) ≺ Θ p ), and sub-optimal, otherwise (represented as p(M x ) ⪰ Θ p ).The desirability of each property is determined by the intended pharmaceutical goal, where either higher or lower property scores increase the molecule's likelihood to be a successful drug candidate.For example, a higher BBBP is desired for drugs targeting the CNS to ensure their access to the brain, whereas a lower BBBP is desired for peripheral targets to prevent damage to the CNS.</p>
<p>Formally, a C-MuMO task optimizing M x to M y aims to improve all sub-optimal properties P i = {p ∈ P|p(M x ) ≺ Θ p } while maintaining all nearoptimal properties P s = {p ∈ P | p(M x ) ⪰ Θ p } such that: (1) M y remains structurally similar to M x (similarity constraint); (2) M y improves upon M x in each sub-optimal property p ∈ P i by at least a property-specific threshold, ∆ p , represented as (M x ≺ ∆p M y ) ∀p∈Pi (property improvement constraint); and (3) the absolute change from M x to M y in each near-optimal property p ∈ P s remains within ∆ p to ensure such properties with already desirable scores are maintained, represented as (M x ∼ = ∆p M y ) ∀p∈Ps (property stability constraint).</p>
<p>Design Principles</p>
<p>Following the above definition, we construct C-MuMOInstruct, the first high-quality instruction tuning dataset for C-MuMO tasks with property-specific objectives.</p>
<p>Our design of C-MuMOInstruct is based on 5 key principles:</p>
<p>(1) Real-world relevance: C-MuMO tasks are widely prevalent in real-world lead optimization, where some properties may already meet desirable levels while others require further improvement.Each optimization task in C-MuMOInstruct is carefully curated to reflect nuanced multi-property objectives encountered in real-world drug design.By combining ADMET properties (e.g., intestinal absorption, mutagenicity) with properties related to specific therapeutic endpoints (e.g., dopamine receptor and potassium channel inhibition), C-MuMOInstruct captures complex and realistic multi-property trade-offs.</p>
<p>(2) Controllable multi-property threshold-based optimization: Unlike prior datasets such as MuMOInstruct, which enforces the same objective for all properties (i.e., 'improve all' simultaneously), C-MuMOInstruct introduces propertyspecific objectives -specifying sub-optimal properties to improve and near-optimal ones to maintain -in addition to 'improve all' objectives.Such property-specific objectives enables modeling diverse multi-property trade-offs, thereby capturing more realistic optimization scenarios.Furthermore, C-MuMOInstruct introduces propertyspecific thresholds, requiring each sub-optimal property to be improved up to a level considered sufficient for pharmaceutical success.This enables models tuned on C-MuMOInstruct to learn more targeted optimization strategies and navigate nuanced multi-property trade-offs more effectively than models tuned on datasets lacking finer control.Meanwhile, learning such nuanced and controllable optimization introduces additional modeling challenges, making C-MuMOInstruct a more practical and difficult dataset than existing ones.</p>
<p>(3) Comprehensive coverage: Spanning across 10 pharmacologically relevant molecular properties, C-MuMOInstruct covers a wide range of multi-property combinations, and multi-objective tasks with property-specific objectives for each property combination.This leads to a comprehensive set of optimization tasks, better capturing the complexity of real-world drug design.</p>
<p>(4) Pairwise optimization: Following MuMOInstruct, C-MuMOInstruct is constructed from molecule pairs that satisfy similarity, property improvement, and stability constraints.This enables models to effectively associate targeted structural modifications with property changes.</p>
<p>(5) Diverse instructions: C-MuMOInstruct provides diverse natural language instructions for each task with varied phrasings.This prevents instruction-tuned LLMs from overfitting to a specific phrasing, and enables them to generalize to unseen instructions -a crucial capability in practice, where task descriptions can widely vary.</p>
<p>Overview of C-MuMOInstruct Tasks</p>
<p>C-MuMOInstruct comprises a total of 28,266 tasks, with 27,401 tasks optimizing a combination of at least 3 properties.All tasks in C-MuMOInstruct are systematically curated by combining subsets of 10 pharmacologically relevant molecular properties: (1) Penalized LogP (PlogP): representing solubility, lipophilicity, synthetic accessibility, and ring complexity -higher PlogP is typically preferred in drug candidates; (2) Quantitative Estimate of Drug-Likeness (QED): assessing overall drug-likeness by incorporating molecular weight, lipophilicity, and hydrogen bonding ability -higher QED is desired for better drug-likeness; (3) Parallel Artificial Membrane Permeability Assay (AMP): evaluating drug permeability across the cellular membrane -higher AMP indicates improved drug absorption; (4) Blood-Brain Barrier Permeability (BBBP): representing the ability of a drug to permeate the blood-brain barrier -higher BBBP is essential for CNS drugs; (5) human Intestinal Absorption (HIA): indicating the ability of a drug to be absorbed through the gastrointestinal tract -higher HIA supports effective absorption of orally administered drugs; (6) human Ether-à-go-go Related Gene inhibition (hERG): referring to the drug's ability to inhibit the human ether-à-go-go related gene, which in turn blocks the potassium channel, causing severe cardiac issues -lower hERG is necessary to reduce cardiac risks; (7) Carcinogenicity (CARC): indicating the potential of a drug to induce cancer by damaging the genome or disrupting cellular processes -lower CARC is desired for safety; (8) Mutagenicity (MUT): referring to the likelihood of a drug causing genetic mutationslower MUT scores are preferred to reduce genotoxicity; (9) Drug-induced Liver Injury (LIV): representing a drug's potential to induce liver damage (hepatotoxicity) -lower DILI is crucial to reduce toxicity; (10) Dopamine Receptor D2 Inhibition (DRD2): indicating binding affinity to dopaminergic pathways -higher DRD2 scores are desired for antipsychotic drugs targeting the DRD2 receptor.</p>
<p>We focus on these 10 properties due to their key role in determining a drug's pharmacokinetic behavior, toxicity risk, and overall drug-likeness -essential factors in real-world lead optimization.Moreover, these properties are well-studied and typically considered in existing optimization benchmarks (Gao et al., 2022;Dey et al., 2025).For eval-uation, 10 representative property combinations (Section B) with 119 multi-objective tasks are selected and grouped into 51 IND and 68 OOD tasks.(Section 2.6).These tasks can be divided into 2 categories: (1) General Drug-Likeness and Toxicity (GT): tasks focused on broadly applicable molecular properties relevant for any successful drug candidate, irrespective of the specific therapeutic endpoint.(2) Context-Specific Objectives (CS): tasks involving properties that are specific to the therapeutic end-point, such as DRD2 inhibition or tissue-specific permeability (e.g., BBBP).</p>
<p>Constructing Task-Specific Training Pairs</p>
<p>Following Algorithm A1, we construct taskspecific training pairs (M x , M y ) from the dataset curated by (Chen et al., 2021), which contains 256K molecule pairs satisfying the similarity constraint (i.e., Tanimoto similarity &gt; 0.6).Out of these pairs, we select those that satisfy all P i property improvement constraints (i.e., (M x ≺ ∆p M y ) ∀p∈P i ) and all P s property stability constraints (i.e., (M x ∼ = ∆p M y ) ∀p∈P s ) for each task optimizing sub-optimal P i properties and nearoptimal P s properties (Appendix B.1).For a given task with P properties, each property p ∈ P is considered sub-optimal or near-optimal based on Θ p (shown in Table 2) as described earlier in Section 2. These thresholds are set to the 60th percentile of all training molecules among 256K pairs, reflecting desirable scores for an optimized lead molecule.</p>
<p>Constructing Task-Specific Test Set</p>
<p>We construct a test set by randomly sampling 250K molecules from ZINC (Sterling and Irwin, 2015), a widely used subset of commercially available molecules.All sampled molecules satisfy Lipsinki's rule of 5 (Lipinski et al., 2001), and do not overlap with the training set to ensure no data leakage.This creates an initial pool of druglike molecules having some near-optimal properties with desirable scores, and some sub-optimal ones requiring further improvement.From this pool, we select a molecule M x into the test set of a task improving P i and maintaining P s properties, if M x has every property p ∈ P i worse than Θ p , and every property p ∈ P s exceeding Θ p .This selection ensures a representative test set for evaluation on diverse multi-objective tasks, given a specific property combination.Following this selection process, we randomly sample 500 molecules for each of 10 representative property combinations in evaluation.</p>
<p>Quality Control</p>
<p>We implement several quality control measures, detailed in Appendix B.2, to ensure the integrity and rigor of C-MuMOInstruct.We eliminate duplicate molecules by comparing their canonicalized SMILES representations.We compute all molecular property scores empirically using established and widely-used tools such as ADMET-AI (Swanson et al., 2024).To promote robustness in instruction following, we curate 30 distinctly phrased instructions that convey the same optimization objective using varied semantics (Appendix C).To assess LLMs' ability to generalize beyond seen instructions, we hold out one instruction per task during training and use it only during inference.</p>
<p>IND and OOD Tasks</p>
<p>To rigorously evaluate instruction-tuned LLMs on both familiar and novel optimization scenarios, we split the 10 evaluation tasks into 2 groups:</p>
<p>In  (Hansch, 1969).GeLLM 4 O-C learns to apply such targeted modifications to improve sub-optimal properties beyond user-defined thresholds specified in the task instruction.GeLLM 4 O-C also learns to preserve specified near-optimal properties by avoiding structural modifications that would otherwise lower their scores.Learning such precise modifications strategies allows for explicit control over each property with varying objectives.We develop both specialist and generalist GeLLM 4 O-Cs.Each specialist GeLLM 4 O-C, denoted as GeLLM 4 O-C-N, is fine-tuned on a single property combination of N properties, with multiple objectives in that specific combination.This enables them to learn focused modification strategies specific to observed trade-offs for that property combination.In contrast, generalist GeLLM 4 O-Cs are trained across multiple property combinations and multiple objectives in each combination.This promotes knowledge transfer of shared chemical semantics and modification strategies to tackle diverse property trade-offs with property-specific objectives.This enables generalist GeLLM 4 O-C to act as a foundational LLM capable of handling novel tasks without task-specific retraining, while offering control over unseen multi-property objectives.</p>
<p>Concretely, we develop a series of generalist GeLLM 4 O-Cs, denoted as GeLLM 4 O-C-P(N), each is jointly trained on multiple C-MuMO tasks involving diverse multi-property, multi-objective combinations with up to N properties.To train these models, we fine-tune 2 general-purpose LLMs: Mistral-7B-Instruct-v0.3 (AI, 2023) and Llama3.1-8B-Instruct(Grattafiori et al., 2024)</p>
<p>Experimental Setup</p>
<p>Baselines</p>
<p>We compare GeLLM 4 O-Cs against 2 categories of baseline models: (1) general-purpose LLMs: Mistral-7B Instruct-v0.3(AI, 2023), Llama-3.1 8B-Instruct (Touvron et al., 2023), Claude-3.5 and and (2) foundational LLMs for chemistry: a Mistral-7B fine-tuned on diverse molecular tasks (Yu et al., 2024), denoted as LlaSMol Mistral .Existing non-LLM models require substantial effort on task-specific tuning or handcrafted reward functions, making them ill-suited baselines given the scale and diversity of C-MuMOInstruct.We use few-shot prompting with only 1 in-context example for all general-purpose LLMs to balance generation quality with computational resources and expenses.For baselines that support beam-search decoding, we generate 20 candidate molecules per input using the same generation strategy as in GeLLM 4 O-C.Additional details and prompts are in Appendix D.2 and Appendix E, respectively.</p>
<p>Evaluation Metrics</p>
<p>We employ multiple evaluation metrics (detailed in Appendix D.3) to enable a comprehensive assessment.For clarity and brevity, we report results primarily using the following metrics: (1) Success Rate (SR): the proportion of input molecules successfully optimized, such that all sub-optimal prop-erties are improved, and all near-optimal ones are maintained within their corresponding ∆ p -reflecting the model's ability to follow property-specific objectives;</p>
<p>(2) Similarity with input (Sim): the average Tanimoto similarity (Bajusz et al., 2015) between the optimized and corresponding input molecule; (3) Relative Improvement (RI): the relative improvement averaged across all sub-optimal properties.Higher SR, Sim, and RI are preferred, denoting more successful and effective optimizations.In Appendix G, we report results with a stricter notion of success, via SR Θ , measuring success only if each property in the task exceeds Θ p .</p>
<p>Experimental Results</p>
<p>Main Findings: The key findings are summarized as: (1) Both specialist and generalist GeLLM 4 O-Cs consistently surpass general-purpose LLMs and foundational LLMs for chemistry across all IND (Section 5.1) and OOD tasks (Section 5.2), achieving up to 126% higher SR and 143% higher RI.(2) Generalist GeLLM 4 O-Cs outperform specialist GeLLM 4 O-Cs on 4 out of 5 IND combinations, with 26% more successful optimizations on challenging tasks, such as DHMQ (Section 5.1).(3) Generalist GeLLM 4 O-Cs demonstrate remarkable 0-shot generalization to OOD tasks, surpassing the best general-purpose LLMs by 35% in SR and 76% in RI (Section 5.2).(4) Generalist GeLLM 4 O-Cs exhibit strong generalization when prompted with unseen instructions across all IND tasks (Section 5.3).</p>
<p>IND Tasks</p>
<p>Table 3 presents the performance comparison of GeLLM 4 O-Cs and baselines across all IND tasks.Detailed task-specific results are in Appendix G.1.</p>
<p>Overall Comparison: Across all IND tasks, all specialist and generalist GeLLM 4 O-Cs consistently outperform all baselines.Notably, the generalist GeLLM 4 O-C-P(10) Mistral outperforms the best baseline by 37% and 102% in SR and RI on average, indicating its superior ability as a foundational model to perform targeted modification across diverse C-MuMO tasks.On two challenging tasks, BDPQ and DHMQ, with a specific therapeutic endpoint (DRD2 inhibition), both specialist and generalist GeLLM 4 O-Cs successfully optimize as much as 150% and 126% more input molecules than the baselines, with even 1-fold better RI.Such strong performance demonstrates the ability of GeLLM 4 O-Cs to tackle complex property trade-offs.↑ and ↓ indicate whether a higher or lower metric is preferred, respectively.For each task, the best-performing model is in bold, and the best baseline is underlined.Impv-Spec and Impv-Gen represent the percentage improvement from the best specialist LLM and best generalist LLM over the best baseline , respectively.The best model in each group is selected based on SR for each task.
↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ General-
Furthermore, when evaluated under the stricter success criteria (via SR Θ ) -which requires each property to exceed pharmaceutically relevant thresholds (i.e., Θ p ) -the performance gap between GeLLM 4 O-Cs and baselines becomes even more pronounced.Table A2 demonstrates that generalist GeLLM 4 O-Cs outperform the best baseline by as much as 218% in SR and 313% in RI.This highlights the ability of GeLLM 4 O-Cs to not only optimize more molecules, but also to improve each desired property up to significant levels.Comparison between specialist and generalist GeLLM 4 O-C: Table 3 demonstrates that generalist GeLLM 4 O-Cs outperform specialist ones on 4 out of 5 IND combinations, with particularly large gains on the challenging DHMQ tasks.This trend is prominent in tasks with fewer task-specific training pairs, such as BPQ, ELQ, and DHMQ, where generalist models outperform specialist ones by up to 26% in SR.Limited training pairs in these tasks hinder the specialist models to learn robust modification strategies.In contrast, generalist ones benefit from transferable knowledge of property trade-offs and learn optimization strategies from other diverse multi-property, multi-objective training tasks.</p>
<p>Interestingly, in the BDPQ tasks, despite having only 895 pairs, GeLLM 4 O-C-N Mistral outperforms all generalist ones.The generalist variant, GeLLM 4 O-C-P(N), -trained only on tasks involving BBBP, DRD2, PlogP and QED -remains competitive due to its focused training on these specific properties.In contrast, GeLLM 4 O-C-P(10)trained on all possible property combinations involving up to 10 properties -performs worse than GeLLM 4 O-C-P(N) and specialist GeLLM 4 O-C.This could be due to GeLLM 4 O-C-P(10) encountering tasks with competing or conflicting objectives, which weakens its ability to specialize in BDPQspecific trade-offs.This highlights a key challenge in developing foundational models: while multitask tuning promotes cross-task knowledge transfer, it may also introduce conflicts that negatively impact performance on specialized tasks (e.g., BDPQ).Comparison with general-purpose LLMs: Table 3 shows that all GeLLM 4 O-Cs consistently outperform all general-purpose LLMs across all IND tasks, achieving up to 109% higher SR than the best general-purpose LLM, Mistral (1-shot).This strong performance gap underscores the benefit of instruction tuning on molecule pairs, which enables GeLLM 4 O-Cs to learn robust and effective modification strategies that are difficult for generalpurpose LLMs to learn through in-context examples alone.Moreover, general-purpose LLMs exhibit lower RI among the limited successfully optimized molecules, compared to GeLLM 4 O-Cs.This demonstrates the ability of GeLLM 4 O-Cs to perform more targeted modifications to yield substantial   Given that our models are fine-tuned on generalpurpose open-source LLMs, they may still retain knowledge about toxic substructures or chemicals from the broader pretraining corpus.While our instruction-tuning encourages models to generate molecules with more pharmaceutically desirable profiles, we cannot fully eliminate the possibility of generating undesirable molecules if misused or prompted adversarially.
↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ General-</p>
<p>Generalizability to Unseen Instructions</p>
<p>We strongly discourage any application of GeLLM 4 O-Cs outside responsible drug discovery research.Deployment of these models should be accompanied by toxicity screening, expert review, and strong usage controls.We expect all users of our dataset and models to uphold the highest standards of ethical research and to take appropriate precautions to prevent unintended consequences.</p>
<p>A Related Work</p>
<p>Computational approaches have primarily focused on single-or double-property optimization tasks (You et al., 2018;Blaschke et al., 2020;Xie et al., 2021;Bung et al., 2022;Sun et al., 2022).</p>
<p>Graph-based methods such as Modof (Chen et al., 2021), MIMOSA (Fu et al., 2021), and f-RAG (Lee et al., 2024) perform substructure modifications on molecular graphs, while sequence-based methods like Chemformer (Irwin et al., 2022) and Prompt-MolOpt (Wu et al., 2024), formulate optimization as translation tasks over SMILES strings.Genetic algorithm-based methods, GraphGA (Jensen, 2019) and MolLeo (Wang et al., 2025) can optimize multiple properties but generate entirely new molecular scaffolds, limiting their practical utility.Furthermore, existing methods (Jensen, 2019;Wang et al., 2025;Kim et al., 2024;Yang et al., 2021), require task-specific fine-tuning and expert-curated reward functions to model multi-property trade-offs, limiting their scalability and applicability.</p>
<p>Recently, LLMs have demonstrated great promise for molecule optimization through natural language instructions (Chang et al., 2024).ChatDrug (Liu et al., 2024) and Re3DF (Le and Chawla, 2024) adopt multi-turn dialogue frameworks for iterative optimization.However, their reliance on closed-source APIs leads to high costs.DrugAssist (Ye et al., 2025) developed taskspecific instruction-tuned LLMs limited to optimization tasks with up to 2 properties.Dey et al. (2025) introduced MuMOInstruct -a large-scale instruction-tuning dataset specifically focused on multi-property optimization tasks involving 3 or more properties -and further demonstrated the remarkable generalization abilities of instructiontuned LLMs.However, MuMOInstruct does not provide controllable property-specific objectives required to mimic realistic C-MuMO tasks.</p>
<p>B Details on C-MuMOInstruct</p>
<p>B.1 Details on Task Construction</p>
<p>Algorithm A1 presents a pseudocode for constructing all valid C-MuMO tasks with all possible property combinations involving up to P properties, given a molecule pair (M x , M y ).To construct C-MuMOInstruct, we run Algorithm A1 on a random sample of 100K molecule pairs sourced from Chen et al. (2021).To create training pairs for a given combination with N properties, we select only those tasks out of all C-MuMO tasks that have all N properties involved.For example, to create task-specific training pairs for BDPQ, we select only tasks that involve all 4 properties:
T BDPQ = {t = (M x , M y , C i , C s ) ∈ T | (C i ∪ C s ) =
P} where P = {BBBP, DRD2, PlogP and QED}.</p>
<p>We use at most 100 molecule pairs for each C-MuMO task (i.e., a unique property combination with explicit property-specific objectives) to balance efficiency and task diversity.Given that C-MuMOInstruct contains over 28K such tasks, training a generalist model with all possible pairs would be computationally prohibitive and may overemphasize overrepresented tasks.Limiting the number of examples per task ensures that the instruction-tuned model is exposed to a broad spectrum of multi-property trade-offs without biasing toward specific tasks.This design supports better generalization across diverse optimization objectives while keeping training tractable.</p>
<p>B.2 Details on Quality Control</p>
<p>To ensure a high-quality instruction-tuning dataset, we applied a series of quality control procedures.</p>
<p>Molecule Deduplication and Canonicalization:</p>
<p>All molecules in C-MuMOInstruct are represented using canonical SMILES strings (Weininger, 1988), standardized via RDKit (rdk, 2025).We remove molecules with identical canonicalized SMILES that are structurally equivalent, thereby eliminating redundancy and ensuring that each molecule appears only once.</p>
<p>Empirical</p>
<p>Property Computation: C-MuMOInstruct uses computationally predicted scores to annotate each molecule with 10 pharmacologically relevant molecular properties.These scores are computed using well-established, high-performing tools widely used in the molecular machine learning community.Specifically, we adopt the official implementation from You et al. (2018) for computing DRD2 and PlogP scores, and leverage the ADMET-AI tool (Swanson et al., 2024) to compute all other properties.These tools rank among the top-performing predictors in the Therapeutics Data Commons (TDC) benchmark (Catacutan et al., 2024), and have been extensively validated and adopted in recent studies (Wei et al., 2024;Thomas et al., 2024;Wahnou et al., 2024;Dey et al., 2025;Averly et al., 2025)
C s ← C ∩ P s // Identify near-optimal subset ; Construct task t = (M x , M y , C i , C s )
// An optimization task ; T ← T ∪ {t} return T enabling the construction of high-quality datasets with broad coverage of chemical space.</p>
<p>While these predictors are not experimentally validated, they demonstrate strong alignment with experimentally measured values and are widely accepted as practical surrogates in virtual screening pipelines.Notably, experimentally validated measurements are severely limited for many key pharmacological properties.For instance, public datasets contain fewer than 2,000 experimentally measured BBBP values -orders of magnitude below what is needed to train large-scale deep learning models or instruction-tuned LLMs.Given these constraints, the use of empirical predictors is not only standard but necessary for enabling scalable dataset creation and evaluation.</p>
<p>Instruction Diversity and Generation:</p>
<p>To avoid LLM overfitting to specific phrasings and to promote generalization to natural word variations in task formulation, we ensure that each optimization task is associated with a diverse set of instructions.Starting from a manually written seed prompt, we use GPT-4o (OpenAI, 2024) to generate several paraphrased variants that preserve the semantic intent while differing in structure and wording.From these, we select 30 semantically equivalent but syntactically diverse instructions per task to include in the training data.</p>
<p>To explicitly assess the models' ability to generalize to new instructions, we hold out one instruction per task as unseen during instruction-tuning.This unseen instruction is then used during evaluation to measure robustness to novel phrasings.This design allows us to evaluate not only tasklevel generalization but also linguistic flexibility in following diverse natural language instructions.All instructions used in training and testing are provided in Appendix C.</p>
<p>B.3 Details on IND Tasks</p>
<p>C Diverse Instructions</p>
<p>Figure A1 presents the prompt template used for instruction-tuning.Each prompt has three parts: (1) '{general instruction}', (2) input source molecule and properties to adjust for the specific optimization task, and ( 3) target optimized molecule.The '{general instruction}' will be replaced with one of 6 diverse task instructions, which are presented below.The first instruction is manually written, and is provided as the seed instruction to GPT-4o to generate 5 more differently phrased instructions.The last one is the hold-out instruction for inference.Below are 6 diverse instructions: In the 2nd part of the prompt template, multiple properties to be adjusted are described via the taskspecific '{adjust_i}' (Figure A1).Each '{adjust_i}' is randomly replaced with one of the following 5 adjustment templates for each sub-optimal property improvement:</p>
<ol>
<li>"change property to be direction <THRESH-OLD> value </THRESHOLD>", 2. "change the value of property to be direction <THRESHOLD> value </THRESHOLD>", 3. "change property aiming for direction <THRESHOLD> value </THRESHOLD>", 4. "change property so it is direction <THRESH-OLD> value </THRESHOLD>", 5. "change property with a goal of direction <THRESHOLD> value </THRESHOLD>" Thus, 6 diverse general instruction templates and 5 diverse adjustment templates together lead to 30 different templates for instruction tuning.</li>
</ol>
<p>Property Names: We used the following names for each property where the former is used during instruction-tuning and the latter is used for evaluation in the unseen instruction setting.For other evaluation settings, we used the same property name as used in tuning.</p>
<ol>
<li>
<p>AMP: "membrane permeability", "Parallel Artificial Membrane Permeability (PAMPA)"</p>
</li>
<li>
<p>BBBP: "BBB permeability", "Blood-brain barrier permeability (BBBP)"</p>
</li>
<li>
<p>CARC: "carcinogenicity", "potential to disrupt cellular metabolic processes" 4. DRD2: "DRD2 inhibition", "inhibition probability of Dopamine receptor D2"' 5. "hERG": "hERG inhibition", "potential to block hERG channel",</p>
</li>
</ol>
<p>D.2 Baselines</p>
<p>In this section, we detailed the baselines selected for our comparison.Table A1 lists the sources and licenses of all the source datasets and models (i.e., artifacts) used in this work.We ensured that all artifacts were utilized in accordance with the usage guidelines specified by their original authors or licensors.For the models we developed, we have considered relevant ethical implications, which are discussed in Section 9.</p>
<p>General-purpose LLMs:</p>
<p>We benchmark 4 publicly available general-purpose LLMs, including 2 open-weights LLMs: Mistral-7B Instruct-v0.3(AI, 2023), Llama-3.1 8B-Instruct (Touvron et al., 2023), and 2 closed-weights LLMs: Claude-3.5, and GPT-4o to assess their performance in molecule optimization tasks.For open-weights LLMs, we utilize their official HuggingFace checkpoints, while for closed-weights ones, we access the checkpoints via their official APIs.</p>
<p>We perform 0-shot and 1-shot inference (i.e., with 0 and 1 in-context examples, respectively) using the prompt templates, detailed in Appendix E.1.While few-shot prompting can improve performance, we selected 1-shot as a practical trade-off to control inference cost, especially for closed-sourced API-based models.Moreover, we found negligible performance improvement using 5-shots in our preliminary experiments.We generate up to 20 molecules per input molecule using the same generation strategy for open-source LLMs as in GeLLM 4 O-Cs.Since Claude and GPT do not support the beam-search decoding strategy or any customized strategy for multiple sequence generations, we generate only one molecule per input prompt.</p>
<p>Foundational LLMs for Chemistry: We adopt LlaSMol Mistral , the Mistral-7B variant of LlaSMol, as the foundational LLM for chemistry due to its strong performance across diverse molecular tasks.In comparison to other instruction-tuned LLMs for chemistry, such as ChemDFM (Zhao et al., 2025), MolInst (Fang et al., 2024) and ChemLLM (Zhang et al., 2024), LlaSMol Mistral consistently achieves state-ofthe-art results.For evaluation, we adopt 0-shot inference.Our preliminary experiments indicated that incorporating in-context examples did not lead to consistent improvements, rather impacted performance.Furthermore, we employ a simplified prompt format (as shown in Appendix E.2) after observing that LlaSMol struggles to follow more complex and structured instruction formats.For ChemDFM, we use 0-shot inference using the same prompt template and generation configuration as of general-purpose LLMs.</p>
<p>Non-LLM Domain-expert Methods: Existing non-LLM methods (Fu et al., 2021;Sun et al., 2022;Angelo et al., 2023;Kim et al., 2024) rely on genetic algorithms or reinforcement learning.These methods typically require carefully curated fitness or reward functions to balance multiple properties.Such functions are often difficult to design and require significant domain expertise, limiting their flexibility and generalizability.</p>
<p>Furthermore, these methods follow a fundamentally different experimental setting: given an initial pool of candidates, these methods iteratively modify molecules based on oracle feedback.This often leads to generating molecules with entirely new scaffolds.In contrast, our setting closely aligns with lead optimization in drug discovery, where the goal is to minimally modify an input molecule while preserving its core scaffold.</p>
<p>D.3 Evaluation Metrics</p>
<p>We adopt multiple evaluation metrics to comprehensively assess model performance.The metrics are defined as follows:</p>
<ol>
<li>
<p>Success Rate (SR): SR denotes the proportion of test cases where at least one of the 20 generated candidate molecules satisfies all specified property objectives -i.e., improving all sub-optimal properties while preserving all nearoptimal ones.When multiple candidates are optimized, the molecule exhibiting the highest cumulative improvement is selected for evaluation.A higher SR reflects the model's effectiveness in achieving task-specific optimization goals.</p>
</li>
<li>
<p>Strict Success Rate (SR Θ ): SR Θ -a stricter variant of SR -measures the proportion of test cases where at least one generated molecule not only improves all sub-optimal properties but also brings each of them above the pharmaceutically relevant threshold Θ p , while still preserving all near-optimal properties within their respective ∆ p bounds.This metric reflects whether the model can generate molecules with desirable properties as specified.</p>
</li>
</ol>
<p>Validity (Val):</p>
<p>Validity refers to the percentage of test instances for which at least one of the generated molecules is chemically valid, determined via successful parsing by RDKit.High Valensures the model's ability to generate syntactically correct and chemically valid structures.</p>
<ol>
<li>
<p>Similarity (Sim): Sim measures the average Tanimoto similarity between optimized and input molecules based on binary Morgan fingerprints (with radius of 2 and dimension of 2048).Higher Sim indicates better preservation of the similarity constraint -a key requirement in lead optimization, where maintaining the core molecular scaffold is essential.</p>
</li>
<li>
<p>Novelty (Nov): Novelty quantifies the fraction of optimized molecules that are not present in the training set.This indicates the model's ability to generate novel and previously unseen drug candidates, crucial for exploration in drug discovery pipelines.</p>
</li>
<li>
<p>Synthetic Accessibility Score (SAS): SAS evaluates how easy a molecule is to synthesize, with scores ranging from 1 (easily synthesizable) to 10 (difficult to synthesize) (Ertl and Schuffenhauer, 2009a).Lower scores indicate simpler, more synthesizable molecules.</p>
</li>
<li>
<p>Relative Improvement (RI): RI is computed as the average relative gain in each sub-optimal property compared to the input molecule.This metric reflects the magnitude of property-level improvements achieved by the model.Formally, for a task improving P i properties, RI is computed as the average of relative change (RI p ) in each property p ∈ P i as:
RI = p∈Pi RI p |P i | ,
where RI p is computed as:
RI p = D<a href="p(M y ) − p(M x )">p</a> p(M x ) ,
where D[p] is an indicator function denoting whether higher scores of p is desirable, p(M x ) and p(M y ) denote the score of property p in the input molecule M x and generated molecule M y , respectively.</p>
</li>
<li>
<p>Average Property Score (APS): APS is computed as the average property score for each molecular property across all successfully optimized molecules.Higher or lower APS, depending on the desired direction for each property, indicates the model consistently generates better molecules with property scores aligned with pharmaceutical objectives.</p>
</li>
</ol>
<p>E Prompt Templates</p>
<p>The prompt templates for general-purpose LLMs and for LlaSMol are provided below.</p>
<p>E.1 Prompt Template for General-purpose LLMs</p>
<p>We use a structured and detailed prompt template with a system prompt, task instruction, and incontext examples for few-shot prompting.Figure A2 shows an example.</p>
<p>E.2 Prompt Template for LlaSMol</p>
<p>Unlike general-purpose language models, LlaSMol was instruction-tuned on a range of chemistry-specific tasks using a dedicated prompt structure.In our preliminary experiments, we found that applying the general-purpose prompt format led to suboptimal performance, as LlaSMol often failed to interpret the task correctly.To address this, we adopted a simplified prompt format that omits the system message and does not explicitly separate the instruction, input, and expected output.Additionally, we restrict our evaluation of LlaSMol to 0-shot inference only.Figure A3 illustrates the simplified prompt used for the same task as above.GeLLM 4 O-C-P(10) Mistral accomplishes this by replacing the morpholine with a para-chlorophenyl group (Figure A4a).This modification eliminates a polar heterocycle and introduces a planar, lipophilic aromatic ring bearing a chlorine atom.This leads to notable improvements in AMP (+0.29) and PlogP (+0.85), while CARC and hERG remain within acceptable ranges.The increased hydrophobicity introduced by the chlorinated aromatic ring contributes to a higher PlogP, as aromatic chlorides are known to enhance lipophilicity due to both the non-polar nature of the phenyl group and the electron-withdrawing effect of chlorine (Hansch et al., 1995).The rigid aromatic system may reduce the molecule's conformational flexibility, which in turn lowers conformational entropy.This structural constraint can limit the number of unintended binding interactions, thereby reducing the likelihood of off-target liabilities (Meanwell, 2011b(Meanwell, , 2016) ) LlaSMol Mistral 's modification replaces the morpholine with a pyrrolidine ring.This change maintains a basic nitrogen atom but removes the oxygen, slightly reducing polarity compared to morpholine.Although this approach achieves a moderate PlogP improvement (+0.63), it shows a concerning increase in hERG liability (+0.16).The pyrrolidine ring, while structurally similar to morpholine (Figure A4b), introduces greater basicity and conformational flexibility.These properties are known risk factors for hERG channel binding in medicinal chemistry, explaining the less favorable safety profile (Cavalli et al., 2002).hit molecule is a symmetric tri-amide structure, composed of three carbonyl linkers connecting aromatic and aliphatic moieties.The goal is to im-prove BBBP, while keeping AMP, MUT, and PlogP stable.</p>
<p>F Case Studies</p>
<p>GeLLM 4 O-C-P(10) Mistral introduces a substantial simplification by collapsing the tri-amide backbone into a more compact structure containing a single central amide and two substituted aromatic rings (Figure A5a).This transformation removes several polar functional groups and incorporates lipophilic features such as methyl and aryl substitutions.These changes are well-aligned with medicinal chemistry strategies for enhancing membrane permeability -primarily through increased lipophilicity and reduced polarity (Meanwell, 2011a;Leeson and Springthorpe, 2007).As a result, GeLLM 4 O-C-P(10) Mistral achieves a favorable outcome, yielding a significant improvement in BBBP (+0.15), along with a modest increase in PlogP (+0.19), while keeping AMP and MUT values stable.</p>
<p>In contrast, LlaSMol Mistral applies a conservative modification by retaining the tri-amide scaffold and appending an isopropyl group to the lefthand side of the molecule (Figure A5b).This change preserves the molecule's original polarity and structural complexity, while introducing additional steric bulk.Crucially, it fails to reduce polarity or increase hydrophobicity -both essential for maintaining or improving PlogP (Ertl and Schuffenhauer, 2009b).As a result, despite a small gain in BBBP (+0.11), the model suffers a substantial drop in PlogP (-0.46) and an increase in toxicity (MUT), indicating an unfavorable optimization outcome.</p>
<p>G Complete Experimental Results</p>
<p>G.1 IND Evaluation</p>
<p>Tables A3, A4, A5, A6 and A7 presents the performance comparison of GeLLM 4 O-Cs with general-purpose LLMs and LlaSMol Mistral under all evaluation metrics for each IND task.</p>
<p>Table A2 presents the overall performance comparison of GeLLM 4 O-Cs with all baselines under the strict success criteria.This requires each suboptimal property to exceed its predefined pharmaceutically relevant threshold, Θ p , in the optimized molecule.We use Θ p to reflect realistic drug design objectives, where each property is expected to reach a clinically meaningful level.However, this is a highly challenging setting, particularly because our evaluation involves only a single-step molecule modification.Starting molecules may be significantly sub-optimal, and a single structural change may not be sufficient to reach such high thresholds.This explains the significantly lower success rates for all models compared to the looser success criteria in Table 3.</p>
<p>G.2 OOD Evaluation</p>
<p>Tables A8, A9, A10, A11 and A12 presents the performance comparison of GeLLM 4 O-Cs with generalpurpose LLMs and LlaSMol Mistral under all evaluation metrics for each OOD task.</p>
<p>G.3 IND Evaluation with Unseen Instructions</p>
<p>Table A13 presents the overall performance comparison of specialist and generalist GeLLM 4 O-Cs when evaluated with seen and unseen instructions.The metrics, notations, and formatting have the same meanings as those in Table A3.The metrics, notations, and formatting have the same meanings as those in Table A3.The metrics, notations, and formatting have the same meanings as those in Table A3.The metrics, notations, and formatting have the same meanings as those in Table A3.The metrics, notations, and formatting have the same meanings as those in Table A3.The metrics, notations, and formatting have the same meanings as those in Table A3.The metrics, notations, and formatting have the same meanings as those in Table A3.</p>
<p>Figure 1 :
1
Figure 1: Overview of C-MuMOInstruct and GeLLM 4 O-C</p>
<p>F. 1
1
Figure A4a and Figure A4b show optimization examples generated by GeLLM 4 O-C-P(10) Mistral and LlaSMol Mistral on the IND task ACEP.The hit molecule features a central urea scaffold with a carboxamide and a morpholine ring.The goal is to improve AMP and PlogP while maintaining CARC and hERG.</p>
<p>&lt;&lt; SYS &gt; &gt; You are an expert medicinal chemist specializing in molecular optimization .You understand how structural modifications affect key ADMET properties and inhibitions of common receptor targets like DRD2 .&lt; &lt;/ SYS &gt; &gt; [ INST ] Your task is to modify the given molecule to adjust specific molecular properties while keeping structural changes as minimal as possible .Use the examples ( if provided ) as a guide .Your response should only contain a valid SMILES representation of the modified molecule enclosed with &lt; SMILES &gt; &lt;/ SMILES &gt; tag .Examples : %%% Input : &lt; SMILES &gt; O=C( Cc1cccc (<a href="= O"> N +</a>[O -]) c1 ) NC1CCN ( Cc2ccccc2 ) CC1 &lt;/ SMILES &gt; %%% Adjust : increase DRD2 inhibition with a goal of at least &lt; THRESHOLD &gt; 0.54 &lt;/ THRESHOLD &gt;, decrease Mutagenicity with a goal of at most &lt; THRESHOLD &gt; 0.1 &lt;/ THRESHOLD &gt; and increase QED aiming for at least &lt; THRESHOLD &gt; 0.89 &lt;/ THRESHOLD &gt; while keeping Intestinal adsorption unchanged .%%% Response : &lt; SMILES &gt; O=C( Cc1ccc (O) cc1 ) NC1CCN ( Cc2ccccc2 ) CC1 &lt;/ SMILES &gt; Task : %%% Input : &lt; SMILES &gt; C# Cc1ccc ( C2CC3CCC ( C2C (= O) OC ) N3C ) cc1 &lt;/ SMILES &gt; %%% Adjust : decrease Mutagenicity with a goal of at most &lt; THRESHOLD &gt; 0.2 &lt;/ THRESHOLD &gt;, increase QED with a goal of at least &lt; THRESHOLD &gt; 0.8 &lt;/ THRESHOLD &gt; and increase the value of DRD2 inhibition to be at least &lt; THRESHOLD &gt; 0.2 &lt;/ THRESHOLD &gt; while keeping Intestinal adsorption unchanged .[/ INST ] %%% Response :</p>
<p>Figure A2 :
A2
Figure A2: An example of a prompt used for general-purpose LLMs</p>
<p>Figure A3 :
A3
Figure A3: An example of a prompt used for LlaSMol</p>
<p>( a )
a
GeLLM 4 O-C-P(10)Mistral optimization (b) LlaSMolMistral optimization</p>
<p>Figure A4 :
A4
Figure A4: An example from ACEP.Modifications are highlighted in red.</p>
<p>(a) GeLLM 4 O-C-P(10)Mistral optimization (b) LlaSMolMistral optimization</p>
<p>Figure A5 :
A5
Figure A5: An example from ABMP.Modifications are highlighted in red.</p>
<p>Table 1 :
1
Comparison among instruction-tuning datasets
Multi-objective✗✗✓Threshold-based✓✗✓Realistic✗✓✓#properties5610#molecules1,595,839331,586433,166#pairs1,029,949255,174256,185#Total tasks86328,266#Tasks ≥ 3 prop04227,401#Eval ≥ 3 prop010119#IND8551#OOD0568
ComparisonMolOpt-Instructions MuMOInstruct C-MuMOInstruct (Ye et al., 2025) (Dey et al., 2025) (ours) best baselines.(2) Generalist GeLLM 4 O-Cs outperform specialist ones on 4 out of 5 IND tasks, with impressive gains of up to 26% on challenging tasks.</p>
<p>Table 2 :
2
Summary of C-MuMOInstruct Tasks for Evaluation BBBP ↑ CARC ↓ DRD2 ↑ hERG ↓ HIA ↑ LIV ↓ MUT ↓ PlogP ↑ QED ↑ and ↓ indicate whether higher or lower scores of a given property are desirable.
Properties#Pairs #Mols #Test #Tasks CatType P-Comb AMP ↑ (∆p =) 0.10.10.20.10.20.1 0.10.11.00.1(Θp =)0.80.80.20.40.30.4 0.90.21.50.9BPQ-✓------✓✓700 1,371 5007 CSELQ----✓-✓--✓700 1,376 5007 GTINDACEP✓-✓-✓---✓-1,242 2,347 50015 GTBDPQ-✓-✓----✓✓895 1,561 50013 CSDHMQ---✓-✓-✓-✓787 1,402 5009 CSCDE--✓✓✓-----516 832 5006 CSABMP✓✓-----✓✓-1,500 2,809 50015 CSOODBCMQ-✓✓----✓-✓ 1,398 2,696 50015 CSBDEQ-✓-✓✓----✓603 840 50011 CSHLMPQ-----✓✓✓✓✓ 1,800 3,329 50021 GT"P-Comb" denotes the combination of P properties with multiple objectives. "#Pairs" and "#Mols", denote the number of moleculepairs and unique molecules in training, respectively. "#Test" and "#Tasks" denote the number of test samples and multi-propertyobjectives for a specific property combination, respectively. "Cat" indicates task category. ✓indicates properties included in thetask; -indicates properties not involved.
↑</p>
<p>Table 3 :
3
Overall Performance in IND Tasks
ModelBPQELQACEPBDPQDHMQSR</p>
<p>Generalist LLMs GeLLM 4 O-C-P(N)Mistral 84.80 0.63 2.64 83.20 0.63 0.33 86.60 0.60 2.34 50.60 0.58 4.93 53.40 0.59 3.26 GeLLM 4 O-C-P(N)Llama 88.80 0.62 2.16 90.80 0.63 0.34 92.80 0.58 2.22 51.00 0.58 5.40 50.40 0.59 3.28 GeLLM 4 O-C-P(10)Mistral 89.40 0.62 2.30 88.40 0.59 0.41 74.60 0.61 1.92 48.40 0.58 5.05 52.20 0.61 2.24 GeLLM 4 O-C-P(10)Llama 79.40 0.57 2.67 79.00 0.56 0.41 72.60 0.57 2.27 42.60 0.55 5.89 41.80 0.57 3.32
purpose LLMsMistral (0-shot)28.80 0.75 1.24 21.60 0.72 0.16 26.20 0.75 1.102.40 0.72 0.494.80 0.71 0.76Llama (0-shot)33.60 0.70 0.78 16.60 0.74 0.10 17.20 0.74 0.698.80 0.72 1.676.00 0.73 1.35Claude-3.5 (0-shot)51.80 0.68 0.89 20.00 0.64 0.20 29.60 0.71 0.69 11.20 0.67 1.805.20 0.63 1.84GPT-4o (0-shot)30.20 0.72 0.55 16.60 0.72 0.10 22.20 0.74 0.524.20 0.72 3.985.80 0.72 0.88Mistral (1-shot)72.80 0.63 1.26 74.80 0.59 0.28 63.80 0.64 1.03 21.60 0.59 4.76 25.60 0.55 1.89Llama (1-shot)49.60 0.68 0.95 36.80 0.68 0.15 40.20 0.70 1.12 14.40 0.63 2.65 13.80 0.56 3.39Claude-3.5 (1-shot)61.80 0.65 1.31 29.20 0.63 0.21 32.60 0.71 1.24 15.60 0.58 3.998.40 0.65 1.38GPT-4o (1-shot)28.60 0.74 0.77 19.60 0.72 0.12 23.00 0.76 1.095.60 0.68 3.475.60 0.71 1.22Foundational LLMs for ChemistryLlaSMol-M78.20 0.64 0.92 81.40 0.62 0.28 68.60 0.66 1.00 22.60 0.68 2.22 24.80 0.62 1.44Specialist LLMsGeLLM 4 O-C-NMistral71.00 0.57 2.59 81.80 0.55 0.39 85.60 0.54 2.46 56.60 0.50 5.48 44.60 0.57 2.96GeLLM 4 O-C-NLlama84.20 0.58 2.09 85.40 0.53 0.41 88.00 0.54 2.24 43.60 0.58 4.85 35.40 0.65 2.63Impv-Spec (%)7.7 -9.4 127.24.9 -14.5 46.428.3 -18.2 124.0 150.4 -26.5 146.874.2 3.6 56.6Impv-Gen (%)14.3 -3.1 150.011.5 1.6 21.435.3 -12.1 122.0 125.7 -14.7 143.2 108.6 7.3 72.5</p>
<p>Table 4 :
4
Overall Performance in OOD Tasks
ModelCDEABMPBCMQBDEQHLMPQSR</p>
<p>The metrics, notations and formatting have the same meanings as those in Table3.
purpose LLMsMistral (0-shot)3.00 0.73 1.33 23.00 0.77 0.93 25.40 0.69 0.253.00 0.71 1.05 11.60 0.79 1.76Llama (0-shot)6.80 0.68 0.77 44.60 0.71 0.61 20.40 0.72 0.202.20 0.68 0.60 20.20 0.72 0.68Claude-3.5 (0-shot)6.80 0.70 1.07 43.60 0.70 0.80 30.00 0.64 0.264.80 0.62 0.57 21.00 0.66 0.59GPT-4o (0-shot)3.80 0.74 1.56 27.00 0.73 0.51 19.60 0.72 0.193.40 0.71 0.42 12.80 0.72 0.47Mistral (1-shot)30.60 0.62 1.66 73.20 0.64 1.09 63.80 0.60 0.31 21.60 0.58 1.16 55.60 0.62 0.77Llama (1-shot)18.20 0.55 1.51 60.80 0.70 0.83 41.60 0.67 0.23 11.40 0.51 1.54 28.00 0.70 0.75Claude-3.5 (1-shot)8.40 0.66 1.09 45.20 0.64 0.87 32.40 0.61 0.307.20 0.55 1.22 25.00 0.61 0.72GPT-4o (1-shot)7.00 0.72 1.04 34.40 0.74 0.65 23.40 0.73 0.212.20 0.70 0.83 13.40 0.71 0.65Foundational LLMs for ChemistryLlaSMolMistral29.80 0.61 1.28 72.40 0.67 0.78 72.80 0.63 0.30 18.20 0.60 0.65 37.80 0.68 0.66Generalist LLMsGeLLM 4 O-C-P(10)Mistral 39.80 0.58 1.66 86.60 0.63 1.68 84.20 0.62 0.42 29.20 0.60 1.22 74.60 0.61 1.36GeLLM 4 O-C-P(10)Llama33.20 0.55 1.50 79.60 0.58 1.81 80.00 0.57 0.44 28.40 0.58 0.88 65.40 0.58 1.35Impv-Gen (%)30.1 -6.5 0.018.3 -1.6 54.115.7 -1.6 40.035.2 3.4 5.234.2 -1.6 76.65.2 OOD TasksTable 4 presents the performance of GeLLM 4 O-Csand baselines across all OOD tasks. SinceGeLLM 4 O-C-Ns and GeLLM 4 O-C-P(N) models usetask-specific pairs, they are inapplicable to OODtasks. Overall, generalist GeLLM 4 O-Cs exhibitstrong 0-shot generalization to novel C-MuMOtasks, consistently outperforming all baselines.Specifically, the best-performing generalist model,GeLLM 4 O-C-P(10) Mistral , achieves an average SRof 63% across all tasks, outperforming the best
improvements on each sub-optimal property.Comparison with foundational LLMs for chemistry: All GeLLM 4 O-Cs substantially outperform the SoTA foundational LLM for chemistry, LlaSMol Mistral , on all IND tasks.Another foundational LLM, ChemDFM, performs worse than LlaSMol (Appendix G).Notably, on BDPQ and DHMQ, GeLLM 4 O-C-P(10) Mistral achieves a 126% and 115% higher SR, respectively, with higher RI by 143% and 126%, respectively, compared to LlaSMol Mistral .While LlaSMol is instructiontuned on a broad range of molecular tasks, GeLLM 4 O-Cs are specifically instruction-tuned on different multi-property optimization tasks.This highlights the efficacy of instruction-tuning on optimization tasks to learn targeted modifications and navigate multi-property trade-offs.Appendix F presents 2 cases of such targeted modifications.baseline,Mistral(1-shot), by as much as 35% and 77% in SR and RI, respectively.These strong results demonstrate the remarkable ability of generalist GeLLM 4 O-Cs to learn transferable optimization strategies and tackle unseen controllable property-specific objectives during inference.Such generalizability is crucial in practice, where evolving therapeutic goals often introduce novel property combinations and novel objectives.</p>
<p>Table A13
A136 Conclusion In this paper, we introduced C-MuMOInstruct, the first instruction-tuning dataset enabling control-lable molecule optimization with property-specific objectives. Leveraging C-MuMOInstruct, we de-veloped GeLLM 4 O-Cs, that consistently and largely outperform strong general-purpose LLMs and foun-dational LLMs for chemistry across all IND and ity to follow property-specific instructions enables iterative optimization workflows, where molecules are refined over multiple steps based on intermedi-ate feedback -a common and necessary paradigm in real-world lead optimization. Through natural language instructions, these models can be flexi-bly adapted to a variety of drug design scenarios without extensive retraining. Such flexibility low-ers the barrier to deploying intelligent drug design pipelines, especially for researchers with limited computational or domain resources. Ultimately, such scalable and generalizable frameworks have the potential to accelerate early-stage drug devel-opment, reduce experimental burden, and democra-tize access to advanced drug design capabilities. OOD tasks. Moreover, generalist GeLLM 7 Limitations While our work represents a significant step toward controllable, multi-objective molecule optimiza-tion, several limitations remain: (1) Our current framework is designed for single-step optimization. In practice, optimizing molecules to reach pharma-ceutically meaningful thresholds for all properties may require multiple iterative modifications. De-signing a feedback mechanism for GeLLM 4 O-C or intermediate reward signal to guide iterative re-finement is non-trivial and is a direction for future work. (2) We rely on computational predictors for molecular properties. Although they are well-established and widely used, they may introduce in-accuracies and may not always reflect exact exper-lab data is a promising direction for future work. (3) Although we demonstrate strong generalization to unseen instructions, our instruction templates are still synthetically generated. Future work could ex-plore more diverse linguistic variation to test LLM robustness in truly open-ended settings. 8 Impact Statement This work presents the first instruction-tuning dataset, C-MuMOInstruct, that explicitly sup-ports property-specific objectives in multi-property molecule optimization -enabling models to se-lectively improve sub-optimal properties while pre-serving near-optimal ones. Built on this dataset, our developed instruction-tuned LLMs (GeLLM 4 O-C) represent a substantial advancement toward control-lable molecule optimization, addressing practical drug design requirements often overlooked by ex-isting approaches. GeLLM 4 O-Cs consistently outper-form both strong general-purpose LLMs and foun-dational LLMs for chemistry across challenging optimization tasks involving conflicting objectives. By demonstrating robust generalization to novel property combinations and novel multi-property constraints, GeLLM 4 O-C paves the way for scalable, general-purpose foundation LLMs that can flexibly handle diverse drug design constraints. We antici-pate that GeLLM 4 O-C will serve as a building block for future iterative LLM optimization frameworks. imental outcomes. Incorporating experimentally validated datasets or feedback to LLMs with wet-9 Ethics Statement
compares specialist GeLLM 4 O-Cs with generalist GeLLM 4 O-Cs when evaluated with a holdout instruction and property name (Appendix C).Overall, specialist GeLLM 4 O-Cs exhibit a performance drop of over 5% in SR on 2 out of 5 IND combinations.In contrast, generalist GeLLM 4 O-Cs retain consistent performance on all tasks.This indicates that generalist models -trained on more tasks and instructions -can generalize better to unseen instructions with different phrasings.Such generalizability is crucial in practice, where task instructions can vary widely.Notably, GeLLM 4 O-C-P(10) Llama demonstrates more robustness than GeLLM 4 O-C-P(10) Mistral , reflecting a reduced tendency to overfit to specific wordings. 4-Cs exhibit strong generalization to unseen tasks, outperforming baselines by 27% on average.This indicates the potential of GeLLM 4 O-C as a foundational model to tackle diverse tasks with realistic, controllable objectives reflecting real-world scenarios.Broader Impacts: The development of foundational LLMs for controllable multi-property molecule optimization represents a significant step toward AI-based molecular design tools.Their abil-Our work introduces instruction-tuning dataset, C-MuMOInstruct and GeLLM 4 O-Cs tuned on C-MuMOInstruct for multi-property molecule optimization.While C-MuMOInstruct is curated with drug-like molecule and to improve pharmaceutically relevant and desirable properties, we cannot fully guarantee the absence of harmful compounds or the potential for misuse.Notably, 4 of the 10 properties in C-MuMOInstruct -carcinogenicity, hERG inhibition, drug-induced liver injury, and mutagenicity -are directly related to drug toxicity.Our models are explicitly tuned to minimize these property scores, and thus, to improve drug safety profiles aligned with widely accepted pharmacological desirability.The objective is to generate drug-like molecules with reduced toxicity, not to increase toxicity or discover harmful compounds.</p>
<p>Khiem Le and Nitesh V Chawla.2024.Utilizing large language models in an iterative paradigm with domain feedback for molecule optimization.arXiv preprint arXiv:2410.13147.
Seul Lee, Karsten Kreis, Srimukh Prasad Veccham,Meng Liu, Danny Reidenbach, Saee Gopal Paliwal,Arash Vahdat, and Weili Nie. 2024. Molecule gener-ation with fragment retrieval augmentation. In TheThirty-eighth Annual Conference on Neural Informa-tion Processing Systems.Paul D Leeson and Brian Springthorpe. 2007. The in-fluence of drug-like concepts on decision-making inmedicinal chemistry. Nature reviews Drug discovery,6(11):881-890.Christopher A Lipinski, Franco Lombardo, Beryl WDominy, and Paul J Feeney. 2001. Experimental andcomputational approaches to estimate solubility andpermeability in drug discovery and development set-tings 1pii of original article: S0169-409x(96)00423-1. the article was originally published in advanceddrug delivery reviews 23 (1997) 3-25. 1. AdvancedDrug Delivery Reviews, 46(1-3):3-26.</p>
<p>. They provide a reliable, computationally efficient means to estimate property scores at scale, Algorithm A1: C-MuMO Task Construction from a Molecule Pair Input: Molecule pair (M x , M y ), Pharmaceutically-relevant levels {Θ p }, Improvement thresholds {∆ p }, Set of properties P Output: List of valid C-MuMO tasks T for (M x , M y ) with at most P properties Initialize T ← ∅ ; foreach p ∈ P do Compute change[p] ← p(M y ) − p(M x ) ; Set dir[p] ← (change[p] &gt; 0) if higher p is desirable, else negative ; // Identify Sub-optimal and near-optimal Properties: P i ← {p ∈ P i | abs(change)[p] &gt; ∆ p } ; P s ← {p ∈ P s | abs(change)[p] ≤ ∆ p and p(M x ) ⪰ Θ p } ; foreach property subset C ⊆ P with |C| ≥ 1 do
C i ← C ∩ P i// Identify sub-optimal subset ;if C i = ∅ thencontinue// Skip if no sub-optimal properties
if not all dir[p] in C i are the same then continue // Require improvement in all sub-optimal ones NeedSwap ← true if all dir[p] in C i are opposite of desired // Determine swap condition ; if NeedSwap then Swap M x ↔ M y // Ensure correct direction of improvement ;</p>
<p>Table A2 :
A2
Overall Performance in IND Tasks with stricter success criteria Sim↑ RI ↑ SRΘ ↑ Sim ↑ RI ↑ SRΘ ↑ Sim ↑ RI ↑ SRΘ ↑ Sim ↑ RI ↑ SRΘ ↑ Sim ↑ RI ↑
ModelBPQELQACEPBDPQDHMQSRΘ ↑ General-purpose LLMsMistral (0-shot)3.40 0.71 1.603.40 0.70 0.382.80 0.70 0.880.00--0.00--Llama (0-shot)3.80 0.69 0.392.20 0.69 0.271.00 0.71 0.530.00--0.20 0.75 3.00Claude-3.5 (0-shot)4.40 0.65 0.563.00 0.63 0.401.60 0.60 0.720.00--0.00--GPT-4o (0-shot)1.60 0.73 0.481.40 0.67 0.331.60 0.72 0.340.00--0.40 0.71 2.51Mistral (1-shot)14.20 0.53 1.45 16.20 0.57 0.49 10.20 0.54 1.313.40 0.32 18.683.40 0.39 3.87Llama (1-shot)6.40 0.63 0.624.80 0.61 0.393.00 0.63 0.470.40 0.15 18.712.20 0.28 14.00Claude-3.5 (1-shot)9.20 0.59 0.953.20 0.63 0.423.60 0.73 0.720.60 0.38 4.160.40 0.69 2.73GPT-4o (1-shot)2.60 0.70 0.452.00 0.67 0.281.20 0.73 0.250.00--1.00 0.71 2.72Foundational LLMs for ChemistryLlaSMol-M14.80 0.61 0.88 17.60 0.60 0.48 10.80 0.62 0.670.60 0.68 9.421.40 0.70 4.12ChemDFMLlama3.20 0.63 0.333.00 0.65 0.381.40 0.69 0.400.20 0.55 0.780.60 0.81 5.44Specialist LLMsGeLLM 4 O-C-NMistral25.40 0.51 2.57 28.80 0.51 0.56 28.00 0.50 4.009.40 0.35 13.246.40 0.52 9.92GeLLM 4 O-C-NLlama29.60 0.53 2.06 31.40 0.50 0.58 31.40 0.50 3.144.60 0.48 16.894.20 0.65 10.68Impv-Spec (%)100.0 -13.1 134.178.4 -16.7 20.8 190.7 -19.4 368.7 176.5 9.4 -29.188.2 33.3 156.3Generalist LLMsGeLLM 4 O-C-P(N)Mistral 27.60 0.59 2.43 23.40 0.62 0.51 31.20 0.57 3.425.40 0.55 11.309.00 0.54 11.53GeLLM 4 O-C-P(N)Llama30.60 0.57 2.15 25.60 0.60 0.51 34.40 0.55 2.776.40 0.50 19.466.80 0.60 13.35GeLLM 4 O-C-P(10)Mistral 32.60 0.59 2.32 32.00 0.57 0.55 23.40 0.58 1.883.80 0.59 13.264.80 0.64 11.14GeLLM 4 O-C-P(10)Llama32.40 0.54 2.59 27.60 0.56 0.54 25.20 0.56 3.115.00 0.51 22.705.40 0.56 13.70Impv-Gen (%)120.3 -3.3 163.681.8 -5.0 14.6 218.5 -11.3 313.488.2 56.2 4.2 164.7 38.5 197.9
↑ and ↓ indicate whether a higher or lower value of the metric is preferred, respectively.For each task, we underline the best baseline performance and highlight in bold the best performing model for each metric.Impv-Spec and Impv-Gen represent the relative percentage improvement from the best specialist LLM and best generalist LLM over the best baseline , respectively.The best model in each group is selected based on SR for each task.</p>
<p>Table A3 :
A3
Overall Performance on BPQ and ↓ indicate whether a higher or lower value of the metric is preferred, respectively.For each task, we underline the best baseline performance and highlight in bold the best performing model for each metric.Impv-Spec and Impv-Gen represent the relative percentage improvement from the best specialist LLM and best generalist LLM over the best baseline , respectively.The best model in each group is selected based on SR for each task.
ModelSR ↑Val ↑Sim ↑Nov ↑SAS ↓RI ↑APSBBBP ↑ PlogP ↑ QED ↑General-purpose LLMsMistral (0-shot)28.8085.800.75100.002.871.240.920.410.77Llama (0-shot)33.6099.000.70100.002.860.780.920.650.76Claude-3.5 (0-shot)51.8096.800.6899.612.750.890.910.700.75GPT-4o (0-shot)30.2088.000.72100.002.700.550.900.650.76Mistral (1-shot)72.8099.200.6397.532.581.260.911.070.77Llama (1-shot)49.60100.000.6899.192.710.950.910.890.75Claude-3.5 (1-shot)61.8096.600.65100.002.681.310.930.900.77GPT-4o (1-shot)28.6086.200.74100.002.760.770.900.700.76Foundational LLMs for ChemistryLlaSMolMistral78.20100.000.6499.742.650.920.910.870.77ChemDFMLlama27.0092.000.6699.262.820.650.930.680.77Specialist LLMsGeLLM 4 O-C-3Mistral71.0098.400.5798.872.452.590.931.510.79GeLLM 4 O-C-3Llama84.20100.000.5899.052.462.090.921.440.79Impv-Spec7.70.0-9.4-0.77.2127.21.165.52.6Generalist LLMsGeLLM 4 O-C-P(3)Mistral84.80100.000.6399.062.462.640.921.470.78GeLLM 4 O-C-P(3)Llama88.80100.000.6299.102.382.160.921.480.79GeLLM 4 O-C-P(10)Mistral89.4099.000.6298.432.492.300.931.390.79GeLLM 4 O-C-P(10)Llama79.4088.800.5797.482.422.670.931.560.79Impv-Gen14.3-1.0-3.1-1.36.0150.02.259.82.6
↑</p>
<p>Table A4 :
A4
Overall Performance on ELQ
ModelSR ↑Val ↑Sim ↑Nov ↑SAS ↓RI ↑APShERG ↓ LIV ↓ QED ↑General-purpose LLMsMistral (0-shot)21.6089.200.72100.002.820.160.37 0.550.77Llama (0-shot)16.6097.400.74100.002.900.100.44 0.560.80Claude-3.5 (0-shot)20.0096.400.64100.002.670.200.41 0.600.76GPT-4o (0-shot)16.6090.800.72100.002.830.100.39 0.530.74Mistral (1-shot)74.8099.800.5994.922.770.280.38 0.550.78Llama (1-shot)36.8099.400.6897.832.900.150.45 0.560.77Claude-3.5 (1-shot)29.2097.600.63100.002.730.210.48 0.580.76GPT-4o (1-shot)19.6090.000.72100.002.850.120.46 0.530.76Foundational LLMs for ChemistryLlaSMolMistral81.4099.800.6299.262.710.280.38 0.560.77ChemDFMLlama15.0091.200.68100.002.910.190.38 0.520.79Specialist LLMsGeLLM 4 O-C-3Mistral81.8099.400.5599.272.850.390.32 0.460.79GeLLM 4 O-C-3Llama85.40100.000.5399.532.870.410.29 0.460.79Impv-Spec4.90.20.3-5.946.423.7 17.92.6Generalist LLMsGeLLM 4 O-C-P(3)Mistral83.2099.800.6398.802.640.330.33 0.530.78GeLLM 4 O-C-P(3)Llama90.80100.000.6398.902.600.340.33 0.520.80GeLLM 4 O-C-P(10)Mistral88.4099.800.5999.552.640.410.29 0.500.81GeLLM 4 O-C-P(10)Llama79.0090.600.5699.492.580.410.30 0.480.81Impv-Gen11.50.21.6-0.44.121.413.27.13.9</p>
<p>Table A5 :
A5
Overall Performance on ACEP
ModelSR ↑Val ↑ Sim ↑Nov ↑ SAS ↓RI ↑APSAMP ↑ CARC ↓ hERG ↓ PlogP ↑General-purpose LLMsMistral (0-shot)26.2087.200.75 100.002.771.100.900.180.380.70Llama (0-shot)17.2098.000.74 100.002.740.690.900.200.470.76Claude-3.5 (0-shot)29.6096.200.71 100.002.780.690.910.170.380.64GPT-4o (0-shot)22.2091.400.7499.102.770.520.900.170.360.54Mistral (1-shot)63.8099.800.6495.922.561.030.920.180.430.92Llama (1-shot)40.2099.000.7098.512.641.120.920.200.460.87Claude-3.5 (1-shot)32.6096.600.71 100.002.741.240.940.160.420.60GPT-4o (1-shot)23.0088.800.76 100.002.791.090.930.170.400.63Foundational LLMs for ChemistryLlaSMolMistral68.60 100.000.6699.712.651.000.930.170.430.90ChemDFMLlama22.0093.000.72 100.002.851.030.930.160.440.84Specialist LLMsGeLLM 4 O-C-4Mistral85.60 100.000.5499.532.392.460.950.140.331.24GeLLM 4 O-C-4Llama88.0099.800.5499.552.382.240.950.140.341.25Impv-Spec28.3-0.2 -18.2-0.210.2 124.02.217.620.938.9Generalist LLMsGeLLM 4 O-C-P(4)Mistral86.60 100.000.6098.612.382.340.960.150.361.25GeLLM 4 O-C-P(4)Llama92.8099.800.5898.922.342.220.950.150.351.26GeLLM 4 O-C-P(10)Mistral 74.60 100.000.6199.202.441.920.950.130.351.11GeLLM 4 O-C-P(10)Llama72.6093.600.5798.622.382.270.960.150.381.33Impv-Gen35.3-0.2 -12.1-0.811.7 122.02.211.818.640.0</p>
<p>Table A6 :
A6
Overall Performance on BDPQ
ModelSR ↑Val ↑ Sim ↑Nov ↑ SAS ↓RI ↑APSBBBP ↑ DRD2 ↑ PlogP ↑ QED ↑General-purpose LLMsMistral (0-shot)2.4075.600.72 100.002.830.490.960.090.66 0.82Llama (0-shot)8.8097.000.72 100.003.241.670.960.060.03 0.79Claude-3.5 (0-shot)11.2096.800.67 100.002.781.800.930.090.60 0.78GPT-4o (0-shot)4.2084.800.72 100.002.923.980.930.070.51 0.82Mistral (1-shot)21.6099.200.5992.592.654.760.940.180.94 0.80Llama (1-shot)14.4099.400.6391.673.012.650.940.110.63 0.78Claude-3.5 (1-shot)15.6095.200.58 100.002.663.990.940.111.26 0.80GPT-4o (1-shot)5.6087.200.68 100.002.653.470.950.091.09 0.85Foundational LLMs for ChemistryLlaSMolMistral22.60 100.000.68 100.002.852.220.930.090.63 0.78ChemDFMLlama6.2093.000.67 100.002.853.510.920.070.64 0.80Specialist LLMsGeLLM 4 O-C-4Mistral56.60 100.000.5097.882.455.480.950.221.25 0.79GeLLM 4 O-C-4Llama43.6099.800.5899.082.524.850.950.161.14 0.79Impv-Spec150.40.0 -26.5-2.114.0 146.82.2144.498.41.3Generalist LLMsGeLLM 4 O-C-P(4)Mistral50.60 100.000.5899.212.514.930.950.171.23 0.79GeLLM 4 O-C-P(4)Llama51.00 100.000.5898.432.495.400.950.171.19 0.78GeLLM 4 O-C-P(10)Mistral 48.4099.400.5899.172.555.050.950.161.22 0.79GeLLM 4 O-C-P(10)Llama42.6088.600.5598.592.475.890.940.171.37 0.79Impv-Gen125.70.0 -14.7-1.612.6 143.22.288.988.90.0</p>
<p>Table A7 :
A7
Overall Performance on DHMQ HIA ↑ MUT ↓ QED ↑
ModelSR ↑Val ↑ Sim ↑Nov ↑ SAS ↓RI ↑APSDRD2 ↑ General-purpose LLMsMistral (0-shot)4.8086.800.71 100.002.88 0.760.05 1.000.290.80Llama (0-shot)6.0097.400.73 100.003.09 1.350.06 1.000.280.79Claude-3.5 (0-shot)5.2095.200.63 100.002.73 1.840.10 1.000.200.75GPT-4o (0-shot)5.8087.800.72 100.002.89 0.880.07 1.000.220.82Mistral (1-shot)25.6099.800.5586.722.89 1.890.18 1.000.210.78Llama (1-shot)13.8099.400.5685.513.06 3.390.18 1.000.240.79Claude-3.5 (1-shot)8.4095.200.65 100.002.77 1.380.12 1.000.210.78GPT-4o (1-shot)5.6087.400.71 100.002.78 1.220.10 1.000.220.81Foundational LLMs for ChemistryLlaSMolMistral24.80 100.000.62 100.002.93 1.440.08 0.990.200.78ChemDFMLlama6.8086.400.67 100.003.03 1.720.07 1.000.170.82Specialist LLMsGeLLM 4 O-C-4Mistral44.6099.800.5799.102.81 2.960.14 0.990.190.78GeLLM 4 O-C-4Llama35.40 100.000.65 100.002.73 2.630.12 0.990.200.79Impv-Spec74.20.03.614.32.8 56.6-22.2-1.09.50.0Generalist LLMsGeLLM 4 O-C-P(4)Mistral53.40 100.000.5999.252.76 3.260.15 0.990.190.78GeLLM 4 O-C-P(4)Llama50.40 100.000.59 100.002.67 3.280.13 0.990.190.79GeLLM 4 O-C-P(10)Mistral 52.2099.600.61 100.002.76 2.240.12 0.990.190.79GeLLM 4 O-C-P(10)Llama41.8083.200.57 100.002.65 3.320.15 0.990.200.79Impv-Gen108.60.27.314.44.5 72.5-16.7-1.09.50.0</p>
<p>Table A8 :
A8
Overall Performance on CDE
ModelSR ↑Val ↑Sim ↑Nov ↑SAS ↓RI ↑APSCARC ↓ DRD2 ↑ hERG ↓General-purpose LLMsMistral (0-shot)3.0086.000.73100.003.131.330.150.140.65Llama (0-shot)6.8096.600.68100.003.320.770.200.060.57Claude-3.5 (0-shot)6.8097.800.70100.002.981.070.160.080.52GPT-4o (0-shot)3.8089.800.74100.003.011.560.150.050.39Mistral (1-shot)30.6099.600.6293.463.001.660.150.090.50Llama (1-shot)18.2099.400.5576.923.501.510.140.120.47Claude-3.5 (1-shot)8.4098.400.66100.002.911.090.120.080.47GPT-4o (1-shot)7.0088.200.72100.003.101.040.160.050.53Foundational LLMs for ChemistryLlaSMolMistral29.8099.800.6197.992.791.280.140.060.46ChemDFMLlama8.2090.600.64100.003.160.840.170.080.53Generalist LLMsGeLLM 4 O-C-P(10)Mistral98.600.58100.002.851.660.110.080.42GeLLM 4 O-C-P(10)Llama33.2086.800.55100.002.861.500.110.080.48Impv-Gen30.1-1.0-6.57.05.00.026.7-11.116.0</p>
<p>Table A9 :
A9
Overall Performance on ABMP Model SR ↑ Val ↑ Sim ↑ Nov ↑ SAS ↓ RI ↑ APS AMP ↑ BBBP ↑ MUT ↓ PlogP ↑
General-purpose LLMsMistral (0-shot)23.0083.000.77 100.002.76 0.930.900.870.240.86Llama (0-shot)44.6098.400.71 100.002.85 0.610.920.900.251.17Claude-3.5 (0-shot)43.6096.200.70 100.002.73 0.800.950.890.240.81GPT-4o (0-shot)27.0087.400.73 100.002.72 0.510.930.890.250.93Mistral (1-shot)73.2099.600.6494.812.62 1.090.930.900.231.10Llama (1-shot)60.8099.600.7099.012.76 0.830.920.890.241.02Claude-3.5 (1-shot)45.2096.400.64 100.002.67 0.870.950.910.231.04GPT-4o (1-shot)34.4087.800.74 100.002.73 0.650.930.890.281.03Foundational LLMs for ChemistryLlaSMolMistral72.40 100.000.67 100.002.75 0.780.940.890.240.93ChemDFMLlama39.6092.400.67 100.002.95 0.980.940.890.231.40Generalist LLMsGeLLM 4 O-C-P(10)Mistral 86.6099.400.6398.852.48 1.680.950.920.201.63GeLLM 4 O-C-P(10)Llama79.6089.600.5898.992.42 1.810.960.910.191.81Impv-Gen18.3-0.2-1.64.35.3 54.12.22.213.048.2</p>
<p>Table A12 :
A12
Overall Performance on HLMPQ Model SR ↑ Val ↑ Sim ↑ Nov ↑ SAS ↓ RI ↑ APS HIA ↑ LIV ↓ MUT ↓ PlogP ↑ QED ↑
General-purpose LLMsMistral (0-shot)11.6082.400.79 100.002.91 1.760.99 0.380.200.51 0.77Llama (0-shot)20.2099.400.7298.022.82 0.681.00 0.540.230.70 0.79Claude-3.5 (0-shot)21.0097.000.6699.052.72 0.591.00 0.460.240.69 0.79GPT-4o (0-shot)12.8087.600.72 100.002.78 0.471.00 0.480.200.49 0.75Mistral (1-shot)55.6099.800.6297.122.59 0.770.99 0.540.211.08 0.77Llama (1-shot)28.0099.600.7097.862.72 0.751.00 0.560.240.83 0.78Claude-3.5 (1-shot)25.0095.000.6197.602.60 0.721.00 0.530.250.89 0.78GPT-4o (1-shot)13.4087.400.71 100.002.82 0.651.00 0.500.210.61 0.73Foundational LLMs for ChemistryLlaSMolMistral37.80 100.000.68 100.002.66 0.661.00 0.580.220.92 0.73ChemDFMLlama10.8090.600.6898.153.01 1.040.98 0.430.190.68 0.77Generalist LLMsGeLLM 4 O-C-P(10)Mistral 74.6099.800.6199.462.49 1.361.00 0.530.181.43 0.79GeLLM 4 O-C-P(10)Llama65.4090.800.5899.692.41 1.351.00 0.530.181.53 0.79Impv-Gen34.20.0-1.62.43.9 76.61.01.914.332.42.6</p>
<p>Table A13 :
A13
Overall Performance with Unseen Instructions in IND TasksSim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ SR ↑ Sim ↑ RI ↑ .572.59 81.80 0.55 0.39 85.60 0.54 2.46 56.60 0.50 5.48 44.60 0.57 2.96 unseen 68.60 0.55 2.33 84.60 0.53 0.41 86.80 0.53 2.28 59.40 0.47 5.79 49.40 0.56 3.19 .572.27 42.60 0.55 5.89 41.80 0.57 3.32 unseen 95.60 0.55 2.63 92.60 0.55 0.42 84.80 0.57 2.21 52.80 0.55 5.67 51.60 0.55 2.96 'Seen' and 'unseen' indicate whether models are evaluated using instructions included during training or entirely novel instructions, respectively.↑ and ↓ indicate whether higher or lower values of the corresponding metric are preferable.Within each row block, the best-performing model is highlighted in bold if the performance difference exceeds 5%.
ModelInstrBPQELQACEPBDPQDHMQGeLLM 4 O-CSR ↑ Specialist LLMs-NMistral 71.00 0-NLlama seen seen 84.20 0.58 2.09 85.40 0.53 0.41 unseen 74.20 0.57 2.02 88.60 0.54 0.4288.00 0.54 2.24 43.60 0.58 4.85 35.40 0.65 2.63 87.00 0.52 2.14 37.00 0.59 5.27 37.60 0.64 2.77Generalist LLMs-P(10)Mistralseen unseen 89.60 0.62 2.01 87.60 0.60 0.37 89.40 0.62 2.30 88.40 0.59 0.4174.60 0.61 1.92 48.40 0.58 5.05 52.20 0.61 2.24 78.00 0.63 1.75 46.60 0.60 4.57 50.20 0.61 2.79-P(10)Llamaseen79.40 0.57 2.67 79.00 0.56 0.4172.60 0
The metrics, notations, and formatting have the same meanings as those in TableA3.TableA11: Overall Performance on BDEQ The metrics, notations, and formatting have the same meanings as those in TableA3.
Multi-and many-objective optimization: present and future in de novo drug design. Jaqueline S Angelo, Isabella A Guedes, J C Helio, Laurent E Barbosa, Dardenne, 10.3389/fchem.2023.1288626Mistral AI. 2023. Mistral 7b. 202311arXiv preprintRdkit: Open-source cheminformatics</p>
<p>Liddia: Language-based intelligent drug discovery agent. Reza Averly, Frazier N Baker, Xia Ning, arXiv:2502.139592025Preprint</p>
<p>Why is tanimoto index an appropriate choice for fingerprint-based similarity calculations. Dávid Bajusz, Anita Rácz, Károly Héberger, 10.1186/s13321-015-0069-3Journal of Cheminformatics. 712015</p>
<p>Reinvent 2.0: an ai tool for de novo drug design. Thomas Blaschke, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, Atanas Patronov, Journal of chemical information and modeling. 60122020</p>
<p>An in silico explainable multiparameter optimization approach for de novo drug design against proteins from the central nervous system. Navneet Bung, Sowmya Ramaswamy Krishnan, Arijit Roy, Journal of Chemical Information and Modeling. 62112022</p>
<p>Machine learning in preclinical drug discovery. Denise B Catacutan, Jeremie Alexander, Autumn Arnold, Jonathan M Stokes, 10.1038/s41589-024-01679-1Nature Chemical Biology. 2082024</p>
<p>Toward a pharmacophore for drugs inducing the long qt syndrome: insights from a comfa study of herg k+ channel blockers. Andrea Cavalli, Elisabetta Poluzzi, Fabrizio De Ponti, Maurizio Recanatini, Journal of medicinal chemistry. 45182002</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289202415</p>
<p>A deep generative model for molecule optimization via one fragment modification. Ziqi Chen, Martin Renqiang Min, Srinivasan Parthasarathy, Xia Ning, Nature machine intelligence. 3122021</p>
<p>Gellmˆ3o Generalizing large language models for multiproperty molecule optimization. Vishal Dey, Xiao Hu, Xia Ning, arXiv:2502.133982025arXiv preprint</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, 10.1186/1758-2946-1-8Journal of Cheminformatics. 112009a</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, Journal of cheminformatics. 12009b</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Hua , The Twelfth International Conference on Learning Representations. jun Chen. 2024</p>
<p>Mimosa: Multi-constraint molecule sampling for molecule optimization. Tianfan Fu, Cao Xiao, Xinhao Li, Lucas M Glass, Jimeng Sun, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Sample efficiency matters: a benchmark for practical molecular optimization. Wenhao Gao, Tianfan Fu, Jimeng Sun, Connor Coley, Advances in neural information processing systems. 202235</p>
<p>Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 2 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, arXiv:2407.217832024PreprintThe llama 3 herd of models</p>
<p>Quantitative approach to biochemical structure-activity relationships. Corwin Hansch, 10.1021/ar50020a002Accounts of Chemical Research. 281969</p>
<p>Exploring QSAR: hydrophobic, electronic, and steric constants. Corwin Hansch, Albert Leo, 1995American Chemical Society2Washington, DCDavid Hoekman, and 1 others</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Jannik Bjerrum, Machine Learning: Science and Technology. 31150222022</p>
<p>A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Jan H Jensen, Chemical science. 10122019</p>
<p>Genetic-guided gflownets: Advancing in practical molecular optimization benchmark. Hyeonah Kim, Minsu Kim, Sanghyeok Choi, Jinkyoo Park, 10.48550/arXiv.2402.05961CoRR, abs/2402.059612024</p>
<p>Conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Improving drug candidates by design: a focus on physicochemical properties as a means of improving compound disposition and safety. Nicholas A Meanwell, Chemical research in toxicology. 2492011a</p>
<p>Synopsis of some recent tactical application of bioisosteres in drug design. Nicholas A Meanwell, Journal of medicinal chemistry. 5482011b</p>
<p>Improving drug design: an update on recent applications of efficiency metrics, strategies for replacing problematic elements, and compounds in nontraditional drug space. Nicholas A Meanwell, Chemical Research in Toxicology. 2942016</p>
<p>Multiobjective optimization methods in drug design. Christos A Nicolaou, Nathan Brown, 10.1016/j.ddtec.2013.02.001Drug Discovery Today: Technologies. 1032013</p>
<p>Strategies of multi-objective optimization in drug discovery and development. Orazio Nicolotti, Ilenia Giangreco, Antonellina Introcaso, Francesco Leonetti, Angela Stefanachi, Angelo Carotti, 10.1517/17460441.2011.588696Expert Opinion on Drug Discovery. 692011</p>
<p>Trading-off multiple properties for molecular optimization. Yifan Niu, Ziqi Gao, Tingyang Xu, Yatao Bian, Yu Rong, Jia Li, 2024</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2024OpenAIPreprint</p>
<p>The blood-brain barrier in psychosis. Svetlana Thomas A Pollak, James M Drndarski, Anthony S Stone, Philip David, Joan Mcguire, Abbott, 10.1016/s2215-0366(17)30293-6The Lancet Psychiatry. 512018</p>
<p>herg potassium channels and cardiac arrhythmia. C Michael, Martin Sanguinetti, Tristani-Firouzi, 10.1038/nature04710Nature. 44070832006</p>
<p>Antipsychotic drug doses and neuroleptic/dopamine receptors. P Seeman, T Lee, M Chau-Wong, K Wong, 10.1038/261717a0Nature. 26155621976</p>
<p>Antipsychotic drugs, dopamine receptors, and schizophrenia. Philip Seeman, 10.1016/S1566-2772(00)00007-4Clinical Neuroscience Research. 112001</p>
<p>Costs of drug development and research and development intensity in the us. Aylin Sertkaya, Trinidad Beleche, Amber Jessup, Benjamin D Sommers, 10.1001/jamanetworkopen.2024.15445JAMA Network Open. 762024. 2000-2018</p>
<p>Ether-a-go-go-related gene potassium channels: What's all the buzz about?. Paul D Shepard, Carmen C Canavier, Edwin S Levitan, 10.1093/schbul/sbm106Schizophrenia Bulletin. 3362007</p>
<p>Zinc 15 -ligand discovery for everyone. Teague Sterling, John J Irwin, 10.1021/acs.jcim.5b0055926479676Journal of Chemical Information and Modeling. 55112015</p>
<p>Molsearch: Searchbased multi-objective molecular generation and property optimization. Mengying Sun, Jing Xing, Han Meng, Huijun Wang, Bin Chen, Jiayu Zhou, 10.1145/3534678.3542676202222New York, NY, USAAssociation for Computing Machinery</p>
<p>Admet-ai: a machine learning admet platform for evaluation of large-scale chemical libraries. Kyle Swanson, Parker Walther, Jeremy Leitz, Souhrid Mukherjee, Joseph C Wu, Rabindra V Shivnaraine, James Zou, 10.1093/bioinformatics/btae416Bioinformatics. 407e4162024</p>
<p>Molscore: a scoring, evaluation and benchmarking framework for generative models in de novo drug design. Morgan Thomas, Noel M O'boyle, Andreas Bender, Chris De Graaf, 10.1186/s13321-024-00861-wJournal of Cheminformatics. 1612024</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 2 others. 2023. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Integrating admet, enrichment analysis, and molecular docking approach to elucidate the mechanism of artemisia herba alba for the treatment of inflammatory bowel disease-associated arthritis. Hicham Wahnou, Fouzia Hmimid, Ahmed Errami, 10.1080/15287394.2024.2379856Journal of Toxicology and Environmental Health, Part A. 87202024Imane Nait Irahal, Youness Limami, and Mounia Oudghiri</p>
<p>Efficient evolutionary search over chemical space with large language models. Haorui Wang, Marta Skreta, Cher Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Alan Aspuru-Guzik, Kirill Neklyudov, Chao Zhang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Investigation of in silico studies for cytochrome p450 isoforms specificity. Yao Wei, Luca Palazzolo, Omar Ben Mariem, Davide Bianchi, Tommaso Laurenzi, Uliano Guerrini, Ivano Eberini, 10.1016/j.csbj.2024.08.002Computational and Structural Biotechnology Journal. 232024</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, 10.1021/ci00057a005Journal of Chemical Information and Computer Sciences. 2811988</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Xu, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020Teven Le Scao, Sylvain Gugger, and 3 others</p>
<p>Leveraging language model for advanced multiproperty molecular optimization via prompt engineering. Zhenxing Wu, Odin Zhang, Xiaorui Wang, Li Fu, Huifeng Zhao, Jike Wang, Hongyan Du, Dejun Jiang, Yafeng Deng, Nature Machine Intelligence. 2024Dongsheng Cao, and 1 others</p>
<p>{MARS}: Markov molecular sampling for multi-objective drug discovery. Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, Lei Li, International Conference on Learning Representations. 2021</p>
<p>Hit and lead discovery with explorative RL and fragment-based molecule generation. Soojung Yang, Doyeong Hwang, Seul Lee, Seongok Ryu, Sung Ju Hwang, Advances in Neural Information Processing Systems. 2021</p>
<p>Drugassist: A large language model for molecule optimization. Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, Briefings in Bioinformatics. 261e6932025</p>
<p>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, Jure Leskovec, ; Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, First Conference on Language Modeling. 2018. 2024Graph convolutional policy network for goal-directed molecular graph generation</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli Ouyang, Dongzhan Zhou, Shufei Zhang, Mao Su, Han-Sen Zhong, Yuqiang Li, arXiv:2402.06852Chemllm: A chemical large language model. 2024Preprint</p>
<p>Developing chemdfm as a large language foundation model for chemistry. Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Kai Yu, Xin Chen, 10.1016/j.xcrp.2025.102523Cell Reports Physical Science. 641025232025</p>
<p>Table A1: Licenses and Sources of Artifacts Artifact Source License Type Accessibility Modof. </p>
<p>v3.0SMolInstruct Creative Commons Attribution 4.0 Checkpoint ChemDFMLlama. -8B GNU Affero General Public License</p>
<p>8B-Instruct. </p>
<p>Instruct Llama 3.1 Community Checkpoint Mistral-7B-Instruct-v0. </p>
<p>Apache license 2.0 Checkpoint. </p>            </div>
        </div>

    </div>
</body>
</html>
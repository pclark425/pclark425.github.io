<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-331 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-331</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-331</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-d5fcad8a3b183642fcf609519a4dbbda9c3541ff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d5fcad8a3b183642fcf609519a4dbbda9c3541ff" target="_blank">Learning Numeral Embedding</a></p>
                <p><strong>Paper Venue:</strong> Findings</p>
                <p><strong>Paper TL;DR:</strong> Two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals are proposed and shown its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling.</p>
                <p><strong>Paper Abstract:</strong> Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e331.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e331.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOM-num-embed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Organizing Map based Prototype Numeral Embedding (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that induces a finite set of prototype numerals via a self-organizing map (SOM) and represents any numeral's embedding as a weighted average of prototype embeddings, with weights based on similarity in (optionally log-)number space; integrated into skip-gram training so prototypes are learned jointly with word embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Skip-gram with SOM-based prototype numeral embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>skip-gram word embedding model (word2vec-style) with additional prototype embedding matrices</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>magnitude comparison / numeral-related tasks (magnitude OVA/SC/BC tests), contextual numeral prediction, order-of-magnitude classification (Numeracy-600K); NOT explicit arithmetic (addition/multiplication) tasks</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Numerals extracted from Wikipedia-1B (range includes very large numbers; paper shows numerals up to ~1e13 and mentions up to 1e15), evaluations used 2342 frequent numerals for magnitude tests and prototype counts ~200-500 for Wiki1B</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Prototype induction by SOM + optional log-space squashing (f(x) = log(x)+1 for |x|>1), weighted averaging of prototype embeddings as numeral embedding, integrated into skip-gram with negative sampling; similarity sim(p,n)=|g(p)-g(n)|^{-β} (β=1 default)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Magnitude tests (Wiki-1B): OVA 67.72%, SC 71.86%, BC 99.40%, AVGR 15.91; Numeration tests: OVA 3.54%, SC 62.83%, BC 100.00%, AVGR 28.98. Non-linear decoding (MLP) RMSE: 2724.12. Numeral prediction (Wiki-1B, scoring S_A): AVGR 381.41, MdAE 825.79, MdAPE 0.9836. Numeracy-600K (order-of-magnitude classification): AVGR 2.91, micro-F1 37.99, macro-F1 13.50. (See paper tables for full breakdown.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Encodes numeracy by mapping numeric values (optionally in log-space) to positions on a learned prototype number line; numeral embedding is a similarity-weighted average of prototype embeddings, so numerals with similar magnitudes get similar vectors; prototypes are trainable and updated via backprop during skip-gram training, enabling joint learning of numeric and lexical semantics. Log-squashing is motivated by psychological compressed (log) mental number line.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Best prototype counts for Wiki1B around 200–500; for small domain data 10–25 prototypes. Performance depends on prototype number and whether log-squashing is applied; no discussion of scaling with large neural LM parameter counts (not applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not designed to perform explicit arithmetic operations; magnitude-narrow tests show SOM less strong than 'Fixed' at linear magnitude retrieval (Fixed explicitly encodes magnitude). On contextual prediction SOM has relatively large AVGR (ranking) compared to some baselines; numeration (matching number words to digits) performance can be poor for some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GMM-based prototype method (this paper), NumAsTok (standard skip-gram treating numerals as tokens), D-LSTM digit-LSTM embedding, Fixed explicit-magnitude embedding, and the numerate language model (Spithourakis & Riedel 2018).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Representing numerals as similarity-weighted averages over learned numeric prototypes (with log squashing) produces embeddings that capture magnitude and contextual numeral prediction substantially better than naive baselines and generalize to OOV numerals, but this approach models magnitude/recognition rather than algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Numeral Embedding', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e331.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e331.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GMM-num-embed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Mixture Model based Prototype Numeral Embedding (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that fits a Gaussian mixture model to the distribution of numeric values and uses component means as prototypes; numeral embeddings are similarity-weighted averages of prototype embeddings where similarity is the posterior P(Z=k|U=n); prototypes are optimized jointly in skip-gram training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Skip-gram with GMM-based prototype numeral embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>skip-gram word embedding model (word2vec-style) with prototype embedding matrices derived from GMM component means</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>magnitude comparison / numeral-related tasks (magnitude OVA/SC/BC tests), contextual numeral prediction, order-of-magnitude classification; NOT classic arithmetic (addition/multiplication) problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Same numerals as SOM case: Wikipedia-1B numerals (wide numeric range, prototypes often 200–500 on large corpora; for experiments GMM prototypes used e.g. 300), GMM means can be initialized by SOM/k-means/random</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Prototype induction via Gaussian mixture models trained with EM or hard-EM; similarity sim(p_k,n) ∝ P(Z=k|U=n) (posterior probability). Optional log-squashing of inputs; prototypes' embeddings updated via backprop in skip-gram training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Magnitude tests (Wiki-1B): OVA 57.86%, SC 58.63%, BC 100.00%, AVGR 1.75; Numeration tests: OVA 4.42%, SC 65.49%, BC 100.00%, AVGR 25.97. Non-linear decoding (MLP) RMSE: 678.78 (substantially better RMSE than SOM and baselines). Numeral prediction (Wiki-1B, scoring S_A): AVGR 343.50, MdAE 1184.85, MdAPE 0.9450. Numeracy-600K (order-of-magnitude classification): AVGR 2.19, micro-F1 41.86, macro-F1 18.47.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>GMM induces soft clustering over the numeric line; similarity equals posterior assignment so embeddings smoothly mix nearby component embeddings; this probabilistic prototype weighting combined with log-squashing can encode magnitude non-linearly and yields better non-linear decodability (lower RMSE) than SOM in experiments, suggesting the soft GMM posterior is effective at encoding numeric magnitude structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Choice of EM type, initialization, number of components affects results (paper explores various initializations and soft vs hard EM); recommended prototype counts scale with log of distinct numerals ((log N)^2 rule). No analysis of scaling with large transformer LM sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Like SOM approach, does not perform arithmetic operations; some magnitude metrics (OVA/SC) lower than Fixed baseline which explicitly encodes magnitude; numeral prediction ranking metrics (AVGR) can still be large on large-vocabulary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared against SOM-based method, NumAsTok, D-LSTM, Fixed embedding, and numerate-LM; GMM often yields much lower RMSE in non-linear decoding compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Using GMM component means as numeric prototypes and weighting by posterior probabilities yields numeral embeddings that capture magnitude non-linearly (low MLP decoding RMSE) and improve contextual numeral prediction and downstream tasks, while remaining a representational approach rather than an arithmetic solver.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Numeral Embedding', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e331.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e331.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D-LSTM-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit-level LSTM Numeral Encoder (baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that encodes numerals by running an LSTM over the character/digit sequence of a numeral and using the final hidden state as its embedding; trained end-to-end within skip-gram objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Digit-LSTM numeral encoder + skip-gram</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>LSTM encoder for digit sequences combined with skip-gram word embedding training</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Evaluated on magnitude/numeration tasks and contextual numeral prediction (same task suite as SOM/GMM); not evaluated on explicit arithmetic (addition/multiplication) tasks</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Digit sequences include 0-9 plus characters +/-/e/' ' to represent numbers in various forms; applied to numerals in Wikipedia-1B and domain datasets (wide range of magnitudes)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Character/digit-level LSTM encoding of numerals, last hidden state used as embedding; trained jointly with skip-gram objective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Magnitude tests: OVA 7.26%, SC 51.79%, BC 92.83%, AVGR 158.82. Numeration tests: OVA 1.77%, SC 54.87%, BC 89.38%, AVGR 53.55. Non-linear decoding RMSE: 20949.66 (worse than SOM/GMM). Numeral prediction (Wiki-1B) performs reasonably but is slower and consumes more parameters (training speed ~8421.66 sent/s vs 12691–22907 for others).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Leverages sequence model over digit characters to build embeddings sensitive to digit patterns and morphology, but lacks explicit structure for magnitude (unless learned) and performs poorly on nonlinear magnitude decoding in these experiments, suggesting character LSTM struggles to generalize magnitude consistently across scales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Requires more parameters and GPU memory; slower training. No analysis on scaling to large transformer LM sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High RMSE in non-linear decoding; poor magnitude OVA and large AVGR in ranking tasks compared to prototype methods; computationally heavier than prototype methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to SOM, GMM, NumAsTok, Fixed. D-LSTM is slower and had weaker magnitude decoding performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Character/digit-LSTM can produce numeral embeddings but is less effective at encoding numeric magnitude non-linearly and is more expensive than prototype-based approaches in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Numeral Embedding', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e331.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e331.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumAsTok</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numerals-as-Tokens baseline (standard skip-gram vocabulary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline treating numerals like ordinary word tokens; the vocabulary contains in-vocab numerals and replaces OOV numerals with a single UNK_num token; standard skip-gram training applied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Standard skip-gram (numerals included in finite vocabulary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>skip-gram word embedding model (word2vec-style) with finite numeral vocabulary and UNK_num for OOV numerals</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Evaluated on magnitude/numeration and contextual numeral prediction; not designed to perform arithmetic algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Limited to numerals present in finite vocabulary; many numerals are OOV and mapped to UNK_num</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>No special numeric treatment; OOV numerals collapsed to UNK_num. Standard skip-gram negative sampling training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Magnitude tests: OVA 12.17%, SC 51.02%, BC 95.99%, AVGR 144.13. Numeration tests: OVA 7.08%, SC 61.95%, BC 99.12%, AVGR 27.08. Non-linear decoding RMSE: 14878.73. Numeral prediction and downstream sequence labeling performance degrade when numerals are OOV or scarce; NumAsTok shows worse generalizability to perturbed numerals (augmented test sets).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanism to generalize to unseen numerals; memorizes seen numerals but fails to represent continuous numeric relations, leading to poor generalization across magnitude and OOV instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance limited by vocabulary coverage; adding more numerals to vocab could improve seen-numeral performance but cannot reasonably cover infinite numeral space.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Numeral OOV problem (many numerals unseen in training), poor generalization to perturbed or rare numerals, large drops on augmented test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baseline compared against SOM/GMM prototype methods, D-LSTM, Fixed, and numerate-LM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Treating numerals as ordinary finite tokens leads to strong performance on high-frequency numerals but fails to generalize to OOV numerals and continuous magnitude relationships; prototype-based methods address this limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Numeral Embedding', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e331.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e331.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Numerate-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Gaussians Numerate Language Model (Spithourakis & Riedel 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A numerate language model that models numbers with a mixture-of-Gaussians over numeric value and integrates this into language modeling; designed to improve prediction of numbers in context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeracy for language models: Evaluating and improving their ability to predict numbers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Numerate language model (mixture-of-Gaussians over numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>language model with explicit mixture-of-Gaussians numeric component integrated into language modeling (not a large transformer in this paper's usage)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Contextual numeral prediction and numeric modeling (probabilistic modeling of numeric values); not evaluated here on arithmetic operations such as addition/subtraction/multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Model operates on numeric values (log-space transformations often used); in this paper it was evaluated on the same numeral prediction tasks (Wikipedia-1B subset) but with a smaller vocabulary (3e4) for memory reasons.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Mixture-of-Gaussians numeric modeling within LM; paper evaluated two prediction methods: default previous-context prediction and using sentence log-likelihood to rank candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On the numeral prediction task referenced, the numerate-LM achieved slightly better median absolute error (MdAE) than the prototype-embedding methods in one metric, but worse median absolute percentage error (MdAPE) and worse average rank (AVGR) when scoring candidates from a large candidate set; exact comparative numbers in the paper show mixed performance depending on scoring method and vocabulary size.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Uses an explicit probabilistic model over numeric value (GMM) to represent continuous numeric information and integrates this with language modeling probabilities, demonstrating that explicit numeric modeling helps certain numeric prediction metrics but may trade off ranking quality depending on candidate sets and vocab sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Required smaller vocabulary (3e4) in the experimental run in this paper due to memory; scaling behavior with model size not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When used with large candidate sets and for ranking candidates, it produced worse AVGR and MdAPE compared to prototype-based skip-gram methods in the reported run; memory constraints limited vocabulary size in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against prototype-based SOM/GMM embedding methods and standard skip-gram baselines in the paper's numeral prediction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Explicit probabilistic numeric modeling (mixture-of-Gaussians) can improve some numeric prediction metrics but may not uniformly outperform prototype-based embedding approaches on contextual candidate-ranking tasks, and practical deployment must manage vocabulary and memory tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Numeral Embedding', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Numeracy for language models: Evaluating and improving their ability to predict numbers <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? <em>(Rating: 2)</em></li>
                <li>Numeracy-600k: Learning numeracy for detecting exaggerated information in market comments <em>(Rating: 2)</em></li>
                <li>Exploring numeracy in word embeddings <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-331",
    "paper_id": "paper-d5fcad8a3b183642fcf609519a4dbbda9c3541ff",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "SOM-num-embed",
            "name_full": "Self-Organizing Map based Prototype Numeral Embedding (this paper)",
            "brief_description": "A method that induces a finite set of prototype numerals via a self-organizing map (SOM) and represents any numeral's embedding as a weighted average of prototype embeddings, with weights based on similarity in (optionally log-)number space; integrated into skip-gram training so prototypes are learned jointly with word embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Skip-gram with SOM-based prototype numeral embeddings",
            "model_size": null,
            "model_architecture": "skip-gram word embedding model (word2vec-style) with additional prototype embedding matrices",
            "arithmetic_operation_type": "magnitude comparison / numeral-related tasks (magnitude OVA/SC/BC tests), contextual numeral prediction, order-of-magnitude classification (Numeracy-600K); NOT explicit arithmetic (addition/multiplication) tasks",
            "number_range_or_complexity": "Numerals extracted from Wikipedia-1B (range includes very large numbers; paper shows numerals up to ~1e13 and mentions up to 1e15), evaluations used 2342 frequent numerals for magnitude tests and prototype counts ~200-500 for Wiki1B",
            "method_or_intervention": "Prototype induction by SOM + optional log-space squashing (f(x) = log(x)+1 for |x|&gt;1), weighted averaging of prototype embeddings as numeral embedding, integrated into skip-gram with negative sampling; similarity sim(p,n)=|g(p)-g(n)|^{-β} (β=1 default)",
            "performance_result": "Magnitude tests (Wiki-1B): OVA 67.72%, SC 71.86%, BC 99.40%, AVGR 15.91; Numeration tests: OVA 3.54%, SC 62.83%, BC 100.00%, AVGR 28.98. Non-linear decoding (MLP) RMSE: 2724.12. Numeral prediction (Wiki-1B, scoring S_A): AVGR 381.41, MdAE 825.79, MdAPE 0.9836. Numeracy-600K (order-of-magnitude classification): AVGR 2.91, micro-F1 37.99, macro-F1 13.50. (See paper tables for full breakdown.)",
            "mechanistic_insight": "Encodes numeracy by mapping numeric values (optionally in log-space) to positions on a learned prototype number line; numeral embedding is a similarity-weighted average of prototype embeddings, so numerals with similar magnitudes get similar vectors; prototypes are trainable and updated via backprop during skip-gram training, enabling joint learning of numeric and lexical semantics. Log-squashing is motivated by psychological compressed (log) mental number line.",
            "performance_scaling": "Best prototype counts for Wiki1B around 200–500; for small domain data 10–25 prototypes. Performance depends on prototype number and whether log-squashing is applied; no discussion of scaling with large neural LM parameter counts (not applicable).",
            "failure_modes": "Not designed to perform explicit arithmetic operations; magnitude-narrow tests show SOM less strong than 'Fixed' at linear magnitude retrieval (Fixed explicitly encodes magnitude). On contextual prediction SOM has relatively large AVGR (ranking) compared to some baselines; numeration (matching number words to digits) performance can be poor for some variants.",
            "comparison_baseline": "Compared to GMM-based prototype method (this paper), NumAsTok (standard skip-gram treating numerals as tokens), D-LSTM digit-LSTM embedding, Fixed explicit-magnitude embedding, and the numerate language model (Spithourakis & Riedel 2018).",
            "key_finding": "Representing numerals as similarity-weighted averages over learned numeric prototypes (with log squashing) produces embeddings that capture magnitude and contextual numeral prediction substantially better than naive baselines and generalize to OOV numerals, but this approach models magnitude/recognition rather than algorithmic arithmetic.",
            "uuid": "e331.0",
            "source_info": {
                "paper_title": "Learning Numeral Embedding",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "GMM-num-embed",
            "name_full": "Gaussian Mixture Model based Prototype Numeral Embedding (this paper)",
            "brief_description": "A method that fits a Gaussian mixture model to the distribution of numeric values and uses component means as prototypes; numeral embeddings are similarity-weighted averages of prototype embeddings where similarity is the posterior P(Z=k|U=n); prototypes are optimized jointly in skip-gram training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Skip-gram with GMM-based prototype numeral embeddings",
            "model_size": null,
            "model_architecture": "skip-gram word embedding model (word2vec-style) with prototype embedding matrices derived from GMM component means",
            "arithmetic_operation_type": "magnitude comparison / numeral-related tasks (magnitude OVA/SC/BC tests), contextual numeral prediction, order-of-magnitude classification; NOT classic arithmetic (addition/multiplication) problems",
            "number_range_or_complexity": "Same numerals as SOM case: Wikipedia-1B numerals (wide numeric range, prototypes often 200–500 on large corpora; for experiments GMM prototypes used e.g. 300), GMM means can be initialized by SOM/k-means/random",
            "method_or_intervention": "Prototype induction via Gaussian mixture models trained with EM or hard-EM; similarity sim(p_k,n) ∝ P(Z=k|U=n) (posterior probability). Optional log-squashing of inputs; prototypes' embeddings updated via backprop in skip-gram training.",
            "performance_result": "Magnitude tests (Wiki-1B): OVA 57.86%, SC 58.63%, BC 100.00%, AVGR 1.75; Numeration tests: OVA 4.42%, SC 65.49%, BC 100.00%, AVGR 25.97. Non-linear decoding (MLP) RMSE: 678.78 (substantially better RMSE than SOM and baselines). Numeral prediction (Wiki-1B, scoring S_A): AVGR 343.50, MdAE 1184.85, MdAPE 0.9450. Numeracy-600K (order-of-magnitude classification): AVGR 2.19, micro-F1 41.86, macro-F1 18.47.",
            "mechanistic_insight": "GMM induces soft clustering over the numeric line; similarity equals posterior assignment so embeddings smoothly mix nearby component embeddings; this probabilistic prototype weighting combined with log-squashing can encode magnitude non-linearly and yields better non-linear decodability (lower RMSE) than SOM in experiments, suggesting the soft GMM posterior is effective at encoding numeric magnitude structure.",
            "performance_scaling": "Choice of EM type, initialization, number of components affects results (paper explores various initializations and soft vs hard EM); recommended prototype counts scale with log of distinct numerals ((log N)^2 rule). No analysis of scaling with large transformer LM sizes.",
            "failure_modes": "Like SOM approach, does not perform arithmetic operations; some magnitude metrics (OVA/SC) lower than Fixed baseline which explicitly encodes magnitude; numeral prediction ranking metrics (AVGR) can still be large on large-vocabulary tasks.",
            "comparison_baseline": "Directly compared against SOM-based method, NumAsTok, D-LSTM, Fixed embedding, and numerate-LM; GMM often yields much lower RMSE in non-linear decoding compared to baselines.",
            "key_finding": "Using GMM component means as numeric prototypes and weighting by posterior probabilities yields numeral embeddings that capture magnitude non-linearly (low MLP decoding RMSE) and improve contextual numeral prediction and downstream tasks, while remaining a representational approach rather than an arithmetic solver.",
            "uuid": "e331.1",
            "source_info": {
                "paper_title": "Learning Numeral Embedding",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "D-LSTM-baseline",
            "name_full": "Digit-level LSTM Numeral Encoder (baseline in this paper)",
            "brief_description": "Baseline that encodes numerals by running an LSTM over the character/digit sequence of a numeral and using the final hidden state as its embedding; trained end-to-end within skip-gram objective.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Digit-LSTM numeral encoder + skip-gram",
            "model_size": null,
            "model_architecture": "LSTM encoder for digit sequences combined with skip-gram word embedding training",
            "arithmetic_operation_type": "Evaluated on magnitude/numeration tasks and contextual numeral prediction (same task suite as SOM/GMM); not evaluated on explicit arithmetic (addition/multiplication) tasks",
            "number_range_or_complexity": "Digit sequences include 0-9 plus characters +/-/e/' ' to represent numbers in various forms; applied to numerals in Wikipedia-1B and domain datasets (wide range of magnitudes)",
            "method_or_intervention": "Character/digit-level LSTM encoding of numerals, last hidden state used as embedding; trained jointly with skip-gram objective.",
            "performance_result": "Magnitude tests: OVA 7.26%, SC 51.79%, BC 92.83%, AVGR 158.82. Numeration tests: OVA 1.77%, SC 54.87%, BC 89.38%, AVGR 53.55. Non-linear decoding RMSE: 20949.66 (worse than SOM/GMM). Numeral prediction (Wiki-1B) performs reasonably but is slower and consumes more parameters (training speed ~8421.66 sent/s vs 12691–22907 for others).",
            "mechanistic_insight": "Leverages sequence model over digit characters to build embeddings sensitive to digit patterns and morphology, but lacks explicit structure for magnitude (unless learned) and performs poorly on nonlinear magnitude decoding in these experiments, suggesting character LSTM struggles to generalize magnitude consistently across scales.",
            "performance_scaling": "Requires more parameters and GPU memory; slower training. No analysis on scaling to large transformer LM sizes.",
            "failure_modes": "High RMSE in non-linear decoding; poor magnitude OVA and large AVGR in ranking tasks compared to prototype methods; computationally heavier than prototype methods.",
            "comparison_baseline": "Compared directly to SOM, GMM, NumAsTok, Fixed. D-LSTM is slower and had weaker magnitude decoding performance in experiments.",
            "key_finding": "Character/digit-LSTM can produce numeral embeddings but is less effective at encoding numeric magnitude non-linearly and is more expensive than prototype-based approaches in these experiments.",
            "uuid": "e331.2",
            "source_info": {
                "paper_title": "Learning Numeral Embedding",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "NumAsTok",
            "name_full": "Numerals-as-Tokens baseline (standard skip-gram vocabulary)",
            "brief_description": "Baseline treating numerals like ordinary word tokens; the vocabulary contains in-vocab numerals and replaces OOV numerals with a single UNK_num token; standard skip-gram training applied.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Standard skip-gram (numerals included in finite vocabulary)",
            "model_size": null,
            "model_architecture": "skip-gram word embedding model (word2vec-style) with finite numeral vocabulary and UNK_num for OOV numerals",
            "arithmetic_operation_type": "Evaluated on magnitude/numeration and contextual numeral prediction; not designed to perform arithmetic algorithms",
            "number_range_or_complexity": "Limited to numerals present in finite vocabulary; many numerals are OOV and mapped to UNK_num",
            "method_or_intervention": "No special numeric treatment; OOV numerals collapsed to UNK_num. Standard skip-gram negative sampling training.",
            "performance_result": "Magnitude tests: OVA 12.17%, SC 51.02%, BC 95.99%, AVGR 144.13. Numeration tests: OVA 7.08%, SC 61.95%, BC 99.12%, AVGR 27.08. Non-linear decoding RMSE: 14878.73. Numeral prediction and downstream sequence labeling performance degrade when numerals are OOV or scarce; NumAsTok shows worse generalizability to perturbed numerals (augmented test sets).",
            "mechanistic_insight": "No mechanism to generalize to unseen numerals; memorizes seen numerals but fails to represent continuous numeric relations, leading to poor generalization across magnitude and OOV instances.",
            "performance_scaling": "Performance limited by vocabulary coverage; adding more numerals to vocab could improve seen-numeral performance but cannot reasonably cover infinite numeral space.",
            "failure_modes": "Numeral OOV problem (many numerals unseen in training), poor generalization to perturbed or rare numerals, large drops on augmented test sets.",
            "comparison_baseline": "Baseline compared against SOM/GMM prototype methods, D-LSTM, Fixed, and numerate-LM.",
            "key_finding": "Treating numerals as ordinary finite tokens leads to strong performance on high-frequency numerals but fails to generalize to OOV numerals and continuous magnitude relationships; prototype-based methods address this limitation.",
            "uuid": "e331.3",
            "source_info": {
                "paper_title": "Learning Numeral Embedding",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Numerate-LM",
            "name_full": "Mixture-of-Gaussians Numerate Language Model (Spithourakis & Riedel 2018)",
            "brief_description": "A numerate language model that models numbers with a mixture-of-Gaussians over numeric value and integrates this into language modeling; designed to improve prediction of numbers in context.",
            "citation_title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
            "mention_or_use": "use",
            "model_name": "Numerate language model (mixture-of-Gaussians over numbers)",
            "model_size": null,
            "model_architecture": "language model with explicit mixture-of-Gaussians numeric component integrated into language modeling (not a large transformer in this paper's usage)",
            "arithmetic_operation_type": "Contextual numeral prediction and numeric modeling (probabilistic modeling of numeric values); not evaluated here on arithmetic operations such as addition/subtraction/multiplication",
            "number_range_or_complexity": "Model operates on numeric values (log-space transformations often used); in this paper it was evaluated on the same numeral prediction tasks (Wikipedia-1B subset) but with a smaller vocabulary (3e4) for memory reasons.",
            "method_or_intervention": "Mixture-of-Gaussians numeric modeling within LM; paper evaluated two prediction methods: default previous-context prediction and using sentence log-likelihood to rank candidates.",
            "performance_result": "On the numeral prediction task referenced, the numerate-LM achieved slightly better median absolute error (MdAE) than the prototype-embedding methods in one metric, but worse median absolute percentage error (MdAPE) and worse average rank (AVGR) when scoring candidates from a large candidate set; exact comparative numbers in the paper show mixed performance depending on scoring method and vocabulary size.",
            "mechanistic_insight": "Uses an explicit probabilistic model over numeric value (GMM) to represent continuous numeric information and integrates this with language modeling probabilities, demonstrating that explicit numeric modeling helps certain numeric prediction metrics but may trade off ranking quality depending on candidate sets and vocab sizes.",
            "performance_scaling": "Required smaller vocabulary (3e4) in the experimental run in this paper due to memory; scaling behavior with model size not discussed here.",
            "failure_modes": "When used with large candidate sets and for ranking candidates, it produced worse AVGR and MdAPE compared to prototype-based skip-gram methods in the reported run; memory constraints limited vocabulary size in experiments.",
            "comparison_baseline": "Compared against prototype-based SOM/GMM embedding methods and standard skip-gram baselines in the paper's numeral prediction experiments.",
            "key_finding": "Explicit probabilistic numeric modeling (mixture-of-Gaussians) can improve some numeric prediction metrics but may not uniformly outperform prototype-based embedding approaches on contextual candidate-ranking tasks, and practical deployment must manage vocabulary and memory tradeoffs.",
            "uuid": "e331.4",
            "source_info": {
                "paper_title": "Learning Numeral Embedding",
                "publication_date_yy_mm": "2019-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
            "rating": 2
        },
        {
            "paper_title": "Do NLP models know numbers?",
            "rating": 2
        },
        {
            "paper_title": "Numeracy-600k: Learning numeracy for detecting exaggerated information in market comments",
            "rating": 2
        },
        {
            "paper_title": "Exploring numeracy in word embeddings",
            "rating": 2
        }
    ],
    "cost": 0.01793475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Numeral Embedding</h1>
<p>Chengyue Jiang<em>, Zhonglin Nian</em>, Kaihao Guo<em>,<br>Yinggong Zhao ${ }^{\dagger}$, Shanbo Chu ${ }^{\dagger}$, Libin Shen ${ }^{\dagger}$, Kewei Tu</em><em><br></em> School of Information Science and Technology, ShanghaiTech University<br>Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences<br>University of Chinese Academy of Sciences<br>Shanghai Engineering Research Center of Intelligent Vision and Imaging<br>${ }^{\dagger}$ Leyan Technology. Inc<br>{jiangchy, nianzhl, guokh, tukw}@shanghaitech.edu.cn<br>{ygzhao, chushb, libin}@leyantech.com</p>
<h4>Abstract</h4>
<p>Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a selforganizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling.</p>
<h2>1 Introduction</h2>
<p>Word embeddings have become an essential building block for deep learning approaches to natural language processing (NLP). The quality of pretrained word embeddings has been shown to significantly impact the performance of neural approaches to a variety of NLP tasks. Over the past two decades, significant progress has been made in the development of word embedding techniques (Lund and Burgess, 1996; Bengio et al., 2003; Bullinaria and Levy, 2007; Mikolov et al., 2013b; Pennington et al., 2014). However, existing word embedding methods do not handle numerals</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>adequately and cannot directly encode the numeracy and magnitude of a numeral Naik et al. (2019). Most methods have a limited vocabulary size and therefore can only represent a small subset of the infinite number of numerals. Furthermore, most numerals have very scarce appearances in training corpora and therefore are more likely to be out-of-vocabulary (OOV) compared to non-numerical words. For example, numerals account for $6.15 \%$ of all unique tokens in English Wikipedia, but in GloVe (Pennington et al., 2014) which is partially trained on Wikipedia, only $3.79 \%$ of its vocabulary is numerals.</p>
<p>Previous work (Spithourakis et al., 2016) also shows that the numeral OOV problem is more severe when learning word embeddings from corpora with abundant numerals such as clinical reports. Even if a numeral is included in the vocabulary, its scarcity in the training corpus would negatively impact the learning accuracy of its embedding. The inadequate handling of numerals in existing word embedding methods can be problematic in scenarios where numerals convey critical information. For instance,</p>
<ul>
<li>"Jeff is $\mathbf{1 9 0}$, so he should wear size XXL." (190 is a reasonable height for size XXL. If we replace 190 with 160 , the sentence becomes unreasonable.)</li>
<li>"Jeff is $\mathbf{1 0}$, so he should wear size XS." (10 is an age instead of a height.)</li>
</ul>
<p>If the numerals in the example are OOV or their embeddings are not accurately learned, then it becomes impossible to judge the categories of the numerals or the reasonableness of the sentences. In this paper, we propose two novel methods that can produce reasonable embeddings for any numerals. The key idea is to represent the embedding of a numeral as a weighted average of a small set</p>
<p>of prototype number embeddings induced from the training corpus using either a self-organizing map (Kohonen, 1990) or a Gaussian mixture model. The weights are computed based on the differences between the target numeral and the prototype numerals, reflecting the inductive bias that numerals with similar quantities are likely to convey similar semantic information and thus should have similar embeddings. Numeral embeddings represented in this manner can then be plugged into a traditional word embedding method for training. We empirically evaluate our methods on four tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. The results show that our methods can produce high-quality embeddings for both numerals and non-numerical words and improve the performance of downstream tasks.</p>
<h2>2 Related Work</h2>
<p>Word Embedding Word embeddings are vector representations of words that carry semantic meanings implicitly and are trained without supervision. Most existing word embedding training methods can be divided into two classes. The first class of methods (Lund and Burgess, 1996; Lebret and Lebret, 2013) extract word co-occurrence statistics from the training corpus, compute a wordword matrix based on measures such as PPMI, and then apply dimension reduction techniques such as principle component analysis to produce a lowdimensional representation for each word. The second class of methods (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a,b) use a simple neural network to model the relation between a word and its context within a sliding window in the training corpus. GloVe (Pennington et al., 2014) has been proposed as a method that combines the advantages of both classes. All the above methods have a finite vocabulary size and use a 'UNK' symbol to represent OOV words. Recent work (Naik et al., 2019) shows that these popular methods do not handle numerals adequately. Wallace et al. (2019) shows that existing word embedding methods can encode numeracy implicitly for high-frequency numerals, but the embedding's numeracy for OOV numerals is not investigated. Our goal is to design numeral embedding methods that can be integrated into traditional word embedding methods and handle the OOV problem for numerals.</p>
<p>Numeracy in natural language Numeral understanding has been found important in textual entailment (Lev et al., 2004; De Marneffe et al., 2008; Roy et al., 2015) and information extraction (Intxaurrondo et al., 2015; Madaan et al., 2016), but existing systems often use manually defined task-specific features and logic rules to identify numerals, which is hard to generalize to other tasks. A lot of research has been done trying to solve math problems, using either manually designed features and rules (Roy et al., 2015; Upadhyay et al., 2016) or sequence-to-sequence neural networks (Wang et al., 2017), but the quantity of numerals is not important in this task and hence existing methods often replace numerals by dummy symbols such as $n_{1}$ and $n_{2}$. Spithourakis and Riedel (2018) studied different strategies to better model numerals in language models. Chen et al. (2019) created Numeracy-600K dataset and studied the ability of neural network models to learn numeracy. Our work differs from previous work in that we aim to produce general-purpose numeral embeddings that can be employed in any neural NLP approach.</p>
<h2>3 Methods</h2>
<p>Given a training corpus $C$, we first extract all the numerals using regular expressions and form a dataset $X$ containing all the numbers represented by these numerals. A number (e.g., 2000) may appear for multiple times in $X$ if its corresponding numerals (e.g., '2000', '2,000', etc.) appear for multiple times in $C$. We then induce a finite set $\mathbb{P}$ of typical numerals (i.e., prototypes) from $X$ using a self-organizing map (Kohonen, 1990) or a Gaussian mixture model. We also define a function $\operatorname{sim}\left(n_{1}, n_{2}\right)$ outputting the similarity between two arbitrary numbers $n_{1}$ and $n_{2}$. Now we represent the embedding of any target numeral $n$ as a weighted average of the prototype number embeddings with the weights computed by the similarity function:</p>
<p>$$
e(n)=\sum_{p \in \mathbb{P}} \alpha \cdot \operatorname{sim}(n, p) \cdot e(p)
$$</p>
<p>We use $e(\cdot)$ to denote the embedding of a number, $\alpha$ is the normalization factor where $\sum_{p \in \mathbb{P}} \alpha$. $\operatorname{sim}(n, p)=1$. This formulation satisfies the intuition that numerals with similar quantities are likely to convey similar semantic information and should have similar embeddings.</p>
<p>Our numeral embeddings can be integrated into traditional word embedding methods such as skip-</p>
<p>gram for training. During training, we backpropagate the error gradient to update the prototype embeddings. In this way, the prototype embeddings (and hence all the numeral embeddings) are learned jointly with non-numerical word embeddings.</p>
<h3>3.1 Squashing numbers to log-space</h3>
<p>Inspired by psychological evidence that our brain compresses large quantities nonlinearly using a logarithmic scale on the mental number line (Nieder and Miller, 2003; Dehaene, 2011), we design the following squashing function to transform all the numbers in $X$ into the log-space before prototype induction. Alternatively, we can apply the function only in the similarity function. Squashing is also necessary for our methods to avoid overflow during training when there are very large numbers such as $10^{15}$ in the training corpus.</p>
<p>$$
f(x)= \begin{cases}\log (x)+1, &amp; \text { if } x&gt;1 \ x, &amp; \text { if } x \in[-1,1] \ -\log (-x)-1, &amp; \text { if } x&lt;-1\end{cases}
$$</p>
<h3>3.2 Prototype Induction</h3>
<p>We develop two methods for inducing a a small set $P$ of $m$ prototypes from the number dataset $X$.</p>
<p>Self-Organizing Map A self-organizing map (SOM) (Kohonen, 1990) is an neural network can be viewed as a clustering method. After training a SOM on the dataset $X$, we regard each cluster centroid as a prototype. One advantage of using a SOM in comparison with traditional clustering methods is that it distributes prototypes more evenly on the number line and may assign prototypes to number ranges with few training samples, which we expect would lead to better generalizability.</p>
<p>Gaussian Mixture Model Inspired by psychological study of the mental number line (Dehaene et al., 2003) and previous work on language modeling (Spithourakis and Riedel, 2018), we train a Gaussian mixture model (GMM) to induce number prototypes. A GMM is defined as follows.</p>
<p>$$
\begin{aligned}
p(U=n) &amp; =\sum_{k=1}^{m} P(Z=k) P(U=n \mid Z=k) \
&amp; =\sum_{k=1}^{m} \pi_{k} \mathcal{N}\left(n ; \mu_{k}, \sigma_{k}^{2}\right)
\end{aligned}
$$</p>
<p>where $Z$ is a latent variable representing the mixture component for random variable $U$, and $\mathcal{N}$ is
the probability density function of a normal distribution, and $\pi_{k}, \mu_{k}, \sigma_{k} \in \mathbb{R}$ represent the mixing coefficient, mean and standard deviation of the $k$-th Gaussian component. We train a GMM on the number dataset $X$ using the expectation-maximization (EM) or hard-EM algorithm and regard the means of the learned Gaussian components as prototypes $\mathbb{P}=\left{\mu_{1}, \cdots, \mu_{m}\right}$. We use three GMM initialization methods described Appendix.A.</p>
<h3>3.3 Similarity Function</h3>
<p>For SOM-induced prototypes, we define the following similarity function:</p>
<p>$$
\operatorname{sim}(p, n)=|g(p)-g(n)|^{-\beta}, \beta&gt;0, p \in \mathbb{P}
$$</p>
<p>where function $g$ is equal to the squashing function $f$ defined in Eq. 2 if we do not apply log transformation before prototype induction and is the identity function $I$ otherwise. $\beta$ is a hyper-parameter set to 1.0 by default.</p>
<p>For GMM-induced prototypes, we can naturally use the posterior probability of the component assignment to define the similarity function, for all $p_{k} \in \mathbb{P}$,</p>
<p>$$
\begin{aligned}
\operatorname{sim}\left(p_{k}, n\right) &amp; \propto P(Z=k \mid U=n) \
&amp; =\frac{\pi_{k} \mathcal{N}\left(n ; \mu_{k}, \sigma_{k}^{2}\right)}{\sum_{k=1}^{m} \pi_{k} \mathcal{N}\left(n ; \mu_{k}, \sigma_{k}^{2}\right)}
\end{aligned}
$$</p>
<h3>3.4 Embedding Training</h3>
<p>We now describe how to integrate our numeral embeddings into traditional word embedding methods for training. We choose skip-gram with negative sampling (Mikolov et al., 2013b) as the word embedding method here, but many other word embedding methods such as CBOW (Mikolov et al., 2013a), HAL (Lund and Burgess, 1996) and GloVe (Pennington et al., 2014) can be used as well. Skip-gram is a word embedding method based on the idea of context word prediction. The training corpus $C$ is regarded as a sequence of words $\left(x_{1}, \ldots, x_{T}\right)$. For token $x_{t}$, we define the preceding and following $c$ tokens as the context of $x_{t}$. Skip-gram aims to maximize $p\left(x_{t+j} \mid x_{t}\right)(-c \leq$ $j \leq c)$, the probability of a context word given the center word $x_{t}$. To formulate $p\left(x_{t+j} \mid x_{t}\right)$, skipgram associates each word $x_{i}$ with two vector representations: the input embedding $\boldsymbol{v}<em t="t">{x</em>}}^{i}$ for being a center word and the output embedding $\boldsymbol{v<em t="t">{x</em>}}^{o}$ for being a context word. The input and output embeddings of all the words in the vocabulary $\mathbb{V}$ constitute matrices $\boldsymbol{E<em O="O">{I} \in \mathbb{R}^{D \times|\mathbb{V}|}$ and $\boldsymbol{E}</em>$ respectively,} \in \mathbb{R}^{D \times|\mathbb{V}|</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The computational graph when the center word is 'is' and the context words are 'he' and the numeral '190'. We look up the embedding vectors of non-numerical words directly from the embedding matrices and use the weighted average of prototype embeddings as the numeral embedding. Negative sampling is not shown in the figure.</p>
<p>where $D$ is the embedding dimension. The conditional probability $p\left(x_{t+j} \mid x_{t}\right)$ is then defined to based on the dot product $s\left(x_{t+j} \mid x_{t}\right)=\boldsymbol{v}<em t="t">{x</em>}}^{i T} \boldsymbol{v<em t_j="t+j">{x</em>$. Negative sampling is used to approximate the normalization factor for the conditional probability.}}^{o</p>
<p>$$
\begin{aligned}
&amp; \log p\left(x_{t+j} \mid x_{t}\right) \approx \log \sigma\left(\boldsymbol{v}<em t_j="t+j">{x</em>}}^{o}{ }^{T} \boldsymbol{v<em t="t">{x</em>\right) \
&amp; \quad+\sum_{i=1}^{k} \underset{x_{i} \sim P_{n}(x)}{\mathbb{E}}\left[\log \sigma\left(-\boldsymbol{v}}}^{i<em i="i">{x</em>}}^{o}{ }^{T} \boldsymbol{v<em t="t">{x</em>\right)\right]
\end{aligned}
$$}}^{i</p>
<p>where $\sigma$ denotes the sigmoid function, and $P_{n}(x)$ is the sampling distribution used to draw $k$ negative word samples. We modify skip-gram by computing numeral embeddings differently from non-numerical word embeddings. We associate each prototype number with an input embedding and an output embedding. The input and output embeddings of all the prototypes constitute matrices $\boldsymbol{M}<em O="O">{I} \in \mathbb{R}^{D \times \mid \mathbb{P} \mid}$ and $\boldsymbol{M}</em>$. During training, we optimize the objective function Eq. 6 by back-propagating the gradient of the error to update both the non-numerical word embed-
ding matrices $\boldsymbol{E}} \in \mathbb{R}^{D \times \mid \mathbb{P} \mid}$ respectively. For any numeral, we can compute its input and output embeddings by taking a weighted average of the prototype input and output embeddings respectively based on Eq. 1 and use them in exactly the same way as the embeddings of nonnumerical words to compute the learning objective (Eq.6). When drawing negative samples, we first set the ratio of numerals and non-numerical words to their actual ratio in the training corpus, to guarantee a sufficient number of numeral negative samples. Then we sample numerals and non-numerical words separately from their respective distributions in the training corpus raised to the power of $\frac{3}{4<em O="O">{I}, \boldsymbol{E}</em>}$ and the prototype number embedding matrices $\boldsymbol{M<em O="O">{I}, \boldsymbol{M}</em>$. In this way, the embeddings of non-numerical words and numerals are learned jointly in the same space. We show an example in Figure 1.</p>
<h2>4 Experiments and Results</h2>
<p>We evaluate our methods on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. We report the results of our methods based on SOM and GMM separately. We choose the hyper-parameters (e.g., the number of prototypes, GMM initialization and training methods) using validation sets and report the best hyper-parameters for each experiment in Appendix.B.</p>
<h3>4.1 Baselines</h3>
<p>NumAsTok This baseline treats numerals and non-numerical words in the same way, which is very similar to the original skip-gram. The vocabulary includes both high-frequency words and highfrequency numerals. OOV non-numerical words are replaced with symbol $\mathrm{UNK}<em _num="{num" _text="\text">{\text {word }}$ and OOV numerals are replaced with symbol $\mathrm{UNK}</em>$.}</p>
<p>D-LSTM Character-level RNNs are often used to encode OOV words (Graves, 2013). Here we apply an LSTM (Hochreiter and Schmidhuber, 1997) to the digit sequence of a numeral and use the last hidden state of the LSTM as the embedding of the numeral. We use the embedding to compute the skip-gram objective function and propagate the gradients back to update the LSTM. The vocabulary of digits is: ${0-9, ' ', '+ ', '-'$, 'e' $}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Non-numerical <br> Word Vocabulary</th>
<th style="text-align: center;">Numeral <br> Vocabulary</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SOM, GMM, <br> D-LSTM, Fixed</td>
<td style="text-align: center;">{In-vocab words} <br> $\mathrm{UNK}_{\text {word }}$</td>
<td style="text-align: center;">all numerals</td>
</tr>
<tr>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">{In-vocab words} <br> $\mathrm{UNK}_{\text {word }}$</td>
<td style="text-align: center;">{In-vocab nums} <br> $\mathrm{UNK}_{\text {num }}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Vocabularies of different methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">WS353</th>
<th style="text-align: center;">MEN</th>
<th style="text-align: center;">SIM999</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">64.40</td>
<td style="text-align: center;">71.79</td>
<td style="text-align: center;">36.09</td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">64.90</td>
<td style="text-align: center;">71.89</td>
<td style="text-align: center;">36.29</td>
</tr>
<tr>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">65.30</td>
<td style="text-align: center;">71.83</td>
<td style="text-align: center;">35.85</td>
</tr>
<tr>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">63.60</td>
<td style="text-align: center;">71.82</td>
<td style="text-align: center;">34.58</td>
</tr>
<tr>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">64.35</td>
<td style="text-align: center;">72.17</td>
<td style="text-align: center;">36.27</td>
</tr>
<tr>
<td style="text-align: center;">SG GoogleNews-100B</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">74.10</td>
<td style="text-align: center;">44.20</td>
</tr>
<tr>
<td style="text-align: center;">GloVe Wiki-6B</td>
<td style="text-align: center;">52.20</td>
<td style="text-align: center;">73.70</td>
<td style="text-align: center;">37.10</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on word similarity tasks trained on Wiki-1B. For reference, we also show the results of the official skip-gram and GloVe trained on larger corpora.</p>
<p>Fixed This baseline fixed embeddings for numerals with no training. We define the embedding a numeral with value $n$ as $[f(n) ; \mathbf{1}] / Z$ where $f$ is the squashing function defined in Eq. $2, \mathbf{1} \in \mathbb{R}^{D-1}$ is an all-ones vector, and $Z$ is a constant used to keep the vector norm close to those of non-numerical words and is set to $2 \times D$ by default.</p>
<p>We compare the vocabularies of different methods in Table 1. Our methods, D-LSTM, and Fixed have finite non-numerical vocabularies but infinite numeral vocabularies. In contrast, the NumAsTok baseline has a finite numeral vocabulary and treats all the OOV numerals as $\mathrm{UNK}_{\text {num }}$.</p>
<h3>4.2 Word Similarity for Non-numerical Words</h3>
<p>To ensure that our methods can still generate high quality embeddings for non-numerical words, we evaluate our trained embeddings on classical intrinsic word similarity tasks, including WordSim353, (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and Simplex-999 (Hill et al., 2014). We train 300-dimensional word embeddings on the 1B Wikipedia dump and set the context window size to 5 , the number of negative samples to 5 , and the vocabulary size to $3 \times 10^{5}$. We use the evaluation tools ${ }^{1}$ provided by Jastrzebski et al. (2017). Note that while the training data contains numerals, the evaluation tasks do not involve numerals and are only designed to evaluate the quality of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>non-numerical word embeddings. The results are shown in Table 2.</p>
<p>It can be seen that our methods can achieve scores comparable to those of the baselines. The performance of SG trained on 100B GoogleNews is much better than all the other methods probably because of its much larger training corpus. The results show that adding our numeral embedding methods into skip-gram does not harm the quality of non-numerical word embeddings. We also show some examples of prototypes and their nearest nonnumerical words in Table 3, and some additional results of our methods in Appendix.C. The results show that our embedding method learns the semantic of numerals. We use the embedding trained by the SOM model with 200 prototypes on Wikipedia1B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prototype</th>
<th style="text-align: left;">Most Related Non-numerical Words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">8186446.58</td>
<td style="text-align: left;">million, billion, total, budget, funding, dollars</td>
</tr>
<tr>
<td style="text-align: left;">10372.49</td>
<td style="text-align: left;">approximately, thousands, millions, roughly</td>
</tr>
<tr>
<td style="text-align: left;">2000.06</td>
<td style="text-align: left;">millennium, decade, internet, twentieth, worldwide, latest</td>
</tr>
<tr>
<td style="text-align: left;">1598.79</td>
<td style="text-align: left;">renaissance, giovanni, dutch, baroque, vii, shakespeare</td>
</tr>
<tr>
<td style="text-align: left;">10.00</td>
<td style="text-align: left;">ten, six, eleven, pm, seconds, eight</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of prototypes and their nearest nonnumerical words.</p>
<h3>4.3 Magnitude and Numeration of Embeddings</h3>
<p>Naik et al. (2019) propose a framework for evaluating the ability of numeral embeddings to capture magnitude and numeration. Given a target numeral, its embedding is evaluated against a set of numerals using the OVA (One-vs-All), SC (Strict Contrastive) and BC (Broad Contrastive) tests:
OVA: The embedding vector distance between the target and its nearest neighbor on the number line should be smaller than that between the target and any other numeral in the set.
SC: The embedding vector distance between the target and its nearest neighbor on the number line should be smaller than that between the target and its second nearest neighbors on the number line.
BC: The embedding vector distance between the target and its nearest neighbor on the number line should be smaller than that between the target and its furthest neighbors on the number line.</p>
<p>We follow the settings described by Naik et al. (2019): for the magnitude evaluation, we run the tests using a set of 2342 numerals that are most frequent in Wikipedia-1B, whose embeddings are well learned by all the methods; and for the numer-</p>
<p>ation evaluation, we run the tests using 113 English words that represent numbers (e.g., 'three', 'billion') sampled from the same corpus and we measure the distance between the target numeral embedding and the word embeddings of these words. We report the accuracy of various embedding models on these three tests, along with the average rank (denoted as AVGR) of the target numeral's nearest neighbor among all the candidates based on their vector distances to the target. We use the embeddings trained on Wikipedia-1B.</p>
<p>Table 4 shows the results. The Fixed baseline has the best performance in the magnitude evaluation, which is unsurprising because the numeral embedding vector explicitly contains the (squashed) magnitude. NumAsTok performs very well in the numeration evaluation, which is because the number-representing words used in the evaluation are high-frequency words and their embeddings are adequately trained. Except for these two special cases, our methods can be seen to outperform the baselines with a large margin.</p>
<p>Wallace et al. (2019) recently show that classic embeddings of numerals may contain magnitude information that can be extracted by neural networks. Following their methodology, we conduct a nonlinear decoding test on our 2342 numerals. We first randomly sample $80 \%$ of the numerals for training and $20 \%$ for test. Then we train an MLP with 2 hidden layers to predict the value of a numeral given its embedding by minimizing the mean squared error. The root mean squared error (RMSE) result on the test set is shown in the last column of Table 4 , which shows that our embeddings are better at capturing the magnitude information non-linearly.</p>
<h3>4.4 Numeral Prediction</h3>
<p>To evaluate the quality of numeral embeddings, we design a new numeral prediction task: choosing the right numeral from a set of candidates given the context of the numeral in a sentence. We randomly sample 2000 sentences containing numerals from a subset of Wikipedia that is not used in training, with 600 for validation and 1400 for testing. For each sentence, we use the five words preceding and following the target numeral as its context. An example is shown below, where the ten bold words are the context and 2.31 is the target numeral.</p>
<p>In Hollywood, the average household size was [2.31] and the average family size was 3.00 .</p>
<p>We use all the 1400 numerals in the test set as the candidates from which one has to select the right numeral for each test sentence. Given the learned word and numeral embeddings, we define two score functions to rank candidate numerals given the context. Following the skip-gram model, we first define the score of center numeral $n$ predicting context word $c_{j}$ as $s\left(c_{j} \mid n\right)=\boldsymbol{v}<em j="j">{c</em>}}^{n}{ }^{T} \boldsymbol{v<em j="j">{n}^{t}$ and the score of context word $c</em>}$ predicting the center numeral $n$ as $s\left(n \mid c_{j}\right)=\boldsymbol{v<em c__j="c_{j">{n}^{n}{ }^{T} \boldsymbol{v}</em>}}^{t}$. Our first candidate-ranking score function $\mathbf{S<em j="j">{\mathbf{A}}$ is the sum of log probabilities of center numeral $n$ predicting each context word $c</em>$. We use softmax here to calculate the probability.</p>
<p>$$
\begin{aligned}
\mathbf{S}<em j="j">{\mathbf{A}}(n) &amp; =\sum</em>} \log p\left(c_{j} \mid n\right) \approx \sum_{j} \log \frac{e^{s\left(c_{j} \mid n\right)}}{\sum_{c_{k} \in \mathbb{V<em k="k">{t}} e^{s\left(c</em> \
&amp; =\sum_{j} s\left(c_{j} \mid n\right)-\sum_{j} \log Z(n)
\end{aligned}
$$} \mid n\right)}</p>
<p>where $\mathbb{V}<em _mathbf_B="\mathbf{B">{t}$ is the vocabulary of non-numerical words and $Z(n)$ is the normalization factor. The other candidate-ranking score function $\mathbf{S}</em>$ predicting center numeral $n$.}}$ is the sum of $\log$ probabilities of each context word $c_{j</p>
<p>$$
\begin{aligned}
\mathbf{S}<em j="j">{\mathbf{B}}(n) &amp; =\sum</em>} \log p\left(n \mid c_{j}\right) \approx \sum_{j} \log \frac{e^{s\left(n \mid c_{j}\right)}}{\sum_{n_{k} \in \mathbb{V<em k="k">{n}} e^{s\left(n</em> \
&amp; =\sum_{j} s\left(n \mid c_{j}\right)-\text { Constant }
\end{aligned}
$$} \mid c_{j}\right)}</p>
<p>where $\mathbb{V}<em _mathbf_A="\mathbf{A">{n}$ is the set of numerals in the dataset. There are a few other possible score functions, but we find that they lead to results similar to $\mathbf{S}</em>$.}}$ and $\mathbf{S}_{\mathbf{B}</p>
<p>We use three metrics to evaluate numeral prediction (Spithourakis and Riedel, 2018). MdAE is the median of the absolute errors between the predicted and true numerals, MdAPE is the median of the absolute percentage errors between the predicted and true numerals, and AVGR is the average rank of the true numeral among the candidates.</p>
<p>We train embeddings on Wikipedia-1B and report the evaluation results in the left part of Table 5. Our methods significantly outperform the NumAsTok and Fixed baselines on all three metrics. D-LSTM also performs well but needs more parameters and computing time than our methods.</p>
<p>For reference, we also test the mixture-of-Gaussians numerate language model ${ }^{2}$ (Spithourakis and Riedel, 2018) on the same task. Although the numerate language model does not learn</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Magnitude</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Numeration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Non-linear Decoding</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metrics</td>
<td style="text-align: center;">OVA</td>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">BC</td>
<td style="text-align: center;">AVGR</td>
<td style="text-align: center;">OVA</td>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">BC</td>
<td style="text-align: center;">AVGR</td>
<td style="text-align: center;">RMSE</td>
</tr>
<tr>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">67.72</td>
<td style="text-align: center;">71.86</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">15.91</td>
<td style="text-align: center;">3.54</td>
<td style="text-align: center;">62.83</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">28.98</td>
<td style="text-align: center;">2724.12</td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">57.86</td>
<td style="text-align: center;">58.63</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">1.75</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">65.49</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">25.97</td>
<td style="text-align: center;">678.78</td>
</tr>
<tr>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">12.17</td>
<td style="text-align: center;">51.02</td>
<td style="text-align: center;">95.99</td>
<td style="text-align: center;">144.13</td>
<td style="text-align: center;">7.08</td>
<td style="text-align: center;">61.95</td>
<td style="text-align: center;">99.12</td>
<td style="text-align: center;">27.08</td>
<td style="text-align: center;">14878.73</td>
</tr>
<tr>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">7.26</td>
<td style="text-align: center;">51.79</td>
<td style="text-align: center;">92.83</td>
<td style="text-align: center;">158.82</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">54.87</td>
<td style="text-align: center;">89.38</td>
<td style="text-align: center;">53.55</td>
<td style="text-align: center;">20949.66</td>
</tr>
<tr>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">83.90</td>
<td style="text-align: center;">78.22</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">49.56</td>
<td style="text-align: center;">99.12</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">5550.97</td>
</tr>
</tbody>
</table>
<p>Table 4: Magnitude, numeration, and non-linear decoding results for our methods and baselines. Accuracies of OVA, SC and BC are expressed as percentages. Lower AVGR indicates better performance. Numbers indicating top-2 performance are highlighted.
numeral embeddings, it is shown to produce accurate numeral prediction. We use a smaller vocabulary size $\left(3 \times 10^{4}\right)$ for the model compared with our methods and the baselines $\left(3 \times 10^{5}\right)$ because we find it requires unacceptable GPU memory and training time with a large vocabulary size. The default prediction method of the model predicts a numeral from its preceding context and the predicted numeral may not belong to the candidate set (hence no AVGR result). A second prediction method is to use the log-likelihood of the whole sentence computed by the model to score and rank the candidates. As shown in the last row of table 5 , the numerate language model reaches slightly better MdAE but worse MdAPE and AVGR than our methods.</p>
<p>We also conduct a slightly different numeral prediction task on the recently released Numeracy600K dataset (the Article Title part) (Chen et al., 2019). This dataset contains 600 k sentences with numerals and in each sentence, a numeral is selected and tagged with its order of magnitude. There are eight possible orders of magnitude and the goal is to predict the correct one for the target numeral from its context. To solve this classification problem, we sample 100 numerals for each magnitude order and use the mean of their numeral embeddings to create a 'meta' embedding; we then use these 'meta' embeddings to replace the numeral embeddings in the score functions $\mathbf{S}<em _mathbf_B="\mathbf{B">{\mathbf{A}}$ and $\mathbf{S}</em>$ and the highest-scoring order of magnitude is returned. We split the dataset to 450 k sentences for training, 50 k for validation and 100 k for testing. We use micro-F1 and macro-F1 in addition to AVGR as the evaluation metrics. The result is shown in the right part of Table 5. The result shows that our methods achieve much better performance compared to the baselines.}</p>
<h3>4.5 Sequence Labeling on Customer Service Data</h3>
<p>To verify the effectiveness of our methods in practice, we evaluate our methods with a sequence labeling task on a dataset of customer service chat logs from an online apparel shopping website. This dataset contains a large number of numerals related to height, weight, foot length, etc., and therefore is a good testbed for evaluating numeral embeddings. The task is to assign a label to each word or numeral in the dataset indicating its information type. We shows two examples below:</p>
<p>WO H O O O O O W H O O O 82 kg 177 cm what size shall I choose 82177 what size ?
$\mathrm{W}, \mathrm{H}, \mathrm{O}$ are labels representing weight, height and ordinary word respectively. We show the statistics of the dataset in Appendix.E. To better evaluate the generalizability, we create two additional test sets. The first one is created by 'augmenting' the original test set with new sentences containing slightly perturbed numerals. For example, we can create new sentences by replacing ' 177 ' in the above example with ' 176 ' and ' 178 '. The second one contains 'hard' sentences from the original test set that do not have explicit cues for label prediction. For example, the first sentence above contains ' kg ' and ' cm ' that can greatly facilitate the prediction of W and H , but the second sentence above does not contain such cues and hence is a 'hard' sentence. Finally, we also test the low-resource settings in which only $30 \%$ or $10 \%$ of the training set is used. We described the augmented and hard test set Appendix.D more detailedly.</p>
<p>We learn embeddings from the training set using our methods and the baselines and use a validation set to do model selection. We plug the learned embeddings into the Neural-CRF model (Yang and Zhang, 2018) ${ }^{3}$ to do sequence labeling without using part-of-speech and character-level features</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Wikipedia-1B, dim 300</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Numeracy-600k, dim 300</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metrics</td>
<td style="text-align: center;">AVGR</td>
<td style="text-align: center;">MdAE</td>
<td style="text-align: center;">MdAPE</td>
<td style="text-align: center;">AVGR</td>
<td style="text-align: center;">MdAE</td>
<td style="text-align: center;">MdAPE</td>
<td style="text-align: center;">AVGR</td>
<td style="text-align: center;">Mi-F1</td>
<td style="text-align: center;">Ma-F1</td>
<td style="text-align: center;">AVGR</td>
<td style="text-align: center;">Mi-F1</td>
<td style="text-align: center;">Ma-F1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{S}_{\mathbf{A}}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{S}_{\mathbf{D}}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{S}_{\mathbf{A}}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{S}_{\mathbf{D}}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">381.41</td>
<td style="text-align: center;">825.79</td>
<td style="text-align: center;">0.9836</td>
<td style="text-align: center;">455.01</td>
<td style="text-align: center;">1184.60</td>
<td style="text-align: center;">0.9880</td>
<td style="text-align: center;">2.91</td>
<td style="text-align: center;">37.99</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">2.02</td>
<td style="text-align: center;">42.74</td>
<td style="text-align: center;">13.66</td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">343.50</td>
<td style="text-align: center;">1184.85</td>
<td style="text-align: center;">0.9450</td>
<td style="text-align: center;">444.15</td>
<td style="text-align: center;">1081.50</td>
<td style="text-align: center;">0.9866</td>
<td style="text-align: center;">2.19</td>
<td style="text-align: center;">41.86</td>
<td style="text-align: center;">18.47</td>
<td style="text-align: center;">2.02</td>
<td style="text-align: center;">44.07</td>
<td style="text-align: center;">13.77</td>
</tr>
<tr>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">600.17</td>
<td style="text-align: center;">1918.00</td>
<td style="text-align: center;">0.9965</td>
<td style="text-align: center;">600.28</td>
<td style="text-align: center;">32772.50</td>
<td style="text-align: center;">19.07</td>
<td style="text-align: center;">4.21</td>
<td style="text-align: center;">9.74</td>
<td style="text-align: center;">5.47</td>
<td style="text-align: center;">6.16</td>
<td style="text-align: center;">24.28</td>
<td style="text-align: center;">4.88</td>
</tr>
<tr>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">357.45</td>
<td style="text-align: center;">1310.65</td>
<td style="text-align: center;">0.9369</td>
<td style="text-align: center;">466.81</td>
<td style="text-align: center;">1080.5</td>
<td style="text-align: center;">0.9908</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">27.98</td>
<td style="text-align: center;">8.80</td>
<td style="text-align: center;">4.49</td>
<td style="text-align: center;">16.49</td>
<td style="text-align: center;">8.42</td>
</tr>
<tr>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">685.58</td>
<td style="text-align: center;">50371.50</td>
<td style="text-align: center;">42.82</td>
<td style="text-align: center;">672.47</td>
<td style="text-align: center;">50525.00</td>
<td style="text-align: center;">61.59</td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Default</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Likelihood Scoring</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Numerate-LM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">795.00</td>
<td style="text-align: center;">0.9923</td>
<td style="text-align: center;">571.37</td>
<td style="text-align: center;">890.00</td>
<td style="text-align: center;">0.9937</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: The results of the numeral prediction tasks. Numerate-LM represents the language model of Spithourakis and Riedel (2018), shown here for reference.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Augmented</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">97.12</td>
<td style="text-align: center;">91.19</td>
<td style="text-align: center;">90.46</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">97.02</td>
<td style="text-align: center;">91.28</td>
<td style="text-align: center;">90.18</td>
<td style="text-align: center;">90.72</td>
<td style="text-align: center;">96.19</td>
<td style="text-align: center;">86.66</td>
<td style="text-align: center;">85.91</td>
<td style="text-align: center;">86.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">97.04</td>
<td style="text-align: center;">90.74</td>
<td style="text-align: center;">90.45</td>
<td style="text-align: center;">90.60</td>
<td style="text-align: center;">97.03</td>
<td style="text-align: center;">91.19</td>
<td style="text-align: center;">90.43</td>
<td style="text-align: center;">90.81</td>
<td style="text-align: center;">96.06</td>
<td style="text-align: center;">86.18</td>
<td style="text-align: center;">85.93</td>
<td style="text-align: center;">86.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">96.72</td>
<td style="text-align: center;">89.84</td>
<td style="text-align: center;">88.80</td>
<td style="text-align: center;">89.32</td>
<td style="text-align: center;">96.72</td>
<td style="text-align: center;">90.40</td>
<td style="text-align: center;">88.99</td>
<td style="text-align: center;">89.69</td>
<td style="text-align: center;">95.52</td>
<td style="text-align: center;">84.19</td>
<td style="text-align: center;">83.30</td>
<td style="text-align: center;">83.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">95.75</td>
<td style="text-align: center;">86.19</td>
<td style="text-align: center;">87.42</td>
<td style="text-align: center;">86.80</td>
<td style="text-align: center;">95.86</td>
<td style="text-align: center;">87.13</td>
<td style="text-align: center;">87.65</td>
<td style="text-align: center;">87.39</td>
<td style="text-align: center;">93.97</td>
<td style="text-align: center;">78.39</td>
<td style="text-align: center;">80.18</td>
<td style="text-align: center;">79.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">96.88</td>
<td style="text-align: center;">91.37</td>
<td style="text-align: center;">89.29</td>
<td style="text-align: center;">90.32</td>
<td style="text-align: center;">96.36</td>
<td style="text-align: center;">90.99</td>
<td style="text-align: center;">87.39</td>
<td style="text-align: center;">89.15</td>
<td style="text-align: center;">96.00</td>
<td style="text-align: center;">87.11</td>
<td style="text-align: center;">85.12</td>
<td style="text-align: center;">86.10</td>
</tr>
<tr>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">96.21</td>
<td style="text-align: center;">89.55</td>
<td style="text-align: center;">86.07</td>
<td style="text-align: center;">87.78</td>
<td style="text-align: center;">95.92</td>
<td style="text-align: center;">89.07</td>
<td style="text-align: center;">85.33</td>
<td style="text-align: center;">87.16</td>
<td style="text-align: center;">95.27</td>
<td style="text-align: center;">84.42</td>
<td style="text-align: center;">81.62</td>
<td style="text-align: center;">82.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">96.20</td>
<td style="text-align: center;">89.50</td>
<td style="text-align: center;">86.18</td>
<td style="text-align: center;">87.81</td>
<td style="text-align: center;">95.88</td>
<td style="text-align: center;">89.12</td>
<td style="text-align: center;">85.29</td>
<td style="text-align: center;">87.16</td>
<td style="text-align: center;">95.23</td>
<td style="text-align: center;">84.44</td>
<td style="text-align: center;">81.50</td>
<td style="text-align: center;">82.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">95.55</td>
<td style="text-align: center;">86.83</td>
<td style="text-align: center;">83.88</td>
<td style="text-align: center;">85.33</td>
<td style="text-align: center;">95.30</td>
<td style="text-align: center;">86.22</td>
<td style="text-align: center;">83.13</td>
<td style="text-align: center;">84.64</td>
<td style="text-align: center;">94.32</td>
<td style="text-align: center;">80.10</td>
<td style="text-align: center;">78.17</td>
<td style="text-align: center;">79.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">94.67</td>
<td style="text-align: center;">83.51</td>
<td style="text-align: center;">82.69</td>
<td style="text-align: center;">83.10</td>
<td style="text-align: center;">94.48</td>
<td style="text-align: center;">83.40</td>
<td style="text-align: center;">82.02</td>
<td style="text-align: center;">82.71</td>
<td style="text-align: center;">92.92</td>
<td style="text-align: center;">75.03</td>
<td style="text-align: center;">75.18</td>
<td style="text-align: center;">75.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">95.58</td>
<td style="text-align: center;">89.18</td>
<td style="text-align: center;">83.55</td>
<td style="text-align: center;">86.27</td>
<td style="text-align: center;">94.57</td>
<td style="text-align: center;">88.39</td>
<td style="text-align: center;">79.94</td>
<td style="text-align: center;">83.95</td>
<td style="text-align: center;">94.65</td>
<td style="text-align: center;">84.42</td>
<td style="text-align: center;">79.06</td>
<td style="text-align: center;">81.65</td>
</tr>
<tr>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">93.43</td>
<td style="text-align: center;">82.36</td>
<td style="text-align: center;">75.01</td>
<td style="text-align: center;">78.51</td>
<td style="text-align: center;">92.78</td>
<td style="text-align: center;">81.48</td>
<td style="text-align: center;">72.85</td>
<td style="text-align: center;">76.92</td>
<td style="text-align: center;">93.19</td>
<td style="text-align: center;">80.26</td>
<td style="text-align: center;">72.71</td>
<td style="text-align: center;">76.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">93.48</td>
<td style="text-align: center;">82.13</td>
<td style="text-align: center;">75.11</td>
<td style="text-align: center;">78.46</td>
<td style="text-align: center;">92.87</td>
<td style="text-align: center;">80.96</td>
<td style="text-align: center;">73.22</td>
<td style="text-align: center;">76.89</td>
<td style="text-align: center;">93.24</td>
<td style="text-align: center;">79.47</td>
<td style="text-align: center;">73.04</td>
<td style="text-align: center;">76.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">92.53</td>
<td style="text-align: center;">77.71</td>
<td style="text-align: center;">71.45</td>
<td style="text-align: center;">74.45</td>
<td style="text-align: center;">91.99</td>
<td style="text-align: center;">76.24</td>
<td style="text-align: center;">69.96</td>
<td style="text-align: center;">72.96</td>
<td style="text-align: center;">92.10</td>
<td style="text-align: center;">73.26</td>
<td style="text-align: center;">68.72</td>
<td style="text-align: center;">70.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">91.90</td>
<td style="text-align: center;">75.39</td>
<td style="text-align: center;">71.41</td>
<td style="text-align: center;">73.34</td>
<td style="text-align: center;">91.48</td>
<td style="text-align: center;">73.96</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">72.02</td>
<td style="text-align: center;">91.06</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">67.47</td>
<td style="text-align: center;">68.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">92.31</td>
<td style="text-align: center;">81.98</td>
<td style="text-align: center;">70.51</td>
<td style="text-align: center;">75.81</td>
<td style="text-align: center;">90.77</td>
<td style="text-align: center;">80.10</td>
<td style="text-align: center;">64.95</td>
<td style="text-align: center;">71.73</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">79.64</td>
<td style="text-align: center;">67.95</td>
<td style="text-align: center;">73.32</td>
</tr>
</tbody>
</table>
<p>Table 6: The results of sequence labeling. We report the accuracy, precision, recall, F1 score for the original, augmented, and harder test sets with different training data sizes. Accuracy is in the token level and the other metrics are in the entity level. All the results are very stable and their standard deviations are often much smaller than the differences between our methods and the baselines.
and embedding fine-tuning.</p>
<p>The results are shown in Table 6. We also include the table with standard deviation in Appendix.F. Our methods consistently outperform all the baselines on the Accuracy, Recall, and F1 metrics in different configurations. NumAsTok trained with $100 \%$ training samples has the highest precision on the original and hard test sets probably because it learns high-quality embeddings for highfrequency numerals included in its vocabulary; but its recall is lower than that of our methods, most likely because of its numeral OOV problem. Comparing the results on the original and augmented test sets, we see that NumAsTok shows a more significant drop in performance than the other methods, which suggests that NumAsTok does not generalize well because of the numeral OOV problem. In the low-resource settings, the advantage of our methods over the baselines becomes even larger, indicating better generalizability and less annotation required for our methods to achieve a promising performance.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we propose two novel numeral embedding methods that represent the embedding of a numeral as a weighted average of a set of prototype numeral embeddings. The methods can be integrated into traditional word embedding approaches such as skip-gram for training. We evaluate our methods on four intrinsic and extrinsic tasks, including word similarity, embedding numeracy, numeral prediction, and sequence labeling, and show that our methods can improve the performance of numeral-related tasks and has better generalizability. Our code and sample data can be found at https://github.com/jeffchy/ Learning-Numeral-Embeddings.</p>
<p>An important future direction is to handle numeral polysemy. For example, the numeral "2019" may denote either a year or an ordinary number. One potential method is to assign a different embedding to each sense of a numeral. In this way, "2019" would have one embedding for representing a year and another for representing an ordinary quantity.</p>
<p>The similarity function would treat different senses of a numeral differently. For example, the year sense of " 2019 " would be similar to the year sense of " 19 " but dissimilar to the sole sense of " 2019.5 ", while the quantity sense of " 2019 " would be similar to that of " 2019.5 ". Our methods also have the potential to apply to contextual word embedding methods, so this would be another future direction.</p>
<h2>Acknowledgments</h2>
<p>This work was supported by the National Natural Science Foundation of China (61976139).</p>
<h2>References</h2>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. JMLR, 3(Feb):1137-1155.</p>
<p>Johannes Blömer and Kathrin Bujna. 2013. Simple methods for initializing the em algorithm for gaussian mixture models. CoRR.</p>
<p>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. J. Artif. Int. Res.</p>
<p>John A Bullinaria and Joseph P Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior research methods.</p>
<p>Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, and Hsin-Hsi Chen. 2019. Numeracy-600k: Learning numeracy for detecting exaggerated information in market comments. In $A C L$.</p>
<p>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML.</p>
<p>Marie-Catherine De Marneffe, Anna N Rafferty, and Christopher D Manning. 2008. Finding contradictions in text. Proceedings of ACL-08: HLT, pages 1039-1047.</p>
<p>Stanislas Dehaene. 2011. The number sense: How the mind creates mathematics. OUP USA.</p>
<p>Stanislas Dehaene, Manuela Piazza, Philippe Pinel, and Laurent Cohen. 2003. Three parietal circuits for number processing. Cognitive neuropsychology, 20(3-6):487-506.</p>
<p>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In WWW.</p>
<p>Alex Graves. 2013. Generating sequences with recurrent neural networks. Computer Science.</p>
<p>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Ander Intxaurrondo, Eneko Agirre, Oier Lopez De Lacalle, and Mihai Surdeanu. 2015. Diamonds in the rough: Event extraction from imperfect microblog data. In NAACL, pages 641-650.</p>
<p>Stanislaw Jastrzebski, Damian Lesniak, and Wojciech Marian Czarnecki. 2017. How to evaluate word embeddings? on importance of data efficiency and simple supervised tasks. CoRR, abs/1702.02170.</p>
<p>Teuvo Kohonen. 1990. The self-organizing map. Proceedings of the IEEE.</p>
<p>Rémi Lebret and Ronan Lebret. 2013. Word emdeddings through hellinger PCA. CoRR.</p>
<p>Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. In Workshop on Text Meaning and Interpretation.</p>
<p>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior research methods, instruments, \&amp; computers.</p>
<p>Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605.</p>
<p>Aman Madaan, Ashish Mittal, Ganesh Ramakrishnan, Sunita Sarawagi, et al. 2016. Numerical relation extraction with minimal supervision. In $A A A I$.</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. ICLR.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26. Curran Associates, Inc.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and Eduard Hovy. 2019. Exploring numeracy in word embeddings. In $A C L$, pages 3374-3380.</p>
<p>Andreas Nieder and Earl K Miller. 2003. Coding of cognitive magnitude: Compressed scaling of numerical information in the primate prefrontal cortex. Neuron, 37(1):149-157.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In EMNLP.</p>
<p>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities in natural language. TACL.</p>
<p>Georgios Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. In $A C L$.</p>
<p>Georgios P. Spithourakis, Isabelle Augenstein, and Sebastian Riedel. 2016. Numerically grounded language models for semantic error correction. CoRR.</p>
<p>Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang, and Wen-tau Yih. 2016. Learning from explicit and implicit supervision jointly for algebra word problems. In EMNLP.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In EMNLP-IJCNLP, Hong Kong, China. ACL.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In EMNLP.</p>
<p>Jie Yang and Yue Zhang. 2018. Ncrf++: An opensource neural sequence labeling toolkit. In $A C L$.</p>
<h2>A GMM Initialization</h2>
<p>Both EM and hard-EM are sensitive to initialization and we use the initialization methods described in (Blömer and Bujna, 2013). We first initialize the mean $\mu_{k}$ of the $k$-th Gaussian component using one of the following three strategies:</p>
<p>Random initialization choose $\mu_{k}$ from $X$ randomly. This is suitable when $X$ contains a wide range of numbers, e.g., numbers collected from Wikipedia.</p>
<p>SOM-based initialization initialize $\mu_{k}$ to $p_{k} \in$ $\mathbb{P}$ produced by the SOM method.</p>
<p>K-means initialization run randomly initialized k-means on $X$ and then use k-means centroids to initialize $\mu_{k}$.</p>
<p>We then assign the data samples to their closest means. The standard deviation of the data samples assigned to the $k$-th mean becomes $\sigma_{k}$.</p>
<h2>B Hyper-parameters</h2>
<p>We list all of the important hyper-parameters we tune for each model.</p>
<p>General hyper-parameters embedding dimension, context window size, SGD learning rate, batch size, vocabulary size, etc.</p>
<p>SOM hyper-parameters number of prototypes, stage of applying the log-squashing function (stage 1: before prototype induction; stage 2: only in the similarity function).</p>
<p>GMM hyper-parameters number of prototypes, whether we apply the log-squashing function to the numerals, EM initialization (from SOM, random initialization, or k-means initialization), type of EM (hard-EM or soft-EM).</p>
<p>We show the values of the SOM and GMM hyper-parameters in Table 7 and the values of the general hyper-parameters of all the methods in Table 8. We find that the general hyper-parameters influence the performance of our methods and the baselines in the same way, so in most cases, these hyper-parameters are set to be identical for all the methods. For large training corpora (Wiki1B, Numeracy-600k), we use 2048 as the batch size for D-LSTM, because D-LSTM consumes much more GPU memory. We set the batch size of the other methods to 4096. For the sequence labeling tasks, because the data is relatively small and confined to a very specific domain (chat log from online apparel shops), we set a small vocabulary size of 500 for all the methods except NumAsTok and set the vocabulary size of NumAsTok to 550 to ensure that different methods have similar numbers of parameters for word embedding training. Consequently, our methods have $(500+|\mathbb{P}|) \times D$ parameters for word embedding training and NumAsTok has $550 \times D$ parameters, where $\mathbb{P}$ is the prototype set, whose size is typically smaller than 50 , and $D$ is the embedding dimension.</p>
<p>Table 7 also shows that the optimal number of prototypes is around 200-500 for the Wiki1B corpus and 10-25 for the much smaller sequence labeling dataset. As a rule of thumb, we suggest setting the number of prototypes to $(\log N)^{2}$, where $N$ is the number of distinct numerals in the training corpus.</p>
<h2>C More Results on Wikipedia-1B</h2>
<p>We show the histograms of numerals in the Wikipedia-1B dataset and the prototypes learned by SOM and GMM in Fig.2. It can be seen that the prototypes induced by our methods have a similar distribution compared to the original numerals.</p>
<p>In addition, we select several typical numerals and non-numerical words and project their embeddings to 2D using t-SNE (Maaten and Hinton, 2008) (Figure 3). We use embeddings learned on</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">SOM <br> prototype number</th>
<th style="text-align: center;">log transform stage</th>
<th style="text-align: center;">prototype number</th>
<th style="text-align: center;">log transform</th>
<th style="text-align: center;">initialization</th>
<th style="text-align: center;">EM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Word similarity (Wiki1B)</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">hard</td>
</tr>
<tr>
<td style="text-align: center;">Magnitude (Wiki1B)</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">soft</td>
</tr>
<tr>
<td style="text-align: center;">Numeration (Wiki1B)</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">similarity function</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">True</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">soft</td>
</tr>
<tr>
<td style="text-align: center;">Numeral Prediction (Wiki1B)</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">similarity function</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">hard</td>
</tr>
<tr>
<td style="text-align: center;">Numeral Prediction (Numeracy-600k)</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">hard</td>
</tr>
<tr>
<td style="text-align: center;">Sequence Labeling $100 \%$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">soft</td>
</tr>
<tr>
<td style="text-align: center;">Sequence Labeling $30 \%$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">k-means</td>
<td style="text-align: center;">soft</td>
</tr>
<tr>
<td style="text-align: center;">Sequence Labeling $10 \%$</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">similarity function</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">from-som</td>
<td style="text-align: center;">soft</td>
</tr>
</tbody>
</table>
<p>Table 7: Hyper-parameter values for GMM and SOM based methods for each experiment.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">embed <br> dim</th>
<th style="text-align: left;">context <br> window</th>
<th style="text-align: left;">negative <br> samples</th>
<th style="text-align: left;">epoch</th>
<th style="text-align: left;">batch size</th>
<th style="text-align: left;">learning <br> rate</th>
<th style="text-align: left;">vocabulary <br> size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Word similarity (Wiki1B)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4096,2048</td>
<td style="text-align: left;">$5 \times 10^{-3}$</td>
<td style="text-align: left;">$3 \times 10^{5}$</td>
</tr>
<tr>
<td style="text-align: left;">Magnitude (-MAG) (Wiki1B)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4096,2048</td>
<td style="text-align: left;">$5 \times 10^{-3}$</td>
<td style="text-align: left;">$3 \times 10^{5}$</td>
</tr>
<tr>
<td style="text-align: left;">Numeration (-NUM) (Wiki1B)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4096,2048</td>
<td style="text-align: left;">$5 \times 10^{-3}$</td>
<td style="text-align: left;">$3 \times 10^{5}$</td>
</tr>
<tr>
<td style="text-align: left;">Numeral Prediction (Wiki1B)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">4096,2048</td>
<td style="text-align: left;">$5 \times 10^{-3}$</td>
<td style="text-align: left;">$3 \times 10^{5}$</td>
</tr>
<tr>
<td style="text-align: left;">Numeral Prediction (Numeracy-600k)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">4096,2048</td>
<td style="text-align: left;">$5 \times 10^{-3}$</td>
<td style="text-align: left;">$1 \times 10^{5}$</td>
</tr>
<tr>
<td style="text-align: left;">Sequence Labeling 100\% 30\% 10\%</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">$5 \times 10^{-2}$</td>
<td style="text-align: left;">500,550</td>
</tr>
</tbody>
</table>
<p>Table 8: Values of general hyper-parameters for each experiment.</p>
<p>Wikipedia-1B corpus using the SOM and GMM methods. The examples and the figures show that our model does capture some semantic relations between numeral quantities and normal words.</p>
<p>We show the training speed of each embedding method on the Wikipedia-1B dataset in Table 9. The batch size is set to 2048 for all the methods. Our methods are slower than NumAsTok but are faster than D-LSTM.</p>
<h2>D Augmented and Hard Test Sets in Sequence Labeling</h2>
<p>The augmented test set is created by reasonably perturbing the numerals in a sentence. For example, for a numeral '173' that describes height, we generate new samples by changing ' 173 ' to ' 174 ' or ' 175 ' while keeping the other non-numerical words in the sentence unchanged. For a decimal such as ' 1.7 meters', we change it to ' 1.6 ' or ' 1.8 '. The perturbation will not change the decimal places of numerals and will only change the quantity slightly, which makes the generated sentences reasonable.</p>
<p>The hard test set is created by manually collect 'hard' samples in the original test set. Hard samples do not have explicit patterns, meaning that a numeral's tag cannot be easily inferred by its adjacent words. For example, tags of numerals followed by units like 'cm', 'm', 'kg', 'years' and 'feet' can be figured out easily, so we exclude them from the hard test set. Customers are very likely to use ambiguous expressions like: 'I'm 16.5, can I buy 24?', where 16.5 is about foot length and 24 is the shoe
size. These ambiguous sentences are included in the hard test set.</p>
<h2>E Statistics of Sequence Labeling Dataset</h2>
<p>We show the statistics of the customer-service dataset in the Table 10. The vocabulary is small because the dataset is confined to a specific domain: online customer service chat log about apparel purchase. In this dataset, most of the sentences are about sizes of various kinds of clothes and are very short and ambiguous.</p>
<h2>F Sequence labeling result with standard deviation.</h2>
<p>We also show the sequence labeling result with standard deviation in Table 11. The standard deviation is small, so the result is stable.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Histograms of numerals and learned prototypes that range from 0 to $10^{13}$. The horizontal axis represents the numeral quantity and the vertical axis represents the number of occurrences, ' 500 ' means the number of prototypes, 'soft' means soft-EM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">SOM</th>
<th style="text-align: center;">GMM</th>
<th style="text-align: center;">NumAsTok</th>
<th style="text-align: center;">D-LSTM</th>
<th style="text-align: center;">Fixed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Speed (sent/s)</td>
<td style="text-align: center;">13590.93</td>
<td style="text-align: center;">12691.18</td>
<td style="text-align: center;">$\mathbf{2 2 9 0 7 . 9 7}$</td>
<td style="text-align: center;">8421.66</td>
<td style="text-align: center;">13055.08</td>
</tr>
</tbody>
</table>
<p>Table 9: Training speed for each methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of Sentences</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Original Test</td>
<td style="text-align: center;">Augmented Test</td>
<td style="text-align: center;">Hard Test</td>
</tr>
<tr>
<td style="text-align: center;">1389</td>
<td style="text-align: center;">793</td>
<td style="text-align: center;">1802</td>
<td style="text-align: center;">8052</td>
<td style="text-align: center;">726</td>
</tr>
<tr>
<td style="text-align: center;">Statistics of Training Set</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Token Vocab</td>
<td style="text-align: center;">Numeral Vocab</td>
<td style="text-align: center;">Avg sent length</td>
<td style="text-align: center;">Numeral Ratio</td>
<td style="text-align: center;">labels</td>
</tr>
<tr>
<td style="text-align: center;">505</td>
<td style="text-align: center;">234</td>
<td style="text-align: center;">10.42</td>
<td style="text-align: center;">15.89 \%</td>
<td style="text-align: center;">21</td>
</tr>
</tbody>
</table>
<p>Table 10: Statistics of customer-service dataset.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) t-SNE plot for embedding trained by the SOM-based method with 200 prototypes.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) t-SNE plot for embedding trained by the GMM-based method with 300 prototypes, random initialization and soft-EM training.</p>
<p>Figure 3: 2D t-SNE results for the SOM-based and GMM-based methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Augmented</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">97.12</td>
<td style="text-align: center;">91.19</td>
<td style="text-align: center;">90.46</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">97.02</td>
<td style="text-align: center;">91.28</td>
<td style="text-align: center;">90.18</td>
<td style="text-align: center;">90.72</td>
<td style="text-align: center;">96.19</td>
<td style="text-align: center;">86.66</td>
<td style="text-align: center;">85.91</td>
<td style="text-align: center;">86.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">$\pm 0.11$</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">$\pm 0.19$</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">$\pm 0.13$</td>
<td style="text-align: center;">$\pm 0.36$</td>
<td style="text-align: center;">$\pm 0.24$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">97.04</td>
<td style="text-align: center;">90.74</td>
<td style="text-align: center;">90.45</td>
<td style="text-align: center;">90.60</td>
<td style="text-align: center;">97.03</td>
<td style="text-align: center;">91.19</td>
<td style="text-align: center;">90.43</td>
<td style="text-align: center;">90.81</td>
<td style="text-align: center;">96.06</td>
<td style="text-align: center;">86.18</td>
<td style="text-align: center;">85.93</td>
<td style="text-align: center;">86.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.04$</td>
<td style="text-align: center;">$\pm 0.15$</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$\pm 0.11$</td>
<td style="text-align: center;">$\pm 0.13$</td>
<td style="text-align: center;">$\pm 0.09$</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$\pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">96.72</td>
<td style="text-align: center;">89.84</td>
<td style="text-align: center;">88.80</td>
<td style="text-align: center;">89.32</td>
<td style="text-align: center;">96.72</td>
<td style="text-align: center;">90.40</td>
<td style="text-align: center;">88.99</td>
<td style="text-align: center;">89.69</td>
<td style="text-align: center;">95.52</td>
<td style="text-align: center;">84.19</td>
<td style="text-align: center;">83.30</td>
<td style="text-align: center;">83.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$\pm 0.24$</td>
<td style="text-align: center;">$\pm 0.25$</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">$\pm 0.29$</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$\pm 0.66$</td>
<td style="text-align: center;">$\pm 0.55$</td>
<td style="text-align: center;">$\pm 0.60$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">95.75</td>
<td style="text-align: center;">86.19</td>
<td style="text-align: center;">87.42</td>
<td style="text-align: center;">86.80</td>
<td style="text-align: center;">95.86</td>
<td style="text-align: center;">87.13</td>
<td style="text-align: center;">87.65</td>
<td style="text-align: center;">87.39</td>
<td style="text-align: center;">93.97</td>
<td style="text-align: center;">78.39</td>
<td style="text-align: center;">80.18</td>
<td style="text-align: center;">79.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.11$</td>
<td style="text-align: center;">$\pm 0.38$</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$\pm 0.29$</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">$\pm 0.25$</td>
<td style="text-align: center;">$\pm 0.28$</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$\pm 0.16$</td>
<td style="text-align: center;">$\pm 0.46$</td>
<td style="text-align: center;">$\pm 0.43$</td>
<td style="text-align: center;">$\pm 0.45$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">96.88</td>
<td style="text-align: center;">91.37</td>
<td style="text-align: center;">89.29</td>
<td style="text-align: center;">90.32</td>
<td style="text-align: center;">96.36</td>
<td style="text-align: center;">90.99</td>
<td style="text-align: center;">87.39</td>
<td style="text-align: center;">89.15</td>
<td style="text-align: center;">96.00</td>
<td style="text-align: center;">87.11</td>
<td style="text-align: center;">85.12</td>
<td style="text-align: center;">86.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">$\pm 0.40$</td>
<td style="text-align: center;">$\pm 0.09$</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">$\pm 0.09$</td>
<td style="text-align: center;">$\pm 0.20$</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">$\pm 0.52$</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">$\pm 0.26$</td>
</tr>
<tr>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">96.21</td>
<td style="text-align: center;">89.55</td>
<td style="text-align: center;">86.07</td>
<td style="text-align: center;">87.78</td>
<td style="text-align: center;">95.92</td>
<td style="text-align: center;">89.07</td>
<td style="text-align: center;">85.33</td>
<td style="text-align: center;">87.16</td>
<td style="text-align: center;">95.27</td>
<td style="text-align: center;">84.42</td>
<td style="text-align: center;">81.62</td>
<td style="text-align: center;">82.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">$\pm 0.15$</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$\pm 0.24$</td>
<td style="text-align: center;">$\pm 0.09$</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$\pm 0.40$</td>
<td style="text-align: center;">$\pm 0.36$</td>
<td style="text-align: center;">$\pm 0.13$</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">$\pm 0.47$</td>
<td style="text-align: center;">$\pm 0.43$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">96.20</td>
<td style="text-align: center;">89.50</td>
<td style="text-align: center;">86.18</td>
<td style="text-align: center;">87.81</td>
<td style="text-align: center;">95.88</td>
<td style="text-align: center;">89.12</td>
<td style="text-align: center;">85.29</td>
<td style="text-align: center;">87.16</td>
<td style="text-align: center;">95.23</td>
<td style="text-align: center;">84.44</td>
<td style="text-align: center;">81.50</td>
<td style="text-align: center;">82.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">$\pm 0.29$</td>
<td style="text-align: center;">$\pm 0.08$</td>
<td style="text-align: center;">$\pm 0.08$</td>
<td style="text-align: center;">$\pm 0.16$</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">$\pm 0.13$</td>
<td style="text-align: center;">$\pm 0.02$</td>
<td style="text-align: center;">$\pm 0.30$</td>
<td style="text-align: center;">$\pm 0.22$</td>
<td style="text-align: center;">$\pm 0.09$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">95.55</td>
<td style="text-align: center;">86.83</td>
<td style="text-align: center;">83.88</td>
<td style="text-align: center;">85.33</td>
<td style="text-align: center;">95.30</td>
<td style="text-align: center;">86.22</td>
<td style="text-align: center;">83.13</td>
<td style="text-align: center;">84.64</td>
<td style="text-align: center;">94.32</td>
<td style="text-align: center;">80.10</td>
<td style="text-align: center;">78.17</td>
<td style="text-align: center;">79.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.08$</td>
<td style="text-align: center;">$\pm 0.29$</td>
<td style="text-align: center;">$\pm 0.36$</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">$\pm 0.46$</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">$\pm 0.33$</td>
<td style="text-align: center;">$\pm 0.39$</td>
<td style="text-align: center;">$\pm 0.35$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">94.67</td>
<td style="text-align: center;">83.51</td>
<td style="text-align: center;">82.69</td>
<td style="text-align: center;">83.10</td>
<td style="text-align: center;">94.48</td>
<td style="text-align: center;">83.40</td>
<td style="text-align: center;">82.02</td>
<td style="text-align: center;">82.71</td>
<td style="text-align: center;">92.92</td>
<td style="text-align: center;">75.03</td>
<td style="text-align: center;">75.18</td>
<td style="text-align: center;">75.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$\pm 0.08$</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">$\pm 0.38$</td>
<td style="text-align: center;">$\pm 0.17$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">95.58</td>
<td style="text-align: center;">89.18</td>
<td style="text-align: center;">83.55</td>
<td style="text-align: center;">86.27</td>
<td style="text-align: center;">94.57</td>
<td style="text-align: center;">88.39</td>
<td style="text-align: center;">79.94</td>
<td style="text-align: center;">83.95</td>
<td style="text-align: center;">94.65</td>
<td style="text-align: center;">84.42</td>
<td style="text-align: center;">79.06</td>
<td style="text-align: center;">81.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">$\pm 0.25$</td>
<td style="text-align: center;">$\pm 0.31$</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">$\pm 0.40$</td>
<td style="text-align: center;">$\pm 0.16$</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">$\pm 0.39$</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">$\pm 0.10$</td>
</tr>
<tr>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">93.43</td>
<td style="text-align: center;">82.36</td>
<td style="text-align: center;">75.01</td>
<td style="text-align: center;">78.51</td>
<td style="text-align: center;">92.78</td>
<td style="text-align: center;">81.48</td>
<td style="text-align: center;">72.85</td>
<td style="text-align: center;">76.92</td>
<td style="text-align: center;">93.19</td>
<td style="text-align: center;">80.26</td>
<td style="text-align: center;">72.71</td>
<td style="text-align: center;">76.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">$\pm 0.52$</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">$\pm 0.25$</td>
<td style="text-align: center;">$\pm 0.36$</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$\pm 0.04$</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">$\pm 0.09$</td>
<td style="text-align: center;">$\pm 0.19$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOM</td>
<td style="text-align: center;">93.48</td>
<td style="text-align: center;">82.13</td>
<td style="text-align: center;">75.11</td>
<td style="text-align: center;">78.46</td>
<td style="text-align: center;">92.87</td>
<td style="text-align: center;">80.96</td>
<td style="text-align: center;">73.22</td>
<td style="text-align: center;">76.89</td>
<td style="text-align: center;">93.24</td>
<td style="text-align: center;">79.47</td>
<td style="text-align: center;">73.04</td>
<td style="text-align: center;">76.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.11$</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">$\pm 0.21$</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">$\pm 0.25$</td>
<td style="text-align: center;">$\pm 0.37$</td>
<td style="text-align: center;">$\pm 0.19$</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">$\pm 0.49$</td>
<td style="text-align: center;">$\pm 0.30$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D-LSTM</td>
<td style="text-align: center;">92.53</td>
<td style="text-align: center;">77.71</td>
<td style="text-align: center;">71.45</td>
<td style="text-align: center;">74.45</td>
<td style="text-align: center;">91.99</td>
<td style="text-align: center;">76.24</td>
<td style="text-align: center;">69.96</td>
<td style="text-align: center;">72.96</td>
<td style="text-align: center;">92.10</td>
<td style="text-align: center;">73.26</td>
<td style="text-align: center;">68.72</td>
<td style="text-align: center;">70.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.19$</td>
<td style="text-align: center;">$\pm 0.38$</td>
<td style="text-align: center;">$\pm 0.82$</td>
<td style="text-align: center;">$\pm 0.61$</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$\pm 0.40$</td>
<td style="text-align: center;">$\pm 0.11$</td>
<td style="text-align: center;">$\pm 0.80$</td>
<td style="text-align: center;">$\pm 0.16$</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">$\pm 0.70$</td>
<td style="text-align: center;">$\pm 0.40$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fixed</td>
<td style="text-align: center;">91.90</td>
<td style="text-align: center;">75.39</td>
<td style="text-align: center;">71.41</td>
<td style="text-align: center;">73.34</td>
<td style="text-align: center;">91.48</td>
<td style="text-align: center;">73.96</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">72.02</td>
<td style="text-align: center;">91.06</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">67.47</td>
<td style="text-align: center;">68.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">$\pm 0.46$</td>
<td style="text-align: center;">$\pm 0.58$</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$\pm 0.64$</td>
<td style="text-align: center;">$\pm 0.73$</td>
<td style="text-align: center;">$\pm 0.40$</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">$\pm 0.78$</td>
<td style="text-align: center;">$\pm 0.27$</td>
<td style="text-align: center;">$\pm 0.25$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumAsTok</td>
<td style="text-align: center;">92.31</td>
<td style="text-align: center;">81.98</td>
<td style="text-align: center;">70.51</td>
<td style="text-align: center;">75.81</td>
<td style="text-align: center;">90.77</td>
<td style="text-align: center;">80.10</td>
<td style="text-align: center;">64.95</td>
<td style="text-align: center;">71.71</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">79.64</td>
<td style="text-align: center;">67.95</td>
<td style="text-align: center;">73.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm 0.12$</td>
<td style="text-align: center;">$\pm 0.44$</td>
<td style="text-align: center;">$\pm 0.56$</td>
<td style="text-align: center;">$\pm 0.29$</td>
<td style="text-align: center;">$\pm 0.14$</td>
<td style="text-align: center;">$\pm 0.65$</td>
<td style="text-align: center;">$\pm 0.66$</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">$\pm 0.64$</td>
<td style="text-align: center;">$\pm 0.38$</td>
<td style="text-align: center;">$\pm 0.20$</td>
</tr>
</tbody>
</table>
<p>Table 11: The results of sequence labeling. We report the accuracy, precision, recall, F1 score for the original, augmented, and harder test sets with different training data sizes. Accuracy is in the token level and the other metrics are in the entity level.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/jiesutd/NCRFpp&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-937 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-937</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-937</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-98ce7af921e7c52d81df64d632d34eb09522cd75</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/98ce7af921e7c52d81df64d632d34eb09522cd75" target="_blank">Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A framework named Dynamic LLM-Agent Network ( DyLAN) is built for LLM-agent collaboration on complicated tasks like reasoning and code generation with reasonable computational cost and an automatic agent team optimization algorithm based on an unsupervised metric termed Agent Importance Score is designed.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e937.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e937.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DyLAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic LLM-Powered Agent Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage multi-agent framework that (1) performs unsupervised team optimization to select contributory agents via Agent Importance Score, and (2) runs dynamic, temporal feed-forward multi-agent collaborations (T-FFN) with agent reformation and early-stopping to solve tasks (QA, code generation, decision-making, reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DyLAN (framework running LLM agents: gpt-35-turbo / GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dynamic multi-agent architecture expressed as a Temporal Feed-Forward Network (T-FFN). Supports role-based agents (LLM-powered or tools), an LLM ranker for agent reformation, forward/backward message passing, Agent Importance Score for unsupervised team selection, and early-stopping based on consensus/BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>MMLU (General Reasoning) ; MATH (Arithmetic Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>MMLU (GR) overall 70.5% accuracy (DyLAN) vs 66.4% single execution; MATH (AR) 35.7% accuracy (DyLAN)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>HumanEval (Code Generation) ; WebShop (Decision Making / sequential web decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code generation (tool-augmented multi-step generation) ; sequential decision-making / web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>HumanEval Pass@1 82.9% (DyLAN) vs 76.2% single-exec baseline; WebShop reward 68.3 (DyLAN) vs 50.6 direct execution; WebShop success rate improvement reported (DyLAN success higher vs baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Temporal Feed-Forward Network (T-FFN); dynamic agent reformation via LLM ranker; Agent Importance Score selection (backward propagation of peer ratings); early-stopping consensus mechanism; tool interfaces (code interpreter) for some agents.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context prompting (no additional supervised fine-tuning or RL reported); experiments use GPT-3.5 / GPT-4 API prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (dynamic multi-agent architecture) + algorithmic intervention (unsupervised agent selection) + runtime mechanisms (early-stopping, LLM ranker for reformation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Two-stage intervention: (1) Team Optimization uses a primary trial to compute Agent Importance Scores via forward peer ratings and backward aggregation to select top-k agents for the task; (2) Task Solving uses the optimized team in a T-FFN with dynamic agent reformation (LLM ranker chooses top agents per timestep) and early-stopping when >2/3 consensus is reached. Tools (code interpreters) are attached to some agents in code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved interactive and QA performance: HumanEval Pass@1 improved from 76.2% to 82.9% after team optimization (12→8 agents); WebShop reward improved from 50.6 to 68.3 (Direct Execution→DyLAN); MMLU subjects: per-subject improvements up to +25.0% accuracy after agent selection; early-stopping reduced #API calls substantially (examples: AR -45.0%, GR -66.2%, CG -11.3%, DM -54.2%) and slightly improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper hypothesizes that (a) knowledge-intensive QA (e.g., arithmetic/math) benefits less from multi-agent collaboration because of high knowledge dependency and less-from collaborative dynamics, (b) sequential/single-thread architectures (e.g., PHP, Reflexion) are vulnerable to incorrect intermediates which can amplify errors, while DyLAN's parallel feed-forward layers, peer rating, reformation and early-stopping mitigate interference and transient hallucinations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e937.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e937.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent Importance Score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Importance Score (IAS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised metric to quantify each agent's contribution in multi-round collaborations by collecting peer 1–5 ratings (normalized) in forward pass and aggregating contributions backward over the T-FFN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Agent Importance Score (unsupervised selection metric)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Peer-rating based scoring: at inference-time each successor rates predecessors' responses (normalized scores); initialize contributions at final layer and propagate backward with weights to compute per-agent importance summed across timesteps; used to select top-k agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>MMLU (used to select agents per subject)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Using IAS-based selection: overall (five-subject aggregate) accuracy improved to 73.6% vs 63.5% before optimization on selected subjects; individual subject improvements up to +25.0% (college mathematics: 40.0→65.0).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>HumanEval (code generation) ; WebShop (decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code generation (tool use) ; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>HumanEval Pass@1 improved from 76.2% to 82.9% when team optimized by IAS; WebShop reward improved (examples above). Also reduced #API calls (CG 23.04→16.85; DM 32.03→24.85).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Not an architecture but an algorithmic selection: backward aggregation inspired by neuron importance propagation; uses LLM-based peer ratings as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>unsupervised evaluation via prompting (no training)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>algorithmic intervention (team optimization / selection)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Compute Agent Importance Scores from a primary trial by (1) forward propagation: successors rate predecessors' outputs; (2) backward aggregation: contributions propagate backwards weighted by peer ratings; (3) selection: sum contributions across timesteps and pick top-k agents for subsequent task solving.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Selecting top agents by IAS produced notable gains: HumanEval Pass@1 +6.7 (76.2→82.9) and large per-subject gains on MMLU (up to +25.0); also reduced API calls and produced more efficient collaborations.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>IAS reduces interference from irrelevant or low-quality agents, addressing performance drops that arise when static, heterogeneous agent teams include low-contributing members; by optimizing team composition, interactive/procedural tasks benefit more where noisy agents can cause harmful operations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e937.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e937.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T-FFN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Feed-Forward Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-layer directed acyclic graph abstraction for multi-agent collaboration where each layer is a time-step and nodes are agent instances, with edges indicating communication channels between times; used as both communication structure and computation graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Temporal Feed-Forward Network (T-FFN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Layered feed-forward structure: nodes v_{t,i} correspond to agent a_i at timestep t; edges link adjacent time-steps only (by default). Message passing defined as aggregator functions; supports dynamic edge addition via agent reformation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>MMLU (used to structure multi-agent reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Enables DyLAN's MMLU performance (70.5% overall) and subject-level improvements; not a standalone model so no separate metric.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>HumanEval (code generation) ; WebShop (decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code generation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Facilitated DyLAN's interactive performance improvements (HumanEval Pass@1 82.9%; WebShop reward 68.3).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>feed-forward multi-agent layers (parallel solutions possible), allows agent reformation (dynamic edges), supports peer rating aggregation and early-stopping consensus checks</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Replacing or formulating agent collaboration as feed-forward temporal layers (rather than single-thread sequential pipelines) to allow parallel solution generation and independent peer evaluation; supports dynamic pruning of agents per timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Claimed benefits: better trade-off between efficiency and effectiveness vs sequential methods (e.g., PHP, Reflexion). DyLAN uses T-FFN plus early-stopping to lower API calls and reduce error propagation that occurs in sequential single-thread methods.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Sequential/chain architectures can amplify single incorrect intermediate steps; T-FFN's parallelism and peer-rating reduce this amplification, improving interactive/procedural task robustness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e937.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e937.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent Team Reformation (LLM Ranker)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Team Reformation using an LLM Ranker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A runtime mechanism that uses an additional LLM (the LLM Ranker) to rank current timestep agent outputs and allow only top-k agents to participate in the next timestep (dynamic edge addition/pruning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLM Ranker (listwise ranking prompt executed by an LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompt-driven listwise ranker implemented with GPT-3.5 used to rank agent messages each timestep; top-k messages determine which agents continue to next timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>MMLU (applied during collaborative reasoning rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Contributes to DyLAN's MMLU accuracy improvements (overall 70.5% reported); no isolated metric for ranker alone.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>HumanEval (code generation) ; WebShop (decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code-review-driven generation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used to filter low-performing agents dynamically, contributing to improvements in HumanEval (Pass@1 82.9%) and WebShop reward (68.3).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>runtime ranking module (LLM-as-evaluator), dynamic pruning of agent participation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (LLM ranker is prompted to compare and rank outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>runtime/architectural intervention (dynamic pruning via LLM-based ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>At each reformation timestep, the LLM ranker compares agent outputs and selects top-k to proceed; edges in the T-FFN are restricted to the selected agents forming a dynamic communication graph.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves correctness by deactivating low-performing agents (reduces hallucination/invalid actions), and in conjunction with early-stopping reduces API calls; ablation shows agent team reformation critical to final answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>By filtering transient low-quality outputs, reformation prevents propagation of invalid actions common in interactive tasks (e.g., environment actions), reducing an important source of the QA vs interactive performance gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e937.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e937.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Early-stopping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early-stopping consensus mechanism (DyLAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stopping rule that terminates the multi-agent rounds when >2/3 of agents in a layer reach a consistent answer (or BLEU threshold for open generation), reducing unnecessary rounds and API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DyLAN early-stopping mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A runtime termination criterion inspired by Byzantine consensus: when over two-thirds of agents in a layer agree (identical for classification/decision tasks or BLEU threshold for generation) the process stops.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>MMLU ; MATH</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Slight performance improvement reported and large reductions in API calls; for GR and AR tasks early-stopping provided efficiency gains while slightly improving or maintaining accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>HumanEval (code generation) ; WebShop (decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>open-ended generation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Early-stopping reduced #API calls: AR -45.0%, GR -66.2%, CG -11.3%, DM -54.2% (reported). Slight performance gains in some tasks but less effective for open-ended code generation due to format variance.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>consensus check across agent outputs; BLEU-based similarity for open generation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>runtime mechanism / architectural control</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Stop further timesteps when a sufficient consensus across agents is detected (>2/3 agreement), reducing rounds and calls; for generation tasks uses BLEU threshold to decide consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Large efficiency gains (up to ~66% fewer API calls in GR) and minor accuracy improvements; for CG less effective due to syntactic/format differences among code outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Early-stopping reduces wasteful interaction rounds and reduces error propagation, thus improving efficiency and sometimes correctness compared to longer sequential interactions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e937.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e937.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single Execution Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single LLM Execution (single-instance baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that queries a single LLM instance (GPT-3.5/GPT-4) with chain-of-thought or standard prompting to produce answers; used as a point of comparison for multi-agent methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Single LLM execution (gpt-35-turbo / GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-agent prompting (CoT or direct) runs with one LLM call per query; no multi-agent collaboration, no team selection, no dynamic reformation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>MMLU (GR) ; MATH (AR)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>MMLU overall 66.4% (single execution reported); MATH baseline 31.6% under CoT (example in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>HumanEval (code generation) ; WebShop (decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code generation ; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>HumanEval Pass@1 reported 76.2% in some settings; WebShop reward 50.6 for direct execution.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Codet: Code generation with generated tests <em>(Rating: 1)</em></li>
                <li>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors <em>(Rating: 2)</em></li>
                <li>CAMEL: Communicative agents for 'mind' exploration of large language model society <em>(Rating: 2)</em></li>
                <li>Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-937",
    "paper_id": "paper-98ce7af921e7c52d81df64d632d34eb09522cd75",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "DyLAN",
            "name_full": "Dynamic LLM-Powered Agent Network",
            "brief_description": "A two-stage multi-agent framework that (1) performs unsupervised team optimization to select contributory agents via Agent Importance Score, and (2) runs dynamic, temporal feed-forward multi-agent collaborations (T-FFN) with agent reformation and early-stopping to solve tasks (QA, code generation, decision-making, reasoning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "DyLAN (framework running LLM agents: gpt-35-turbo / GPT-4)",
            "model_description": "Dynamic multi-agent architecture expressed as a Temporal Feed-Forward Network (T-FFN). Supports role-based agents (LLM-powered or tools), an LLM ranker for agent reformation, forward/backward message passing, Agent Importance Score for unsupervised team selection, and early-stopping based on consensus/BLEU.",
            "model_size": null,
            "qa_task_name": "MMLU (General Reasoning) ; MATH (Arithmetic Reasoning)",
            "qa_performance": "MMLU (GR) overall 70.5% accuracy (DyLAN) vs 66.4% single execution; MATH (AR) 35.7% accuracy (DyLAN)",
            "interactive_task_name": "HumanEval (Code Generation) ; WebShop (Decision Making / sequential web decision-making)",
            "interactive_task_type": "code generation (tool-augmented multi-step generation) ; sequential decision-making / web navigation",
            "interactive_performance": "HumanEval Pass@1 82.9% (DyLAN) vs 76.2% single-exec baseline; WebShop reward 68.3 (DyLAN) vs 50.6 direct execution; WebShop success rate improvement reported (DyLAN success higher vs baseline)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Temporal Feed-Forward Network (T-FFN); dynamic agent reformation via LLM ranker; Agent Importance Score selection (backward propagation of peer ratings); early-stopping consensus mechanism; tool interfaces (code interpreter) for some agents.",
            "training_method": "prompting / in-context prompting (no additional supervised fine-tuning or RL reported); experiments use GPT-3.5 / GPT-4 API prompting",
            "intervention_type": "architectural change (dynamic multi-agent architecture) + algorithmic intervention (unsupervised agent selection) + runtime mechanisms (early-stopping, LLM ranker for reformation)",
            "intervention_description": "Two-stage intervention: (1) Team Optimization uses a primary trial to compute Agent Importance Scores via forward peer ratings and backward aggregation to select top-k agents for the task; (2) Task Solving uses the optimized team in a T-FFN with dynamic agent reformation (LLM ranker chooses top agents per timestep) and early-stopping when &gt;2/3 consensus is reached. Tools (code interpreters) are attached to some agents in code generation.",
            "intervention_effect": "Improved interactive and QA performance: HumanEval Pass@1 improved from 76.2% to 82.9% after team optimization (12→8 agents); WebShop reward improved from 50.6 to 68.3 (Direct Execution→DyLAN); MMLU subjects: per-subject improvements up to +25.0% accuracy after agent selection; early-stopping reduced #API calls substantially (examples: AR -45.0%, GR -66.2%, CG -11.3%, DM -54.2%) and slightly improved performance.",
            "hypothesized_cause_of_gap": "Paper hypothesizes that (a) knowledge-intensive QA (e.g., arithmetic/math) benefits less from multi-agent collaboration because of high knowledge dependency and less-from collaborative dynamics, (b) sequential/single-thread architectures (e.g., PHP, Reflexion) are vulnerable to incorrect intermediates which can amplify errors, while DyLAN's parallel feed-forward layers, peer rating, reformation and early-stopping mitigate interference and transient hallucinations.",
            "uuid": "e937.0"
        },
        {
            "name_short": "Agent Importance Score",
            "name_full": "Agent Importance Score (IAS)",
            "brief_description": "An unsupervised metric to quantify each agent's contribution in multi-round collaborations by collecting peer 1–5 ratings (normalized) in forward pass and aggregating contributions backward over the T-FFN.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Agent Importance Score (unsupervised selection metric)",
            "model_description": "Peer-rating based scoring: at inference-time each successor rates predecessors' responses (normalized scores); initialize contributions at final layer and propagate backward with weights to compute per-agent importance summed across timesteps; used to select top-k agents.",
            "model_size": null,
            "qa_task_name": "MMLU (used to select agents per subject)",
            "qa_performance": "Using IAS-based selection: overall (five-subject aggregate) accuracy improved to 73.6% vs 63.5% before optimization on selected subjects; individual subject improvements up to +25.0% (college mathematics: 40.0→65.0).",
            "interactive_task_name": "HumanEval (code generation) ; WebShop (decision-making)",
            "interactive_task_type": "code generation (tool use) ; sequential decision-making",
            "interactive_performance": "HumanEval Pass@1 improved from 76.2% to 82.9% when team optimized by IAS; WebShop reward improved (examples above). Also reduced #API calls (CG 23.04→16.85; DM 32.03→24.85).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Not an architecture but an algorithmic selection: backward aggregation inspired by neuron importance propagation; uses LLM-based peer ratings as inputs.",
            "training_method": "unsupervised evaluation via prompting (no training)",
            "intervention_type": "algorithmic intervention (team optimization / selection)",
            "intervention_description": "Compute Agent Importance Scores from a primary trial by (1) forward propagation: successors rate predecessors' outputs; (2) backward aggregation: contributions propagate backwards weighted by peer ratings; (3) selection: sum contributions across timesteps and pick top-k agents for subsequent task solving.",
            "intervention_effect": "Selecting top agents by IAS produced notable gains: HumanEval Pass@1 +6.7 (76.2→82.9) and large per-subject gains on MMLU (up to +25.0); also reduced API calls and produced more efficient collaborations.",
            "hypothesized_cause_of_gap": "IAS reduces interference from irrelevant or low-quality agents, addressing performance drops that arise when static, heterogeneous agent teams include low-contributing members; by optimizing team composition, interactive/procedural tasks benefit more where noisy agents can cause harmful operations.",
            "uuid": "e937.1"
        },
        {
            "name_short": "T-FFN",
            "name_full": "Temporal Feed-Forward Network",
            "brief_description": "A multi-layer directed acyclic graph abstraction for multi-agent collaboration where each layer is a time-step and nodes are agent instances, with edges indicating communication channels between times; used as both communication structure and computation graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Temporal Feed-Forward Network (T-FFN)",
            "model_description": "Layered feed-forward structure: nodes v_{t,i} correspond to agent a_i at timestep t; edges link adjacent time-steps only (by default). Message passing defined as aggregator functions; supports dynamic edge addition via agent reformation.",
            "model_size": null,
            "qa_task_name": "MMLU (used to structure multi-agent reasoning)",
            "qa_performance": "Enables DyLAN's MMLU performance (70.5% overall) and subject-level improvements; not a standalone model so no separate metric.",
            "interactive_task_name": "HumanEval (code generation) ; WebShop (decision-making)",
            "interactive_task_type": "code generation; sequential decision-making",
            "interactive_performance": "Facilitated DyLAN's interactive performance improvements (HumanEval Pass@1 82.9%; WebShop reward 68.3).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "feed-forward multi-agent layers (parallel solutions possible), allows agent reformation (dynamic edges), supports peer rating aggregation and early-stopping consensus checks",
            "training_method": null,
            "intervention_type": "architectural change",
            "intervention_description": "Replacing or formulating agent collaboration as feed-forward temporal layers (rather than single-thread sequential pipelines) to allow parallel solution generation and independent peer evaluation; supports dynamic pruning of agents per timestep.",
            "intervention_effect": "Claimed benefits: better trade-off between efficiency and effectiveness vs sequential methods (e.g., PHP, Reflexion). DyLAN uses T-FFN plus early-stopping to lower API calls and reduce error propagation that occurs in sequential single-thread methods.",
            "hypothesized_cause_of_gap": "Sequential/chain architectures can amplify single incorrect intermediate steps; T-FFN's parallelism and peer-rating reduce this amplification, improving interactive/procedural task robustness.",
            "uuid": "e937.2"
        },
        {
            "name_short": "Agent Team Reformation (LLM Ranker)",
            "name_full": "Agent Team Reformation using an LLM Ranker",
            "brief_description": "A runtime mechanism that uses an additional LLM (the LLM Ranker) to rank current timestep agent outputs and allow only top-k agents to participate in the next timestep (dynamic edge addition/pruning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "LLM Ranker (listwise ranking prompt executed by an LLM)",
            "model_description": "A prompt-driven listwise ranker implemented with GPT-3.5 used to rank agent messages each timestep; top-k messages determine which agents continue to next timestep.",
            "model_size": null,
            "qa_task_name": "MMLU (applied during collaborative reasoning rounds)",
            "qa_performance": "Contributes to DyLAN's MMLU accuracy improvements (overall 70.5% reported); no isolated metric for ranker alone.",
            "interactive_task_name": "HumanEval (code generation) ; WebShop (decision-making)",
            "interactive_task_type": "code-review-driven generation; sequential decision-making",
            "interactive_performance": "Used to filter low-performing agents dynamically, contributing to improvements in HumanEval (Pass@1 82.9%) and WebShop reward (68.3).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "runtime ranking module (LLM-as-evaluator), dynamic pruning of agent participation",
            "training_method": "prompting only (LLM ranker is prompted to compare and rank outputs)",
            "intervention_type": "runtime/architectural intervention (dynamic pruning via LLM-based ranking)",
            "intervention_description": "At each reformation timestep, the LLM ranker compares agent outputs and selects top-k to proceed; edges in the T-FFN are restricted to the selected agents forming a dynamic communication graph.",
            "intervention_effect": "Improves correctness by deactivating low-performing agents (reduces hallucination/invalid actions), and in conjunction with early-stopping reduces API calls; ablation shows agent team reformation critical to final answer correctness.",
            "hypothesized_cause_of_gap": "By filtering transient low-quality outputs, reformation prevents propagation of invalid actions common in interactive tasks (e.g., environment actions), reducing an important source of the QA vs interactive performance gap.",
            "uuid": "e937.3"
        },
        {
            "name_short": "Early-stopping",
            "name_full": "Early-stopping consensus mechanism (DyLAN)",
            "brief_description": "A stopping rule that terminates the multi-agent rounds when &gt;2/3 of agents in a layer reach a consistent answer (or BLEU threshold for open generation), reducing unnecessary rounds and API calls.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "DyLAN early-stopping mechanism",
            "model_description": "A runtime termination criterion inspired by Byzantine consensus: when over two-thirds of agents in a layer agree (identical for classification/decision tasks or BLEU threshold for generation) the process stops.",
            "model_size": null,
            "qa_task_name": "MMLU ; MATH",
            "qa_performance": "Slight performance improvement reported and large reductions in API calls; for GR and AR tasks early-stopping provided efficiency gains while slightly improving or maintaining accuracy.",
            "interactive_task_name": "HumanEval (code generation) ; WebShop (decision-making)",
            "interactive_task_type": "open-ended generation; sequential decision-making",
            "interactive_performance": "Early-stopping reduced #API calls: AR -45.0%, GR -66.2%, CG -11.3%, DM -54.2% (reported). Slight performance gains in some tasks but less effective for open-ended code generation due to format variance.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": false,
            "architectural_features": "consensus check across agent outputs; BLEU-based similarity for open generation",
            "training_method": null,
            "intervention_type": "runtime mechanism / architectural control",
            "intervention_description": "Stop further timesteps when a sufficient consensus across agents is detected (&gt;2/3 agreement), reducing rounds and calls; for generation tasks uses BLEU threshold to decide consistency.",
            "intervention_effect": "Large efficiency gains (up to ~66% fewer API calls in GR) and minor accuracy improvements; for CG less effective due to syntactic/format differences among code outputs.",
            "hypothesized_cause_of_gap": "Early-stopping reduces wasteful interaction rounds and reduces error propagation, thus improving efficiency and sometimes correctness compared to longer sequential interactions.",
            "uuid": "e937.4"
        },
        {
            "name_short": "Single Execution Baseline",
            "name_full": "Single LLM Execution (single-instance baseline)",
            "brief_description": "Baseline that queries a single LLM instance (GPT-3.5/GPT-4) with chain-of-thought or standard prompting to produce answers; used as a point of comparison for multi-agent methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Single LLM execution (gpt-35-turbo / GPT-4)",
            "model_description": "Single-agent prompting (CoT or direct) runs with one LLM call per query; no multi-agent collaboration, no team selection, no dynamic reformation.",
            "model_size": null,
            "qa_task_name": "MMLU (GR) ; MATH (AR)",
            "qa_performance": "MMLU overall 66.4% (single execution reported); MATH baseline 31.6% under CoT (example in tables).",
            "interactive_task_name": "HumanEval (code generation) ; WebShop (decision-making)",
            "interactive_task_type": "code generation ; sequential decision-making",
            "interactive_performance": "HumanEval Pass@1 reported 76.2% in some settings; WebShop reward 50.6 for direct execution.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": "prompting / in-context prompting",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e937.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "LLM-blender: Ensembling large language models with pairwise ranking and generative fusion",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Codet: Code generation with generated tests",
            "rating": 1
        },
        {
            "paper_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors",
            "rating": 2
        },
        {
            "paper_title": "CAMEL: Communicative agents for 'mind' exploration of large language model society",
            "rating": 2
        },
        {
            "paper_title": "Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents",
            "rating": 1
        }
    ],
    "cost": 0.016427999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration</h1>
<p>Zijun Liu ${ }^{1}$, Yanzhe Zhang ${ }^{2}$, Peng $\mathbf{L i}^{3}$, Yang Liu ${ }^{1,3,4}$, Diyi Yang ${ }^{5}$<br>${ }^{1}$ Dept. of Comp. Sci. \&amp; Tech., Institute for AI, Tsinghua University, Beijing, China<br>${ }^{2}$ Georgia Institute of Technology, Georgia, USA<br>${ }^{3}$ Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China<br>${ }^{4}$ Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China<br>${ }^{5}$ Stanford University, California, USA<br>liuzi jun2@@mails.tsinghua.edu.cn, diyiy@stanford.edu</p>
<h4>Abstract</h4>
<p>Recent studies show that collaborating multiple large language model (LLM) powered agents is a promising way for task solving. However, current approaches are constrained by using a fixed number of agents and static communication structures. In this work, we propose automatically selecting a team of agents from candidates to collaborate in a dynamic communication structure toward different tasks and domains. Specifically, we build a framework named Dynamic LLM-Powered Agent Network (DyLAN) for LLM-powered agent collaboration, operating a two-stage paradigm: (1) Team Optimization and (2) Task Solving. During the first stage, we utilize an agent selection algorithm, based on an unsupervised metric called Agent Importance Score, enabling the selection of best agents according to their contributions in a preliminary trial, oriented to the given task. Then, in the second stage, the selected agents collaborate dynamically according to the query. Empirically, we demonstrate that DyLAN outperforms strong baselines in code generation, decision-making, general reasoning, and arithmetic reasoning tasks with moderate computational cost. On specific subjects in MMLU, selecting a team of agents in the team optimization stage improves accuracy by up to $25.0 \%$ in DyLAN. ${ }^{1}$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: DyLAN adopts a two-stage paradigm. Agents communicate in a structure of the temporal feed-forward network (T-FFN). At the "Team Optimization" stage, DyLAN performs agent selection for the most contributory agents in a primary collaboration, oriented to tasks or domains. The selected agents then collaborate dynamically for an answer on the given query at the "Task Solving" stage.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Large Language Model (LLM) agents (Richards \&amp; et al., 2023; Nakajima, 2023; Reworkd, 2023) have demonstrated promising performance on various tasks, ranging from reasoning (Yao et al., 2023), code generation (Shinn et al., 2023) to embodied tasks such as video gaming (Wang et al., 2023a) and autopilot systems (Jin et al., 2023). Given the impracticality of a single agent managing all these tasks efficiently, recent research has shifted towards multi-agent collaborations, yielding significant advancements (Li et al., 2023; Du et al., 2023; Wang et al., 2023c; Jiang et al., 2023; Shinn et al., 2023; Chen et al., 2024; Wu et al., 2023).
As an analogy to human society, how human teams function may provide valuable insights for developing more effective multi-agent collaboration systems. For instance, recent studies have demonstrated that certain effective communication structures, derived from human society, also play a positive role in multi-agent collaborations (Yin et al., 2023; Chen et al., 2024). In addition to communication structures, another notable characteristic of human teams is that they would optimize team members according to the given task. Take medical consultations as an example. Collaborations in dynamic structures are evident when the composition of the team changes within the procedure of consultation, as some doctors may become less relevant in major as the conversation goes deeper and "leave" the consultation, leading to corresponding changes in the communication structure. Team optimization is often observed in the varying initial composition of doctors for consultations with different diseases, influenced by changes in the related medical fields and the contribution of each doctor. These characteristics prompt an important question: Does a dynamically changing team of agents benefit LLM-powered agent collaborations similarly?
However, the question is not well-addressed yet. While various communication structures have been studied for different tasks, such as debating for reasoning (Du et al., 2023; Liang et al., 2023; Xiong et al., 2023) and self-collaboration for coding (Dong et al., 2023; Qian et al., 2023a;b), these communication structures do not alter members in the agent team and remain fixed throughout the collaboration. It indicates that task-oriented dynamic selections in agents are not thoroughly explored in current research. Furthermore, in the context of agent teams, most existing studies opt for hand-crafting agents from human priors (Liu et al., 2023; Nakajima, 2023; Hong et al., 2024; Shinn et al., 2023; Li et al., 2023) or employ an LLM to generate them (Wang et al., 2023c; Chen et al., 2023b; Christianos et al., 2023). These approaches generally predefine agents without further validation of the collaboration process. This leads to static agent teams or rebuilding teams without principled verification (Chen et al., 2024). Challenges still remain for optimization methods.
As a first attempt towards addressing the above question, we introduce a novel framework named Dynamic LLM-Powered Agent Network (DyLAN). DyLAN conceptualizes multiagent collaboration using temporal feed-forward networks (T-FFNs). In this formulation, each communication step of the agents corresponds to a network layer, with nodes representing the agents involved at that step and edges indicating communications between agents, for incorporating dynamic agent teams agnostically. DyLAN functions in two stages to incorporate task-oriented agent collaborations (Figure 1). The first stage is termed Team Optimization, where we select top contributory agents unsupervisedly among the initial team of candidates according to the task query, based on their individual contributions. We propose a forward-backward message passing algorithm on the T-FFN termed agent selection in Section 3.4, inspired by the back-propagation algorithm (Rumelhart et al., 1986) and neuron importance scores (Yu et al., 2018). This algorithm measures the contribution of each agent at the first stage with an unsupervised metric named Agent Importance Score. The most contributory agents form a smaller team to collaborate at the second stage Task Solving, thereby minimizing the impact of less effective agents on the final answer. Specifically, the collaboration begins with a team of agents, and an LLM-powered ranker in the middle dynamically deactivates low-performing agents (i.e., agent team reformation), thus expanding the T-FFN, integrating dynamic communication structures into DyLAN (Section 3.3.2). Incorporating agent selection, DyLAN effectively identifies and coordinates a task-oriented team of agents in a principled way. Extensive experiments demonstrate that DyLAN outperforms strong baselines in various tasks, including code generation, decisionmaking, general reasoning, and arithmetic reasoning. Notably, agent selection in DyLAN has</p>
<p>improved accuracy by up to $25.0 \%$ in certain subjects of the MMLU dataset (Hendrycks et al., 2021a), underlining the significance of dynamic agent teams.
In summary, our contributions are threefold:</p>
<ul>
<li>We introduce a novel framework named DyLAN for task-oriented agent collaboration in two stages with agent selection, marking a significant advancement in the study of dynamic agent teams.</li>
<li>DyLAN innovatively formulates agent collaborations in temporal feed-forward networks with agent team reformation, enhancing its adaptability and reducing dependence on human preconceptions.</li>
<li>Empirical results demonstrate the superior accuracy, efficiency, and stability of DyLAN across various tasks, underscoring the need for dynamic agent teams.</li>
</ul>
<h1>2 Related Work</h1>
<p>Team Optimization of LLM-Powered Agents The construction of agent teams is the essential and initial step for LLM-powered agent collaboration. TPTU (Ruan et al., 2023) and Chameleon (Lu et al., 2023) decompose tasks to choose or create tools accordingly. Recent studies also use LLMs to generate a fixed number of role prompts for agents in response to a task query (Wang et al., 2023c; Suzgun \&amp; Tauman Kalai, 2024), or for each round of discussion (Chen et al., 2024). However, manual prompts require careful design, which is impractical for adaptation on each task or domain, and prompting LLMs with predefined or generated descriptions may not result in the desired abilities of the agents without verification. Therefore, posteriorly selecting a team of agents based on their actual behaviors in the collaboration according to the task becomes essential. While team optimization for LLM agents is a relatively new area, human-team optimization has been studied for a long time. For instance, Liu et al. (2015) show that skill contribution is essential for selecting crowd workers to solve outsourced tasks efficiently. Based on peer rating, researchers have developed an algorithm for managing online workers in an optimal organization (Lykourentzou et al., 2022). Drawing inspirations, we introduce an unsupervised algorithm to select a team of agents by quantifying their contributions based on peer ratings in Section 3.4.
Communication Structures in LLM-Powered Agent Collaboration Collaboration between multiple LLM agents has demonstrated strong performance on various tasks in recent years and has emerged as a promising approach to enhance the capabilities of individual LLMs. To enable collaborations between multiple agents, recent studies have developed different communication structures and assigned agents in pre-defined architecture. For instance, researchers have found taking multiple LLM instances to debate for a fixed number of rounds can boost their factuality and reasoning capacities (Du et al., 2023; Liang et al., 2023; Xiong et al., 2023). To aggregate multiple LLM responses, LLM-Blender (Jiang et al., 2023) calls different LLMs in one round and uses pairwise ranking to combine the top responses. It has also been shown effective in distributing workloads to LLMs and concatenating their answers, thus producing better results (Ning et al., 2024; Suzgun \&amp; Tauman Kalai, 2024; Qiao et al., 2024). It is worth noting that existing studies (Hao et al., 2023; Zhang et al., 2023b) have tried organizing LLM instances into linear layers, but they mainly studied supervised learning in context space and LLM evaluation, respectively, not the scenario in which we are interested. However, running LLMs in a static architecture may limit the performance and generalization. On specific reasoning tasks, adopting a dynamic directed acyclic graph structure for LLMs has been shown effective (Zhang et al., 2023). Also, recent studies (Yin et al., 2023; Chen et al., 2024; Zhang et al., 2023a; Zhuge et al., 2024) have demonstrated that optimal communication structures vary with tasks and compositions of agents. Aligned with the findings, we propose a structure that adjusts dynamically based on selecting agents according to the tasks and the construction of the agent team in Section 3.3.2.
Evaluation of the Contribution of LLM-Powered Agents It is non-trivial to evaluate the contribution of each LLM agent in a multi-agent system, especially when they communicate over multiple rounds. In the single-round setting, existing methods use LLMs heavily for evaluation. To overcome the over confidence of LLMs (Xiong et al., 2024), pairwise</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Single Exec.</th>
<th style="text-align: center;">LLM-Blender</th>
<th style="text-align: center;">LLM Debate</th>
<th style="text-align: center;">Reflexion</th>
<th style="text-align: center;">CAMEL</th>
<th style="text-align: center;">AgentVerse</th>
<th style="text-align: center;">DyLAN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Communication Struc- <br> ture <br> $(V ; E)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Multiple Roles</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">Generated</td>
<td style="text-align: center;">Man \&amp;Gen.</td>
</tr>
<tr>
<td style="text-align: center;">Early Stopping</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Dynamic Structure</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Team Optimization</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison between DyLAN and representative previous works. In the second row, nodes denote agents at different time steps $(V)$, arrows represent edges $(E)$, and color indicates the role of agents.
ranking based on an additional LLM ranker has been introduced in LLM-Blender (Jiang et al., 2023). To rank $n$ responses with an independent LLM in a single round, they compare all $O\left(n^{2}\right)$ pairs. For better efficiency, researchers use a $k$-length sliding window to choose top $k$ responses within $O(n k)$ pairwise comparisons (Qin et al., 2023). However, these methods have not been extended to multi-round settings. Inspired by the neuron importance score (Yu et al., 2018), we evaluate agents by propagating and aggregating single-round peer ratings in a back-propagation manner (Rumelhart et al., 1986). In this way, we then introduce an unsupervised metric called Agent Importance Score to quantify the contribution of each agent in multi-round collaborations (Section 3.4).</p>
<h1>3 Dynamic LLM-Powered Agent Network</h1>
<h3>3.1 Overview</h3>
<p>We introduce a framework for LLM-powered agent collaboration named Dynamic LLMPowered Agent Network (DyLAN), facilitating dynamic communication structures and automatically task-oriented agent selection in a two-stage fashion (Figure 1): an optimized agent team is constructed in the first stage "Team Optimization" through a preliminary trial and then the team collaborates to solve the task in the second stage "Task Solving".
A core component of DyLAN is the temporal feed-forward networks (T-FFNs), whose nodes denote agents and edges denote the communication channels between agents (Figure 2 left). T-FFNs serve not only as the abstraction of communication structures but also the computation graph. From this perspective, as shown in Table 1, various LLM-powered agent collaboration systems (Jiang et al., 2023; Shinn et al., 2023; Du et al., 2023; Li et al., 2023; Chen et al., 2024) can be represented by similar network structures as T-FFNs. DyLAN is the only framework that supports multiple agents with roles and tools, early stopping (Section 3.3.2), dynamic communication structures and team optimization simultaneously. To be specific, for team optimization, our agent selection algorithm is performed as a backward message passing algorithm on the T-FFN (Figure 2 right), and for task solving, agent team reformation expands the T-FFN dynamically with messages passing forward.
To make it easier to understand, we will start by explaining the formulation of T-FFNs. Then, we will move on to the task solving stage, and finally, we will explain the team optimization stage, which relies on components of task solving.</p>
<h3>3.2 Temporal Feed-Forward Networks (T-FFNs)</h3>
<p>A T-FFN is a multi-layer network, of which each layer represents a time step. Its formal definition is as follows.</p>
<p>Definition 1 (Agents) Agents participating the collaboration are represented by</p>
<p>$$
\mathcal{A}=\left{a_{1}, a_{2}, \cdots, a_{N}\right}
$$</p>
<p>where $N$ denotes the total number of agents, and $a_{i}$ can be (I) an LLM-powered agent possibly equipped with tools, or (II) an independent tool, e.g., scripts, code interpreters.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The left part shows how DyLAN outputs the answer in a temporal feed-forward network (T-FFN), where nodes represent agents at specific time steps (Section 3.2). Agent team reformation functions in the middle steps, during which the low-performing agent is deactivated in subsequent time steps. The right part depicts agent selection (Section 3.4), where the contribution of each agent in a primary trial is automatically evaluated in three steps using Agent Importance Score, denoted as $\boldsymbol{I}$. Then, the top-ranked agents based on $\boldsymbol{I}$ will be selected as the optimized, task-oriented team of agents.</p>
<p>Definition 2 (Nodes) The t-th layer of a T-FFN consists of $N$ nodes, each of which corresponding to one agent:</p>
<p>$$
\mathcal{V}<em 1="1" t_="t,">{t}=\left{v</em>\right}
$$}, \cdots, v_{t, N</p>
<p>where $t=1, \cdots, T$, and node $v_{t, i}$ corresponds to agent $a_{i}$.
Definition 3 (Edges) Edges in a T-FFN refer to the communication channels between nodes, forming the communication structure between agents. Specifically, the set of edges between the nodes in layer $t-1$ and $t$ is denoted as</p>
<p>$$
E_{t-1, t}=\left{\left(v_{t-1, i}, v_{t, j}\right)\right} \subseteq \mathcal{V}<em t="t">{t-1} \times \mathcal{V}</em>
$$</p>
<p>where $t=2, \cdots, T$, and $\left(v_{t-1, i}, v_{t, j}\right)$ denotes an edge connecting nodes $v_{t-1, i}$ and $v_{t, j}$.
Definition 4 (T-FFN) Finally, the T-FFN corresponding to the collaboration is defined as a T-layer network:</p>
<p>$$
\mathcal{G}=\left(\mathcal{V}<em T="T">{1}, \cdots, \mathcal{V}</em>\right)
$$} ; E_{1,2}, \cdots, E_{T-1, T</p>
<p>Note that we only consider T-FFNs where edges only exist in adjacent layers. However, edge can be added to arbitrary pairs of nodes, making the T-FFN capable of representing more complex communication structures.</p>
<h1>3.3 Task Solving</h1>
<p>Task solving involves performing inference on the T-FFNs, jointly with agent team reformation, which be elaborated on in the subsequent two sections.</p>
<h3>3.3.1 Inference</h3>
<p>Before details, we first introduce the formulation of message passing on T-FFNs.
Definition 5 (Message Passing) Given a T-FFN, a node $v_{t, j}$, a set of adjacent nodes $\mathcal{U}=$ $\left{u_{1}, \cdots, u_{K}\right}$, and the messages $\mathcal{M}=\left{m_{u_{1}}, \cdots, m_{u_{K}}\right}$ received by $v_{t, j}$, where $K$ is the size of $\mathcal{U}$, and $m_{u_{k}}$ is the message sent from $u_{k}$ to $v_{t, j}$, message passing aggregates all the messages $\mathcal{M}$ to produce a updated message $\hat{m}<em j="j" t_="t,">{v</em>$, which is formally defined as}}$ for $v_{t, j</p>
<p>$$
\hat{m}<em j="j" t_="t,">{v</em>\right)
$$}}=f_{\mathrm{mp}}\left(\mathcal{M}, v_{t, j</p>
<p>where $f_{\mathrm{mp}}(\cdot, \cdot)$ is the aggregator function.</p>
<p>Definition 6 We refer the algorithm as forward message passing when $\mathcal{U}$ is the set of all adjacent nodes of $v_{t, j}$ from the previous time step, i.e., $\mathcal{U}=\left{u_{k} \mid \forall u_{k},\left(u_{k}, v_{t, j}\right) \in E_{t-1, t}\right}$. Similarly, it is referred as backward message passing when $\mathcal{U}$ is the set of all adjacent nodes from the next time step: $\mathcal{U}=\left{u_{k} \mid \forall u_{k},\left(v_{t, j}, u_{k}\right) \in E_{t, t+1}\right}$.</p>
<p>With the above formulation, we can describe the inference process of a T-FFN $\mathcal{G}$ in the manner of forward message passing. During collaborations on a given task, an agent at a specific time step takes the responses, i.e., messages, from other agents at the previous time step as input and generates responses based on the task query. Based on different types of agents at $v_{t, j}$, we can implement $f_{\mathrm{mp}}\left(\cdot, v_{t, j}\right)$ respectively: (I) concatenating input messages along with the task query into prompt templates (refer to task instructions in Appendix D) and take the response from LLM after generation or tool calling, or (II) filter the input that the tool can process, e.g., code completions and structured text.
During inference, we begin feeding the task query $q \in \mathcal{Q}$ into agents at time step $1\left(\mathcal{V}<em t-1="t-1">{1}\right)$, where $\mathcal{Q}$ denotes the dataset. By passing responses of nodes $\mathcal{V}</em>$ at $t$, agents can perceive responses from all nodes at the previous time step and perform collaborative behavior, which might include criticizes, advice, refinement, or quality reviews, depending on the implementation of agents. Formally, the inference process is defined as}$ at time step $t-1$ to nodes $\mathcal{V}_{t</p>
<p>$$
f_{\text {Infer }}(q, \mathcal{G})=o
$$</p>
<p>where $o=\operatorname{argmax}\left{\mathcal{M}<em T="T">{T}\right}$ and $\mathcal{M}</em>$. Please refer to Algorithm 1 for detailed procedure.}$ denotes the responses from $\mathcal{V}_{T</p>
<h1>3.3.2 Agent Team Reformation</h1>
<p>Given a set of agents $\mathcal{A}$, agent team reformation aims to identify more contributory agents and construct a dynamic communication structure accordingly. To this end, we leverage an additional LLM instance, referred as the "LLM Ranker", to analyze responses from the agents participate in the current time step and give out a ranking, prompted by the template of "Ranker" in Appendix D. Then, the top-ranked agents are allowed to participate in the next time step. In other words, edges will only be added for these top-ranked agents, resulting in a dynamic communication structure.
Formally, suppose the set of agents participates in time step $t$ is $\mathcal{A}<em k="k">{t}=\left{a</em>}\right}$, and the topranked agents are $\mathcal{A<em l="l">{t+1}=\left{a</em>}\right}$, where $k$ and $l$ are the indices of the agents as defined in Equation (1), then we can obtain two nodes sets $\mathcal{V<em k="k" t_="t,">{t}^{\prime}=\left{v</em>} \mid \forall a_{k} \in \mathcal{A<em t_1="t+1">{t}\right}$ and $\mathcal{V}</em>}^{\prime}=$ $\left{v_{t+1, t} \mid \forall a_{l} \in \mathcal{A<em t_="t," t_1="t+1">{t+1}\right}$, and the edge set $E</em>$ is defined as</p>
<p>$$
E_{t, t+1}=\mathcal{V}<em t_1="t+1">{t}^{\prime} \times \mathcal{V}</em>
$$}^{\prime</p>
<p>The process progresses iteratively until the stop condition is met, and finally, we get a T-FFN $\mathcal{G}<em _IAS="{IAS" _text="\text">{q}^{\mathcal{A}}$ for the query input $q$. We use function $f</em>$ to denote the entire computation:}</p>
<p>$$
\mathcal{G}<em _mathrm_IAS="\mathrm{IAS">{q}^{\mathcal{A}}=f</em>, q)
$$}}(\mathcal{A</p>
<p>To further enhance efficiency, we introduce an early-stopping mechanism. Inspired by the Byzantine Consensus theory (Castro \&amp; Liskov, 1999), at least $3 p+1$ agents are needed to tolerate $p$ faulty agents in a single round of communication. Following the theory, the inference process will be terminated when over $2 / 3$ of agents in a single layer have a consistent answer. In practice, the inference process will also be terminated when the maximum time step is reached. Note that none of the consistency measures used in prior work (Wang et al., 2023b; Aggarwal et al., 2023; Yin et al., 2023) applies to multi-round multi-agent interaction since their theories are assumed to execute a single LLM instance multiple times or expect all agents to reach the same answer.</p>
<h3>3.4 Team Optimization</h3>
<p>The goal of team optimization is to select a subset of agents from candidates based on their contributions evaluated from a primary trial, such that the new team solves the task query</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Pass@1</th>
<th style="text-align: right;">#API Calls</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single Execution</td>
<td style="text-align: right;">73.2</td>
<td style="text-align: right;">$(+0.0)$</td>
</tr>
<tr>
<td style="text-align: left;">CodeT</td>
<td style="text-align: right;">65.8</td>
<td style="text-align: right;">$(-7.4)$</td>
</tr>
<tr>
<td style="text-align: left;">CodeT (Codex)</td>
<td style="text-align: right;">74.8</td>
<td style="text-align: right;">$(+1.6)$</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion</td>
<td style="text-align: right;">68.3</td>
<td style="text-align: right;">$(-4.9)$</td>
</tr>
<tr>
<td style="text-align: left;">LATS</td>
<td style="text-align: right;">81.1</td>
<td style="text-align: right;">$(+7.9)$</td>
</tr>
<tr>
<td style="text-align: left;">CAMEL</td>
<td style="text-align: right;">69.5</td>
<td style="text-align: right;">$(-4.1)$</td>
</tr>
<tr>
<td style="text-align: left;">AgentVerse</td>
<td style="text-align: right;">75.0</td>
<td style="text-align: right;">$(+1.8)$</td>
</tr>
<tr>
<td style="text-align: left;">DyLAN (Ours)</td>
<td style="text-align: right;">$\mathbf{8 2 . 9}$</td>
<td style="text-align: right;">$(+9.7)$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Reward</th>
<th style="text-align: right;">Success <br> Rate</th>
<th style="text-align: right;">#API <br> Calls</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Direct Execution</td>
<td style="text-align: right;">50.6</td>
<td style="text-align: right;">$(+0.0)$</td>
<td style="text-align: right;">28.0</td>
</tr>
<tr>
<td style="text-align: left;">ReAct</td>
<td style="text-align: right;">53.8</td>
<td style="text-align: right;">$(+3.2)$</td>
<td style="text-align: right;">30.0</td>
</tr>
<tr>
<td style="text-align: left;">ReAct-SC</td>
<td style="text-align: right;">58.0</td>
<td style="text-align: right;">$(+7.4)$</td>
<td style="text-align: right;">36.0</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion (trial=4)</td>
<td style="text-align: right;">62.0</td>
<td style="text-align: right;">$(+11.4)$</td>
<td style="text-align: right;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">LATS</td>
<td style="text-align: right;">64.5</td>
<td style="text-align: right;">$(+13.9)$</td>
<td style="text-align: right;">38.0</td>
</tr>
<tr>
<td style="text-align: left;">BOLAA</td>
<td style="text-align: right;">66.0</td>
<td style="text-align: right;">$(+15.4)$</td>
<td style="text-align: right;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">DyLAN (Ours)</td>
<td style="text-align: right;">$\mathbf{6 8 . 3}$</td>
<td style="text-align: right;">$(+17.7)$</td>
<td style="text-align: right;">$\mathbf{4 2 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results on the CG task (left) and results on the DM task (right). The number in parentheses indicates the difference relative to the single execution or direct execution. We indicate the foundation model of methods except for GPT-35-turbo. The median of three trials is reported when non-zero temperature is used.
more effectively and efficiently. Formally, given a task query $q$, a set of agents $\mathcal{A}$, a trial is performed based on the algorithm proposed in Section 3.3.2 resulting in a T-FNN $\mathcal{G}_{q}^{\mathcal{A}}$. And team optimization is formulated as</p>
<p>$$
\tilde{\mathcal{A}}=f_{\text {Optim }}\left(\mathcal{A}, \mathcal{G}_{q}^{\mathcal{A}}, q\right), \text { where } \tilde{\mathcal{A}} \subset \mathcal{A}
$$</p>
<p>$f_{\text {Optim }}$ is implemented as in a three-step procedure of agent selection (Figure 2 right):
(1) Propagation: Each node rates the solutions to the task query from its predecessors, which is a forward message passing process. Formally, given a node $v_{t, j}$ and an edge $\left(v_{t-1, i}, v_{t, j}\right)$, the message $m_{v_{t-1, i}}$ sent from $v_{t-1, i}$ to $v_{t, j}$ is defined as the response to the task query $q$ from the agent $a_{i}$ at the previous time step. The aggregator function $f_{\mathrm{mp}}\left(\cdot, v_{t, j}\right)$ is implemented as a scoring function $f_{t, j}^{(s)}(\cdot, \cdot, \cdot)$, which maps the prompt $p_{j}$, the input query $q$, and all the messages $\mathcal{M}$ to the rating scores. Here, we use $w_{t-1, i, j}$ to refer to the rating score on $v_{t-1, i}$ from $v_{t, j}$, and</p>
<p>$$
\left[w_{t-1,1, j}, w_{t-1,2, j}, \ldots, w_{t-1, N, j}\right]=f_{t, j}^{(s)}\left(p_{j}, q, \mathcal{M}\right)
$$</p>
<p>(2) Aggregation: Each node aggregates the ratings it has received from its successors towards itself to quantify its own contribution independently at different time steps. The contribution of node $v_{t-1, i}$ is the sum of its successors' contribution multiplied by their peers' ratings on the agent's response. Aggregation is a backward message passing process. Formally, given a node $v_{t-1, i}$ and an edge $\left(v_{t-1, i}, v_{t, j}\right)$, the message $m_{v_{t, j}}$ sent from $v_{t, j}$ to $v_{t-1, i}$ is defined as $w_{t-1, i, j}$. And the aggregator function $f_{\mathrm{mp}}$ is defined as a weighted sum function:</p>
<p>$$
\boldsymbol{I}<em _left_v__t-1_="\left(v_{t-1," i="i">{t-1, i}=\sum</em>}, v_{t, j}\right) \in E_{t-1, t}} \boldsymbol{I<em i_="i," j="j" t-1_="t-1,">{t, j} \cdot w</em>
$$</p>
<p>where $\boldsymbol{I}<em i="i" t_="t,">{t, i}$ denotes the contribution of $a</em>$.
(3) Selection: During the last step, we sum up the scores for the same agent over all time steps to derive an importance score for each agent, and extract the top- $k$ agents that are most contributory according to these scores to form the optimized agent team. Formally, the Agent Importance Score $\boldsymbol{I}<em i="i">{i}$ for agent $a</em>$ is defined as</p>
<p>$$
\boldsymbol{I}<em t="1">{i}=\sum</em>
$$}^{T} \boldsymbol{I}_{t, i</p>
<p>In practice, we initialize the contributions in the final layer first, and step backward to perform Aggregation layer by layer (Algorithm 2). The definition guarantees that the agent importance scores add up to 1 in each layer, which benefits fair comparison. Other details, such as initializing contributions in the final layer, are presented in Appendix B.2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Prompting</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Counting and <br> Probability</th>
<th style="text-align: center;">Geometry</th>
<th style="text-align: center;">Intermediate <br> Algebra</th>
<th style="text-align: center;">Number <br> Theory</th>
<th style="text-align: center;">Pre- <br> Algebra</th>
<th style="text-align: center;">Pre- <br> Calculus</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">#API <br> Calls</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single Execution</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">$31.6(+0.0)$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Blender</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">$31.7(+0.1)$</td>
<td style="text-align: center;">6.00</td>
</tr>
<tr>
<td style="text-align: left;">LLM Debate</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">$32.4(+0.8)$</td>
<td style="text-align: center;">8.00</td>
</tr>
<tr>
<td style="text-align: left;">DyLAN (Ours)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">$\mathbf{3 5 . 7}(-4.1)$</td>
<td style="text-align: center;">7.15</td>
</tr>
<tr>
<td style="text-align: left;">Single Execution</td>
<td style="text-align: center;">Complex</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">$34.1(+0.0)$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">PHP</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">$36.5(+2.4)$</td>
<td style="text-align: center;">3.67</td>
</tr>
<tr>
<td style="text-align: left;">DyLAN (Ours)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">$\mathbf{3 7 . 6}(-3.5)$</td>
<td style="text-align: center;">6.21</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy (\%) on the AR task. The number in parentheses indicates the performance difference relative to a single execution.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Hum- <br> anities</th>
<th style="text-align: center;">Social <br> Science</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Other</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">#API <br> Calls</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Single Exec.</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">$66.4(+0.0)$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Blender</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">$67.3(+0.9)$</td>
<td style="text-align: center;">6.00</td>
</tr>
<tr>
<td style="text-align: left;">LLM Debate</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">$69.3(+2.9)$</td>
<td style="text-align: center;">12.00</td>
</tr>
<tr>
<td style="text-align: left;">DyLAN</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">$\mathbf{7 0 . 5}(-4.1)$</td>
<td style="text-align: center;">4.39</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy (\%) on the GR task. "Other" stands for subjects like business, health, and misc in the MMLU dataset. We report the median of three runs for experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">#Agents</th>
<th style="text-align: center;">Tool <br> Usage</th>
<th style="text-align: center;">Performance <br> Improvement</th>
<th style="text-align: center;">#API Calls</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CG</td>
<td style="text-align: center;">$12 \rightarrow 8$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$76.2 \rightarrow 82.9$</td>
<td style="text-align: center;">$23.04 \rightarrow 16.85$</td>
</tr>
<tr>
<td style="text-align: left;">DM</td>
<td style="text-align: center;">$8 \rightarrow 4$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$53.0 \rightarrow 68.3$</td>
<td style="text-align: center;">$32.03 \rightarrow 24.85$</td>
</tr>
<tr>
<td style="text-align: left;">GR</td>
<td style="text-align: center;">$7 \rightarrow 4$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$69.5 \rightarrow 70.5$</td>
<td style="text-align: center;">$8.30 \rightarrow 4.39$</td>
</tr>
<tr>
<td style="text-align: left;">AR</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 5: Demonstration of experiment settings, including the number of agents and the performance throughout team optimization. We report reward for the DM task.</p>
<h1>4 Experiments</h1>
<h3>4.1 Setup</h3>
<p>Code Generation (CG) We use the HumanEval benchmark, with 164 human-labeled function-level completion codes and unit tests (Chen et al., 2021). Unit tests are used to validate the correctness of generated codes. We leverage two strong baselines CodeT (Chen et al., 2023a) and Reflexion (Shinn et al., 2023) along with the single execution. For multiagent baselines, we re-implement CAMEL (Li et al., 2023) and AgentVerse (Chen et al., 2024) under their original configurations for fair comparisons.
Decision Making (DM) We evaluate our methods in the WebShop environment, selecting 50 environments in its test set (Chen et al., 2021) following the setting of LATS (Zhou et al., 2023). WebShop requires to find the item given an instruction of the customer. It provides "reward" as an intrinsic metric for item-instruction relevance, and "success" is marked when the reward is 1.0. Besides ReAct (Yao et al., 2023) and Reflexion, we re-ran a multi-agent method BOLAA (Liu et al., 2023), and a single-agent method LATS as strong baselines.
General Reasoning (GR) For the general reasoning task, we use the MMLU dataset (Hendrycks et al., 2021a), which contains four categories of a vast amount of problems in 57 subjects. We down-sample $1 / 5$ of the problems in the test set because of its huge quantity. We choose LLM Debate (Du et al., 2023), LLM-Blender (Jiang et al., 2023), and the single execution on LLM as baselines.
Arithmetic Reasoning (AR) We leverage the test set of MATH (Hendrycks et al., 2021b) for evaluation, which consists of 7 subareas and 5,000 questions in total. To draw a fair comparison and verify the robustness, we categorize methods by different prompting strategies and select strong baselines accordingly. Preliminary experiments show that collaborating agents in different domains (e.g., algebra and geometry experts) does not make significant improvement, therefore we adopt agents with same prompts for all methods.
DyLAN Setup In Table 5, we elaborate the setup of DyLAN. To keep in line with baseline methods, we only equipped DyLAN with code interpreters as tools in the CG task. It is worth noting that agent selection is performed for each subject in the GR task, for each web page in the DM task, and directly for CG task in the team optimization stage. Due to space limitations, please refer to Appendix B. 1 for details.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Impact of optimized agent team size. $2 \sim 4$ agents are selected from 7 candidate agents based on Agent Importance Score. Accuracy (left) and #API calls (right) on the GR task are visualized.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">AR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">#API</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">#API</td>
</tr>
<tr>
<td style="text-align: center;">DyLAN</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">7.15</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">4.39</td>
</tr>
<tr>
<td style="text-align: center;">tole es</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">13.00</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">13.00</td>
</tr>
<tr>
<td style="text-align: center;">tele atr</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">8.20</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">7.05</td>
</tr>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">CG</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DM</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">#API</td>
<td style="text-align: center;">Reward</td>
<td style="text-align: center;">#API</td>
</tr>
<tr>
<td style="text-align: center;">DyLAN</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">16.85</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">24.85</td>
</tr>
<tr>
<td style="text-align: center;">tele es</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">19.00</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">54.25</td>
</tr>
<tr>
<td style="text-align: center;">tele atr</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">17.98</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">48.90</td>
</tr>
</tbody>
</table>
<p>Table 6: Impact off the earlystopping mechanism (es) and agent team reformation (atr).</p>
<h1>4.2 Main Results</h1>
<p>In Table 2, Table 3, and Table 4, we report the results on each dataset respectively. The number of API calls serves as a proxy for the efficiency of communication structures for agents, which cannot be clearly determined from token consumption which varies greatly depending on tasks and prompting strategies. Since "Task solving" after "Team Optimization" is essential for testing and deployment, we mainly report the cost from the second stage. The difference can be seen in Table 5, and further discussions in Appendix C.1.</p>
<p>DyLAN improves overall performance on different tasks with a reasonable computational cost. From Table 3, we find DyLAN realizes an $10.2 \%$ improvement to LLM Debate in terms of accuracy, with $10.6 \%$ lower #API calls (L3 vs. L4), suggesting it is a better trade-off between efficiency and effectiveness. Similar trends can be observed as DyLAN has a better performance with only $36.6 \%$ API calls of LLM Debate (L5 vs. L4 in Table 4), and $35.1 \%$ of LATS on CG and $&lt;6 \%$ on DM (L8 vs. L5 (left), L7 vs. L5 (right) in Table 2). We argue it can be attributed to the feed-forward structure and early-stopping mechanism, which allows different solutions to be delivered simultaneously and confirmed rapidly. In contrast, for methods in sequential architecture like PHP and Reflexion (Table 2), incorrect intermediates might easily influence the final output due to the single thread of reasoning or code generation and review. Also, ReAct on DM tasks exhibits similar failures due to misoperations in the middle. In contrast, Reflexion and LATS access the environment at certain states for multiple times to for reflection, limiting generalizabilities. In our case, any feedback from predecessors could be rated by successor nodes, making it easier to rectify potential invalid actions. Moreover, we see that DyLAN dynamically adjust the cost based on the difficulty of tasks. For instance, most questions in the MMLU dataset are less challenging than MATH, DyLAN has 2.76 fewer API calls on the query from the former. However, compared to other tasks, DyLAN introduces relatively lower improvements in AR tasks, which might be due to the high knowledge dependency of the MATH dateset.
DyLAN benefits from the team optimization. Moreover, we found that a dynamic team of task-oriented agents could enhance DyLAN. For different subjects in GR tasks, agent compositions are adjusted correspondingly to improve up to $25.0 \%$ in accuracy, as shown in Table 7. As denoted in Table 5, a dynamically selected team of agents could result in significant performance improvement, especially for DM tasks, where agents might have great interference from others. The overall performance can be significantly improved (up to $6.7 \%$ ) with lower computational costs on each tasks after agent selection. Moreover, it also suggests that Agent Importance Scores can effectively capture and reflect the actual contributions of agents on a wide range of tasks. We further verify this claim in Appendix C.6.</p>
<h3>4.3 Ablation Studies</h3>
<p>Impact of Optimized Agent Team Size Fewer proper agents in a team could have better performance. As shown in Figure 3, DyLAN with an optimized team of 3 agents can outperform both DyLAN before team optimization and LLM Debate with 4 agents, suggesting the effectiveness of our proposed agent selection. The efficiency is also significantly improved</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: center;">Optimized Composition</th>
<th style="text-align: center;">Performance Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">college <br> mathematics</td>
<td style="text-align: center;">Economist, Lawyer, <br> Programmer, Mathematician</td>
<td style="text-align: center;">$\mathbf{2 5 . 0}: 40.0 \rightarrow 65.0$</td>
</tr>
<tr>
<td style="text-align: left;">management</td>
<td style="text-align: center;">Lawyer, Psychologist, <br> Economist, Programmer</td>
<td style="text-align: center;">$\mathbf{1 4 . 3}: 76.2 \rightarrow 90.5$</td>
</tr>
<tr>
<td style="text-align: left;">high school <br> statistics</td>
<td style="text-align: center;">Historian, Programmer, <br> Psychologist, Mathematician</td>
<td style="text-align: center;">$\mathbf{9 . 3}: 65.1 \rightarrow 74.4$</td>
</tr>
<tr>
<td style="text-align: left;">clinical <br> knowledge</td>
<td style="text-align: center;">Doctor, Mathematician, <br> Programmer, Psychologist</td>
<td style="text-align: center;">$\mathbf{5 . 7}: 69.8 \rightarrow 75.5$</td>
</tr>
<tr>
<td style="text-align: left;">public <br> relations</td>
<td style="text-align: center;">Historian, Psychologist, <br> Lawyer, Mathematician</td>
<td style="text-align: center;">$\mathbf{4 . 5}: 54.5 \rightarrow 59.1$</td>
</tr>
</tbody>
</table>
<p>Table 7: The optimized composition of agents and performance improvement on different subjects in the GR task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#Code <br> Writers</th>
<th style="text-align: center;">#Code <br> Reviewers</th>
<th style="text-align: center;">Pass@1</th>
<th style="text-align: center;">#API <br> Calls</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">23.04</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">16.85</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">14.64</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">12.50</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">11.73</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">9.60</td>
</tr>
</tbody>
</table>
<p>Table 8: Different compositions of agents on the CG task of an optimized team of agents. Agent teams are optimized by the Agent Importance Score from the first line.
by $52.9 \%$ and $67.8 \%$, respectively. Probably because the imbalance of agents' expertise and opinions interfere with each other before optimization, especially on GR, where few candidates are relevant to a subject.
Robustness of Agent Importance Score The Agent Importance Score is robust over the imbalance of agent roles. On GR tasks, the candidates are imbalanced in terms of expertise. For most queries, there are less than 2 candidates that are related according to their role prompts. In Table 7, we found agent selection is capable for selecting related agents, e.g., "Mathematician" for "college mathematics", that matches human priors. However, if candidates are all vastly different from the task domain, e.g., "public relation", where the improvement is less significant. We also tested DyLAN on CG tasks with different amount of code writers and code reviewers after the "Team Optimization" stage. It is worth noting that a single run of "Team Optimization" could provides reusable Agent Importance Scores for multiple trials of agent selection. In Table 8, we exhibit the results under imbalanced teams of agents. We verified the imbalance of code writers and reviewers after optimization won't cause great performance drops. Though, reviewers affect the performance slightly greater than writers (L2,3 vs. L4,5), indicating the necessity of the amount of reviewers for code refinement.
Impact of Early-Stopping and Agent Team Reformation As shown in Table 6, earlystopping mechanism boosts efficiency to a great extent by minimizing #API calls by $45.0 \%$, $66.2 \%, 11.3 \%$, and $54.2 \%$ on AR, GR, CG, and DM tasks respectively, while providing slight performance improvement. Agent team reformation, however, is critical to enhance the correctness of the final answer. We conjecture it is because agents are filtered for temporary mistakes in LLMs, such as hallucinations, etc. Additionally, answer comparison is more challenging for open-ended tasks like CG or DM tasks. We use the BLEU score with a 0.9 threshold for consistency checks. This makes early stopping less effective since agents may generate codes in different formats, leading to fewer opportunities to stop early.
Stability of DyLAN with Different Backbone Models There is also a notable difference in CG tasks when the backbone model changes (Table 2). Reflexion and CodeT's performances are heavily related to the backbone model (L4 vs. L5 and L6 vs. L9). Instead, DyLAN shows a steady, consistent high performance (L7 vs. L10) under different backbone models with almost the same amount of API calls.</p>
<h1>5 Conclusion and Future Work</h1>
<p>This work introduces a framework named Dynamic LLM-Powered Agent Network (DyLAN) for collaboration of dynamic agent teams on complicated tasks. DyLAN functions in a two-stage paradigm, enabling agents to interact in a dynamic structure with agent team reformation. In "Team Optimization" stage, the agent selection algorithm based on an unsupervised metric termed Agent Importance Score, selects top contributory agents in a principled way for collaboration on "Task Solving". Overall, DyLAN reveals improvement on diverse tasks with relatively less computational cost compared to baselines. In the future, we plan to explore the effectiveness of DyLAN built on open-source foundation models.</p>
<h1>Acknowledgments</h1>
<p>We thank Yilun Du for the helpful assistance on code implementation. We sincerely thank William Held, Ruibo Liu, Dr. Yue Zhuge, Noah Shinn and Kangfu Zheng for their valuable feedback on the project.</p>
<h2>Ethics Statement</h2>
<p>LLM-powered agent systems are widely used in practical applications. DyLAN could also effortlessly cover practical software development, virtual room chat, video games, and so on (Hong et al., 2024; Nascimento et al., 2023; Zhou et al., 2023; Zhu et al., 2023; Chan et al., 2024). In these open-world environments, agents may operate as planners, actors, etc. DyLAN only requires people to give rough instructions on the constitute of agents and could automatically optimize a better team of agents to construct an efficient multi-agent system. These systems could benefit from DyLAN to reduce human labor on designing agents and have a better performance on their target tasks.
Also, the overall architecture of DyLAN (Figure 2) reflects the optimal collaboration organization of human online workers (Lykourentzou et al., 2022), and reveals significant performance in agent collaborations. Therefore, simulating human collaboration by LLMpowered agent collaborations under DyLAN might also be possible. Optimizing human collaboration by searching and simulating LLM agents will hopefully be more convenient and effective. We also acknowledge the potential risk in pretrained language models used in the paper, e.g., GPT-3.5 and GPT-4, which may cause improper responses. Furthermore, creating agents by hand and LLM generations might prompt LLM-powered agents to act or response misaligned with principles of society, which may possibly happen during agent collaborations. However, we think team optimization process could potentially alleviate this situation.</p>
<h2>References</h2>
<p>Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. Let's sample step by step: Adaptive-consistency for efficient reasoning and coding with LLMs. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 12375-12396, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.761. URL https: //aclanthology.org/2023.emnlp-main. 761.</p>
<p>Miguel Castro and Barbara Liskov. Practical byzantine fault tolerance. In Proceedings of the Third Symposium on Operating Systems Design and Implementation, OSDI '99, pp. 173-186, USA, 1999. USENIX Association. ISBN 1880446391.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better LLM-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=FQepisCUWu.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. In Proceedings of The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/ forum?id=ktrw68Cmu9c.</p>
<p>Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Sesay Jaward, Karlsson Börje, Jie Fu, and Yemin Shi. Autoagents: The automatic agents generation framework. arXiv preprint arXiv:2309.17288, 2023b.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,</p>
<p>Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=EHgSGDnyq1.</p>
<p>Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, Zheng Xiong, Yicheng Luo, Jianye Hao, Kun Shao, Haitham Bou-Ammar, and Jun Wang. PanguAgent: A Fine-Tunable Generalist Agent with Structured Reasoning. arXiv preprint arXiv:2312.14878, 2023.</p>
<p>Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration Code Generation via ChatGPT. arXiv preprint arXiv:2304.07590, 2023.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.</p>
<p>Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie. Chatllm network: More brains, more intelligence. arXiv preprint arXiv:2304.12998, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2021a.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of Thirty-fifth Conference on Neural Information Processing Systems, 2021b.</p>
<p>Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill ${ }^{\mathrm{TM}}$ : A bayesian skill rating system. In B. Schölkopf, J. Platt, and T. Hoffman (eds.), Proceedings of Advances in Neural Information Processing Systems, volume 19. MIT Press, 2006.</p>
<p>Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VtmBAGCN7o.</p>
<p>Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1416514178, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long. 792.</p>
<p>Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, and Jiangtao Gong. Surrealdriver: Designing generative driver agent simulation framework in urban contexts based on large language model. arXiv preprint arXiv:2309.13193, 2023.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative agents for "mind" exploration of large language model society. In Proceedings of Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=3IyL2XWDkG.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.</p>
<p>Qing Liu, Tie Luo, Ruiming Tang, and Stéphane Bressan. An efficient and truthful pricing mechanism for team formation in crowdsourcing markets. In 2015 IEEE International Conference on Communications (ICC), pp. 567-572, 2015.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= zAdUB0aCTQ.</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960, 2023.</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, SongChun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In Proceedings of Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HtqnVSCj3q.</p>
<p>Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pp. 4768-4777, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.</p>
<p>Ioanna Lykourentzou, Federica Lucia Vinella, Faez Ahmed, Costas Papastathis, Konstantinos Papangelis, Vassilis-Javed Khan, and Judith Masthoff. Self-organization in online collaborative work settings. Collective Intelligence, 1(1), sep 2022.</p>
<p>Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, and Dong Yu. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172, 2023.</p>
<p>Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.
Nathalia Nascimento, Paulo Alencar, and Donald Cowan. GPT-in-the-Loop: Adaptive Decision-Making for Multiagent Systems. arXiv preprint arXiv:2308.10435, 2023.</p>
<p>Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= mqVgBbNCm9.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Belgium, Brussels, October 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ W18-6319.</p>
<p>Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023a.</p>
<p>Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Experiential co-learning of software-developing agents. arXiv preprint arXiv:2312.17025, 2023b.</p>
<p>Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. arXiv preprint arXiv:2401.05268, 2024.</p>
<p>Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023.</p>
<p>Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, M. Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arxiv:2009.10297, 2020.</p>
<p>Reworkd. Agentgpt. https://github.com/reworkd/AgentGPT, 2023.
Toran Bruce Richards and et al. Auto-gpt: An autonomous gpt-4 experiment. https: //github.com/Significant-Gravitas/Auto-GPT, 2023.</p>
<p>Jingqing Ruan, YiHong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, du qing, shi shiwei, Hangyu Mao, Xingyu Zeng, and Rui Zhao. TPTU: Task planning and tool usage of large language model-based AI agents. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL https://openreview.net/forum?id=GrkgKtOjaH.</p>
<p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533-536, 1986.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=vAEIhFcKW6.</p>
<p>Mirac Suzgun and Adam Tauman Kalai. Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding. arXiv preprint arXiv:2401.12954, 2024.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. In Second Agent Learning in Open-Endedness Workshop, 2023a. URL https://openreview.net/forum?id=pAMNKGwja6.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=1PL1NIMMrw.</p>
<p>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. arXiv preprint arXiv:2307.05300, 2023c.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=_VjQIMeSB_J.</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.</p>
<p>Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank: Theory and algorithm. In Proceedings of the 25th International Conference on Machine Learning, ICML '08, pp. 1192-1199, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605582054.
Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 7572-7590, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.508. URL https: //aclanthology.org/2023.findings-emnlp.508.</p>
<p>Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=gjeQKFxFpZ.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In Proceedings of The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WE_vluYUL-X.</p>
<p>Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuanjing Huang, and Xipeng Qiu. Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 15135-15153, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.936. URL https://aclanthology.org/2023.emnlp-main. 936 .</p>
<p>Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and Larry S. Davis. Nisp: Pruning networks using neuron importance score propagation. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9194-9203, 2018.
Jintian Zhang, Xin Xu, and Shumin Deng. Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View. arXiv preprint arXiv:2310.02124, 2023a.
Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and Deeper LLM Networks are Fairer LLM Evaluators. arXiv preprint arXiv:2308.01862, 2023b.
Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language models. arXiv preprint arXiv:2308.04371, 2023.
Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.</p>
<p>Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023.
Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. LLM As DBA. arXiv preprint arXiv:2308.05481, 2023.</p>
<p>Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.
Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. Language agents as optimizable graphs. arXiv preprint arXiv:2402.16823, 2024.</p>
<h1>A Discussion \&amp; Limitation</h1>
<p>In experiments, we view code generation tasks as representative of open-ended generation tasks and adopt BLEU to decide whether two answers are consistent in early stopping mechanism in Section 3.3.2. In fact, the performance could be further leveraged by taskspecific methods like CodeBLEU (Ren et al., 2020) or CodeT (Chen et al., 2023a).
For practical usage, the agent-evaluation metrics could cooperate with human annotation to give a more precise evaluation result on individual contributions of agents, mainly when facing data scarcity problems. Furthermore, we simply incorporating agent selection on DyLAN with agent team reformation, as a primary step towards collaboration of dynamic agent teams. It still remains to be seen how to cooperate off-collaboration and in-collaboration optimization methods in a finer granularity to further improve performance and efficiency in LLM-powered agent collaboration systems.
Additionally, though agent selection could differentiate top contributory agents, in extreme cases where the majority of agents are designed to contradict the task requirement, low performance might be caused, e.g., agents are prompted or trained to generate codes may face difficulties in clinical question answering. To tackle the imbalance of high- and lowperforming agents, replicating agents with high Agent Importance Score instead of including low-score agents could be a solution. Additionally, in extreme circumstances, we can automatically introduce agents from more capable LLMs with validation, in addition to agent selection.</p>
<h2>B Implementation Details</h2>
<h2>B. 1 Detailed Experiment Settings</h2>
<p>Algorithm 1 The Inference Process of DyLAN on an Arbitrary Query</p>
<p>Input: T-FFN $\mathcal{G}=\left(\mathcal{V}<em T="T">{1}, \cdots, \mathcal{V}</em>\right)$ Query $q$
Output: Final Answer $o$
$/ / E=\left{\left(v_{t, i}, v_{t+1, j}\right)\right}} ; E_{1}, \cdots, E_{T-1, T<em i="i" t_="t,">{t=1}^{T-1}, v</em>$
$/ /=\bigcup_{t=1}^{T} \mathcal{V}}, v_{t+1, j} \in \mathcal{V<em i="i" t_="t,">{t}$.
$/ / m</em>} \in \mathcal{M<em i="i" t_="t,">{t}$ denotes the response from $v</em>} \in \mathcal{V<em _text="\text" _top="{top">{t}$.
for $t=1 ; T$ do
if agent team reformation at time step $t$ then
$\mathcal{M}</em>}} \leftarrow \operatorname{top}-k\left(\left{m_{t-1, j} \mid v_{t-1, j} \in \mathcal{V<em t_prime="t^{\prime">{t-1}\right}\right)$
$E \leftarrow E \backslash\left{\left(v</em>\right) \mid\right.$
$\left.m_{t^{\prime}, j} \in \mathcal{M}}, j}, v_{t^{\prime \prime}, j^{\prime}}\right),\left(v_{t^{\prime \prime}, j^{\prime}}, v_{t^{\prime}, j<em t_prime="t^{\prime">{\text {top }}, m</em>}, j^{\prime}} \notin \mathcal{M<em j="j" t_="t,">{\text {top }}, t^{\prime}, t^{\prime \prime} \geq t-1\right}$
$m</em>\right) \in E$
else
$\forall i, \exists k,\left(v_{t, i}, v_{t+1, k}\right) \in E_{t, t+1}$,
$m_{t, i} \leftarrow$
$f_{\text {mp }}\left(\left{m_{t-1, j} \mid\left(v_{t-1, j}, v_{t, i}\right) \in E_{t, t+1}\right}, v_{t, i}\right)$
end if
if early stopping then
$T \leftarrow t$
break
end if
end for
$o \leftarrow \operatorname{postProcess}\left(\operatorname{maxCount}\left{\mathcal{M}_{T}\right}\right)$} \leftarrow m_{t-1, j}, \forall\left(v_{t-1, j}, v_{t, j</p>
<p>Algorithm 2 The Team Optimization Process $f_{\text {Optim }}$ of Agent Importance Score within DyLAN
Input: Output $o$, T-FFN $\mathcal{G}=(\mathcal{V}, E)$
Output: Agent Importance Score of agents $I$
$/ / m_{t, i} \in \mathcal{M}<em i="i" t_="t,">{t}$ denotes the response from $v</em>} \in \mathcal{V<em i="i" t_="t,">{t}$.
flag $\leftarrow$ False
for $t=T ; 1$ do
if $\left{v</em>\right) \in E\right} \neq \phi$ then
if -flag then
flag $\leftarrow$ True
distribute scores for $I_{t, i}$
else
$\mathcal{M}} \mid \exists k,\left(v_{t-1, k}, v_{t, i<em j="j" t-1_="t-1,">{t-1} \leftarrow$
$\left{m</em>\right) \in E\right}$
$\left[w_{t-1,1, i}, \ldots, w_{t-1, m, i}\right] \leftarrow$
$f_{t, i}^{(s)}\left(p_{i}, q, \mathcal{M}} \mid\left(v_{t-1, j}, v_{t, i<em j="j" t-1_="t-1,">{t-1}\right)$
$\boldsymbol{I}</em>} \leftarrow \boldsymbol{I<em i="i" t_="t,">{t-1, j}+\boldsymbol{I}</em>$,
$\exists\left(v_{t-1, j}, v_{t, i}\right) \in E$
end if
end if
end for} w_{t-1, j, i</p>
<p>Common Settings In all experiments, we use gpt-35-tubo-0301 for every method if not specified. The version of GPT-4 is GPT-4-0613. In Table 2, "(Codex)" denotes code-davinci-002 from OpenAI (Chen et al., 2021; OpenAI, 2023). All experiments with non-zero temperature is repeated for three times and the median is reported. To avoid the context length issue in prior work (Du et al., 2023; Liu et al., 2024), we set memory space for agents in DyLAN to 1 only to keep the freshest responses of predecessors. We set max tokens to 2048 for GR and AR tasks and 1024 for CG and DM tasks to avoid exceeding the maximum context length. The construction of candidates are demonstrated in Appendix D. We set $N=4$ in T-FFN after team optimization because the early-stopping mechanism requires at least four agents to tolerate one different response at a specific time step (Section 3.3.2); when reaching consensus over $2 / 3$ of agents, it allows for $4-\left[\frac{2}{3} N\right]=1$ excetional response. We use a listwise ranker in the agent team reformation of DyLAN because of the effectiveness and efficiency, compared to ELo rating (Herbrich et al., 2006) or Sliding Window (Qin et al., 2023) we have tested in Appendix C.5. We use the same ranker to implement LLM-Blender (Jiang et al., 2023) in experiments. We set $k=2$ in the agent team reformation, because it's the minimal number for collaborations and we empirically found it brings great trade-off between effectiveness and efficiency. To avoid positional bias, for each time step $t$, we shuffle the responses from agents at $t-1$ when passing messages towards agents at $t$. The detailed inference algorithm is in Algorithm 1. To implement the early-stopping mechanism, we need to determine whether the answers from the nodes in the same layer of DyLAN are consistent. For classification and decision-making problems, the answers are consistent if identical, and for open-ended generation, the consistency is determined by a threshold of BLEU score.</p>
<p>Experiments on Reasoning Tasks In general reasoning, we extract the answer from the response by matching the last "(X" or "(X)", where "X" represents A, B, C or D. On average, Agent team reformation functions on the third time step. They could go through at maximum $T=4$ rounds of interaction. We also searched temperature in ${0,0.2,0.8,1.0}$ for the best configuration for each system. In arithmetic reasoning, we set temperature to 0 for the single execution and PHP, 0.2 for LLM Debate, LLM-Blender, and DyLAN with Complex CoT prompts, and 1.0 for DyLAN with simple CoT prompts in Table 3, since systems with the same prompts will give all the same responses if temperature is zero, causing degradation. Prompting templates are replicated from their original studies, including normal CoT prompts (Wei et al., 2022) from the MATH dataset (Hendrycks et al., 2021b) and Complex CoT from PHP (Zheng et al., 2023). We follow the answer extraction method from the origin paper (Hendrycks et al., 2021b). We construct DyLAN with 4 agents assigned no specific roles and let agents to interact for at maximum $T=4$ rounds under T-FFN formulation. We reported the classification accuracy of each category averaged across subjects and the numbers of API calls of running DyLAN on the optimized team of agents.</p>
<p>Experiments on Code Generation Tasks In the code generation task, we set temperature to 0 for the single execution, Reflexion, and 0.8 for LLM Debate, LLM-Blender, CodeT, and DyLAN in Table 2. In DyLAN, we optimized four agents to write code and four agents to give code reviews from 12 candidates in Appendix D. The selected code writers are "Python Assistant", "Algorithm Developer", "Computer Scientist", and "Programmer"; and the selected code reviewers are "Syntax Checker", "Unit Tester", "Reflector", and "Ranker". "Syntax Checker" is pure external tools using a code interpreter for syntax checking without LLMs, and "Unit Tester" is equipped with a code interpreter. The tool is triggered when LLM generated codes inside the format ' ' python\n(code) $\backslash n^{\prime} \cdot$ '. In DyLAN, solutions given by code writers are reviewed by code reviewers in at maximum $T=6$ rounds. At time step $t=1,3,4,6$, code writers gives solutions and code reviewers review it at $t=2,5$. And agent team reformation occurs at $t=4$. To ensure the participation of each agent, earlystopping mechanism functions at the third layer and later $(t \geq 3)$. We use BLEU score in the early-stopping mechanism. We calculate BLEU by sacreBLEU2 (Post, 2018). For answer post-processing, we store all unit tests from the unit tester (if exists in the system) and randomly select the final output from the top 5 code completions from all nodes that pass most tests.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Experiments on Decision Making Tasks In the decision-making task, we set temperature to 0 for DyLAN and all baselines in Table 2. For ReAct with self-consistency (denoted by ReAct-SC in the table) (Wang et al., 2023b), we sampled three times for each response. In DyLAN, we optimized four agents from 8 candidates which are depicted in Appendix D. All methods are also conducted on gpt-35-tubo-1106. We did not select LASER (Ma et al., 2023) as a baseline, because it requires GPT-4 for better performance and it extracts all valid actions in each page into function calls, instead of detected by agent itself, which we decide to be a different setting. We divide the pages of the WebShop environment into 3 parts: the initialization page for "searching" part, the item list page for "exploring" part, and the item details pages for "item" part. Thus, we managed to optimize teams for each part from agents in Appendix D: "Search Optimizer", "Budget Analyst", "Instruction Analyst", "Decision Reflector" for "searching" group, "Decision Maker", "Budget Analyst", "Product Explorer", "InstructionAnalyst" for "exploring" group, and "Budget Analyst", "Description Reader", "Decision Maker", "Result Estimater" for "item" group. Agents interact for at maximum $T=4$ steps for each action. We simply concatenate observations of previous actions on each decision. For answer post-processing, we skip invalid actions from the outputs of $V_{T}$.</p>
<h1>B. 2 Calculation of Agent Importance Score</h1>
<p>To implement the agent selection algorithm under DyLAN, only one sentence needs to be injected into the end of the prompt of each node in T-FFN: "Along with the answer, give a score ranging from 1 to 5 to the solutions of other agents. Put all $\left{n u m_{\mathrm{p}}\right}$ scores in the form like $[[1,5,2, \ldots]]^{\prime \prime}$, where $n u m_{\mathrm{p}}$ denotes the number of predecessors of the node. The prompt functions as the $f_{t, i}^{(s)}$ in Section 3.4 and we extract $w_{t, i, j}$ from its response at the same time when we extract the message that passes between nodes. The scores are normalized so that their sum $\left(\sum_{i=1}^{N} w_{t, i, j}\right)$ equals 1 . To avoid positional bias, responses from agents at previous time step are shuffled when rating.
In Algorithm 2, initial contributions are distributed on nodes at the last layer. For reasoning and dicision-making tasks, we uniformly distribute contributions to agents that give consistent answers in the last layer. On code generation tasks, we uniformly distribute contributions in the final round with no syntax error in their answers.</p>
<h2>C Additional Results</h2>
<p>In this section, detailed results and additional experiments are presented.</p>
<h2>C. 1 Data Efficiency of Team Optimization</h2>
<p>We further demonstrate the data efficiency of agent selection by performing it based on different amounts of data. The experiments are conducted on five subjects in the GR task (the same as Table 7) and the CG task. We sample the subsets with the proportions of $1 \%$ and $10 \%$ of the original dataset. Agent Importance Score for agent selection is averaged on the subsets, and the selected team is tested on the whole dataset. We raise random selection and human prior selection as baselines. The latter is simulated by GPT-4 prompted by the task and agent descriptions (Appendix D).
As shown in Table 9, by optimizing the team $10 \%$ of the original dataset, DyLAN has demonstrated similar performance compared to using the whole dataset, with only 0.2 loss on GR and 0.6 loss on CG. We can observe that even with only $1 \%$ of the original dataset,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Indicator</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">GR</th>
<th style="text-align: center;">CG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: left;">Human Prior</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">78.0</td>
</tr>
<tr>
<td style="text-align: left;">Agent</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">79.3</td>
</tr>
<tr>
<td style="text-align: left;">Imp. Score</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">82.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$\mathbf{7 3 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Experimental results of different indicators used in agent selection during team optimization in DyLAN on five subjects in the GR and CG tasks. "Dataset" denotes the proportion of dataset used in team optimization.</p>
<p>DyLAN could obtain a significant improvement of +3.7 over random selection on CG. From observation, agents augmented with tools are always selected during team optimization under different proportions of the dataset, indicating the effectiveness of Agent Importance Score as an indicator. Please refer to Appendix C. 3 for a detailed analysis of the human priors.</p>
<h1>C. 2 Robustness of different foundation models in DyLAN</h1>
<p>Besides using GPT-3.5 for DyLAN on CG tasks in Table 2, we also experiment with GPT-4 in Table 10. Due to budget limits, we directly reuse the performance reported in the paper of baselines, including LATS (Zhou et al., 2023), Reflexion (Shinn et al., 2023), Meta-GPT (Hong et al., 2024), and AgentVerse (Chen et al., 2024), and estimate the cost in terms of numbers of API calls. DyLAN is also constructed by agents which are optimized based on GPT-3.5, as demonstrated in Appendix B.1. We found that DyLAN consistently outperforms other multi-agent methods, indicating the effectiveness of dynamic agent team in T-FFN structure and the cross-model transferability of optimization results on agent teams. Although LATS outperforms DyLAN, it requires over 40 times GPT-4 calls per sample to conduct inference-time MCTS on GPT-4, which demonstrates poor efficiency.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Pass@1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">#API Calls</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single-Agent Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Single Execution</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">$(+0.0)$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">LATS</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">$(+6.0)$</td>
<td style="text-align: center;">$&gt;40.00$</td>
</tr>
<tr>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">$(+3.0)$</td>
<td style="text-align: center;">7.32</td>
</tr>
<tr>
<td style="text-align: center;">Multi-Agent Methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Meta-GPT</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">$(-3.5)$</td>
<td style="text-align: center;">$&gt;30.00$</td>
</tr>
<tr>
<td style="text-align: center;">AgentVerse</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">$(+0.6)$</td>
<td style="text-align: center;">27.00</td>
</tr>
<tr>
<td style="text-align: center;">DyLAN (Ours)</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">$(+3.7)$</td>
<td style="text-align: center;">15.94</td>
</tr>
</tbody>
</table>
<p>Table 10: Experimental results on the CG task on GPT-4-0613. The number in parentheses indicates the difference relative to the single execution or direct execution. The bold font denotes the results of our method and the best results are underlined.</p>
<h2>C. 3 Human Priors and Agent Importance Scores</h2>
<p>We further investigated how these agents selected by our unsupervised metric Agent Importance Score differ from human priors (e.g., these predefined roles). To do so, we calculated agent importance scores for 7 agents on each subject of the MMLU dataset. As an example, we show the subjects where the agent of "Doctor" and "Programmer" has the highest agent importance score among all agents in Table 11 and Table 12.</p>
<p>Though most subjects seems to be reasonably aligned with the role of the agent based on human priors (with green annotations), there are some subjects that do not match human priors, e.g., High School Computer Science as the subject that "Doctor" has the highest score. It exhibits the difference between human priors and the evaluation results of agent importance scores on agents with human-made or LLM-generated prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Role</th>
<th style="text-align: center;">Doctor</th>
<th style="text-align: center;">Programmer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Top 10 Subjects</td>
<td style="text-align: center;">high school computer science clinical knowledge college biology professional medicine nutrition high school US history human aging anatomy high school biology high school psychology</td>
<td style="text-align: center;">high school physics electrical engineering high school government and politics college computer science college chemistry high school mathematics formal logic abstract algebra machine learning computer security</td>
</tr>
</tbody>
</table>
<p>Table 11: Subjects on which agents have the top-ranked Agent Importance Score in the experiment with DyLAN of 7 agents on the GR task. Green annotation denotes the fields related to the role from the human perspective, which are annotated manually.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Role</th>
<th style="text-align: center;">Mathematician</th>
<th style="text-align: center;">Lawyer</th>
<th style="text-align: center;">Historian</th>
<th style="text-align: center;">Economist</th>
<th style="text-align: center;">Psychologist</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Top 10 <br> Sub- <br> jects</td>
<td style="text-align: center;">college physics <br> US foreign policy <br> college computer science <br> econometrics <br> marketing <br> high school mathematics <br> abstract algebra <br> international law <br> professional accounting <br> human sexuality</td>
<td style="text-align: center;">high school microeconomics medical genetics <br> prehistory <br> sociology <br> human aging <br> management <br> formal logic <br> world religions <br> jurisprudence <br> international law</td>
<td style="text-align: center;">US foreign policy econometrics world religions public relations high school government and politics philosophy astronomy high school statistics machine learning high school European history</td>
<td style="text-align: center;">high school computer science <br> jurisprudence <br> logical fallacies <br> professional accounting <br> high school microeconomics <br> high school European history <br> computer security <br> moral disputes <br> professional law <br> college mathematics</td>
<td style="text-align: center;">global facts <br> public relations <br> business ethics <br> high school US history <br> philosophy <br> moral disputes <br> management</td>
</tr>
</tbody>
</table>
<p>Table 12: Subjects on which agents have the top-ranked Agent Importance Score in the same experiment in Table 11. Green annotation denotes the fields highly related to the role from the human perspective.</p>
<p>We also compare current agent selection method that is implemented with Agent Importance Score with the implementation with Human Prior Selection on a few subjects in the MMLU and the HumanEval datasets. For Human Prior Selection, we setup GPT-4 mimicking human selecting the agents for collaborations based on the description of the task and role prompts of each agent. We provide prompt templates in Appendix D. As shown in Table 13, the implementation with Agent Importance Score steadily outperforms Human Prior Selection. There are two major reasons: (1) Compared to posterior optimization methods, prior selection may not grasp the actual behaviors of agents, and may not understand which agents are most contributory or helpful to others in the real collaboration process. Thus, in High School Statistics, Clinical Knowledge, and Public Relations subjects in the MMLU dataset, prior selection performs even worse than random selection. (2) Human Prior Selection might struggle to understand tool augmentation without peer ratings from fellow agents. From our observation, "Unit Tester" and "Syntax Checker" were not selected for code generation tasks, which may cause lower performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#Agents</th>
<th style="text-align: center;">Optimization <br> Indicator</th>
<th style="text-align: center;">College <br> Mathematics</th>
<th style="text-align: center;">Management</th>
<th style="text-align: center;">High School <br> Statistics</th>
<th style="text-align: center;">Clinical <br> Knowledge</th>
<th style="text-align: center;">Public <br> Relations</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">(before optimization)</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$63.5(+0.0)$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Random Selection</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$64.8(+0.3)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human Prior Selection</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$66.7(+3.2)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Agent Importance Score</td>
<td style="text-align: center;">$\mathbf{6 5 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 6}(+10.1)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Agents</td>
<td style="text-align: center;">Optimization Indicator</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">#API Calls</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">(before optimization)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$76.2(+0.0)$</td>
<td style="text-align: center;">23.04</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Random Selection</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$75.6(+0.6)$</td>
<td style="text-align: center;">17.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human Prior Selection</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$78.0(+1.8)$</td>
<td style="text-align: center;">16.37</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Agent Importance Score</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{8 2 . 9}(+6.7)$</td>
<td style="text-align: center;">16.85</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 13: Detailed performance of different indicators of agent selection on five subjects in GR tasks (top) and the CG task (bottom). The five subjects in GR tasks and other settings are identical to Table 7. The overall accuracy in the top table denotes the accuracy across the five subjects.</p>
<h1>C. 4 Stability of DyLAN on Temperature</h1>
<p>We tested a few methods on the AR (with simple CoT prompts) and the CG tasks under both low and high temperatures and repeated each experiment three times when the temperature was not zero. We exhibit the experimental results in Figure 4. From experimental results, we found that DyLAN is more stable on different hyper-parameters.
Experiments show that temperature greatly influences arithmetic reasoning and code generation tasks. In Figure 4, we found that most baseline methods have significant performance drops when temperature increases, but DyLAN shows strong robustness to various temperatures. We surprisingly found that DyLAN gets better results when temperature rises, suggesting it has benefited from diversity instead of being disturbed by low-quality answers of high-temperature agents. The agent team reformation may lead to the higher accuracy by keeping best responses when agents' replies become more diverse. In conclusion, the collaboration of different roles functions effectively and robustly in the dynamic architecture.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance of different methods under low and high temperatures on AR (left) and CG (right) tasks. DyLAN shows better robustness to different temperature and even takes advantage of higher temperature.</p>
<p>Nonetheless, higher temperature requires DyLAN to take more API calls (about +0.98 on average on AR tasks (temperature: $0.2 \rightarrow 1.0$ )).</p>
<h1>C. 5 Different Ranking Methods</h1>
<p>We also tested different ranking methods for agent team reformation of DyLAN on the GR task. We tested listwise ranker with our own prompts, pairwise GPT ranker from original LLM-Blender (Jiang et al., 2023), Elo Score from TrueSkill (Herbrich et al., 2006) also implemented with pairwise ranker, and pairwise ranker with Sliding Window algorithm (Qin et al., 2023). In Table 14, we show that different ranking methods have a relatively low impact on performance, probably because of strong discrimination ability of GPT-3.5, but pairwise ranking methods always consume higher computational cost. Thus, we chose a listwise ranker in our implementation of DyLAN.</p>
<h2>C. 6 Does Agent Importance Score Captures Actual Contributions?</h2>
<p>Shapley Value is a widely used supervised metric for evaluating contribution of a single agent in a multi-agent system. Though it is not suitable for unsupervised Team Optimization, by viewing it as a ground-truth metric for measuring individual contributions, we can use it for validating the Agent Importance Score. We implement a simplified algorithm for LLMpowered agent collaboration systems. Given that the collaboration process is symmetric in the formulation of the temporal feed-forward network (Section 3.2), we could reduce the permutation set in the original formula (Lundberg \&amp; Lee, 2017) to the combination set:</p>
<p>$$
S_{i}(\mathcal{R})=\frac{1}{|\mathcal{C}||\mathcal{R}|} \sum_{\mathcal{T} \in \mathcal{C}}(\operatorname{Performance}(\mathcal{T} \cup{i})-\operatorname{Performance}(\mathcal{T}))
$$</p>
<p>where $\mathcal{R}$ is the set of agents in the system, $\mathcal{C}$ is the combination set of $\mathcal{R} \backslash{i}, i \in \mathcal{R}$, and Performance denotes the overall performance of the system on the current task, e.g., classification accuracy or Pass@1. The metric requires ground truth and multi-pass results of the system with different subsets of agents. We use classification accuracy for classification tasks and Pass@1 for code generation tasks. However, its computation cost is still too high when the number of agents grows larger due to its combinatorial complexity.
To examine Agent Importance Score as an indicator of agent selection with Shapley Value, we also randomly chose three combinations of three agents out of all 7 candidates to assemble a T-FFN and calculated the Shapley Value and the Agent Importance Score on GR tasks. In the GR task, The roles of candidates in DyLAN match the categories of MMLU in human priors, including "Mathematician" and "Programmer" for STEM, "Lawyer" and "Historian" for</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The signature of sacreBLEU is "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1".&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
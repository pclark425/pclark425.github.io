<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1210 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1210</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1210</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-264145997</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09615v1.pdf" target="_blank">STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of $126.7\%$ on the Atari $100$k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with $1.85$ hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only $4.3$ hours, showcasing improved efficiency compared to previous methodologies.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1210.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1210.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Transformer-based wORld Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A world model combining a categorical VAE image encoder (discrete stochastic latent) with a GPT-like masked Transformer sequence model to generate imagined trajectories for model-based RL; optimized end-to-end and designed for efficient parallel training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space predictive world model: images are encoded by a categorical VAE (Z_t with 32 categories × 32 classes) producing discrete stochastic latents z_t; z_t is concatenated with action a_t into a single token e_t which is fed to a GPT-like masked Transformer sequence model (2 layers by default) to produce hidden states h_t; heads predict next latent distribution Ẑ_{t+1}, reward, and continuation flag; imagination samples from prior Ẑ during rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic discrete latent + neural sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (Atari 100k benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (L_rec = ||ô_t - o_t||^2), symlog two-hot loss for reward prediction, KL divergences for dynamics and representation (L_dyn and L_rep); overall optimized end-to-end via sum of these terms.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No direct scalar next-frame prediction MSE reported; image reconstruction and KL losses are used as training objectives. Task-level fidelity manifested as downstream policy performance: mean human-normalized score = 126.7% on Atari 100k (average over 26 games).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural model, but attention mechanism in the Transformer provides an implicit, inspectable mechanism that preserves object history and helps infer motion (speed/direction) for large/multiple objects; discrete latent (categorical VAE) may be more interpretable than high-dimensional continuous vectors, but no explicit latent-to-attribute mapping is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Implicitly via attention mechanism (discussion of attention preserving history); use of discrete categorical latents (Z_t) noted as design choice that could aid inspection; no explicit latent visualizations or probing analyses presented in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported training for 1.85 hours of environment interaction (Atari 100k) required 4.3 hours wall-clock on a single NVIDIA GeForce RTX 3090; Transformer configured with 2 layers (smaller than prior Transformer world models). Categorical VAE latent size: 32×32 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reported as more computationally efficient than previous Transformer-based world models (IRIS, TWM) and RNN-based SimPLe; benefits from parallelizable Transformer training. Compared to DreamerV3, STORM required less wall-clock for the same 100k interactions in the authors' experiments and achieved higher mean score.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mean human-normalized score on Atari 100k = 126.7% (state-of-the-art among methods without lookahead search reported in the paper). Better than DreamerV3 (112%), IRIS (105%), TWM (96%), SimPLe (33%) as reported in the paper's summary table.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>STORM's stochastic discrete latent reduces accumulated autoregressive prediction errors and improves robustness of imagined rollouts, producing high-quality imagined data that improves policy learning; particularly effective on games where reward-relevant objects are large or multiple (e.g., MsPacman, Amidar). Performs worse on tasks where key relevant objects are single and very small (e.g., Pong, Breakout) due to autoencoder limitations and noise affecting attention.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Design trades: (1) stochastic discrete latent (helps reduce compounding error but introduces sampling randomness that can destabilize attention for very small objects), (2) small Transformer (2 layers) yields good efficiency but scaling to more layers did not improve performance under limited data; (3) end-to-end training couples encoder and sequence model creating non-stationarity limiting scalability; (4) using single-token per timestep (z_t + a_t) is computationally cheaper than multi-token spatio-temporal tokenizations but loses some per-pixel token expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Categorical VAE encoder (32×32 categorical latent), straight-through gradients for sampling; action+latent fused into single token via MLP; GPT-like masked Transformer (default 2 layers); predict next latent distribution, reward, and continuation via MLP heads; sample from prior Ẑ during imagination; KV-cache used to accelerate inference; loss weighting β1=0.5 (dynamics KL) and β2=0.1 (representation KL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to RNN-based models (SimPLe, Dreamer series) STORM is more parallelizable and shows higher sample-efficiency task performance on Atari 100k (STORM 126.7% vs DreamerV3 112%). Compared to Transformer-based alternatives (TWM, IRIS), STORM uses fewer layers and a single stochastic latent token, achieving better training efficiency and improved average scores; IRIS/TWM's multi-token or spatial-temporal attention increases computational cost and can slow training.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend a relatively small Transformer (2 layers) with categorical discrete latent, fusing z_t and a_t into one token, and using [h_t, z_t] as the agent state; increasing Transformer depth did not help under the 100k-sample regime. They caution end-to-end optimization coupling and recommend attention to starting-point sampling for imagination and potential inclusion of demonstration trajectories in sparse-reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1210.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1210.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3 (Mastering diverse domains through world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GRU-based latent world model (successor in the Dreamer series) that learns latent dynamics with stochastic latent variables and trains agents by imagining trajectories in latent space; demonstrates strong performance across multiple domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent variable world model with image encoder and recurrent (GRU) sequence model; learns compact latent representations via VAE-style objectives and performs latent imagination to train actor-critic policies (as used in the Dreamer family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic latent + recurrent sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and multiple control domains (cited usage across diverse domains; in this paper compared on Atari 100k)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Uses reconstruction loss, KL divergences and symlog two-hot losses for reward prediction (as in Dreamer family).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No per-step predictive fidelity numbers given in this paper; downstream task performance reported as mean human-normalized score of 112% on Atari 100k (as presented in comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural model; uses recurrent hidden state which carries temporal context but not presented as explicitly interpretable; no interpretability analysis presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper beyond noting that Dreamer uses latent and hidden states as agent state.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>GRU-based recurrence limits parallelism compared to Transformers; DreamerV3 was directly evaluated on the authors' hardware and used as a baseline for efficiency comparisons (specific wall-clock not reproduced here except comparative statements).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less parallelizable than Transformer architectures but shown previously to be strong; STORM claims improved efficiency over DreamerV3 in the 100k-sample setting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported mean human-normalized score on Atari 100k = 112% (as reported in the paper's comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Dreamer family provides effective imagined trajectories for policy optimization; robust across many tasks, but recurrent architecture limits parallel training speed relative to Transformer-based designs.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Recurrent models capture temporal dependencies but impede parallel training speed; may be less efficient wall-clock-wise than Transformer designs under the same data budget.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses GRU recurrence, stochastic latent variables, symlog two-hot reward loss, and uses latent+hidden state for the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed by STORM on average Atari 100k score according to paper; STORM trades recurrent recurrence for Transformer parallelism and discrete latents to reduce compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in detail in this paper; DreamerV3 settings are used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1210.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1210.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated Policy Learning (SimPLe) / VAE-LSTM based world model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early VAE-LSTM latent world model for image-based RL that uses an LSTM sequence model and a VAE encoder/decoder to learn environment dynamics for model-based policy training from imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model based reinforcement learning for atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe (VAE + LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image encoder (VAE) to latent, LSTM recurrent sequence model to predict next latent/image, trained with reconstruction and prediction losses; used to generate imagined trajectories for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic latent + recurrent sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (Atari 100k and related sample-efficient benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss and predictive losses (reconstruction of next frames); use of VAE objectives to model stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No scalar predictive fidelity reported here; in task-level comparison SimPLe achieved a mean human-normalized score of ~33% on Atari 100k in the paper's table.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural model (VAE + LSTM); no interpretability analyses are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SimPLe uses LSTM recurrence which limits parallelization; original SimPLe reported long training (paper cites 20 days on NVIDIA P100 as the original computing resource).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Slower training than Transformer-based approaches due to recurrence and less parallelism; STORM and later methods claim improved wall-clock efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mean human-normalized score on Atari 100k reported in paper comparison table = 33%.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrated feasibility of learning policies solely from generated data but underperforms more recent world models in the 100k-sample regime.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>LSTM provides temporal modeling but hurts parallel training efficiency; older architecture compared to Dreamer/STORM.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>LSTM sequence model, VAE image encoder/decoder, end-to-end training of dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed by Dreamer series and STORM in the paper's Atari 100k comparisons; less efficient in wall-clock and lower task scores under limited samples.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper beyond noting the limitations of RNN recurrence for parallel training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1210.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1210.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TWM (Transformer-XL based world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-XL based world model that treats observation, action, and reward as separate tokens fed to a Transformer sequence model; cited as a Transformer-based alternative with higher token counts and training cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TWM (Transformer-XL variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence model based on Transformer-XL organizing sequences similarly to Decision Transformer, treating observation, action and reward as separate tokens; relies on self-attention over these tokens to model dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (Transformer-XL based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (compared on Atari 100k in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not explicitly specified in this paper beyond standard reconstruction and predictive losses used for latent world models.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Task-level mean human-normalized score reported in comparison table = 96% on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Transformer attention allows inspecting token interactions in principle, but TWM's treatment of heterogeneous tokens (obs/action/reward) may complicate interpretability; paper notes potential negative impact of treating types equally.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher computational cost due to more tokens per timestep and Transformer-XL architecture; training reported on A100 hardware in original work and extrapolated for comparisons here—paper notes increased number of tokens considerably slows down training.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Slower training relative to STORM due to multi-token design and larger Transformer-XL structure; authors report TWM did not surpass GRU-based DreamerV3 in some regimes and was slower.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mean human-normalized score on Atari 100k reported = 96%.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transformer-XL preserves long context but multi-token design and heterogenous inputs can complicate modeling and slow training; utility depends on design choices for tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Long-context capability versus computational cost and complexity of treating different data types as equal tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transformer-XL backbone; separate tokens for observation, action, reward; more extensive attention over tokens increases compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared unfavorably in efficiency to STORM; competitive in sample efficiency but higher compute cost and not consistently better than DreamerV3 on reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided in this paper; authors contrast TWM multi-token design with STORM's single-token-per-timestep design as a more efficient alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1210.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1210.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRIS (Transformer-based world model with VQ-VAE encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer world model that encodes images into spatial token grids (VQ-VAE producing 4×4 latent tokens) and applies spatial-temporal Transformer attention to model dynamics, at the cost of many tokens and heavier attention computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses VQ-VAE image encoder mapping images into a 4×4 grid of discrete latent tokens; a spatial-temporal Transformer captures within-image and across-image dependencies via attention over the token grid and time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete tokenized latent + spatial-temporal Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (discussed and compared on Atari 100k)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trained with reconstruction/prediction losses appropriate for VQ-VAE + Transformer; specifics not numerically reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Task-level mean human-normalized score reported in comparison table = 105% on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Tokenized spatial latent representation (4×4 tokens) provides a more spatially explicit representation that can be inspected, but the paper notes heavy attention over many tokens complicates computation; no explicit interpretability analyses provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Use of VQ-VAE produces discrete tokens that are in principle inspectable; no explicit visualization reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High computational cost due to attention over many spatial-temporal tokens (slows training significantly according to the paper). Original work used large GPUs (A100) and incurred greater training time.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient than STORM due to larger token counts and deeper Transformer (IRIS uses many Transformer layers per the paper); STORM opts for single latent token per image to improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mean human-normalized score on Atari 100k reported = 105%.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Spatial tokenization can capture within-image structure better (useful for tasks where spatial layout matters), but the computational overhead may limit practicality under tight sample/compute budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher spatial fidelity and token-level representation versus much larger attention cost and slower training.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VQ-VAE encoder producing 4×4 token grid; spatial-temporal Transformer; attention across many tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to STORM's single stochastic latent token, IRIS provides finer spatial tokenization but at a heavier computational cost and without clear average performance advantage in the 100k regime.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified here; the paper highlights the cost of spatial-temporal attention and suggests token count affects efficiency negatively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1210.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1210.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TransDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TransDreamer (Transformer replacement of Dreamer RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant replacing Dreamer-style GRUs with Transformer blocks to model latent dynamics; an attempt to combine Dreamer-style latent modeling with Transformer sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transdreamer: Reinforcement learning with transformer world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TransDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Direct replacement of GRU recurrence in the Dreamer architecture with Transformer blocks to predict latent dynamics and imagined rollouts for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (Transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari and other control domains (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Same classes of losses as Dreamer family (reconstruction, KL, reward loss); no per-step fidelity scalars provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>The paper notes a lack of widespread evidence demonstrating TransDreamer's performance under limited-sample widely used environments, implying mixed or unestablished advantage in the 100k regime.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in detail here; inherits Transformer attention interpretability potential but no explicit analyses in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformer replacement increases parallelism but may come with larger model and training time depending on tokenization and model size; the paper notes that prior Transformer replacements did not clearly surpass GRU-based DreamerV3 in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mixed: while Transformers enable parallelism, existing TransDreamer implementations have not demonstrated consistent wall-clock or sample-efficiency advantages over GRU Dreamer in the 100k regime according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not explicitly reported in the paper's summary table for TransDreamer; authors state lack of clear evidence of its performance under the 100k constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Potential to combine Dreamer-style latent imagination benefits with Transformer parallelism, but empirical utility under tight-sample budgets is not established in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Replacing recurrence with Transformer can improve parallelism but may require design care (tokenization, model scale) and enough data to train larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Directly replace GRU with Transformer in Dreamer pipeline; specific tokenization and training choices affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors indicate TransDreamer has not clearly outperformed GRU-based DreamerV3 in standard limited-sample benchmarks and thus may not offer a straightforward improvement without further design adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper; authors encourage careful design to harness Transformer benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1210.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1210.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS-based (MuZero/EfficientZero/SpeedyZero)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MCTS-augmented learned models (e.g., MuZero, EfficientZero, SpeedyZero)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>World models combined with Monte Carlo Tree Search (lookahead search) used to improve policy/planning beyond pure imagined rollouts; effective but computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering Atari, Go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MCTS-augmented learned models (MuZero / EfficientZero / SpeedyZero)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learned dynamics and value/policy predictors combined with Monte Carlo Tree Search to perform lookahead planning at decision time; the learned model supports search-based decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid world model + planning (learned model + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (and other board/control domains); cited here for Atari 100k context</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality of the learned model is typically evaluated via search performance and downstream task score (e.g., game score); specific fidelity metrics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not directly compared in numbers here because the paper focuses on pure world-model-driven imagination methods and excludes lookahead search methods from head-to-head comparison; these methods achieve strong performance in other work but at higher compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Planning-based methods expose search trajectories that can be inspected, providing some interpretability of decision-time reasoning; learned model components remain neural and less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of MCTS rollouts and node statistics in principle; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High computational demands due to lookahead search at decision time (authors explicitly note MCTS is computationally expensive and thus not directly compared in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Higher computational cost but can yield stronger scores on some benchmarks; not compared directly in experiments in this paper because focus was on world-model improvements without lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in the paper's Atari 100k comparison table because lookahead methods were intentionally excluded from direct comparison; cited as competitive in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Lookahead search improves policy by leveraging the learned model at decision time, but high runtime compute can limit applicability in sample-limited/time-limited settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Better planning and often higher performance versus substantially greater computational cost at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Combine learned dynamics/value/policy heads with MCTS; tuning search depth/width and model fidelity is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors excluded these from primary comparisons due to high compute; note they could potentially be combined with STORM in future work to further improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Model based reinforcement learning for atari <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Transdreamer: Reinforcement learning with transformer world models <em>(Rating: 2)</em></li>
                <li>Transformer-based world models are happy with 100k interactions <em>(Rating: 2)</em></li>
                <li>Transformers are sample-efficient world models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1210",
    "paper_id": "paper-264145997",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "STORM",
            "name_full": "Stochastic Transformer-based wORld Model",
            "brief_description": "A world model combining a categorical VAE image encoder (discrete stochastic latent) with a GPT-like masked Transformer sequence model to generate imagined trajectories for model-based RL; optimized end-to-end and designed for efficient parallel training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "STORM",
            "model_description": "Latent-space predictive world model: images are encoded by a categorical VAE (Z_t with 32 categories × 32 classes) producing discrete stochastic latents z_t; z_t is concatenated with action a_t into a single token e_t which is fed to a GPT-like masked Transformer sequence model (2 layers by default) to produce hidden states h_t; heads predict next latent distribution Ẑ_{t+1}, reward, and continuation flag; imagination samples from prior Ẑ during rollouts.",
            "model_type": "latent world model (stochastic discrete latent + neural sequence model)",
            "task_domain": "Atari games (Atari 100k benchmark)",
            "fidelity_metric": "Reconstruction loss (L_rec = ||ô_t - o_t||^2), symlog two-hot loss for reward prediction, KL divergences for dynamics and representation (L_dyn and L_rep); overall optimized end-to-end via sum of these terms.",
            "fidelity_performance": "No direct scalar next-frame prediction MSE reported; image reconstruction and KL losses are used as training objectives. Task-level fidelity manifested as downstream policy performance: mean human-normalized score = 126.7% on Atari 100k (average over 26 games).",
            "interpretability_assessment": "Primarily a black-box neural model, but attention mechanism in the Transformer provides an implicit, inspectable mechanism that preserves object history and helps infer motion (speed/direction) for large/multiple objects; discrete latent (categorical VAE) may be more interpretable than high-dimensional continuous vectors, but no explicit latent-to-attribute mapping is reported.",
            "interpretability_method": "Implicitly via attention mechanism (discussion of attention preserving history); use of discrete categorical latents (Z_t) noted as design choice that could aid inspection; no explicit latent visualizations or probing analyses presented in the paper.",
            "computational_cost": "Reported training for 1.85 hours of environment interaction (Atari 100k) required 4.3 hours wall-clock on a single NVIDIA GeForce RTX 3090; Transformer configured with 2 layers (smaller than prior Transformer world models). Categorical VAE latent size: 32×32 tokens.",
            "efficiency_comparison": "Reported as more computationally efficient than previous Transformer-based world models (IRIS, TWM) and RNN-based SimPLe; benefits from parallelizable Transformer training. Compared to DreamerV3, STORM required less wall-clock for the same 100k interactions in the authors' experiments and achieved higher mean score.",
            "task_performance": "Mean human-normalized score on Atari 100k = 126.7% (state-of-the-art among methods without lookahead search reported in the paper). Better than DreamerV3 (112%), IRIS (105%), TWM (96%), SimPLe (33%) as reported in the paper's summary table.",
            "task_utility_analysis": "STORM's stochastic discrete latent reduces accumulated autoregressive prediction errors and improves robustness of imagined rollouts, producing high-quality imagined data that improves policy learning; particularly effective on games where reward-relevant objects are large or multiple (e.g., MsPacman, Amidar). Performs worse on tasks where key relevant objects are single and very small (e.g., Pong, Breakout) due to autoencoder limitations and noise affecting attention.",
            "tradeoffs_observed": "Design trades: (1) stochastic discrete latent (helps reduce compounding error but introduces sampling randomness that can destabilize attention for very small objects), (2) small Transformer (2 layers) yields good efficiency but scaling to more layers did not improve performance under limited data; (3) end-to-end training couples encoder and sequence model creating non-stationarity limiting scalability; (4) using single-token per timestep (z_t + a_t) is computationally cheaper than multi-token spatio-temporal tokenizations but loses some per-pixel token expressivity.",
            "design_choices": "Categorical VAE encoder (32×32 categorical latent), straight-through gradients for sampling; action+latent fused into single token via MLP; GPT-like masked Transformer (default 2 layers); predict next latent distribution, reward, and continuation via MLP heads; sample from prior Ẑ during imagination; KV-cache used to accelerate inference; loss weighting β1=0.5 (dynamics KL) and β2=0.1 (representation KL).",
            "comparison_to_alternatives": "Compared to RNN-based models (SimPLe, Dreamer series) STORM is more parallelizable and shows higher sample-efficiency task performance on Atari 100k (STORM 126.7% vs DreamerV3 112%). Compared to Transformer-based alternatives (TWM, IRIS), STORM uses fewer layers and a single stochastic latent token, achieving better training efficiency and improved average scores; IRIS/TWM's multi-token or spatial-temporal attention increases computational cost and can slow training.",
            "optimal_configuration": "Authors recommend a relatively small Transformer (2 layers) with categorical discrete latent, fusing z_t and a_t into one token, and using [h_t, z_t] as the agent state; increasing Transformer depth did not help under the 100k-sample regime. They caution end-to-end optimization coupling and recommend attention to starting-point sampling for imagination and potential inclusion of demonstration trajectories in sparse-reward tasks.",
            "uuid": "e1210.0",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DreamerV3",
            "name_full": "DreamerV3 (Mastering diverse domains through world models)",
            "brief_description": "A GRU-based latent world model (successor in the Dreamer series) that learns latent dynamics with stochastic latent variables and trains agents by imagining trajectories in latent space; demonstrates strong performance across multiple domains.",
            "citation_title": "Mastering diverse domains through world models",
            "mention_or_use": "mention",
            "model_name": "DreamerV3",
            "model_description": "Latent variable world model with image encoder and recurrent (GRU) sequence model; learns compact latent representations via VAE-style objectives and performs latent imagination to train actor-critic policies (as used in the Dreamer family).",
            "model_type": "latent world model (stochastic latent + recurrent sequence model)",
            "task_domain": "Atari games and multiple control domains (cited usage across diverse domains; in this paper compared on Atari 100k)",
            "fidelity_metric": "Uses reconstruction loss, KL divergences and symlog two-hot losses for reward prediction (as in Dreamer family).",
            "fidelity_performance": "No per-step predictive fidelity numbers given in this paper; downstream task performance reported as mean human-normalized score of 112% on Atari 100k (as presented in comparison table).",
            "interpretability_assessment": "Black-box neural model; uses recurrent hidden state which carries temporal context but not presented as explicitly interpretable; no interpretability analysis presented here.",
            "interpretability_method": "Not discussed in this paper beyond noting that Dreamer uses latent and hidden states as agent state.",
            "computational_cost": "GRU-based recurrence limits parallelism compared to Transformers; DreamerV3 was directly evaluated on the authors' hardware and used as a baseline for efficiency comparisons (specific wall-clock not reproduced here except comparative statements).",
            "efficiency_comparison": "Less parallelizable than Transformer architectures but shown previously to be strong; STORM claims improved efficiency over DreamerV3 in the 100k-sample setting.",
            "task_performance": "Reported mean human-normalized score on Atari 100k = 112% (as reported in the paper's comparison table).",
            "task_utility_analysis": "Dreamer family provides effective imagined trajectories for policy optimization; robust across many tasks, but recurrent architecture limits parallel training speed relative to Transformer-based designs.",
            "tradeoffs_observed": "Recurrent models capture temporal dependencies but impede parallel training speed; may be less efficient wall-clock-wise than Transformer designs under the same data budget.",
            "design_choices": "Uses GRU recurrence, stochastic latent variables, symlog two-hot reward loss, and uses latent+hidden state for the agent.",
            "comparison_to_alternatives": "Outperformed by STORM on average Atari 100k score according to paper; STORM trades recurrent recurrence for Transformer parallelism and discrete latents to reduce compounding errors.",
            "optimal_configuration": "Not discussed in detail in this paper; DreamerV3 settings are used as a comparative baseline.",
            "uuid": "e1210.1",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "Simulated Policy Learning (SimPLe) / VAE-LSTM based world model",
            "brief_description": "An early VAE-LSTM latent world model for image-based RL that uses an LSTM sequence model and a VAE encoder/decoder to learn environment dynamics for model-based policy training from imagined rollouts.",
            "citation_title": "Model based reinforcement learning for atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe (VAE + LSTM)",
            "model_description": "Image encoder (VAE) to latent, LSTM recurrent sequence model to predict next latent/image, trained with reconstruction and prediction losses; used to generate imagined trajectories for policy learning.",
            "model_type": "latent world model (stochastic latent + recurrent sequence model)",
            "task_domain": "Atari games (Atari 100k and related sample-efficient benchmarks)",
            "fidelity_metric": "Reconstruction loss and predictive losses (reconstruction of next frames); use of VAE objectives to model stochasticity.",
            "fidelity_performance": "No scalar predictive fidelity reported here; in task-level comparison SimPLe achieved a mean human-normalized score of ~33% on Atari 100k in the paper's table.",
            "interpretability_assessment": "Black-box neural model (VAE + LSTM); no interpretability analyses are reported in this paper.",
            "interpretability_method": "None reported in this paper.",
            "computational_cost": "SimPLe uses LSTM recurrence which limits parallelization; original SimPLe reported long training (paper cites 20 days on NVIDIA P100 as the original computing resource).",
            "efficiency_comparison": "Slower training than Transformer-based approaches due to recurrence and less parallelism; STORM and later methods claim improved wall-clock efficiency.",
            "task_performance": "Mean human-normalized score on Atari 100k reported in paper comparison table = 33%.",
            "task_utility_analysis": "Demonstrated feasibility of learning policies solely from generated data but underperforms more recent world models in the 100k-sample regime.",
            "tradeoffs_observed": "LSTM provides temporal modeling but hurts parallel training efficiency; older architecture compared to Dreamer/STORM.",
            "design_choices": "LSTM sequence model, VAE image encoder/decoder, end-to-end training of dynamics.",
            "comparison_to_alternatives": "Outperformed by Dreamer series and STORM in the paper's Atari 100k comparisons; less efficient in wall-clock and lower task scores under limited samples.",
            "optimal_configuration": "Not discussed in this paper beyond noting the limitations of RNN recurrence for parallel training.",
            "uuid": "e1210.2",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "TWM",
            "name_full": "TWM (Transformer-XL based world model)",
            "brief_description": "A Transformer-XL based world model that treats observation, action, and reward as separate tokens fed to a Transformer sequence model; cited as a Transformer-based alternative with higher token counts and training cost.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "TWM (Transformer-XL variant)",
            "model_description": "Sequence model based on Transformer-XL organizing sequences similarly to Decision Transformer, treating observation, action and reward as separate tokens; relies on self-attention over these tokens to model dynamics.",
            "model_type": "latent world model (Transformer-XL based)",
            "task_domain": "Atari games (compared on Atari 100k in the paper)",
            "fidelity_metric": "Not explicitly specified in this paper beyond standard reconstruction and predictive losses used for latent world models.",
            "fidelity_performance": "Task-level mean human-normalized score reported in comparison table = 96% on Atari 100k.",
            "interpretability_assessment": "Transformer attention allows inspecting token interactions in principle, but TWM's treatment of heterogeneous tokens (obs/action/reward) may complicate interpretability; paper notes potential negative impact of treating types equally.",
            "interpretability_method": "Not detailed in this paper.",
            "computational_cost": "Higher computational cost due to more tokens per timestep and Transformer-XL architecture; training reported on A100 hardware in original work and extrapolated for comparisons here—paper notes increased number of tokens considerably slows down training.",
            "efficiency_comparison": "Slower training relative to STORM due to multi-token design and larger Transformer-XL structure; authors report TWM did not surpass GRU-based DreamerV3 in some regimes and was slower.",
            "task_performance": "Mean human-normalized score on Atari 100k reported = 96%.",
            "task_utility_analysis": "Transformer-XL preserves long context but multi-token design and heterogenous inputs can complicate modeling and slow training; utility depends on design choices for tokenization.",
            "tradeoffs_observed": "Long-context capability versus computational cost and complexity of treating different data types as equal tokens.",
            "design_choices": "Transformer-XL backbone; separate tokens for observation, action, reward; more extensive attention over tokens increases compute.",
            "comparison_to_alternatives": "Compared unfavorably in efficiency to STORM; competitive in sample efficiency but higher compute cost and not consistently better than DreamerV3 on reported metrics.",
            "optimal_configuration": "Not provided in this paper; authors contrast TWM multi-token design with STORM's single-token-per-timestep design as a more efficient alternative.",
            "uuid": "e1210.3",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "IRIS",
            "name_full": "IRIS (Transformer-based world model with VQ-VAE encoder)",
            "brief_description": "A Transformer world model that encodes images into spatial token grids (VQ-VAE producing 4×4 latent tokens) and applies spatial-temporal Transformer attention to model dynamics, at the cost of many tokens and heavier attention computation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "IRIS",
            "model_description": "Uses VQ-VAE image encoder mapping images into a 4×4 grid of discrete latent tokens; a spatial-temporal Transformer captures within-image and across-image dependencies via attention over the token grid and time.",
            "model_type": "latent world model (discrete tokenized latent + spatial-temporal Transformer)",
            "task_domain": "Atari games (discussed and compared on Atari 100k)",
            "fidelity_metric": "Trained with reconstruction/prediction losses appropriate for VQ-VAE + Transformer; specifics not numerically reported in this paper.",
            "fidelity_performance": "Task-level mean human-normalized score reported in comparison table = 105% on Atari 100k.",
            "interpretability_assessment": "Tokenized spatial latent representation (4×4 tokens) provides a more spatially explicit representation that can be inspected, but the paper notes heavy attention over many tokens complicates computation; no explicit interpretability analyses provided here.",
            "interpretability_method": "Use of VQ-VAE produces discrete tokens that are in principle inspectable; no explicit visualization reported in this paper.",
            "computational_cost": "High computational cost due to attention over many spatial-temporal tokens (slows training significantly according to the paper). Original work used large GPUs (A100) and incurred greater training time.",
            "efficiency_comparison": "Less efficient than STORM due to larger token counts and deeper Transformer (IRIS uses many Transformer layers per the paper); STORM opts for single latent token per image to improve efficiency.",
            "task_performance": "Mean human-normalized score on Atari 100k reported = 105%.",
            "task_utility_analysis": "Spatial tokenization can capture within-image structure better (useful for tasks where spatial layout matters), but the computational overhead may limit practicality under tight sample/compute budgets.",
            "tradeoffs_observed": "Higher spatial fidelity and token-level representation versus much larger attention cost and slower training.",
            "design_choices": "VQ-VAE encoder producing 4×4 token grid; spatial-temporal Transformer; attention across many tokens.",
            "comparison_to_alternatives": "Compared to STORM's single stochastic latent token, IRIS provides finer spatial tokenization but at a heavier computational cost and without clear average performance advantage in the 100k regime.",
            "optimal_configuration": "Not specified here; the paper highlights the cost of spatial-temporal attention and suggests token count affects efficiency negatively.",
            "uuid": "e1210.4",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "TransDreamer",
            "name_full": "TransDreamer (Transformer replacement of Dreamer RNN)",
            "brief_description": "A variant replacing Dreamer-style GRUs with Transformer blocks to model latent dynamics; an attempt to combine Dreamer-style latent modeling with Transformer sequence modeling.",
            "citation_title": "Transdreamer: Reinforcement learning with transformer world models",
            "mention_or_use": "mention",
            "model_name": "TransDreamer",
            "model_description": "Direct replacement of GRU recurrence in the Dreamer architecture with Transformer blocks to predict latent dynamics and imagined rollouts for policy learning.",
            "model_type": "latent world model (Transformer-based)",
            "task_domain": "Atari and other control domains (cited)",
            "fidelity_metric": "Same classes of losses as Dreamer family (reconstruction, KL, reward loss); no per-step fidelity scalars provided in this paper.",
            "fidelity_performance": "The paper notes a lack of widespread evidence demonstrating TransDreamer's performance under limited-sample widely used environments, implying mixed or unestablished advantage in the 100k regime.",
            "interpretability_assessment": "Not discussed in detail here; inherits Transformer attention interpretability potential but no explicit analyses in this paper.",
            "interpretability_method": "None reported in this paper.",
            "computational_cost": "Transformer replacement increases parallelism but may come with larger model and training time depending on tokenization and model size; the paper notes that prior Transformer replacements did not clearly surpass GRU-based DreamerV3 in this benchmark.",
            "efficiency_comparison": "Mixed: while Transformers enable parallelism, existing TransDreamer implementations have not demonstrated consistent wall-clock or sample-efficiency advantages over GRU Dreamer in the 100k regime according to the authors.",
            "task_performance": "Not explicitly reported in the paper's summary table for TransDreamer; authors state lack of clear evidence of its performance under the 100k constraint.",
            "task_utility_analysis": "Potential to combine Dreamer-style latent imagination benefits with Transformer parallelism, but empirical utility under tight-sample budgets is not established in this paper.",
            "tradeoffs_observed": "Replacing recurrence with Transformer can improve parallelism but may require design care (tokenization, model scale) and enough data to train larger models.",
            "design_choices": "Directly replace GRU with Transformer in Dreamer pipeline; specific tokenization and training choices affect outcomes.",
            "comparison_to_alternatives": "Authors indicate TransDreamer has not clearly outperformed GRU-based DreamerV3 in standard limited-sample benchmarks and thus may not offer a straightforward improvement without further design adjustments.",
            "optimal_configuration": "Not specified in this paper; authors encourage careful design to harness Transformer benefits.",
            "uuid": "e1210.5",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MCTS-based (MuZero/EfficientZero/SpeedyZero)",
            "name_full": "MCTS-augmented learned models (e.g., MuZero, EfficientZero, SpeedyZero)",
            "brief_description": "World models combined with Monte Carlo Tree Search (lookahead search) used to improve policy/planning beyond pure imagined rollouts; effective but computationally expensive.",
            "citation_title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MCTS-augmented learned models (MuZero / EfficientZero / SpeedyZero)",
            "model_description": "Learned dynamics and value/policy predictors combined with Monte Carlo Tree Search to perform lookahead planning at decision time; the learned model supports search-based decision making.",
            "model_type": "hybrid world model + planning (learned model + MCTS)",
            "task_domain": "Atari games (and other board/control domains); cited here for Atari 100k context",
            "fidelity_metric": "Quality of the learned model is typically evaluated via search performance and downstream task score (e.g., game score); specific fidelity metrics not detailed in this paper.",
            "fidelity_performance": "Not directly compared in numbers here because the paper focuses on pure world-model-driven imagination methods and excludes lookahead search methods from head-to-head comparison; these methods achieve strong performance in other work but at higher compute cost.",
            "interpretability_assessment": "Planning-based methods expose search trajectories that can be inspected, providing some interpretability of decision-time reasoning; learned model components remain neural and less interpretable.",
            "interpretability_method": "Inspection of MCTS rollouts and node statistics in principle; not detailed in this paper.",
            "computational_cost": "High computational demands due to lookahead search at decision time (authors explicitly note MCTS is computationally expensive and thus not directly compared in this work).",
            "efficiency_comparison": "Higher computational cost but can yield stronger scores on some benchmarks; not compared directly in experiments in this paper because focus was on world-model improvements without lookahead.",
            "task_performance": "Not reported in the paper's Atari 100k comparison table because lookahead methods were intentionally excluded from direct comparison; cited as competitive in prior literature.",
            "task_utility_analysis": "Lookahead search improves policy by leveraging the learned model at decision time, but high runtime compute can limit applicability in sample-limited/time-limited settings.",
            "tradeoffs_observed": "Better planning and often higher performance versus substantially greater computational cost at inference time.",
            "design_choices": "Combine learned dynamics/value/policy heads with MCTS; tuning search depth/width and model fidelity is crucial.",
            "comparison_to_alternatives": "Authors excluded these from primary comparisons due to high compute; note they could potentially be combined with STORM in future work to further improve performance.",
            "uuid": "e1210.6",
            "source_info": {
                "paper_title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Model based reinforcement learning for atari",
            "rating": 2,
            "sanitized_title": "model_based_reinforcement_learning_for_atari"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Transdreamer: Reinforcement learning with transformer world models",
            "rating": 2,
            "sanitized_title": "transdreamer_reinforcement_learning_with_transformer_world_models"
        },
        {
            "paper_title": "Transformer-based world models are happy with 100k interactions",
            "rating": 2,
            "sanitized_title": "transformerbased_world_models_are_happy_with_100k_interactions"
        },
        {
            "paper_title": "Transformers are sample-efficient world models",
            "rating": 2,
            "sanitized_title": "transformers_are_sampleefficient_world_models"
        }
    ],
    "cost": 0.01866975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning
14 Oct 2023</p>
<p>Weipu Zhang 
Gang Wang gangwang@bit.edu.cn 
Jian Sun sunjian@bit.edu.cn 
Yetian Yuan ytyuan@bit.edu.cn 
Gao Huang gaohuang@tsinghua.edu.cn </p>
<p>Chongqing Innovation Center
National Key Lab of Autonomous Intelligent Unmanned Systems
Beijing Institute of Technology Beijing Institute of Technology</p>
<p>Department of Automation
Tsinghua University
BNRist</p>
<p>STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning
14 Oct 20233C398AF15E1AD1907EFB5A6423249299arXiv:2310.09615v1[cs.LG]
Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments.These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning.By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment.The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model.However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible.Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment.Introducing random noise into model-based reinforcement learning has been proven beneficial.In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders.STORM achieves a mean human performance of 126.7% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques.Moreover, training an agent with 1.85 hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only 4.3 hours, showcasing improved efficiency compared to previous methodologies.We release our code at https://github.com/weipu-zhang/STORM.</p>
<p>Introduction</p>
<p>Deep reinforcement learning (DRL) has exhibited remarkable success across diverse domains.However, its widespread application in real-world environments is hindered by the substantial number of interactions with the environment required for achieving such success.This limitation becomes particularly challenging when dealing with broader real-world settings in e.g., unmanned and manufacturing systems [1,2] that lack adjustable speed simulation tools.Consequently, improving the sample efficiency has emerged as a key challenge for DRL algorithms.</p>
<p>Popular DRL methods, including Rainbow [3] and PPO [4], suffer from low sample efficiency due to two primary reasons.Firstly, the estimation of the value function proves to be a challenging task.This involves approximating the value function using a deep neural network (DNN) and updating it with n-step bootstrapped temporal difference, which naturally requires numerous iterations to converge [5].Secondly, in scenarios where rewards are sparse, many samples exhibit similarity in terms of value functions, providing limited useful information for training and generalization of the DNN [6,7].This further exacerbates the challenge of improving the sample efficiency of DRL algorithms.</p>
<p>To address these challenges, model-based DRL algorithms have emerged as a promising approach that tackles both issues simultaneously while demonstrating significant performance gains in sampleefficient settings.These algorithms start by constructing a parameterized simulation world model of the real environment through self-supervised learning.Self-supervised learning can be implemented in various ways, such as reconstructing the original input state using a decoder [8][9][10], predicting actions between frames [7], or employing contrastive learning to capture the internal consistency of input states [6,7].These approaches provide more supervision information than conventional modelfree RL losses, enhancing the feature extraction capabilities of DNNs.Subsequently, the agent's policy is improved by leveraging the experiences generated using the world model, eliminating sampling constraints and enabling faster updates to the value function compared to model-free algorithms.Training FPS on V100 SimPLe [11] and DreamerV3 [10] employ RNNs as their world models, whereas TWM [12], IRIS [13], and STORM use Transformers.The training frames per second (FPS) results on a single NVIDIA V100 GPU are extrapolated from other graphics cards for SimPLe, TWM, and IRIS, while DreamerV3 and STORM are directly evaluated.</p>
<p>However, the process of imagining with a world model involves an autoregressive process that can accumulate prediction errors over time.In situations where discrepancies arise between the imagined trajectory and the real trajectory, the agent may inadvertently pursue virtual goals, resulting in subpar performance in the real environment.To mitigate this issue, introducing random noise into the world model has been proven beneficial [9][10][11]14].Variational autoencoders, capable of automatically learning low-dimensional latent representations of highdimensional data while incorporating reasonable random noise into the latent space, offer an ideal choice for image encoding.</p>
<p>Numerous endeavors have been undertaken to construct an efficient world model.For instance, SimPLe [11] leverages LSTM [15], while DreamerV3 [10] employs GRU [16] as the sequence model.LSTM and GRU, both variants of recurrent neural networks (RNNs), excel at sequence modeling tasks.However, the recurrent nature of RNNs impedes parallelized computing, resulting in slower training speeds [17].</p>
<p>In contrast, the Transformer architecture [17] has lately demonstrated superior performance over RNNs in various sequence modeling and generation tasks.It overcomes the challenge of forgetting long-term dependencies and is designed for efficient parallel computing.While several attempts have been made to incorporate Transformers into the world model [12,13,18], these works do not fully harness the capabilities of this architecture.Furthermore, these approaches require even longer training times and fail to surpass the performance of the GRUbased DreamerV3.</p>
<p>In this paper, we introduce The comparison of our approach with the state-of-the-art methods is depicted in Figure 1.</p>
<p>Related work</p>
<p>Model-based DRL algorithms aim to construct a simulation model of the environment and utilize simulated experiences to improve the policy.While traditional model-based RL techniques like Dyna-Q have shown success in tabular cases [5], modeling complex environments such as video games and visual control tasks presents significant challenges.Recent advances in computing and DNNs have enabled model-based methods to learn the dynamics of the environments and start to outperform model-free methods on these tasks.</p>
<p>The foundation of VAE-LSTM-based world models was introduced by Ha and Schmidhuber [14] for image-based environments, demonstrating the feasibility of learning a good policy solely from generated data.SimPLe [11] applied this methodology to Atari games, resulting in substantial sample efficiency improvements compared to Rainbow [3], albeit with relatively lower performance under limited samples.The Dreamer series [8][9][10] also adopt this framework and showcase notable capabilities in Atari games, DeepMind Control, Minecraft, and other domains, using GRU [16] as the core sequential model.However, as discussed earlier, RNN structures suffer from slow training [17].</p>
<p>Recent approaches such as IRIS [13], TWM [12], and TransDreamer [18] incorporate the Transformer architecture into their world models.IRIS [13] employs VQ-VAE [19] as the encoder to map images into 4×4 latent tokens and uses a spatial-temporal Transformer [20] to capture information within and across images.However, the attention operations on a large number of tokens in the spatial-temporal structure can result in a significant training slowdown.TWM [12] adopts Transformer-XL [21] as its core architecture and organizes the sequence model in a structure similar to Decision Transformer [22], treating the observation, action, and reward as equivalent input tokens for the Transformer.Performing self-attention across different types of data may have a negative impact on the performance, and the increased number of tokens considerably slows down training.TransDreamer [18] directly replaces the GRU structure of Dreamer with Transformer.However, there is a lack of evidence demonstrating their performance in widely accepted environments or under limited sample conditions.</p>
<p>Other model-based RL methods such as MuZero [23], EfficientZero [7], and SpeedyZero [24] incorporate Monte Carlo tree search (MCTS) to enhance policy and achieve promising performance on the Atari 100k benchmark.Lookahead search techniques like MCTS can be employed to enhance other model-based RL algorithms, but they come with high computational demands.Additionally, certain model-free methods [6,[25][26][27] incorporate a self-supervised loss as an auxiliary term alongside the standard RL loss, demonstrating their effectiveness in sample-efficient settings.Additionally, recent studies [28,29] delve deeper into model-free RL, demonstrating strong performance and high data efficiency, rivaling that of model-based methods on several benchmarks.However, since the primary objective of this paper is to enhance the world model, we do not delve further into these methods.</p>
<p>We highlight the distinctions between STORM and recent approaches in the world model as follows:   2).The Agent block, represented by a neural network, corresponds to π θ (a t |s t ) in Equation ( 6).
Agent Agent C a 1 o 1 ENC Z 1 z 1 Ẑ2 ô1 DEC h 1 r1 ĉ1 a 2 o 2 Z 2 z 2 Ẑ3 ô2 h 2 r2 ĉ2 ENC DEC C Transformer Blocks
• SimPLe [11] and Dreamer [10] rely on RNN-based models, whereas STORM employs a GPT-like Transformer [30] as the sequence model.</p>
<p>• In contrast to IRIS [13] that employs multiple tokens, STORM utilizes a single stochastic latent variable to represent an image.</p>
<p>• STORM follows a vanilla Transformer [17] structure, while TWM [12] adopts a Transformer-XL [21] structure.</p>
<p>• In the sequence model of STORM, an observation and an action are fused into a single token, whereas TWM [12] treats observation, action, and reward as three separate tokens of equal importance.</p>
<p>• Unlike Dreamer [10] and TransDreamer [18], which incorporate hidden states, STORM reconstructs the original image without utilizing this information.</p>
<p>3 Method</p>
<p>World model learning</p>
<p>Our approach adheres to the established framework of model-based RL algorithms, which focus on enhancing the agent's policy by imagination [5,[9][10][11]13].We iterate through the following steps until reaching the prescribed number of real environment interactions.</p>
<p>S1) Gather real environment data by executing the current policy for several steps and append them to the replay buffer.</p>
<p>S2</p>
<p>) Update the world model using trajectories sampled from the replay buffer.</p>
<p>S3</p>
<p>) Improve the policy using imagined experiences generated by the world model, with the starting points for the imagination process sampled from the replay buffer.</p>
<p>At each time t, a data point comprises an observation o t , an action a t , a reward r t , and a continuation flag c t (a Boolean variable indicating whether the current episode is ongoing).The replay buffer maintains a first-in-first-out queue structure, enabling the sampling of consecutive trajectories from the buffer.</p>
<p>Section 3.1 provides a detailed description of the architecture and training losses employed by STORM.On the other hand, Section 3.2 elaborates on the imagination process and the training methodology employed by the agent.It provides an thorough explanation of how the agent leverages the world model to simulate experiences and improve its policy.</p>
<p>Model structure The complete structure of our world model is illustrated in Figure 2. In our experiments, we focus on Atari games [31], which generate image observations o t of the environment.</p>
<p>Modeling the dynamics of the environment directly on raw images is computationally expensive and prone to errors [7-11, 13, 23].To address this, we leverage a VAE [32] formulated in Equation ( 1) to convert o t into latent stochastic categorical distributions Z t .Consistent with prior work [9,10,12], we set Z t as a stochastic distribution comprising 32 categories, each with 32 classes.The encoder (q ϕ ) and decoder (p ϕ ) structures are implemented as convolutional neural networks (CNNs) [33].Subsequently, we sample a latent variable z t from Z t to represent the original observation o t .Since sampling from a distribution lacks gradients for backward propagation, we apply the straight-through gradients trick [9,34] to preserve them.</p>
<p>Image encoder:
z t ∼ q ϕ (z t |o t ) = Z t Image decoder: ôt = p ϕ (z t ).(1)
Before entering the sequence model, we combine the latent sample z t and the action a t into a single token e t using multi-layer perceptrons (MLPs) and concatenation.This operation, denoted as m ϕ , prepares the inputs for the sequence model.The sequence model f ϕ takes the sequence of e t as input and produces hidden states h t .We adopt a GPT-like Transformer structure [30] for the sequence model, where the self-attention blocks are masked with a subsequent mask allowing e t to attend to the sequence e 1 , e 2 , . . ., e t .By utilizing MLPs g D ϕ , g R ϕ , and g C ϕ , we rely on h t to predict the current reward rt , the continuation flag ĉt , and the next distribution Ẑt+1 .The formulation of this part of the world model is as follows Action mixer:
e t = m ϕ (z t , a t ) Sequence model: h 1:T = f ϕ (e 1:T )
Dynamics predictor:
Ẑt+1 = g D ϕ (ẑ t+1 |h t ) Reward predictor: rt = g R ϕ (h t ) Continuation predictor: ĉt = g C ϕ (h t ).(2)
Loss functions The world model is trained in a self-supervised manner, optimizing it end-to-end.The total loss function is calculated as in Equation (3) below, with fixed hyperparameters β 1 = 0.5 and β 2 = 0.1.In the equation, B denotes the batch size, and T denotes the batch length
L(ϕ) = 1 BT B n=1 T t=1 L rec t (ϕ) + L rew t (ϕ) + L con t (ϕ) + β 1 L dyn t (ϕ) + β 2 L rep t (ϕ) .(3)
The individual components of the loss function are defined as follows: L rec t (ϕ) represents the reconstruction loss of the original image, L rew t (ϕ) represents the prediction loss of the reward, and L con t (ϕ) represents the prediction loss of the continuation flag.
L rec t (ϕ) = ||ô t − o t || 2 (4a) L rew t (ϕ) = L sym (r t , r t ) (4b) L con t (ϕ) = c t log ĉt + (1 − c t ) log(1 − ĉt ).(4c)
Additionally, L sym in Equation (4b) denotes the symlog two-hot loss, as described in [10].This loss function transforms the regression problem into a classification problem, ensuring consistent loss scaling across different environments.</p>
<p>The losses L dyn t (ϕ) and L rep t (ϕ) are expressed as Kullback-Leibler (KL) divergences but differ in their gradient backward and weighting.The dynamics loss L dyn t (ϕ) guides the sequence model in predicting the next distribution, while the representation loss L rep t (ϕ) allows the output of the encoder to be weakly influenced by the sequence model's prediction.This ensures that the learning of distributional dynamics is not excessively challenging.
L dyn t (ϕ) = max 1, KL sg(q ϕ (z t+1 |o t+1 )) || g D ϕ (ẑ t+1 |h t )(5a)L rep t (ϕ) = max 1, KL q ϕ (z t+1 |o t+1 ) || sg(g D ϕ (ẑ t+1 |h t ))(5b)
where sg(•) denotes the operation of stop-gradients.</p>
<p>Agent learning</p>
<p>The agent's learning is solely based on the imagination process facilitated by the world model, as illustrated in Figure 2. To initiate the imagination process, a brief contextual trajectory is randomly selected from the replay buffer, and the initial posterior distribution Z t is computed.During inference, rather than sampling directly from the posterior distribution Z t , we sample z t from the prior distribution Ẑt .To accelerate the inference, we employ the KV cache technique [35] within the Transformer structure.</p>
<p>The agent's state is formed by concatenating z t and h t , as shown below:</p>
<p>State:
s t = [z t , h t ] Critic: V ψ (s t ) ≈ E π θ ,p ϕ ∞ k=0 γ k r t+k
Actor:
a t ∼ π θ (a t |s t ).(6)
We adopt the actor learning settings from DreamerV3 [10].The complete loss of the actor-critic algorithm is described by Equation (7), where rt corresponds to the reward predicted by the world model, and ĉt represents the predicted continuation flag:
L(θ) = 1 BL B n=1 L t=1 −sg G λ t − V ψ (s t ) max(1, S) ln π θ (a t |s t ) − ηH π θ (a t |s t )(7a)L(ψ) = 1 BL B n=1 L t=1 V ψ (s t ) − sg G λ t 2 + V ψ (s t ) − sg V ψ EMA (s t ) 2(7b)
where H(•) denotes the entropy of the policy distribution, while constants η and L represent the coefficient for entropy loss and the imagination horizon, respectively.The λ-return G λ t [5,10] is recursively defined as follows
G λ t . = r t + γc t (1 − λ)V ψ (s t+1 ) + λG λ t+1 (8a) G λ L . = V ψ (s L ).(8b)
The normalization ratio S utilized in the actor loss (7a) is defined in Equation ( 9), which is computed as the range between the 95th and 5th percentiles of the λ-return G λ t across the batch [10]
S = percentile(G λ t , 95) − percentile(G λ t , 5).(9)
To regularize the value function, we maintain the exponential moving average (EMA) of ψ.The EMA is defined in Equation (10), where ψ t represents the current critic parameters, σ is the decay rate, and ψ EMA t+1 denotes the updated critic parameters.This regularization technique aids in stabilizing training and preventing overfitting
ψ EMA t+1 = σψ EMA t + (1 − σ)ψ t . (10)</p>
<p>Experiments</p>
<p>We evaluated the performance of STORM on the widely-used benchmark for sample-efficient RL, Atari 100k [31].For detailed information about the benchmark, evaluation methodology, and the baselines used for comparison, please refer to Section 4.1.The comprehensive results for the Atari 100k games are presented in Section 4.2.</p>
<p>Benchmark and baselines</p>
<p>Atari 100k consists of 26 different video games with discrete action dimensions of up to 18.The 100k sample constraint corresponds to 400k actual game frames, taking into account frame skipping (4 frames skipped) and repeated actions within those frames.This constraint corresponds to approximately 1.85 hours of real-time gameplay.The agent's human normalized score τ = A−R H−R is calculated based on the score A achieved by the agent, the score R obtained by a random policy, and the average score H achieved by a human player in a specific environment.To determine the human player's performance H, a player is allowed to become familiar with the game under the same sample constraint.</p>
<p>To demonstrate the efficiency of our proposed world model structure, we compare it with model-based DRL algorithms that share a similar training pipeline, as discussed in Section 2. However, similarly to [10,12,13], we do not directly compare our results with lookahead search methods like MuZero [23] and EfficientZero [7], as our primary goal is to refine the world model itself.Nonetheless, lookahead search techniques can be combined with our method in the future to further enhance the agent's performance.</p>
<p>Results on Atari 100k</p>
<p>Detailed results for each environment can be found in Table 2, and the corresponding performance curve is presented in Appendix A due to space limitations.In our experiments, we trained STORM using 5 different seeds and saved checkpoints every 2, 500 sample steps.We assessed the agent's performance by conducting 20 evaluation episodes for each checkpoint and computed the average score.The result reported in Table 2 is the average of the scores attained using the final checkpoints.</p>
<p>STORM demonstrates superior performance compared to previous methods in environments where the key objects related to rewards are large or multiple, such as Amidar, MsPacman, Chopper Command, and Gopher.This advantage can be attributed to the attention mechanism, which explicitly preserves the history of these moving objects, allowing for an easy inference of their speed and direction information, unlike RNN-based methods.However, STORM faces challenges when handling a single small moving object, as observed in Pong and Breakout, due to the nature of autoencoders.Moreover, performing attention operations under such circumstances can potentially harm performance, as the randomness introduced by sampling may excessively influence the attention weights.</p>
<p>Ablation studies</p>
<p>In our experiments, we have observed that the design and configuration choices of the world model and the agent can have significant impacts on the final results.To further investigate this, we conduct ablation studies on the design and configuration of the world model in Section 5.1, as well as on the agent's design in Section 5.2.Additionally, we propose a novel approach to enhancing the exploration efficiency through the imagination capability of the world model using a single demonstration trajectory, which is explained in Section 5.3.</p>
<p>World model design and configuration</p>
<p>The RNN-based world models utilized in SimPLe [11] and Dreamer [9,10] can be formulated clearly using variational inference over time.However, the non-recursive Transformer-based world model does not align with this practice and requires manual design.Figure 3a shows alternative structures and their respective outcomes.In the "Decoder at rear" configuration, we employ z t ∼ Ẑt instead of z t ∼ Z t for reconstructing the original observation and calculating the loss.The results indicate that the reconstruction loss should be applied directly to the output of the encoder rather than relying on the sequence model.In the "Predictor at front" setup, we utilize z t as input for g R ϕ (•) and g C ϕ (•) in Equation ( 2), instead of h t .These findings indicate that, while this operation has minimal impact on the final performance for tasks where the reward can be accurately predicted from a single frame (in e.g., Pong), it leads to a performance drop on tasks that require several contextual frames to predict the reward accurately (in e.g., Ms. Pacman).</p>
<p>By default, we configure our Transformer with 2 layers, which is significantly smaller than the 10 layers used in IRIS [13] and TWM [12].Figure 3b presents the varied outcomes obtained by increasing the number of Transformer layers.The results reveal that increasing the layer count does not have a positive impact on the final performance.However, in the case of the game Pong, even when the sample limit is increased from 100k to 400k, the agent still achieves the maximum reward in this environment regardless of whether a 4-layer or 6-layer Transformer is employed.This scaling discrepancy, which differs from the success observed in other fields [36][37][38], may be attributed to three reasons.Firstly, due to the minor difference between adjacent frames and the presence of residual connections in the Transformer structure [17], predicting the next frame may not require a complex model.Secondly, training a large model naturally requires a substantial amount of data, yet the Atari 100k games neither provide images from diverse domains nor offer sufficient samples for training a larger model.Thirdly, the world model is trained end-to-end, and the representation loss L rep in Equation (5b) directly influences the image encoder.The encoder may be overly influenced when tracking the output of a large sequence model.</p>
<p>Selection of the agent's state</p>
<p>The choice of the agent's state s t offers several viable options: ôt [13], h t , z t (as in TWM [12]), or the combination [h t , z t ] (as demonstrated by Dreamer, [9,10]).In the case of STORM, we employ s t = [h t , z t ], as in Equation (6).Ablation studies investigating the selection of the agent's state are presented in Figure 4.The results indicate that, in environments where a good policy requires contextual information, such as in Ms. Pacman, the inclusion of h t leads to improved performance.However, in other environments like Pong and Kung Fu Master, this inclusion does not yield a significant difference.When solely utilizing h t in environments that evolve with the agent's policy, like Pong, the agent may exhibit behaviors similar to catastrophic forgetting [39] due to the nonstationary and inaccurate nature of the world model.Consequently, the introduction of randomness through certain distributions like Z t proves to be beneficial.</p>
<p>Impact of the demonstration trajectory</p>
<p>The inclusion of a demonstration trajectory is a straightforward implementation step when using a world model, and it is often feasible in real-world settings.Figure 5 showcases the impact of incorporating a single demonstration trajectory in the replay buffer.Details about the provided trajectory can be found in Appendix D. In environments with sparse rewards, adding a trajectory can improve the robustness, as observed in Pong, or the performance, as seen in Freeway.However, in environments with dense rewards like Ms Pacman, including a trajectory may hinder the policy improvement of the agent.</p>
<p>Freeway serves as a prototypical environment characterized by challenging exploration but a simple policy.To receive a reward, the agent must take the "up" action approximately 70 times in a row, but it quickly improves its policy once the first reward is obtained.Achieving the first reward is extremely challenging if the policy is initially set as a uniform distribution of actions.In the case of TWM [12], the entropy normalization technique is employed across all environments, while IRIS [13] specifically reduces the temperature of the Boltzmann exploration strategy for Freeway.These tricks are critical in obtaining the first reward in this environment.It is worth noting that even for most humans, playing exploration-intensive games without prior knowledge is challenging.Typically, human players require instructions about the game's objectives or watch demonstrations from teaching-level or expert players to gain initial exploration directions.Inspired by this observation, we aim to directly incorporate a demonstration trajectory to train the world model and establish starting points for imagination.By leveraging a sufficiently robust world model, the utilization of limited offline information holds the potential to surpass specially designed curiosity-driven exploration strategies in the future.As an integral part of our methodology, we integrate a single trajectory from Freeway into our extensive results.Furthermore, to ensure fair comparisons in future research, we provide the results without incorporating Freeway's trajectory in Table 2 and Figure 6.It is important to highlight that even without this trajectory, our approach consistently outperforms previous methods, attaining a mean human normalized score of 122.3%.</p>
<p>Conclusions and limitations</p>
<p>In this work, we introduce STORM, an efficient world model architecture for model-based RL, surpassing previous methods in terms of both performance and training efficiency.STORM harnesses the powerful sequence modeling and generation capabilities of the Transformer structure while fully exploiting its parallelizable training advantages.The improved efficiency of STORM broadens its applicability across a wider range of tasks while reducing computational costs.</p>
<p>Nevertheless, it is important to acknowledge certain limitations.Firstly, both the world model of STORM and the compared baselines are trained in an end-to-end fashion, where the image encoder and sequence model undergo joint optimization.As a result, the world model must predict its own internal output, introducing additional non-stationarity into the optimization process and potentially impeding the scalability of the world model.Secondly, the starting points for imagination are uniformly sampled from the replay buffer, while the agent is optimized using an on-policy actor-critic algorithm.Although acting in the world model is performed on-policy, the corresponding on-policy distribution µ(s) for these starting points is not explicitly considered, despite its significance in the policy gradient formulation: ∇J(θ) ∝ s µ(s) a q π θ (s, a)∇π θ (a|s) [5].</p>
<p>A Atari 100k curves Figure 6: Performance comparison on the Atari 100k benchmark.Our method is represented in blue, while DreamerV3 [10] is in orange.The solid line represents the average result over 5 seeds, and the filled area indicates the range between the maximum and minimum results across these 5 seeds.</p>
<p>B Details of model structure</p>
<p>Table 3: Structure of the image encoder.The size of the submodules is omitted and can be derived from the shape of the tensors.ReLU refers to the rectified linear units used for activation, while Linear represents a fully-connected layer.Flatten and Reshape operations are employed to alter the indexing method of the tensor while preserving the data and their original order.Conv denotes a CNN layer [33], characterized by kernel = 4, stride = 2, and padding = 1.BN denotes the batch normalization layer [40].</p>
<p>Submodule</p>
<p>E Computational cost details and comparison</p>
<p>Table 12: Computational comparison.In the V100 column, an item marked with a star indicates extrapolation based on other graphics cards, while items without a star are tested using actual devices.The extrapolation method employed aligns with the setup used in DreamerV3 [10], where it assumes the P100 is twice as slow and the A100 is twice as fast.</p>
<p>Method</p>
<p>Original computing resource V100 hours</p>
<p>SimPLe [11] NVIDIA P100, 20 days 240 * TWM [12] NVIDIA A100, 10 hours 20 * NVIDIA GeForce RTX 3090, 12.5 hours IRIS [13] NVIDIA A100,</p>
<p>Si m PL e TW M IR IS D re am er V3 ST O RM (o ur s) score Si m PL e TW M IR IS D re am er V3 ST O RM (o ur s)</p>
<p>Figure 1 :
1
Figure 1: Comparison of methods on Atari 100k.SimPLe[11] and DreamerV3[10] employ RNNs as their world models, whereas TWM[12], IRIS[13], and STORM use Transformers.The training frames per second (FPS) results on a single NVIDIA V100 GPU are extrapolated from other graphics cards for SimPLe, TWM, and IRIS, while DreamerV3 and STORM are directly evaluated.</p>
<p>Figure 2 :
2
Figure 2: Structure and imagination process of STORM.The symbols used in the figure are explained in Sections 3.1 and 3.2.The Transformer blocks depict the sequence model f ϕ in Equation (2).The Agent block, represented by a neural network, corresponds to π θ (a t |s t ) in Equation (6).</p>
<p>Number of layers in the Transformer.</p>
<p>Figure 3 :
3
Figure 3: Ablation studies on the design and configuration of the STORM's world model.</p>
<p>Figure 4 :
4
Figure 4: Ablation studies on the selection of the agent's state.</p>
<p>Figure 5 :
5
Figure 5: Ablations studies on adding a demonstration trajectory to the replay buffer.</p>
<p>modeling and generation quality while accelerating training.STORM achieves a remarkable mean human normalized score of 126.7% on the challenging Atari 100k benchmark, establishing a new record for methods without resorting to lookahead search.Furthermore, training an agent with 1.85 hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only 4.3 hours, demonstrating superior efficiency compared to previous methodologies.</p>
<p>the Stochastic Transformer-based wORld Model (STORM), a highly effective and efficient structure for model-based RL.STORM employs a categorical variational autoencoder (VAE) as the image encoder, enhancing the agent robustness and reducing accumulated autoregressive prediction errors.Subsequently, we incorporate the Transformer as the sequence model, improving</p>
<p>Table 1
1: Comparison between STORM and recent approaches. "Tokens" refers to the input tokensintroduced to the sequence model during a single timestep. "Historical information" indicates whetherthe VAE reconstruction process incorporates historical data, such as the hidden states of an RNN.AttributesSimPLe [11]TWM [12]IRIS [13]DreamerV3 [10]STORM (ours)Sequence modelLSTM [15]Transformer-XL [21]Transformer [17]GRU [16]TransformerTokens Latent representationLatent Binary-VAELatent, action, reward Categorical-VAELatent(4 × 4) VQ-VAELatent Categorical-VAE Categorical-VAE LatentHistorical informationYesNoYesYesNoAgent stateReconstructed imageLatentReconstructed imageLatent, hiddenLatent, hiddenAgent trainingPPO [4]As DreamerV2 [9]As DreamerV2 [9]DreamerV3As DreamerV3</p>
<p>Table 2 :
2
[9]e scores and overall human-normalized scores on the 26 games in the Atari 100k benchmark.Following the conventions of[9], scores that are the highest or within 5% of the highest score are highlighted in bold.
GameRandom Human SimPLe [11] TWM [12] IRIS [13] DreamerV3 [10] STORM (ours)Alien2287128617675420959984Amidar6172074122143139205Assault2227425276831524706801Asterix210850311281116854932Bank Heist147533446753649641Battle Zone23603718840315068130741225013540Boxing012878707880Breakout2301620843116Chopper Command811738897916971565420Crazy Climber10780358296258471820592349719066776Demon Attack15219712083502034303165Freeway030172431034Freeway w/o traj03017243100Frostbite6543352371476259909Gopher2582413597167522363730Hero1027308262657725470371116111044James Bond29303101362463445509Kangaroo5230355112408384098Krull159826662204634966167782Kung Fu Master256227361486224555217602142026182Ms Pacman3076952148015889991327Pong-21151319151811Private Eye25695713587100882Qbert16413455128933317463405Road Runner1278455641910996151556517564Seaquest6842055683774661618525Up N Down5331169333501598235467667Human Mean0%100%33%96%105%112%126.7%Human Median0%100%13%51%29%49%58.4%</p>
<p>Table 4 :
4
[41]cture of the image decoder.DeConv denotes a transpose CNN layer[41], characterized by kernel = 4, stride = 2, and padding = 1.
Output tensor shapeInput image (o t ) Conv1 + BN1 + ReLU Conv2 + BN2 + ReLU Conv3 + BN3 + ReLU Conv4 + BN4 + ReLU Flatten3 × 64 × 64 32 × 32 × 32 64 × 16 × 16 128 × 8 × 8 256 × 4 × 4 4096Linear1024Reshape (produce Z t )32 × 32SubmoduleOutput tensor shapeRandom sample (z t ) Flatten32 × 32 1024Linear + BN0 + ReLU4096Reshape DeConv1 + BN1 + ReLU DeConv2 + BN2 + ReLU DeConv3 + BN3 + ReLU DeConv4 (produce ôt )256 × 4 × 4 128 × 8 × 8 64 × 16 × 16 32 × 32 × 32 3 × 64 × 64</p>
<p>Table 5 :
5
[42]on mixer e t = m ϕ (z t , a t ).Concatenate denotes combining the last dimension of two tensors and merging them into one new tensor.The variable A represents the action dimension, which ranges from 3 to 18 across different games.D denotes the feature dimension of the Transformer.LN is an abbreviation for layer normalization[42].
SubmoduleOutput tensor shapeRandom sample (z t ), Action (a t ) Reshape and concatenate32 × 32, A 1024 + ALinear1 + LN1 + ReLUDLinear2 + LN2 (output e t )D</p>
<p>Table 6 :
6
Positional encoding module.w 1:T is a learnable parameter matrix with shape T × D, and T refers to the sequence length.
SubmoduleOutput tensor shapeInput (e 1:T )Add (e 1:T + w 1:T ) LNT × D</p>
<p>Table 7 :
7
[43]sformer block.Dropout mechanism[43]can prevent overfitting.
SubmoduleModule alias Output tensor shapeInput features (label as x 1 )T × DMulti-head self attentionLinear1 + Dropout(p) Residual (add x 1 ) LN1 (label as x 2 )MHSAT × DLinear2 + ReLU Linear3 + Dropout(p) Residual (add x 2 ) LN2FFNT × 2D T × D T × D T × D</p>
<p>Table 8 :
8
Transformer based sequence model h 1:T = f ϕ (e 1:T ).Positional encoding is explained in Table6and Transformer block is explained in Table7.
SubmoduleOutput tensor shapeInput (e 1:T )Positional encoding Transformer blocks ×K Output (h 1:T )T × D</p>
<p>Table 9 :
9
[10] MLP structures.A 1-layer MLP corresponds to a fully-connected layer.255 is the size of the bucket of symlog two-hot loss[10].
Module nameSymbolMLP layers Input/ MLP hidden/ Output dimensionDynamics headg D ϕ1D/ -/ 1024Reward predictorg R ϕ3D/ D/ 255Continuation predictor Policy network Critic networkg C ϕ π θ (a t |s t ) V ψ (s t )3 3 3D/ D/ 1 D/ D/ A D/ D/ 255</p>
<p>Table 11 :
11
[45]ccount for frame skipping, the frame count is multiplied by 4. These trajectories were gathered using pre-trained DQN agents[45].
GameEpisode returnFramesMs Pacman Pong Freeway5860 18 271612 × 4 2079 × 4 2048 × 4
Acknowledgments and Disclosure of FundingWe would like to thank anonymous reviewers for their constructive comments.The work was supported partially by the National Key R&amp;D Program of China under Grant 2021YFB1714800, partially by the National Natural Science Foundation of China under Grants 62173034, 61925303, 62088101, and partially by the Chongqing Natural Science Foundation under Grant 2021ZX4100027.C HyperparametersTable10: Hyerparameters.Note that the environment will provide a "done" signal when losing a life, but will continue running until the actual reset occurs.This life information configuration aligns with the setup used in IRIS[13]
From unmanned systems to autonomous intelligent systems. Jie Chen, Jian Sun, Gang Wang, Engineering. 122022</p>
<p>Competitive meta-learning. Boxi Weng, Jian Sun, Gao Huang, Fang Deng, Gang Wang, Jie Chen, IEEE/CAA Journal of Automatica Sinica. 1092023</p>
<p>Rainbow: Combining improvements in deep reinforcement learning. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Introduction to Reinforcement Learning. Andrew G Richard S Sutton, Barto, 1998MIT Press Cambridge135</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. Michael Laskin, Aravind Srinivas, Pieter Abbeel, International Conference on Machine Learning. PMLR2020</p>
<p>Mastering Atari games with limited data. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao, Advances in Neural Information Processing Systems. 202134</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, International Conference on Learning Representations. 2020</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, Jimmy Ba, International Conference on Learning Representations. 2021</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Model based reinforcement learning for atari. Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski, International Conference on Learning Representations. 2020</p>
<p>Transformer-based world models are happy with 100k interactions. Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling, The International Conference on Learning Representations. 2023</p>
<p>Transformers are sample-efficient world models. Vincent Micheli, Eloi Alonso, François Fleuret, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural Computation. 981997</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, Yoshua Bengio, EMNLP. 2014</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Transdreamer: Reinforcement learning with transformer world models. Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn, arXiv:2202.094812022arXiv preprint</p>
<p>Neural discrete representation learning. Aaron Van Den, Oriol Oord, Vinyals, Advances in Neural Information Processing Systems. 201730</p>
<p>Vivit: A video vision transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, Ruslan Salakhutdinov, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems. 202134</p>
<p>Mastering Atari, Go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 58878392020</p>
<p>Speedyzero: Mastering atari with limited data and time. Yixuan Mei, Jiaxuan Gao, Weirui Ye, Shaohuai Liu, Yang Gao, Yi Wu, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Data-efficient reinforcement learning with self-predictive representations. Max Schwarzer, Ankesh Anand, Rishab Goel, Devon Hjelm, Aaron Courville, Philip Bachman, International Conference on Learning Representations. 2021</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. Denis Yarats, Ilya Kostrikov, Rob Fergus, International Conference on Learning Representations. 2021</p>
<p>When to use parametric models in reinforcement learning?. Matteo Hado P Van Hasselt, John Hessel, Aslanides, Advances in Neural Information Processing Systems. 322019</p>
<p>Sample-efficient reinforcement learning by breaking the replay ratio barrier. D' Pierluca, Max Oro, Evgenii Schwarzer, Pierre-Luc Nikishin, Marc G Bacon, Aaron Bellemare, Courville, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Bigger, better, faster: Human-level atari with human-level efficiency. Max Schwarzer, Johan Samir Obando Ceron, Aaron Courville, Marc G Bellemare, Rishabh Agarwal, Pablo Samuel Castro, International Conference on Machine Learning. PMLR2023</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>. P Diederik, Max Kingma, Welling, arXiv:1312.61142013Auto-encoding variational Bayes. arXiv preprint</p>
<p>Backpropagation applied to handwritten zip code recognition. Yann Lecun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, Lawrence D Jackel, Neural Computation. 141989</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Yoshua Bengio, Nicholas Léonard, Aaron Courville, arXiv:1308.34322013arXiv preprint</p>
<p>Transformer inference arithmetic. kipp.ly. Carol Chen, 2022</p>
<p>. Openai, Chatgpt, 2021. April 19, 2023</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, Neal J Cohen, Psychology of learning and motivation. Elsevier198924</p>
<p>Batch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, International conference on machine learning. 2015</p>
<p>Deconvolutional networks. Dilip Matthew D Zeiler, Graham W Krishnan, Rob Taylor, Fergus, 2010 IEEE Computer Society Conference on computer vision and pattern recognition. IEEE2010</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>Dropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, The journal of machine learning research. 1512014</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Atari agents. Florin Gogianu, Tudor Berariu, Lucian Bus, Elena Burceanu, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
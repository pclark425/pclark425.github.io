<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1269 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1269</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1269</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-272826772</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14084v2.pdf" target="_blank">One-shot World Models Using a Transformer Trained on a Synthetic Prior</a></p>
                <p><strong>Paper Abstract:</strong> A World Model is a compressed spatial and temporal representation of a real world environment that allows one to train an agent or execute planning methods. However, world models are typically trained on observations from the real world environment, and they usually do not enable learning policies for other real environments. We propose One-Shot World Model (OSWM), a transformer world model that is learned in an in-context learning fashion from purely synthetic data sampled from a prior distribution. Our prior is composed of multiple randomly initialized neural networks, where each network models the dynamics of each state and reward dimension of a desired target environment. We adopt the supervised learning procedure of Prior-Fitted Networks by masking next-state and reward at random context positions and query OSWM to make probabilistic predictions based on the remaining transition context. During inference time, OSWM is able to quickly adapt to the dynamics of a simple grid world, as well as the CartPole gym and a custom control environment by providing 1k transition steps as context and is then able to successfully train environment-solving agent policies. However, transferring to more complex environments remains a challenge, currently. Despite these limitations, we see this work as an important stepping-stone in the pursuit of learning world models purely from synthetic data.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1269.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1269.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-Shot World Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based world model trained purely on synthetic trajectories sampled from a synthetic prior (randomly initialized per-dimension neural nets + a momentum prior) that uses in-context learning to adapt to a new environment from â‰ˆ1,000 real transitions and then serve as a learned simulator for policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>One-Shot World Model (OSWM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A GPT-style transformer that ingests sequences of concatenated state-action vectors and is trained to predict next-state and reward targets via Prior-Fitted-style masking (random cut-offs) on synthetic batches. Inputs are normalized; padding handles variable state/action sizes. At inference the model conditions on a context of real transitions (typically 1k) to produce next-state and reward predictions for downstream RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive neural simulator (neural world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>classical control and simple grid navigation (GridWorld, CartPole-v0, SimpleEnv, MountainCar-v0, Pendulum-v1, Reacher-v4)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mean-squared error (MSE) on next-state and reward prediction (used as training and proxy evaluation metric); also qualitative reward-function visual comparisons and downstream RL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Fidelity reported qualitatively; training and evaluation use MSE loss but the paper does not report explicit numeric MSE values for OSWM predictions in the main text. Predictive fidelity sufficient for training agents in simple domains (GridWorld, CartPole, SimpleEnv) but insufficient in more complex domains (MountainCar, Pendulum) according to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable via analysis: the paper inspects prior-generated marginal distributions (histograms) and visualizes learned reward surfaces; overall model is a black-box transformer but priors (momentum vs NN prior) afford interpretability of data-generating behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of prior histograms (per-dimension distributions), reward-function visualizations across state grids, component-wise analysis (NN prior vs momentum prior), proxy-set MSE diagnostics; no attention-map or latent-variable disentanglement analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Qualitative: trained on synthetic batches using gradient descent (MSE objective). Hyperparameter optimization ran 45 BO iterations with 3 workers and model training for fixed 50 epochs during BO; inference adapts with ~1k context steps. Exact GPU/parameter counts and wall-clock times are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitative: OSWM enables training policies without environment interactions beyond the 1k context, improving apparent sample efficiency of downstream policy learning in simple tasks versus training from scratch on real env; no direct compute/time or parameter-count comparisons to baselines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>OSWM-PPO (PPO trained entirely on OSWM predictions using 1k context) solved or matched baseline performance in simple tasks: GridWorld ~5.2 (matching PPO), CartPole-v0 ~196.5 vs PPO 200.0, SimpleEnv -4.7 vs PPO -0.8. It underperforms in MountainCar and Pendulum (large gaps) and shows partial improvement on Reacher-v4 (-10.2 vs PPO -4.6).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>OSWM's predictive fidelity translates to successful downstream policy learning in simple/low-complexity domains (discrete GridWorld, CartPole, SimpleEnv) but not in tasks requiring precise momentum/edge dynamics (MountainCar, Pendulum). Smoothness in predicted rewards helps exploration but introduces inaccuracies near unstable states.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Training on a very synthetic prior allows rapid one-shot adaptation (low real-sample requirement) but sacrifices fidelity in complex continuous/momentum-sensitive tasks; momentum prior inclusion improves momentum tasks at cost of added prior complexity; reward smoothing aids policy learning but can mislead near rare/edge states.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses PFN-style in-context masked supervised training on synthetic prior; prior composed of per-dimension randomly initialized 3-layer NNs (NN prior) plus a physics-inspired momentum prior concatenated into NN prior inputs; context size sampled at training time; rewards sometimes replaced by constant (prob 0.5) to encourage episode-extension behaviours; states/actions shuffled and padded to handle heterogeneous envs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively/empirically against PPO trained on real envs (baseline): OSWM-PPO matches or approaches PPO on simple tasks but fails on more complex tasks. Compared to existing transformer world models (TWM/IRIS/STORM) and other learned model approaches, OSWM's novelty is training purely on synthetic prior rather than target-environment data; paper argues other transformer-based world models typically require target-environment training.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends (empirically) (1) include momentum prior for dynamics that rely on momentum, (2) use richer context-generation strategies (mixture or p-expert) rather than purely expert or repeated random actions, (3) collect ~1k context transitions for one-shot adaptation, and (4) improve priors and context sampling to extend to more complex environments; also suggests early stopping for policy training on OSWM to avoid degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1269.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NN prior</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Network Prior (per-dimension random NNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic prior component in OSWM that generates diverse state-dynamics by using independently randomly-initialized, untrained small neural networks per state-dimension to produce next-state values from previous state and action.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Network Prior (component of OSWM prior)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each state dimension s_i is produced by a separate randomly initialized 3-layer linear MLP with random activation types (ReLU/tanh/sigmoid) and residual connections; networks take full previous state and action as input and output next-state dimension; random scale and offset applied at initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>synthetic neural-data prior (data generator for world-model pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>used to generate synthetic trajectories for training OSWM applicable to a range of simple RL environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not a learned model for fidelity measurement; used to produce training data distributions. Paper inspects per-dimension histograms (distribution shapes) as an analysis metric.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not applicable as a predictive model of real envs; produces three typical distribution types (peaked, smooth/wide, multi-modal). No numeric fidelity reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Limited interpretability: distribution histograms of NN prior dimensions show types but internal mappings are random and less interpretable than physics-based priors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Histogramming of per-dimension min/max binned distributions; qualitative comparison of distribution shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Lightweight: small 3-layer MLPs used to generate synthetic data; cost dominated by OSWM transformer training rather than prior generation. Exact cost not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Generates diverse synthetic data cheaply relative to running real environment sims; but alone (without momentum prior) insufficient for momentum-heavy tasks (e.g., CartPole).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As a prior component, using only the NN prior reduces performance in momentum-dependent tasks; GridWorld and SimpleEnv remain largely unaffected but CartPole requires momentum prior to match performance, and MountainCar/Reacher/Pendulum remain unsolved.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>NN prior provides multimodal and varied dynamics helpful for learning generic behaviors and reward smoothness, but lacks explicit physical structure needed for tasks relying on momentum or precise physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Offers diversity and multimodality (good for generalization) but lacks structured physics leading to poor performance on momentum-dominant tasks; combining with momentum prior mitigates this.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Per-dimension independent random nets, random activation types, periodic resets of prior networks, random scale and offset of initial states, shuffling of state/action indices to avoid overfitting to dimension order.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared internally to momentum prior: NN prior is more diverse but less interpretable; when used alone it underperforms on momentum tasks compared to NN+momentum combination.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests combining NN prior with a momentum prior for environments where physical dynamics matter; tuning NN architecture and BO used to find better prior hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1269.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Momentum prior</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-based Momentum Prior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic prior component modeling simple physical laws (velocity and position updates) used alongside the NN prior to generate trajectories exhibiting momentum/physical interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Momentum prior (component of OSWM prior)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Simple physics-inspired dynamics: velocity updated by action and gravity (v_{t+1} = v_t + a_t * dt - g * dt) and position updated by velocity (p_{t+1} = p_t + v_{t+1} * dt); outputs can be represented as angles (radian/sin/cos) or linear positions and concatenated into NN prior inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>physics-inspired synthetic prior (explicit simple dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>used to generate synthetic data for training OSWM; intended to capture momentum dynamics found in tasks like CartPole, Pendulum, MountainCar</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Analysed via per-dimension distribution histograms (shape: broad, multimodal, sparse). No numeric fidelity vs real env reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported numerically; qualitatively provides dynamics that improve OSWM's performance on momentum-dependent tasks when combined with NN prior.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>More interpretable than NN prior because it maps directly to velocity/position updates and physical parameters (gravity, dt); its output distributions correspond to understandable physical behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Histogramming of prior output distributions, qualitative mapping to physical interactions (gravity, reflections, angular motion).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Negligible compared to NN prior and transformer training; closed-form updates for velocity and position are computationally cheap.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides targeted inductive bias for momentum dynamics at very low computational cost compared to attempting to learn such structure solely from generic NN prior data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Inclusion of momentum prior improves OSWM-PPO performance in momentum-dependent tasks such as CartPole; removing it harms performance on those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Momentum prior supplies physical inductive biases that are task-relevant (momentum/velocity dynamics) and therefore increases utility of synthetic pretraining for such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Adds domain-specific structure which improves momentum tasks but may reduce diversity of purely NN-generated behaviours; combining both priors is used to balance diversity and physics bias.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Simple explicit velocity/position updates, sampling initial positions/velocities from specified ranges, optionally representing angles via sin/cos, concatenation with NN prior outputs for subsequent transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to NN prior alone: momentum prior increases interpretability and supports momentum tasks where NN prior alone was insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends including momentum prior when target tasks rely on physical momentum; combine with NN prior for diversity while keeping momentum inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1269.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (Dream to Control line of work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of latent-space world models for RL that learn compact latent representations (often via VAE-like encoders) and perform policy learning via imagination in latent space; cited as a successful model-based RL approach applied to diverse domains including robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space world model (latent dynamics model with encoder/decoder and latent predictor) that allows imagination-based policy optimization; typically uses learned latent dynamics and reward models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>diverse RL domains, including continuous control and robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Evaluation typically uses predictive losses in latent space and downstream RL returns; paper mentions Dreamer applied across diverse domains but does not report Dreamer-specific metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper (only cited as effective in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generally latent and less directly interpretable; no interpretability discussion in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not provided here; Dreamer is referenced as a successful world-model approach but resource details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited for improving sample efficiency in prior work; no numerical comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Cited qualitatively as applied across diverse domains; no numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used as an example of model-based RL success where learned dynamics enable efficient policy learning; contrast drawn with OSWM which seeks synthetic pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed in this paper beyond general background.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not discussed here beyond being a latent imagination-based world model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned along with other successful learned world-model approaches; contrasted with OSWM in that Dreamer is typically trained on target-environment data.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1269.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that learns environment dynamics and reward models jointly (value/policy/dynamics heads) and uses planning (MCTS) over the learned model; cited as an advancement in model-based RL for board games and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned model combining dynamics, value and reward prediction used within a planning algorithm (MCTS) rather than direct trajectory rollout imitation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid learned dynamics + planning (model-based with planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>board games (e.g., chess, Go, shogi) and other planning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper; typically evaluated by game-play performance and learned model losses in original references.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not compared numerically here; cited as highly effective approach historically.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mentioned historically as highly effective for board games; no new results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Provided as background on world models that learn dynamics and rewards; contrasted with OSWM which focuses on synthetic pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not discussed beyond high-level mention.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned as part of the model-based RL landscape; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1269.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TWM (Transformer World Model / Transformer-XL variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based world model (Transformer-XL variant) previously used to attain strong sample efficiency on Atari 100k; cited as an example of transformer success in world-modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based World Model (TWM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-XL style architecture applied to sequence prediction in RL rollouts to model dynamics for sample-efficient learning on Atari-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k benchmark (Atari games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper; prior work reports sample-efficiency and downstream task returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here; cited as surpassing other methods on Atari 100k in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited as pushing sample efficiency boundaries on Atari; no direct quantitative comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Cited as surpassing other methods on Atari 100k benchmark in referenced prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Serves as a background example of transformer efficacy in world modelling where training on target environment data is required; contrasted with OSWM's synthetic-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Transformer-XL style to capture long contexts; no detailed design described here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned alongside IRIS and STORM as transformer successes trained on target env data.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1269.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style transformer world model cited as achieving human-level performance on Atari 100k when trained on environment observations; used as background for transformer-based world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IRIS (transformer-based world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-style transformer trained on environment rollouts to predict future states/rewards; reported in prior work to reach high performance on Atari benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k benchmark (Atari games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not provided in this paper (cited work reports human-level performance metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited as high-performing on Atari when trained on target env data; no direct comparison to OSWM's synthetic pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Referenced as achieving human-level performance on Atari 100k in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used to motivate transformer-based world models but contrasted with OSWM since IRIS trains on target environment data.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>GPT-style training objective and architecture (cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned together with STORM and TWM as competitive transformer approaches that require target-environment training.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1269.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic transformer-based world model cited as achieving human-level or above human performance on Atari 100k in prior work; included in the paper's related-work discussion of transformer world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>STORM (stochastic transformer world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stochastic transformer architecture for world modelling, designed to model uncertainty/stochasticity in environment dynamics; reported effective on Atari tasks in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based stochastic predictive model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper; prior work would evaluate predictive likelihoods/returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited among top transformer-based world models; no direct compute comparisons to OSWM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Referenced as achieving strong performance on Atari 100k in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Representative of transformer-based models that require environment-specific training data, unlike OSWM which trains on synthetic prior.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Incorporates stochastic modeling in a transformer; specifics are in cited work, not this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Grouped with TWM and IRIS as high-performing transformer world models trained on target data.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1269.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier model-based RL approach that learned dynamics models to generate simulated data for policy learning, demonstrated on Atari; cited as background for world-model-driven sample-efficiency improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learned dynamics model used to hallucinate rollouts for policy learning to improve sample efficiency in RL; historically influential in model-based Atari work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>learned dynamics model (neural simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited for sample-efficiency gains in prior work; no direct comparisons given here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Cited as demonstrating strong results on Atari in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used as background to motivate world-model approaches that enable simulated training data; contrasted with OSWM's synthetic-only pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned as a representative early learned-dynamics approach.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1269.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augmented World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmented World Models (Ball et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that learns a world model from offline data and provides predicted dynamics plus latent context about possible changes to generalize to environment variations; cited as related work addressing dynamics changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Augmented World Models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>World model augmented with latent context representing possible dynamics changes learned from offline data to improve generalization to varying environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>augmented latent world model (with latent context for variations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robust generalization to environment variations (domain shifts)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not detailed here; conceptually the latent context provides interpretable cues about environment variations but specifics are in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited as one approach to handle dynamics changes without retraining on target env; no direct comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Mentioned as a complementary line of work tackling transfer/generalization of world models across environment variations, compared to OSWM's synthetic prior approach.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not elaborated; refer to original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Positioned as tackling dynamics changes via offline-latent-context vs OSWM's synthetic-data in-context adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1269.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evans RNN/Transformer context encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit identification via sequence encoders (Evans et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work using transformers or RNNs to encode environment parameterization into latent spaces enabling robustness to environment variations (e.g., friction/mass changes); cited as related work on encoding environment parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Implicit environment-parameter encoding via sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Use of sequence encoders (transformers/RNNs) to map trajectories/parameters into latent embeddings that condition dynamics models for robust generalization across parametric variations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent-conditioned world model / encoder-based adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>dynamics variations like friction and object mass changes (control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Cited as enabling robustness to parameter changes; no numerical values provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Serves as background showing an alternative way to achieve dynamics adaptation compared to OSWM's purely synthetic in-context approach.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not elaborated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually with OSWM as another adaptation mechanism relying on encoded context rather than synthetic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1269.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1269.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PFN / Prior-Fitted Networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior-Fitted Networks (PFNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised learning paradigm where a model is trained on synthetic data sampled from a prior over data-generating processes to perform in-context learning at test time; OSWM adapts this paradigm to world-model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prior-Fitted Networks (PFNs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Train a neural predictor on many synthetic datasets sampled from a prior so that at inference time the model can do Bayesian-like predictions or in-context adaptation when provided with a small dataset from a real target problem.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>synthetic-prior supervised pretraining for in-context adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>originally classification/regression tasks; adapted here for world-model dynamics prediction</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically evaluated via predictive error on held-out real datasets; in this paper PFN paradigm is applied and MSE is used for next-state/reward prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>PFN conceptually yields strong in-context adaptation in prior works (e.g., TabPFN); in this paper PFN paradigm underpins OSWM but no PFN-specific numeric fidelity is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>PFN paradigm is not particularly interpretable by itself; interpretability depends on prior choice and analysis of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in detail here beyond using proxy-set MSE and visual analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>PFN-style pretraining can be compute-heavy depending on synthetic-data generation and model size; this paper does not provide numeric costs.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PFN-based OSWM can make fast in-context adaptation (one-shot) but overall transformer compute for pretraining remains; no quantitative compute comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>PFN prior usages (e.g., TabPFN) reported strong performance in other domains; here PFN-inspired OSWM yields good downstream policy performance in simple tasks but not in harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Using PFN paradigm allows OSWM to adapt from small real context (1k transitions) and avoid target-environment pretraining, offering utility in low-sample regimes for simpler tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Success depends strongly on prior design and context sampling; poor prior or context leads to low fidelity and poor downstream policies.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Adopt supervised training with masking/cutoffs on synthetic batches sampled from a hand-designed RL prior; use in-context learning at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to typical target-environment pretraining approaches used by transformer world models; PFN-based approach trades real-data training for reliance on a synthetic prior.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper emphasizes careful prior construction (NN + momentum) and context generation strategies (mixture/p-expert) as critical for PFN-style performance in world-modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One-shot World Models Using a Transformer Trained on a Synthetic Prior', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Transformers are sample-efficient world models <em>(Rating: 2)</em></li>
                <li>STORM: Efficient stochastic transformer based world models for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Prior-Fitted Networks <em>(Rating: 2)</em></li>
                <li>Augmented World Models <em>(Rating: 1)</em></li>
                <li>Context is everything: Implicit identification for dynamics adaptation <em>(Rating: 1)</em></li>
                <li>World models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1269",
    "paper_id": "paper-272826772",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "OSWM",
            "name_full": "One-Shot World Model",
            "brief_description": "A transformer-based world model trained purely on synthetic trajectories sampled from a synthetic prior (randomly initialized per-dimension neural nets + a momentum prior) that uses in-context learning to adapt to a new environment from â‰ˆ1,000 real transitions and then serve as a learned simulator for policy training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "One-Shot World Model (OSWM)",
            "model_description": "A GPT-style transformer that ingests sequences of concatenated state-action vectors and is trained to predict next-state and reward targets via Prior-Fitted-style masking (random cut-offs) on synthetic batches. Inputs are normalized; padding handles variable state/action sizes. At inference the model conditions on a context of real transitions (typically 1k) to produce next-state and reward predictions for downstream RL training.",
            "model_type": "transformer-based predictive neural simulator (neural world model)",
            "task_domain": "classical control and simple grid navigation (GridWorld, CartPole-v0, SimpleEnv, MountainCar-v0, Pendulum-v1, Reacher-v4)",
            "fidelity_metric": "Mean-squared error (MSE) on next-state and reward prediction (used as training and proxy evaluation metric); also qualitative reward-function visual comparisons and downstream RL performance.",
            "fidelity_performance": "Fidelity reported qualitatively; training and evaluation use MSE loss but the paper does not report explicit numeric MSE values for OSWM predictions in the main text. Predictive fidelity sufficient for training agents in simple domains (GridWorld, CartPole, SimpleEnv) but insufficient in more complex domains (MountainCar, Pendulum) according to experiments.",
            "interpretability_assessment": "Partially interpretable via analysis: the paper inspects prior-generated marginal distributions (histograms) and visualizes learned reward surfaces; overall model is a black-box transformer but priors (momentum vs NN prior) afford interpretability of data-generating behaviours.",
            "interpretability_method": "Visualization of prior histograms (per-dimension distributions), reward-function visualizations across state grids, component-wise analysis (NN prior vs momentum prior), proxy-set MSE diagnostics; no attention-map or latent-variable disentanglement analyses reported.",
            "computational_cost": "Qualitative: trained on synthetic batches using gradient descent (MSE objective). Hyperparameter optimization ran 45 BO iterations with 3 workers and model training for fixed 50 epochs during BO; inference adapts with ~1k context steps. Exact GPU/parameter counts and wall-clock times are not reported.",
            "efficiency_comparison": "Qualitative: OSWM enables training policies without environment interactions beyond the 1k context, improving apparent sample efficiency of downstream policy learning in simple tasks versus training from scratch on real env; no direct compute/time or parameter-count comparisons to baselines reported.",
            "task_performance": "OSWM-PPO (PPO trained entirely on OSWM predictions using 1k context) solved or matched baseline performance in simple tasks: GridWorld ~5.2 (matching PPO), CartPole-v0 ~196.5 vs PPO 200.0, SimpleEnv -4.7 vs PPO -0.8. It underperforms in MountainCar and Pendulum (large gaps) and shows partial improvement on Reacher-v4 (-10.2 vs PPO -4.6).",
            "task_utility_analysis": "OSWM's predictive fidelity translates to successful downstream policy learning in simple/low-complexity domains (discrete GridWorld, CartPole, SimpleEnv) but not in tasks requiring precise momentum/edge dynamics (MountainCar, Pendulum). Smoothness in predicted rewards helps exploration but introduces inaccuracies near unstable states.",
            "tradeoffs_observed": "Training on a very synthetic prior allows rapid one-shot adaptation (low real-sample requirement) but sacrifices fidelity in complex continuous/momentum-sensitive tasks; momentum prior inclusion improves momentum tasks at cost of added prior complexity; reward smoothing aids policy learning but can mislead near rare/edge states.",
            "design_choices": "Uses PFN-style in-context masked supervised training on synthetic prior; prior composed of per-dimension randomly initialized 3-layer NNs (NN prior) plus a physics-inspired momentum prior concatenated into NN prior inputs; context size sampled at training time; rewards sometimes replaced by constant (prob 0.5) to encourage episode-extension behaviours; states/actions shuffled and padded to handle heterogeneous envs.",
            "comparison_to_alternatives": "Compared qualitatively/empirically against PPO trained on real envs (baseline): OSWM-PPO matches or approaches PPO on simple tasks but fails on more complex tasks. Compared to existing transformer world models (TWM/IRIS/STORM) and other learned model approaches, OSWM's novelty is training purely on synthetic prior rather than target-environment data; paper argues other transformer-based world models typically require target-environment training.",
            "optimal_configuration": "Paper recommends (empirically) (1) include momentum prior for dynamics that rely on momentum, (2) use richer context-generation strategies (mixture or p-expert) rather than purely expert or repeated random actions, (3) collect ~1k context transitions for one-shot adaptation, and (4) improve priors and context sampling to extend to more complex environments; also suggests early stopping for policy training on OSWM to avoid degradation.",
            "uuid": "e1269.0",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NN prior",
            "name_full": "Neural Network Prior (per-dimension random NNs)",
            "brief_description": "A synthetic prior component in OSWM that generates diverse state-dynamics by using independently randomly-initialized, untrained small neural networks per state-dimension to produce next-state values from previous state and action.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Neural Network Prior (component of OSWM prior)",
            "model_description": "Each state dimension s_i is produced by a separate randomly initialized 3-layer linear MLP with random activation types (ReLU/tanh/sigmoid) and residual connections; networks take full previous state and action as input and output next-state dimension; random scale and offset applied at initialization.",
            "model_type": "synthetic neural-data prior (data generator for world-model pretraining)",
            "task_domain": "used to generate synthetic trajectories for training OSWM applicable to a range of simple RL environments",
            "fidelity_metric": "Not a learned model for fidelity measurement; used to produce training data distributions. Paper inspects per-dimension histograms (distribution shapes) as an analysis metric.",
            "fidelity_performance": "Not applicable as a predictive model of real envs; produces three typical distribution types (peaked, smooth/wide, multi-modal). No numeric fidelity reported.",
            "interpretability_assessment": "Limited interpretability: distribution histograms of NN prior dimensions show types but internal mappings are random and less interpretable than physics-based priors.",
            "interpretability_method": "Histogramming of per-dimension min/max binned distributions; qualitative comparison of distribution shapes.",
            "computational_cost": "Lightweight: small 3-layer MLPs used to generate synthetic data; cost dominated by OSWM transformer training rather than prior generation. Exact cost not reported.",
            "efficiency_comparison": "Generates diverse synthetic data cheaply relative to running real environment sims; but alone (without momentum prior) insufficient for momentum-heavy tasks (e.g., CartPole).",
            "task_performance": "As a prior component, using only the NN prior reduces performance in momentum-dependent tasks; GridWorld and SimpleEnv remain largely unaffected but CartPole requires momentum prior to match performance, and MountainCar/Reacher/Pendulum remain unsolved.",
            "task_utility_analysis": "NN prior provides multimodal and varied dynamics helpful for learning generic behaviors and reward smoothness, but lacks explicit physical structure needed for tasks relying on momentum or precise physical laws.",
            "tradeoffs_observed": "Offers diversity and multimodality (good for generalization) but lacks structured physics leading to poor performance on momentum-dominant tasks; combining with momentum prior mitigates this.",
            "design_choices": "Per-dimension independent random nets, random activation types, periodic resets of prior networks, random scale and offset of initial states, shuffling of state/action indices to avoid overfitting to dimension order.",
            "comparison_to_alternatives": "Compared internally to momentum prior: NN prior is more diverse but less interpretable; when used alone it underperforms on momentum tasks compared to NN+momentum combination.",
            "optimal_configuration": "Paper suggests combining NN prior with a momentum prior for environments where physical dynamics matter; tuning NN architecture and BO used to find better prior hyperparameters.",
            "uuid": "e1269.1",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Momentum prior",
            "name_full": "Physics-based Momentum Prior",
            "brief_description": "A synthetic prior component modeling simple physical laws (velocity and position updates) used alongside the NN prior to generate trajectories exhibiting momentum/physical interactions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Momentum prior (component of OSWM prior)",
            "model_description": "Simple physics-inspired dynamics: velocity updated by action and gravity (v_{t+1} = v_t + a_t * dt - g * dt) and position updated by velocity (p_{t+1} = p_t + v_{t+1} * dt); outputs can be represented as angles (radian/sin/cos) or linear positions and concatenated into NN prior inputs.",
            "model_type": "physics-inspired synthetic prior (explicit simple dynamics)",
            "task_domain": "used to generate synthetic data for training OSWM; intended to capture momentum dynamics found in tasks like CartPole, Pendulum, MountainCar",
            "fidelity_metric": "Analysed via per-dimension distribution histograms (shape: broad, multimodal, sparse). No numeric fidelity vs real env reported.",
            "fidelity_performance": "Not reported numerically; qualitatively provides dynamics that improve OSWM's performance on momentum-dependent tasks when combined with NN prior.",
            "interpretability_assessment": "More interpretable than NN prior because it maps directly to velocity/position updates and physical parameters (gravity, dt); its output distributions correspond to understandable physical behaviours.",
            "interpretability_method": "Histogramming of prior output distributions, qualitative mapping to physical interactions (gravity, reflections, angular motion).",
            "computational_cost": "Negligible compared to NN prior and transformer training; closed-form updates for velocity and position are computationally cheap.",
            "efficiency_comparison": "Provides targeted inductive bias for momentum dynamics at very low computational cost compared to attempting to learn such structure solely from generic NN prior data.",
            "task_performance": "Inclusion of momentum prior improves OSWM-PPO performance in momentum-dependent tasks such as CartPole; removing it harms performance on those tasks.",
            "task_utility_analysis": "Momentum prior supplies physical inductive biases that are task-relevant (momentum/velocity dynamics) and therefore increases utility of synthetic pretraining for such tasks.",
            "tradeoffs_observed": "Adds domain-specific structure which improves momentum tasks but may reduce diversity of purely NN-generated behaviours; combining both priors is used to balance diversity and physics bias.",
            "design_choices": "Simple explicit velocity/position updates, sampling initial positions/velocities from specified ranges, optionally representing angles via sin/cos, concatenation with NN prior outputs for subsequent transitions.",
            "comparison_to_alternatives": "Compared to NN prior alone: momentum prior increases interpretability and supports momentum tasks where NN prior alone was insufficient.",
            "optimal_configuration": "Paper recommends including momentum prior when target tasks rely on physical momentum; combine with NN prior for diversity while keeping momentum inductive bias.",
            "uuid": "e1269.2",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dreamer (Dream to Control line of work)",
            "brief_description": "A family of latent-space world models for RL that learn compact latent representations (often via VAE-like encoders) and perform policy learning via imagination in latent space; cited as a successful model-based RL approach applied to diverse domains including robotics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Dreamer",
            "model_description": "Latent-space world model (latent dynamics model with encoder/decoder and latent predictor) that allows imagination-based policy optimization; typically uses learned latent dynamics and reward models.",
            "model_type": "latent world model",
            "task_domain": "diverse RL domains, including continuous control and robotics",
            "fidelity_metric": "Evaluation typically uses predictive losses in latent space and downstream RL returns; paper mentions Dreamer applied across diverse domains but does not report Dreamer-specific metrics here.",
            "fidelity_performance": "Not reported in this paper (only cited as effective in prior work).",
            "interpretability_assessment": "Generally latent and less directly interpretable; no interpretability discussion in this paper's mention.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Not provided here; Dreamer is referenced as a successful world-model approach but resource details are not provided in this paper.",
            "efficiency_comparison": "Cited for improving sample efficiency in prior work; no numerical comparison provided in this paper.",
            "task_performance": "Cited qualitatively as applied across diverse domains; no numbers provided in this paper.",
            "task_utility_analysis": "Used as an example of model-based RL success where learned dynamics enable efficient policy learning; contrast drawn with OSWM which seeks synthetic pretraining.",
            "tradeoffs_observed": "Not discussed in this paper beyond general background.",
            "design_choices": "Not discussed here beyond being a latent imagination-based world model.",
            "comparison_to_alternatives": "Mentioned along with other successful learned world-model approaches; contrasted with OSWM in that Dreamer is typically trained on target-environment data.",
            "optimal_configuration": "Not discussed in this paper.",
            "uuid": "e1269.3",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero",
            "brief_description": "A model that learns environment dynamics and reward models jointly (value/policy/dynamics heads) and uses planning (MCTS) over the learned model; cited as an advancement in model-based RL for board games and planning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MuZero",
            "model_description": "A learned model combining dynamics, value and reward prediction used within a planning algorithm (MCTS) rather than direct trajectory rollout imitation.",
            "model_type": "hybrid learned dynamics + planning (model-based with planning)",
            "task_domain": "board games (e.g., chess, Go, shogi) and other planning tasks",
            "fidelity_metric": "Not specified in this paper; typically evaluated by game-play performance and learned model losses in original references.",
            "fidelity_performance": "Not reported here.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Not discussed in this paper.",
            "efficiency_comparison": "Not compared numerically here; cited as highly effective approach historically.",
            "task_performance": "Mentioned historically as highly effective for board games; no new results provided here.",
            "task_utility_analysis": "Provided as background on world models that learn dynamics and rewards; contrasted with OSWM which focuses on synthetic pretraining.",
            "tradeoffs_observed": "Not discussed in this paper.",
            "design_choices": "Not discussed beyond high-level mention.",
            "comparison_to_alternatives": "Mentioned as part of the model-based RL landscape; no direct empirical comparison in this paper.",
            "optimal_configuration": "Not discussed here.",
            "uuid": "e1269.4",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "TWM",
            "name_full": "TWM (Transformer World Model / Transformer-XL variant)",
            "brief_description": "A transformer-based world model (Transformer-XL variant) previously used to attain strong sample efficiency on Atari 100k; cited as an example of transformer success in world-modeling.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Transformer-based World Model (TWM)",
            "model_description": "Transformer-XL style architecture applied to sequence prediction in RL rollouts to model dynamics for sample-efficient learning on Atari-like tasks.",
            "model_type": "transformer-based predictive world model",
            "task_domain": "Atari 100k benchmark (Atari games)",
            "fidelity_metric": "Not specified in this paper; prior work reports sample-efficiency and downstream task returns.",
            "fidelity_performance": "Not reported here; cited as surpassing other methods on Atari 100k in prior work.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed.",
            "computational_cost": "Not reported here.",
            "efficiency_comparison": "Cited as pushing sample efficiency boundaries on Atari; no direct quantitative comparisons in this paper.",
            "task_performance": "Cited as surpassing other methods on Atari 100k benchmark in referenced prior work.",
            "task_utility_analysis": "Serves as a background example of transformer efficacy in world modelling where training on target environment data is required; contrasted with OSWM's synthetic-only training.",
            "tradeoffs_observed": "Not discussed in this paper.",
            "design_choices": "Transformer-XL style to capture long contexts; no detailed design described here.",
            "comparison_to_alternatives": "Mentioned alongside IRIS and STORM as transformer successes trained on target env data.",
            "optimal_configuration": "Not discussed in this paper.",
            "uuid": "e1269.5",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "IRIS",
            "name_full": "IRIS",
            "brief_description": "A GPT-style transformer world model cited as achieving human-level performance on Atari 100k when trained on environment observations; used as background for transformer-based world models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "IRIS (transformer-based world model)",
            "model_description": "GPT-style transformer trained on environment rollouts to predict future states/rewards; reported in prior work to reach high performance on Atari benchmarks.",
            "model_type": "transformer-based predictive model",
            "task_domain": "Atari 100k benchmark (Atari games)",
            "fidelity_metric": "Not provided in this paper (cited work reports human-level performance metrics).",
            "fidelity_performance": "Not reported here.",
            "interpretability_assessment": "Not discussed here.",
            "interpretability_method": "Not discussed.",
            "computational_cost": "Not reported here.",
            "efficiency_comparison": "Cited as high-performing on Atari when trained on target env data; no direct comparison to OSWM's synthetic pretraining.",
            "task_performance": "Referenced as achieving human-level performance on Atari 100k in prior work.",
            "task_utility_analysis": "Used to motivate transformer-based world models but contrasted with OSWM since IRIS trains on target environment data.",
            "tradeoffs_observed": "Not discussed in this paper.",
            "design_choices": "GPT-style training objective and architecture (cited prior work).",
            "comparison_to_alternatives": "Mentioned together with STORM and TWM as competitive transformer approaches that require target-environment training.",
            "optimal_configuration": "Not discussed here.",
            "uuid": "e1269.6",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "STORM",
            "name_full": "STORM",
            "brief_description": "A stochastic transformer-based world model cited as achieving human-level or above human performance on Atari 100k in prior work; included in the paper's related-work discussion of transformer world models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "STORM (stochastic transformer world model)",
            "model_description": "Stochastic transformer architecture for world modelling, designed to model uncertainty/stochasticity in environment dynamics; reported effective on Atari tasks in cited literature.",
            "model_type": "transformer-based stochastic predictive model",
            "task_domain": "Atari 100k benchmark",
            "fidelity_metric": "Not specified in this paper; prior work would evaluate predictive likelihoods/returns.",
            "fidelity_performance": "Not reported here.",
            "interpretability_assessment": "Not discussed here.",
            "interpretability_method": "Not discussed.",
            "computational_cost": "Not reported here.",
            "efficiency_comparison": "Cited among top transformer-based world models; no direct compute comparisons to OSWM.",
            "task_performance": "Referenced as achieving strong performance on Atari 100k in prior work.",
            "task_utility_analysis": "Representative of transformer-based models that require environment-specific training data, unlike OSWM which trains on synthetic prior.",
            "tradeoffs_observed": "Not discussed in this paper.",
            "design_choices": "Incorporates stochastic modeling in a transformer; specifics are in cited work, not this paper.",
            "comparison_to_alternatives": "Grouped with TWM and IRIS as high-performing transformer world models trained on target data.",
            "optimal_configuration": "Not discussed here.",
            "uuid": "e1269.7",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "SimPLe",
            "brief_description": "An earlier model-based RL approach that learned dynamics models to generate simulated data for policy learning, demonstrated on Atari; cited as background for world-model-driven sample-efficiency improvements.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SimPLe",
            "model_description": "Learned dynamics model used to hallucinate rollouts for policy learning to improve sample efficiency in RL; historically influential in model-based Atari work.",
            "model_type": "learned dynamics model (neural simulator)",
            "task_domain": "Atari games",
            "fidelity_metric": "Not specified in this paper.",
            "fidelity_performance": "Not reported here.",
            "interpretability_assessment": "Not discussed here.",
            "interpretability_method": "Not discussed.",
            "computational_cost": "Not discussed here.",
            "efficiency_comparison": "Cited for sample-efficiency gains in prior work; no direct comparisons given here.",
            "task_performance": "Cited as demonstrating strong results on Atari in prior work.",
            "task_utility_analysis": "Used as background to motivate world-model approaches that enable simulated training data; contrasted with OSWM's synthetic-only pretraining.",
            "tradeoffs_observed": "Not discussed here.",
            "design_choices": "Not elaborated in this paper.",
            "comparison_to_alternatives": "Mentioned as a representative early learned-dynamics approach.",
            "optimal_configuration": "Not discussed here.",
            "uuid": "e1269.8",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Augmented World Models",
            "name_full": "Augmented World Models (Ball et al., 2021)",
            "brief_description": "An approach that learns a world model from offline data and provides predicted dynamics plus latent context about possible changes to generalize to environment variations; cited as related work addressing dynamics changes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Augmented World Models",
            "model_description": "World model augmented with latent context representing possible dynamics changes learned from offline data to improve generalization to varying environments.",
            "model_type": "augmented latent world model (with latent context for variations)",
            "task_domain": "robust generalization to environment variations (domain shifts)",
            "fidelity_metric": "Not reported in this paper.",
            "fidelity_performance": "Not reported here.",
            "interpretability_assessment": "Not detailed here; conceptually the latent context provides interpretable cues about environment variations but specifics are in cited work.",
            "interpretability_method": "Not discussed here.",
            "computational_cost": "Not discussed here.",
            "efficiency_comparison": "Cited as one approach to handle dynamics changes without retraining on target env; no direct comparisons provided.",
            "task_performance": "Not quantified here.",
            "task_utility_analysis": "Mentioned as a complementary line of work tackling transfer/generalization of world models across environment variations, compared to OSWM's synthetic prior approach.",
            "tradeoffs_observed": "Not discussed here.",
            "design_choices": "Not elaborated; refer to original paper.",
            "comparison_to_alternatives": "Positioned as tackling dynamics changes via offline-latent-context vs OSWM's synthetic-data in-context adaptation.",
            "optimal_configuration": "Not discussed here.",
            "uuid": "e1269.9",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Evans RNN/Transformer context encoding",
            "name_full": "Implicit identification via sequence encoders (Evans et al., 2022)",
            "brief_description": "Work using transformers or RNNs to encode environment parameterization into latent spaces enabling robustness to environment variations (e.g., friction/mass changes); cited as related work on encoding environment parameters.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Implicit environment-parameter encoding via sequence models",
            "model_description": "Use of sequence encoders (transformers/RNNs) to map trajectories/parameters into latent embeddings that condition dynamics models for robust generalization across parametric variations.",
            "model_type": "latent-conditioned world model / encoder-based adaptation",
            "task_domain": "dynamics variations like friction and object mass changes (control tasks)",
            "fidelity_metric": "Not specified here.",
            "fidelity_performance": "Not reported here.",
            "interpretability_assessment": "Not discussed here.",
            "interpretability_method": "Not discussed here.",
            "computational_cost": "Not discussed here.",
            "efficiency_comparison": "Not discussed here.",
            "task_performance": "Cited as enabling robustness to parameter changes; no numerical values provided in this paper.",
            "task_utility_analysis": "Serves as background showing an alternative way to achieve dynamics adaptation compared to OSWM's purely synthetic in-context approach.",
            "tradeoffs_observed": "Not detailed in this paper.",
            "design_choices": "Not elaborated here.",
            "comparison_to_alternatives": "Compared conceptually with OSWM as another adaptation mechanism relying on encoded context rather than synthetic priors.",
            "optimal_configuration": "Not discussed here.",
            "uuid": "e1269.10",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PFN / Prior-Fitted Networks",
            "name_full": "Prior-Fitted Networks (PFNs)",
            "brief_description": "A supervised learning paradigm where a model is trained on synthetic data sampled from a prior over data-generating processes to perform in-context learning at test time; OSWM adapts this paradigm to world-model learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Prior-Fitted Networks (PFNs)",
            "model_description": "Train a neural predictor on many synthetic datasets sampled from a prior so that at inference time the model can do Bayesian-like predictions or in-context adaptation when provided with a small dataset from a real target problem.",
            "model_type": "synthetic-prior supervised pretraining for in-context adaptation",
            "task_domain": "originally classification/regression tasks; adapted here for world-model dynamics prediction",
            "fidelity_metric": "Typically evaluated via predictive error on held-out real datasets; in this paper PFN paradigm is applied and MSE is used for next-state/reward prediction.",
            "fidelity_performance": "PFN conceptually yields strong in-context adaptation in prior works (e.g., TabPFN); in this paper PFN paradigm underpins OSWM but no PFN-specific numeric fidelity is provided.",
            "interpretability_assessment": "PFN paradigm is not particularly interpretable by itself; interpretability depends on prior choice and analysis of outputs.",
            "interpretability_method": "Not discussed in detail here beyond using proxy-set MSE and visual analyses.",
            "computational_cost": "PFN-style pretraining can be compute-heavy depending on synthetic-data generation and model size; this paper does not provide numeric costs.",
            "efficiency_comparison": "PFN-based OSWM can make fast in-context adaptation (one-shot) but overall transformer compute for pretraining remains; no quantitative compute comparison provided.",
            "task_performance": "PFN prior usages (e.g., TabPFN) reported strong performance in other domains; here PFN-inspired OSWM yields good downstream policy performance in simple tasks but not in harder tasks.",
            "task_utility_analysis": "Using PFN paradigm allows OSWM to adapt from small real context (1k transitions) and avoid target-environment pretraining, offering utility in low-sample regimes for simpler tasks.",
            "tradeoffs_observed": "Success depends strongly on prior design and context sampling; poor prior or context leads to low fidelity and poor downstream policies.",
            "design_choices": "Adopt supervised training with masking/cutoffs on synthetic batches sampled from a hand-designed RL prior; use in-context learning at inference.",
            "comparison_to_alternatives": "Compared conceptually to typical target-environment pretraining approaches used by transformer world models; PFN-based approach trades real-data training for reliance on a synthetic prior.",
            "optimal_configuration": "Paper emphasizes careful prior construction (NN + momentum) and context generation strategies (mixture/p-expert) as critical for PFN-style performance in world-modeling.",
            "uuid": "e1269.11",
            "source_info": {
                "paper_title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Transformers are sample-efficient world models",
            "rating": 2,
            "sanitized_title": "transformers_are_sampleefficient_world_models"
        },
        {
            "paper_title": "STORM: Efficient stochastic transformer based world models for reinforcement learning",
            "rating": 2,
            "sanitized_title": "storm_efficient_stochastic_transformer_based_world_models_for_reinforcement_learning"
        },
        {
            "paper_title": "Prior-Fitted Networks",
            "rating": 2,
            "sanitized_title": "priorfitted_networks"
        },
        {
            "paper_title": "Augmented World Models",
            "rating": 1,
            "sanitized_title": "augmented_world_models"
        },
        {
            "paper_title": "Context is everything: Implicit identification for dynamics adaptation",
            "rating": 1,
            "sanitized_title": "context_is_everything_implicit_identification_for_dynamics_adaptation"
        },
        {
            "paper_title": "World models",
            "rating": 1,
            "sanitized_title": "world_models"
        }
    ],
    "cost": 0.019504749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>One-shot World Models Using a Transformer Trained on a Synthetic Prior
24 Oct 2024</p>
<p>Fabio Ferreira ferreira@cs.uni-freiburg.de 
Moreno Schlageter 
Raghu Rajan 
AndrÃ© Biedenkapp 
Frank Hutter </p>
<p>University of Freiburg</p>
<p>University of Freiburg</p>
<p>University of Freiburg</p>
<p>University of Freiburg</p>
<p>ELLIS Institute TÃ¼bingen University of Freiburg</p>
<p>One-shot World Models Using a Transformer Trained on a Synthetic Prior
24 Oct 202496520CE3088B2D48824936A854B37CE3arXiv:2409.14084v2[cs.LG]
A World Model is a compressed spatial and temporal representation of a real world environment that allows one to train an agent or execute planning methods.However, world models are typically trained on observations from the real world environment, and they usually do not enable learning policies for other real environments.We propose One-Shot World Model (OSWM), a transformer world model that is learned in an in-context learning fashion from purely synthetic data sampled from a prior distribution.Our prior is composed of multiple randomly initialized neural networks, where each network models the dynamics of each state and reward dimension of a desired target environment.We adopt the supervised learning procedure of Prior-Fitted Networks by masking next-state and reward at random context positions and query OSWM to make probabilistic predictions based on the remaining transition context.During inference time, OSWM is able to quickly adapt to the dynamics of a simple grid world, as well as the CartPole gym and a custom control environment by providing 1k transition steps as context and is then able to successfully train environment-solving agent policies.However, transferring to more complex environments remains a challenge, currently.Despite these limitations, we see this work as an important stepping-stone in the pursuit of learning world models purely from synthetic data.</p>
<p>Introduction</p>
<p>World models have emerged as a powerful approach for creating compressed spatial and temporal representations of real-world environments, enabling efficient agent training and planning in reinforcement learning (RL) tasks [Ha and Schmidhuber, 2018, Kaiser et al., 2019, Hafner et al., 2023, Wu et al., 2022].These models have shown significant promise in improving sample efficiency and performance across various RL domains.For instance, SimPLe [Kaiser et al., 2019] demonstrated strong results on Atari games by using a learned dynamics model to generate simulated data.More recently, transformer-based world models have pushed the boundaries of sample efficiency and performance.TWM [Robine et al., 2023] utilized a Transformer-XL architecture to surpass other methods on the Atari 100k benchmark [Kaiser et al., 2019], while IRIS [Micheli et al., 2023] and STORM [Zhang et al., 2023] achieved human-level performance using GPT-style transformers.However, these approaches typically require training on observations from the target environment, which can be time-consuming and impractical in many real-world scenarios.Moreover, traditional world models often lack the ability to generalize across different environments, limiting their applicability in diverse OSWM is trained on synthetic data sampled from a prior distribution of randomly initialized, untrained neural networks that mimic RL environments (left).Given a sequence of synthetic interactions, OSWM is optimized by predicting future dynamics at random cut-offs (center).RL agents can then be trained on OSWM to solve simple real environments given a context.</p>
<p>RL tasks.The challenge of transferring learned dynamics efficiently to new environments remains a significant hurdle in the field of model-based RL.</p>
<p>To address these challenges, we explore the potential of training world models with in-context learning using purely synthetic data.We propose the One-Shot World Model (OSWM), a transformerbased approach that learns a world model from a synthetic prior distribution.Our method draws inspiration from Prior-Fitted Networks [MÃ¼ller et al., 2022] and leverages in-context learning to adapt to new environments with minimal real-world interactions.By training on a diverse synthetic prior, OSWM aims to capture a wide range of environment dynamics, potentially enabling rapid adaptation to various RL tasks.We release our code under https://github.com/automl/oswmand our contributions can be summarized as follows:</p>
<p>â€¢ We explore training world models with synthetic data sampled from a synthetic prior distribution based on randomly initialized and untrained neural networks, using in-context learning to predict future dynamics and rewards given previous state and action sequences.â€¢ We demonstrate that our model, One-Shot World Model (OSWM), is capable of adapting to the dynamics of unseen environments in a one-shot manner by only providing 1,000 randomly sampled transitions as context.â€¢ Although OSWM adaptability is still limited to very simple environments, we show that training world models on such a synthetic prior surprisingly allows for the training of RL agents that solve the GridWorld, CartPole gym and a custom control environment.â€¢ We investigate OSWM's limitations and analyze learned reward functions, as well as strategies for prior construction and the relevance of context sampling, providing insights for future improvements in this direction.</p>
<p>Related Work</p>
<p>World Models and Model-Based Reinforcement Learning Classical RL often suffers from sample inefficiency, as it requires many interactions with the environment.Model-Based Reinforcement Learning (MBRL) mitigates this by learning environment dynamics, allowing agents to train using simulated data.For example, Ha and Schmidhuber [2018] proposed World Models, which use generative neural networks to encode compact spatial-temporal representations, optimizing RL efficiency.</p>
<p>MuZero [Schrittwieser et al., 2020] advanced MBRL by learning both environment dynamics and reward functions, which proved highly effective across board games.Dreamer [Hafner et al., 2020[Hafner et al., , 2021[Hafner et al., , 2023] ] applied learned world models across diverse domains, including real-world robotics [Wu et al., 2022].More recently, TD-MPC2 [Hansen et al., 2024] demonstrated scalability and robustness in continuous control tasks.Transformer-based models have also become prominent, with TransDreamer [Chen et al., 2022] or TWM [Robine et al., 2023] that excelled in sample efficiency on Atari 100k.Other Transformer-based approaches such as IRIS [Micheli et al., 2023] or STORM [Zhang et al., 2023] achieved over 100% human performance on Atari 100k with GPT-style training.However, Most if not all methods are trained on the target environment and utilize the attention mechanism to attend to previous parts of the roll-out.</p>
<p>Despite these successes, transferring learned dynamics across environments remains a significant hurdle in the field of MBRL.Augmented World Models [Ball et al., 2021] tackle environmental dynamics changes by learning a world model from offline data.During training, they provide predicted dynamics and possible changes as latent context, helping agents generalize to new variations.Similarly, Evans et al. [2022] uses transformers or RNNs to encode environment parameterization into a latent space, enabling a world model robust to variations like friction or object mass changes.</p>
<p>Synthetic Data and Priors and RL Synthetic data plays a crucial role in RL, particularly in methods that transfer knowledge from simulation to real environments, known as sim2real.Domain randomization [Tobin et al., 2017, Rajan et al., 2023], which varies simulation settings like lighting and object shapes, enhances generalization and improves the transfer from simulation to the real world.Pretraining with synthetic data has also gained prominence.For example, Wang et al. [2024] pretrains the Decision Transformer using synthetic Markov chain data, outperforming pretraining with natural text (e.g., DPT trained on Wikipedia [Lee et al., 2023]) in both performance and sample efficiency.Other techniques include training on synthetic reward distributions to allow zero-shot transfer to new tasks [Frans et al., 2024], while TDM [Schubert et al., 2023] demonstrates strong few-shot and zero-shot generalization across procedural control environments.UniSim [Yang et al., 2023] uses internet-scale data to train realistic robotic control models, enabling more efficient RL training.A meta-learning approach trains Synthetic Environments [Ferreira et al., 2021] for RL that serve as proxies for a target environment, providing only synthetic environment dynamics.These synthetic dynamics allow RL agents to significantly reduce the number of interactions needed during training.Lastly, Prior Fitted Networks (PFNs) [MÃ¼ller et al., 2022] utilize synthetic priors for supervised learning, with its adaptation to tabular data, TabPFN [Hollmann et al., 2023], achieving state-of-the-art results while significantly speeding up inference.</p>
<p>Unlike previous approaches that depend on real-world observations or extensive training in target environments, we introduce a new approach that trains a transformer world model entirely on synthetic data sampled from a prior distribution which is based much further away from reality as it based on randomly initiliazed neural networks.Using the Prior-Fitted Networks paradigm, OSWM employs in-context learning to adapt to new environments with just a simple context sequence.</p>
<p>Method</p>
<p>Let x t = s 1:ds t , a 1:da t denote the concatenated state-action vector (or input) at time step t, where s 1:ds t represents the state and a 1:da t represents the action, with d s and d a being the dimensionalities of the state and action, respectively.Similarly, let y t = s 1:ds t+1 , r t+1 represent the next state and reward vector (or target).The sequences of these vectors, {x 1 , . . ., x T } and {y 1 , . . ., y T }, are summarized as X 1:T and Y 1:T , respectively.To ensure consistent input sizes across varying environments, padding is applied: x t = [s 1 t , ..., s ds t , pad s , a 1 t , ..., a da t , pad a ], where pad s and pad a are zero vectors used to match the maximum state and action dimensions across environments.The same padding scheme is applied to y t .The OSWM is trained on synthetic batches (X 1:T , Y 1:T ) sampled from a prior distribution P RL .At randomly sampled cut-off positions, the synthetic batches are divided into context and target data and the model is trained to predict the target data given the context, which we visualized in Fig. 1</p>
<p>(center).</p>
<p>At inference, OSWM adapts to a new environment using a few context samples (X 1:T âˆ’1 , Y 1:T âˆ’1 ) collected from the real environment, i.e. the target environment (see Fig. 1).This context consist of state-action transitions and their corresponding rewards, which provide information about the dynamics of the real environment.To ensure sufficient coverage of the target environment, multiple transitions are collected, often spanning several episodes.We typically collect 1,000 transitions from random rollouts, though the collection process can be performed using any policy, ranging from random to expert-driven actions.We analyze the role of context generation on the predictive performance of the model in Section 4.3.</p>
<p>Once the context is collected, OSWM predicts the next state and reward (s t+1 , r t+1 ) given the current state-action pair (s t , a t ) and the prior context.The OSWM acts as a learned simulator, enabling RL agents to interact with predicted dynamics and learning by standard RL algorithms.Note, that the OSWM is initialized by sampling an initial state from the real environment at inference time.Both inputs X 1:T and targets Y 1:T are normalized to zero mean and unit variance.OSWM predicts in this normalized space, and the predictions are projected back to the original value space using the mean and variance of the context data.Finally, we assume that the termination condition of the target environment is known, but we note that it could also be learned.</p>
<p>Training the One-Shot World Model (OSWM)</p>
<p>The OSWM is trained on synthetic data sampled from a prior distribution P RL (see Section 4.2), which is constructed to simulate the dynamics of various environments.The goal is to optimize the model for predicting the dynamics of unseen target environments based on initial interactions used by in-context learning.We describe the entire training procedure in Algorithm 1.</p>
<p>At first, the model weights Î¸ are initialized randomly.During each training step, a batch of (X 1:T , Y 1:T ) âˆ¼ P RL is sampled, with each batch containing input and target sequences.A context size eval is sampled from the interval
[k, T âˆ’ 1],
where k is the minimum context size used for the prediction (see Appendix C for more details about the sampling).The model is provided with X 1:eval and Y 1:eval to predict future targets Å¶eval+1:T based on the remaining inputs X eval+1:T .The training loss is computed using the mean-squared error (MSE) between the predicted and actual future transitions: L = M SE( Å¶eval+1:T , Y eval+1:T ).</p>
<p>Algorithm 1 Training the OSWM with the synthetic prior Initialize Î¸ â–· Initialize OSWM's parameters while not finished do
X 1:T , Y 1:T âˆ¼ P RL â–· Sample batch from RL prior eval âˆ¼ U(k, T âˆ’ 1) â–· Sample eval size Å¶eval_pos+1:T â† M Î¸ (X 1:eval , Y 1:eval , X eval+1:T ) â–· Predict dynamics with OSWM L â† M SE( Å¶eval+1:T , Y eval+1:T ) â–· Calculate loss Î¸ â† Î¸ âˆ’ Î±âˆ‡ Î¸ L â–· Update parameters end while return M Î¸</p>
<p>Prior for Training OSWM</p>
<p>One of the core contributions of this method is the design of a prior that aims to mimic the properties of RL environments while incorporating stochasticity for diverse dynamics.The prior consists of two components: a neural network-based (NN) prior and a physics-based momentum prior.These two priors are combined, with the states produced by both the NN and momentum priors concatenated as input to the NN prior for further updates.This split allows the model to capture both complex, neural network-generated behaviors and simple, physics-driven interactions, like pendulum motion or velocity-position relations.In Figure 1 (left), we illustrate the mechanics of the NN prior, and below we describe both priors in more detail.</p>
<p>Neural Network Prior</p>
<p>The NN prior generates dynamics using randomly initialized neural networks.Each state dimension s i t is produced by a separate neural network f i Î¸ i , which is randomlyinitialized and untrained and takes as input the entire previous state
s tâˆ’1 = [s 1 tâˆ’1 , ..., s ds tâˆ’1 ] and action a t = [a 1 tâˆ’1 , ..., a da t ]. The next state is computed as s i t = f i Î¸ i (s tâˆ’1 , a tâˆ’1 )
. The networks consist of three linear layers, with random activations (ReLU, tanh, or sigmoid) after the first two layers, and a residual connection that aggregates the outputs of the first and second layers.This structure allows for complex dependencies between state dimensions and actions.To introduce variability, each NN-based state dimension is initialized with a random scale and offset.When the individual NN prior networks are reset, which occurs periodically after a pre-defined fixed interval, their initial state values s 0 are sampled from U(0, 1), and then scaled and offset according to the prior configuration (see Table 6 for the prior hyperparameters in the appendix), ensuring stochastic behavior across environments.This method allows the model to capture rich and diverse dynamics by introducing different dependencies between states and actions across dimensions.</p>
<p>Momentum Prior</p>
<p>The momentum prior models physical interactions through two components: velocity and positional updates.Velocity is updated based on the action and gravity (v t+1 = v t + a t â€¢ âˆ†t âˆ’ g â€¢ âˆ†t), while position is updated using the current velocity (p t+1 = p t + v t+1 â€¢ âˆ†t).In this model, velocity v t and position p t are influenced by factors such as gravity and the current action, and the position updates rely on velocity.The initial position is sampled from [0, 2Ï€], and the initial velocity is sampled from U(âˆ’3, 3).This setup enables the model to simulate both linear and angular motion.Angular dynamics can incorporate gravity, and they are represented internally in radians, though the output can be sine, cosine, or radian values.The momentum prior values are concatenated with the NN prior values and fed into the NN prior networks for the subsequent transitions.</p>
<p>Rewards and Invariance</p>
<p>The reward function follows a similar structure to the NN prior used for state dynamics but with different inputs, including the new state, action, and the previous state.This reflects how rewards in real RL environments are based on state transitions and action costs, such as penalizing high action magnitudes.The reward at time step t can be expressed as: r t+1 = g(s t+1 , a t , s t ) where g represents the reward model that takes the new state s t+1 , the action a t , and, optionally, the previous state s t as inputs.To maintain flexibility, the reward is replaced by a constant reward of 1 with a probability of 0.5, a common approach in tasks like CartPole, where extending the episode is rewarded, or MountainCar, where faster completion is incentivized.To prevent the model from overfitting to the order of state-action dimensions, we shuffle both states and actions and apply identical permutations to X 1 and Y 1 .</p>
<p>Experiments</p>
<p>We first test the model's performance on various environments with the goal to provide an overview of the capabilities and limitations.We then describe how different prior components affect the predictions of OSWM, explore the impact of various context generation methods, and analyze learned reward functions.</p>
<p>Results for Agent Training</p>
<p>We evaluate the performance of OSWM by training an RL agent using the PPO algorithm [Schulman et al., 2017], as implemented in stable-baselines 3 [Raffin et al., 2021].We chose PPO because it can handle both discrete and continuous action spaces, making it well suited for the variety of environments in this study.We selected environments that provide a mix of discrete and continuous state and action spaces, allowing us to assess OSWM's performance across different types of RL challenges.The selected environments include two custom environments, GridWorld and SimpleEnv (see Appendix D for details), as well as CartPole-v0, MountainCar-v0, Pendulum-v1, and Reacher-v4 from the classic control gym suite.</p>
<p>In GridWorld, the agent navigates a discrete, 8x8 grid to reach a target location, receiving a positive reward for reaching the target and small penalties for each step, and the environment is considered solved when the agent consistently reaches the target efficiently.SimpleEnv involves moving a point along a 1D continuous line toward the center, with rewards negatively proportional to the distance from the center.CartPole-v0 is solved with an average reward of 195, MountainCar-v0 with an average reward of -110, Pendulum-v1 maximizes the reward when balancing the pendulum upright, and Reacher-v4 is solved with an average reward of around -3.75.</p>
<p>We trained agents for 50k steps in all environments, except MountainCar-v0, where training was extended to 200k steps with actions repeated five times to enhance exploration.All PPO hyperparameters were kept in their default settings.In all experiments, unless stated otherwise, OSWM was provided with 1k context steps collected from the real environment using random actions.</p>
<p>Quantitative Evaluation of Agent Performance</p>
<p>In Table 1, we compare the average performance of 100 test episodes for three agents: OSWM-PPO, PPO, and a Random Baseline.OSWM-PPO is trained purely on the dynamics predicted by OSWM using 1k context steps from the real environment, while PPO is trained only on the real environment, and the Random Baseline selects actions randomly.Since OSWM's synthetic rewards may not be indicative of the current agent's performance on the real environment, we evaluate each agent periodically after 100 training steps.Moreover, as discussed below in Section 4.1.2,training the agent too long on OSWM can result in performance degradation.Therefore, we apply an early stopping heuristic that takes the best agent training checkpoint.We do this on a per-seed-basis and compute the mean over multiple seeds. .OSWM-PPO is a PPO agent trained only on the OSWM, PPO is a PPO agent trained on the real environment and the random baseline is an agent taking random actions.All agents are evaluated on the real environment, and we apply an early stopping heuristic for each seed before we compute the mean.</p>
<p>In GridWorld, OSWM-PPO matches PPO with a reward of 5.2, outperforming the random baseline (-14.2) and demonstrating robustness in simple environments.In CartPole-v0, OSWM-PPO achieves 196.5, close to PPO's 200 (random baseline: 21.3).Also in SimpleEnv, OSWM-PPO reaches -4.7, performing well compared to PPO (-0.8) and significantly better than the random baseline (-256.2).These results are particularly surprising, as they show that pretraining on synthetic dynamics generated by random, untrained neural networks can still lead to strong performance in certain tasks, even without direct training on real environment data.</p>
<p>In more complex environments like MountainCar-v0 and Pendulum-v1, OSWM-PPO struggles to match PPO, with larger gaps in rewards, indicating that the approach here is less effective.However, for Reacher-v4, OSWM-PPO shows noticeable improvement, coming closer to PPO performance and performing far better than the random baseline.In MountainCar-v0, the model appears inferior at interpolating behavior in unseen states or areas of the environment, as the random context covers only a small part of this task.In contrast, Pendulum-v1 should benefit from better exploration through random actions, as the initial state covers all pendulum rotations, and the random actions provide a wide range of velocities.Despite this, OSWM does not provide sufficiently accurate dynamics to support effective training, suggesting that Pendulum-v1 requires more precise control and dynamic predictions than OSWM can currently offer.This may be due to the inherent difficulty posed by these environments, including sparse rewards and continuous action spaces, which likely require more sophisticated priors to improve performance.</p>
<p>Performance Progression Across Training Steps</p>
<p>To better understand the progression of agent training when training on OSWM, we report the learning curves in Figure 2. Here, we depict the mean evaluation rewards over training steps for three PPO agents using OSWM, with the best and worst performances highlighted.2Performance is measured on the real environment over 10 test episodes.</p>
<p>In the GridWorld environment (left), agents quickly solve the task after minimal interaction, with only one agent showing slightly suboptimal behavior after about 15,000 steps.This demonstrates the robustness of OSWM in simple environments.</p>
<p>For CartPole-v0 (center), agents show strong early performance, with the mean curve stabilizing after a brief drop.The best-performing agent continues to improve, while the worst-performing agent experiences a notable drop-off later in training.This phenomenon, where initial improvements are followed by a decline, can be attributed to gaps in the OSWM's understanding of certain environment dynamics.For instance, OSWM might model the dynamics accurately at higher angular velocities but struggle at lower velocities, failing to account for subtle drifts that are not captured.As a result, the agent may receive overconfident reward signals, leading to poor performance when these unmodeled drifts become significant in the real environment.</p>
<p>In SimpleEnv (right), agents exhibit a sharp initial increase in performance, followed by a plateau or decline.The worst-performing agent's reward nearly returns to its initial level, highlighting variability in training outcomes.This suggests that while OSWM supports learning, the one-shot prediction approach can introduce variability in performance, particularly in continuous environments where fine control is crucial.</p>
<p>Studying the Prior</p>
<p>In this section, we analyze the behavior of the Neural Network (NN) prior used in OSWM, which generates diverse dynamics through randomly initialized neural networks.To understand the state dynamics produced by the NN prior, we sample batches of data, reflecting what OSWM encounters during training.For each prior dimension (e.g., the agent's position in GridWorld), we calculate the minimum and maximum values and divide them into 100 equal bins, visualizing the distribution for each dimension.The histograms in Fig. 3 show three distinct types of distributions produced by the NN prior.Some prior dimensions exhibit highly peaked distributions, as shown in Fig. 3a, where most values fall within a narrow range.For other dimensions, we observe wider and smooth distributions with a more even spread of values, as seen in Fig. 3b.Finally, some prior dimensions follow multimodal distributions, with two or more distinct peaks, as depicted in Fig. 3c.This pattern of three distinct distribution types is commonly observed across various dimensions.</p>
<p>The variation in distribution types suggests that the NN prior can capture both simple and more complex, multimodal scenarios.However, as shown in Table 2 (left), using only the NN prior impacts OSWM-PPO performance in environments like CartPole-v0, where momentum is key for modeling the pole's angular dynamics.In contrast, GridWorld and SimpleEnv, which do not entail momentum, perform similarly to when both the NN and momentum priors are used (see Table 1).MountainCar-v0, Reacher-v4, and Pendulum-v1 were unsolvable before, and as expected, removing complexity from the prior does not make them solvable.This highlights that while the NN prior's multimodality supports diverse behaviors, it is insufficient for tasks that rely on accurate momentum-based dynamics.</p>
<p>The distributions of the momentum prior are reported in Appendix A. The right column of Table 2 on improved context generation is analyzed further in Section 4.3.</p>
<p>Studying the Context Sampling</p>
<p>Context is crucial for the predictive performance of OSWM.This section explores how different context sampling methods affect the model's predictions.Assessing the role of sampling strategies -10.0 Â± 0.6 -Table 2: Average performances when the OSWM is trained with the NN prior only (left; with randomly sampled context), as well as when a more sophisticated context sampling strategy is adopted (with NN+momentum prior).Higher values are better.</p>
<p>requires multiple agent trainings in OSWM and evaluations across multiple test episodes and environments.Since this is computationally expensive, we make use of a proxy dataset to evaluate the effectiveness of various sampling strategies more efficiently.The details of the generation of the proxy set are provided in Appendix B, but a high-level overview is given here.</p>
<p>The proxy set is created from transitions collected in the real environment using a PPO agent trained to perform at the expert level.First, the PPO agent is used to generate 5000 expert transitions across multiple episodes.From this, 500 transitions are sampled for each of three settings: 0% randomness (expert actions only), 50% randomness (half expert, half random) and 100% randomness (random actions only).This total of 1500 transitions spans expert behavior to exploratory actions and the proxy.The intuition behind mixing random and expert transitions is to cover states that are not typically encountered by an expert agent alone and thus, the proxy set can capture a wider range of environment dynamics.We then tested five different context sampling strategies: random (actions sampled uniformly), repeat (random actions repeated for three steps), expert (policy solving the environment), p-expert (mixing PPO expert and random actions 50/50), and mixture (first third random, second third p-expert, final third PPO expert).</p>
<p>For evaluation, OSWM is provided with 1000 context steps from each strategy, and the proxy set is used to assess their impact on model predictions (Table 3 in the appendix) by computing the mean squared error (MSE) between predicted dynamics and true targets from the proxy set.Based on the proxy loss, the best strategy is selected for each environment and evaluated in Table 2 (right).In complex tasks like MountainCar-v0 and Pendulum-v1 (using mixture), even with improved context, these environments remain unsolved.For Reacher-v4 (random), simple random sampling proves best, reflecting that basic methods can sometimes capture the necessary dynamics.In SimpleEnv (p-expert), the improved context sampling enhances performance.GridWorld (mixture) sees minimal variation, with random sampling generally being sufficient to capture its simpler dynamics.Overall, p-expert and mixture often yield the best results, while repeat and expert strategies are less effective.</p>
<p>Random proves to be a reliable default, offering solid performance across many environments.</p>
<p>Studying the Reward Function</p>
<p>Visualizing the reward function across environments reveals the dynamics learned by OSWM and their differences from the real environment.Figure 4 shows the reward analysis, with each plot generated based on the rewards predicted by OSWM given a random seed and a randomly sampled context.</p>
<p>In the GridWorld environment, each triangle corresponds to an action, which means that for every state (x, y) we represent four possible actions.OSWM smooths the reward function, making it easier for agents to be guided toward a goal state.Nevertheless, some inaccuracies appear in the reward distribution, for instance, in areas like the bottom-right corner, where states are rewarding instead of penalizing.We hypothesize that this may stem from a combination of undersampled states in the context as well as the inherent smoothness induced by the synthetic prior training.</p>
<p>For CartPole, the reward distribution predicted by OSWM is smoother than in the real environment, and we argue that this makes it easier for the agent to keep the pole upright.However, this smoothness can result in certain states near instability receiving rewards when they should be penalized.</p>
<p>We report the results for Pendulum and further details in Appendix E. The original Pendulum reward function is already dense and OSWM mostly mimics the original reward function.However, OSWM seems to provide higher rewards for safe states, for instance, where the pendulum is almost upright.</p>
<p>In some of the originally penalizing states, OSWM rewards the agent, which may negatively affect the model's ability to capture nuanced control at the edges of the pendulum's range of motion.This could potentially explain the weaker performance on Pendulum.</p>
<p>Our results suggest that the smooth reward distributions produced by OSWM may be a reflection of the multifaceted prior dimensions analyzed in Section 4.2.The mixture of distribution types, ranging from peaked to smooth distributions, is mirrored in the learned smooth-reward function.Despite some imprecision in certain states, this smoothness helps the agent efficiently grasp the environment's dynamics, particularly in sparse reward environments where the combination of a smooth synthetic prior and in-context learning proves to be well-suited.These observations are consistent across multiple seeds, and no specific seeds were cherry-picked for the results presented here.</p>
<p>Conclusion</p>
<p>We introduced One-Shot World Model (OSWM), a world model trained purely on synthetic data sampled from a prior distribution based on randomly initialized, untrained neural networks by leveraging In-context Learning.Despite the simplicity of the prior, OSWM achieved promising results as it is able to train RL agents to solve tasks like GridWorld and control tasks such as CartPole-v0, demonstrating the potential of synthetic pretraining in Model-Based Reinforcement Learning.Although the model still struggles with more complex environments like Pendulum-v1 and MountainCar-v0, our empirical analysis suggests that improving the priors and refining context sampling are key to enhancing performance.Our results highlight the potential of synthetic pretraining in RL, suggesting that with further optimization, this approach could be a key step towards foundation world models, capable of tackling increasingly complex tasks.With further optimization of the prior, synthetic pretraining could enable the development of more generalizable foundation world models, offering a scalable solution for RL training, especially when evaluating real-world environments is costly and challenging.</p>
<p>A Studying the Momentum Prior</p>
<p>Unlike the Neural Network prior, the Momentum prior is based on physics-driven dynamics, modeling velocity and positional updates to simulate environments with simple physical laws.</p>
<p>To analyze the behavior of the Momentum prior, we generate histograms in the same manner as with the NN prior, sampling batches of data and calculating the minimum and maximum values for each dimension.These dimensions reflect aspects like velocity and position, which are updated according to basic physical interactions such as gravity or action forces.The range of each dimension is then divided into 100 equal bins, and the occurrences in each bin are counted to visualize the distribution of values.</p>
<p>The Momentum prior produces a variety of distributions across dimensions, as shown in Figure 5.In some cases, we observe broad distributions with values spread uniformly across the range (Fig. 5a).This often occurs in environments with elastic reflections or angular motion without gravity.In other cases, the distribution is multi-modal, featuring multiple peaks (Fig. 5b), which can arise from non-elastic reflections or angular dynamics with insufficient torque to overcome gravity.Finally, some dimensions exhibit sparse distributions (Fig. 5c), where values cluster into a few discrete states.This pattern typically results from environments lacking friction or other forces that would normally smooth out the motion.</p>
<p>These distribution patterns reflect the diversity of physical interactions captured by the Momentum prior.Compared to the NN prior, the behavior here is more interpretable, as it directly corresponds to simplified physical models of motion and interaction.</p>
<p>B Context Generation Evaluation</p>
<p>To further investigate the effect of different context-generation strategies, we performed an evaluation using a proxy loss for full OSWM training and RL agent evaluation.The proxy set was constructed by generating transitions from a PPO agent trained on each specific environment.These transitions included state-action pairs, next states, and rewards.</p>
<p>We simulated different levels of randomness to capture a range of behaviors.Specifically, we generated rollouts with 0% randomness (only expert actions), 50% randomness (half expert, half random), and 100% randomness (only random actions).For each level of randomness, we collected 5000 transitions across multiple episodes and randomly subsampled 500 transitions per level, resulting in a total of 1500 transitions per environment.This proxy set was used to compute the mean squared error (MSE) between the predicted dynamics from OSWM and the actual transitions.</p>
<p>The intuition behind why we believe the proxy set is effective lies in its ability to cover a wide range of environment dynamics.Certain environments, like MountainCar-v0, require exploration using both efficient, expert-like actions to solve the task, and suboptimal actions to discover diverse states in the environment.Similarly, for environments like CartPole, non-goal-oriented actions-such as those where the pole is not upright or the cart velocity is high-allow the model to observe critical states not typically encountered by an expert agent alone.By including random actions in the proxy set, we aim to capture these middle-ground dynamics, such as a scenario in MountainCar where a fast-moving car decelerates, a behavior not covered by either purely expert or random actions.Additionally, this</p>
<p>D.1 Custom GridWorld</p>
<p>The GridWorld environment is designed as a simple environment with discrete states and actions.It is deliberately easy to solve, as its main goal is to test the modeling capabilities of the OSWM in an easy base case.This is further helped by the fact that for discrete spaces, the OSWM predictions are rounded to the next integer value.This allows us to use the same condition for termination.And give the agent the same interface as the real environment.</p>
<p>D.2 Custom SimpleEnv</p>
<p>The SimpleEnv serves to provide a first intuition for continuous action and state space environments, while using simplistic dynamics.Similar to the GridWorld, it is designed to be easily solved by RL agents with smooth and dense goal-oriented rewards.</p>
<p>It has 1-dimensional continuous action space (a âˆˆ [âˆ’10., 10.]) and a 1-dimensional continuous state space (s âˆˆ [âˆ’30., 30.]).The immediate reward is the negative absolute state, r = âˆ’1 â€¢ abs(s).</p>
<p>Episodes have a fixed length of 20 steps.The dynamics of the environment are defined by the action being added to the state, s t = s tâˆ’1 + a t .The initial state of the environment is sampled uniformly between -5 and 5.</p>
<p>D.3 Solved Reward for Custom Environments</p>
<p>To establish the solved threshold for custom environments, a relative score is determined based on a comparison between expert performance and random actions.This approach allows for the definition of a consistent threshold across various environments.The solved reward is calculated using the following equation:
R solved = R max âˆ’ (R max âˆ’ R random ) Ã— 0.03(1)
In this equation, R max represents the expert-level performance, while R random is the expected reward when taking random actions.The coefficient of 0.03 is chosen as it aligns with the solved threshold established for the CartPole-v0 environment, providing a standard for evaluating other environments.</p>
<p>E Additional Reward Function Analysis</p>
<p>Here, we conduct the reward function analysis for the Pendulum environment, shown in Figure 7.</p>
<p>The original environment's reward function is already dense, and OSWM generally preserves this dense structure.However, OSWM introduces more discrete rewards, especially in states where the pendulum is near upright, offering higher rewards compared to the real environment.In some extreme states, where penalties should be applied, OSWM rewards the agent instead.This smoothing may reduce the model's ability to capture more precise control at the edges of the pendulum's motion range, which could explain the model's inferior performance in this task.For both the Cartpole and Pendulum environments, the reward depends solely on the angular position.To assess whether the OSWM can accurately model this relationship, we plot the results based on this dimension, although the observation space for both environments has higher dimensionality.We aggregate the results as follows.</p>
<p>We sample 1000 observation-action pairs and predict the dynamics using OSWM.For Pendulum, the entire observation space is sampled, while for Cartpole, the velocity components of the observation space are capped at a magnitude of 5.The angular position is discretized into 100 bins for Pendulum and 20 bins for Cartpole.For each bin, we compute the mean over all observations that fall within that bin to represent the relationship between angular position and the predicted reward.</p>
<p>F BO for Prior and Model Hyperparameter</p>
<p>In order to determine the ideal hyperparameter for both the OSWM model and the underlying prior, an automatic optimization was performed.For the prior, especially, the architecture of the neural networks in NN prior play a crucial role in its performance.The library used for this optimization is HpBandSter.The model is trained for a fixed 50 epochs, we omit using Hyperband, as it is unclear how the different complexity of priors plays into the reliance of the low-cost proxy for the OSWM.</p>
<p>The optimization was performed for 45 iterations with 3 workers.The configuration space and results can be found in 6.The target function, being optimized, is the same validation loss used for evaluating the context generation types in Sec.4.3.</p>
<p>For the optimization of the encoder and decoder models of the OSWM, the same optimization was performed.The baseline is a linear encoder and decoder, for more complex data, a more expressive encoder and decoder might aid in representation.Additionally, it allows us to separately encode action and state, and separately decode the next state and reward.The choices for encoding and decoding are a standard MLP or a model with separate MLPs concatenating both outputs, denoted with Cat.An overview of the entire configuration space and the results are given in table 7.</p>
<p>Figure1: OSWM is trained on synthetic data sampled from a prior distribution of randomly initialized, untrained neural networks that mimic RL environments (left).Given a sequence of synthetic interactions, OSWM is optimized by predicting future dynamics at random cut-offs (center).RL agents can then be trained on OSWM to solve simple real environments given a context.</p>
<p>Figure 2 :
2
Figure 2: Evaluation scores for RL agent training on the OSWM for GridWorld, CartPole-v0, and SimpleEnv.Blue shows the mean over 3 runs, with the standard deviation in light blue.Orange and green depict the best and worst-performing agents, respectively.</p>
<p>Figure 3 :
3
Figure 3: Typical distribution patterns generated by the NN prior: (a) highly peaked, (b) wide or smoother, and (c) multi-modal distributions.</p>
<p>Figure 4: Reward distributions for the real and OSWM GridWorld and CartPole environments.</p>
<p>Figure 5 :
5
Figure 5: Typical distribution patterns generated by the Momentum prior: (a) broad, (b) multi-modal, and (c) sparse distributions.</p>
<p>Figure 6 :
6
Figure 6: Visualization of the custom GridWorld environment.Terminal states are in red, goal states are in green, and initial states are highlighted in black.Immediate reward in the cells.The GridWorld consists of an 8x8 grid.Observations are the x-postion and y-postion.Actions are 4 discrete moves (up, down, left, and right).With the outer ring of cells being terminal states with a negative ten reward.The goal states give a positive ten reward and are located in the second last column to the right.They span from the second row to the second last row.Each step gives a negative one reward and a small positive (0.01 * x pos ) for being further to the right.Episodes are truncated after exceeding 25 episode steps.The agent starts the episode at x pos = 1 and with a y pos between 1 and 6.A visualization of the GridWorld can be found in fig.6.</p>
<p>Figure 7 :
7
Figure 7: Reward distributions for the real and OSWM Pendulum environments.</p>
<p>Table 1 :
1
Average performances over 3 seeds on 100 test episodes of 3 different agents (higher values are better)
EnvironmentOSWM-PPOPPORandom BaselineGridWorld5.2 Â± 0.05.2 Â± 0.0-14.2 Â± 0.3CartPole-v0196.5 Â± 4.2200.0 Â± 0.021.3 Â± 3.9SimpleEnv-4.7 Â± 5.2-0.8 Â± 0.1-256.2 Â± 16.6MountainCar-v0-200.0 Â± 0.0-110.5 Â± 2.1-200.0 Â± 0.0Pendulum-v1-1185.4 Â± 31.2-268.9 Â± 22.2-1230.3 Â± 8.6Reacher-v4-10.2 Â± 0.9-4.6 Â± 0.3-42.8 Â± 0.3
We point out that the mean curves in Fig.2do not use the early stopping heuristic and therefore, do not correspond to the mean values of Tab. 1 where we take the mean over early stopped agents.
Acknowledgments and Disclosure of FundingFrank Hutter acknowledges the financial support of the Hector Foundation.We also acknowledge funding by the European Union (via ERC Consolidator Grant DeepLearning 2.0, grant no.101045765).Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council.Neither the European Union nor the granting authority can be held responsible for them.Moreover, we acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828.strategy helps represent the trajectory from suboptimal to successful actions, enhancing OSWM's capacity to generalize across different levels of agent performance.The results for each context-generation strategy (random, repeat, expert, p-expert, and mixture) across the various environments are shown in Table3.This table provides a detailed view of how the different strategies affect the proxy loss, which serves as a reliable proxy for predictive performance.C HyperparametersThe hyperparameters for the OSWM can be found in table4. For training the OSWM, the hyperparameters can be found in 5.q for eval function 0.4D Custom Environment DetailsThis following section will describe the details of the custom environment used to evaluate the OSWM.First, the GridWorld environment will be described, afterward the SimpleEnv.7: The hyperparameters of the encoder and decoder of the OSWM optimized using BO.Each hyperparameter, with its type, the range or choices, and final best-performing value.
Augmented world models facilitate zero-shot dynamics generalization from a single offline environment. Philip J Ball, Cong Lu, Jack Parker-Holder, Stephen Roberts, 2021</p>
<p>Transdreamer: Reinforcement learning with transformer world models. Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn, 2022</p>
<p>Context is everything: Implicit identification for dynamics adaptation. Ben Evans, Abitha Thankaraj, Lerrel Pinto, 2022</p>
<p>Learning synthetic environments and reward networks for reinforcement learning. F Ferreira, T Nierhoff, A SÃ¤linger, F Hutter, Proc. of ICLR'21. of ICLR'212021</p>
<p>Unsupervised zero-shot reinforcement learning via functional reward encodings. Kevin Frans, Seohong Park, Pieter Abbeel, Sergey Levine, 2024</p>
<p>World models. David Ha, JÃ¼rgen Schmidhuber, CoRR, abs/1803.101222018</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T P Lillicrap, J Ba, M Norouzi, Proc. of ICLR'20. of ICLR'202020</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, Proc. of ICLR'21. of ICLR'212021</p>
<p>D Hafner, J Pasukonis, J Ba, T P Lillicrap, arXiv:2301.04104[cs.AI]Mastering diverse domains through world models. 2023</p>
<p>TD-MPC2: scalable, robust world models for continuous control. N Hansen, H Su, X Wang, Proc. of ICLR'24. of ICLR'242024</p>
<p>TabPFN: A transformer that solves small tabular classification problems in a second. N Hollmann, S MÃ¼ller, K Eggensperger, F Hutter, Proc. of ICLR'23. of ICLR'232023</p>
<p>Model-based reinforcement learning for atari. L Kaiser, M Babaeizadeh, P Milos, B Osinski, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, A Mohiuddin, R Sepassi, G Tucker, H Michalewski, 2019</p>
<p>Supervised pretraining can learn in-context reinforcement learning. Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, Emma Brunskill, 2023</p>
<p>Transformers are sample-efficient world models. Vincent Micheli, Eloi Alonso, FranÃ§ois Fleuret, 2023</p>
<p>Transformers can do Bayesian inference. S MÃ¼ller, N Hollmann, S Arango, J Grabocka, F Hutter, Proc. of ICLR'22. of ICLR'222022</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, Noah Dormann, Journal of Machine Learning Research. 222682021</p>
<p>Mdp playground: An analysis and debug testbed for reinforcement learning. Raghu Rajan, Jessica Lizeth Borja, Suresh Diaz, Fabio Guttikonda, AndrÃ© Ferreira, ; Biedenkapp, Frank Hutter, 10.1613/jair.1.14314Journal of Artificial Intelligence Research. 77Jan Ole von Hartz,. 2023JAIR</p>
<p>Transformer-based world models are happy with 100k interactions. Jan Robine, Marc HÃ¶ftmann, Tobias Uelwer, Stefan Harmeling, 2023</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, T P Lillicrap, D Silver, 10.1038/s41586-020-03051-4Nat. 58878392020</p>
<p>A generalist dynamics model for control. Ingmar Schubert, Jingwei Zhang, Jake Bruce, Sarah Bechtle, Emilio Parisotto, Martin Riedmiller, Jost Tobias Springenberg, Arunkumar Byravan, Leonard Hasenclever, Nicolas Heess, 2023</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 2017</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, 2017</p>
<p>Pre-training with synthetic data helps offline reinforcement learning. Zecheng Wang, Che Wang, Zixuan Dong, Keith Ross, 2024</p>
<p>Daydreamer: World models for physical robot learning. Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, Ken Goldberg, of Proceedings of Machine Learning Research. Karen Liu, Dana Kulic, Jeffrey Ichnowski, Auckland, New ZealandPMLRCoRL 2022, 14-18 December 2022. 2022205Conference on Robot Learning</p>
<p>Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel, arXiv:2310.06114Learning interactive real-world simulators. 2023arXiv preprint</p>
<p>Storm: Efficient stochastic transformer based world models for reinforcement learning. Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, Gao Huang, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
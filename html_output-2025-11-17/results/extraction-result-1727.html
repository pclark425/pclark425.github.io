<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1727 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1727</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1727</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-268385444</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.09631v1.pdf" target="_blank">3D-VLA: A 3D Vision-Language-Action Generative World Model</a></p>
                <p><strong>Paper Abstract:</strong> Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1727.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1727.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLIP2-FlanT5 XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLIP-2 with Flan-T5 XL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained vision-language backbone combining BLIP-2 image encoders with the instruction-tuned Flan-T5 XL language model; used in this paper as the frozen / partially-finetuned LLM backbone for 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BLIP2-FlanT5 XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Vision-language model (BLIP-2) paired with an instruction-tuned language model (Flan-T5 XL). In 3D-VLA it is used as the LLM backbone; input embeddings and Q-Former weights are unfrozen and special interaction tokens are added.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>vision-language and instruction-tuned language corpora (text + image-caption corpora / instruction-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper states BLIP-2 with Flan-T5 XL is used as the pretrained backbone; the paper does not enumerate exact datasets or sizes for the BLIP-2/Flan-T5 pretraining within this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D reasoning, multimodal goal generation, and embodied action planning (evaluated on Open-X, RoboVQA, RLBench, CALVIN, RT-1 datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Tasks include embodied QA, task captioning, what-if QA, dense captioning, localization, RGB-D / point-cloud goal generation, and robot action prediction/planning in simulated and real robotic datasets (Open-X collection, RoboVQA, RLBench, CALVIN, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level natural language / instruction tokens (instruction text describing manipulations and goals).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete tokenization of robot 7-DoF actions represented by tokens (<aloc0-255>, <arot0-255>, <gripper0/1>), plus <ACT SEP> separators; actions correspond to arm absolute location, rotation, and gripper openness.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Integration via interaction tokens and discrete action-token vocabulary inside the LLM; 3D scene tokens and location tokens ground object references; a projector aligns LLM output embeddings to diffusion decoders for goal generation. The paper describes directly tokenizing 7-DoF actions into discrete action tokens and supervising them in language-formatted prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images, estimated or ground-truth depth (RGB-D), lifted 3D point clouds, 3D bounding boxes; object masks and optical flow used during annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When used in 3D-VLA, models built on BLIP2-FlanT5 XL achieved strong held-in language-reasoning results (e.g., 3D-VLA outperforms BLIP2-FlanT5 XL baseline in Table 1; BLIP2-FlanT5 XL baseline reported in table for Embodied QA: BLEU-1 37.31, BLEU-2 27.20, BLEU-3 20.32, BLEU-4 15.48, METEOR 17.80, ROUGE-L 38.92). These are baseline numbers; 3D-VLA improves upon them.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not reported explicitly for BLIP2-FlanT5 XL (paper compares to zero-shot and held-in baselines; 3D-LLM and 2D VLM baselines presented).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported (training used the curated 3D-language-action dataset of ~2M pairs; 3D-VLA pretrain for 30 epochs, alignment stage up to 20 epochs — but no sample-to-performance curves provided).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Availability of instruction-tuned language model weights, addition of interaction tokens (object, location, scene, action tokens), grounding of 3D scene features via multi-view lifting and point-cloud annotations, and alignment projector to connect LLM embeddings to multimodal decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not specifically observed for BLIP2-FlanT5 XL in this work; general risks mentioned include mismatch between pretraining domains and embodied data, and limited scale of curated 3D embodied dataset compared to internet-scale corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction- and vision-language-pretrained LLMs (BLIP-2 + Flan-T5 XL) can be adapted to 3D embodied tasks effectively when augmented with explicit 3D interaction tokens, grounding (3D bounding boxes / point clouds), and an alignment projector to connect to goal-generation decoders; this yields better reasoning and localization compared to 2D VLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1727.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Point-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point‑E (text-to-3D diffusion model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained diffusion model that generates 3D point clouds from text prompts; used here as the base for point-to-point diffusion pretraining and as a baseline for point-cloud goal generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Point-E (adapted point-to-point diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Point-E is used as the pretrained backbone for point-cloud-to-point-cloud diffusion models; the authors add a point-cloud condition input and fine-tune (LoRA) to perform embodied point-cloud goal generation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>text-conditioned 3D generation (text-to-3D) — multimodal text + 3D point cloud data used to train Point‑E originally (as referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper states Point-E is used as the pretrained model; it does not list Point-E's original dataset sizes here. The authors fine-tune Point-E on their curated 3D-language video data for point-to-point generation.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Point-cloud goal generation (Open-X test episodes) and point-cloud-conditioned goal imagination for action planning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Given an initial scene point cloud and an instruction, generate a goal point cloud representing the imagined final state of the scene to guide planning and action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions describing the manipulation goal (text prompts conditioning the generation).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Robotic action tokens (discrete 7-DoF tokenization) used downstream; for generation the action conditioning is via language and scene point clouds.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM produces goal-specifying tokens (<pcd> ... </pcd>) and object/location tokens; a transformer-based projector maps LLM embeddings into the Point-E decoder latent/conditioning space; LoRA fine-tuning reduces catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Input point clouds (lifted from RGB-D), 3D bounding boxes, object masks, and depth maps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>3D-VLA (using aligned Point-E backbone) outperforms raw Point-E baseline on point-cloud generation metrics reported in Table 3/4 (example: Point-E* P-FID 5.241 / Chamfer-L1 0.159 vs 3D-VLA P-FID 4.796 / Chamfer-L1 0.139).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Point-E baseline metrics reported (Point-E* P-FID 5.241, Chamfer-L1 0.159) — these represent performance without the LLM alignment and embodied fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported (training used curated dataset; absolute sample counts to reach accuracy thresholds are not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained point-cloud decoder provided strong inductive bias for 3D generation; aligning LLM embeddings to the point-cloud decoder via a projector and fine-tuning on robotics-specific 3D-language data improved goal realism and semantic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Noted failure modes when applying internet pretrained generation models directly (view distortions, object deformation) unless fine-tuned on embodied datasets and aligned to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a pretrained text-to-3D generator (Point-E) as a decoder and aligning it to an LLM with a projector plus fine-tuning on robotics 3D-language data yields better point-cloud goal generation than directly using Point-E; LLM-conditioned decoders improve semantic correctness of imagined goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1727.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stable Diffusion V1.4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stable Diffusion v1.4 (latent diffusion image model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used pretrained latent diffusion image model used here as the starting point for RGB-D to RGB-D embodied goal generation (concatenating RGB latent and depth latent as condition).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Stable Diffusion v1.4 (adapted RGBD->RGBD diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Stable Diffusion V1.4 is used as the pretrained image generator; authors adapt it to take RGB + depth latents as conditions and fine-tune (LoRA) on their embodied RGB-D paired data to generate final-state RGB-D goal images.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large-scale image-text internet datasets (image generative model pretrained on image data with text-conditioning in original Stable Diffusion; this paper uses the pretrained weights but does not re-describe SD's pretraining datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper states Stable Diffusion v1.4 is used as the pretrained model for RGBD-to-RGBD generation; no SD training dataset sizes are reported here. The authors fine-tune SD on their curated 3D-language video data.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RGB-D goal image generation (Open-X, RT-1, Jaco Play test episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Given initial RGB-D frames and a manipulation instruction, generate an RGB-D goal image that preserves background and modifies the target object's state to reflect the completed action.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions conditioning image edits / goal state.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Downstream discrete robotic action tokens; image generation is used to inform planning rather than directly producing low-level motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM emits <image> ... </image> token-encapsulated prompts and bounding-box / object tokens; a transformer projector maps LLM features into the latent space required by the diffusion decoder; LoRA fine-tuning on SD preserves pretrained knowledge while adapting to embodied edit tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Input RGB images, estimated or ground-truth depth, object bounding boxes and masks, camera intrinsics/poses for lifting to point clouds.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>3D-VLA (using aligned Stable Diffusion) achieves higher image-goal metrics compared to baselines in Table 3 (example: 3D-VLA PSNR 17.21, CLIP Sim 0.920, SSIM 0.636, FID 0.177; Instruct-P2P* had PSNR 16.67, CLIP Sim 0.941, SSIM 0.628, FID 0.178 — 3D-VLA is comparable or better across metrics presented).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Zero-shot web-scale generation methods (e.g., NeXT-GPT) perform worse; direct application of off-the-shelf diffusion without embodied fine-tuning produced collapse or poor goal fidelity (qualitative observations discussed).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported (authors fine-tune on their curated dataset; dataset overall ~2M episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Using a pretrained high-quality latent image generator reduced the amount of image generator learning required; aligning the LLM outputs to the SD latent space and fine-tuning on robotics-specific data corrected domain mismatches (view/layout consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Direct use of internet-scale SD without embodied fine-tuning led to view changes, object deformation, and layout distortion; domain shift between web images and robotic scenes is a limiting factor unless corrected.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained image diffusion decoders can be adapted for embodied RGB-D goal generation by conditioning on depth and aligning to an LLM; fine-tuning on robotics 3D-language data plus a projector yields realistic, semantically-correct goal images that improve downstream planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1727.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM (Injecting 3D world into large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior 3D foundation-model approach that integrates 3D features into LLMs; used as methodological inspiration but not loaded as pretrained weights in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A 3D large language model that generates 3D scene features from multi-view inputs and injects them into an LLM for 3D reasoning. The authors followed its methodology but explicitly chose not to load its pretrained weights.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>3D scene data and multimodal data (described in cited works; not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper notes 3D-LLM training datasets mostly comprise objects and indoor scenes (Objaverse, ScanNet, HM3D, etc. referenced), but says these do not directly align with the robotics embodied setup; authors therefore did not use the 3D-LLM pretrained model.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Referenced for 3D reasoning tasks; not directly used for embodied transfer experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D-LLM is designed for 3D scene understanding and reasoning from multi-view inputs; the paper cites it in relation to 3D perception in LLMs but finds it insufficient for robot manipulation tasks without robotics-specific data.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable (3D-LLM focuses on perception and language reasoning rather than direct robot action tokenization in this citation).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not applicable in this paper (authors did not use 3D-LLM weights to drive robot actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>N/A in this paper; 3D-VLA adopts a different route (BLIP2-FlanT5 XL + interaction tokens) instead of loading 3D-LLM pretrained weights.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multi-view images and 3D scene reconstructions are typical inputs for 3D-LLM (as noted), but not used further here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>3D-LLM demonstrates value in injecting 3D features into LLMs, but the paper notes domain mismatch between its training data (objects / indoor scenes) and robotics manipulation data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain mismatch: pretrained 3D-LLM datasets did not align with embodied robotics scenarios, motivating the authors to use BLIP2-FlanT5 XL and curate a robotics-focused dataset instead.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained 3D-LM approaches exist, but direct reuse of their checkpoints may fail to transfer well to robotics manipulation tasks unless the pretraining domain matches the embodied task domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1727.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Vision-Language-Action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced vision-language-action embodied foundation model that transfers web-scale (2D) vision-language knowledge to robotic control; cited as prior work in 2D-centered embodied VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A vision-language-action model that conditions action generation on 2D visual inputs and language; cited as an example of 2D VLA models (not used in experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Web-scale vision-language corpora and image-language pretraining (as referenced in RT-2 original work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not detailed in this paper (RT-2 is only cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>2D image-conditioned robot planning / action generation (cited; not run in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>RT-2 generates plans or low-level actions conditioned on images; authors contrast it with their 3D-focused approach which incorporates 3D perception and world modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level language-conditioned commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level robot actions (RT-2 produces action outputs in original work; not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Cited models learn direct perception-to-action mappings; authors argue this lacks world-model-style future-state imagination, motivating 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Primarily 2D image inputs in RT-2 (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>RT-2 demonstrates that web-scale vision-language pretraining can inform robot control when adapted to the domain (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper argues RT-2's 2D focus misses essential 3D world dynamics and rich future-state imagination needed for some embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RT-2 is an example of transferring vision-language pretraining to embodied control, but its 2D-centric approach motivates the need for 3D-aware world models like 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1727.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PALM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E (Embodied multimodal language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced embodied multimodal LLM that produces plans or actions conditioned on images and language; cited as related work in vision-language-action models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A multimodal LLM that combines large language model capabilities with vision inputs for embodied reasoning and action generation (cited for context).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large language model pretraining plus vision-language fine-tuning (as described in PaLM-E literature), referenced but not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-language-conditioned planning and action generation (cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Generates high-level plans or low-level actions based on 2D visual inputs and language prompts (cited as prior art for VLA models).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language instructions and high-level commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level actions and plans (specifics not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Cited models typically learn direct mappings from vision+language to actions; 3D-VLA contrasts by adding generative world-model imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>2D images and language (as noted in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Demonstrates that large language models can be coupled with vision to inform embodied behavior (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in detail here; authors note these models generally lack explicit 3D world modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PALM-E is representative of multimodal LLMs used for embodied tasks, but 3D-VLA aims to extend beyond 2D perception to include generative 3D world models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1727.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruct-P2P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-P2P (image-editing diffusion guided by instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-conditioned image-editing diffusion model used as a baseline; the authors compare against both an off-the-shelf Instruct-P2P and a version trained on the paper's robotics dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Instruct-P2P (and Instruct-P2P*)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Image-editing diffusion model that follows textual editing instructions; Instruct-P2P* denotes the variant trained on the paper's curated robotics dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image-editing instruction data (text + image pairs) in original Instruct-P2P work; Instruct-P2P* fine-tuned on robotics 3D-language data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper reports an Instruct-P2P* variant trained on the authors' robotics data (same curated dataset of 3D-language pairs), but does not list exact size for that fine-tuning subset.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RGB goal image generation for robotics manipulation tasks (Open-X test set).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Given an initial image and an edit instruction (e.g., 'open the drawer'), generate an edited image showing the goal state.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language editing instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not directly actuator-level; used to imagine visual outcomes to inform planning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct instruction-conditioned image editing (no explicit LLM alignment projector in baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB (and for this paper depth appended for RGB-D variants), object bounding boxes when included in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Instruct-P2P* (trained on the robotics dataset) reported PSNR 16.67, CLIP Sim 0.941, SSIM 0.628, FID 0.178 (Table 3); 3D-VLA outperforms or is comparable when LLM alignment and bounding-box inputs are used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Off-the-shelf Instruct-P2P (not trained on robotics data) scored lower (Instruct-P2P PSNR 14.41, CLIP Sim 0.909, SSIM 0.389, FID 0.309).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported (fine-tuning details for Instruct-P2P* not fully enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Fine-tuning on robotics-specific data improves goal image generation quality; including predicted bounding boxes in prompts further helps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Off-the-shelf image-editing models trained on internet data may produce unrealistic view changes or deformations when applied to robotics scenes without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Image-editing diffusion models can be adapted to robotics goal generation by fine-tuning on robotics 3D-language data, but aligning them with an LLM and using 3D spatial cues (bounding boxes) yields better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1727.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeXT-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeXT-GPT (Any-to-any multimodal LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLM with image generation capability used as a baseline for RGB goal generation; when applied zero-shot to robotics data its performance is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>NeXT-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An 'any-to-any' multimodal LLM with image generation abilities; used as a baseline in image goal generation comparisons (zero-shot transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal (web-scale) image-text and other internet multimodal corpora (cited work), not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper treats NeXT-GPT as a zero-shot baseline and does not provide its pretraining statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RGB goal image generation (zero-shot application to robotics episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Attempt to generate goal images for robotics tasks from initial frames and instructions without robotics-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not applicable — generation-only baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Zero-shot text-to-image generation; no explicit mapping to robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Input instruction and sometimes initial images; performance degraded on robotics scenes due to domain mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>NeXT-GPT zero-shot reported lower metrics in Table 3 (PSNR 8.86, CLIP Sim 0.199, SSIM 0.153, FID 0.432) on robotics goal generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>N/A (this entry is the pretrained model applied zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not applicable / not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale multimodal pretraining gives generative ability, but domain mismatch limits direct applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Zero-shot transfer from web multimodal pretraining to robotics scenes fails due to viewpoint, object-state, and layout mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large multimodal LLMs with image generation capabilities do not reliably transfer to robotics goal generation zero-shot; robotics-specific fine-tuning and alignment are necessary for good results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1727.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kosmos-2 / CoVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kosmos-2 and CoVLM (grounding multimodal LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>2D grounding multimodal LLM baselines used for localization baselines; they are applied zero-shot to detect 2D boxes then projected to 3D for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Kosmos-2 / CoVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>2D grounding-capable multimodal LLMs used as baselines for localization by running 2D detection zero-shot and projecting to 3D; they are not adapted to 3D in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language pretraining on 2D image-text corpora (as per cited works), not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; these models are used only as comparators in localization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Localization on held-in robotics datasets (projecting 2D detections to 3D bounding boxes).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Detect the object mentioned in the instruction and localize it; Kosmos-2 / CoVLM produce 2D boxes which are then projected into 3D for IoU/accuracy comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (these models are used for perception/localization).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>2D detection followed by projection into 3D via camera intrinsics/poses.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>2D image inputs (they require image + text to ground object mentions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported localization IoU/Acc: Kosmos-2 (w/ GT Depth) IoU 10.92, Acc@25 12.73, Acc@50 3.85; CoVLM IoU 19.81, Acc@25 25.39, Acc@50 16.61; both are below 3D-VLA (IoU 29.33, Acc@25 42.26, Acc@50 27.09).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong 2D grounding capabilities, but require depth & projection to yield 3D localization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Lack of native 3D reasoning and direct 3D training makes 2D-grounded models less accurate after projection compared to a model trained with 3D annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>2D-grounded multimodal LLMs can be adapted to 3D localization by projecting 2D detections to 3D, but performance lags behind models trained with explicit 3D annotations and interaction tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1727.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1727.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used here for prompt diversification and automatic generation of diverse language templates and annotations for the 3D embodied instruction dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Instruction-following LLM used to rewrite templates, diversify prompts, and generate language inputs/outputs given the annotated objects and bounding boxes; used in dataset construction rather than directly producing embodied policies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale text corpora / instruction-tuned language model (chat LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper states ChatGPT (GPT-3.5-turbo-0125) was used for prompt diversification and to generate annotations; no dataset sizes provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Dataset annotation for embodied tasks (3D-language-action pair generation).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Generates diverse natural-language prompts and answers that include tokens for images, point clouds, actions, and 3D annotations to augment the training dataset for 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language templates and instruction text produced by ChatGPT for dataset examples.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (ChatGPT used for text only; downstream models consume this text for embodied training).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Generates textual descriptions and templates that include explicit action tokens and 3D annotation tokens used to supervise the LLM and downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>N/A for ChatGPT usage; it operates on textual and annotation inputs provided by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Not applicable as ChatGPT was used only to generate annotations; contributed to construction of the 2M 3D-language-action dataset that improved embodied-model training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>High-quality, diversified language annotations aided model instruction tuning and improved the diversity and naturalness of prompts for embodied training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Language-generation errors would propagate to dataset noise; authors mitigated this by providing few-shot human-written examples and structured annotation inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a pretrained conversational LLM to generate diverse language annotation for a large curated 3D-language-action dataset enabled large-scale instruction tuning necessary for 3D-VLA; ChatGPT served as a data-augmentation tool rather than an embodied controller.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D-VLA: A 3D Vision-Language-Action Generative World Model', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Point‑E: A system for generating 3D point clouds from complex prompts <em>(Rating: 2)</em></li>
                <li>High-resolution image synthesis with latent diffusion models <em>(Rating: 2)</em></li>
                <li>3D-LLM: Injecting the 3D world into large language models <em>(Rating: 2)</em></li>
                <li>InstructPix2Pix (Instruct-P2P): Learning to follow image editing instructions <em>(Rating: 2)</em></li>
                <li>NeXT-GPT: Any-to-any multimodal LLM <em>(Rating: 1)</em></li>
                <li>Kosmos-2: Grounding multimodal large language models to the world <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1727",
    "paper_id": "paper-268385444",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "BLIP2-FlanT5 XL",
            "name_full": "BLIP-2 with Flan-T5 XL",
            "brief_description": "A pretrained vision-language backbone combining BLIP-2 image encoders with the instruction-tuned Flan-T5 XL language model; used in this paper as the frozen / partially-finetuned LLM backbone for 3D-VLA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "BLIP2-FlanT5 XL",
            "model_agent_description": "Vision-language model (BLIP-2) paired with an instruction-tuned language model (Flan-T5 XL). In 3D-VLA it is used as the LLM backbone; input embeddings and Q-Former weights are unfrozen and special interaction tokens are added.",
            "pretraining_data_type": "vision-language and instruction-tuned language corpora (text + image-caption corpora / instruction-tuning)",
            "pretraining_data_details": "The paper states BLIP-2 with Flan-T5 XL is used as the pretrained backbone; the paper does not enumerate exact datasets or sizes for the BLIP-2/Flan-T5 pretraining within this manuscript.",
            "embodied_task_name": "3D reasoning, multimodal goal generation, and embodied action planning (evaluated on Open-X, RoboVQA, RLBench, CALVIN, RT-1 datasets)",
            "embodied_task_description": "Tasks include embodied QA, task captioning, what-if QA, dense captioning, localization, RGB-D / point-cloud goal generation, and robot action prediction/planning in simulated and real robotic datasets (Open-X collection, RoboVQA, RLBench, CALVIN, etc.).",
            "action_space_text": "High-level natural language / instruction tokens (instruction text describing manipulations and goals).",
            "action_space_embodied": "Discrete tokenization of robot 7-DoF actions represented by tokens (&lt;aloc0-255&gt;, &lt;arot0-255&gt;, &lt;gripper0/1&gt;), plus &lt;ACT SEP&gt; separators; actions correspond to arm absolute location, rotation, and gripper openness.",
            "action_mapping_method": "Integration via interaction tokens and discrete action-token vocabulary inside the LLM; 3D scene tokens and location tokens ground object references; a projector aligns LLM output embeddings to diffusion decoders for goal generation. The paper describes directly tokenizing 7-DoF actions into discrete action tokens and supervising them in language-formatted prompts.",
            "perception_requirements": "RGB images, estimated or ground-truth depth (RGB-D), lifted 3D point clouds, 3D bounding boxes; object masks and optical flow used during annotation.",
            "transfer_successful": true,
            "performance_with_pretraining": "When used in 3D-VLA, models built on BLIP2-FlanT5 XL achieved strong held-in language-reasoning results (e.g., 3D-VLA outperforms BLIP2-FlanT5 XL baseline in Table 1; BLIP2-FlanT5 XL baseline reported in table for Embodied QA: BLEU-1 37.31, BLEU-2 27.20, BLEU-3 20.32, BLEU-4 15.48, METEOR 17.80, ROUGE-L 38.92). These are baseline numbers; 3D-VLA improves upon them.",
            "performance_without_pretraining": "Not reported explicitly for BLIP2-FlanT5 XL (paper compares to zero-shot and held-in baselines; 3D-LLM and 2D VLM baselines presented).",
            "sample_complexity_with_pretraining": "Not reported (training used the curated 3D-language-action dataset of ~2M pairs; 3D-VLA pretrain for 30 epochs, alignment stage up to 20 epochs — but no sample-to-performance curves provided).",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Availability of instruction-tuned language model weights, addition of interaction tokens (object, location, scene, action tokens), grounding of 3D scene features via multi-view lifting and point-cloud annotations, and alignment projector to connect LLM embeddings to multimodal decoders.",
            "transfer_failure_factors": "Not specifically observed for BLIP2-FlanT5 XL in this work; general risks mentioned include mismatch between pretraining domains and embodied data, and limited scale of curated 3D embodied dataset compared to internet-scale corpora.",
            "key_findings": "Instruction- and vision-language-pretrained LLMs (BLIP-2 + Flan-T5 XL) can be adapted to 3D embodied tasks effectively when augmented with explicit 3D interaction tokens, grounding (3D bounding boxes / point clouds), and an alignment projector to connect to goal-generation decoders; this yields better reasoning and localization compared to 2D VLM baselines.",
            "uuid": "e1727.0",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Point-E",
            "name_full": "Point‑E (text-to-3D diffusion model)",
            "brief_description": "A pretrained diffusion model that generates 3D point clouds from text prompts; used here as the base for point-to-point diffusion pretraining and as a baseline for point-cloud goal generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "Point-E (adapted point-to-point diffusion)",
            "model_agent_description": "Point-E is used as the pretrained backbone for point-cloud-to-point-cloud diffusion models; the authors add a point-cloud condition input and fine-tune (LoRA) to perform embodied point-cloud goal generation.",
            "pretraining_data_type": "text-conditioned 3D generation (text-to-3D) — multimodal text + 3D point cloud data used to train Point‑E originally (as referenced).",
            "pretraining_data_details": "The paper states Point-E is used as the pretrained model; it does not list Point-E's original dataset sizes here. The authors fine-tune Point-E on their curated 3D-language video data for point-to-point generation.",
            "embodied_task_name": "Point-cloud goal generation (Open-X test episodes) and point-cloud-conditioned goal imagination for action planning",
            "embodied_task_description": "Given an initial scene point cloud and an instruction, generate a goal point cloud representing the imagined final state of the scene to guide planning and action prediction.",
            "action_space_text": "Natural language instructions describing the manipulation goal (text prompts conditioning the generation).",
            "action_space_embodied": "Robotic action tokens (discrete 7-DoF tokenization) used downstream; for generation the action conditioning is via language and scene point clouds.",
            "action_mapping_method": "LLM produces goal-specifying tokens (&lt;pcd&gt; ... &lt;/pcd&gt;) and object/location tokens; a transformer-based projector maps LLM embeddings into the Point-E decoder latent/conditioning space; LoRA fine-tuning reduces catastrophic forgetting.",
            "perception_requirements": "Input point clouds (lifted from RGB-D), 3D bounding boxes, object masks, and depth maps.",
            "transfer_successful": true,
            "performance_with_pretraining": "3D-VLA (using aligned Point-E backbone) outperforms raw Point-E baseline on point-cloud generation metrics reported in Table 3/4 (example: Point-E* P-FID 5.241 / Chamfer-L1 0.159 vs 3D-VLA P-FID 4.796 / Chamfer-L1 0.139).",
            "performance_without_pretraining": "Point-E baseline metrics reported (Point-E* P-FID 5.241, Chamfer-L1 0.159) — these represent performance without the LLM alignment and embodied fine-tuning.",
            "sample_complexity_with_pretraining": "Not reported (training used curated dataset; absolute sample counts to reach accuracy thresholds are not provided).",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretrained point-cloud decoder provided strong inductive bias for 3D generation; aligning LLM embeddings to the point-cloud decoder via a projector and fine-tuning on robotics-specific 3D-language data improved goal realism and semantic alignment.",
            "transfer_failure_factors": "Noted failure modes when applying internet pretrained generation models directly (view distortions, object deformation) unless fine-tuned on embodied datasets and aligned to the LLM.",
            "key_findings": "Using a pretrained text-to-3D generator (Point-E) as a decoder and aligning it to an LLM with a projector plus fine-tuning on robotics 3D-language data yields better point-cloud goal generation than directly using Point-E; LLM-conditioned decoders improve semantic correctness of imagined goals.",
            "uuid": "e1727.1",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Stable Diffusion V1.4",
            "name_full": "Stable Diffusion v1.4 (latent diffusion image model)",
            "brief_description": "A widely used pretrained latent diffusion image model used here as the starting point for RGB-D to RGB-D embodied goal generation (concatenating RGB latent and depth latent as condition).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "Stable Diffusion v1.4 (adapted RGBD-&gt;RGBD diffusion)",
            "model_agent_description": "Stable Diffusion V1.4 is used as the pretrained image generator; authors adapt it to take RGB + depth latents as conditions and fine-tune (LoRA) on their embodied RGB-D paired data to generate final-state RGB-D goal images.",
            "pretraining_data_type": "large-scale image-text internet datasets (image generative model pretrained on image data with text-conditioning in original Stable Diffusion; this paper uses the pretrained weights but does not re-describe SD's pretraining datasets).",
            "pretraining_data_details": "Paper states Stable Diffusion v1.4 is used as the pretrained model for RGBD-to-RGBD generation; no SD training dataset sizes are reported here. The authors fine-tune SD on their curated 3D-language video data.",
            "embodied_task_name": "RGB-D goal image generation (Open-X, RT-1, Jaco Play test episodes)",
            "embodied_task_description": "Given initial RGB-D frames and a manipulation instruction, generate an RGB-D goal image that preserves background and modifies the target object's state to reflect the completed action.",
            "action_space_text": "Natural language instructions conditioning image edits / goal state.",
            "action_space_embodied": "Downstream discrete robotic action tokens; image generation is used to inform planning rather than directly producing low-level motor commands.",
            "action_mapping_method": "LLM emits &lt;image&gt; ... &lt;/image&gt; token-encapsulated prompts and bounding-box / object tokens; a transformer projector maps LLM features into the latent space required by the diffusion decoder; LoRA fine-tuning on SD preserves pretrained knowledge while adapting to embodied edit tasks.",
            "perception_requirements": "Input RGB images, estimated or ground-truth depth, object bounding boxes and masks, camera intrinsics/poses for lifting to point clouds.",
            "transfer_successful": true,
            "performance_with_pretraining": "3D-VLA (using aligned Stable Diffusion) achieves higher image-goal metrics compared to baselines in Table 3 (example: 3D-VLA PSNR 17.21, CLIP Sim 0.920, SSIM 0.636, FID 0.177; Instruct-P2P* had PSNR 16.67, CLIP Sim 0.941, SSIM 0.628, FID 0.178 — 3D-VLA is comparable or better across metrics presented).",
            "performance_without_pretraining": "Zero-shot web-scale generation methods (e.g., NeXT-GPT) perform worse; direct application of off-the-shelf diffusion without embodied fine-tuning produced collapse or poor goal fidelity (qualitative observations discussed).",
            "sample_complexity_with_pretraining": "Not reported (authors fine-tune on their curated dataset; dataset overall ~2M episodes).",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Using a pretrained high-quality latent image generator reduced the amount of image generator learning required; aligning the LLM outputs to the SD latent space and fine-tuning on robotics-specific data corrected domain mismatches (view/layout consistency).",
            "transfer_failure_factors": "Direct use of internet-scale SD without embodied fine-tuning led to view changes, object deformation, and layout distortion; domain shift between web images and robotic scenes is a limiting factor unless corrected.",
            "key_findings": "Pretrained image diffusion decoders can be adapted for embodied RGB-D goal generation by conditioning on depth and aligning to an LLM; fine-tuning on robotics 3D-language data plus a projector yields realistic, semantically-correct goal images that improve downstream planning.",
            "uuid": "e1727.2",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "3D-LLM",
            "name_full": "3D-LLM (Injecting 3D world into large language models)",
            "brief_description": "A prior 3D foundation-model approach that integrates 3D features into LLMs; used as methodological inspiration but not loaded as pretrained weights in this work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "3D-LLM",
            "model_agent_description": "A 3D large language model that generates 3D scene features from multi-view inputs and injects them into an LLM for 3D reasoning. The authors followed its methodology but explicitly chose not to load its pretrained weights.",
            "pretraining_data_type": "3D scene data and multimodal data (described in cited works; not detailed in this paper).",
            "pretraining_data_details": "Paper notes 3D-LLM training datasets mostly comprise objects and indoor scenes (Objaverse, ScanNet, HM3D, etc. referenced), but says these do not directly align with the robotics embodied setup; authors therefore did not use the 3D-LLM pretrained model.",
            "embodied_task_name": "Referenced for 3D reasoning tasks; not directly used for embodied transfer experiments in this paper.",
            "embodied_task_description": "3D-LLM is designed for 3D scene understanding and reasoning from multi-view inputs; the paper cites it in relation to 3D perception in LLMs but finds it insufficient for robot manipulation tasks without robotics-specific data.",
            "action_space_text": "Not applicable (3D-LLM focuses on perception and language reasoning rather than direct robot action tokenization in this citation).",
            "action_space_embodied": "Not applicable in this paper (authors did not use 3D-LLM weights to drive robot actions).",
            "action_mapping_method": "N/A in this paper; 3D-VLA adopts a different route (BLIP2-FlanT5 XL + interaction tokens) instead of loading 3D-LLM pretrained weights.",
            "perception_requirements": "Multi-view images and 3D scene reconstructions are typical inputs for 3D-LLM (as noted), but not used further here.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "3D-LLM demonstrates value in injecting 3D features into LLMs, but the paper notes domain mismatch between its training data (objects / indoor scenes) and robotics manipulation data.",
            "transfer_failure_factors": "Domain mismatch: pretrained 3D-LLM datasets did not align with embodied robotics scenarios, motivating the authors to use BLIP2-FlanT5 XL and curate a robotics-focused dataset instead.",
            "key_findings": "Pretrained 3D-LM approaches exist, but direct reuse of their checkpoints may fail to transfer well to robotics manipulation tasks unless the pretraining domain matches the embodied task domain.",
            "uuid": "e1727.3",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Vision-Language-Action model)",
            "brief_description": "A referenced vision-language-action embodied foundation model that transfers web-scale (2D) vision-language knowledge to robotic control; cited as prior work in 2D-centered embodied VLA models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2",
            "model_agent_description": "A vision-language-action model that conditions action generation on 2D visual inputs and language; cited as an example of 2D VLA models (not used in experiments here).",
            "pretraining_data_type": "Web-scale vision-language corpora and image-language pretraining (as referenced in RT-2 original work).",
            "pretraining_data_details": "Not detailed in this paper (RT-2 is only cited in related work).",
            "embodied_task_name": "2D image-conditioned robot planning / action generation (cited; not run in this paper).",
            "embodied_task_description": "RT-2 generates plans or low-level actions conditioned on images; authors contrast it with their 3D-focused approach which incorporates 3D perception and world modeling.",
            "action_space_text": "High-level language-conditioned commands.",
            "action_space_embodied": "Low-level robot actions (RT-2 produces action outputs in original work; not detailed here).",
            "action_mapping_method": "Cited models learn direct perception-to-action mappings; authors argue this lacks world-model-style future-state imagination, motivating 3D-VLA.",
            "perception_requirements": "Primarily 2D image inputs in RT-2 (as cited).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "RT-2 demonstrates that web-scale vision-language pretraining can inform robot control when adapted to the domain (cited).",
            "transfer_failure_factors": "Paper argues RT-2's 2D focus misses essential 3D world dynamics and rich future-state imagination needed for some embodied tasks.",
            "key_findings": "RT-2 is an example of transferring vision-language pretraining to embodied control, but its 2D-centric approach motivates the need for 3D-aware world models like 3D-VLA.",
            "uuid": "e1727.4",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PALM-E",
            "name_full": "PaLM-E (Embodied multimodal language model)",
            "brief_description": "A referenced embodied multimodal LLM that produces plans or actions conditioned on images and language; cited as related work in vision-language-action models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E",
            "model_agent_description": "A multimodal LLM that combines large language model capabilities with vision inputs for embodied reasoning and action generation (cited for context).",
            "pretraining_data_type": "Large language model pretraining plus vision-language fine-tuning (as described in PaLM-E literature), referenced but not detailed in this paper.",
            "pretraining_data_details": "Not provided in this manuscript.",
            "embodied_task_name": "Vision-language-conditioned planning and action generation (cited work).",
            "embodied_task_description": "Generates high-level plans or low-level actions based on 2D visual inputs and language prompts (cited as prior art for VLA models).",
            "action_space_text": "Language instructions and high-level commands.",
            "action_space_embodied": "Low-level actions and plans (specifics not detailed in this paper).",
            "action_mapping_method": "Cited models typically learn direct mappings from vision+language to actions; 3D-VLA contrasts by adding generative world-model imagination.",
            "perception_requirements": "2D images and language (as noted in related work).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Demonstrates that large language models can be coupled with vision to inform embodied behavior (cited).",
            "transfer_failure_factors": "Not discussed in detail here; authors note these models generally lack explicit 3D world modeling.",
            "key_findings": "PALM-E is representative of multimodal LLMs used for embodied tasks, but 3D-VLA aims to extend beyond 2D perception to include generative 3D world models.",
            "uuid": "e1727.5",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Instruct-P2P",
            "name_full": "Instruct-P2P (image-editing diffusion guided by instructions)",
            "brief_description": "An instruction-conditioned image-editing diffusion model used as a baseline; the authors compare against both an off-the-shelf Instruct-P2P and a version trained on the paper's robotics dataset.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Instruct-P2P (and Instruct-P2P*)",
            "model_agent_description": "Image-editing diffusion model that follows textual editing instructions; Instruct-P2P* denotes the variant trained on the paper's curated robotics dataset.",
            "pretraining_data_type": "Image-editing instruction data (text + image pairs) in original Instruct-P2P work; Instruct-P2P* fine-tuned on robotics 3D-language data in this paper.",
            "pretraining_data_details": "Paper reports an Instruct-P2P* variant trained on the authors' robotics data (same curated dataset of 3D-language pairs), but does not list exact size for that fine-tuning subset.",
            "embodied_task_name": "RGB goal image generation for robotics manipulation tasks (Open-X test set).",
            "embodied_task_description": "Given an initial image and an edit instruction (e.g., 'open the drawer'), generate an edited image showing the goal state.",
            "action_space_text": "Natural language editing instructions.",
            "action_space_embodied": "Not directly actuator-level; used to imagine visual outcomes to inform planning.",
            "action_mapping_method": "Direct instruction-conditioned image editing (no explicit LLM alignment projector in baseline).",
            "perception_requirements": "RGB (and for this paper depth appended for RGB-D variants), object bounding boxes when included in prompt.",
            "transfer_successful": null,
            "performance_with_pretraining": "Instruct-P2P* (trained on the robotics dataset) reported PSNR 16.67, CLIP Sim 0.941, SSIM 0.628, FID 0.178 (Table 3); 3D-VLA outperforms or is comparable when LLM alignment and bounding-box inputs are used.",
            "performance_without_pretraining": "Off-the-shelf Instruct-P2P (not trained on robotics data) scored lower (Instruct-P2P PSNR 14.41, CLIP Sim 0.909, SSIM 0.389, FID 0.309).",
            "sample_complexity_with_pretraining": "Not reported (fine-tuning details for Instruct-P2P* not fully enumerated).",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Fine-tuning on robotics-specific data improves goal image generation quality; including predicted bounding boxes in prompts further helps.",
            "transfer_failure_factors": "Off-the-shelf image-editing models trained on internet data may produce unrealistic view changes or deformations when applied to robotics scenes without fine-tuning.",
            "key_findings": "Image-editing diffusion models can be adapted to robotics goal generation by fine-tuning on robotics 3D-language data, but aligning them with an LLM and using 3D spatial cues (bounding boxes) yields better performance.",
            "uuid": "e1727.6",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "NeXT-GPT",
            "name_full": "NeXT-GPT (Any-to-any multimodal LLM)",
            "brief_description": "A multimodal LLM with image generation capability used as a baseline for RGB goal generation; when applied zero-shot to robotics data its performance is poor.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "NeXT-GPT",
            "model_agent_description": "An 'any-to-any' multimodal LLM with image generation abilities; used as a baseline in image goal generation comparisons (zero-shot transfer).",
            "pretraining_data_type": "Multimodal (web-scale) image-text and other internet multimodal corpora (cited work), not detailed here.",
            "pretraining_data_details": "The paper treats NeXT-GPT as a zero-shot baseline and does not provide its pretraining statistics.",
            "embodied_task_name": "RGB goal image generation (zero-shot application to robotics episodes).",
            "embodied_task_description": "Attempt to generate goal images for robotics tasks from initial frames and instructions without robotics-specific fine-tuning.",
            "action_space_text": "Natural language instructions.",
            "action_space_embodied": "Not applicable — generation-only baseline.",
            "action_mapping_method": "Zero-shot text-to-image generation; no explicit mapping to robot actions.",
            "perception_requirements": "Input instruction and sometimes initial images; performance degraded on robotics scenes due to domain mismatch.",
            "transfer_successful": false,
            "performance_with_pretraining": "NeXT-GPT zero-shot reported lower metrics in Table 3 (PSNR 8.86, CLIP Sim 0.199, SSIM 0.153, FID 0.432) on robotics goal generation tasks.",
            "performance_without_pretraining": "N/A (this entry is the pretrained model applied zero-shot).",
            "sample_complexity_with_pretraining": "Not applicable / not reported.",
            "sample_complexity_without_pretraining": "Not applicable.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large-scale multimodal pretraining gives generative ability, but domain mismatch limits direct applicability.",
            "transfer_failure_factors": "Zero-shot transfer from web multimodal pretraining to robotics scenes fails due to viewpoint, object-state, and layout mismatches.",
            "key_findings": "Large multimodal LLMs with image generation capabilities do not reliably transfer to robotics goal generation zero-shot; robotics-specific fine-tuning and alignment are necessary for good results.",
            "uuid": "e1727.7",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Kosmos-2 / CoVLM",
            "name_full": "Kosmos-2 and CoVLM (grounding multimodal LLMs)",
            "brief_description": "2D grounding multimodal LLM baselines used for localization baselines; they are applied zero-shot to detect 2D boxes then projected to 3D for comparison.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Kosmos-2 / CoVLM",
            "model_agent_description": "2D grounding-capable multimodal LLMs used as baselines for localization by running 2D detection zero-shot and projecting to 3D; they are not adapted to 3D in this paper.",
            "pretraining_data_type": "Vision-language pretraining on 2D image-text corpora (as per cited works), not detailed here.",
            "pretraining_data_details": "Not specified in this paper; these models are used only as comparators in localization experiments.",
            "embodied_task_name": "Localization on held-in robotics datasets (projecting 2D detections to 3D bounding boxes).",
            "embodied_task_description": "Detect the object mentioned in the instruction and localize it; Kosmos-2 / CoVLM produce 2D boxes which are then projected into 3D for IoU/accuracy comparison.",
            "action_space_text": "N/A (these models are used for perception/localization).",
            "action_space_embodied": "N/A.",
            "action_mapping_method": "2D detection followed by projection into 3D via camera intrinsics/poses.",
            "perception_requirements": "2D image inputs (they require image + text to ground object mentions).",
            "transfer_successful": false,
            "performance_with_pretraining": "Reported localization IoU/Acc: Kosmos-2 (w/ GT Depth) IoU 10.92, Acc@25 12.73, Acc@50 3.85; CoVLM IoU 19.81, Acc@25 25.39, Acc@50 16.61; both are below 3D-VLA (IoU 29.33, Acc@25 42.26, Acc@50 27.09).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Not reported.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong 2D grounding capabilities, but require depth & projection to yield 3D localization.",
            "transfer_failure_factors": "Lack of native 3D reasoning and direct 3D training makes 2D-grounded models less accurate after projection compared to a model trained with 3D annotations.",
            "key_findings": "2D-grounded multimodal LLMs can be adapted to 3D localization by projecting 2D detections to 3D, but performance lags behind models trained with explicit 3D annotations and interaction tokens.",
            "uuid": "e1727.8",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ChatGPT (GPT-3.5-turbo-0125)",
            "name_full": "ChatGPT (GPT-3.5-turbo-0125)",
            "brief_description": "A large language model used here for prompt diversification and automatic generation of diverse language templates and annotations for the 3D embodied instruction dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "ChatGPT (GPT-3.5-turbo-0125)",
            "model_agent_description": "Instruction-following LLM used to rewrite templates, diversify prompts, and generate language inputs/outputs given the annotated objects and bounding boxes; used in dataset construction rather than directly producing embodied policies.",
            "pretraining_data_type": "Large-scale text corpora / instruction-tuned language model (chat LLM).",
            "pretraining_data_details": "The paper states ChatGPT (GPT-3.5-turbo-0125) was used for prompt diversification and to generate annotations; no dataset sizes provided here.",
            "embodied_task_name": "Dataset annotation for embodied tasks (3D-language-action pair generation).",
            "embodied_task_description": "Generates diverse natural-language prompts and answers that include tokens for images, point clouds, actions, and 3D annotations to augment the training dataset for 3D-VLA.",
            "action_space_text": "Language templates and instruction text produced by ChatGPT for dataset examples.",
            "action_space_embodied": "N/A (ChatGPT used for text only; downstream models consume this text for embodied training).",
            "action_mapping_method": "Generates textual descriptions and templates that include explicit action tokens and 3D annotation tokens used to supervise the LLM and downstream models.",
            "perception_requirements": "N/A for ChatGPT usage; it operates on textual and annotation inputs provided by the authors.",
            "transfer_successful": true,
            "performance_with_pretraining": "Not applicable as ChatGPT was used only to generate annotations; contributed to construction of the 2M 3D-language-action dataset that improved embodied-model training.",
            "performance_without_pretraining": "Not applicable.",
            "sample_complexity_with_pretraining": "Not reported.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": null,
            "transfer_success_factors": "High-quality, diversified language annotations aided model instruction tuning and improved the diversity and naturalness of prompts for embodied training.",
            "transfer_failure_factors": "Language-generation errors would propagate to dataset noise; authors mitigated this by providing few-shot human-written examples and structured annotation inputs.",
            "key_findings": "Using a pretrained conversational LLM to generate diverse language annotation for a large curated 3D-language-action dataset enabled large-scale instruction tuning necessary for 3D-VLA; ChatGPT served as a data-augmentation tool rather than an embodied controller.",
            "uuid": "e1727.9",
            "source_info": {
                "paper_title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Point‑E: A system for generating 3D point clouds from complex prompts",
            "rating": 2,
            "sanitized_title": "pointe_a_system_for_generating_3d_point_clouds_from_complex_prompts"
        },
        {
            "paper_title": "High-resolution image synthesis with latent diffusion models",
            "rating": 2,
            "sanitized_title": "highresolution_image_synthesis_with_latent_diffusion_models"
        },
        {
            "paper_title": "3D-LLM: Injecting the 3D world into large language models",
            "rating": 2,
            "sanitized_title": "3dllm_injecting_the_3d_world_into_large_language_models"
        },
        {
            "paper_title": "InstructPix2Pix (Instruct-P2P): Learning to follow image editing instructions",
            "rating": 2,
            "sanitized_title": "instructpix2pix_instructp2p_learning_to_follow_image_editing_instructions"
        },
        {
            "paper_title": "NeXT-GPT: Any-to-any multimodal LLM",
            "rating": 1,
            "sanitized_title": "nextgpt_anytoany_multimodal_llm"
        },
        {
            "paper_title": "Kosmos-2: Grounding multimodal large language models to the world",
            "rating": 1,
            "sanitized_title": "kosmos2_grounding_multimodal_large_language_models_to_the_world"
        }
    ],
    "cost": 0.0247265,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>3D-VLA: A 3D Vision-Language-Action Generative World Model
14 Mar 2024</p>
<p>Haoyu Zhen 
Xiaowen Qiu 
Peihao Chen 
Jincheng Yang 
Xin Yan 
Yilun Du 
Yining Hong 
Chuang Gan 
3D-VLA: A 3D Vision-Language-Action Generative World Model
14 Mar 20245533F55C4522A7EB9227EFA696F005D0arXiv:2403.09631v1[cs.CV]
Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world.Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics.In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly.To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model.Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment.Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds.To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets.Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.</p>
<p>Introduction</p>
<p>Nowadays, there has been a proliferation of vision-language models (Liu et al., 2023;Alayrac et al., 2022;Li et al., 2023b) that can take images as inputs and perform a series of reasoning tasks in the 2D space, mirroring the versatility of the human brain.Such 2D foundation models also lay the foundation for recent embodied foundation models such as RT-2 (Brohan et al., 2023) and PALM-E (Driess et al., 2023a) that could generate high-level plans or low-level actions contingent on the images.However, they neglect the fact that human beings are situated within a far richer 3D physical world beyond 2D images -they reason, plan, and act based on their 3D understanding of the environment (Palmer, 1975;Pylyshyn, 2003;Marr, 2010).It's crucial that human-like intelligent embodied agents are equipped with the same 3D understanding ability.</p>
<p>Taking a step forward, recent works (Huang et al., 2023b;Hong et al., 2024) develop embodied foundation models that could plan and act in the 3D environment.However, such models mainly learn a direct mapping from perception to action, devoid of a broader understanding of the dynamics of the world, and the relations between actions and world dynamics.On the other hand, human beings are blessed with world models that simulate future events based on 3D internal representations.By depicting the imagination and anticipation about the future states, one could better plan actions toward the predicted goals.</p>
<p>Challenges inevitably exist for building such human-like 3D world models.Firstly, existing foundation models focus on language generation, unable to imagine modalities beyond language and simulate future states to facilitate action generation, which is a crucial aspect of world models.Secondly, existing embodied datasets mainly contain 2D images or videos, lacking 3D-related annotations for reasoning and planning in the 3D space.</p>
<p>To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model.Specifically, we build our 3D-VLA on top of a 3D large language model (Hong et al., 2023) to equip the model with 3D understanding ability.Since embodied tasks could not be accomplished via language generation solely and require deeper digging into the dynamic scenes, the manipulated objects as well as actions to interact with the scenes, we add special interactive tokens to the LLM vocabulary (e.g., scene, object, and action tokens).These added tokens enable our model to perform a wider range of embodied tasks and support interleaved 3D-text data.</p>
<p>Recognizing the inadequacy of multimodal generation ability in embodied foundation models, we propose to inject the goal generation ability into 3D-VLA.We first pretrain a set of embodied diffusion models for RGBD-to-RGBD and point-to-point generation respectively.To efficiently bridge between the diffusion decoders of various modalities and the LLM embedding space, we employ a projector that aligns multi-modal goal generation in 3D-VLA.It strategically incorporates multimodal signals to specify the type of modality for a generation.</p>
<p>Another challenge for building such a generative world model lies in the lack of data.The embodied datasets in use (Padalkar et al., 2023;Brohan et al., 2022;Jang et al., 2022) mainly consist of 2D images, deficient in 3D-related information.Thus, we curate a large-scale 3D embodied instruction tuning dataset.Specifically, we first gather a diverse collection of datasets that includes real and synthetic data featuring robot manipulations and human-object inter-actions.For datasets lacking depth data, we utilize a depth estimator to append necessary 3D details and project them to 3D point clouds.Additionally, we design a pipeline to use the off-the-shelf models to extract 3D-related annotations and enrich the language descriptions.In this way, we collect 2M 3D-language-action data pairs, covering various tasks such as task captioning, action prediction, localization, multimodal goal generation, etc, as shown in Figure 1.</p>
<p>To sum up, we have the following contributions:</p>
<p>• We propose 3D-VLA, a new family of 3D vision-languageaction embodied foundation models that unify 3D perception, reasoning, and action with a generative world model.</p>
<p>• We create a large-scale 3D embodied instruction tuning dataset addressing the absence of 3D-related information in existing embodied datasets.</p>
<p>• We add interaction tokens to better interact with the environment.We further train diffusion models for goal image and point cloud generation.We utilize a projector to efficiently align LLM output features and diffusion models.• Our 3D-VLA can conduct a series of tasks, including goal generation (in terms of images, depths, and point clouds), goal-based planning, and embodiment action prediction.It outperforms baseline models by a large margin in these novel embodied tasks.It also outshines baseline models in traditional language-based tasks.</p>
<p>Related Works</p>
<p>Multimodal Language Models Recent Multimodal Language Models have made remarkable advances in various domains, including vision and language understanding (Li et al., 2022;2023b;Liu et al., 2023;Huang et al., 2023c;Peng et al., 2023;Zhu et al., 2023), interleaved image and text understanding (Alayrac et al., 2022), interleaved image and text generation (Dong et al., 2023).Some more unified models can perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio (Wu et al., 2023;Lu et al., 2023).However, none of these models can perceive 3D inputs or output actions according to 3D input.</p>
<p>Vision-Language-Action Models Previous vision-language models with action output have predominantly leveraged 2D features, thereby lacking the capability of 3D spatial understanding (Driess et al., 2023b;Brohan et al., 2022;2023).In contrast, our model is guided by 3D features, which are predicted in alignment with goal objectives in our general world model.We are the first to leverage 3D features such as point clouds for action token generation, significantly improving action planning accuracy.Additionally, this pipeline possesses the potential to be extended for applications in real-world scenarios.</p>
<p>3D Foundation Models Our paper is closely related to the 3D foundation models that integrate 3D features in MLLMs (Hong et al., 2023;Chen et al., 2023b;Qi et al., 2023;Xu et al., 2023;Huang et al., 2023a;Zhou et al., 2023;Guo et al., 2023;Li et al., 2024).These studies have successfully stepped forward to leverage foundation models to comprehend 3D features.However, they primarily focus on analyzing and reasoning in the current observable state of the 3D scenes, thereby revealing a limitation in predicting future features that extend beyond immediate perception.Contrasting with them, we aim to not only understand the perceivable scenes but also predict imperceptible multimodal features guided by specific goals.This capability enables our model to further generate action tokens to interact with the 3D world.</p>
<p>3D Embodied Instruction Tuning Dataset</p>
<p>Recently, benefiting from billion-scale datasets on the internet, VLMs have demonstrated exceptional proficiency in various tasks.Similarly, million-level datasets comprising video-action pairs lay the foundation for embodied VLMs for robot control.However, they mostly don't provide depth or 3D annotations and precise control in robot operations that necessitate the inclusion of 3D spatial reasoning and interaction.Without 3D information, it is challenging for a robot to comprehend and execute the commands that require 3D spatial reasoning, such as "place the farthest cup into the middle drawer".</p>
<p>To bridge this gap, we build a large-scale 3D embodied instruction tuning dataset that provides sufficient 3D-related information as well as paired text instructions to train our model.We design a pipeline to extract 3D-language-action pairs from existing embodied datasets, obtaining annotations for point clouds, depth maps, 3D bounding boxes, the robot's 7D actions, and textual descriptions.The details are outlined as follows.</p>
<p>Dataset Collection</p>
<p>Our data are curated from various sources.We provide an overview here, with details available in the Appendix:</p>
<p>Robot Datasets: We select 12 datasets (Brohan et al., 2022;Jang et al., 2022;Walke et al., 2023;Lynch et al., 2023;Feng et al., 2023;Chen et al., 2023a;Dass et al., 2023;Mandlekar et al., 2019;Mees et al., 2023;Shah et al., 2023;Sawhney et al., 2021;Sermanet et al., 2023) from the Open-X Embodiment Dataset (Padalkar et al., 2023).They have high-quality images with linguistic instructions in the real world but lack more in-depth information and 3D annotations.We also select datasets with excellent depth information, such as Dobb-E (Shafiullah et al., 2023) and RH20T (Fang et al., 2023).Additionally, we use datasets collected from two simulator environments, RLBench (James et al., 2020) and CALVIN (Mees et al., 2022).</p>
<p>Human Object Interaction Datasets: Human/hand-object interactions could provide demonstrations that benefit robot decision-making and imitation.Therefore, we utilize several human-object interaction datasets, including datasets without depth information, such as Epic-Kitchens (Damen et al., 2018), and datasets with better 3D annotations, such as HOI4D (Liu et al., 2022).</p>
<p>Visual Annotations</p>
<p>Estimating depths and optical flows.Given that over 95% of the video datasets for embodied tasks do not provide 3D information, we employ ZoeDepth (Bhat et al., 2023) on each frame of the video from these datasets.Additionally, to better utilize video data, we use RAFT (Teed &amp; Deng, 2020) for optical flow estimation.Optical flow aids in refining the data we generate.Thus, for video segments where the camera pose does not change, we use optical flow to estimate which pixels are the unmoved background.We align the depth maps of these backgrounds across different frames of the same video, multiplying each frame's depth map by a coefficient to ensure depth consistency.After getting the depth maps, we can directly lift the RGB-D images into 3D point clouds using camera intrinsics and poses.</p>
<p>Generating 3D annotations.We aim to generate several 3D-related annotations: 3D bounding boxes of the objects, goal images, depths, or point clouds as the imagination outcomes, as well as robot actions in the 3D space.We first extract the 3D bounding boxes of the objects in the scenes.Such information could benefit 3D models' ability to capture 3D information and attend to the manipulated object for better decision-making.The embodied datasets that serve as sources provide text instructions to describe the commands executed by the robots.We use spaCy (Honnibal &amp; Montani, 2017) to parse the instructions to obtain all noun chunks, including the manipulated object.We utilize a pre-trained grounding model (e.g., Grounded-SAM (Ren et al., 2024) ) to obtain the 2D mask of each object.These 2D masks, when lifted to 3D, correspond to parts of the point cloud, allowing us to obtain the 3D bounding boxes of all the objects in space.When selecting masks, the manipulated object is chosen based on the highest confidence value in areas of significant optical flow.Since we reconstruct the depths and point clouds, we could use images, depths, and point clouds in future frames as ground-truth goals.For actions, we use the 7 DoF actions from the provided datasets.</p>
<p>Language Annotations</p>
<p>Inspired by (Li et al., 2023a;Peng et al., 2023), we propose to generate dense language annotations consisting of tokens (e.g., <image></image>; <pcd></pcd>) that encompass the 3D annotations (bounding boxes, goal images / depths / point clouds, actions) we generated before, as shown in the prompts in Figure 2.</p>
<p>We use pre-defined language templates with tokens to construct these 3D annotations into prompts and answers.Following (Hong et al., 2023), we use ChatGPT-based prompting to diversify prompts.Specifically, we provide instructions to ChatGPT, as well as our annotated objects and bounding boxes.We also give 2-3 few-shot human-written demonstrations to guide the GPT on the type of data it is instructed to generate.ChatGPT is asked to summarize the information and rewrite the template-generated prompts into more diverse forms.For tasks without pre-defined templates, ChatGPT is also asked to generate prompts and answers as language inputs and outputs of these tasks by itself.We show the detailed templates and prompts to generate all types of data in the Appendix.</p>
<p>Methods</p>
<p>Overview</p>
<p>In this section, we introduce 3D-VLA, a world model for 3D reasoning, goal generation, and decision-making in embodied environments.As shown in Figure 2, we first build our backbone on top of 3D-LLM (Hong et al., 2023), and further enhance the model's capabilities to interact with the 3D world by adding a series of interaction tokens.Next, we inject goal generation ability into 3D-VLA by first pretraining the embodied diffusion models and employing a projector for aligning the LLM and the diffusion models.</p>
<p>3D-VLA</p>
<p>BACKBONE</p>
<p>In the first stage, we develop the 3D-VLA base model following the methodology of 3D-LLM (Hong et al., 2023).</p>
<p>Since the dataset we collected is not at the billion-level scale required for training a multi-modal LLM from scratch, we follow the approach of 3D-LLM by leveraging multi-view features to generate 3D scene features.This enables the seamless integration of visual features into a pre-trained VLM with no need for adaptation.Meanwhile, the training datasets for 3D-LLM mostly comprise objects (Deitke et al., 2022) and indoor scenes (Dai et al., 2017;Ramakrishnan et al., 2021), which do not directly align with our embodied setup.Therefore, we choose not to load the 3D-LLM pretrained model.Instead, we utilize BLIP2-FlanT5 XL (Li et al., 2023b) as our pretrained model.During training, we unfreeze both the input and output embeddings for tokens, as well as the weights of the Q-Former.</p>
<p>INTERACTION TOKENS</p>
<p>To enhance the model's comprehension of 3D scenes and facilitate interaction within these environments, we introduce a novel set of interaction tokens.Firstly, We incorporate object tokens <obj> </obj> that enclose the object nouns in the parsed sentences (e.g., <obj> a chocolate bar </obj> [loc tokens] on the table) so that the model could better capture which objects are manipulated or referred to.Secondly, to better represent spatial information by language, we devise a set of location tokens <loc0-255> for grounding referred objects, which are represented by six tokens for the 3D bounding box in the form of AABB.Thirdly, to better encode dynamics with our framework, we introduce the <scene> </scene> tokens to enclose the embeddings of a static scene.By composing over the scene tokens, 3D-VLA could comprehend dynamic scenes and manage inputs that interleave 3D scenes and text.</p>
<p>We further enhance the architecture with an expanded set of specialized tokens that represent robotic actions.The robot's actions, with 7 degrees of freedom, are represented by discrete tokens such as <aloc0-255>, <arot0-255>, and <gripper0/1> to denote the arm's intended absolute location, rotation, gripper openness.These actions are separated by token <ACT SEP>.</p>
<p>Injecting Goal Generation Ability into 3D-VLA</p>
<p>In this section, we introduce how our 3D-VLA performs goal generation in terms of images, depths, and point clouds.</p>
<p>Human beings pre-visualize the final states of the scenes to facilitate action prediction or decision making, which is a key aspect in building world models.Moreover, during preliminary experiments, we also discover that provid-ing the ground-truth final states can enhance the model's reasoning and planning capabilities.However, training an MLLM to generate images, depths, and point clouds is nontrivial.Firstly, state-of-the-art video diffusion models are not tailored for embodied setups.For instance, when asking Runway (Esser et al., 2023) to generate future frames given the instruction "open the drawer", the entire scene is altered to a great extent with regard to view change, unexpected object deformation, and weird texture replacement, as well as layout distortion.Similarly, using the method of DreamLLM (Dong et al., 2023) to directly freeze the stable diffusion trained on internet data, can lead to collapsed outputs.Secondly, how to incorporate diffusion models of various modalities into a single foundation model remains a challenge.Therefore, we propose to inject the ability to generate images, depths and point clouds into 3D-VLA.We first pretrain the embodied diffusion models in terms of different modalities such as images, depths and point clouds, and then align the decoders of these diffusion models to the embedding space of 3D-VLA through an alignment stage.</p>
<p>PRETRAINING EMBODIED DIFFUSION MODELS FOR GOAL GENERATION</p>
<p>To address the limitations of current diffusion models for goal generation in an embodied environment, we train RGB-D to RGB-D and point-cloud to point-cloud diffusion models.We utilize our curated 3D-language video data to train a conditional diffusion model that edits the initial state modality based on instructions to generate the corresponding final state modality.The specific training details for these models are as follows: For RGBD to RGBD generation, we employ Stable Diffusion V1.4 (Rombach et al., 2022) as our pretrained model due to the efficiency and quality of image generation by latent diffusion when operating in the latent space of a pretrained VAE (Kingma &amp; Welling, 2013).We concatenate the RGB latent and depth latent as the image condition.Similarly, for point-to-point generation, we use Point-E (Nichol et al., 2022) as the pretrained model, to which we add a point cloud condition input.</p>
<p>BRIDGING LLM AND GOAL GENERATION</p>
<p>After pretraining the diffusion models, we are equipped with various decoders that could generate goals by conditioning the latent spaces in their modalities.Challenges remain as to how to seamlessly incorporate the pretrained decoders into the LLMs so that 3D-VLA could generate goals with regard to any pretrained modalities conditioned on the input instructions.To bridge the gap between the LLM and the diffusion models of different modalities, we develop an alignment stage into our 3D-VLA.We first introduce additional special tokens such as <image> </image> and <pcd> </pcd>.These tokens are intricately designed to inform the decoder about the type of modal content to output.Between the enclosing tokens, we supervise the LLM in generating instructions for a robot to execute, which may include object tokens and location tokens, such as <image> pick up the <obj> apple </obj> [loc tokens] </image>.Based on this, we can apply a transformer-based projector, which is capable of mapping the decoder features and embeddings from the Large Language Model (LLM) into the space of the DM framework.It plays a crucial role in enhancing the model's capability to understand and generate multi-modal data, establishing a connection between high-level language understanding and multi-modal goal generation.To make training 3D-VLA more efficient and to avoid catastrophic forgetting, we utilize LoRA (Hu et al., 2021) to fine-tune different diffusion models.At the same time, we only train the newly introduced special tokens embeddings, the corresponding embedding output linear layer, and the entire projector.We minimize both the LLM and DM denoising loss.</p>
<p>Experiments</p>
<p>3D-VLA is a versatile 3D-based generative world model that can perform reasoning and grounding in the 3D world, imagine multi-modal goal content, and generate actions for robot manipulation.In this section, we evaluate 3D-VLA in three aspects: 3D reasoning and localization, multi-modal goal generation, and embodied action planning.</p>
<p>3D Reasoning and Localization</p>
<p>Tasks.Our primary focus is on scenes involving robots that are characterized by greater dynamism and a higher degree of interaction, which require a greater level of reasoning and localization abilities.We build several tasks on 3D embodied instruction tuning datasets for learning these abilities in the robotics domain.The tasks include 1) embodied QA on RoboVQA dataset (Sermanet et al., 2023); 2) task captioning on 11 Open-X datasets (Padalkar et al., 2023), where we input the initial and final scenes and ask the agent to reason what has happened; 3) what-if QA on RT-1 dataset (Brohan et al., 2022), where the agent is asked a question that what will happen if some specified actions (represented by action tokens) are executed; 4) dense captioning on 11 Open-X datasets, where the agent need to caption the content specified by a 3d bounding box; 5) localization on 11 Open-X datasets, where the agent is to localize the object mentioned in the robot manipulation instruction.We evaluate 3D-VLA on these tasks using held-in datasets.</p>
<p>Baselines.We compare 3D-VLA with 3D-LLM (Hong et al., 2023) and 2D vision-language models, including BLIP2 (Li et al., 2023b), OpenFlamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2023).We implement these baselines in two ways: 1) zero-shot transfer where we test the released trained model on these new tasks; 2) held-in evaluation where we train the released model on 2D-imageaction-language pairs (i.e., , 11 datasets selected from Open- X and RoboVQA dataset).For the localization task, we compare with 2D grounding MLLM, namely Kosmos-2 (Peng et al., 2023) and CoVLM (Li et al., 2023a).Specifically, we use these models to detect 2D bounding boxes in a zero-shot manner and then transfer them to 3D bounding boxes using projection.</p>
<p>Result analysis.In Tables 1, 3D-VLA outperforms all 2D VLM methods on language reasoning tasks.We attribute it to the leverage of 3D information, which provides more accurate spatial information for reasoning.Besides, since our dataset contains a bunch of 3D localization annotations, 3D-VLA learns to localize the relevant objects, which helps the model focus more on key objects for reasoning.Moreover, we find that 3D-LLM performs poorly on these robotic reasoning tasks, which demonstrates the necessity of collecting and training on a robotics-related 3D dataset.In Table 2, 3D-VLA demonstrates a marked superiority over the 2D baseline methods in terms of localization performance.This finding serves as compelling evidence of the efficacy of our annotation process, which supplies a substantial quantity of 3D annotations, thereby facilitating the acquisition of robust 3D localization capabilities within our model.</p>
<p>Multi-modal Goal Generation</p>
<p>Tasks.We quantitatively evaluate the RGB goal and point cloud goal generation capability of 3D-VLA on Open-X test sets.We randomly sample 4000 episodes from the Open-X test set which 3D-VLA does not see in the training process.</p>
<p>Baselines.For image generation, we compare 3D-VLA with three types of image generation methods: 1) imageediting methods Instruct-P2P (Brooks et al., 2023); 2) goal image/video generation methods SuSIE (Black et al., 2023); 3) LLMs with image generation ability NeXT-GPT (Wu et al., 2023).For point cloud generation, we compare with text-to-3D diffusion model Point-E (Nichol et al., 2022).</p>
<p>Qualitative results.The image goal generation results are shown in Table 3.When compared with the existing generation methods that directly zero-shot transfers to the robotics domain (rows 1, 2, 3 in Table 3), 3D-VLA achieves a promising performance in terms of most metrics.This underscores the importance of training a world model using datasets specifically designed for robotics applications.Even in a direct comparison with Instruct-P2P*, which was trained on the same robotics datasets we employed (row 4 in the table), 3D-VLA consistently outperforms it.This highlights that the integration of a large language model into 3D-VLA results in a more comprehensive and insightful comprehension of robotics manipulation instructions, leading to better goal image generation performance.Furthermore, when we exclude the predicted bounding box from the input prompt (row 5), we observe a slight decrease in performance.This observation confirms the effectiveness of using these intermediate predicted bounding boxes as they assist the model in comprehending the overall scene, allowing the model to allocate more attention to the specific object mentioned in the given instruction, ultimately enhancing its ability to imagine the final goal images.</p>
<p>The point cloud generation results are presented in Table 4. 3D-VLA with intermediate predicted bounding boxes performs the best.This outcome reinforces the significance of incorporating large language models and precise object localization in the context of comprehending both the instruction and the scene.</p>
<p>Quantitative results.In the first row of Figure 3, we visualize the generated RGB-D goal images on the test set of RT-1 (Brohan et al., 2022) and Jaco Play (Dass et al., 2023) datasets.These samples are not seen in the training process.Given the initial scenes and instructions, the 3D-VLA model consistently exhibits the capability to maintain the background elements unchanged while accurately identifying the target object of interaction and correctly modifying the states of these identified objects following the provided instructions.The generated RGB-D goal images closely align both in terms of visual appearance and semantic content with the ground truth goal.In addition to our controlled experimental settings, we extended our testing to encompass scenes captured from the internet or everyday life.In these diverse and uncontrolled environments, our 3D-VLA model consistently and robustly demonstrated its efficacy.</p>
<p>Embodied Action Planning</p>
<p>Tasks We evaluate the ability of 3D-VLA for robot arm action prediction on two benchmarks, namely RL-Bench (James et al., 2020) and CALVIN (Mees et al., 2022).We select three tasks from RLBench for evaluation.Besides, we also select var1 from the pick-up-cup task as an unseen task to test the model's generalization ability.For CALVIN,  we evaluate our model under the long-horizon multi-task language control setting, where the agent is required to execute 5 tasks sequentially.We train the agent on scenes A, B, C, D and test on scene D.</p>
<p>Baselines.For RLBench, we compare our model 3D-VLA with LanCon-Learn (Silva et al., 2021), which is a multi-task approach that can predict actions based on instruction-conditioned inputs.For CALVIN, we compare with MCIL (Lynch &amp; Sermanet, 2020), which is a conditional sequence-to-sequence variational autoencoder.matches the baseline performance in most tasks within the RLBench action prediction, showing its planning capability.It's worth noting that the baseline uses history observations, object states, and current state information, whereas we only execute via open-loop control.Additionally, our generalization capability is proven in the pick-up-cup task.In Table 6, 3D-VLA also achieves promising results in CALVIN.We attribute the superiority to the ability to localize the objects of interest and imagine the goal state, which provides rich information for inferring actions.</p>
<p>Result analysis. As shown in</p>
<p>Conclusion</p>
<p>In this paper, we introduce 3D-VLA, a generative world model that can reason, understand, generate, and plan in the embodied environment.We devise a novel data generation pipeline to construct a dataset including 2M 3D-Languageaction data pairs to train our model.These data enable it to perform diverse tasks such as task caption, localization, goal image/point cloud generation, action prediction, etc.Our model uses 3D-LLM as the backbone and introduces interaction tokens to interact with the environment.We train a image to image and point to point diffusion model for embodied AI.They are further aligned by a projector with the LLM to enhance the LLM's multimodal generation capabilities.The experiment further shows that our 3D-VLA has stronger capabilities in embodied tasks than the 2D baseline.</p>
<p>Impact Statement</p>
<p>This paper introduces research aimed at pushing the boundaries of Machine Learning in the realm of robot manipulation.Given that robots operate in the physical world, the potential for collisions with objects and humans arises when the robot system is not adequately configured.To mitigate this issue, our approach involves initial training in a simulator environment followed by real-world deployment under human supervision, to minimize any adverse impacts.8. Datasets used in our paper.We categorize them into four categories: Robotics, HOI, and Room datasets.</p>
<p>Figure 1 .
1
Figure 1.Examples from our 3D Embodied Instruction Tuning Dataset.</p>
<p>Figure 2 .
2
Figure 2. Overview of our 3D-VLA pipeline.The left part shows our goal-generation capability.Our model can imagine the final state image and point cloud based on the user's input.This generated goal state can then be fed back to our model to guide the robot control.</p>
<p>Table 1 .
1
Evaluation on reasoning ability using held-in data.<em> denotes zero-shot transfer results without training on our pre-train datasets.
TasksModelsBLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGH-L EM@13D-LLM  </em>1.050.380.150.0212.960.910.00BLIP2 OPT 2.7B<em>7.393.170.030.023.877.403.03Embodied QABLIP2 FlanT5 XL OpenFlamingo 4B  *   </em>22.84 9.5016.17 6.5112.50 5.1410.11 4.2911.41 6.8432.01 10.4010.31 1.21LLaVA 7B<em>11.668.066.014.5812.5914.175.67BLIP2 FlanT5 XL37.3127.2020.3215.4817.8038.9215.353D-VLA48.3438.5531.7226.8023.7249.3324.533D-LLM  </em>0.780.160.070.050.571.330.00BLIP2 FlanT5 XL<em>8.502.070.350.003.408.450.00Task CaptionOpenFlamingo 4B LLaVA 7B  </em><em>7.61 2.631.64 0.690.37 0.160.00 0.004.74 2.639.36 4.650.00 0.00BLIP2 FlanT5 XL22.0511.405.723.168.7226.127.753D-VLA55.6945.8839.3934.8827.5762.0129.34What-if QABLIP2 FlanT5 XL 3D-VLA28.23 53.0911.47 40.944.49 34.340.06 29.388.27 26.8328.41 52.825.85 14.73D-LLM  </em>0.520.220.160.130.340.640.00Dense CaptionBLIP2 FlanT5 XL36.1724.7218.0613.9617.8340.5613.103D-VLA51.9042.8338.1134.6225.2555.9139.49</p>
<p>Table 2 .
2
Localization results on held-in robotics datasets.
MethodsIoUAcc@25 Acc@50Kosmos-2 (w/ GT Depth) 10.9212.733.85CoVLM (w/ GT Depth)19.8125.3916.613D-VLA29.3342.2627.09</p>
<p>Table 3 .
3
RGB image goal generation results.<em> denotes the model is trained on our pretrained dataset.
MethodPSNR ↑ CLIP Sim ↑ SSIM ↑ FID ↓Instruct-P2P14.410.9090.3890.309SuSIE15.200.8980.5490.182NeXT-GPT8.860.1990.1530.432Instruct-P2P  </em>16.670.9410.6280.1783D-VLA w/o Pred BBox17.020.9190.6320.1733D-VLA17.210.9200.6360.177ModelsP-FID ↓ Chamfer-L 1 ↓Point-E  *5.2410.1593D-VLA w/o Pred BBox4.9140.1433D-VLA4.7960.139</p>
<p>Table 4 .
4
Point Cloud goal generation results.* denotes the model is trained on our pretrained dataset.</p>
<p>Table 5 .
5
Evaluation of action planning on RLBench dataset.</p>
<p>Table 5 ,
5
3D-VLA surpasses or
Tasks completed in a row12345MCIL28.2 2.50.3 003D-VLA 44.7 16.3 8.1 1.6 0</p>
<p>Table 6 .
6
Evaluation of action planning on CALVIN dataset.</p>
<p>University of Massachusetts Amherst
Shanghai Jiao Tong University
South China University of Technology
Wuhan Uni-
versity 5 Massachusetts Institute of
Technology 6 University of California
, Los Angeles 7 MIT-IBM Watson AI Lab.
3D-VLA: A 3D Vision-Language-Action Generative World Model
A. Model Implementation DetailsWe use pretrained BLIP-2 FlanT5 as backbone.In the pretrain stage, we train 3D-VLAs for 30 epochs on 6 × 32 V100s, and validate every epoch.The batch size is set to 4 on each node during training.Additionally, we apply a linear warmup of the learning rate during the initial 1K steps, increasing from 10 −8 to 10 −5 , followed by a cosine decay with a minimum learning rate of 10 −6 .In the alignment stage, we train 3D-VLAs for a maximum of epochs of 20 on 6 × 64 V100s.The batch size is set to 2 on each node for training.The AdamW optimizer is used, with beta 1 = 0.9, beta 2 = 0.999, and a weight decay of 0.05.We use Distributed Data Parallel to train our models.B. Datasets DetailsB.1. Details on Question TemplatesIn this section, we show the question templates for data generation in Table7.We designed corresponding templates six tasks.We design the templates for six tasks, and we replace the INSTRUCTION, OBJECT, LOCATION, and ACTION in each template with the information processed from each sample.B.2. Details on ChatGPT-based PromptingIn this section, we show the prompt used in ChatGPT-based data generation in Figure4.The ChatGPT version used in our paper is GPT-3.5-turbo-0125.We generate data for all seven tasks, and we provide all the information in the form of text, such as the instructions performed by the robot, total execution time, objects and their locations in the scene, etc.Additionally, for each prompt, we provide two manually written samples as guidance to direct ChatGPT towards more natural data generation.B.3. Details on Dataset ConstructionWe show the number of the episodes and how we use them in tableTable 8.We utilize two main categories of datasets, namely robotics datasets and human object interaction (HOI) datasets.For the former, we filtered out complex scene datasets to prevent the Grounded-SAM from detecting incorrect object locations.However, within the same robotics dataset, the background settings are largely the same.Therefore, in the Goal Generation tasks, we included HOI datasets to better allow the diffusion model to learn diverse scenes, object interaction methods, etc.C. More Visualization Results about Goal GenerationWe show more qualitative examples in Figure5, 6. messages=[{"role": "system" , "content": " You are an AI visual assistant and a question-answering generator capable of analyzing dynamic 3D scenes.Suppose you have observed a robotic arm successfully executing an instruction:[instruction].The scene's initial state is <initial scene> and <final scene>, where the final scene is the [num frame] frame, and we assume that the task was definitely not completed in the first 2/3 of the time.You have the action sequence <action> of the robot arm.In this instruction, the initial positions of these objects are[object + location].Note that the location is the center points of objects represented by a 3D coordinate (x, y, z) with units of meters.Utilizing all the information above, you can choose to rewrite the instruction while retaining its original meaning.Further, you need to generate multiple rounds of dialogue or a question answer pair, which should correspond to one of the following tasks:1. Verification: Given the initial state and a mid-state frame, ask if the robot has completed the instruction.2. Task Caption: Given the initial and final states, ask what task the robot performed.3. Embodied QA: Please conduct some questions and answers about the current dynamic scene.4. Localization: Detect where objects are, answer the location of the objects.5. Dense Caption: Given the location of objects, answer with a description of those objects.6. Image or Point Cloud Generation: Given the initial scene and instruction, generate an image or point cloud of the final state.If choosing this task, enclose the instruction with the <image> </image> or <pcd> </pcd> token to represent generation.7. Action Prediction: Given the initial scene, or having both initial and final scenes, predict actions.You can include a simple task decomposition, but the length of the decomposition must not exceed 3.
Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>S F Bhat, R Birkl, D Wofk, P Wonka, M Müller, Zoedepth, arXiv:2302.12288Zero-shot transfer by combining relative and metric depth. 2023arXiv preprint</p>
<p>Zero-shot robotic manipulation with pretrained image-editing diffusion models. K Black, M Nakamoto, P Atreya, H Walke, C Finn, A Kumar, S Levine, 2023</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>T Brooks, A Holynski, A A Efros, Instructpix2pix, Learning to follow image editing instructions. 2023</p>
<p>Playfusion: Skill acquisition via diffusion from language-annotated play. L Chen, S Bahl, D Pathak, Conference on Robot Learning. PMLR2023a</p>
<p>S Chen, X Chen, C Zhang, M Li, G Yu, H Fei, H Zhu, J Fan, T Chen, Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning. 2023b</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nießner, 2017</p>
<p>Scaling egocentric vision: The epic-kitchens dataset. D Damen, H Doughty, G M Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Clvr jaco play dataset. S Dass, J Yapeter, J Zhang, J Zhang, K Pertsch, S Nikolaidis, J J Lim, 2023</p>
<p>M Deitke, D Schwenk, J Salvador, L Weihs, O Michel, E Vanderbilt, L Schmidt, K Ehsani, A Kembhavi, A Farhadi, Objaverse, A universe of annotated 3d objects. 2022</p>
<p>R Dong, C Han, Y Peng, Z Qi, Z Ge, J Yang, L Zhao, J Sun, H Zhou, H Wei, X Kong, X Zhang, K Ma, L Yi, Dreamllm, arXiv:2309.11499Synergistic multimodal comprehension and creation. 2023arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023aarXiv preprint</p>
<p>. D Driess, F Xia, M S M Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, Y Chebotar, P Sermanet, D Duckworth, S Levine, V Vanhoucke, K Hausman, M Toussaint, K Greff, A Zeng, I Mordatch, P Florence, 2023bPalm-e: An embodied multimodal language model</p>
<p>Structure and content-guided video synthesis with diffusion models. P Esser, J Chiu, P Atighehchian, J Granskog, A Germanidis, 2023</p>
<p>Rh20t: A robotic dataset for learning diverse skills in one-shot. H.-S Fang, H Fang, Z Tang, J Liu, J Wang, H Zhu, C Lu, arXiv:2307.005952023arXiv preprint</p>
<p>Y Feng, N Hansen, Z Xiong, C Rajagopalan, X Wang, arXiv:2310.16029Finetuning offline world models in the real world. 2023arXiv preprint</p>
<p>Point-bind &amp; point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. Z Guo, R Zhang, X Zhu, Y Tang, X Ma, J Han, K Chen, P Gao, X Li, H Li, P.-A Heng, 2023</p>
<p>Y Hong, H Zhen, P Chen, S Zheng, Y Du, Z Chen, C Gan, arXiv:2307.129813d-llm: Injecting the 3d world into large language models. 2023arXiv preprint</p>
<p>Y Hong, Z Zheng, P Chen, Y Wang, J Li, C Gan, Multiply, arXiv:2401.08577A multisensory object-centric embodied large language model in 3d world. 2024arXiv preprint</p>
<p>spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. M Honnibal, I Montani, 2017To appear</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>H Huang, Z Wang, R Huang, L Liu, X Cheng, Y Zhao, T Jin, Zhao, Chat-3d v2: Bridging 3d scene and large language models with object identifiers. 2023a</p>
<p>J Huang, S Yong, X Ma, X Linghu, P Li, Y Wang, Q Li, S.-C Zhu, B Jia, S Huang, arXiv:2311.12871An embodied generalist agent in 3d world. 2023barXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. S Huang, L Dong, W Wang, Y Hao, S Singhal, S Ma, T Lv, L Cui, O K Mohammed, Q Liu, arXiv:2302.140452023carXiv preprint</p>
<p>The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, Rlbench, IEEE Robotics and Automation Letters. 522020</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, arXiv:1312.6114PMLR, 2022. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. 2013arXiv preprintConference on Robot Learning</p>
<p>Bootstrapping language-image pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, Blip, International Conference on Machine Learning. PMLR2022</p>
<p>J Li, D Chen, Y Hong, Z Chen, P Chen, Y Shen, C Gan, Covlm, arXiv:2311.03354Composing visual entities and relationships in large language models via communicative decoding. 2023aarXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.125972023barXiv preprint</p>
<p>3dmit: 3d multi-modal instruction tuning for scene understanding. Z Li, C Zhang, X Wang, R Ren, Y Xu, R Ma, X Liu, 2024</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Hoi4d: A 4d egocentric dataset for category-level human-object interaction. Y Liu, Y Liu, C Jiang, K Lyu, W Wan, H Shen, B Liang, Z Fu, H Wang, L Yi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>A unified model for vision, language, and multi-modal tasks. J Lu, C Clark, R Zellers, R Mottaghi, A Kembhavi, Unified-Io, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Language conditioned imitation learning over unstructured data. C Lynch, P Sermanet, arXiv:2005.076482020arXiv preprint</p>
<p>Interactive language: Talking to robots in real time. C Lynch, A Wahid, J Tompson, T Ding, J Betker, R Baruch, T Armstrong, P Florence, IEEE Robotics and Automation Letters. 2023</p>
<p>Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity. A Mandlekar, J Booher, M Spero, A Tung, A Gupta, Y Zhu, A Garg, S Savarese, L Fei-Fei, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. D Marr, 10.7551/mitpress/9780262514620.001.000107 2010The MIT Press9780262514620</p>
<p>A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, Calvin, IEEE Robotics and Automation Letters (RA-L). 732022</p>
<p>Grounding language with visual affordances over unstructured data. O Mees, J Borja-Diaz, W Burgard, Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). the IEEE International Conference on Robotics and Automation (ICRA)London, UK2023</p>
<p>A Nichol, H Jun, P Dhariwal, P Mishkin, M Chen, arXiv:2212.08751A system for generating 3d point clouds from complex prompts. 2022arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. A Padalkar, A Pooley, A Jain, A Bewley, A Herzog, A Irpan, A Khazatsky, A Rai, A Singh, A Brohan, arXiv:2310.088642023arXiv preprint</p>
<p>The of contextual scenes on the identification of objects. S Palmer, Memory &amp; Cognition. 31975</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Z Peng, W Wang, L Dong, Y Hao, S Huang, S Ma, F Wei, arXiv:2306.148242023arXiv preprint</p>
<p>Seeing and Visualizing: It's Not What You Think. 01. Z Pylyshyn, 10.7551/mitpress/6137.001.000120039780262316316</p>
<p>Z Qi, Y Fang, Z Sun, X Wu, T Wu, J Wang, D Lin, H Zhao, Gpt4point, A unified framework for point-language understanding and generation. 2023</p>
<p>S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, M Savva, Y Zhao, D Batra, Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. 2021</p>
<p>Grounded sam: Assembling open-world models for diverse visual tasks. T Ren, S Liu, A Zeng, J Lin, K Li, H Cao, J Chen, X Huang, Y Chen, F Yan, Z Zeng, H Zhang, F Li, J Yang, H Li, Q Jiang, L Zhang, 2024</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Playing with food: Learning food item representations through interactive exploration. A Sawhney, S Lee, K Zhang, M Veloso, O Kroemer, Experimental Robotics: The 17th International Symposium. Springer2021</p>
<p>P Sermanet, T Ding, J Zhao, F Xia, D Dwibedi, K Gopalakrishnan, C Chan, G Dulac-Arnold, S Maddineni, N J Joshi, P Florence, W Han, R Baruch, Y Lu, S Mirchandani, P Xu, P Sanketi, K Hausman, I Shafran, B Ichter, Y Cao, Robovqa, arXivpreprintarXiv:2311.00899Multimodal long-horizon reasoning for robotics. 2023</p>
<p>N M M Shafiullah, A Rai, H Etukuru, Y Liu, I Misra, S Chintala, L Pinto, arXiv:2311.16098On bringing robots home. 2023arXiv preprint</p>
<p>Learning unified policies from multimodal task specifications. R Shah, R Martín-Martín, Y Zhu, Mutex, 7th Annual Conference on Robot Learning. 2023</p>
<p>Lancon-learn: Learning with language to enable generalization in multi-task manipulation. A Silva, N Moorman, W Silva, Z Zaidi, N Gopalan, M Gombolay, IEEE Robotics and Automation Letters. 722021</p>
<p>Raft: Recurrent all-pairs field transforms for optical flow. Z Teed, J Deng, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part II 16</p>
<p>Bridgedata v2: A dataset for robot learning at scale. H R Walke, K Black, T Z Zhao, Q Vuong, C Zheng, P Hansen-Estruch, A W He, V Myers, M J Kim, M Du, Conference on Robot Learning. PMLR2023</p>
<p>Nextgpt: Any-to-any multimodal llm. S Wu, H Fei, L Qu, W Ji, T.-S Chua, arXiv:2309.055192023arXiv preprint</p>
<p>Pointllm: Empowering large language models to understand point clouds. R Xu, X Wang, T Wang, Y Chen, J Pang, D Lin, 2023</p>
<p>Uni3d: Exploring unified 3d representation at scale. J Zhou, J Wang, B Ma, Y.-S Liu, T Huang, X Wang, 2023</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
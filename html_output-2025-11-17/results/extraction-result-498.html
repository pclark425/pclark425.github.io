<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-498 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-498</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-498</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-263608443</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.01831v2.pdf" target="_blank">Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?</a></p>
                <p><strong>Paper Abstract:</strong> Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a programs intent. However, there is typically no guarantee that a programs implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically. The emergent abilities of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice. In this paper, we describe nl2postcond, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions. We introduce and validate metrics to measure and compare different nl2postcond approaches, using the correctness and discriminative power of generated postconditions. We then use qualitative and quantitative methods to assess the quality of nl2postcond postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that nl2postcond via LLMs has the potential to be helpful in practice; nl2postcond generated postconditions were able to catch 64 real-world historical bugs from Defects4J.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e498.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e498.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ambiguous NL specification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguity in natural-language function specifications (docstrings/Javadoc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Informal natural-language descriptions often permit multiple interpretations of intended behavior (e.g., whether to remove all duplicates vs keep one copy), causing mismatches between described intent and implemented code; the paper demonstrates this with HumanEval examples and uses LLM-generated postconditions to disambiguate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>nl2postcond LLM-based postcondition generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline that prompts chat LLMs (GPT-4, GPT-3.5, StarChat) with a natural-language method description (docstring/Javadoc) and optional reference code to produce executable postconditions which are then evaluated against a reference implementation and test suite.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>function docstring / Javadoc (informal natural-language specification)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>function implementation (Python/Java reference method)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language method descriptions can be semantically ambiguous about key behavioral choices (e.g., whether to drop all duplicates or keep one), so the same docstring may map to multiple valid but distinct formal behaviors; this ambiguity leads to potential misalignment between the written intent and what code actually implements.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>specification layer (documentation / docstring) versus implementation behavior</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>LLM-based generation of candidate postconditions followed by evaluation against the reference implementation and tests; manual qualitative inspection of examples (HumanEval remove_duplicates case study).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative example demonstration plus quantitative metrics: test-set-correctness (accept@k) to check if a generated postcondition is consistent with reference code on test suite; bug-completeness to measure discriminative power among mutants (how many alternative implementations are distinguished).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Ambiguity can cause multiple plausible postconditions to be generated (both may pass tests); formalizing intent with generated postconditions resolves ambiguity and prevents incorrect assumptions—demonstrated by generated postconditions that disambiguate and match developer intent in examples. (No single numeric 'loss' reported for ambiguity itself, but concrete detection improved ability to flag mismatches.)</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High prevalence of natural-language artifacts in codebases supports broad relevance: paper cites prior work finding natural-language documentation in 98% of GitHub repos and comments present in >20% of code lines, indicating ambiguity is widespread.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Informal, underspecified natural language lacking precise constraints and implicit assumptions about intended behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Generate formal, executable postconditions from the natural-language description using LLMs (nl2postcond); present candidate postconditions for developer review to disambiguate intent; prefer simpler prompt variants to increase correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>In examples the approach produced unambiguous assertions that matched developer intent; empirically, GPT-4 produced test-set-correct postconditions for up to 96% of EvalPlus problems and simple prompts increased correctness (statistically significant, p=0.008, Cohen's d = 1.73).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / program specification and verification</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e498.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e498.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Doc-code misalignment (Defects4J)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrepancies between Javadoc (natural-language intent) and implementation causing real-world bugs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instances where the implementation violates documented behavior (Javadoc), producing functional bugs; the paper demonstrates that LLM-generated postconditions can formalize the Javadoc and discriminate buggy from fixed implementations (Defects4J examples such as Commons CLI and Commons Math).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>nl2postcond instrumentation and evaluation on Defects4J</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instrument buggy and fixed Java methods with LLM-generated postconditions derived from in-file natural language (Javadoc) plus file-level context, then run regression and trigger tests to identify discriminating postconditions that fail on buggy and pass on fixed versions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Javadoc / in-file comments</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java library code (Defects4J buggy and fixed method versions)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation violates documented behavior (incomplete/incorrect implementation relative to natural-language spec)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The actual code sometimes fails to satisfy documented constraints (for example, lines exceeding specified width or insufficient numeric precision in reversed Line objects), creating misalignment between the stated intent in Javadoc and the implemented behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>implementation logic (algorithmic behavior / numerical precision / formatting behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Generate postconditions from Javadoc using LLMs (with/without code context), instrument both buggy and fixed versions, and run existing regression and trigger tests; a postcondition is bug-discriminating if it passes on fixed version and fails on buggy version.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Count of bug-discriminating postconditions and number of distinct bugs found; test-set-correctness (accept@k) on fixed versions to ensure postconditions reflect intended behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>LLM-generated postconditions discriminated 70 buggy methods across 64 unique Defects4J bugs (12.2% of the 525 bugs studied); GPT-4 generated discriminating postconditions for up to 47/525 (9%) bugs in some prompt variants. This demonstrates concrete ability to surface doc-code misalignments and catch historical real-world bugs.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>In the Defects4J subset studied (525 bugs), nl2postcond found discriminating postconditions for 64 unique bugs (12.2%); prevalence depends on clarity of Javadoc and project context.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implementation errors or omissions relative to implied constraints in natural-language documentation; documentation may state constraints that code fails to implement (or tests do not cover).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Translate natural-language documentation into executable postconditions and check them at test time or runtime to detect violations; include file-level context in prompts for complex real-world functions to improve generated assertions and compilability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Practical effect demonstrated: 64 real historical bugs were discriminated via nl2postcond. Including code/context in prompts improved compilability and increased the rate of test-set-correct postconditions in Defects4J experiments (qualitative and tabulated improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / empirical software testing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e498.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e498.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-suite insufficiency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Insufficient test suites causing false positives in test-set-correctness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Because test-set-correctness uses finite test suites as a stand-in for universal correctness, incomplete tests can cause vacuous or overfitting postconditions to appear correct; the paper quantifies false positives when using smaller test suites (HumanEval) versus larger ones (EvalPlus).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>nl2postcond evaluation harness (test-set-correctness metric)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation framework that treats a candidate postcondition as correct if it holds across a given set of test inputs T; used to compute accept@k and to evaluate completeness against code mutants.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>natural-language docstring (EvalPlus/HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reference implementation used to produce expected outputs for tests</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete evaluation oracle (test coverage insufficiency) leading to measurement error</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Finite test suites can fail to exercise corner-case inputs; weak tests allow postconditions that overfit observed tests (or are vacuously true) to be labeled as correct even though they do not hold universally.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation / validation stage (test suites used for metric computation)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual annotation and comparison of postcondition behavior across different test suites: EvalPlus (extensive tests) vs HumanEval (fewer tests); manual review identified false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported false-positive rates: only 1.1% of 900 manually annotated postconditions were affected using EvalPlus; using the smaller HumanEval suite resulted in ~7% false positives for GPT-4 accept@10.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Overestimation of correctness when using small/incomplete test suites — smaller suites (HumanEval) produced substantially higher false-positive rates compared to EvalPlus, which demonstrates that evaluation metrics can be inflated by insufficient tests.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Depends on test-suite size; in this study false positives were 1.1% with EvalPlus and ~7% with HumanEval for vulnerable measurements reported.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Limited test coverage and reliance on finite test sets as proxies for universal correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use more extensive test suites (as in EvalPlus), manual annotation/validation for edge cases, and complement test-based correctness with other analyses (e.g., manual review or formal verification when feasible).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Using EvalPlus (avg. 775 tests per problem) substantially reduced false positives (1.1% vs ~7%); the paper validates test-set-correctness as reasonable on EvalPlus.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software testing / empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e498.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e498.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mutation vs natural-bug mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between traditional mutation operators and real-world bug distributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard mutation operators (small syntactic edits) do not adequately reflect the diversity of realistic bugs; the paper proposes using LLMs to sample 'natural' code mutants and shows different completeness behavior compared to artificial mutants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>bug-completeness evaluation using LLM-sampled mutants</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Completeness metric that measures fraction of LLM-sampled distinct buggy implementations (CM) that a postcondition discriminates; mutants are sampled via LLMs to better approximate natural bug distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>problem description (natural language intent nl)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>LLM-generated alternative implementations (mutants) in Python for EvalPlus</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>evaluation gap: synthetic mutation operators poorly approximate real bugs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Traditional mutant operators change small tokens/ops and miss syntactically diverse or semantically realistic bugs (e.g., alternate algorithmic constructs); this misalignment can distort metrics of postcondition completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>metric construction / mutant generation phase</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison of bug-completeness scores computed with (a) natural LLM-sampled mutants and (b) artificially seeded mutants; observation that natural mutants are harder to kill.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Bug-completeness-score (fraction of mutants killed); comparison of scores when using natural-only vs augmented (natural + explicit artificial) mutants. Reported that completeness was consistently lower when only considering natural bugs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using artificial/seeding mutations alone may artificially inflate completeness metrics; relying solely on classical mutation operators risks overestimating specification discriminative power.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>In EvalPlus experiments, number of unique LLM-sampled buggy codes per problem varied from 4 to 233 (median 55); natural mutants alone provided a meaningful but sometimes smaller set than when augmented with seeded bugs.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mismatch between simple mutation operators and the real distribution of programmer errors and algorithmic variants.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use LLMs to sample diverse, natural-looking mutant implementations and retain only pairwise-distinct mutants w.r.t. test behavior; augment with seeded artificial bugs when necessary to boost coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>LLM-sampled mutants created a diverse set with median 55 unique buggy codes per problem; augmented mutants increased the mutant pool and allowed more robust estimation of bug-completeness (no single numeric 'correction' value provided).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>mutation testing / software fault-injection</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e498.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e498.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weak postcondition types (Type Checks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prevalence of weak type-check postconditions that have low bug-discriminative power</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs often output simple type-check assertions (e.g., isinstance(return_val,int)), which are common but weak: they are prevalent in generated postconditions yet kill far fewer real/natural mutants than richer semantic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>postcondition taxonomy and completeness measurement</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Qualitative coding of atomic postcondition categories (Type Check, Format Check, Arithmetic Bounds, Arithmetic Equality, Container Property, Element Property, Forall-Element Property, Implication, Null Check) and measurement of per-category bug-completeness on EvalPlus.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>docstring-based intent</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python reference implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>superficial/incomplete specification (too weak to capture bugs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Type-check postconditions are easy for LLMs to generate and frequent (47.4% prevalence) but they detect relatively few natural bugs (avg. bug-completeness 0.14 for natural mutants), indicating mismatch between generated checks and actual bug causes.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>postcondition expressiveness / specification strength</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual qualitative coding of 900 generated postconditions into atomic categories and computation of average bug-completeness per category.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Category prevalence and average bug-completeness-score per category (natural/all mutants). For Type Checks: prevalence 47.4% and avg. bug-completeness 0.14 (natural) / 0.27 (all). For Arithmetic Equality: prevalence 17.5% and avg. bug-completeness 0.82 (natural) / 0.89 (all).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevalence of weak type checks lowers overall discriminative power of generated postconditions; models that overproduce Type Checks (e.g., StarChat) show lower completeness scores overall.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Type Checks appear in ~47.4% of generated atomic postconditions (EvalPlus sample).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>LLMs' tendency to prefer simple, low-risk assertions and the presence of type hints in stubs which encourage type-focused constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Prompt engineering to encourage richer semantic properties (base prompts produce more expressive postconditions but risk incorrectness), combine atomic checks conjunctively, prefer arithmetic equality or element-wise properties when appropriate, and post-filter/rank by discriminative power.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Arithmetic Equality postconditions demonstrate much higher bug-detection power (avg. bug-completeness ~0.82 natural), and using 'base' prompt (which asks for fuller behavior) yields higher bug-completeness at the expense of correctness; using 'simple' prompt increases correctness but reduces completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>program specification generation / empirical software testing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e498.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e498.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt sensitivity / mis-specification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLM outputs to prompt design leading to misaligned or overly complex postconditions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small changes in prompts (base vs simple, inclusion of reference code) substantially change the nature, correctness, and discriminative power of generated postconditions; overly ambitious prompts produce complex but incorrect postconditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>prompt ablation study within nl2postcond pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Four prompt variants combining (a) base vs simple requested postcondition complexity and (b) inclusion vs omission of reference solution; 10 samples per prompt per model used to study effects.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>docstring + optional reference code</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python/Java function present or omitted in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>instruction-to-model mis-specification (prompt-induced misalignment)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>LLMs produce different qualities of postconditions depending on prompt wording and context: base prompts often elicit complex postconditions that can be incorrect, while simple prompts elicit simpler but more often correct postconditions; inclusion of reference code did not significantly change correctness but modestly increased completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input specification (prompt engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Controlled ablation experiments comparing accept@k and bug-completeness across prompt variants and LLMs; statistical testing for significance.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>accept@1/5/10 correctness metrics; bug-completeness-score; statistical tests (paired Student's t-test) with p-values and effect sizes reported (p=0.008, Cohen's d=1.73 for simple vs base correctness advantage). Inclusion of reference code produced no significant accept@1 difference (p=0.42) but increased average bug-completeness by ~5% (p=0.06).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prompt choice materially affects measured correctness and discriminative power; using the wrong prompt can yield many invalid or unhelpful postconditions, undermining the system's usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across all models and benchmarks in the study; simple prompt consistently produced higher correctness across models.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Inherent sensitivity of large LLMs to framing and instruction details combined with a desire by models to fulfill overly broad instructions (leading to over-complex outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Careful prompt design: prefer explicit 'simple' constraints for correctness-critical scenarios; tune prompts on held-out examples; include contextual code when necessary for compilation; sample multiple outputs and use accept@k-style selection.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Simple prompts increased accept@1 significantly; including reference solution modestly increased bug-completeness (~5%). Sampling multiple postconditions (accept@k) yields higher chance of obtaining correct/discriminative assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>LLM-based program synthesis / specification generation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e498.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e498.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context omission in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of sufficient code/context in prompts causes non-compilable or irrelevant postconditions for real-world methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For complex, tightly-coupled real-world Java functions, omitting file-level context (types, related methods) from prompts makes LLMs produce postconditions that rarely compile or meaningfully capture behavior; including context improves compilability and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Defects4J prompt-context augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Defects4J experiments where prompt tokens included function-level and class-level in-file comments and greedily included methods from the call graph until LLM context limits were reached, to provide necessary type and API context for generating Java postconditions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Javadoc plus surrounding file-level comments</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java method plus call-graph methods / class definitions</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing implementation context in model inputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Without file-level and call-graph context, LLMs fail to produce compile-ready postconditions for complex Java methods; this leads to fewer usable assertions and lower test-set-correct rates on real-world projects.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>prompt composition / model input stage (context window)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison on Defects4J where prompts omitted or included buggy function code and additional class context; observed compilability and correctness differences.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported compilability and accept@k metrics (Table 4); the paper reports that including the buggy code/context increased the probability that generated postconditions compiled and increased test-set-correct rates (qualitative and tabulated improvements; examples: GPT-4 compilability rose when code included).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Omitting context reduces number of usable, compilable postconditions and thereby reduces bug-finding capability in real-world code; including context enabled catching of a larger set of defects (contributed to the 64 bugs found).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed specifically in Defects4J experiments (real-world Java functions) where project methods are tightly coupled; less an issue for small benchmark functions.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Limited LLM context window and the need for project-specific type and API information to write valid assertions in statically-typed languages (Java).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Include relevant file-level/type context and call-graph methods in prompts (greedily until token budget exhausted); prefer simple prompts when code context is absent; post-filter generated postconditions by compilation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Including context materially improved compilability and test-set-correctness rates in Defects4J experiments (exact per-cell numbers reported in Table 4); overall enabled generation of bug-discriminating postconditions in real projects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>program analysis / LLM prompt engineering for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e498.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e498.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data leakage / training overlap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Potential training-data leakage between benchmarks and LLM training corpora</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks used (EvalPlus, Defects4J) may be present in The Stack or other LLM training sets (especially for StarCoder-derived models), risking inflated performance estimates if models have seen similar artifacts during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>evaluation datasets vs LLM training corpora</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use of public benchmarks (EvalPlus/HumanEval and Defects4J) as sources of natural-language intent, reference implementations, and tests for evaluating nl2postcond generated postconditions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>benchmark-provided docstrings and in-file comments</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>benchmark reference implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>external validity threat: data leakage / overlap between evaluation corpora and model training data</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>If LLMs had seen parts of EvalPlus/Defects4J (or similar code/comments) in their pretraining data, generated postconditions could reflect memorized artifacts rather than true generalization, leading to overly optimistic performance estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation validity (model training -> test overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Threat-to-validity discussion in limitations; no direct measurement of leakage was possible within the study.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No direct measurement provided; authors note the risk qualitatively and partially mitigate by arguing postconditions themselves are unlikely to be part of training artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potential overestimation of model capability if leakage exists; the paper flags this as a caveat and recommends caution interpreting results for closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified in experiments; noted as a general threat given widespread inclusion of public benchmarks in large code corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Use of public benchmarks that may have been included in large-scale code training datasets (e.g., The Stack).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Make generated artifacts and prompts public for scrutiny; use open-access models (StarChat) for replication; emphasize that postconditions as an output space are unlikely to exist in training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: sharing artifacts and using an open model helps reproducibility, but cannot fully remove the possibility of leakage for closed models; no quantitative mitigation effect reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>ML evaluation / empirical software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toga: A neural method for test oracle generation <em>(Rating: 2)</em></li>
                <li>Dynamically discovering likely program invariants to support program evolution <em>(Rating: 2)</em></li>
                <li>Testing javadoc comments to detect comment-code inconsistencies <em>(Rating: 2)</em></li>
                <li>Translating code comments to procedure specifications <em>(Rating: 1)</em></li>
                <li>Inferring method specifications from natural language API descriptions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-498",
    "paper_id": "paper-263608443",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Ambiguous NL specification",
            "name_full": "Ambiguity in natural-language function specifications (docstrings/Javadoc)",
            "brief_description": "Informal natural-language descriptions often permit multiple interpretations of intended behavior (e.g., whether to remove all duplicates vs keep one copy), causing mismatches between described intent and implemented code; the paper demonstrates this with HumanEval examples and uses LLM-generated postconditions to disambiguate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "nl2postcond LLM-based postcondition generation pipeline",
            "system_description": "A pipeline that prompts chat LLMs (GPT-4, GPT-3.5, StarChat) with a natural-language method description (docstring/Javadoc) and optional reference code to produce executable postconditions which are then evaluated against a reference implementation and test suite.",
            "nl_description_type": "function docstring / Javadoc (informal natural-language specification)",
            "code_implementation_type": "function implementation (Python/Java reference method)",
            "gap_type": "ambiguous description / incomplete specification",
            "gap_description": "Natural-language method descriptions can be semantically ambiguous about key behavioral choices (e.g., whether to drop all duplicates or keep one), so the same docstring may map to multiple valid but distinct formal behaviors; this ambiguity leads to potential misalignment between the written intent and what code actually implements.",
            "gap_location": "specification layer (documentation / docstring) versus implementation behavior",
            "detection_method": "LLM-based generation of candidate postconditions followed by evaluation against the reference implementation and tests; manual qualitative inspection of examples (HumanEval remove_duplicates case study).",
            "measurement_method": "Qualitative example demonstration plus quantitative metrics: test-set-correctness (accept@k) to check if a generated postcondition is consistent with reference code on test suite; bug-completeness to measure discriminative power among mutants (how many alternative implementations are distinguished).",
            "impact_on_results": "Ambiguity can cause multiple plausible postconditions to be generated (both may pass tests); formalizing intent with generated postconditions resolves ambiguity and prevents incorrect assumptions—demonstrated by generated postconditions that disambiguate and match developer intent in examples. (No single numeric 'loss' reported for ambiguity itself, but concrete detection improved ability to flag mismatches.)",
            "frequency_or_prevalence": "High prevalence of natural-language artifacts in codebases supports broad relevance: paper cites prior work finding natural-language documentation in 98% of GitHub repos and comments present in &gt;20% of code lines, indicating ambiguity is widespread.",
            "root_cause": "Informal, underspecified natural language lacking precise constraints and implicit assumptions about intended behavior.",
            "mitigation_approach": "Generate formal, executable postconditions from the natural-language description using LLMs (nl2postcond); present candidate postconditions for developer review to disambiguate intent; prefer simpler prompt variants to increase correctness.",
            "mitigation_effectiveness": "In examples the approach produced unambiguous assertions that matched developer intent; empirically, GPT-4 produced test-set-correct postconditions for up to 96% of EvalPlus problems and simple prompts increased correctness (statistically significant, p=0.008, Cohen's d = 1.73).",
            "domain_or_field": "software engineering / program specification and verification",
            "reproducibility_impact": true,
            "uuid": "e498.0",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Doc-code misalignment (Defects4J)",
            "name_full": "Discrepancies between Javadoc (natural-language intent) and implementation causing real-world bugs",
            "brief_description": "Instances where the implementation violates documented behavior (Javadoc), producing functional bugs; the paper demonstrates that LLM-generated postconditions can formalize the Javadoc and discriminate buggy from fixed implementations (Defects4J examples such as Commons CLI and Commons Math).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "nl2postcond instrumentation and evaluation on Defects4J",
            "system_description": "Instrument buggy and fixed Java methods with LLM-generated postconditions derived from in-file natural language (Javadoc) plus file-level context, then run regression and trigger tests to identify discriminating postconditions that fail on buggy and pass on fixed versions.",
            "nl_description_type": "Javadoc / in-file comments",
            "code_implementation_type": "Java library code (Defects4J buggy and fixed method versions)",
            "gap_type": "implementation violates documented behavior (incomplete/incorrect implementation relative to natural-language spec)",
            "gap_description": "The actual code sometimes fails to satisfy documented constraints (for example, lines exceeding specified width or insufficient numeric precision in reversed Line objects), creating misalignment between the stated intent in Javadoc and the implemented behavior.",
            "gap_location": "implementation logic (algorithmic behavior / numerical precision / formatting behavior)",
            "detection_method": "Generate postconditions from Javadoc using LLMs (with/without code context), instrument both buggy and fixed versions, and run existing regression and trigger tests; a postcondition is bug-discriminating if it passes on fixed version and fails on buggy version.",
            "measurement_method": "Count of bug-discriminating postconditions and number of distinct bugs found; test-set-correctness (accept@k) on fixed versions to ensure postconditions reflect intended behavior.",
            "impact_on_results": "LLM-generated postconditions discriminated 70 buggy methods across 64 unique Defects4J bugs (12.2% of the 525 bugs studied); GPT-4 generated discriminating postconditions for up to 47/525 (9%) bugs in some prompt variants. This demonstrates concrete ability to surface doc-code misalignments and catch historical real-world bugs.",
            "frequency_or_prevalence": "In the Defects4J subset studied (525 bugs), nl2postcond found discriminating postconditions for 64 unique bugs (12.2%); prevalence depends on clarity of Javadoc and project context.",
            "root_cause": "Implementation errors or omissions relative to implied constraints in natural-language documentation; documentation may state constraints that code fails to implement (or tests do not cover).",
            "mitigation_approach": "Translate natural-language documentation into executable postconditions and check them at test time or runtime to detect violations; include file-level context in prompts for complex real-world functions to improve generated assertions and compilability.",
            "mitigation_effectiveness": "Practical effect demonstrated: 64 real historical bugs were discriminated via nl2postcond. Including code/context in prompts improved compilability and increased the rate of test-set-correct postconditions in Defects4J experiments (qualitative and tabulated improvements reported).",
            "domain_or_field": "software engineering / empirical software testing",
            "reproducibility_impact": true,
            "uuid": "e498.1",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Test-suite insufficiency",
            "name_full": "Insufficient test suites causing false positives in test-set-correctness",
            "brief_description": "Because test-set-correctness uses finite test suites as a stand-in for universal correctness, incomplete tests can cause vacuous or overfitting postconditions to appear correct; the paper quantifies false positives when using smaller test suites (HumanEval) versus larger ones (EvalPlus).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "nl2postcond evaluation harness (test-set-correctness metric)",
            "system_description": "Evaluation framework that treats a candidate postcondition as correct if it holds across a given set of test inputs T; used to compute accept@k and to evaluate completeness against code mutants.",
            "nl_description_type": "natural-language docstring (EvalPlus/HumanEval)",
            "code_implementation_type": "reference implementation used to produce expected outputs for tests",
            "gap_type": "incomplete evaluation oracle (test coverage insufficiency) leading to measurement error",
            "gap_description": "Finite test suites can fail to exercise corner-case inputs; weak tests allow postconditions that overfit observed tests (or are vacuously true) to be labeled as correct even though they do not hold universally.",
            "gap_location": "evaluation / validation stage (test suites used for metric computation)",
            "detection_method": "Manual annotation and comparison of postcondition behavior across different test suites: EvalPlus (extensive tests) vs HumanEval (fewer tests); manual review identified false positives.",
            "measurement_method": "Reported false-positive rates: only 1.1% of 900 manually annotated postconditions were affected using EvalPlus; using the smaller HumanEval suite resulted in ~7% false positives for GPT-4 accept@10.",
            "impact_on_results": "Overestimation of correctness when using small/incomplete test suites — smaller suites (HumanEval) produced substantially higher false-positive rates compared to EvalPlus, which demonstrates that evaluation metrics can be inflated by insufficient tests.",
            "frequency_or_prevalence": "Depends on test-suite size; in this study false positives were 1.1% with EvalPlus and ~7% with HumanEval for vulnerable measurements reported.",
            "root_cause": "Limited test coverage and reliance on finite test sets as proxies for universal correctness.",
            "mitigation_approach": "Use more extensive test suites (as in EvalPlus), manual annotation/validation for edge cases, and complement test-based correctness with other analyses (e.g., manual review or formal verification when feasible).",
            "mitigation_effectiveness": "Using EvalPlus (avg. 775 tests per problem) substantially reduced false positives (1.1% vs ~7%); the paper validates test-set-correctness as reasonable on EvalPlus.",
            "domain_or_field": "software testing / empirical evaluation",
            "reproducibility_impact": true,
            "uuid": "e498.2",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mutation vs natural-bug mismatch",
            "name_full": "Mismatch between traditional mutation operators and real-world bug distributions",
            "brief_description": "Standard mutation operators (small syntactic edits) do not adequately reflect the diversity of realistic bugs; the paper proposes using LLMs to sample 'natural' code mutants and shows different completeness behavior compared to artificial mutants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "bug-completeness evaluation using LLM-sampled mutants",
            "system_description": "Completeness metric that measures fraction of LLM-sampled distinct buggy implementations (CM) that a postcondition discriminates; mutants are sampled via LLMs to better approximate natural bug distribution.",
            "nl_description_type": "problem description (natural language intent nl)",
            "code_implementation_type": "LLM-generated alternative implementations (mutants) in Python for EvalPlus",
            "gap_type": "evaluation gap: synthetic mutation operators poorly approximate real bugs",
            "gap_description": "Traditional mutant operators change small tokens/ops and miss syntactically diverse or semantically realistic bugs (e.g., alternate algorithmic constructs); this misalignment can distort metrics of postcondition completeness.",
            "gap_location": "metric construction / mutant generation phase",
            "detection_method": "Empirical comparison of bug-completeness scores computed with (a) natural LLM-sampled mutants and (b) artificially seeded mutants; observation that natural mutants are harder to kill.",
            "measurement_method": "Bug-completeness-score (fraction of mutants killed); comparison of scores when using natural-only vs augmented (natural + explicit artificial) mutants. Reported that completeness was consistently lower when only considering natural bugs.",
            "impact_on_results": "Using artificial/seeding mutations alone may artificially inflate completeness metrics; relying solely on classical mutation operators risks overestimating specification discriminative power.",
            "frequency_or_prevalence": "In EvalPlus experiments, number of unique LLM-sampled buggy codes per problem varied from 4 to 233 (median 55); natural mutants alone provided a meaningful but sometimes smaller set than when augmented with seeded bugs.",
            "root_cause": "Mismatch between simple mutation operators and the real distribution of programmer errors and algorithmic variants.",
            "mitigation_approach": "Use LLMs to sample diverse, natural-looking mutant implementations and retain only pairwise-distinct mutants w.r.t. test behavior; augment with seeded artificial bugs when necessary to boost coverage.",
            "mitigation_effectiveness": "LLM-sampled mutants created a diverse set with median 55 unique buggy codes per problem; augmented mutants increased the mutant pool and allowed more robust estimation of bug-completeness (no single numeric 'correction' value provided).",
            "domain_or_field": "mutation testing / software fault-injection",
            "reproducibility_impact": true,
            "uuid": "e498.3",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Weak postcondition types (Type Checks)",
            "name_full": "Prevalence of weak type-check postconditions that have low bug-discriminative power",
            "brief_description": "LLMs often output simple type-check assertions (e.g., isinstance(return_val,int)), which are common but weak: they are prevalent in generated postconditions yet kill far fewer real/natural mutants than richer semantic checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "postcondition taxonomy and completeness measurement",
            "system_description": "Qualitative coding of atomic postcondition categories (Type Check, Format Check, Arithmetic Bounds, Arithmetic Equality, Container Property, Element Property, Forall-Element Property, Implication, Null Check) and measurement of per-category bug-completeness on EvalPlus.",
            "nl_description_type": "docstring-based intent",
            "code_implementation_type": "Python reference implementation",
            "gap_type": "superficial/incomplete specification (too weak to capture bugs)",
            "gap_description": "Type-check postconditions are easy for LLMs to generate and frequent (47.4% prevalence) but they detect relatively few natural bugs (avg. bug-completeness 0.14 for natural mutants), indicating mismatch between generated checks and actual bug causes.",
            "gap_location": "postcondition expressiveness / specification strength",
            "detection_method": "Manual qualitative coding of 900 generated postconditions into atomic categories and computation of average bug-completeness per category.",
            "measurement_method": "Category prevalence and average bug-completeness-score per category (natural/all mutants). For Type Checks: prevalence 47.4% and avg. bug-completeness 0.14 (natural) / 0.27 (all). For Arithmetic Equality: prevalence 17.5% and avg. bug-completeness 0.82 (natural) / 0.89 (all).",
            "impact_on_results": "Prevalence of weak type checks lowers overall discriminative power of generated postconditions; models that overproduce Type Checks (e.g., StarChat) show lower completeness scores overall.",
            "frequency_or_prevalence": "Type Checks appear in ~47.4% of generated atomic postconditions (EvalPlus sample).",
            "root_cause": "LLMs' tendency to prefer simple, low-risk assertions and the presence of type hints in stubs which encourage type-focused constraints.",
            "mitigation_approach": "Prompt engineering to encourage richer semantic properties (base prompts produce more expressive postconditions but risk incorrectness), combine atomic checks conjunctively, prefer arithmetic equality or element-wise properties when appropriate, and post-filter/rank by discriminative power.",
            "mitigation_effectiveness": "Arithmetic Equality postconditions demonstrate much higher bug-detection power (avg. bug-completeness ~0.82 natural), and using 'base' prompt (which asks for fuller behavior) yields higher bug-completeness at the expense of correctness; using 'simple' prompt increases correctness but reduces completeness.",
            "domain_or_field": "program specification generation / empirical software testing",
            "reproducibility_impact": true,
            "uuid": "e498.4",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prompt sensitivity / mis-specification",
            "name_full": "Sensitivity of LLM outputs to prompt design leading to misaligned or overly complex postconditions",
            "brief_description": "Small changes in prompts (base vs simple, inclusion of reference code) substantially change the nature, correctness, and discriminative power of generated postconditions; overly ambitious prompts produce complex but incorrect postconditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "prompt ablation study within nl2postcond pipeline",
            "system_description": "Four prompt variants combining (a) base vs simple requested postcondition complexity and (b) inclusion vs omission of reference solution; 10 samples per prompt per model used to study effects.",
            "nl_description_type": "docstring + optional reference code",
            "code_implementation_type": "Python/Java function present or omitted in prompt",
            "gap_type": "instruction-to-model mis-specification (prompt-induced misalignment)",
            "gap_description": "LLMs produce different qualities of postconditions depending on prompt wording and context: base prompts often elicit complex postconditions that can be incorrect, while simple prompts elicit simpler but more often correct postconditions; inclusion of reference code did not significantly change correctness but modestly increased completeness.",
            "gap_location": "model input specification (prompt engineering)",
            "detection_method": "Controlled ablation experiments comparing accept@k and bug-completeness across prompt variants and LLMs; statistical testing for significance.",
            "measurement_method": "accept@1/5/10 correctness metrics; bug-completeness-score; statistical tests (paired Student's t-test) with p-values and effect sizes reported (p=0.008, Cohen's d=1.73 for simple vs base correctness advantage). Inclusion of reference code produced no significant accept@1 difference (p=0.42) but increased average bug-completeness by ~5% (p=0.06).",
            "impact_on_results": "Prompt choice materially affects measured correctness and discriminative power; using the wrong prompt can yield many invalid or unhelpful postconditions, undermining the system's usefulness.",
            "frequency_or_prevalence": "Observed across all models and benchmarks in the study; simple prompt consistently produced higher correctness across models.",
            "root_cause": "Inherent sensitivity of large LLMs to framing and instruction details combined with a desire by models to fulfill overly broad instructions (leading to over-complex outputs).",
            "mitigation_approach": "Careful prompt design: prefer explicit 'simple' constraints for correctness-critical scenarios; tune prompts on held-out examples; include contextual code when necessary for compilation; sample multiple outputs and use accept@k-style selection.",
            "mitigation_effectiveness": "Simple prompts increased accept@1 significantly; including reference solution modestly increased bug-completeness (~5%). Sampling multiple postconditions (accept@k) yields higher chance of obtaining correct/discriminative assertions.",
            "domain_or_field": "LLM-based program synthesis / specification generation",
            "reproducibility_impact": true,
            "uuid": "e498.5",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Context omission in prompts",
            "name_full": "Lack of sufficient code/context in prompts causes non-compilable or irrelevant postconditions for real-world methods",
            "brief_description": "For complex, tightly-coupled real-world Java functions, omitting file-level context (types, related methods) from prompts makes LLMs produce postconditions that rarely compile or meaningfully capture behavior; including context improves compilability and correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Defects4J prompt-context augmentation",
            "system_description": "Defects4J experiments where prompt tokens included function-level and class-level in-file comments and greedily included methods from the call graph until LLM context limits were reached, to provide necessary type and API context for generating Java postconditions.",
            "nl_description_type": "Javadoc plus surrounding file-level comments",
            "code_implementation_type": "Java method plus call-graph methods / class definitions",
            "gap_type": "missing implementation context in model inputs",
            "gap_description": "Without file-level and call-graph context, LLMs fail to produce compile-ready postconditions for complex Java methods; this leads to fewer usable assertions and lower test-set-correct rates on real-world projects.",
            "gap_location": "prompt composition / model input stage (context window)",
            "detection_method": "Empirical comparison on Defects4J where prompts omitted or included buggy function code and additional class context; observed compilability and correctness differences.",
            "measurement_method": "Reported compilability and accept@k metrics (Table 4); the paper reports that including the buggy code/context increased the probability that generated postconditions compiled and increased test-set-correct rates (qualitative and tabulated improvements; examples: GPT-4 compilability rose when code included).",
            "impact_on_results": "Omitting context reduces number of usable, compilable postconditions and thereby reduces bug-finding capability in real-world code; including context enabled catching of a larger set of defects (contributed to the 64 bugs found).",
            "frequency_or_prevalence": "Observed specifically in Defects4J experiments (real-world Java functions) where project methods are tightly coupled; less an issue for small benchmark functions.",
            "root_cause": "Limited LLM context window and the need for project-specific type and API information to write valid assertions in statically-typed languages (Java).",
            "mitigation_approach": "Include relevant file-level/type context and call-graph methods in prompts (greedily until token budget exhausted); prefer simple prompts when code context is absent; post-filter generated postconditions by compilation.",
            "mitigation_effectiveness": "Including context materially improved compilability and test-set-correctness rates in Defects4J experiments (exact per-cell numbers reported in Table 4); overall enabled generation of bug-discriminating postconditions in real projects.",
            "domain_or_field": "program analysis / LLM prompt engineering for code",
            "reproducibility_impact": true,
            "uuid": "e498.6",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Data leakage / training overlap",
            "name_full": "Potential training-data leakage between benchmarks and LLM training corpora",
            "brief_description": "Benchmarks used (EvalPlus, Defects4J) may be present in The Stack or other LLM training sets (especially for StarCoder-derived models), risking inflated performance estimates if models have seen similar artifacts during pretraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "evaluation datasets vs LLM training corpora",
            "system_description": "Use of public benchmarks (EvalPlus/HumanEval and Defects4J) as sources of natural-language intent, reference implementations, and tests for evaluating nl2postcond generated postconditions.",
            "nl_description_type": "benchmark-provided docstrings and in-file comments",
            "code_implementation_type": "benchmark reference implementations",
            "gap_type": "external validity threat: data leakage / overlap between evaluation corpora and model training data",
            "gap_description": "If LLMs had seen parts of EvalPlus/Defects4J (or similar code/comments) in their pretraining data, generated postconditions could reflect memorized artifacts rather than true generalization, leading to overly optimistic performance estimates.",
            "gap_location": "evaluation validity (model training -&gt; test overlap)",
            "detection_method": "Threat-to-validity discussion in limitations; no direct measurement of leakage was possible within the study.",
            "measurement_method": "No direct measurement provided; authors note the risk qualitatively and partially mitigate by arguing postconditions themselves are unlikely to be part of training artifacts.",
            "impact_on_results": "Potential overestimation of model capability if leakage exists; the paper flags this as a caveat and recommends caution interpreting results for closed models.",
            "frequency_or_prevalence": "Not quantified in experiments; noted as a general threat given widespread inclusion of public benchmarks in large code corpora.",
            "root_cause": "Use of public benchmarks that may have been included in large-scale code training datasets (e.g., The Stack).",
            "mitigation_approach": "Make generated artifacts and prompts public for scrutiny; use open-access models (StarChat) for replication; emphasize that postconditions as an output space are unlikely to exist in training corpora.",
            "mitigation_effectiveness": "Partial: sharing artifacts and using an open model helps reproducibility, but cannot fully remove the possibility of leakage for closed models; no quantitative mitigation effect reported.",
            "domain_or_field": "ML evaluation / empirical software engineering",
            "reproducibility_impact": true,
            "uuid": "e498.7",
            "source_info": {
                "paper_title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toga: A neural method for test oracle generation",
            "rating": 2,
            "sanitized_title": "toga_a_neural_method_for_test_oracle_generation"
        },
        {
            "paper_title": "Dynamically discovering likely program invariants to support program evolution",
            "rating": 2,
            "sanitized_title": "dynamically_discovering_likely_program_invariants_to_support_program_evolution"
        },
        {
            "paper_title": "Testing javadoc comments to detect comment-code inconsistencies",
            "rating": 2,
            "sanitized_title": "testing_javadoc_comments_to_detect_commentcode_inconsistencies"
        },
        {
            "paper_title": "Translating code comments to procedure specifications",
            "rating": 1,
            "sanitized_title": "translating_code_comments_to_procedure_specifications"
        },
        {
            "paper_title": "Inferring method specifications from natural language API descriptions",
            "rating": 1,
            "sanitized_title": "inferring_method_specifications_from_natural_language_api_descriptions"
        }
    ],
    "cost": 0.0201355,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?
15 Apr 2024</p>
<p>Madeline Endres 
Sarah Fakhoury 
USAMicrosoft Research 
Saikat Chakraborty 
Shuvendu K Lahiri </p>
<p>University of Michgain
USA</p>
<p>Microsoft Research
USA</p>
<p>Microsoft Research
USA</p>
<p>Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?
15 Apr 2024C1E199E2DFFC1D7E5983643AA24C2083arXiv:2310.01831v2[cs.SE]CCS Concepts:General and reference → MetricsValidation• Software and its engineering → CorrectnessCompletenessSoftware reliabilitySoftware verificationFormal software verification• Computing methodologies → Artificial Intelligence
Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program's intent.However, there is typically no guarantee that a program's implementation and natural language documentation are aligned.In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness.In practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically.The "emergent abilities" of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions.However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent.Additionally, it is unclear if such translation could be useful in practice.In this paper, we describe nl2postcond, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.We introduce and validate metrics to measure and compare different nl2postcond approaches, using the correctness and discriminative power of generated postconditions.We then use qualitative and quantitative methods to assess the quality of nl2postcond postconditions, finding that they are generally correct and able to discriminate incorrect code.Finally, we find that nl2postcond via LLMs has the potential to be helpful in practice; nl2postcond generated postconditions were able to catch 64 real-world historical bugs from Defects4J.</p>
<p>INTRODUCTION</p>
<p>Informal natural language specifications are omnipresent in modern software.For example, Pfeiffer [39] found natural language documentation in 98% of over 20,000 GitHub repositories, with 10% of repository artifacts specifically for documentation.He [16] found over 20% of non-blank program lines contained in-file comments in their study of 150 of the most starred projects on GitHub.At the same time, it is well known that software bugs (unexpected exceptions, incorrect output) often arise from the weak association between the intended behavior (documented in natural language) and the behavior of the implementation [46,48].This issue is exacerbated with AI-assisted programming where users generate code from natural language intent [2,14,45], without a good way to ensure their association.Reliably translating informal natural language descriptions to formal specifications could help catch bugs before production and improve trust in AI-generated code [23].</p>
<p>Current approaches to translating natural language to formal specifications are heuristic-based and either rely on the input being in a structured format [3,48] or can only generate a restricted class of specifications (e.g., regarding nullness or exceptions) [15,46].Further, most of these approaches are customized for only one specific programming language (such as Java).In the past, large-scale neural modeling for the problem of generating specifications has been difficult given the absence of large code corpora with matching natural language intent and corresponding specifications.</p>
<p>Large Language Models (LLMs) have generated a lot of interest in the area of programming owing to their ability to synthesize high-quality code from natural language intent in a surrounding context [6,28,33].Given the limitations of the current approaches for translating natural language to formal specifications, we explore the use of LLMs for this problem.Even though these LLMs have not seen structured data matching natural language intent to specifications, larger models such as GPT-4 have demonstrated "emergent abilities" to do well on tasks that they were not explicitly trained for, or unlikely to be common in the training corpus [53].In particular, models such as GPT-4 demonstrate capabilities to follow natural language instructions to perform reasoning tasks, for example, through prompting strategies like few-shot learning [24], chain-of-thought [54] and multi-step reasoning [57].</p>
<p>In this paper, we explore the feasibility of leveraging LLMs to act as a usable and practical bridge between informal natural language and useful method postconditions.A postcondition for a method is an assertion that relates the input and output states of the method, and holds true after any successful execution of the method.We assess this ability in a programming language-agnostic way, by targeting postconditions that can be expressed as assertions in the underlying programming language.We term this approach as nl2postcond -leveraging LLMs for the purpose of translating natural language method-level comments to corresponding postconditions.</p>
<p>Motivating Examples</p>
<p>1.1.1Formalizing User Intent.Consider the example in Fig. 1, taken from the popular Python code generation benchmark, HumanEval [6].A programmer intends a function that removes all numbers with duplicates from a list.For example, given the list [1,2,3,2,4] the function should return [1,3,4] without the element 2 because it appears more than once (Fig. 1a).The programmer describes the function specification in a docstring (see Fig. 1b).However, the natural language specification is ambiguous; it does not indicate if all copies of the duplicated elements should be removed, or if one copy should be retained.In this case, the programmer intends the former, however, it is not uncommon to expect that the program should fulfill the latter.</p>
<p>Figure 1c contains two postconditions, each satisfying one of the two possible intents of the ambiguous docstring.The programmer writing the remove _ duplicates function can verify that the second postcondition "assert all(numbers.count(i)==1for i in return _ list)" correctly matches their intent, by ensuring that all numbers in the returned list occur exactly once in the input list.The first postcondition, however, incorrectly asserts that return _ list is a set of the input list numbers.In this example from HumanEval, it is not immediately clear at first glance what the user intent is.Generating such postconditions from natural language allows for checkable and unambiguous statements about a program's intended behavior, formalizing a user's intent about a program.</p>
<p>(a) Programmer intent for a function that removes from a list all instances of numbers that have duplicates.</p>
<p>(b) Ambiguous natural language specification: it does not specify if all copies or all but one copy of a duplicated element should be removed.In this case, the programmer intends the former.</p>
<p>1 def remove_duplicates(numbers: List[int]):</p>
<p>2 """ From a list of integers, remove all elements that occur more than once.Keep order of elements left the same as in the input """</p>
<p>(c) Postconditions generated by GPT-4.Note that while both could be correct with a literal reading of the natural language specification, only the second one is correct with respect to developer intent Line.revert()only maintains ∼10 digits for the direction.This becomes an issue when the line's position is evaluated far from the origin.</p>
<p>(c) Two bug-catching post conditions generated by GPT 4  This example is a historical bug from Defects4J (Math-9): the Line constructor does not return a new line with enough precision.The postconditions were generated by GPT-4 in our evaluation, and both catch the bug.</p>
<p>1.1.2Detecting Real-World Functional Bugs.In practice, postconditions generated in the target programming language can be used in assertions, as demonstrated in fig.1c, to check the correctness of a function, enabling the early detection of bugs or violations of a programmer's intent.The example in Figure 2 shows how formal specifications can be used to catch bugs in realworld programs.A bug from the Apache Commons Math project, the function revert() calls a constructor Line() that should return a new Line object with a reversed direction.The bug report1 associated with the issue explains that revert() does not maintain enough precision, and fails in certain scenarios.Both of the provided postconditions in Figure 2c catch the bug by leveraging project-specific context and general mathematical knowledge about the specifications of a reversed line.</p>
<p>Overview</p>
<p>In these examples, we demonstrated that GPT-4 can generate postconditions from natural language that closely capture informal intent and also detect program bugs.However, it is unclear to what extent LLMs are capable of the nl2postcond problem in general.We pose the high-level question:</p>
<p>Given a natural language description of a method and a candidate postcondition, how do we judge the quality of the postcondition?</p>
<p>We attempt to study this question through two high-level research questions:</p>
<p>• RQ1: How well do LLM-generated postconditions formalize informal natural language intent?• RQ2: Can LLM-generated postconditions help catch real-world bugs?</p>
<p>To answer these questions, we define automated metrics for measuring the usefulness of LLMgenerated postconditions, describe different ways to encode the problem statement for an LLM, explore different LLMs, and perform an empirical investigation (both quantitative and qualitative) on benchmarks across multiple programming languages.We first define automated evaluation metrics for the correctness and completeness (i.e., the discriminative power) of a postcondition (Section 2.1), and we propose a generic "prompt" and variants to transform nl2postcond into an input for LLM (Section 2.2).We evaluate RQ1 using a Python programming dataset and present a detailed analysis of generated postconditions quality across different LLMs and prompt variants (Section 3).Next, we evaluate RQ2 on a benchmark of real-world Java defects and report on the ability of postconditions to find bugs by distinguishing the fixed version from the buggy version (Section 4).Finally, we articulate the limitations (Section 6) and discuss related works (Section 5).</p>
<p>Contributions</p>
<p>• Evaluating the feasibility of LLMs to facilitate nl2postcond via an empirical evaluation of the quality and usefulness of LLM generated postconditions on multiple benchmarks in multiple mainstream programming languages.• A set of metrics (both correctness and completeness) for evaluating natural language generated postconditions, validated through an empirical and qualitative investigation.In particular, we believe this paper is the first to propose the use of LLMs to derive a natural distribution of code mutants to evaluate the completeness of specifications.• The finding that with sufficiently robust natural language descriptions, LLMs can use nl2postcond to generate correct postconditions with high discriminative power.We illustrate that with GPT-4 we can generate correct postconditions for up to 96% of problems for our benchmark, EvalPlus, with correct postconditions able to discriminate up to 81% of buggy programs on average.• The finding that LLM-generated nl2postcond postconditions are precise enough to capture real-world bugs in large industrial projects; nl2postcond postconditions detect 64 historical bugs from 70 buggy methods in industrial-scale Java projects.• We contribute and release the artifacts of our study including LLM-generated code mutants and postconditions that can be useful for future research in this area.</p>
<p>2 NL2POSTCOND: OVERALL APPROACH</p>
<p>Problem formulation and metrics</p>
<p>We first formalize the nl2postcond problem through metrics to evaluate the quality of generated postconditions.Consider an example ⟨nl, r, T ⟩, where nl is the natural language description of a problem, r is a reference code implementation, and T is a set of test inputs.For this section, we assume that each test  ∈ T is an input that assigns a value to the input parameters and globals of r.</p>
<p>We further assume that the reference solution is deterministic and returns a single output value ret containing the output.In this simple setting, it suffices to only have the set of inputs in T , as the desired output for each input  can be obtained by executing r ().For the purpose of postcondition generation through an LLM, the set of tests T is hidden from the LLM that generates a postcondition.The reference implementation r may or may not be present during the postcondition generation.However, both r and T are used to define the metrics for the offline evaluation for a benchmark set.</p>
<p>2.1.1Test-set correctness.Given an example  ⟨nl, r, T ⟩, a candidate postcondition post is an assertion over the input and output states of r.For an expression expr and a state  that assigns valuation to variables, let eval(expr, s) be the result of evaluating expr after replacing the variables in expr with their values from .A postcondition is correct if the reference implementation r satisfies it for every possible (legal) input.Therefore, a candidate postcondition post is correct if for every input , if r () is the output value, then eval(post, (i, r (i))) is true, where (, r ()) is the joint state of the input parameters and output return variable.However, such a notion of correctness is difficult to establish in the absence of formal verification tools, and may further require manual effort to establish such proof even for verification-aware languages [25,44].We take a pragmatic approach, assuming that the test cases in T are sufficiently comprehensive to approximate the space of all legal inputs.Therefore, an expression post is test-set-correct w.r.t.T (denoted as correct T ) iff ∀ ∈ T : eval(post, (i, r (i)) == true.Henceforth, we may refer to "test-set-correct" as simply correct, since correctness in the remainder of the paper is with respect to the provided tests.Given a set of  postconditions from an LLM, we define a metric accept@k for 1 ≤  ≤  to capture the statistical expected value of containing at least one test-set-correct postcondition while sampling subsets of size  from the set of  conditions.This is inspired by the pass@k metric proposed for evaluating the quality of correctness of generated code given a set of tests [6].</p>
<p>2.1.2Test-set completeness for code mutants: bug-completeness-score. (Test-set) correctness is a necessary condition for a valid and useful postcondition, however, it is not sufficient.For example, the expression true vacuously satisfies any implementation  for any input  ∈ T , and is therefore correct.The value of a postcondition comes from how well it captures the desired intent expressed in the natural language intent nl.However, given that nl is informal, we cannot establish a check to ensure the association.Instead, we leverage the reference implementation and tests as the source of ground truth for what the user intends.However, this again poses the problem that the most desired postcondition is simply the strongest postcondition of r program, which is computationally intractable [8].Instead, we use a concept of completeness that measures the degree to which the postcondition distinguishes the reference implementation r from other incorrect implementations.</p>
<p>Inspired by mutation-testing literature (c.f.Jia and Harman [19]) that assigns a score to a test  based on the fraction of code mutants "killed" or distinguished under , we assign a measure of bug-completeness to a postcondition post as the fraction of code mutants that can be distinguished given the set of tests T .Unlike traditional mutation testing, we parameterize completeness with a semantically distinct code mutant set CM that are guaranteed to differ from r (and from each other) on at least one test in T .In other words, for each  ∈ CM, there exists a test input  ∈ T that distinguishes from r (i.e., r () ≠  ()) and (a possibly different)  that distinguishes from any other  ′ ∈ CM \ {c} (i.e.,  () ≠  ′ ()).Given an example  ⟨nl, r, T ⟩, a correct T postcondition post and a set of distinct code mutants CM, we define the bug-completeness-score of post as: bug-completeness-score(post, CM, T ) |{c ∈ CM | ∃i ∈ T : eval(post, (i, c(i))) == false}|/|CM | In other words, bug-completeness-score measures the fraction of code mutants that fail the correct postcondition.If the bug-completeness-score of a postcondition is 1, we say that the postcondition is bug-complete.One can easily lift the idea to the completeness of a set postconditions P by taking a Fig. 3. Prompt template for generating postconditions from natural language via chat models (including changes needed for the simple and no reference variations).We found that the bold text greatly improved the quality of the postconditions: without it, the model tended to return point-based tests or code blocks with side effects.While modified here slightly for clarity, our full prompts are included in our replication package.</p>
<p>union of all the code mutants "killed" using all the correct postconditions in the set: We now discuss why we use a parameterized set of code mutants instead of creating variants of r by mutating different operators.We believe that such a fixed set of mutation operators does not approximate real-world bugs for two reasons: (a) first, since code mutants only differ from the reference implementation in one or two operators at a time, it may not cover mutations that are further away in the edit distance, and (b) it may not cover subtle bugs that a human would write using different syntactic constructs (e.g., a while loop instead of a for loop) or APIs.We propose the use of LLMs to sample mutants from the natural distribution of implementations to the problem described by the natural language intent nl.In other words, we enumerate a set of likely implementations Impls for nl using a LLM (such as GPT-4), and define CM to be the subset of Impls that differ from r on at least one test  ∈ T , and also pairwise distinct in terms of the tests in T .</p>
<p>Prompt Design for LLM-based Postcondition Generation</p>
<p>LLM performance has been shown to be impacted by small changes in prompts for the same problem task, and designing the optimal prompt is not always a straightforward task.We explore several prompt templates, i.e. varied textual representations of the problem description nl, and reference solution r, optimizing for a number of outcomes.First, the prompts should work with chat-based models, and the generated postconditions should be symbolic (e.g., not point-wise tests), directly executable, and side-effect free.Also, the prompt should encourage the LLM to produce expressions that are syntactically and semantically valid while being as programming language agnostic as possible.Several prompt iterations were considered until we observed satisfactory performance on a subset of example problems, though we acknowledge further prompt tuning may result in different outcomes.Figure 3 outlines our prompt template.This template shows four (a) Informal natural language specification for problem 12 from HumanEval Given a string text, replace all spaces in it with underscores, and if a string has more than 2 consecutive spaces, then replace all consecutive spaces with -.For example: fix _ spaces(" Example 1") == " _ Example _ 1", fix _ spaces(" Example 2") == " _ Example-2" (b) base vs. simple: the base postcondition tries to capture all intended functionality, but does so incorrectly.The simple postcondition is less complex (capturing less functionality), but is correct.possible prompt iterations along two orthogonal axes (a) whether the reference r is included, and (b) requested postcondition complexity.We now discuss each axis in more detail.</p>
<p>Including reference code: Our default prompt includes only the nl, not the reference code r.This is useful for specification-driven AI-based programming scenarios [23] where the user first accepts a few specifications that are used to constrain AI generated code suggestions.However, we also provide a prompt variant that includes the reference code r along with nl.This allows us to assess if natural language alone can be as effective as code in conveying programming intent to an LLM.</p>
<p>Postcondition complexity: we also consider a simple variation of the prompt that explicitly instructs the LLM to generate postconditions that capture an aspect of a function, rather than the whole function.We include this variation as we noticed that when using the base prompt, LLMs have a tendency to construct complex postconditions, often striving for a fully functional implementation of the problem.While useful, we observe these complex postconditions are more likely to be incorrect.To understand our motivation for including both prompt variations, Fig. 4 compares postconditions produced by the base and simple prompt for a problem from the HumanEval benchmark.</p>
<p>We combine these two prompt variations into four distinct prompts in our evaluation:</p>
<p>(1) Base prompt with only natural language description nl (no reference solution r)</p>
<p>(2) Base prompt with both reference r and natural language description nl (3) Simple prompt with only natural language description nl (no reference solution r) (4) Simple prompt with both the reference r and natural language description nl</p>
<p>RQ1: HOW WELL DO LLM-GENERATED POSTCONDITIONS FORMALIZE INFORMAL NATURAL LANGUAGE INTENT?</p>
<p>To assess if LLMs can generate high-quality postconditions that capture and formalize intent, we report a detailed empirical study of LLM-generated postconditions on a popular benchmark.</p>
<p>RQ1 Experimental Setup</p>
<p>3.1.1Evaluation Benchmark.We use the benchmark EvalPlus, which includes 164 Python problems, each with an associated function stub and natural language description in the form of a Python docstring, a reference implementation, and validation tests [29].EvalPlus is an update to the popular</p>
<p>HumanEval benchmark [6], containing the same problems but with the addition of a more extensive test suite (775 tests per problem on average).We choose EvalPlus because each example has (a) a descriptive natural language intent, (b) a set of extensive test inputs, and (c) a unique reference solution.Using these three components, we can evaluate if a postcondition formalizes the user intent expressed in the natural langauge docstring, nl, while also satisfying the reference solution.</p>
<p>3.1.2Large Language Models.We generate postconditions using three recent chat-based models, including both closed and open-source approaches, that have demonstrated state-of-the-art performance on various programming tasks:</p>
<p>• OpenAI: GPT-3.5 and GPT-4 are based on the pre-trained GPT-3 model, which is further fine-tuned using Reinforcement Learning with Human Feedback (RLHF) [36].While GPT-3.5 and GPT-4 are not explicitly fine-tuned for code generation, they have demonstrated strong capabilities on several related tasks [11,34].We use OpenAI APIs for the gpt-3.5-turboand gpt-4 endpoints.• The BigCode Project: StarChat.StarCoder [28] is an open-access 16B parameter model pretrained on The Stack [22], one trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebook.We use StarChat2 , a version of StarCoder fine-tuned for assisting coding.StarChat is currently one of the few open-access chat model alternatives to GPT-3.5 and GPT-4, permitting replication of and comparison with our results.This model allows us to use the same prompt we used for the OpenAI models, rendering a fairer comparison.</p>
<p>Postcondition Generation.</p>
<p>For each EvalPlus problem, we generate 10 postconditions for each of the 4 prompt variants (Section 2.2) per LLM model.We use a temperature of 0.7 as it is the default for both GPT-3.5 and GPT-4, and has been found to be a reasonable temperature for code generation tasks. 3As we consider four prompt variants, we generate 40 postconditions per problem per model.This results in 19,680 postconditions across all variants, models, and EvalPlus problems.</p>
<p>Code Mutant Generation.</p>
<p>To generate the set of code mutants CM needed for bug-completeness, we use an LLM (GPT-3.5 with temperature 0.9) to generate a set of codes that satisfy the natural language intent nl, and filter the ones that fail at least one test in T .We generate 200 code solutions to each problem and then save only those that fail the test suite.We term these bugs as natural code mutants, as they represent natural yet buggy implementations for the problem description.However, we noticed that for some examples, the number of such natural code mutants is fairly small.To amplify the set of buggy codes, we generate 200 additional buggy codes by explicitly instructing GPT-3.5 to include an error in its solution.As mentioned in Section 2.1.2,we only retain distinct buggy codes so that no two mutants fail the same set of tests.The number of unique buggy codes varies per problem, ranging from 4 to 233 with a median of 55.While we combine the two bug sources, we also consider the natural mutants alone in our evaluation to see if the source of the bug impacts the efficacy of our metrics.We make available the set of mutants for use by the broader research community and to support the reproducibility of our results.</p>
<p>RQ1-Results: Do LLM-generated postconditions formalize user intent?</p>
<p>We now discuss the results of our empirical and qualitative evaluations, structured around postcondition correctness, postcondition completeness, and qualitative insights.Regardless of the prompt variation, GPT-4 postconditions were the most likely to be correct (0.63 ≤ accept@1 ≤ 0.77) followed by GPT-3.5 (0.46 ≤ accept@1 ≤ 0.56).StarChat postconditions were consistently the least correct, with accept@1 between 0.21 and 0.25.While the raw number of correct StarChat postconditions was low, the number of benchmark problems with at least one correct postcondition was relatively high, ranging from 78% to 86% depending on the prompt.</p>
<p>As described in section 2.2, we consider both a base postcondition prompt and a simple prompt for generating simpler postconditions that capture only an aspect of program behavior.Regardless of LLM model, simple postconditions are more likely to be correct than base postconditions.Using a paired students -test [17] between accept@1 ablation pairs where the only difference is the prompt complexity type, simple prompt postconditions are significantly more likely to be correct with  = 0.008, a large effect (standardized Cohen's  = 1.73).This indicates that when prioritizing correctness, using a prompt that explicitly asks for simpler postconditions improves the result.</p>
<p>We also compared the efficacy of generating postconditions from natural language alone to generating when a reference solution is included in the prompt.We did not observe a significant difference in accept@1 between postconditions generated with natural language specifications alone and those including a reference solution ( = 0.42).This indicates that the presence of a reference solution does not necessarily enhance postcondition correctness when compared to natural language alone.Therefore, it might be feasible to rely solely on natural language intent (when comprehensive enough) without needing to provide a reference solution.</p>
<p>Correctness false positives.EvalPlus has more comprehensive tests than its predecessor HumanEval, but it is still possible that the tests do not capture all possible inputs.If so, our test-set correctness metric may have false positives.We validate our metric for EvalPlus: we find only one problem (# 122) with false positives relating to negative inputs.Overall, only 1.1% of the 900 postconditions that we manually annotated were effected (see Section 3.2.3 for our annotation process).In contrast, we also compare our results to the hypothetical results if using HumanEval (which has the same problems, but many fewer tests).The HumanEval results contain 7% false positives for accept@10 for GPT-4, much higher than our results.Thus, we find that test-set correctness is a reasonable approximation for true correctness on EvalPlus (but perhaps less so for HumanEval).</p>
<p>RQ1 Summary: Postcondition Correctness on EvalPlus</p>
<p>On EvalPlus, LLMs are good at producing correct postconditions from informal natural language specifications.All prompt variants generate a correct postcondition for at least 77% and up to 96% of problems.GPT-4 consistently outperformed GPT-3.5 and StarChat.Explicitly asking for simple postconditions leads to more correct postconditions.However, we do not observe a significant difference between including or omitting a reference solution in the prompt; using natural language (when descriptive) alone can be just as powerful.</p>
<p>Postcondition</p>
<p>Completeness.While our test-set correctness results are encouraging, test-set correctness is necessary but not sufficient for assessing if a postcondition meaningfully captures the natural language specification.To capture a notion of completeness, we measure bug-completeness for all test-set correct postconditions (see Section 2.1).Table 2 contains our results.We report both the percentage of postconditions that are bug-complete (kill all unique code mutants) and the average bug-completeness score (fraction of code mutants killed).The results indicate the GPT-4 postconditions can kill all the code mutants for up to 62.2% of examples in EvalPlus.Overall, both GPT-3.5 and GPT-4 generate relatively bug-complete postconditions, with average scores of up to 0.76 and 0.85 respectively.That is, the average correct postcondition generated by these models can discriminate over three-quarters of unique buggy code mutants.The bug-completeness scores for StarChat were lower but still substantial, catching up to one-third of mutants.Our bug-completeness results suggest that LLMs, especially the advanced models like GPT-3.5 and GPT-4, can use natural language to produce postconditions that meaningfully capture desired aspects of program behavior.</p>
<p>In contrast to the correctness results (Section 3.2.1),base postconditions generally have higher bug-completeness scores than simple postconditions (up to a 30% difference).This trend hints that the simple prompt may generate more correct postconditions at the expense of bug-catching power.Even so, as shown in Fig. 4, simple postconditions still meaningfully capture aspects of program behavior: simple GPT-4 and GPT-3.5 postconditions discriminate over half of unique buggy mutants.</p>
<p>We also compare the bug-completeness of postconditions generated from natural language intent alone to those generated with a reference solution in the prompt.While the difference was not quite significant, the average bug-completeness score was 5% higher for the case with the reference code included ( = 0.06).From our qualitative investigation, this seems to be caused by an increase in the number of postconditions that are functional re-implementations of the reference solution.</p>
<p>Natural vs. Artificial bugs.To help validate our proposed bug-completeness metric (see Section 2.1), we examine the impact of using natural or artificial LLM code generation bugs.As shown in Table 2, our completeness metric was consistently (though not always substantially) lower when only considering natural bugs; naturally occurring LLM code generation bugs are harder to kill via nl2postcond than artificially seeded bugs.This finding highlights a potential limitation in using artificially seeded faults to assess postcondition correctness as it may artificially inflate the metric.However, generating unique natural bugs is more expensive than using artificial bugs.To ensure metric robustness, augmenting the evaluation metric with artificial bugs may still be useful.</p>
<p>RQ1 summary: Postcondition Completeness on EvalPlus</p>
<p>We find that for the benchmark EvalPlus, nl2postcond postconditions generated by GPT-3.5 and GPT-4 can meaningfully capture program intent especially when using our base prompt: the average correct postcondition generated by these models can discriminate three-quarters of unique buggy code mutants depending on the prompt variation.</p>
<p>3.2.3</p>
<p>Qualitative Analysis of Generated Postconditions.Evaluating postcondition correctness and completeness tells us how well LLMs can generate specifications that capture the programs intent, however it does not give us insight into the kinds of generated specifications, and how they differ in terms of performance.We ask two questions: 1) Are there patterns within LLM generated postconditions and 2) How do these categories differ in terms of correctness and completeness?These insights can help to inform future improvements around LLMs generated specifications, and may guide ranking or selection strategies when using generated postconditions in practice.</p>
<p>To determine what program aspects nl2postcond postconditions verify, we conduct a manual qualitative analysis.We first select 230 postconditions generated for 23 EvalPlus problems.We use the best-performing prompt version for correctness: GPT-4 with the simple prompt and no reference solution.The first two authors developed a set of qualitative coding categories for postcondition structure and jointly labeled all 230 postconditions.The first author then used this set of categories to label an additional 670 postconditions for a total of 900 labeled postconditions from 139 EvalPlus problems We present these categories in Table 3 and report prevalence and completeness.</p>
<p>From the classification process, we observe that postconditions can take the form of either atomic or conjoined statements.For example, an LLM may generate a single postcondition that checks several distinct properties about a program, conjoined with logical &amp;&amp; operators.Results of the classification process show that 33% of LLM-generated postconditions consists of multiple atomic postconditions, conjoined using &amp;&amp; (logical and).We categorize nine basic types of atomic properties.Table 3 contains an example of each, along with its dataset prevalence and completeness measures.Prevalence is counted across both atomic and conjoined statements, e.g. if an assertion conjoins specifications across two categories, both are counted.As a result, prevalence adds to over 100%.Type Checks enforce a constraint on the type of a return value using isinstanace.Format Checks ensure that the return value follows a certain string format constraint.Arithmetic Bounds and Arithmetic Equality enforce a numeric bounding or equality constraint against another expression.Container Property checks an aspect of a complex type or object (e.g., the length of an array).Element Property and Forall-Element Property enforce some constraint on one or all elements of a collection.Implications include conditional logic, and Null Check ensures that the return value is not None.</p>
<p>We did not observe a significant relation between postcondition type and correctness.However, we do observe significant differences in bug-completeness across categories.For example, postconditions labeled as Type Checks, i.e. specifications enforcing the type of the return value, were the weakest, only killing 27% of bugs on average.This difference was particularly pronounced for natural bugs (see Sections 2.1 and 3.1.4),where Type Checkers only killed 14% of bugs on average.Interestingly, Type Checks are also the most prevalent category, indicating LLM preference towards generating such constraints.Low completeness scores indicate that, for the studied dataset, type-mismatch errors is not a common bug source.This may be explained by the inclusion of type hints in the EvalPlus dataset, which appear in function stubs provided to the LLM.</p>
<p>On the other hand, Arithmetic Equality checks, i.e. specifications that assert that parts of the return value must be equivalent to another expression, provide a strong postcondition.On average, this category of postcondition kills 89% of all bugs and appears in 17.5% of labeled postconditions.</p>
<p>Using our categorization, we can partially explain the lower completeness scores of StarChat postconditions in section 3.2.2.While we do not perform a systematic qualitative analysis, we observe that the majority of correct StarChat postconditions are atomic Type Checks, which is the weakest postcondition type (see Table 3).This hypothesis is also validated by results of GPT-4, where in contrast, only 16% of generated postconditions are atomic Type Checks alone.Instead, the majority of Type Checks in GPT-4 are in conjoined statements with other atomic checks, which may explain the relatively higher average completeness scores between the two models.</p>
<p>RQ1 summary: Qualitative analysis of Postconditions for EvalPlus</p>
<p>We qualitatively identify nine atomic component categories of LLM-generated postconditions.While we observe minimal correctness differences, bug completeness varied significantly; the weakest postcondition type, Type Checks, killed only 14% of natural bugs on average while the strongest, Arithmetic Equality check, killed 82%, a 6x difference.</p>
<p>RQ2: CAN NL2POSTCOND HELP CATCH REAL WORLD BUGS?</p>
<p>Beyond understanding whether LLMs can capture natural language intent via executable postconditions, we also want to understand nl2postcond's real-world potential.To do so, we investigate the second motivating use case in Section 1.1: finding bugs in an existing code base.We evaluate nl2postcond's bug-catching potential using Defects4J [20], a benchmark of historical Java bugs.</p>
<p>RQ2-Research Methodology and Experimental Setup</p>
<p>We outline our methodology for evaluating the capabilities of postconditions to catch real-world bugs: we describe the target benchmark Defects4J, discuss prompt variations for Java, and provide our criteria for bug-discriminating postconditions.We model our approach after TOGA's approach [9], where the goal is to find specifications/tests that a user could have used to catch a bug as they fail on the buggy version, and succeed on the fixed version.</p>
<p>4.1.1Benchmark: Defects4J.For our experiments, we use Defects4J 2.0 [20], a well-known benchmark of 835 manually curated real-world bugs gathered from 17 Java projects.For each bug, the dataset contains a set of bug-reproducing test cases (trigger tests), and regression test cases which load the class in which the method under test is contained.Each bug in Defects4J also contains buggy and fixed versions of the code.We consider a postcondition to be test-set-correct if it passes all trigger and regression tests on the fixed version.</p>
<p>As our prompt leverages functional syntax introduced in Java 8 (see the postcondition in fig.5c as an example), we only consider the subset pf 525 bugs from Defects4J that are reproducible when compiled targeting Java 8.Each bug may involve changes to multiple functions, for which we each generate postconditions.In total 840 functions are modified across the 525 bugs.</p>
<p>Bug Discriminating Postconditions.</p>
<p>To evaluate whether LLM-generated postconditions are capable of catching real-world bugs, we instrument the buggy and fixed function versions with each associated postcondition.We consider a generated postcondition to be bug-discriminating if it satisfies the following criteria:</p>
<p>(1) The postcondition passes all the trigger and regression tests, on the fixed version of a function.</p>
<p>(2) The postcondition fails a a trigger test or regression test on the buggy version of a function.</p>
<p>The Defects4J benchmark ensures that the difference between the buggy and fixed versions is minimized to only changes related to the bug-fix.Therefore, assuming a comprehensive test suite, any discriminating postcondition satisfying the above criteria is related to the (bug-related) change for the example.Finally, similar to our qualitative evaluation for RQ1 (see Section 3.2.1)we qualitatively analyze bug-discriminating postconditions to gain greater insight.</p>
<p>Prompt Design and Ablations.</p>
<p>To generate postcondtions for buggy functions in the dataset, we use the same prompt as in RQ1 (see fig. 3).Designed as language agnostic, the only change Model Prompt has: NL Only = ✗ buggy code = ✓</p>
<p>Compiles</p>
<p>Test-set correct # distinguishable bugs @1 @5 @10 @1 @5 @10 GPT-4 needed to adapt the prompt for Defects4J is including additional code context.Given that Defects4J problems are extracted from real-world projects, functions are comparatively more complex than those in EvalPlus and are often tightly coupled with other project functions.Our initial investigations found that without some file-level context, LLMs rarely generate meaningful postconditions that also compile.Therefore, we include additional class and type-related context in the prompt.Given the limited context window of the LLMs used, we greedily include methods in the call graph for the buggy function (ordered by in-file placement) until the prompt tokens are exhausted.The call graph and in-file dependencies are determined using the Java language binding for Tree-sitter 4 .</p>
<p>For each buggy function, we combine of function and class-level in-file comments to formulate a natural language specification.In practice, this is primarily the buggy function's JavaDoc.We do not generate additional natural language (i.e., through code summarization) nor do we use external documentation: all natural language is pulled directly from the buggy function's source code file.</p>
<p>We choose to use only the simple prompt from RQ1, as it led to more correct postconditions than did the base version.Following the approach in RQ1, we report two variants of the prompt: 1) that only includes the natural language of the function 2) that includes both the natural language and the code of the buggy function body.For each variant, we generate 10 postconditions for every function modified between the buggy and fixed projects (840 functions across 525 unique bugs).</p>
<p>We choose to generate postconditions using two of the three earlier introduced models, used in RQ1: GPT-4 and StarChat.Given that GPT-4 and GPT-3.5 are comparable, closed-access chat models from OpenAI, we choose to focus on GPT-4 as it shows superior performance in RQ1.We choose to use StarChat as it is one of the few open-source chat-based models available.In total, we evaluate 33,600 postconditions (2 models * 2 ablations * 10 postconditions * 840 functions).</p>
<p>RQ2-Results: can LLM-generated postconditions catch real-world bugs?</p>
<p>We detail our findings on if nl2postcond postconditions are test-set correct, and if they can catch bugs in real-world industrial-scale projects.We find that even with the increased complexity over EvalPlus, GPT-4 is still able to produce correct postconditions for Defects4J at a high rate.In addition, both GPT-4 and StarChat are able to generate bug-discriminating postconditions for a subset of Defects4J bugs.All bug-discriminating postconditions were further analyzed via a qualitative analysis to gain insight into the ability of LLMs to catch bugs via nl2postcond.</p>
<p>4.2.1</p>
<p>Test-set correctness.Our full test-set correctness results for Defects4J are in table 4. We find that while lower than the results from EvalPlus, GPT-4 still generate a significant number of test-set correct postconditions with respect to the fixed version of a function (e.g., correct with respect to programmer intent), achieving accept@1 of up to 0.39 and accept@10 of up to 0.75.StarChat performs worse, with accept@1 and accept@10 of 0.12 and 0.56 respectively.We note that these numbers may be higher in practice if postconditions are filtered by those that compile (see table 4, Compiles column).In general, including the buggy code in the prompt leads to more test-set correct postconditions.This contrasts with the results from EvalPlus, where we did not observe a difference.We hypothesize that this is the case because of (a) the comments not being completely descriptive, and (b) the increased program and object complexity in Defects4J, as supported by the fact that postconditions are also less likely to compile when the buggy code is omitted from the prompt.4.2.2Bug-discriminating postconditions.We find that LLMs can generate postconditions that distinguish between buggy and fixed code in real-world projects with respect to regression and trigger tests.As seen in Table 4, GPT-4 was able to generate discriminating postconditions for up to 47/525 (9%) bugs.StarChat caught fewer, but still generated postconditions that distinguished up to 25 bugs.Across all prompt variants and models, we were able to generate a bug-discriminating postcondition for 70 buggy methods from 64 unique bugs in Defects4J, 12.2% of all bugs considered.</p>
<p>RQ2 summary: Correctness and bug catching power on Defects4J</p>
<p>We find that nl2postcond postconditions are often test-set correct for real-world functions (accept@10 up to 0.75) and can be powerful enough to catch real-world bugs (nl2postcond discriminates 70 buggy methods from 64 bugs in Defects4J).</p>
<p>4.2.3</p>
<p>Qualitative analysis of bug-discriminating postconditions.We conduct a qualitative evaluation of the bug-discriminating postconditions to gain insight into how nl2postcond postconditions discriminate real-world bugs.We observed additional evidence both motivating the potential usefulness of nl2postcond and examples of why LLMs may be a good tool to solve this problem.To communicate these findings, we detail two cases.</p>
<p>The first case is a historical bug from the Apache Commons CLI project. 5As shown in fig.5, the program should render multi-line text such that 1) white space padding is added at the beginning of every line after the first one and 2) that no line length exceeds a specified width.The requirement that each line should be width characters long is clearly specified in the Javadoc.However, the program sometimes incorrectly rendered lines longer than width due to a bug in the white space padding implementation.In our evaluation, GPT-4 generated multiple postconditions that catch this bug, including the example in 5c.These bug-catching postconditions were generated by both prompt variations.This example evidences both that 1) informal natural language can meaningfully telegraph code bugs and 2) modern LLMs, such as GPT-4, can sufficiently formalize natural language intent to capture the disagreement.Overall, this example shows the potential of nl2postcond to unearth coding inconsistencies solely from informal natural language documentation.</p>
<p>For our second example, we refer back to one of our initial examples motivating nl2postcond in Section 1.1, fig. 2. This example was adapted from Defects4J, and consists of a historical bug in another popular Apache library project, Commons Math. 6In this bug, a method returning a reversed instance of a mathematical Line object does not retain sufficient precision in its internal state.GPT-4 is again able to generate multiple postconditions that correctly detect this bug: both postconditions in fig. 2 are actual postconditions from our evaluation generated using the prompt with the buggy code included.As with the first example, this example demonstrates the potential of LLMs to generate postconditions powerful enough to capture real-world bugs.However, this example additionally provides evidence that LLMs in particular are helpful for realizing nl2postcond. 1 / ** Render the text and return the rendered Options in a StringBuffer. 2<em> @param width The number of characters to display per line 3 * @param nextTab The position on the next line for the first tab. 4</em> @param text The text to be rendered.* / (b) Bug report indicating that the function sometimes erroneously renders text with more than width characters per line, behavior that directly conflicts with the Javadoc.</p>
<p>The method... has couple of bugs in the way that it deals with the [nextTab] variable.This causes it to format every line beyond the first line by [nextTab] too many characters beyond the specified width.</p>
<p>(c) Bug catching nl2postcond postcondition generated by GPT-4.rVal is the function return value.This postcondition was generated without the buggy function code in the prompt.</p>
<p>1 // Checks if the rendered text does not exceed the specified width per line 2 assert rVal.toString().lines().allMatch(line−&gt; line.length()&lt;= width);</p>
<p>(d) Bug-catching test from TOGA where TOGA expects this test prefix to through a RuntimeException.While this catches the bug, it is semantically removed from the bug's root cause.where the bug can be caught via nl2postcond.Cli 8 is a bug in the implementation for calculating the width of lines when wrapping output text.The natural language function description specifically says that each line must be at most width characters long.GPT-4 translates this intent into the provided postcondition, which correctly catches the bug.</p>
<p>Both postconditions detect the bug by leveraging general mathematical knowledge about the properties of a reversed line.The second postcondition in particular exemplifies the ability of LLMs to dynamically combine methods such as dotProduct from the project file's context with algebraic world knowledge that is external to the project's code.</p>
<p>Baseline comparison: TOGA, Daikon</p>
<p>To contextualise our results, we provide an empirical and qualitative comparison of the effectiveness of nl2postcond with respect to two other popular methods of inferring test oracles and invariant specifications.We choose a state-of-the-art technique for each: (a) TOGA [9], a neural approach to generating test oracles, and (b) Daikon [10], a popular technique to infer program invariants (including method postconditions) from multiple dynamic executions.There exists related efforts on generating unit tests neurally such as AthenaTest [50] but no public release exists for evaluating it for our setup. 7We focus our comparison on Defects4J and the ability of each technique to generate correct tests or postconditions that distinguish historical bugs: to the best of our knowledge, neither TOGA nor Daikon support Python (and thus are not compatible with EvalPlus).</p>
<p>TOGA.</p>
<p>TOGA is a neural approach to generating test oracles for a focal method.Given a test prefix, TOGA generates an assertion or expected exception that the test prefix is expected to satisfy. 1 \old(in.getClass().getName())== java.io.BufferedInputStream.class.getName()Fig. 6.TOGA test oracle and Daikon postcondition for a historical bug caught by both TOGA and Daikon, but not by nl2postcond (Compress 11).This bug involved incorrectly processing files less than 512 bytes files as tar archives, and it was fixed by throwing an exception.</p>
<p>Although both approaches can generate assertions that may not agree with the implementation of a focal method, there are fundamental differences between test oracle generation (as in TOGA) and specification generation (as in nl2postcond).nl2postcond infers method postconditions that are expected to hold for all inputs.These can not only be checked during testing, but also at runtime on unseen inputs and trigger assertion failures instead of producing corrupted values; TOGA generated assertions can only be applied at testing time, since the assertions apply to the specific test prefix that reaches the buggy location.On the other hand, there may be some (algebraic) specifications that are best expressed over multiple method calls (e.g., s.pop(s.push(5))== 5 for a stack object s and specific values of inputs such as 5); expressing such a specification as a method postcondition (for either s.pop or s.push for even a single value 5) will require adding auxiliary ghost variables.Finally, assertions in test oracles are most often equalities (to match the expected output value on the specific input), whereas the assertions in method postconditions can be arbitrary Boolean expressions to capture all possible output values (see Table 3 for some examples).</p>
<p>Setup.We compare nl2postcond's results on Defects4J with the results reported on TOGA [9].To enable TOGA to catch bugs without access to the failing trigger test, Dinella et al. integrated TOGA with EvoSuite [12], a popular automated testing tool.We used the set of 57 bugs found by TOGA (by reproducing their experimental setup released as a docker), each accompanied by a EvoSuite-generated test prefix and the corresponding test oracle.Of the 57 bugs reported by TOGA, 15 bugs were excluded from our nl2postcond evaluation due to Java limitations (Section 4.1.1).</p>
<p>Evaluation Results.Overall, we find that the bug finding capabilities of TOGA and nl2postcond are complementary.Of the 101 distinct bugs caught by at least one approach, only 5 are caught by both nl2postcond and TOGA. 837 are only caught by TOGA, while 59 are only caught by nl2postcond.To better understand the differences between the two techniques, we conduct a qualitative evaluation of bugs caught by at least one approach.We note the following observations:</p>
<p>(a) Example bug-catching post conditions generated by nl2postcond which correctly asserts that the domain of a continuous distribution function should be greater than or equal zero.This postcondition catches a large number of bug-triggering inputs for this method.[10] uses multiple program runs to dynamically infer program invariants, including postconditions.Unlike nl2postcond, Daikon invariants are always implementationconsistent (it only retains expressions that are true across tested executions) and can only be generated from testable code (e.g., can not be generated from natural language alone).</p>
<p>Daikon. Daikon</p>
<p>Setup.We used Daikon to generate likely invariants from running the set of regression tests (without any failing trigger tests) on the buggy version.We then check if these specifications are bug-discriminating.We run Daikon using standard parameters for each buggy method to generate a set of likely postconditions.Due to challenges integrating Daikon with several of the projects in Defects4J, we scope our evaluation to the 101 bugs found by either nl2postcond or TOGA.</p>
<p>Results.Overall, we find that while Daikon generates many postconditions that are consistent with all tests, bug-discriminating postconditions are rare.Daikon generated postconditions for an associated buggy method for the majority of tested bugs (78/101).For the rest, Daikon either failed to generate any method postconditions on the buggy version using just the regression tests (17/101), or timed out after 10 minutes (6/101).The number of postconditions generated for any given method varied widely.However, we observed only three instances of a Daikon-generated postcondition that is bug-discriminating.Daikon finds one bug that is not found by nl2postcond, but the other two specifications are incorrect.Fig. 6b shows the case where Daikon is able to catch a bug that nl2postcond does not, by detecting a class name change instigated through a factory function.For the remaining two cases, the bug-discriminating postcondtions overfit the regression tests and do not hold for all inputs.For example, the specification for Math 95 in Fig. 7b) states that a return value should be close to one of two values {1.0, 1.5}.However, the fixed program admits many more positive return values; this is correctly reflected by the nl2postcond postcondition in Fig. 7a.In general, we observe that Daikon generated invariants are either very weak (e.g., a field is not modified), or are incorrect (do not generalize to all inputs).To the best of our understanding of Defects4J, this is in contrast to the majority of bug-discriminating nl2postcond postconditions.</p>
<p>Baseline Comparison with TOGA and Daikon</p>
<p>Compared to two other approaches, we find that nl2postcond postconditions are either more widely applicable or find more bugs.We also note that the bugs found via nl2postcond are largely non-overlapping with those found by TOGA, indicating that the two approaches may be complementary.nl2postcond finds many more bugs compared to Daikon, which usually generates invariants that overfit the observed executions.</p>
<p>RELATED WORK</p>
<p>Specification Generation.A specification provides a comprehensive description of a program's intended behavior, encompassing the functional relationships between inputs and outputs, as well as the internal state dynamics.Specifications may vary in formality, ranging from informal descriptions such as API documentation to formal representations like test cases or runtime assertions.The applications of program specifications are extensive and include bug identification [1,18], verification [5,31], specification-driven development [26,35,40], code comprehension [4].Our goal is to generate formal and functional specifications in the form of postconditions, articulating the desired input-output relationship of a code, given the informal natural language description.There has been a long line of work for automatically inferring specifications using static analysis [43], abstract interpretation [7], dynamic analysis [10], and so on.While most of these existing works rely on a code implementation inferring the specification of existing code, our approach is to infer the desired behavior of the code from natural language.Similar to us, several approaches attempted to generate specification by analyzing API documentation or code comments using different natural language processing techniques such as named pattern matching [37,[46][47][48], text normalization [3], entity recognition [56], natural language parsing [58], etc. Being dependent on mostly hand-crafted rules and heuristics, most of these techniques only work on the semi-structured natural language format of the input and are not easily extensible across different programming languages and domains.In contrast, our technique relies on LLMs for world knowledge and our experiment shows the extensibility of our technique in two different languages -Python and Java.</p>
<p>Machine Learning for Specifications.Machine Learning approaches for specification generation have shown promise in several orthogonal directions, including synthesizing test oracles [9,30,51], improving test coverage [27], generating unit tests [23,49] and so on.Depending on the application scenario, the specifications generated by these approaches are dependent on different inputs.AthenaTest [49] generates both the input and the oracle of a unit test from the implementation of the focal method (recall TOGA only generates test oracle).Closer to our work, TiCoder [23] leverages LLM to generate test input and output to formalize the user intent.While these approaches focus on generating concrete test cases (and potentially oracles), our approach is geared toward generating abstract functional relationships between the input and output of a procedure, which allows us to reason about a range of inputs.Similar to our work, EvoSpex [32] generates functional relationships of input-output with evolutionary learning.While their approach is aimed at summarizing existing program behavior (and therefore cannot be used to find bugs), our approach contributes towards generating formal specifications of desired input-output behavior.Recent work by Vikram et al. [52] proposes to leverage LLM for generating property-based tests (PBTs).Speculyzer [21] uses LLMs to enumerate likely properties and inputs similar to PBT, but use them as heuristics to improve code generation.Unlike our work, they do not seek to evaluate the correctness and completeness of these specifications.In addition to the input-output specification generation, machine learning has been applied to generate intermediate specifications of a code such as invariants, using traditional machine learning [13,42], deep learning [41,55], and LLMs [38].</p>
<p>LIMITATIONS AND THREATS TO VALIDITY</p>
<p>LLM-Related Approach Limitations.We note that there are several inherent weaknesses of our approach relating to the use of LLMs.In particular, we note that as we are using popular LLMs as a black box, the underlying model is not well understood.This can lead to a lack of interpretability of the results, as well as raise questions regarding result generalizability.In addition, due to the quickly evolving AI landscape, the results may become obsolete quickly.We consider some specific instances of these limitations in the rest of this section.</p>
<p>Data leakage.One potential concern to the generalizability of the study is the use of popular benchmarks EvalPlus and Defects4J which are included in The Stack [22], the dataset used to train StarChat, and may have been included as part of training datasets for both GPT-3.5 and GPT-4.The risk of data leakage could pose a threat to the internal validity of our study.Nevertheless, this concern is partially mitigated by the target task: the use of models to produce postconditions, which are not artifacts of either dataset.To our knowledge, postconditions have not been previously generated as part of any public-facing dataset.</p>
<p>Stability of models' output.Two of the models used in the experiments are accessed using OpenAI web APIs.OpenAI models are not open-access and are often updated or deprecated.This poses a threat to the replicability of our study.To mitigate this threat, we make available all postconditions generated by the closed-access models.We also use the open-access StarChat, and share all generated artifacts.In addition, we report results using the widely adopted metric accept@k, which accounts for the stochasticity of model output.</p>
<p>Generalization of findings.Given the relatively small number of bugs (525) considered in the Defects4J benchmark, our findings may not generalize to arbitrary bugs across different languages and repositories.We partially mitigate this threat by using real-world bugs from open-source projects and evaluating the capabilities of LLMs on both Python and Java benchmarks.In addition, the proposed taxonomy of postconditions (Section 3.2.3) is representative of only the programs in the EvalPlusbenchmark and may not generalize across languages or program complexities.</p>
<p>Measure of postcondition completeness.Our metric for postcondition completeness relies on a set of generated code mutants.The code mutants are generated to cover the space of possible bugs in the target function, however, the set of code mutants generated per problem will never represent a comprehensive set of possible bugs.Therefore, our measure of completeness is dependent on the range and quality of bugs covered in the set of mutants.This poses a threat to the internal validity of our study.To mitigate this threat we maximize the diversity of bugs by retaining only distinct mutants, and generate up to 233 buggy codes per problem.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce and define nl2postcond as the problem of translating natural language comments into programmatically checkable postconditions via LLMs.Our work proposes and validates metrics for assessing the correctness and completeness of postconditions derived from natural language, offering an initial step in systematizing the nl2postcond problem.Through an empirical and qualitative evaluation on two benchmarks, we find that LLMs are adept at translating natural language descriptions to formulate non-trivial postconditions that accurately capture programming intent.Our study also finds that LLM-generated postconditions can exhibit high discriminative power: we generate postconditions via nl2postcond that are able to discriminate 64 real-world historical bugs from industrial-scale Java projects.These findings underscore the feasibility and promise of leveraging natural language documentation into executable specifications.Our research highlights the possibility of LLMs acting as a bridge between informal language descriptions and formal code specifications, such that natural language comments can be used effectively to improve software validation and bug detection.</p>
<p>1Fig. 1 . 3 final
13
Fig. 1.Example of how postconditions could be used to clarify ambiguous natural language specifications.</p>
<p>Fig. 2 .
2
Fig. 2. Example of how postconditions or other formal specifications of program behavior could catch bugs.This example is a historical bug from Defects4J (Math-9): the Line constructor does not return a new line with enough precision.The postconditions were generated by GPT-4 in our evaluation, and both catch the bug.</p>
<p>bug-completeness-score(P, CM, T ) | post ∈P {c ∈ CM | ∃i ∈ T : eval(post, (i, c(i))) == false}|/|CM |</p>
<p>Fig. 4 .
4
Fig. 4. Example of how the base and simple prompt variations can impact postcondition construction.Both postconditions were generated for HumanEval problem 12 using GPT-4.</p>
<p>(a) Buggy function stub and javadoc.</p>
<p>5</p>
<p>StringBuffer renderWrappedText(StringBuffer sb, int width, int nextTab, String text);</p>
<p>1</p>
<p>public void test27() throws Throwable { 2 HelpFormatter helpFormatter0 = new HelpFormatter(); 3 MockPrintWriter mockPrintWriter0 = new MockPrintWriter("−");</p>
<p>4 helpFormatter0.Fig. 5 .
45
Fig.5.Example from Defects4J (Cli project, bug 8) where the bug can be caught via nl2postcond.Cli 8 is a bug in the implementation for calculating the width of lines when wrapping output text.The natural language function description specifically says that each line must be at most width characters long.GPT-4 translates this intent into the provided postcondition, which correctly catches the bug.</p>
<p>3 ByteArrayInputStream
3
(a) TOGA test oracle that catches Compress 11.TOGA finds the bug by simulating a small file and then explicitly catching the resulting exception.1 public void test16() throws Throwable { 2 byte[] byteArray0 = new byte[179]; org.apache.commons.compress.archivers.ArchiveStreamFactory", e); 10 }} (b) Daikon postcondition distinguishes Compress 11.It does so as the buggy function involves a using a ArchiveStream factory function that can change the class name of the Input Stream class.</p>
<p>Table 1 .
1
Test-set correctness on EvalPlus for three models (GPT-3.5,GPT-4, and StarChat), differing prompt complexities (base vs. simple), and including or omitting the reference solution in the prompt.Darker highlighted cells are more correct.Bolded values are the largest for a specific model.
Prompt has:ModelPromptNL Only=✗ ref code=✓Accept @ 1Accept @ 5Accept @ 10x/164 correctGPT-3.5base✗0.460.800.87143GPT-3.5base✓0.490.810.88145GPT-3.5simple✗0.550.820.87143GPT-3.5simple✓0.560.820.88144GPT-4base✗0.630.830.88144GPT-4base✓0.710.890.91150GPT-4simple✗0.770.940.96158GPT-4simple✓0.760.920.96157StarChatbase✗0.210.610.82134StarChatbase✓0.200.590.77126StarChat simple✗0.250.690.85139StarChat simple✓0.230.670.86141
3.2.1 Postcondition Correctness.Table1has our test-set-correctness (Section 2.1.1)results.Overall, we find that for EvalPlus, LLM-generated postconditions are likely to be test-set correct; in our best-performing prompt variation, 77% of postconditions were test-set-correct and a test-set-correct postcondition was generated for 96% of problems (158/164).As we show later in this section, testset-correctness on EvalPlus largely corresponds to true correctness.Our results indicate that LLMs have the potential to reliably generate correct postconditions from natural language specifications.</p>
<p>Table 2 .
2
Table of bug-completeness for EvalPlus.% bug-complete is the % of postconditions that detect all buggy code mutants.% problems with bug-complete is the % of all EvalPlus problems with at least one bug-complete postcondition.% problems union bug-complete is the % of problems where the union of correct postconditions is bug-complete.Finally, the last two columns are the average bug-completeness-score, a fraction between 0 and 1, for all correct postconditions, normalized by EvalPlus problem.We report this for both natural and all generated code mutants.Bolded values are the largest value per column per model.</p>
<p>Table 3 .
3
Atomic categories of nl2postcond postconditions that are often combined by LLMs via &amp;&amp; (logical and).return _ val refers to the function's return value.% test-set correct and bug-completeness columns are defined in Section 2.1.Example postconditions are adapted from our EvalPlus results, only modified for space.
Avg. Bug-complete-scoreCategoryExample Postconditon% Prevalent(Natural/All)Type Check Format Check Arithmetic Bounds Arithmetic Equality Container Property Element Property Forall-Element Property Implication Null Checkisinstance(return _ val, int) return _ val.startswith("ab") return _ val &gt;= 0 return _ val[0] == 2 * input _ val len(return _ val) &gt; len(input _ val) return _ val[0] % 2 == 0 all(ch.isalpha() for ch in return _ val) (return _ val==False) if 'A'not in string return _ val is not None47.4 11.2 30.8 17.5 27.0 12.6 8.3 12.7 4.40.14 / 0.27 0.43 / 0.57 0.23 / 0.34 0.82 / 0.89 0.45 / 0.57 0.39 / 0.53 0.23 / 0.44 0.58 / 0.64 0.40 / 0.50Average0.32 / 0.46</p>
<p>Table 4 .
4
Table containing our Defects4J results for postconditions generated for 840 methods across 525 historical bugs.We report the likelihood of generated postconditions to compile, and the likelihood that they pass all tests when instrumenting the fixed function (test-set correct columns).# distinguishable bugs is the number of bugs for which at least one generated postcondition was discriminating (see Section 4.1.2).
✗0.65 0.860.89 0.32 0.570.6635GPT-4✓0.73 0.90 0.93 0.39 0.66 0.7547StarChat✗0.25 0.680.83 0.11 0.380.5519StarChat✓0.29 0.72 0.84 0.12 0.39 0.5624
https://issues.apache.org/jira//browse/MATH-938</p>
<h1>Base prompt: postcondition that incorrectly attempts to fully specify the problem</h1>
<p>assert all(map(lambda x: x == "_" or x == "−", re.split(r'\w+', return_value))) and " " not in return_value and "__" not in return_value and "−−" not in return_value</p>
<h1>Simple prompt: postcondition checks that return_value does not contain any spaces.</h1>
<p>assert not re.search(r' {1,}', return_value), "The return value contains one or more spaces"
FSE'24, July 15-19, 2024, Porto de Galinhas, Brazil M. Endres, S. Fakhoury, S. Chakraborty, S. Lahiri
HuggingFace model identifier HuggingFaceH4/starchat-alpha
We also considered additional temperatures of 0.2 and 1.2, and we include the results in our replication package. However, since high-level trends were the same regardless of temperature, we only report results of 0.7 here for clarity.
https://tree-sitter.github.io/tree-sitter/
Project page: https://commons.apache.org/proper/commons-cli/, Bug: https://issues.apache.org/jira/browse/CLI-151
Project page: https://commons.apache.org/proper/commons-math/, Bug: https://issues.apache.org/jira/browse/MATH-938
Personal communication with the author of AthenaTest.
The 5 in common are Cli 8, Cli 32, JacksonCore 8, Jsoup 88, and Math</p>
<p>// Checks if the returnValue is greater than or equal to zero
DATA AVAILABILITYWe plan to make a replication package publicly available in near future with our postcondition generation scripts and prompts, postcondition evaluation harness, and qualitative codebook.We also plan to make all generated postconditions available, along with the results of additional temperature ablations.Finally, we will include the unique natural and artificial LLM-generated mutants for EvalPlus.(b) Daikon postcondition that distinguishes Math 9, but overfitts to the regression tests.1 daikon.Quant.fuzzy.eq(\result,1.000020000400008) || daikon.Quant.fuzzy.eq(\result,1.5)(c)Math 95 from Defects4J: This function returns a domain for use by an Inverse Cumulative Probability function.The buggy version did not have sufficient bounds on getDenominatorDegreesOfFreedom, leading to a potential negative domain (impossible for a cumulative Probability function) or a division by zero error.Highlighted tokens are those that were added for the fixed version.1 / ** Access the initial domain value, based on <code>p</code>, used to 2 * bracket a CDF root.This method is used by 3 * {@link #inverseCumulativeProbability(double)} to find critical values.4 * @param p the desired probability for the critical value 5 * @return initial domain value * / • A majority of nl2postcond-caught bugs (52/64) could not be found by TOGA, due to the lack of any EvoSuite generated test prefix that reaches the bug location.This includes Math 9, one of the two Defects4J bugs we use to motivate nl2postcond (see fig.2).This demonstrates the usefulness of nl2postcond's ability to be checked at runtime on unseen inputs.• A majority of TOGA-caught bugs are "exceptional" bugs where the buggy code either throws an unexpected exception or fails to throw an expected exception.Since we do not currently model exceptional postconditions in nl2postcond, we fail to find most of these bugs.Fig.6shows how leveraging a model for predicting exceptional postconditions enables TOGA to catch bugs that nl2postcond does not.Incorporating exceptional postconditions into nl2postcond is an intriguing direction for future work.Beyond exceptional postconditions, we also find that TOGA can model test prefixes that involve objects from different classes and methods (similar to the stack example with push and pop).• For the 5 common bugs, we observe that TOGA and nl2postcond find the same underlying bug with different means.For example, one of our motivating examples for nl2postcond, Cli 8, is also caught by a TOGA test oracle.While both are helpful, nl2postcond's assertion directly captures the semantics of the root cause of the bug (useful for both fault localization and patch construction).TOGA, however, provides a higher-level end-to-end test that is more removed from the buggy method, necessitating the developer spend additional time for root cause analysis.We present both bug catches for Cli 8 in fig.5d.
On the automation of fixing software bugs. Andrea Arcuri, Companion of the 30th international conference on Software engineering. 2008</p>
<p>Amazon CodeWhisperer. Aws Amazon, 2023. September 27, 2023</p>
<p>Translating code comments to procedure specifications. Arianna Blasi, Alberto Goffi, Konstantin Kuznetsov, Alessandra Gorla, D Michael, Mauro Ernst, Sergio Delgado Pezzè, Castellanos, Proceedings of the 27th ACM SIGSOFT international symposium on software testing and analysis. the 27th ACM SIGSOFT international symposium on software testing and analysis2018</p>
<p>Formal specifications in software maintenance: From code to Z++ and back again. Jonathan P Bowen, Peter T Breuer, Kevin C Lano, Information and Software Technology. 351993. 1993</p>
<p>Beyond assertions: Advanced specification and verification with JML and ESC/Java2. Patrice Chalin, Gary T Joseph R Kiniry, Erik Leavens, Poll, Formal Methods for Components and Objects: 4th International Symposium. Amsterdam, The NetherlandsSpringer2006. 2005. November 1-4, 2005Revised Lectures 4</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>An abstract interpretation framework for refactoring with application to extract methods with contracts. Radhia Patrick M Cousot, Francesco Cousot, Michael Logozzo, Barnett, Proceedings of the ACM international conference on Object oriented programming systems languages and applications. the ACM international conference on Object oriented programming systems languages and applications2012</p>
<p>The strongest postcondition. Predicate Calculus and Program Semantics. W Edsger, Dijkstra, Carel S Scholten, 1990. 1990</p>
<p>Toga: A neural method for test oracle generation. Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, Shuvendu, Lahiri, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Dynamically discovering likely program invariants to support program evolution. Jake Michael D Ernst, William G Cockrell, David Griswold, Notkin, Proceedings of the 21st international conference on Software engineering. the 21st international conference on Software engineering1999</p>
<p>Sarah Fakhoury, Saikat Chakraborty, Madan Musuvathi, Shuvendu, Lahiri, arXiv:2304.03816Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions. 2023. 2023arXiv preprint</p>
<p>Evosuite: automatic test suite generation for object-oriented software. Gordon Fraser, Andrea Arcuri, Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering. the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering2011</p>
<p>Learning invariants using decision trees and implication counterexamples. Pranav Garg, Daniel Neider, P Madhusudan, Dan Roth, 10.1145/2837614.2837664Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. Rastislav Bodík, Rupak Majumdar, the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming LanguagesSt. Petersburg, FL, USAACM2016. 2016. January 20 -22, 2016</p>
<p>Github, GitHub Copilot. 2023. September 27, 2023</p>
<p>Automatic generation of oracles for exceptional behaviors. Alberto Goffi, Alessandra Gorla, Michael D Ernst, Mauro Pezzè, Proceedings of the 25th international symposium on software testing and analysis. the 25th international symposium on software testing and analysis2016</p>
<p>Understanding source code comments at large-scale. Hao He, Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering2019</p>
<p>Henry Hsu, Peter A Lachenbruch, Paired t test. Wiley StatsRef: statistics reference online. 2014. 2014</p>
<p>Aspect, a formal specification language for detecting bugs. Daniel Jackson, 1992Ph. D. Dissertation. Citeseer</p>
<p>An analysis and survey of the development of mutation testing. Yue Jia, Mark Harman, IEEE transactions on software engineering. 372010. 2010</p>
<p>Defects4J: A database of existing faults to enable controlled testing studies for Java programs. René Just, Darioush Jalali, Michael D Ernst, Proceedings of the 2014 international symposium on software testing and analysis. the 2014 international symposium on software testing and analysis2014</p>
<ol>
<li>I speak, you verify: Toward trustworthy neural program synthesis. Darren Key, Wen-Ding Li, Kevin Ellis, arXiv:2210.008482022arXiv preprint</li>
</ol>
<p>Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, arXiv:2211.15533The stack: 3 tb of permissively licensed source code. 2022. 2022arXiv preprint</p>
<p>Aaditya Shuvendu K Lahiri, Georgios Naik, Piali Sakkas, Choudhury, Madanlal Curtis Von Veh, Jeevana Musuvathi, Chenglong Priya Inala, Jianfeng Wang, Gao, arXiv:2208.05950Interactive code generation via test-driven user-intent formalization. 2022. 2022arXiv preprint</p>
<p>Can language models learn from explanations in context?. Andrew K Lampinen, Ishita Dasgupta, C Y Stephanie, Kory Chan, Michael Henry Matthewson, Antonia Tessler, James L Creswell, Jane X Mcclelland, Felix Wang, Hill, arXiv:2204.02329[cs.CL]2022</p>
<p>Dafny: An automatic program verifier for functional correctness. M Rustan, Leino, Logic for Programming, Artificial Intelligence, and Reasoning: 16th International Conference, LPAR-16. Revised Selected Papers. Dakar, SenegalSpringer2010. April 25-May 1, 201016</p>
<p>Contract driven development= test driven development-writing test cases. Andreas Leitner, Ilinca Ciupa, Manuel Oriol, Bertrand Meyer, Arno Fiva, Proceedings of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering. the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering2007</p>
<p>CODAMOSA: Escaping coverage plateaus in test generation with pre-trained large language models. Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, Siddhartha Sen, International conference on software engineering (ICSE). 2023</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161StarCoder: may the source be with you!. 2023. 2023arXiv preprint</p>
<p>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, 2023. 2023202337th cconference on Neural Information processing Systems</p>
<p>Using transfer learning for code-related tasks. Antonio Mastropaolo, Nathan Cooper, David Nader Palacio, Simone Scalabrino, Denys Poshyvanyk, Rocco Oliveto, Gabriele Bavota, IEEE Transactions on Software Engineering. 492022. 2022</p>
<p>The Spec# programming system: An overview. Mike, K Rustan M Leino, Wolfram, Construction and Analysis of Safe, Secure, and Interoperable Smart devices (CASSIS). Lecture Notes in Computer Science. 20043362</p>
<p>EvoSpex: An evolutionary algorithm for learning postconditions. Facundo Molina, Pablo Ponzio, Nazareno Aguirre, Marcelo Frias, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.13474Codegen: An open large language model for code with multi-turn program synthesis. 2022. 2022arXiv preprint</p>
<p>Demystifying GPT Self-Repair for Code Generation. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Solar-Lezama, arXiv:2306.098962023. 2023arXiv preprint</p>
<p>Agile specification-driven development. Jonathan S Ostroff, David Makalsky, Richard F Paige, International Conference on Extreme Programming and Agile Processes in Software Engineering. Springer2004</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Inferring method specifications from natural language API descriptions. Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie, Stephen Oney, Amit Paradkar, 2012 34th international conference on software engineering (ICSE). IEEE2012</p>
<p>Can Large Language Models Reason about Program Invariants?. Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, Pengcheng Yin, 2023. 2023</p>
<p>What constitutes software? An empirical, descriptive study of artifacts. Rolf-Helge Pfeiffer, Proceedings of the 17th International Conference on Mining Software Repositories. the 17th International Conference on Mining Software Repositories2020</p>
<p>Formal specification-driven development. Richard Rutledge, Sheryl Duggins, Dan Lo, Frank Tsui, Proceedings of the International Conference on Software Engineering Research and Practice (SERP). The Steering Committee of The World Congress in Computer Science. the International Conference on Software Engineering Research and Practice (SERP). The Steering Committee of The World Congress in Computer Science20141</p>
<p>CLN2INV: Learning Loop Invariants with Continuous Logic Networks. Justin Gabriel Ryan, Jianan Wong, Ronghui Yao, Suman Gu, Jana, International Conference on Learning Representations. 2020</p>
<p>From invariant checking to invariant inference using randomized search. Rahul Sharma, Alex Aiken, Formal Methods in System Design. 482016. 2016</p>
<p>Static specification mining using automata-based abstractions. Sharon Shoham, Eran Yahav, Stephen Fink, Marco Pistoia, Proceedings of the 2007 International Symposium on Software Testing and Analysis. the 2007 International Symposium on Software Testing and Analysis2007</p>
<p>Secure Distributed Programming with Value-Dependent Types. Nikhil Swamy, Juan Chen, Cédric Fournet, Pierre-Yves Strub, Karthikeyan Bhargavan, Jean Yang, 10.1145/2034773.2034811Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming. the 16th ACM SIGPLAN International Conference on Functional ProgrammingTokyo, Japan; New York, NY, USAAssociation for Computing Machinery2011</p>
<p>Tabnine Code Completion. Tabnine, 2023. September 27, 2023</p>
<p>/<em> iComment: Bugs or bad comments?</em>. Lin Tan, Ding Yuan, Gopal Krishna, Yuanyuan Zhou, Proceedings of twenty-first ACM SIGOPS symposium on Operating systems principles. twenty-first ACM SIGOPS symposium on Operating systems principles2007</p>
<p>aComment: mining annotations from comments and code to detect interrupt related concurrency bugs. Lin Tan, Yuanyuan Zhou, Yoann Padioleau, Proceedings of the 33rd international conference on software engineering. the 33rd international conference on software engineering2011</p>
<p>@ tcomment: Testing javadoc comments to detect comment-code inconsistencies. Shin Hwei Tan, Darko Marinov, Lin Tan, Gary T Leavens, 2012 IEEE Fifth International Conference on Software Testing, Verification and Validation. IEEE2012</p>
<p>Unit Test Case Generation with Transformers and Focal Context. Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Shao Kun Deng, Sundaresan, 10.48550/ARXIV.2009.056172020</p>
<p>Unit Test Case Generation with Transformers and Focal Context. Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Shao Kun Deng, Sundaresan, arXiv:2009.05617[cs.SE]2021</p>
<p>Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers. Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Sundaresan, 10.1145/3524481.3527220IEEE/ACM International Conference on Automation of Software Test, AST@ICSE 2022. Pittsburgh, PA, USAACM/IEEE2022. May 21-22, 2022</p>
<p>Can Large Language Models Write Good Property-Based Tests?. Vasudev Vikram, Caroline Lemieux, Rohan Padhye, arXiv:2307.043462023. 2023arXiv preprint</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022. 2022</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903[cs.CL]Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2023</p>
<p>Learning nonlinear loop invariants with gated continuous logic networks. Jianan Yao, Gabriel Ryan, Justin Wong, Suman Jana, Ronghui Gu, Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation. the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation2020</p>
<p>Inferring resource specifications from natural language API documentation. Hao Zhong, Lu Zhang, Tao Xie, Hong Mei, 2009 IEEE/ACM International Conference on Automated Software Engineering. IEEE2009</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, arXiv:2205.10625[cs.AI]Quoc Le, and Ed Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. </p>
<p>Analyzing APIs documentation and code to detect directive defects. Yu Zhou, Ruihang Gu, Taolue Chen, Zhiqiu Huang, Sebastiano Panichella, Harald Gall, 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE2017</p>            </div>
        </div>

    </div>
</body>
</html>
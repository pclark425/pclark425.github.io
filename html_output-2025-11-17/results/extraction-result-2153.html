<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2153 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2153</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2153</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-278788499</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16938v1.pdf" target="_blank">N OVEL S EEK : When Agent Becomes the Scientist – Building Closed-Loop System from Hypothesis to Verification</a></p>
                <p><strong>Paper Abstract:</strong> Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce N OVEL S EEK , a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. N OV - EL S EEK highlights three key advantages: 1) Scalability : N OVEL S EEK has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity : N OVEL S EEK provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency : N OVEL S EEK has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, the Pearson correlation coefficient rose from 0.65 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2153.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2153.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOVELSEEK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NOVELSEEK (Unified Closed-loop Multi-agent Framework for Autonomous Scientific Research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM- and tool-driven closed-loop system that generates, evolves, implements, and experimentally validates research ideas across 12 diverse scientific tasks using automated literature/code survey, idea generation/evolution, methodology construction, automated coding/debugging, multi-round experiment planning, and human-interactive feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NOVELSEEK</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid multi-agent system (LLMs + code-generation/debug agents + orchestration + human-in-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific research (chemistry, molecular dynamics, power systems, time series, genomics, vision, point-cloud, NLP, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>research ideas/hypotheses, methodological designs, implementation code, model architecture proposals, and optimization strategies</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>multi-pronged: automatic implementation and execution of generated methods (coder agents Aider/OpenHands + Claude-3.7-Sonnetto), iterative experiment planning and adaptive evolution (multiple runs + performance assessment), automated exception-guided debugging, quantitative evaluation on domain-specific metrics (e.g., R^2, MAE, MIoU, mAP), and human expert evaluation of idea soundness/novelty; pipelines support end-to-end experimental validation including repo-level runs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>combination of automated Assessment Agent multidimensional scoring (coherence, credibility, verifiability, novelty, alignment; 0–10 per dimension with weighted sum) and human expert ratings (soundness, contribution, overall, confidence) over batches of generated ideas; also promotes diversity among top-ranked ideas</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>demonstrated quantitative improvements over baselines across tasks (examples: Reaction Yield Prediction improved from baseline 24.2±4.2 to NOVELSEEK 34.8±1.1 R^2 in few-shot setting; Enhancer Activity Prediction from 0.65 to 0.79 PCC; 2D semantic segmentation from 78.8% to 81.0% mIoU). Reported high ability to produce implementable ideas: many proposals produced runnable code (repo-level support) with varying success rates per task.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation performed via executed experiments and domain metrics; NOVELSEEK shows higher validation success and performance-gain rates compared to baselines and prior auto-research systems (e.g., higher percentage of experiments with performance gains vs DOLPHIN), but performance varies by task (e.g., execution success: AutoPCDet ~50%, Auto2DSeg ~90% reported); precise per-task metric improvements are given in tables but no global statistical false-positive/negative metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>The paper reports that highly novel ideas are challenging to implement and validate, but NOVELSEEK's idea-to-methodology construction, multi-round planning/adaptive evolution, and human-interactive feedback substantially mitigate this: novelty can reduce initial execution success, but iterative debugging and refinement increase validated gains; adaptive evolution increased the percentage of ideas that improved performance (e.g., AutoRYP improved from 20% to 40% with AE).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Evidence of asymmetry: generating diverse/novel ideas is achievable, but reliably implementing and validating them is harder — prior systems (e.g., AI-Scientist-V2) produced ideas harder to implement; NOVELSEEK reduces but does not eliminate this gap via methodology construction, Assessment Agent scoring, exception-guided debugging, and adaptive evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Qualitative/generalization claim: NOVELSEEK was applied across 12 heterogeneous tasks (scientific and AI tasks) and improved performance in many, indicating robust cross-domain generalization; however, no formal OOD benchmark metrics were provided. Performance and execution success vary by task complexity (repo-level tasks more costly and less universally successful).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantitatively reported; system includes Assessment Agent scores and human confidence labels but no calibration/uncertainty calibration metrics (e.g., reliability diagrams) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Reported concrete costs: idea-generation cost ≈ $0.6 per idea using GPT-4o; coder-debug run cost ranges (example: single-file runs <$1, repo-level ~ $1.2 using Claude Sonnet); training time per task reported in A100 GPU hours (examples: AutoRYP 6.0 A100h, AutoVLM 192 A100h). NOVELSEEK is reported significantly more economical than AI-Researcher (≈1/6 cost on AutoRYP).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Assessment Agent (multidimensional scoring + diversity), human-interactive feedback loops, idea-to-methodology construction (detailed method descriptions), exception-guided debugging, adaptive evolution (multi-round planning and reimplementation), orchestration agent for coordinated workflows, repository-level code review (Code Review Agent), and use of automated testers/coders (Aider, OpenHands) to produce runnable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NOVELSEEK can autonomously generate, evolve, implement, and experimentally validate novel research ideas across diverse domains, yielding measurable performance improvements and greater implementation success than several prior auto-research systems; iterative mechanisms (assessment, methodology construction, adaptive evolution, exception-guided debugging and human-in-the-loop review) substantially reduce the generation-to-validation gap though some tasks (complex repo-level ones) still show lower execution success.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2153.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2153.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Researcher (Fully-automated scientific discovery with LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously-proposed closed-loop LLM-agent system intended to automate scientific discovery; used here as a baseline comparator in experiments (AutoRYP and Auto2DCls).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai-researcher: Fully-automated scientific discovery with llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM-driven automated research pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated scientific research (evaluated here on reaction yield prediction and image classification)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>research ideas and code modifications intended to improve baseline models</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>automatic code generation and execution on provided baselines, followed by empirical evaluation on task-specific metrics; comparison of performance gains versus baseline</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not clearly specified in this paper for AI-Researcher; described qualitatively as being more dependent on user-provided reference papers and thus less novel</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported as weak on the evaluated tasks: AI-Researcher failed to improve baselines in the AutoRYP and Auto2DCls comparisons (Table 9) and exhibited higher monetary cost (e.g., ~$25–32 total reported) with lower or no performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Poor relative to NOVELSEEK on the compared tasks — unable to improve provided baselines in reported experiments; specific numeric task metrics for AI-Researcher are not fully enumerated in the paper beyond summary statements in Table 9.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>AI-Researcher's reliance on reference papers limited novelty and implementability; less-novel, reference-dependent ideas translated poorly into validated performance gains in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Paper reports that AI-Researcher often ignored prior codebase structure and produced ideas that were not implementable or did not improve performance, indicating a generation-validation mismatch in this system.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; on the two evaluated tasks performance was poor compared to NOVELSEEK, suggesting limited generalization in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Reported higher total cost relative to NOVELSEEK on AutoRYP (AI-Researcher ~ $25 vs NOVELSEEK ~$3 in Table 9); no detailed run-by-run breakdown provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Not emphasized; paper suggests AI-Researcher lacks extensive idea-to-methodology construction and repo-level code comprehension which NOVELSEEK uses to close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In direct comparisons reported, AI-Researcher was unable to improve baselines and was more costly, illustrating limitations of some prior end-to-end LLM research systems in converting generated ideas into validated experimental improvements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2153.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2153.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Scientist-V2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Scientist-V2 (Workshop-level automated scientific discovery via agentic tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An advanced multi-agent automated research system that uses agent tree search, vision-language feedback, and parallelized experiment execution; used here as a comparator in human evaluations of idea novelty and soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Scientist-V2</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM-driven automated research framework (agent-tree search + VLM feedback + parallel experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated scientific research (general; prior examples in ML domain and workshop-level outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates research ideas, hypotheses, experimental plans, and can orchestrate experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>parallelized experiment execution and vision-language feedback in its own pipeline; in this paper AI-Scientist-V2's ideas were human-evaluated for novelty, soundness, and contribution by qualified reviewers; authors note AI-Scientist-V2 struggled to write runnable code in prior comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>human reviewer scoring across dimensions (soundness, contribution, overall, confidence) averaged over ideas; in paper NOVELSEEK outscored AI-Scientist-V2 on these human-judged dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitatively described as generating divergent ideas but with lower implementability in the authors' comparisons; specific numeric task-performance improvements not reported here for AI-Scientist-V2.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported to have weaknesses in producing runnable code and in improving baselines when compared in the study; human reviewers rated NOVELSEEK higher in soundness and overall ratings across sampled tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>AI-Scientist-V2 tended to generate divergent (potentially novel) ideas, but novelty sometimes made ideas hard to implement and validate; human scores indicated lower soundness on average versus NOVELSEEK.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Observed: generation of divergent ideas did not reliably translate to validated, runnable implementations in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not tabulated here for AI-Scientist-V2 in direct numeric terms in this manuscript; earlier reports and referenced work would contain cost details.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Agent-tree search and vision-language feedback in original system aim to improve validation; however, in reported comparisons these mechanisms were insufficient to yield as many validated runnable improvements as NOVELSEEK.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human-evaluation comparisons show NOVELSEEK's ideas scored higher in soundness and overall quality than AI-Scientist-V2 on sampled tasks; AI-Scientist-V2's more divergent ideas were often harder to implement reliably, illustrating the generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2153.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2153.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DOLPHIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DOLPHIN (Closed-loop open-ended auto-research through thinking, practice, and feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior closed-loop LLM-driven auto-research system referenced as a baseline comparator; primarily supports single-file code modification experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DOLPHIN</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM agent-based closed-loop research system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated research for AI tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>idea generation, code edits for single-file experiments, and iterative experiment-feedback loops</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>automatic execution of generated code (single-file experiments) and performance comparison to baseline; lacks repo-level code support</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not specified in detail in this paper; compared by reported performance gains and max/mean improvements in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported in comparisons as being outperformed by NOVELSEEK across tasks (NOVELSEEK achieved higher max and average performance improvements and higher percentage of experiments with gains).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Limited to single-file tasks; lower execution/gain rates compared to NOVELSEEK on evaluated tasks (exact numeric rates shown in the paper's Tables 1–4 for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Because DOLPHIN is more limited to single-file edits and lacks NOVELSEEK's idea-to-methodology and human-feedback mechanisms, it produces fewer validated gains especially on repo-level/complex tasks — novelty that requires repo-level changes is not well supported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Present: DOLPHIN can propose edits but has difficulty scaling to complex repo-level validation; NOVELSEEK's additional modules address this asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported quantitatively here beyond cross-task comparisons showing lower improvements than NOVELSEEK.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not detailed here; NOVELSEEK claimed to outperform DOLPHIN in both performance and execution success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>DOLPHIN includes closed-loop feedback but lacks NOVELSEEK's comprehensive idea-to-methodology construction, Assessment Agent, and repo-level coder, limiting its ability to close generation-validation gaps on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DOLPHIN, as a prior auto-research baseline, performs worse than NOVELSEEK on multi-file/repo-level and many single-file tasks, demonstrating that additional modules (methodology construction, assessment, human feedback, adaptive evolution) materially help close the generation-to-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2153.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2153.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentRxiv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentRxiv (Collaborative autonomous research via shared preprint server)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that enables LLM agent laboratories to publish and build upon each other's outputs via a shared preprint server, with reported benefits for collaborative validation and performance improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentrxiv: Towards collaborative autonomous research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentRxiv</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>collaborative multi-agent platform (LLM agents + shared repository of outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>autonomous scientific research collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generation and sharing of research artifacts (ideas, preprints, experimental outputs) among agent labs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>collaborative review and reuse of agent outputs via a shared server; experiments in referenced work showed agent labs using AgentRxiv achieved greater performance improvements compared to isolated settings (collaborative validation and incremental improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>benefit measured via empirical comparisons of isolated vs collaborative agent labs (performance improvement gains); novelty also inferred from agent-to-agent building on prior outputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not quantified numerically in this paper beyond qualitative claim that AgentRxiv-enabled labs achieved better performance improvements in referenced experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported improvements when agents share and build on each other's outputs, indicating collaborative mechanisms improve validation and refinement of generated outputs; detailed metrics would be found in the referenced paper.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Collaboration via shared outputs helps agents validate and refine novel ideas by enabling others to test and extend them; authors cite this as evidence that collaborative infrastructures can reduce failures of validation for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>AgentRxiv attempts to reduce asymmetry by enabling cross-validation and reuse of agent-generated artifacts; evidence cited supports improvement but quantitative gap remains subject to referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>shared preprint server for agent outputs, enabling cross-agent validation and incremental refinement; promotes reproducibility and collective testing</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Collaborative infrastructures like AgentRxiv can help agents validate and improve novel research outputs by enabling cross-agent reuse and scrutiny, thereby reducing the validation burden on any single agent and improving realized performance gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search <em>(Rating: 2)</em></li>
                <li>Ai-researcher: Fully-automated scientific discovery with llm agents <em>(Rating: 2)</em></li>
                <li>Agentrxiv: Towards collaborative autonomous research <em>(Rating: 2)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 2)</em></li>
                <li>Towards an ai co-scientist <em>(Rating: 1)</em></li>
                <li>Large language models as biomedical hypothesis generators: a comprehensive evaluation <em>(Rating: 1)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2153",
    "paper_id": "paper-278788499",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "NOVELSEEK",
            "name_full": "NOVELSEEK (Unified Closed-loop Multi-agent Framework for Autonomous Scientific Research)",
            "brief_description": "A multi-agent LLM- and tool-driven closed-loop system that generates, evolves, implements, and experimentally validates research ideas across 12 diverse scientific tasks using automated literature/code survey, idea generation/evolution, methodology construction, automated coding/debugging, multi-round experiment planning, and human-interactive feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NOVELSEEK",
            "system_type": "hybrid multi-agent system (LLMs + code-generation/debug agents + orchestration + human-in-loop)",
            "domain": "general scientific research (chemistry, molecular dynamics, power systems, time series, genomics, vision, point-cloud, NLP, etc.)",
            "generation_capability": "research ideas/hypotheses, methodological designs, implementation code, model architecture proposals, and optimization strategies",
            "validation_method": "multi-pronged: automatic implementation and execution of generated methods (coder agents Aider/OpenHands + Claude-3.7-Sonnetto), iterative experiment planning and adaptive evolution (multiple runs + performance assessment), automated exception-guided debugging, quantitative evaluation on domain-specific metrics (e.g., R^2, MAE, MIoU, mAP), and human expert evaluation of idea soundness/novelty; pipelines support end-to-end experimental validation including repo-level runs",
            "novelty_measure": "combination of automated Assessment Agent multidimensional scoring (coherence, credibility, verifiability, novelty, alignment; 0–10 per dimension with weighted sum) and human expert ratings (soundness, contribution, overall, confidence) over batches of generated ideas; also promotes diversity among top-ranked ideas",
            "generation_performance": "demonstrated quantitative improvements over baselines across tasks (examples: Reaction Yield Prediction improved from baseline 24.2±4.2 to NOVELSEEK 34.8±1.1 R^2 in few-shot setting; Enhancer Activity Prediction from 0.65 to 0.79 PCC; 2D semantic segmentation from 78.8% to 81.0% mIoU). Reported high ability to produce implementable ideas: many proposals produced runnable code (repo-level support) with varying success rates per task.",
            "validation_performance": "validation performed via executed experiments and domain metrics; NOVELSEEK shows higher validation success and performance-gain rates compared to baselines and prior auto-research systems (e.g., higher percentage of experiments with performance gains vs DOLPHIN), but performance varies by task (e.g., execution success: AutoPCDet ~50%, Auto2DSeg ~90% reported); precise per-task metric improvements are given in tables but no global statistical false-positive/negative metrics reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "The paper reports that highly novel ideas are challenging to implement and validate, but NOVELSEEK's idea-to-methodology construction, multi-round planning/adaptive evolution, and human-interactive feedback substantially mitigate this: novelty can reduce initial execution success, but iterative debugging and refinement increase validated gains; adaptive evolution increased the percentage of ideas that improved performance (e.g., AutoRYP improved from 20% to 40% with AE).",
            "generation_validation_asymmetry": "Evidence of asymmetry: generating diverse/novel ideas is achievable, but reliably implementing and validating them is harder — prior systems (e.g., AI-Scientist-V2) produced ideas harder to implement; NOVELSEEK reduces but does not eliminate this gap via methodology construction, Assessment Agent scoring, exception-guided debugging, and adaptive evolution.",
            "out_of_distribution_performance": "Qualitative/generalization claim: NOVELSEEK was applied across 12 heterogeneous tasks (scientific and AI tasks) and improved performance in many, indicating robust cross-domain generalization; however, no formal OOD benchmark metrics were provided. Performance and execution success vary by task complexity (repo-level tasks more costly and less universally successful).",
            "calibration_quality": "Not quantitatively reported; system includes Assessment Agent scores and human confidence labels but no calibration/uncertainty calibration metrics (e.g., reliability diagrams) are provided.",
            "validation_computational_cost": "Reported concrete costs: idea-generation cost ≈ $0.6 per idea using GPT-4o; coder-debug run cost ranges (example: single-file runs &lt;$1, repo-level ~ $1.2 using Claude Sonnet); training time per task reported in A100 GPU hours (examples: AutoRYP 6.0 A100h, AutoVLM 192 A100h). NOVELSEEK is reported significantly more economical than AI-Researcher (≈1/6 cost on AutoRYP).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Assessment Agent (multidimensional scoring + diversity), human-interactive feedback loops, idea-to-methodology construction (detailed method descriptions), exception-guided debugging, adaptive evolution (multi-round planning and reimplementation), orchestration agent for coordinated workflows, repository-level code review (Code Review Agent), and use of automated testers/coders (Aider, OpenHands) to produce runnable implementations.",
            "evidence_type": "supports",
            "key_findings": "NOVELSEEK can autonomously generate, evolve, implement, and experimentally validate novel research ideas across diverse domains, yielding measurable performance improvements and greater implementation success than several prior auto-research systems; iterative mechanisms (assessment, methodology construction, adaptive evolution, exception-guided debugging and human-in-the-loop review) substantially reduce the generation-to-validation gap though some tasks (complex repo-level ones) still show lower execution success.",
            "uuid": "e2153.0"
        },
        {
            "name_short": "AI-Researcher",
            "name_full": "AI-Researcher (Fully-automated scientific discovery with LLM agents)",
            "brief_description": "A previously-proposed closed-loop LLM-agent system intended to automate scientific discovery; used here as a baseline comparator in experiments (AutoRYP and Auto2DCls).",
            "citation_title": "Ai-researcher: Fully-automated scientific discovery with llm agents",
            "mention_or_use": "use",
            "system_name": "AI-Researcher",
            "system_type": "multi-agent LLM-driven automated research pipeline",
            "domain": "automated scientific research (evaluated here on reaction yield prediction and image classification)",
            "generation_capability": "research ideas and code modifications intended to improve baseline models",
            "validation_method": "automatic code generation and execution on provided baselines, followed by empirical evaluation on task-specific metrics; comparison of performance gains versus baseline",
            "novelty_measure": "not clearly specified in this paper for AI-Researcher; described qualitatively as being more dependent on user-provided reference papers and thus less novel",
            "generation_performance": "Reported as weak on the evaluated tasks: AI-Researcher failed to improve baselines in the AutoRYP and Auto2DCls comparisons (Table 9) and exhibited higher monetary cost (e.g., ~$25–32 total reported) with lower or no performance gains.",
            "validation_performance": "Poor relative to NOVELSEEK on the compared tasks — unable to improve provided baselines in reported experiments; specific numeric task metrics for AI-Researcher are not fully enumerated in the paper beyond summary statements in Table 9.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "AI-Researcher's reliance on reference papers limited novelty and implementability; less-novel, reference-dependent ideas translated poorly into validated performance gains in the reported comparisons.",
            "generation_validation_asymmetry": "Paper reports that AI-Researcher often ignored prior codebase structure and produced ideas that were not implementable or did not improve performance, indicating a generation-validation mismatch in this system.",
            "out_of_distribution_performance": "Not reported; on the two evaluated tasks performance was poor compared to NOVELSEEK, suggesting limited generalization in these experiments.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Reported higher total cost relative to NOVELSEEK on AutoRYP (AI-Researcher ~ $25 vs NOVELSEEK ~$3 in Table 9); no detailed run-by-run breakdown provided in the paper.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Not emphasized; paper suggests AI-Researcher lacks extensive idea-to-methodology construction and repo-level code comprehension which NOVELSEEK uses to close the gap.",
            "evidence_type": "supports",
            "key_findings": "In direct comparisons reported, AI-Researcher was unable to improve baselines and was more costly, illustrating limitations of some prior end-to-end LLM research systems in converting generated ideas into validated experimental improvements.",
            "uuid": "e2153.1"
        },
        {
            "name_short": "AI-Scientist-V2",
            "name_full": "AI-Scientist-V2 (Workshop-level automated scientific discovery via agentic tree search)",
            "brief_description": "An advanced multi-agent automated research system that uses agent tree search, vision-language feedback, and parallelized experiment execution; used here as a comparator in human evaluations of idea novelty and soundness.",
            "citation_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "mention_or_use": "use",
            "system_name": "AI-Scientist-V2",
            "system_type": "multi-agent LLM-driven automated research framework (agent-tree search + VLM feedback + parallel experiments)",
            "domain": "automated scientific research (general; prior examples in ML domain and workshop-level outputs)",
            "generation_capability": "generates research ideas, hypotheses, experimental plans, and can orchestrate experiments",
            "validation_method": "parallelized experiment execution and vision-language feedback in its own pipeline; in this paper AI-Scientist-V2's ideas were human-evaluated for novelty, soundness, and contribution by qualified reviewers; authors note AI-Scientist-V2 struggled to write runnable code in prior comparisons",
            "novelty_measure": "human reviewer scoring across dimensions (soundness, contribution, overall, confidence) averaged over ideas; in paper NOVELSEEK outscored AI-Scientist-V2 on these human-judged dimensions",
            "generation_performance": "Qualitatively described as generating divergent ideas but with lower implementability in the authors' comparisons; specific numeric task-performance improvements not reported here for AI-Scientist-V2.",
            "validation_performance": "Reported to have weaknesses in producing runnable code and in improving baselines when compared in the study; human reviewers rated NOVELSEEK higher in soundness and overall ratings across sampled tasks.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "AI-Scientist-V2 tended to generate divergent (potentially novel) ideas, but novelty sometimes made ideas hard to implement and validate; human scores indicated lower soundness on average versus NOVELSEEK.",
            "generation_validation_asymmetry": "Observed: generation of divergent ideas did not reliably translate to validated, runnable implementations in the reported comparisons.",
            "out_of_distribution_performance": "Not quantified in this paper.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Not tabulated here for AI-Scientist-V2 in direct numeric terms in this manuscript; earlier reports and referenced work would contain cost details.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Agent-tree search and vision-language feedback in original system aim to improve validation; however, in reported comparisons these mechanisms were insufficient to yield as many validated runnable improvements as NOVELSEEK.",
            "evidence_type": "supports",
            "key_findings": "Human-evaluation comparisons show NOVELSEEK's ideas scored higher in soundness and overall quality than AI-Scientist-V2 on sampled tasks; AI-Scientist-V2's more divergent ideas were often harder to implement reliably, illustrating the generation-validation gap.",
            "uuid": "e2153.2"
        },
        {
            "name_short": "DOLPHIN",
            "name_full": "DOLPHIN (Closed-loop open-ended auto-research through thinking, practice, and feedback)",
            "brief_description": "A prior closed-loop LLM-driven auto-research system referenced as a baseline comparator; primarily supports single-file code modification experiments.",
            "citation_title": "Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback",
            "mention_or_use": "mention",
            "system_name": "DOLPHIN",
            "system_type": "LLM agent-based closed-loop research system",
            "domain": "automated research for AI tasks",
            "generation_capability": "idea generation, code edits for single-file experiments, and iterative experiment-feedback loops",
            "validation_method": "automatic execution of generated code (single-file experiments) and performance comparison to baseline; lacks repo-level code support",
            "novelty_measure": "not specified in detail in this paper; compared by reported performance gains and max/mean improvements in experiments",
            "generation_performance": "Reported in comparisons as being outperformed by NOVELSEEK across tasks (NOVELSEEK achieved higher max and average performance improvements and higher percentage of experiments with gains).",
            "validation_performance": "Limited to single-file tasks; lower execution/gain rates compared to NOVELSEEK on evaluated tasks (exact numeric rates shown in the paper's Tables 1–4 for comparisons).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Because DOLPHIN is more limited to single-file edits and lacks NOVELSEEK's idea-to-methodology and human-feedback mechanisms, it produces fewer validated gains especially on repo-level/complex tasks — novelty that requires repo-level changes is not well supported.",
            "generation_validation_asymmetry": "Present: DOLPHIN can propose edits but has difficulty scaling to complex repo-level validation; NOVELSEEK's additional modules address this asymmetry.",
            "out_of_distribution_performance": "Not reported quantitatively here beyond cross-task comparisons showing lower improvements than NOVELSEEK.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Not detailed here; NOVELSEEK claimed to outperform DOLPHIN in both performance and execution success rates.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "DOLPHIN includes closed-loop feedback but lacks NOVELSEEK's comprehensive idea-to-methodology construction, Assessment Agent, and repo-level coder, limiting its ability to close generation-validation gaps on complex tasks.",
            "evidence_type": "supports",
            "key_findings": "DOLPHIN, as a prior auto-research baseline, performs worse than NOVELSEEK on multi-file/repo-level and many single-file tasks, demonstrating that additional modules (methodology construction, assessment, human feedback, adaptive evolution) materially help close the generation-to-validation gap.",
            "uuid": "e2153.3"
        },
        {
            "name_short": "AgentRxiv",
            "name_full": "AgentRxiv (Collaborative autonomous research via shared preprint server)",
            "brief_description": "A referenced system that enables LLM agent laboratories to publish and build upon each other's outputs via a shared preprint server, with reported benefits for collaborative validation and performance improvements.",
            "citation_title": "Agentrxiv: Towards collaborative autonomous research",
            "mention_or_use": "mention",
            "system_name": "AgentRxiv",
            "system_type": "collaborative multi-agent platform (LLM agents + shared repository of outputs)",
            "domain": "autonomous scientific research collaboration",
            "generation_capability": "generation and sharing of research artifacts (ideas, preprints, experimental outputs) among agent labs",
            "validation_method": "collaborative review and reuse of agent outputs via a shared server; experiments in referenced work showed agent labs using AgentRxiv achieved greater performance improvements compared to isolated settings (collaborative validation and incremental improvement)",
            "novelty_measure": "benefit measured via empirical comparisons of isolated vs collaborative agent labs (performance improvement gains); novelty also inferred from agent-to-agent building on prior outputs",
            "generation_performance": "Not quantified numerically in this paper beyond qualitative claim that AgentRxiv-enabled labs achieved better performance improvements in referenced experiments",
            "validation_performance": "Reported improvements when agents share and build on each other's outputs, indicating collaborative mechanisms improve validation and refinement of generated outputs; detailed metrics would be found in the referenced paper.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Collaboration via shared outputs helps agents validate and refine novel ideas by enabling others to test and extend them; authors cite this as evidence that collaborative infrastructures can reduce failures of validation for novel outputs.",
            "generation_validation_asymmetry": "AgentRxiv attempts to reduce asymmetry by enabling cross-validation and reuse of agent-generated artifacts; evidence cited supports improvement but quantitative gap remains subject to referenced work.",
            "out_of_distribution_performance": "Not detailed here.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Not reported in this paper.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "shared preprint server for agent outputs, enabling cross-agent validation and incremental refinement; promotes reproducibility and collective testing",
            "evidence_type": "supports",
            "key_findings": "Collaborative infrastructures like AgentRxiv can help agents validate and improve novel research outputs by enabling cross-agent reuse and scrutiny, thereby reducing the validation burden on any single agent and improving realized performance gains.",
            "uuid": "e2153.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "rating": 2
        },
        {
            "paper_title": "Ai-researcher: Fully-automated scientific discovery with llm agents",
            "rating": 2
        },
        {
            "paper_title": "Agentrxiv: Towards collaborative autonomous research",
            "rating": 2
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 2
        },
        {
            "paper_title": "Towards an ai co-scientist",
            "rating": 1
        },
        {
            "paper_title": "Large language models as biomedical hypothesis generators: a comprehensive evaluation",
            "rating": 1
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 1
        }
    ],
    "cost": 0.01823375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evolutionary Experimental Planning and Execution Method Details
25 May 2025</p>
<h1>Description</h1>
<p>Evolutionary Experimental Planning and Execution Method Details
25 May 2025A45D3C06851EF8514A0225A84AB87597arXiv:2505.16938v2[cs.AI]
Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation.We introduce NOVELSEEK, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision.NOV-ELSEEK highlights three key advantages: 1) Scalability: NOVELSEEK has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code.2) Interactivity: NOVELSEEK provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge.3) Efficiency: NOVELSEEK has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts.For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, the Pearson correlation coefficient rose from 0.65 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.</p>
<p>Introduction</p>
<p>Autonomous Scientific Discovery (ASD) refers to the use of Large Language Models (LLMs) (Yang et al., 2024;Chen et al., 2024;Wang et al., 2024a;Guo et al., 2025) and robotics to independently perform scientific research without direct human intervention (Yuan et al., 2025;Yan et al., 2025;Gottweis et al., 2025;Yamada et al., 2025;Lu et al., 2024).This approach holds transformative potential for accelerating the pace of discovery across various scientific domains.By automating tasks such as data analysis, hypothesis generation, experiment design, and result interpretation, automated systems (Yuan et al., 2025;Lu et al., 2024) can efficiently process vast amounts of information and uncover patterns or insights that may be difficult for human researchers to detect.ASR, while promising, faces significant challenges in generating effective and novel proposals, as well as achieving closed-loop feedback for the experimental validation of these proposals:</p>
<p>• First, generating proposals that are both effective and novel is a complicated task.</p>
<p>Autonomous systems must identify research gaps and generate hypotheses that are not only innovative but also scientifically valid.This requires balancing creativity and rigor, which is difficult for AI models that rely on existing data and patterns.Additionally, ensuring the novelty of proposals often demands a deep understanding of the broader scientific context, which can be challenging for models limited by the quality and scope of their training data.• Second, implementing closed-loop feedback for end-to-end experimental validation is another major hurdle.Autonomous systems need to design experiments, execute them, analyze results, and iteratively refine their hypotheses in a seamless loop.This requires integration across multiple domains, such as robotics for experiment execution and advanced analytics for result interpretation.Furthermore, real-world experiments often come with unexpected variables and noise, making it challenging for autonomous systems to adapt and learn effectively.Achieving a truly closedloop system demands robust coordination, adaptability, and the ability to handle uncertainty, which remain technical and conceptual barriers.</p>
<p>To further facilitate the advancement of ASR, we propose the NOVELSEEK, an end-to-end auto-research pipeline, which covers four main modules: self-evolving idea generation, human-interactive feedback, idea-to-methodology construction, and multi-round experiment planning and execution.With the help of the Self-Evolving Human-interactive Idea Generation and Idea-to-Methodology Construction, NOVELSEEK can transform a rough proposal into a detailed and easily implementable method, which further increases the success rate of code implementation process and enhances the efficiency of closed-loop experiments.</p>
<p>Besides, by leveraging multi-round experiment planning and execution, NOVELSEEK is capable of designing experimental plans and decomposing the experimental process according to the NOVELSEEK-proposed modules, thereby validating the effectiveness of each NOVELSEEK-generated module through experimentation.</p>
<p>As shown in Fig. 1, NOVELSEEK has been validated across 12 scientific research tasks, and we are excited to see that the experimental results demonstrate the significant value of NOVELSEEK in the entire process from hypothesis generation to experimental validation.For instance, in the Reaction Yield Prediction task, the baseline model only achieved a performance of 24.2% ± 4.2, while our model improved it to 34.8% ± 1.1 in just 12 hours.In contrast, human researchers typically require several months to achieve a similar level of performance improvement.Another example of performance improvement is the Enhancer Activity Prediction task.The baseline model, DeepSTARR, achieved a result of 0.65.By utilizing NOVELSEEK to search relevant domain literature, automatically generate code, and conduct validation, the performance can be improved to 0.79, representing a promising enhancement.In addition, NOVELSEEK also supports complex project-level modifications and debugging, which consist of multiple code files.These results clearly indicate that NOVELSEEK can autonomously generate ideas and design algorithms, effectively reducing the dependence on human effort in scientific research.To facilitate reproducibility, we have open-sourced both the baselines and the codes generated by NOVELSEEK used in all involved scientific research tasks at https://github.com/Alpha-Innovator/NovelSeek.</p>
<p>Furthermore, the contributions of this paper are summarized below:</p>
<p>• Unified Multi-agent Framework for Diverse Scientific Research Tasks: We present NOVELSEEK, a unified closed-loop scientific research framework that can automate the entire research cycle, including idea generation, idea-to-methodology transformation, experiment execution, and result feedback.This unified framework can be directly applied to various scientific research scenarios and fields.</p>
<p>• Interactive Interfaces for Enhanced Cooperative Research: NOVELSEEK offers interactive interfaces for human-machine collaboration within the idea generation module and across the entire system.By selecting collaboration modes, such as leveraging AI or human experts, it provides evaluations of idea generation effectiveness and facilitates the assessment, reflection, and documentation of experimental results.</p>
<p>• Comprehensive Experimental Validation and Human Studies: We conducted extensive human studies centered around NOVELSEEK, including inviting domain experts to evaluate and score the novelty of ideas generated by NOVELSEEK, and comparing the research efficiency between human researchers and NOVELSEEK.These experiments and human studies are crucial for gaining insights into the capabilities of multi-agent systems in conducting scientific research tasks in open-ended environments.We observed many promising phenomena, while also identifying certain technical modules that require improvement.</p>
<p>NOVELSEEK</p>
<p>As a unified closed-loop multi-agent framework for ASR, NOVELSEEK is designed to facilitate innovative research across diverse scientific domains, as illustrated in Fig. 2. It incorporates three primary capabilities: self-evolving idea generation with human-interactive feedback (Sec.2.1), comprehensive idea-to-methodology construction (Sec.2.2), and multiround automated experiment execution (Sec.2.3).Each capability is realized through the collaboration of specialized agents, allowing for seamless integration of different processes to enhance scientific discoveries.</p>
<p>Self-Evolving Idea Generation with Human-interactive Feedback</p>
<p>The self-evolving idea generation capability is central to NOVELSEEK, enabling the framework to autonomously generate and refine innovative research ideas.This process involves several specialized agents, each contributing to different stages of idea development and refinement.</p>
<p>Survey Agent.The Survey Agent is designed to meet the diverse needs of various scientific research tasks by adaptively aligning with user-specified requirements and the necessary depth of detail for exploring existing methodologies.This adaptability is crucial for effectively generating new ideas across different research contexts, and the primary responsibility of the Survey Agent is to search for scientific papers, offering two distinct modes to address the varying needs for depth and breadth in literature research during the scientific discovery process: 1) the literature review mode and 2) the deep research mode.</p>
<p>In the literature review mode, the Survey Agent deconstructs the research task into multiple keyword combinations, enabling a broad search across various academic databases.It collects scientific literature from these sources and evaluates the relevance of each document by analyzing abstracts in relation to the task at hand.Denote the keyword generation process by the function P : T → K, where T represents the descriptions of research tasks and K is the set of generated keyword combinations.The relevance evaluation of each document can be represented by the function:
R : L abs × T → [0, 1],(1)
where L abs is the abstract of retrieved literature L, and R(r, t) measures the relevance of literature l to the task t as a floating-point score between 0 and 1, with higher scores indicating greater relevance.</p>
<p>In the deep research mode, following the initial literature survey, the Survey Agent downloads and thoroughly examines the full texts of relevant scientific papers.This deeper analysis allows it to generate new keyword combinations, facilitating further rounds of literature exploration.The process of generating new keywords can be denoted by the function:
P : L → K ′ ,(2)
where K ′ is the expanded set of keyword combinations generated from the detailed analysis of full texts.</p>
<p>By dynamically adjusting its search strategies based on the context of the research stage, the Survey Agent ensures a comprehensive and nuanced understanding of the research landscape.This capability not only supports the generation of innovative ideas but also ensures that the NOVELSEEK framework remains at the cutting edge of scientific discovery.</p>
<p>Code Review Agent.The Code Review Agent is crucial for understanding baseline codes for different research tasks, serving as a foundation for innovation by identifying improvements and developing new methodologies.It provides detailed analyses of code structures, dependencies, and functionalities, enabling NOVELSEEK to fully comprehend existing code-bases and identify potential enhancements to advance research objectives.Moreover, the agent's ability to document and summarize complex code-bases ensures efficient navigation and utilization of existing methods.The agent manages two scenarios: 1) reviewing user-provided code or 2) searching for relevant code-bases.For user-uploaded code, it conducts a comprehensive review of the structure, logic, and functionality.Alternatively, in the absence of user-uploaded code, it searches public repositories like GitHub to find relevant code-bases, performing thorough analyses at both the repository and file levels to understand inter-dependencies and assess logic, efficiency, and correctness.Furthermore, the agent employs static code analysis using Python's 'ast' module to parse and understand code structure without execution, while the LLM generates human-readable descriptions and summaries, transforming technical details into structured documentation.By using parallel processing with Python's 'multiprocessing' module, the agent enhances efficiency and scalability for large code-bases.Overall, the Code Review Agent offers detailed documentation that deepens the understanding of code repositories and supports innovation in scientific research.</p>
<p>Idea Innovation Agent.</p>
<p>The Idea Innovation Agent is an integral part of NOVELSEEK, designed to enhance the creative and iterative processes of scientific research.This agent plays a crucial role by automating the generation and evolution of ideas, thereby addressing the limitations of traditional research works (Yuan et al., 2025;Yamada et al., 2025;Lu et al., 2024), which often rely on time-consuming manual efforts and are constrained by human cognitive biases.The agent's dual responsibilities, idea generation and idea evolution, are specifically designed to address the diverse needs of various scientific disciplines.</p>
<p>In the context of idea generation, the agent utilizes a general LLM configured with a higher temperature setting.This configuration encourages the generation of more diverse and creative outputs.This enables the agent to identify patterns and insights that might be overlooked in traditional research, generating novel hypotheses and strategies based on task definitions, baseline methods, and current scientific knowledge.The process can be represented by the function:
G : (T , B, L) → I,(3)
where B denotes analysis of baseline methods and I is the set of generated ideas.The LLM facilitates the exploration of a broader spectrum of possibilities, accelerating the pace of discovery and innovation by leveraging its comprehensive understanding of language and context.</p>
<p>Idea evolution leverages the capabilities of an LLM to improve existing ideas.The process involves analyzing the content of these ideas, incorporating reflections, which include evaluations of novelty, feasibility, and scientific validity, and integrating insights from related literature.This approach enables the generation of refined and innovative ideas by addressing the inherent limitations of initial concepts.The process can be represented by the same function:
G : (I, C, L) → I ′ , (4)
where I is the initial set of ideas, C denotes the critique, and I ′ is the set of evolved ideas.</p>
<p>Overall, the Idea Innovation Agent enhances scientific ideas into viable and creative solutions by synthesizing and contextualizing information.It critically examines current ideas and employs feedback loops with human experts and other NOVELSEEK agents for continuous improvement.This iterative process balances novelty, feasibility, and ethical considerations, producing impactful and well-rounded ideas.</p>
<p>Assessment Agent.The Assessment Agent is a vital component of NOVELSEEK, designed to ensure the quality and viability of generated ideas through a rigorous evaluation process.In the rapidly evolving landscape of scientific research, the systematic and objective assessment of ideas is essential.Traditional methods often suffer from subjectivity and lack comprehensive coverage of all relevant dimensions, which can lead to promising ideas being overlooked (Qiu et al., 2025;Si et al., 2024).Therefore, the Assessment Agent addresses these challenges by providing a structured and multidimensional evaluation process, which in turn enhances the reliability and effectiveness of idea selection.</p>
<p>The primary responsibility of the Assessment Agent is to critically evaluate ideas using multidimensional scoring.Each idea is analyzed across key dimensions: coherence, credibility, verifiability, novelty, and alignment.Coherence checks the logical consistency and structure of the idea, while credibility assesses its trustworthiness based on existing knowledge.Verifiability examines the idea's testability through empirical methods.Novelty measures originality, and alignment ensures consistency with research goals.</p>
<p>Moreover, for each dimension, the Assessment Agent provides a detailed evaluation narrative to explain its reasoning.It assigns scores from 0 to 10, which are combined using a weighted summation to produce an overall score for each idea, aiding in the ranking process.By utilizing advanced LLMs, the agent can accurately process and evaluate complex scientific concepts.This capability allows a comprehensive assessment that includes both qualitative and quantitative aspects, ensuring the evaluation is thorough and well-rounded.</p>
<p>Furthermore, the Assessment Agent possesses the ability to ensure diversity among topranked ideas.This capability prevents high-scoring ideas from being overly similar or derived from the same original concept.By promoting a varied pool of ideas, the agent encourages the exploration of diverse pathways in the research process.This is crucial for maintaining a balance between innovation and practicality, ensuring that the most promising ideas are both high-quality and distinct from each other.</p>
<p>In summary, by employing LLMs for multidimensional scoring and leveraging its ability to promote diversity among ideas, the Assessment Agent ensures that only the most viable and innovative concepts are selected for further development.This process not only enhances the efficiency of the research cycle but also fosters a more dynamic and diverse research environment.</p>
<p>Human-interactive Feedback.In the context of multi-agent systems, human-interactive feedback is a crucial component for effectively managing and solving complex tasks.This integration of human insights enables agents to navigate dynamic environments more effectively, aligning their outputs with complex user requirements and ensuring practical applicability.</p>
<p>The human-interactive feedback mechanism of NOVELSEEK is categorized into two primary types: 1) feedback directly provided by humans and 2) feedback automatically generated by agent.</p>
<p>Human-provided feedback can address one or multiple ideas, offering insights and critiques that lead to further refinement and adjustment of these ideas based on the feedback received.This iterative process facilitates the continuous improvement of ideas, ensuring they are honed to meet specific objectives and challenges.</p>
<p>For example, in a scenario involving medical image segmentation, an LLM multi-agent system might initially propose a broad idea focused on developing more advanced segmentation algorithms.However, human feedback can refine this idea by directing attention specifically to the medical domain.Human experts can provide insights that encourage the Adding a graph-derived reaction descriptor as a precondition for attention scores in the transformer architecture can bias computations toward chemically relevant substructures.</p>
<p>Init Idea 0</p>
<p>Using a modality alignment objective with a contrastive learning loss can fuse textual SMILES embeddings with molecular graphbased embeddings extracted via a GNN to improve yield prediction accuracy.</p>
<p>Init Idea 1</p>
<p>Introducing cyclic attention mechanisms specifically designed to focus on circular dependencies in SMILES-based token sequences improves the attention to stereochemistry and reaction-specific quirks crucial for yield prediction.</p>
<p>Init Idea 2</p>
<p>…</p>
<p>Evolved Idea 0-2 Using graph-derived reaction descriptors such as bond valence, reaction center importance, and molecular transformations to compute graph-aware positional embeddings for SMILES tokens will introduce chemical context within the attention mechanism, focusing the model on chemically significant regions and improving the accuracy of yield predictions.</p>
<p>Leveraging graph-derived reaction descriptors such as bond valence and reaction center importance to generate attention masks that selectively modify attention scores for SMILES tokens associated with chemically relevant substructures will improve the model's yield prediction accuracy by steering the network's focus toward critical chemical regions during training.Evolved Idea 0-0-0 Integrating graph-derived reaction descriptors such as bond valence and reaction center importance as auxiliary inputs into transformer attention layers using crossattention mechanisms will enhance the model's ability to identify and attend to chemically relevant substructures, thereby improving yield prediction performance while managing overfitting risks.</p>
<p>Evolved Idea 0-1</p>
<p>…</p>
<p>Employing multi-scale cross-modality attention modules to integrate graph-derived reaction descriptors and SMILES representations, while introducing a hierarchical weighting system to balance local and global reaction factors, will enhance yield prediction accuracy by effectively synthesizing substructurespecific chemical insights and reaction context.</p>
<p>Evolved Idea 0-0-1</p>
<p>Combining graph neural network (GNN)derived reaction descriptors, such as functional group and reaction center features, with SMILES token embeddings through a cross-attention bridging module focusing on semantic alignment of representations will improve prediction accuracy by creating a unified chemical feature space that captures substructural and contextual reaction elements.</p>
<p>Evolved Idea 0-0-2 Evolved Idea 0-0-1-0 A dual-encoder framework combining a SMILES-based LLaMA encoder with a graph convolutional network (GCN) for reaction descriptors, fused via modality-specific attention layers and a residual weighting mechanism to balance local functional group transformations and global reaction conditions, will enhance yield prediction accuracy by incorporating both structural and contextdependent information.</p>
<p>Evolved Idea 0-0-1-1</p>
<p>Enhancing SMILES and graph-derived reaction descriptor integration using a hybrid graph-transformer network that incorporates hierarchical token-attention mechanisms at the molecular substructure and reactioncondition levels, combined with adaptive layer freezing for fine-tuning, will improve chemical yield prediction by leveraging both 1D and 2D molecular representations while minimizing overfitting.</p>
<p>Evolved Idea 0-0-1-2 … development of adaptive solutions tailored to the unique challenges of medical imaging, such as handling diverse tissue types and ensuring high accuracy in identifying critical structures.This targeted feedback not only sharpens the focus of the idea but also ensures it aligns with the specific needs and priorities of medical research, enhancing its practical applicability and impact.</p>
<p>Orchestration Agent.The Orchestration Agent coordinates all other agents within the system, facilitating collaboration by synchronizing tasks and managing data flow.This ensures the process remains efficient, coherent, and aligned with research objectives, allowing the framework to function as an effective research tool.</p>
<p>Central to the Orchestration Agent's role is designing and managing workflows among agents like the Survey Agent, Code Review Agent, Idea Innovation Agent, and Assessment Agent.It also oversees the timing of human feedback, especially for high-scoring ideas.This requires understanding each agent's capabilities and their interactions to optimize task execution and completion.For example, the Survey Agent conducts adaptive literature exploration, providing insights that the Idea Innovation Agent uses to generate novel hypotheses.The Orchestration Agent ensures these findings are communicated effectively.Similarly, it synchronizes the Code Review Agent's analyses to enhance idea evaluation and development.Furthermore, the Orchestration Agent manages the Assessment Agent's evaluation process, ensuring timely and relevant outputs.This helps guide the development of diverse top ideas.Additionally, it determines optimal points for human feedback, integrating expert insights after identifying high-scoring ideas to refine and adapt them, aligning outputs with user requirements.</p>
<p>In summary, as illustrated in Fig. 3, by managing multi-agent collaboration and integrating human feedback, the Orchestration Agent enables NOVELSEEK to operate as a cohesive and innovative research tool, driving scientific discovery forward.</p>
<p>Comprehensive Idea-to-Methodology Construction</p>
<p>The idea-to-methodology construction process systematically bridges the gap between concise research ideas and concrete, implementable methodologies, ensuring that the AI-NovelSeek: Starting Point of Innovation generated ideas could be realized and their validity verified.This process is orchestrated by the Methodology Development Agent, which collaborates closely with other agents and integrates both automated processes and human-interactive feedback loops to ensure that methodological development is rigorous, traceable, and practically relevant.Specifically, to develop a comprehensive method corresponding to the concise research idea, the Method Development Agent possesses two core capabilities: 1) Methodology Initialization: which involves constructing the basic structure and content of a method by integrating the idea with baseline codes and the methodology content of relevant literature; 2) Methodology Refinement: which iteratively enhances the basic method structure for the purpose of rigor and completeness, ensuring a more detailed and robust methodology.</p>
<p>Methodology Initialization</p>
<p>To convert concise research ideas into detailed methodological frameworks, the Method Development Agent uses its Methodology Initialization capability.The process begins by extracting core objectives and hypotheses from research ideas, identifying key variables, and understanding their interrelationships to construct a coherent framework.The agent uses multiple resources: task descriptions T provide context and constraints; baseline implementations B offer adaptable methods; and relevant literature L integrates existing knowledge and ensures that the framework aligns with current research.</p>
<p>By formalizing mechanisms that require empirical investigation, the agent details processes and conditions for conducting research and specifies methods for data collection and analysis.The outcome is a methodological framework that is both theoretically sound and practically executable.The transformation function is represented as:
T : I × T × B × L → M,(5)
where I denotes research ideas, T includes task descriptions, B represents baseline methods, L is the literature corpus, and M is the resulting methodological framework.Overall, through Methodology Initialization, the Method Development Agent effectively turns initial ideas into detailed, actionable methods, ready for further refinement.</p>
<p>Methodology Refinement</p>
<p>After the initialization, the Methodology Development Agent leverages its refinement capability to critically evaluate and iteratively improve the methodological framework.</p>
<p>The agent conducts a comprehensive analysis of the initial methodology M, incorporating structured critiques C, which include both automated assessments and expert human feedback.Additionally, it synthesizes insights from the latest scientific literature L. The refinement process is formally defined as:
R : M × C × L → M ′ ,(6)
where M represents the initial methodology, C denotes the critique space, potentially including human feedback and automated assessments, L is the literature corpus, and M ′ is the refined methodological framework.</p>
<p>During both initialization and refinement, the Methodology Development Agent collaborates closely with other agents, such as the Assessment Agent for multidimensional evaluation and the Orchestration Agent for workflow coordination.This collaboration ensures that each methodological step benefits from comprehensive feedback and current domain knowledge.The integrated, multi-agent approach guarantees that the transformation from idea to methodology is systematic and adaptable, supporting the continuous evolution and optimization of scientific research within the NOVELSEEK framework.</p>
<p>Evolutionary Experimental Planning and Execution</p>
<p>Exception-Guided Debugging Framework</p>
<p>Converting theoretical concepts into functional code is challenging.To this end, we developed an exception-guided debugging framework that systematically converts abstract methodological text descriptions into executable implementation codes.This framework operates by systematically capturing runtime exceptions during execution attempts, analyzing error contexts, and formulating targeted fixes through reasoning of the large language model.</p>
<p>Our coder module employs a dual-strategy approach according to the complexity of given baseline code.For single-file or limited-scope implementation tasks, we use the Aider coding assistant (Gauthier &amp; Contributors, 2023), which facilitates localized code modifications with minimal overhead.For complex repository-level codes requiring comprehensive structural understanding across different functions, we deploy OpenHands framework (Wang et al., 2024b), which enables thorough codebase analysis and coordinated multi-file modifications while maintaining the integrity of the overall code architecture.</p>
<p>Once the initial code implementation is completed, the framework transitions to a systematic debugging phase to ensure functionality and robustness.The debugging process follows a systematic cycle: (1) execution attempt, ( 2) exception capture and traceback analysis, (3) contextual code structure understanding, ( 4) debugging strategy formulation, and ( 5) targeted implementation.This cycle continues iteratively until successful execution or reaching a predefined iteration threshold.</p>
<p>Experimental Planning and Adaptive Evolution</p>
<p>After establishing basic functionality through debugging, we transition to implementation planning focused on identifying critical structures and integration points.Our planning process first determines which core modules require modification, then develops a step-bystep implementation strategy with clear priorities and dependencies.</p>
<p>Implementation planning operates at multiple abstraction levels: architectural modifications for methodological alignment, algorithmic transformations for core functionality, and optimization adjustments for performance characteristics.This approach aims to provide structure when implementing methodological improvements across interconnected components in AI systems, which helps guide development efforts.</p>
<p>Rather than employing a single-pass implementation strategy, we designed an adaptive evolution approach for our implementation process.This approach involves structured iterations where each implementation attempt is followed by performance assessment and potential refinement.We maintain records of implementation decisions across iterations, which helps track changes and their corresponding effects.This directed adaptation process enables the gradual refinement of complex implementations based on empirical results rather than theoretical assumptions alone.</p>
<p>Experiments</p>
<p>In this section, we evaluate the effectiveness of NOVELSEEK in conducting autonomous research and accelerating scientific discovery.We begin by providing a brief overview of the selected multi-domain tasks and detailing the experimental implementation in Sec.3.1.Subsequently, we present the quantitative results across various tasks in Sec.3.2 and conduct an analysis of the different modules within NOVELSEEK in Sec.3.3.</p>
<p>Experimental Setup</p>
<p>Task Description</p>
<p>We select 12 distinct tasks to demonstrate NOVELSEEK's capability in conduct Autonomous Scientific Research (ASR).These tasks span multiple modalities, including science (e.g., reaction yield prediction, molecular dynamics), time series (e.g., time series forecasting), natural language (e.g., sentiment classification), image (e.g., semantic segmentation), and point cloud (e.g., 3D object detection), which cover both discriminative and generative tasks.We believe that experiments ranging from fundamental tasks to complex multi-modal tasks can comprehensively illustrate the effectiveness of NOVELSEEK.Below, we detail the datasets, the base code repositories, and the experimental settings for each task.</p>
<p>• Reaction Yield Prediction (AutoRYP).We conduct experiments on the widely-used Suzuki-Miyaura reaction dataset (Perera et al., 2018), which contains 5,760 reaction data.Each data point includes structured chemical reaction information, such as reactants, products, reaction types, reaction conditions (solvent, catalyst, ligand, and base), functional group, and yield values.We use the LoRA-finetuned LLaMA3-8B as our baseline, an embedding model that converts chemical reaction texts into high-dimensional vector representations, which are subsequently fed into a fully connected prediction network predictor to perform chemical yield prediction. • Molecular Dynamics (AutoMD).We conduct experiments on the widely-used MD17 dataset (Chmiela et al., 2017), which contains energy and force calculation results for seven small organic molecules: aspirin, ethanol, malonaldehyde, naphthalene, salicylic acid, toluene, and uracil.We use VisNet (Wang et al., 2024d) as our baseline, an equivariant geometry-enhanced graph neural network that achieves excellent chemical property prediction.• Power Flow Estimation (AutoPower).We conduct experiments on the IEEE 39-Bus dataset (Zimmerman et al., 2010), which is a medium-scale benchmark based on the New England power system, comprising 39 buses, 10 synchronous generators, 19 load buses and 46 transmission lines, and providing AC power flow snapshots under a variety of load conditions.We use SenseFlow (Zhao et al., 2024) as our baseline, a novel physics-informed, self-ensembling power flow estimation model that has demonstrated state-of-the-art accuracy on standard IEEE test systems consistently outperforming both traditional state-estimation techniques and recent data-driven approaches in voltage and power-flow recovery tasks.• Time Series Forecasting (AutoTSF).We conduct experiments on the ETTh1 dataset, which is a 1-hour-level subset of the Electricity Transformer Temperature (ETT) benchmark (Zhou et al., 2021).This dataset comprises two years of hourly multivariate time series, including the target oil temperature and six power-load covariates, collected from transformer stations in two Chinese counties.We use DLinear (Zeng et al., 2023)  We use BERT-base (Devlin et al., 2019) as our baseline, a Transformer-based pretrained language model that has shown excellent performance on various NLP tasks.• 2D Image Classification (Auto2DCls).We conduct experiments on the widelyused CIFAR-100 dataset (Krizhevsky et al., 2009), which contains 60,000 32×32 color images in 100 classes, with 500 training images and 100 testing images per class.We use Wide Residual Networks (WRN) (Zagoruyko, 2016) as our baseline, which improves performance by increasing the width rather than the depth of convolutional networks.• 3D Point Cloud Classification (Auto3DCls).We conduct experiments on the ModelNet40 dataset (Wu et al., 2015), which contains 12,311 CAD models across 40 common object categories and is widely used for 3D shape classification tasks.We use PointNet (Qi et al., 2017) as our baseline, a pioneering deep learning architecture that directly processes point cloud data.• 2D Semantic Segmentation (Auto2DSeg).We conduct experiments on the widelyused Pascal VOC 2012 dataset (Everingham et al., 2012), which includes 20 object classes and a background class for semantic segmentation tasks.The dataset contains 1,464 images for training and 1,449 for validation.We use DeepLabV3Plus (Chen et al., 2018) as our baseline method, which enhances segmentation performance by employing atrous convolution and a more refined encoderdecoder structure to capture multiscale contextual information effectively.• 3D Point Cloud Autonomous Driving (AutoPCDet).We conduct experiments on the widely-used dataset ONCE (Mao et al., 2021) and use CenterPoint (Yin et al., 2021) as our baseline.Our code is based on OpenPCDet (Team, 2020) and we filter out all code irrelevant to the baseline model to avoid knowledge leakage.• Large Vision-Language Model Fine-tuning (AutoVLM).We conduct experiments on filtered geometry subset of the URSA dataset (Luo et al., 2025), comprising manually curated multimodal QA pairs and CoT process.Natural images were excluded, and data were downsampled to control experimental budgets, enabling training completion within 20 hours on 8 A800 GPUs.We use LLaVA-Onevision (Li et al., 2024a) as our baseline, a robust multimodal alignment framework using a simple MLP to align visual encoders with LLMs, forming an effective LMM with strong scalability on vision-language tasks.We take SigLIP (Zhai et al., 2023) and Qwen2.5-Math-7B-Instruct(Yang et al.) as the visual and language modules, respectively.</p>
<p>Evaluation Metric</p>
<p>Since our NOVELSEEK has been validated across a wide range of scientific research fields, the evaluation metrics used for tasks in each field are not consistent.Therefore, in this part, we provide a detailed introduction to the evaluation metrics used for each scientific research task.</p>
<p>• AutoRYP.For Reaction Yield Prediction, we evaluate model performance using the coefficient of determination (R²), which quantifies the proportion of variance in the actual reaction yields that is predictable from the model's predictions.• AutoMD.Our method is evaluated on the MD17 dataset, a molecular chemical property prediction task.The performance is measured using Force-MAE, representing the mean absolute error between the true and predicted forces of molecules.• AutoSenCls.We evaluate our method on the SST-2 dataset, which is a binary sentiment classification task.The performance is measured using accuracy (Acc), which represents the percentage of correctly classified samples.</p>
<p>• Auto2DCls.For 2D image classification, we conduct experiments on CIFAR-100 dataset, which contains 100 classes.The performance is measured using classification accuracy (Acc), representing the percentage of correctly classified images.</p>
<p>• Auto3DCls.For the task of 3D point cloud classification, we use the widely adopted ModelNet40 benchmark, which comprises 40 distinct object categories.We report the Overall Accuracy (OA) as our primary evaluation metric, which calculates the proportion of correctly classified instances in the entire test set.</p>
<p>• Auto2DSeg.For 2D semantic segmentation, we conduct experiments on the Pascal VOC 2012 dataset, which includes 20 object classes and a background class.The performance is measured using the mean Intersection over Union (MIoU), which quantifies the average overlap between the predicted segmentation and the ground truth across all classes, providing a comprehensive assessment of the model's segmentation accuracy.</p>
<p>• AutoPCD.Following ONCE official evaluation metric, we merge the car, bus and truck class into a super-class (i.e., vehicle).AP 3D is used to evaluate the performance of the ONCE dataset, we report Mean average precision (mAP) which is the average of the scores of the three categories.</p>
<p>• AutoVLM.We evaluated our model on the geometry subset of MathVista (Lu et al., 2023), a widely adopted multimodal mathematical benchmark.Model's answers to questions were extracted using GPT-4o and compared against the ground truth to calculate accuracy.</p>
<p>Implementation Details</p>
<p>In the self-evolving idea generation process, the survey agent, code review agent, generation agent, self-evolving agent, and orchestration agent are based on GPT-4o (Hurst et al., 2024).</p>
<p>The survey agent searches and reviews 50 papers to provide domain knowledge for the subsequent idea generation agent, and then the idea generation agent generates 15 ideas.The self-evolving agent evolves each idea into 3 ideas and then selects the top 5 ideas for the next evolving process until the maximum number of evolutions (i.e., 4) is reached.In the idea-to-methodology process, each idea is initialized and refined once by the method development agent.In the evolutionary experimental planning and execution process.We use Claude-3.7-Sonnetto generate codes and debug.We set the max debug attempt to 4. The max run number is set to 5 for Aider (Gauthier &amp; Contributors, 2023) and 3 for OpenHands (Wang et al., 2024c).</p>
<p>Experimental Results</p>
<p>To comprehensively evaluate the effectiveness of NOVELSEEK in accelerating scientific discovery, we first provide quantitative experimental results as shown in Tab. 1, Tab. 2, Tab. 3, and Tab. 4. Extensive results demonstrate that NOVELSEEK excels in the following aspects:</p>
<p>• Outperforming existing auto-research systems on multiple tasks.We first compare NOVELSEEK with existing auto-research system (i.e., DOLPHIN (Yuan et al., 2025)) on single-file tasks.Tab. 1 and Tab. 2 show the max performance and average performance (i.e., the average performance across experiments with performance gains) achieved by NOVELSEEK and DOLPHIN.It can be observed that NOVELSEEK consistently improves the performance compared to the baseline and outperforms DOLPHIN across all tasks including both generative and discriminative tasks.This suggests that NOVELSEEK can generate better ideas on each specific domain benefiting from the self-evolving idea generation process and automatically implement them.For example, in AutoRYP, methods proposed by NOVELSEEK can largely   Besides, Tab. 3 and Tab. 4 report the percentage of experiments with performance gains and executable experiments out of the total number of experiments.First, results show that even on complex tasks such as AutoPCDet (i.e., 50%) and Auto2DSeg (i.e., 90%), NOVELSEEK can still ensure a reasonable execution success rate which is due to the carefully designed idea-to-methodology process, enabling the coder to auto-implement based on detailed methodologies.Second, NOVELSEEK demonstrates a higher performance improvement rate compared to DOLPHIN.This improvement is mainly attributed to the idea-to-methodology feature of NOVELSEEK, which enables the concretization of high-level ideas.Additionally, through the process of multi-round experimental planning and execution, the submodules of the AI-generated methodology are progressively integrated into the baseline code.</p>
<p>• Covering a wide range of tasks including the scientific research tasks and AI tasks.Further, NOVELSEEK exhibits strong generalization capability across a wide range of tasks, enabling it to handle tasks from the AI domain (e.g., Auto2DSeg) to the scientific domain (e.g., AutoMD).As shown in Tab. 1 and Tab. 2, NOVELSEEK can support 12 different tasks ranging from simple classification tasks to complex multimodal and cross-disciplinary tasks.This is because the survey agent in NOV-ELSEEK can auto-search task-related literature on academic websites such as arXiv and review the literature to understand each task.Besides, NOVELSEEK is highly extensible, as it can support new tasks with just a task description and reference codes.This capability not only assists AI researchers in automatically updating algorithms, but also empowers researchers in scientific domains to utilize AI tools at a lower cost, thereby accelerating the pace of scientific discovery.</p>
<p>• Support repo-level experiments.Most of existing auto-research systems such as DOLPHIN (Yuan et al., 2025) only support single-file experiments.On more complex tasks, researchers are required to manually consolidate complex task codes into a single file, which is highly time-consuming and limits their ability to conduct experiments on complex tasks.In contrast, NOVELSEEK can support repo-level tasks such as AutoPCDet, AutoVLM, AutoTPPR, and so on, and achieve better performance on these repo-level tasks compared to their baselines.For example, on Auto2DSeg, NOVELSEEK pipeline can improve the DeepLabV3Plus baseline (Chen et al., 2018) from the original 78.80% to 81.0%.This is attributed to the detailed methodology, code comprehension achieved by the code review agent, and the auto-exploration ability of the coder agent.</p>
<p>Runtime Statistics.We further provide the runtime statistics of NOVELSEEK on all 12 tasks including the training costs (i.e., GPU hours) and monetary costs in the idea generation stage (including self-evolving idea generation and idea-to-methodology) and code execution and debug stage.As shown in Tab. 5 and Tab. 6.As mentioned in Sec.3.1.3,we select top 5 ideas in each idea generation process and then generate detailed methodology for the selected ideas.Therefore, we report the average cost of 5 ideas as the idea generation cost.It can be seen that the idea generation cost of each idea is about $0.6 using GPT-4o which is cost-efficient.The coder-debug cost denote the cost of each run, for example, if running for 5 times for a single idea as mentioned in Sec.2.3, we calculate the average cost of 5 runs.</p>
<p>It can be seen from the table that the coder-debug cost varies between the file-level codes and repo-level codes and repo-level codes generally need more cost for high complexity of codes.For example, for single-file code such as Auto2DCls, the cost is below $1 for each run and for more complex AutoPCDet, the cost is about $1.2 using claude-sonnet-3.7.Generally, NOVELSEEK is a cost-efficient auto-research framework that can generate ideas and execute codes at a reasonable cost.</p>
<p>Insightful Analyses</p>
<p>Analysis on Survey Agent.As mentioned in Sec.2.1, survey agent mainly have two modes (i.e., the literature review mode and the deep research mode).As shown in Fig. 4 (a), under the literature review mode, the survey agent can search for domain-related papers and automatically select the most relevant literature to read and extract task-related information.</p>
<p>For example, the agent can identify works such as "Multimodal Transformer-based Model for Buchwald-Hartwig and Suzuki-Miyaura Reaction Yield Prediction" or "ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models" to quickly gather foundational studies in the field.Such a process is essential for idea generation process since the used agent may not have relevant domain knowledge, especially in emerging fields.Besides, under deep research mode, the survey agent needs to search for literature related to specific technical terms used in generated ideas.As shown in Fig. 4 (b), the agent updates its queries based on generated technical terms and retrieves papers like "Large Language Models to Accelerate Organic Chemistry Synthesis" which are closely aligned with these refined research directions.This process is highly similar to human researchers, they initially perform a comprehensive review of the relevant field to build foundational knowledge, and then search for articles focused on specific techniques to further refine the research direction.</p>
<p>Analysis on Idea Innovation Agent.Idea innovation agent can first generate ideas and then evolve the generated ideas in an iterative manner.We take the idea evolution tree as an example to show the iterative process of polishing ideas.As shown in Fig. 3, the root node  (i.e., Init Idea 0) denotes an initially generated idea and the child nodes are evolved from the parent node.As ideas continue to evolve, more external knowledge sourced from the survey agent is incorporated into ideas, which enriches the content and enhances the practicality of the ideas.For example, starting with a basic idea such as "adding a graph-derived reaction descriptor as a precondition for attention scores in the transformer architecture", the agent refines and evolves the ideas in an iterative manner.At each step, the evolved idea shows improvements over its predecessor in terms of technical sophistication, novelty, or practical applicability.As illustrated in Fig. 3, the process can involve incorporating more specific chemical descriptors, introducing cross-modality attention mechanisms, or leveraging hierarchical architectures, with each evolution step guided by additional insights from literature or domain knowledge, thus ensuring continuous advancement of the ideas.</p>
<p>Analysis on Idea-to-Methodology Phase.The correspondence between an idea and its final code implementation plays a crucial role in assessing the effectiveness of the idea since the idea can be verified once the experiments have been conducted.The goal of the idea-tomethodology process is to generate detailed methodologies so that code can be written based on these comprehensive method descriptions (e.g., method-level descriptions in research papers).As illustrated in Fig. 5, our idea-to-method approach enables the generation of fine-grained methodologies, which facilitates accurate and faithful code implementation.</p>
<p>Analysis on Evolutionary Experimental Planning and Execution.</p>
<p>To verify the effectiveness of the adaptive evolution (AE), we conduct ablation studies on three tasks, ranging from AI tasks to scientific tasks including AutoRYP, Auto2DCls, and AutoSenCls.As shown in Tab. 8, the performance can be further improved on multiple tasks with adaptive evolution.For example, on the image classification task, both the max accuracy and the mean accuracy can be improved by 1.7% and 0.7%, compared to the setting without AE.This is because our coder agent can automatically analyze the previous results and baseline results and further re-plan the following experiments.Besides, the successful execution rate and the percentage of performance gains will also improve (e.g., on AutoRYP, the percentage of performance gains is 40% compared to 20% without AE).This is due to with AE, the coder will implement the idea step by step and analyze the experimental phenomena after each stage of the experiments.</p>
<p>Improving Baseline in Multi-Dimension.NOVELSEEK not only can improve the performance on different tasks, but it also enhance the quality of ideas in other dimensions.For example, as shown in Tab. 7, on few-shot yield prediction task.We find that the results of baseline methods are unstable, for the results of multiple repeated experiments tend to exhibit large variance (e.g., 24.2±4.2 when train-set=60).In contrast, the methods proposed by NOVELSEEK can improve both the performance and the stability of the results.For example, when the train-set=60, ADAGT proposed by NOVELSEEK achieves 34.8 average R 2 across 5 repeated experiments compared to 24.2 achieved by the baseline method.Besides, the variance of the results achieved by ADAGT (i.e., ±1.1) is much lower than baseline methods (i.e., ±4.2).This phenomenon further shows the quality of ideas and code implementation of NOVELSEEK.</p>
<p>Comparison with AI-Researcher.We evaluate the performance and cost of NOVELSEEK and AI-Researcher (Lab, 2025) on AutoRYP and Auto2DCls research tasks.To ensure a fair comparison, we supplied AI-Researcher with the same code templates that NOVELSEEK uses.Both systems employ GPT-4o-2024-08-06 for idea generation and Claude-3-7-Sonnet-20250219 for code generation.As shown in Tab. 9, NOVELSEEK outperforms both the baseline methods and AI-Researcher across both tasks, whereas AI-Researcher is unable to improve the provided baselines.One important reason for NOVELSEEK's outstanding performance is its ability to generate novel ideas through extensive search and reflection.</p>
<p>Moreover, NOVELSEEK has a complete experimental planning and adaptive evolution mechanism, thereby enabling it to achieve better performance.In contrast, the idea generated by AI-Researcher is more dependent on user-provided reference papers, limiting its novelty.Moreover, AI-Researcher often ignore the prior information of the existing codebases, which further hinders its performance.In terms of cost, NOVELSEEK is significantly more economical than AI-Researcher.For an instance, the economic cost required for NOVELSEEK is approximately one-sixth that of AI-Researcher on the AutoRYP task.This lower cost allows</p>
<p>Method</p>
<p>Adaptive Dual-Attention Graph-Transformer with Dynamic Freezing for Chemical Reaction Yield Prediction</p>
<p>Enhancing SMILES and graph-derived reaction descriptor integration using a hybrid graph-transformer network that incorporates hierarchical tokenattention mechanisms at the molecular substructure and reaction-condition levels, combined with adaptive layer freezing for fine-tuning, will improve chemical yield prediction by leveraging both 1D and 2D molecular representations while minimizing overfitting.-Instead of static layer freezing, use gradient magnitudes to adapt dynamically with a formal threshold decay function: $$\tau_t = \tau_0 e^{-\lambda t},$$ where $\tau_0$ is the initial threshold, $t$ is the training epoch, and $\lambda$ controls decay rate.</p>
<p>-At each epoch, compute the gradient norm $G_l$ for layer $l$: $$G_l = |\nabla W_l|_2.$$-Freeze all layers $W_l$ where $G_l &lt; \tau_t$.This ensures only critical layers are trained as the dataset is gradually learned.</p>
<p>Case Studies</p>
<p>Qualitative Results</p>
<p>In this section, we present the results from various tasks implemented using NOVELSEEK.</p>
<p>Visual Examples of Various Tasks</p>
<p>We present showcases for three distinct tasks: AutoRYP, AutoMD, and AutoPower, to highlight the innovative methodologies discovered and their applications.These showcases are illustrated in Figs. 5, 6, and 9, respectively.Each task demonstrates a unique approach to solving complex problems, showcasing the potential impact of NOVELSEEK across different scientific domains.</p>
<p>In the AutoRYP task illustrated in Fig. 5, NOVELSEEK autonomously discovered an innovative approach called "Adaptive Dual-Attention Graph-Transformer with Dynamic Freezing" for predicting chemical reaction yields.This method effectively integrates SMILES and graph-derived descriptors using a hybrid graph-transformer network, incorporating hierarchical attention mechanisms to enhance accuracy while minimizing overfitting.The approach features a Dual-Attention Fusion Mechanism (DAFM) that systematically com-  1. Initialize $\mathbf{h}_i^{(0)}$ for all nodes.</p>
<ol>
<li>For each layer $t = 1, \dots, T$: a. Compute augmented geometric encodings $\mathbf{g}_{ijk}^{(\mathrm{aug})}$.</li>
</ol>
<p>b. Calculate attention weights $\alpha_{ij}$ using GEDA.</p>
<p>c. Aggregate atomic features $\mathbf{m}<em>i$ and update embeddings $\mathbf{h}</em> i^{(t+1)}$.</p>
<ol>
<li>Group nodes into substructures and compute substructural embeddings $\mathbf {h}_c$.</li>
</ol>
<p>Aggregate global features for energy prediction $E(G)$:</p>
<p>$$ E(G) = g\left( \sum_{c \in C} \mathbf{W}_E \mathbf{h}_c \right), $$ where $g(\cdot)$ is a differentiable pooling function.</p>
<ol>
<li>Backpropagate energy gradients to compute forces $\mathbf{F}_i = -\partial E(G)/\partial \mathbf{r}_i$.</li>
</ol>
<p>class HEDGE_MP(MessagePassing):</p>
<h1>HEDGE-Net Message Passing with Geometry-Enhanced Directional Attention (GEDA) def <strong>init</strong>(self, num_heads, hidden_channels, activation, attn_activation, cutoff, vecnorm_type, trainable_vecnorm, last_layer=False): super(HEDGE_MP, self).<strong>init</strong>(aggr="add",node_dim=0) ... def reset_parameters(self):</h1>
<p>... def forward(self, x, vec, edge_index, r_ij, f_ij, d_ij):</p>
<p>x = self.layernorm(x)vec = self.vec_layernorm(vec)# Compute node features q = self.q_proj(x).reshape(-1,self.num_heads,self.head_dim)k = self.k_proj(x).reshape(-1,self.num_heads,self.head_dim)v = self.v_proj(x).reshape(-bines token and graph embeddings with reaction conditions, ensuring effective information flow across different representations.Furthermore, a dynamic layer freezing mechanism, based on gradient magnitudes, optimizes which layers are trained, thereby enhancing generalization in low-data scenarios.Implemented with self-attention and cross-modality attention modules, this system not only combines 1D and 2D molecular representations effectively but also improves prediction accuracy and model adaptability.Consequently, this showcase underscores the method's potential for advancing research in complex chemical tasks using deep learning.</p>
<p>In the AutoMD task illustrated in Fig. 6, a novel framework called "Hierarchical Equivariant Directional Graph Encoder" (HEDGE-Net) has been autonomously discovered for predicting molecular energy and forces.This approach utilizes SE(3)-equivariant graph neural networks with hierarchical geometric self-attention and multi-hop message enrichment.By integrating angular and directional features into aggregated substructures, the method captures interacting atomic patterns and propagates dynamic weight updates, aligning with both local and global molecular geometries.The core of this method, the Geometry-Enhanced Directional Attention (GEDA) mechanism, ensures SE(3)-equivariance, enabling precise predictions for complex molecular systems at both atomic and substructural scales.Implemented with advanced message passing techniques, HEDGE-Net effectively combines directional and substructural information, enhancing scalability and precision in molecular modeling.This showcases the method's potential to advance research in complex molecular tasks using deep learning techniques.</p>
<p>Visual Examples of Experimental Planning and Adaptive Evolution</p>
<p>To further illustrate the practical utility of our experimental planning and adaptive evolution framework as described in Sec.2.3.2,we present some concrete examples of its application in the development and optimization for 3D point cloud classification and transcription prediction for perturbation response.Fig. 7 and Fig. 8 visually summarize the stepwise experimental planning and adaptive evolution process that guided the implementation and refinement of our method.</p>
<p>Human Evaluation</p>
<p>Table 10 compares the novelty of ideas generated by our NOVELSEEK and AI-Scientist-V2 (Yamada et al., 2025), across various research tasks.Each task involves generating 20 ideas, which are evaluated by five qualified reviewers.The assessments focus on four Table 10: From the perspectives of soundness, contribution, and overall, we compare the novelty of ideas generated by NOVELSEEK and AI-Scientist-V2 (Yamada et al., 2025).For each research task, we generate 20 ideas.Each idea is scored by 5 qualified reviewers, and the final score for each task is reported as the average score of all 20 ideas.The detailed scores for each idea can be found in the Appendix B.1.</p>
<p>Research Task Idea-gen Method Soundness Contribution Overall Confidence</p>
<p>Reaction Yield Prediction AI-Scientist-V2 criteria: soundness, contribution, overall rating, and confidence.For each research task, the average scores of the 20 ideas are reported.</p>
<p>In the Reaction Yield Prediction task, NOVELSEEK outperforms AI-Scientist-V2 in all aspects, especially in overall rating and soundness.Similarly, for 2D Semantic Segmentation, NOV-ELSEEK shows better idea generation ability, particularly in soundness and overall rating.In 2D Image Classification and Point Cloud Autonomous Driving, NOVELSEEK scores higher across all criteria, indicating a consistent advantage over AI-Scientist-V2 in generating more effective and novel ideas.</p>
<p>Related Works</p>
<p>Recent advances in Large Language Models (LLMs) and agent-based systems have demonstrated significant potential in the field of Autonomous Scientific Research (ASR), enabling progress from creative idea generation to end-to-end research automation.Some studies (Li et al., 2024b;Wang et al., 2023;Zhou et al., 2024) have shown that LLMs are capable of generating novel research ideas, which has sparked widespread discussion in the academic community.For example, Li et al. (2024b) introduce a method that derives research ideas through the analysis of interconnected scholarly works.Beyond idea generation, several studies have examined the use of LLMs for hypothesis formulation (Qi et al., 2023;2024), such as extracting hypotheses from large-scale web data (Yang et al., 2023), multi-agent framework using LLMs to enhance collaborative hypothesis generation in biomedicine (Qi et al., 2023;2024), and scientific literature (Wang et al., 2023;Zhou et al., 2024).However, most of these efforts remain at the stage of idea or hypothesis generation, lacking systematic empirical validation of their practical effectiveness.</p>
<p>In terms of end-to-end research automation, Lu et al. (2024) introduced the AI Scientist framework, which was among the first to achieve a fully automated pipeline in the machine learning domain, covering problem definition, experimental execution, and result reporting.</p>
<p>The subsequent AI Scientist-V2 (Yamada et al., 2025) further enhanced the framework by incorporating agent tree search, vision-language model feedback, and parallelized experiment execution, leading to the first workshop paper fully generated and peer-reviewed by AI.</p>
<p>Similarly, systems such as AI-Researcher (Lab, 2025) and Dolphin (Yuan et al., 2025) have proposed closed-loop, LLM-driven frameworks that automate the entire research process on a range of simple tasks.</p>
<p>Human-AI collaboration is gaining traction in ASR.Systems like Agent Laboratory (Schmidgall et al., 2025) integrate human feedback into multi-stage LLM agent workflows, automating literature review, experiment execution, and report writing, while allowing user input at each step to enhance research quality.AgentRxiv (Schmidgall &amp; Moor, 2025) addresses the collaborative nature of scientific discovery by enabling LLM agent laboratories to communicate and build upon each other's work via a shared preprint server, thus facilitating knowledge sharing and collective innovation.Experimental results demonstrate that agent laboratories utilizing AgentRxiv for collaboration achieve greater performance improvements compared to isolated settings.Similarly, AI Co-Scientist (Gottweis et al., 2025), based on Gemini 2.0, employs a multi-agent system with a "generate-debate-evolve" strategy for hypothesis generation, and has demonstrated effectiveness in biomedical domains such as drug repurposing, novel target identification, and interpretation of bacterial evolution, with several hypotheses validated through experiments.</p>
<p>Despite these advances, most current systems are still evaluated primarily on relatively simple tasks or within narrow scientific domains.However, when applied to more complex, system-level scientific challenges, these approaches often face significant limitations.Key challenges include generating truly novel and scientifically sound research ideas, establishing robust closed-loop feedback between experiments and idea generation, and developing systematic evaluation standards to rigorously assess the effectiveness and real-world value of autonomous research systems.</p>
<p>Conclusion and Future Works</p>
<p>Summary.We have introduced a closed-loop multi-agent framework for the first time, which supports 12 types of scientific research tasks.It has been validated to generate novel ideas and transform these ideas into code that can effectively improve performance.NOV-ELSEEK refines the initially generated ideas through human-interactive feedback enriched with a self-evolutionary path of ideas.It facilitates the transformation from coarse-grained proposals to fine-grained methodologies via an idea-to-methodology construction process.Furthermore, by leveraging multi-round experimental planning and execution, it implements the corresponding theoretical methods, thereby completing the closed-loop process in scientific research-from hypothesis generation to verification.</p>
<p>Future Outlook.NOVELSEEK faces several key technical challenges that need to be addressed in the future:</p>
<p>• Knowledge Retrieval: This involves establishing connections and relationships between papers, utilizing deep research techniques to conduct extensive searches across a wide range of papers, and performing meta-analyses on the search results.Additionally, it requires transforming the papers into structured representations such as triples, and utilizing graph networks to uncover relationships between papers, including associations in paper ideas, methodologies, experimental conditions, and experimental results.Moreover, retrieval-augmented generation will be employed to alleviate the hallucination issues of LLMs when generating viewpoints or citing references.</p>
<p>• Knowledge Understanding and Representation: This involves utilizing VLM/LLM to accurately analyze relevant academic papers, aiming to understand the significance of their core concepts, methodologies, and research findings, while also refining knowledge and formulating hypotheses.Additionally, it focuses on extracting valuable knowledge from a large number of papers, identifying common patterns, trends, and connections, thereby advancing the understanding and representation of knowledge in the field.</p>
<p>• Agent Capability Enhancement: This focuses on improving the ability of AI systems to autonomously perform complex tasks in scientific research.The strength of agents lies in their ability to dynamically adapt, rather than solely relying on historical records to determine subsequent actions.Through self-modification, they can flexibly redefine their initial goals and planning strategies while utilizing feedback, as well as communication logs between agents or between humans and agents, to train and improve themselves.This mechanism should focus on improving their ability to gather feedback from three key sources: the environment, interactions with other agents, and human experts.</p>
<p>• Scientific Discovery-related Benchmark Construction: This involves evaluating the value that an idea can bring, rather than simply evaluating its novelty.It also in-cludes evaluating whether the methods proposed by AI align with their corresponding code implementations and determining whether NOVELSEEK demonstrates a certain level of generalization ability in broader scientific scenarios.It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.• Confidence: 3: You are fairly confident in your assessment.It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.Math/other details were not carefully checked.</p>
<p>• Confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work.Math/other details were not carefully checked.</p>
<p>• Confidence: 1: Your assessment is an educated guess.The submission is not in your area or the submission was difficult to understand.Math/other details were not carefully checked.</p>
<p>B.2 Information on the Expert Review Process</p>
<p>Qualifications for Human Evaluators: Evaluators must hold a Ph.D. or be Ph.D. candidates with reviewing experience in top-tier AI conferences such as ICLR, ICML, NeurIPS, CVPR, ICCV, and ACL.</p>
<p>Steps for Expert Evaluation and Validation:</p>
<p>• Before the evaluation begins, evaluators are required to carefully read the scoring guidelines, as outlined in Appendix B.1.</p>
<p>• Each evaluator is assigned 20 ideas generated by NOVELSEEK and 20 ideas generated by AI-Scientist-V2 (Yamada et al., 2025).For each idea, evaluators must carefully review the generated content and provide final scores across four dimensions: Soundness, Contribution, Overall, and Confidence.</p>
<p>• Qualified evaluators are required to spend at least 10 minutes reading each idea.</p>
<p>During the evaluation process, they are allowed to conduct relevant literature searches and verify idea redundancy to ensure that the scoring results are objective and representative.</p>
<p>C NOVELSEEK Software Development</p>
<p>Fig. 13 shows the front-end interface of the current NOVELSEEK software platform.Overall, NOVELSEEK software platform employs a frontend-backend separation design pattern, building a highly scalable distributed service platform.</p>
<p>The frontend layer is developed based on the React framework, featuring an advanced visual interaction system.Key innovations include an infinite canvas rendering engine supporting multi-node topology, a collaborative mind mapping component driven by state synchronization, a code editor supporting multiple formats, and a real-time training metrics visualization dashboard.</p>
<p>The backend leverages a cloud-native technology stack, utilizing a dynamic container orchestration engine for elastic resource scheduling, a distributed asynchronous task queue for high concurrency support, and a cross-cloud storage gateway for data synchronization across heterogeneous cloud environments.Additionally, a microservice governance system is established using a Service Mesh architecture.</p>
<p>The entire system is delivered through containerization, with Kubernetes cluster management enabling self-healing and intelligent scaling, ensuring business continuity while significantly improving resource utilization efficiency.</p>
<p>D Visualization Results</p>
<p>We further conducted detailed visualization and analysis around the multi-round experimental planning and execution (Fig. 8) in NOVELSEEK, as well as the automated scientific research tasks (Figs. 9, 10, 11 and 12) supported by NOVELSEEK.This method refines and extends transformer-based sentiment analysis on the SST-2 dataset by introducing a mathematically formalized and algorithmically detailed hybrid noise augmentation approach.The refinement integrates psycholinguistically-grounded neural text generation with rule-based handling of sarcasm, negation, and polysemy through a unified framework.The approach uses adversarial benchmarks like TextFlint for robustness evaluation under noisy and low-resource conditions, promoting reproducibility and practical feasibility.</p>
<p>Method Hierarchical Adaptive Regularization and Consistency Network for Robust Image Classification</p>
<p>Develop a synergistic hierarchical framework for improving image classification that integrates (a) adaptive, data-distribution-driven augmentation mechanisms combining MixUp with dynamically controlled geometric transformations to enhance data robustness, (b) a decayed temporal consistency regularization method that minimizes stochastic noise in pseudo-labels using exponentially weighted past predictions, and (c) auxiliary losses that explicitly optimize feature group consistency inspired by human visual pattern differentiation, with additional grounding in loss function calibration.This framework will be rigorously evaluated using ablation studies focusing on compatibility and interaction among augmentation, regularization, and loss mechanisms, alongside benchmarking against ResNet and Vision Transformer models on CIFAR-100.</p>
<p>HARCNet combines hierarchical adaptive augmentation with mathematically grounded regularization mechanisms inspired by human visual processing to improve robustness in image classification tasks.The method integrates (1) an adaptive augmentation mechanism that dynamically modulates geometric transformations based on data distribution, and (2) a decayed temporal consistency regularization framework underpinned by formal mathematical formulations, ensuring smoother pseudo-labeling and improved convergence.These components collaborate synergistically to achieve robust classification performance on CIFAR-100.Figure 13: NOVELSEEK software platform includes features such as the user entry interface, task selection interface, idea-tree visualization and human-computer interaction interface, code generation, and auto-debug interface.In the near future, we plan to support additional functionalities, including custom dataset uploads and academic idea thinking modes.</p>
<p>Figure 1 :
1
Figure 1: NOVELSEEK can support 12 types of scientific research tasks ranging from the AI field to the science field, including reaction yield prediction, molecular dynamics, power flow estimation, time series forecasting, transcription prediction, enhancer activity prediction, sentiment classification, 2D image classification, 3D point classification, 2D semantic segmentation, 3D autonomous driving, large vision-language model fine-tuning.</p>
<p>Figure 2 :
2
Figure 2: NOVELSEEK covers three main capabilities: 1) Self-evolving Idea Generation with Human-interactive Feedback, 2) Idea-to-Methodology Construction, and 3) Evolutionary Experimental Planning and Execution.</p>
<p>Figure 3 :
3
Figure 3: NOVELSEEK Self-evolutionary path of ideas for reaction yield prediction task.</p>
<p>Figure 4 :
4
Figure 4: Analysis of two modes on survey agent.</p>
<ol>
<li><strong>Dual-Attention Fusion Mechanism (DAFM)</strong>: -<strong>Local Attention within each Modality</strong>: Employ self-attention for intra-modal dependencies: $$H^S = \text{SelfAttention}(E_S), \quad H^G = \text{SelfAttention}(E_G), \quad H^Z = \text{SelfAttention}(Z)$$ -<strong>Cross-Modality Attention</strong>: Fuse localized representations with pairwise crossattention among modalities.Each input modality queries the others, ensuring representation exchange, H^Z), \quad H^{GZ} = \text{CrossAttention}(H^G, H^Z).$$ -<strong>Hierarchical Aggregation</strong>: Aggregate fused representations with weighted gating for final representation: $$H_\text{joint} = \alpha_1 H^{SG} + \alpha_2 H^{SZ} + \alpha_3 H^{GZ},$$ 2. <strong>Dynamic Layer Freezing</strong>:</li>
</ol>
<p>Figure 5 :
5
Figure 5: Visual Examples of AutoRYP Task.</p>
<p><strong>: Molecular graph $G = (V, E)$, features $\mathbf{h}<em ij="ij">i$, position vectors $\mathbf{d}</em>$.</strong>Output**: Energy prediction $E(G)$, atomic forces $\mathbf{F}_i$.</p>
<p>Figure 6 :
6
Figure 6: Visual Examples of AutoMD Task.</p>
<p>MethodFigure 7 :
7
Figure 7: Visual Examples of Experimental Planning and Adaptive Evolution on Auto3DCls task.</p>
<p>Fig. 8
8
Fig.8illustrates the process of Experimental Planning and Adaptive Evolution on AutoTPPR.Each block in the figure represents a step in the multi-round experiment planning process, where the complete NOVELSEEK-generated method is decomposed into multiple logical</p>
<p>dataset (D), psycholinguistic features (P), linguistic rules (L) -For each token in D: -Compute psycholinguistic importance scores (S) covering valence, arousal, and dominance -Normalize S to appropriate scaling ranges for noise generation -Identify tokens susceptible to sarcasm, negation, and polysemy phenomena 2. ** Hybrid Noise Generation<strong> -For each token embedding e: -Generate psycholinguistic neural noise component: $$e' = e + \mathcal{N}(0, \sigma^2 \cdot S)$$ where S reflects psycholinguistic importance scores -Apply linguistic rule-based transformations: $$e'' = R_{\text{rule}} \cdot e$$ using appropriate rule matrices for sarcasm, negation, or polysemy -Compute final hybrid embedding: $$e_\text{aug} = \alpha e' + (1 -\alpha)e''$$ with α ∈ [0,1] controlling the balance 3. ** Transformer Integration</strong> -For each batch during training: -Replace original token embeddings with hybrid embeddings e_aug -Modify attention mechanism to incorporate psycholinguistic alignment: $$ A_{\text{aug}} = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \gamma \cdot H\right) $$ where H emphasizes linguistic phenomena relevance -Propagate augmented representations through transformer layers</p>
<p>Figure 10 :
10
Figure 10: Visual Examples of AutoSenCls Task</p>
<p>def get_aggregated_predictions(self, indices):""" Get aggregated predictions for each sample using decay weights ỹ_i = Σ(ω_k • ŷ_i^(t-k)) """ weights = self.compute_decay_weights().to(indices.device)... def compute_consistency_loss(self, current_preds, indices): # Compute consistency loss between current and aggregated past predictions ...</p>
<p>Figure 12 :
12
Figure 12: Visual Examples of Auto2DCls Task</p>
<p>Transcription Prediction for Perturbation Response (AutoTPPR). We
NeuralNetworks (GNNs) and Multi-Layer Perceptrons (MLPs), designed to learn jointrepresentations of single-cell multi-omics data.• Enhancer Activity Prediction (AutoEAP). We conduct experiments on the UMI-STARR-seq dataset (Arnold et al., 2013), which contains genome-wide, high-resolution quantitative activity maps of developmental and housekeeping en-hancers in Drosophila S2 cells. We use DeepSTARR (de Almeida et al., 2022) as ourbaseline, a deep learning model that excels at quantitatively predicting enhanceractivity from DNA sequences.• Sentiment Analysis (AutoSenCls). We conduct experiments on the Stanford Senti-ment Treebank (SST-2) dataset (Socher et al., 2013), a binary sentiment classificationdataset consisting of movie reviews with approximately 67,000 training samples.
(Roohani et al., 2024)P-based forecasting model that decomposes each series into trend and seasonality and employs simple linear layers, outperforming Transformer-based methods on multiple time series benchmarks.We report the average results of 96, 192, 336, and 720 prediction length.•conductexperimentson the Perturb-seq dataset(Norman et al., 2019), which contains singlecell gene expression data measuring the transcriptional responses of cells to various perturbations.We use GEARS (Generative Energy-based Autoencoder for scRNAseq)(Roohani et al., 2024)as our baseline, a framework based on Graph</p>
<p>•</p>
<p>AutoPower.For Power Flow Estimation, we use Root Mean Square Error (RMSE) on PQ node to evaluate the estimation performance on IEEE 39-Bus datasets, representing the root mean square error between the true and predicted voltage magnitudes and phase angles.•AutoTSF.For Time Series Forecasting, we use Mean Absolute Error (MAE) to evaluate the prediction performance on ETTh1 dataset.The performance is calculated by taking the average of the four prediction steps of {96, 192, 336, 720}.• AutoTPPR.For Transcription Prediction for Perturbation Response, we employ</p>
<p>the Top 20 DE MSE as the evaluation metric, calculating the mean squared error between the predicted and actual expression levels of the top 20 most differentially expressed genes under each perturbation condition.•AutoEAP.For Enhancer Activity Prediction, we use Housekeeper Pearson Correlation Coefficient (HK-PCC) as the metric, which quantifies the correlation between the true enhancer activities and the predicted values.</p>
<p>Table 1 :
1
Performance comparison across six types of scientific research tasks.We conduct experiments using 10 NOVESEEK generated ideas for each task.
Tasks</p>
<p>Table 2 :
2
(Yuan et al., 2025)son for six types of scientific research tasks.We conduct experiments using 10 NOVESEEK generated ideas for each task, where baseline codes for Auto2DSeg, AutoPCDet, and AutoVLM are project-level, consisting of multiple code files with complex call relation between functions.Therefore, the coder in Dolphin(Yuan et al., 2025)does not support modifying this type of baseline codes.
Tasks</p>
<p>.5 (+1.5) 82.2 (+1.0) 93.4 (+2.4) 80.1 (+1.3) 65.7 (+0.7) 67.6 (+0.5) outperform</p>
<p>those proposed by DOLPHIN (i.e., +3.6 on max performance).We highlight that NOVELSEEK can achieve SoTA performance on some tasks such as 3D point cloud classification (i.e., 95.5% overall accuracy without pre-training achieved by NOVELSEEK compared to 95.3% overall accuracy with pre-training achieved by human experts).</p>
<p>Table 3 :
3
Experiments statistics across different tasks.Each cell shows the number of ideas that improved performance, the number of ideas that successfully ran, and the total number of ideas tested (format: improved / successful / tested).For all the tasks, we conduct experiments with 10 ideas.
Research Task</p>
<p>Table 4 :
4
Experiments statistics across different tasks.Each cell shows the number of ideas that improved performance, the number of ideas that successfully ran, and the total number of ideas tested (format: improved / successful / tested).For all the tasks, we conduct experiments with 10 ideas.
Research Task</p>
<p>Table 5 :
5
Computational and financial cost analysis for all tasks.Training time is measured using A100 GPU hours, while idea generation and code debugging costs are measured in USD using gpt-4o and claude-sonnet-3.7 models respectively.
Cost MetricAutoRYP AutoMD AutoPower AutoTSF AutoTPPR AutoEAPTraining time (A100 hours)6.010.05.00.11.01.0Idea-Gen cost (gpt-4o) ($)0.60.60.60.60.60.6Coder-Debug cost (claude-sonnet-3.7) ($)0.70.51.00.40.90.6</p>
<p>Table 6 :
6
Computational and financial cost analysis for all tasks.Training time is measured using A100 GPU hours, while idea generation and code debugging costs are measured in USD using gpt-4o and claude-sonnet-3.7 models respectively.
Cost MetricAuto2DCls Auto3DCls AutoSenCls Auto2DSeg AutoPCDet AutoVLMTraining time (A100 hours)2.00.80.330.09.0192.0Idea-Gen cost (gpt-4o) ($)0.60.60.60.60.60.6Coder-Debug cost (claude-sonnet-3.7) ($)0.70.60.71.11.21.0</p>
<p>Table 7 :
7
To compare the performance of the baseline and NOVELSEEK-generated code, we adopted a few-shot training setup for the yield prediction task.Due to the large variance in experimental results under this setting, we report the outcomes of 5 independent repeated experiments.
Epoch=300Repeat=1 Repeat=2 Repeat=3 Repeat=4 Repeat=5 AVG/VARBaseline (train-set=60)20.026.227.626.620.124.2±4.2GAT (ours, train-set=60)34.734.833.932.734.234.1±1.4ADAGT (ours, train-set=60)35.435.234.535.233.734.8±1.1Baseline (train-set=100)38.830.634.839.034.535.5±4.9GAT (ours, train-set=100)36.939.134.441.435.037.4±4.0ADAGT (ours, train-set=100)38.538.038.637.940.438.7±1.7</p>
<p>Table 8 :
8
Ablation Study on Adaptive Evolution (AE).Ideas (i/s/t) shows the number of ideas that improved performance, the number of ideas that successfully ran, and the total number of ideas tested (format: improved / successful / tested).
MethodAutoRYPAuto2DClsAutoSenClsMax R 2 Avg R 2 Ideas (i/s/t) Max Acc Avg Acc Ideas (i/s/t) Max Acc Avg Acc Ideas (i/s/t)Baseline27.627.6-81.281.2-91.091.0-NOVELSEEK (w/o AE)34.733.02/5/1081.681.52/5/1092.491.96/8/10NOVELSEEK35.433.54/6/1083.382.25/7/1093.592.59/9/10Initital Keyword Queries:LLaMA, LoRA adapter, regression model, reaction yield prediction, SMILES fine-tuningMode: Literature ReviewMode:Deep Research1. DRG-LLaMA : Tuning LLaMA Model to PredictNew Keyword Queries:Diagnosis-related Group for Hospitalized PatientsAI chemistry, molecular property prediction, reactionmodeling, transformer, SMILES optimization2. LoRA-Pro: Are Low-Rank Adapters ProperlyOptimized1. Pre-training Transformers for Molecular Property3. Multimodal Transformer-based Model for Buchwald-Prediction Using Reaction PredictionHartwig and Suzuki-Miyaura Reaction Yield Prediction2. Demystifying Molecules: Unveiling the Power of AI in4. ReacLLaMA: Merging chemical and textualComputational Chemistryinformation in chemical reactivity AI models3. Permutation invariant graph-to-sequence model for5. HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models Based on Hybrid Compute-in-template-free retrosynthesis and reaction prediction …Memory ArchitectureNew Keyword Queries:large language models, chemistry reaction yield6. Regression with Large Language Models foroptimization, embedding molecular representationMaterials and Molecular Property Prediction7. AI-Guided Design of MALDI Matrices: Exploring the Electron Transfer Chemical Space for Mass1. Large Language Models to Accelerate Organic Chemistry SynthesisSpectrometric Analysis of Low-Molecular-Weight Compounds2. Exploring BERT for Reaction Yield Prediction: Evaluating the Impact of Tokenization, Molecular8. Generative LLMs in Organic Chemistry:Representation, and Pretraining Data Augmentation.Transforming Esterification Reactions into Natural Language Procedures3. Chemist-X: Large Language Model-empowered Agent for Reaction Condition Recommendation in Chemical…Synthesis …</p>
<p>Table 9 :
9
Comparison with AI-Scientist-V2 and AI-Researcher on AutoRYP and Auto2DCls task.Total cost means the cost of the whole session.For each task, we conduct 10 experiments.AI-Scientist-V2 and AI-Researcher demonstrate relatively weak baseline improvement capabilities, with AI-Scientist-V2 in particular struggling to write code that runs correctly.The primary reason lies in the fact that AI-Scientist-V2's pipeline utilizes limited task-related information (e.g.task formulation and type, relevant papers, and commonly used code) when generating new ideas or coding.As a result, their generated ideas tend to be more divergent and difficult to implement.
MethodAutoRYPAuto2DClsMax R 2 Avg R 2 Total Cost Max Acc Avg Acc Total costBaseline27.627.6-81.281.2-AI-Scientist-V2 (Yamada et al., 2025)--15$--10$AI-Researcher (Lab, 2025)12.3-25$80.3-32$NOVELSEEK35.433.53$83.382.23$</p>
<p>Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI and excellent impact on multiple areas of AI, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.•Rating: 8: Strong Accept: Technically strong paper with, with novel ideas, excellent impact on at least one area of AI or high-to-excellent impact on multiple areas of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.• Rating: 7: Accept: Technically solid paper, with high impact on at least one subarea of AI or moderate-to-high impact on more than one area of AI, with good-You are confident in your assessment, but not absolutely certain.
• 3 good• 2 fair• 1 poorContribution:• 4 excellent• 3 good• 2 fair• 1 poorOverall:• Rating: 10: Award quality: Technically flawless paper with groundbreaking impacton one or more areas of AI, with exceptionally strong evaluation, reproducibility,and resources, and no unaddressed ethical considerations.• Rating: 9: to-excellent evaluation, resources, reproducibility, and no unaddressed ethicalconsiderations.• Rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, withno major concerns with respect to evaluation, resources, reproducibility, ethicalconsiderations.• Rating: 5: Borderline accept: Technically solid paper where reasons to acceptoutweigh reasons to reject, e.g., limited evaluation. Please use sparingly.• Rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g.,limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please usesparingly.• Rating: 3: Reject: For instance, a paper with technical flaws, weak evaluation,inadequate reproducibility and incompletely addressed ethical considerations.• Rating: 2: Strong Reject: For instance, a paper with major technical flaws, and/orpoor evaluation, limited impact, poor reproducibility and mostly unaddressedethical considerations.• Rating: 1: Very Strong Reject: For instance, a paper with trivial results or unad-dressed ethical considerationsConfidence:• Confidence: 5: You are absolutely certain about your assessment. You are veryfamiliar with the related work and checked the math/other details carefully.• Confidence: 4:</p>
<h1>Multi-head attention query = self.query_conv(x).view(batch_size,self.num_heads,self.head_dim,num_points)...MethodHierarchical Graph-Attentive Network for Efficient 3D Point Cloud Classification Developing a lightweight hierarchical graph neural network equipped with optimized local neighborhood graph construction and self-attentive feature alignment for 3D point cloud classification, aiming to balance computational efficiency and robust feature representation leveraging hierarchical adaptation to point density and global context.This framework incorporates density-sensitive graph construction that dynamically adjusts neighborhood relationships based on local point distributions, coupled with multi-level graph partitioning to capture both fine-grained details and global structures.The optimized multi-head self-attention mechanism enables effective cross-level feature alignment, creating a framework that efficiently processes irregular point clouds while maintaining robustness to density variations.An enhanced method combining a refined hierarchical graph neural network (HGNN) with density-sensitive neighborhood graph construction and an optimized multi-head self-attentive mechanism tailored for 3D point cloud classification.The approach introduces explicit mathematical modeling of graph construction, hierarchical adaptation, and attention mechanisms, aimed at improving computational efficiency and feature robustness.The hierarchical graph construction explicitly models local-toglobal relationships in a density-adaptive manner while the self-attention mechanism is redefined to align point-level and graph-level features effectively.Idea class AdaptiveAugmentation: def <strong>init</strong>(self, alpha=0.5, beta=0.5, gamma=2.0):self.alpha= alpha self.beta= beta self.gamma= gamma self.device= torch.device("cuda"if torch.cuda.is_available()else "cpu") {} # Store past predictions for each sample def compute_decay_weights(self): weights = torch.exp(-torch.arange(1,self.memory_size+ 1) / self.decay_rate)return weights / weights.sum()</h1>
<p>def compute_variance(self, x):# shape: [B, C, H, W]# Compute variance across channels for each spatial locationvar = torch.var(x, dim=1, keepdim=True) # [B, 1, H, W]return var.mean(dim=[1, 2, 3]) # [B]def compute_entropy(self, probs):# probs shape: [B, C] where C is number of classesprobs = torch.clamp(probs, min=1e-8, max=1.0)...def get_geometric_strength(self, x, model=None, probs=None):var = self.compute_variance(x)# If model predictions are provided, use them for entropy calculation...def get_mixup_params(self, y, num_classes=100):Idea# Generate MixUp parameters based on label entropy y_onehot = F.one_hot(y, num_classes=num_classes).float()def knn(x, k): ..."""Compute k-nearest neighbors for each point in the point cloud def apply_mixup(self, x, y, num_classes=100):""" """Apply MixUp augmentation with adaptive coefficient"""...lam, index = self.get_mixup_params(y, num_classes)...def get_graph_feature(x, k=20, idx=None, dim9=False):"""Construct edge features for each point""" class TemporalConsistencyRegularization:... def <strong>init</strong>(self, memory_size=5, decay_rate=2.0, consistency_weight=0.1):def density_adaptive_knn(x, k_min=10, k_max=30, density_threshold=0.1, use_xyz_only=True): self.memory_size = memory_size"""Enhanced density-adaptive k-nearest neighbors with improved density estimation""" self.decay_rate = decay_rate...self.consistency_weight = consistency_weightself.prediction_history =class GraphConvolution(nn.Module):…class GraphAttention(nn.Module):"""Enhanced Graph Attention Layer with residual connections"""def <strong>init</strong>(self, in_channels, out_channels, num_heads=8, dropout=0.1):super(GraphAttention, self).<strong>init</strong>()...def forward(self, x):batch_size, _, num_points = x.size()residual = x# Compute attention scoresattention = torch.matmul(query.permute(0, 1, 3, 2), # [B, num_heads, N, N]...# Apply attention to valuesout = torch.matmul(attention, value.permute(0, 1, 3, 2)) # [B, num_heads, N, head_dim]...return outclass HierarchicalGraphLayer(nn.Module):"""Optimized Hierarchical Layer with efficient feature interaction"""def <strong>init</strong>(self, in_channels, out_channels, k=20, pool_ratio=0.25):...def forward(self, x, adaptive_knn=False):...class HGANet(nn.Module):...def forward(self, x, gts=None):"""Args:x: input point cloud, [B, C, N]gts: ground truth labels, [B]Returns:loss: loss value if gts is not Nonelogits: classification logits, [B, num_classes]"""...class ChannelAttention(nn.Module):"""Channel Attention Module"""...
def update_history(self, indices, predictions):"""Update prediction history for each sample""" for i, idx in enumerate(indices): idx = idx.item()...</p>
<p>This method refines the integration of SMILES-based and graph-based reaction descriptors by introducing a Dual-Attention Fusion Mechanism (DAFM). DAFM systematically combines token and graph embeddings with reaction condition features, ensuring a multi-level attention strategy for improved information flow. It incorporates a dynamic freezing mechanism that develops a decaying threshold system for gradientbased selection of trainable layers, enhancing generalization in low-data regimes while addressing overfitting issues.
Code ### Full Algorithm Code ### Full Algorithm Algorithm: Adaptive Hierarchical Graph Transformer (AHGT) Input: Graph $G = (\mathcal{V}, \mathcal{E}, \mathbf{X}, \mathbf{E})$, node types, timesteps $T$.Output: Voltage magnitudes $\hat{V}_m$, angles $\hat{V}_a$.Code ### Full Algorithm Code ### Full AlgorithmAppendixA Contributions and AcknowledgmentsProject Management and ProductYilan Zhang, Meng Li, Shaowei Hou, Zhongying TuAdvisingBowen Zhou, Wanli Ouyang, Xiangyu YueProject Co-leadLei Bai, bailei@pjlab.org.cnBo Zhang, zhangbo@pjlab.org.cnB Evaluation Details B.1 Scoring Criteria for Idea ReviewIn Table10of the main text, we conducted a human evaluation to assess the novelty of ideas generated by AI-Scientist-V2(Yamada et al., 2025)and our NOVELSEEK.The evaluation was carried out across four dimensions: soundness, contribution, overall rating, and confidence.Specifically, considering the evaluation cost, we opted to evaluate four types of research tasks: reaction yield prediction, 2D semantic segmentation, 2D image classification, and point cloud autonomous driving.Each invited researcher was required to have peer-review qualifications for top-tier journals or conferences in the relevant field.For each research task, we generated 20 ideas using both AI-Scientist-V2 and NOVELSEEK, and five experienced researchers were invited to score each idea.In this part, we provide a detailed description of the scoring criteria for each reviewer, as outlined below: Soundness:• 4 excellentMethodThe GEARS_LocalRegularization framework improves upon the existing GEARS methodology by introducing biologically-grounded, local graph regularization that explicitly connects spectral graph penalties to biological domain knowledge, such as chromatin interactions and ... steps.This allows for task decomposition during the experimental validation phase, thereby facilitating more significant benchmark results.Initial PlanningFurthermore, in the AutoPower task illustrated in Fig.9, the "Adaptive Hierarchical Graph Transformer" (AHGT) introduces significant advancements for power flow estimation in energy systems.This approach features two key innovations: the Enhanced Edge-Node Hierarchical Pooling (EENHPool) mechanism, which integrates global and local features to retain crucial graph structures while reducing ambiguities, and the Stability-Regularized Temporal Graph Transformer (SRT-GT), designed to capture temporal dynamics while maintaining training stability.These components together enhance the model's robustness and accuracy, validated on IEEE benchmarks under scenarios involving renewable energy and grid perturbations.The AHGT method outputs precise voltage magnitude and angle predictions, assessed using metrics such as MAE, RMSE, and the Graph Perturbation Robustness Index (GPRI).This approach showcases the potential for improved power system modeling through advanced graph transformer techniques.
Genome-wide quantitative enhancer activity maps identified by starr-seq. Daniel Cosmas D Arnold, Christoph Gerlach, Stelzer, Martina Łukasz M Bory Ń, Alexander Rath, Stark, Science. 33961232013</p>
<p>Encoder-decoder with atrous separable convolution for semantic image segmentation. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Zhe Chen, Weiyun Wang, Shenglong Hao Tian, Zhangwei Ye, Erfei Gao, Wenwen Cui, Kongzhi Tong, Jiapeng Hu, Zheng Luo, Ma, Science China Information Sciences. 67122201012024</p>
<p>Machine learning of accurate energy-conserving molecular force fields. Stefan Chmiela, Alexandre Tkatchenko, E Huziel, Igor Sauceda, Kristof T Poltavsky, Klaus-Robert Schütt, Müller, Science advances. 35e16030152017</p>
<p>Deepstarr predicts enhancer activity from dna sequence and enables the de novo design of synthetic enhancers. Franziska Bernardo P De Almeida, Michaela Reiter, Alexander Pagani, Stark, Nature genetics. 5452022</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American Chapterthe Association for Computational Linguistics2019</p>
<p>The PAS-CAL Visual Object Classes Challenge 2012 (VOC2012) Results. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, 2012</p>
<p>Aider: Ai pair programming in your terminal. Paul Gauthier, Aider-Ai Contributors, 2023</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Learning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 2009</p>
<p>Ai-researcher: Fully-automated scientific discovery with llm agents. 2025HKU Data Intelligence Lab</p>
<p>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, arXiv:2408.03326Llava-onevision: Easy visual task transfer. 2024aarXiv preprint</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xinxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research in novel idea development with llm agents. 2024barXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao ; Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang, arXiv:2310.02255arXiv:2501.04686Understanding and verifying chain-of-thought reasoning in multimodal mathematics. Ursa2023. 2025arXiv preprint</p>
<p>One million scenes for autonomous driving. Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang, Jingheng Chen, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, arXiv:2106.110372021Once dataset. arXiv preprint</p>
<p>Exploring genetic interaction manifolds constructed from rich single-cell phenotypes. Max A Thomas M Norman, Horlbeck, Alex Y Joseph M Replogle, Albert Ge, Marco Xu, Luke A Jost, Jonathan S Gilbert, Weissman, Science. 36564552019</p>
<p>A platform for automated nanomolescale reaction screening and micromole-scale synthesis in flow. Damith Perera, Joseph W Tucker, Shalini Brahmbhatt, Christopher J Helal, Ashley Chong, William Farrell, Paul Richardson, Neal W Sach, arXiv:2311.05965Science. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou35963742018. 2023arXiv preprintLarge language models are zero shot hypothesis proposers</p>
<p>Large language models as biomedical hypothesis generators: a comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, arXiv:2407.089402024arXiv preprint</p>
<p>Pointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Yansheng Qiu, Haoquan Zhang, Zhaopan Xu, Ming Li, Diping Song, Zheng Wang, Kaipeng Zhang, arXiv:2504.14191Ai idea bench 2025: Ai research idea generation benchmark. 2025arXiv preprint</p>
<p>Predicting transcriptional outcomes of novel multigene perturbations with gears. Yusuf Roohani, Kexin Huang, Jure Leskovec, Nature Biotechnology. 4262024</p>
<p>Samuel Schmidgall, Michael Moor, arXiv:2503.18102Agentrxiv: Towards collaborative autonomous research. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>Openpcdet: An open-source toolbox for 3d object detection from point clouds. 2020OpenPCDet Development Team</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, arXiv:2409.121912024aarXiv preprint</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.14259Scimon: Scientific inspiration machines optimized for novelty. 2023arXiv preprint</p>
<p>Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, H Hoang, Fuqiang Tran, Ren Li, Mingzhang Ma, Bill Zheng, Yanjun Qian, Niklas Shao, Yizhe Muennighoff, Binyuan Zhang, Junyang Hui, Robert Lin, Hao Brennan, Heng Peng, Graham Ji, Neubig, OpenHands: An Open Platform for AI Software Developers as Generalist Agents. 2024b</p>
<p>Openhands: An open platform for ai software developers as generalist agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, The Thirteenth International Conference on Learning Representations. 2024c</p>
<p>Enhancing geometric representations for molecules with equivariant vector-scalar interactive message passing. Yusong Wang, Tong Wang, Shaoning Li, Xinheng He, Mingyu Li, Zun Wang, Nanning Zheng, Bin Shao, Tie-Yan Liu, Nature Communications. 1513132024d</p>
<p>3d shapenets: A deep representation for volumetric shapes. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Surveyforge: On the outline heuristics, memory-driven generation, and multidimensional evaluation for automated survey writing. Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, Lei Bai, arXiv:2503.046292025arXiv preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. 2024b</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Center-based 3d object detection and tracking. Xingyi Tianwei Yin, Philipp Zhou, Krahenbuhl, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou, arXiv:2501.039162025arXiv preprint</p>
<p>Sergey Zagoruyko, arXiv:1605.07146Wide residual networks. 2016arXiv preprint</p>
<p>Are transformers effective for time series forecasting. Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202337</p>
<p>Sigmoid loss for language image pre-training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Senseflow: A physicsinformed and self-ensembling iterative framework for power flow estimation. Zhen Zhao, Zhen Huang, Zicheng Wang, Wenqi Huang, Lei Bai, 2024</p>
<p>Informer: Beyond efficient transformer for long sequence time-series forecasting. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang, The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference. AAAI Press202135</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>
<p>Matpower: Steady-state operations, planning, and analysis tools for power systems research and education. Ray Daniel Zimmerman, Carlos Edmundo Murillo-Sánchez, Robert John, Thomas , IEEE Transactions on power systems. 2612010</p>
<p>Initialize $\mathbf{X}^{(0)} \leftarrow \text{NodeEncoder}(\mathbf{X}, \text{node types})$. </p>
<p>For each timestep $t = 1, \ldots, T$: a. Apply EENHPool: Determine node importance and lift local features. b. Form coarsened graph $G^{(t)}$ using lifted features. c. Compute edge-node attention for temporal graph. Update node embeddings using SRT-GT with stability constraints. </p>
<p>Decode final node embeddings $\mathbf{X}^{(T)}$ to predict $\hat{V}_m. </p>
<p>Return predictions $\hat{V}_m, \hat{V}_a$. </p>
<p>Sensitive Local Neighborhood Graph Construction**: For each input point cloud $X = {x_1, x_2, ..., x_n}$, define $k$-nearest neighbors (kNN) graph $G = (V, E)$, where $V$ represents points, and edges $E$ are weighted based on a distance metric $d(x_i, x_j)$. To incorporate density variation, adaptively determine $k$ for each point under a threshold $\epsilon$ based on local density, $\rho(x_i) = \frac{1}{|N(x_i)|}. ** Density, \sum_{j \in N(x_i)} d(x_i, x_j)$</p>
<p>L_h$) aggregate global features through graph coarsening using voxelization or farthestpoint-sampling mechanisms. ** Hierarchical, L_h}$, where $L_1$ retains finer details and higher levels. Graph Partitions**: The point set is partitioned into multiple levels, ${L_1</p>
<p>** Graph, Convolution and Pooling**: At each level $L_l$, apply spectral graph convolution on features $H^{(l)}$: $$ H'^{(l)} = \sigma(D^{-\frac{1}{2}} A D^{-\frac{1}{2}} H^{(l)} W^{(l)}), $$. </p>
<p>Feature Alignment<strong>: For feature matrix $H^{(l)}$, compute selfattention scores $\alpha$: $$ \alpha_{ij}=\text{softmax}\left(\frac{(H^{(l)}W_Q)(H^{(l)}W_K)^T}{\sqrt{d_k}}\right. </strong>attentive, </p>
<p>** Multi, Head Aggregation**: Multi-head attention layers produce features $Z$: $$ Z = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_m) W_O, $$ Figure 11: Visual Examples of Auto3DCls Task. </p>
<p>Augmentation Phase<strong>: Input images are preprocessed using dynamically tuned MixUp and geometric transformations based on their variance and entropy. Let $S_{g}$ represent the geometric augmentation strength, which is updated as follows: $$ S_{g}(x_i) = \alpha \cdot \text{Var}(x_i) + \beta \cdot \text{Entropy}(x_i). </strong>adaptive, </p>
<p>. $$ , </p>
<p>Prediction and Temporal Aggregation**: For each batch, the network evaluates predictions and refines pseudo-labels by aggregating past outputs weighted with the exponential decay mechanism. ** , </p>
<p>Loss Optimization**: The total training loss integrates primary classification loss $\mathcal{L}<em consistency="consistency">{cls}$, consistency regularization $\mathcal{L}</em>}$, and regularized auxiliary losses: $$ \mathcal{L} = \mathcal{L<em consistency="consistency">{cls} + \lambda</em>} \mathcal{L<em auxiliary="auxiliary">{consistency} + \lambda</em> $$. ** Total, } \mathcal{L}_{auxiliary</p>
<p>The step sizes for $\lambda_{consistency}$ and $\lambda_{auxiliary}$ are determined via grid search over the validation set. ** Optimizer, Parameters**: We employ SGD with momentum (0.9) and weight decay ($5 \times 10^{-4}$). </p>            </div>
        </div>

    </div>
</body>
</html>
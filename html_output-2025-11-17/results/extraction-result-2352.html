<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2352 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2352</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2352</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-12541cb75c5a8d0d9a859ff0e5f947568bb8bfed</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/12541cb75c5a8d0d9a859ff0e5f947568bb8bfed" target="_blank">Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs</a></p>
                <p><strong>Paper Venue:</strong> Nature Methods</p>
                <p><strong>Paper TL;DR:</strong> The challenge of accurate particle picking in cryo-EM analysis is addressed with Topaz, a neural-network-based algorithm that shows advantages over other tools, especially in picking unusually shaped particles.</p>
                <p><strong>Paper Abstract:</strong> Cryo-electron microscopy is a popular method for the determination of protein structures; however, identifying a sufficient number of particles for analysis can take months of manual effort. Current computational approaches find many false positives and require ad hoc postprocessing, especially for unusually shaped particles. To address these shortcomings, we develop Topaz, an efficient and accurate particle-picking pipeline using neural networks trained with a general-purpose positive-unlabeled learning method. This framework enables particle detection models to be trained with few sparsely labeled particles and no labeled negatives. Topaz retrieves many more real particles than conventional picking methods while maintaining low false-positive rates, is capable of picking challenging unusually shaped proteins (for example, small, non-globular and asymmetric particles), produces more representative particle sets and does not require post hoc curation. We demonstrate the performance of Topaz on two difficult datasets and three conventional datasets. Topaz is modular, standalone, free and open source (http://topaz.csail.mit.edu). The challenge of accurate particle picking in cryo-EM analysis is addressed with Topaz, a neural-network-based algorithm that shows advantages over other tools, especially in picking unusually shaped particles.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2352.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2352.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topaz</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topaz: Positive-unlabeled convolutional neural networks for particle picking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A particle-picking pipeline for cryo-electron microscopy that uses convolutional neural networks trained with positive-unlabeled (PU) learning (GE-criteria) and optional autoencoder regularization to identify particles from micrographs with very few labeled positives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>cryo-electron microscopy (single-particle cryoEM) particle picking / structural biology</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automatically identify particle coordinates in noisy cryoEM micrographs to produce sufficiently large, accurate particle sets for high-resolution 3D reconstruction; manual particle picking is slow and previous automated methods have high false positive rates or require large labeled negative sets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant unlabeled data (whole micrographs with many candidate regions) and limited labeled positives; experiments used scenarios with as few as 10 labeled positives up to 1,000+, with published particle sets ranging up to ~200k particles for some datasets; micrographs publicly accessible for many datasets (EMPIAR etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>2D image data (electron micrographs), high-dimensional pixel arrays, low signal-to-noise ratio, structured background features (grid bars, gold contaminants), class imbalance (positives rare vs. background), spatial locality important.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: low SNR and structured noise make detection difficult; diverse and irregular particle morphologies (including non-globular or stick-like particles); requires ranking many candidate regions and extracting coordinates accurately; large search space over micrograph pixels; need for scale and orientation invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature experimental domain (cryoEM) with established downstream reconstruction methods, but automated particle-picking is an active area with limited adoption of ML due to labeling burdens and dataset heterogeneity; manual curation remains a common standard.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: primary requirement is reliable identification and ranking of particle coordinates rather than mechanistic interpretability of the classifier; downstream scientific validity depends on particle quality rather than model interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional neural networks with positive-unlabeled (PU) learning (Topaz pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A three-layer CNN (conv7x7 32 -> conv5x5 64 -> conv5x5 128 with strides, batch normalization, PReLU, final linear output) is trained using a PU learning objective (GE-binomial or GE-KL) that constrains the expected fraction of positives on unlabeled data; optional deconvolutional decoder (autoencoder) reconstructs inputs from encoder features as an auxiliary regularizer. Training uses minibatched stochastic gradient descent and minibatch-aware GE-binomial regularization; prediction is done by convolving the classifier over micrographs to produce dense per-pixel probabilities, followed by non-maximum suppression to extract coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised / semi-supervised hybrid (positive-unlabeled learning + deep convolutional networks; hybrid classifier+autoencoder regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable and appropriate: methodology leverages the abundant unlabeled data in particle-picking, mitigates need for large negative labels, and adapts to varied particle shapes; modifications (GE-binomial minibatch-aware constraint, per-micrograph normalization, downsampling) tailored to cryoEM specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Topaz trained with 1,000 positives: retrieved 3.22x, 1.72x, and 3.68x more particles than curated sets for EMPIAR-10025, EMPIAR-10028, and NYSBC-aldo respectively; improved reconstruction resolution by ~0.15 Å (EMPIAR-10025) and ~0.05 Å (EMPIAR-10028); achieved a 3.0 Å reconstruction on EMPIAR-10028 (reported as best for that dataset), improved 0.1 Å over prior best in EM map challenge; training runs in a few hours on a single GPU; inference for hundreds of micrographs takes minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Worked well across diverse particle shapes including non-globular and stick-like particles; produced well-ranked candidate lists with low false-positive rates even at relaxed probability thresholds; improvements obtained without any ad-hoc postprocessing. Limitations: autoencoder regularization helps only in very-low-label regimes and can over-regularize with larger labeled sets; dependence on an estimate of positive class prior π (though reported as insensitive in practice).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High practical impact for cryoEM workflows: reduces manual labeling and curation time, enables picking for previously difficult datasets, increases particle yields and reconstruction resolution, and is computationally efficient and reusable across imaging runs for the same particle; potential to be integrated into cryoEM suites (Appion, cryoSPARC workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly against traditional DoG and template matching (limitations described) and against other PU learning approaches (NNPU and PN) and prior CNN approaches; Topaz's GE-criteria PU training outperformed NNPU and PN baselines on benchmark datasets, and exceeded curated published particle sets in reconstruction results without postprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key factors: framing particle-picking as PU learning (leveraging abundant unlabeled data), minibatch-aware GE-binomial regularization to prevent overfitting, per-micrograph normalization, efficient CNN architecture, optional autoencoder regularization for very small labeled sets, and efficient inference+non-maximum suppression to extract coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Framing cryoEM particle picking as positive-unlabeled learning and using a minibatch-aware GE-binomial constraint plus optional autoencoder regularization enables accurate CNN classifiers from very few labeled positives, increasing particle yield and improving 3D reconstruction quality without extensive manual curation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2352.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2352.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GE-binomial</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalized Expectation (GE) criteria — binomial minibatch-aware objective (GE-binomial)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel GE-criteria PU learning objective that regularizes a classifier by matching the classifier's distribution over the number of positives in each minibatch to the binomial distribution parameterized by the prior π, implemented with a Gaussian approximation for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>machine learning for object detection in cryo-electron microscopy (generalizable to other imaging domains)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Train classifiers from positive and unlabeled data without large labeled negative sets, avoiding overfitting in neural networks trained with SGD on minibatches.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Scenario: many unlabeled examples and few labeled positives (experiments down to 10 positives, up to 1,000+); unlabeled minibatches drawn during SGD.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Image patches (micrograph regions) presented in minibatches; stochastic sampling across unlabeled pool.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate ML complexity: requires estimating minibatch-level distributions induced by model posteriors and efficiently computing gradients compatible with SGD; must avoid biased gradient estimates from naive expectation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>GE-criteria is a known posterior regularization technique (Mann & McCallum); GE-binomial is a novel adaptation for minibatch-aware PU learning in neural networks introduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: method is a regularization objective; interpretability of the classifier remains secondary to performance, but the prior π must be specified or cross-validated.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>GE-binomial PU learning objective</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Adds a regularization term to the classifier loss equal to the cross-entropy between the classifier's induced distribution q(k) over number of positives in a minibatch and the binomial prior p(k) with parameter π; approximates q(k) with a Gaussian using mean sum g(x_i) and variance sum g(x_i)(1-g(x_i)) for efficiency during minibatched SGD.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Semi-supervised / posterior-regularization (positive-unlabeled learning)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for minibatch SGD training of neural networks from positive and unlabeled data; tailored to domains with abundant unlabeled data and sparse positives such as cryoEM.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Outperformed NNPU and PN baselines on cryoEM benchmarks; specifically gave significantly better average-precision on some low-label regimes versus GE-KL (p<0.05) and enabled picking on challenging stick-like particle dataset at 1,000 labeled examples (p<0.05); exact AP numbers are reported in figures (not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Mitigates overfitting experienced by other PU approaches in neural network settings by modeling minibatch sampling statistics; showed robust performance across both easy and challenging cryoEM datasets and performed especially well in very-low-label or difficult-particle regimes compared to GE-KL and NNPU in select cases.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to generalize to other object-detection problems in imaging domains where positives are rare and labels incomplete (e.g., light microscopy, medical imaging), by providing a practical minibatch-aware PU objective for training deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to GE-KL (KL between expected classifier output and π), GE-binomial often performed similarly but outperformed GE-KL in two notable cases: extremely small labeled sets (10 positives on an easy dataset) and large labeled sets on a difficult dataset (1,000 positives on Shapiro-lab); also outperformed NNPU and PN baselines overall.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Modeling minibatch-level distributional properties rather than only global expectation reduces biased gradient estimation and overfitting under minibatch SGD; Gaussian approximation makes computation tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Enforcing a minibatch-aware constraint (matching predicted minibatch positive counts to a binomial prior) regularizes neural PU learning better than naive expectation constraints, enabling robust training from few positives in noisy imaging tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2352.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2352.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GE-KL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GE-KL: Generalized Expectation with KL-divergence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GE-criteria PU learning objective that regularizes the classifier by penalizing the KL-divergence between the expectation of classifier outputs on unlabeled data and the known positive prior π.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>machine learning for cryoEM particle detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Regularize classifiers trained on P and U by matching expected classifier positive rate on unlabeled data to known π, implemented as a soft constraint in the loss.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Many unlabeled examples with a known or estimated positive prior π; few labeled positives in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Image patches (micrograph regions) aggregated to compute expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: requires estimating global expectation during minibatch optimization; biased gradient estimates from samples can hurt convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>GE-KL leverages established GE posterior-regularization ideas (Mann & McCallum) but its minibatch gradient bias is a challenge for neural networks, motivating GE-binomial.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: used as an implicit regularizer; does not provide interpretability beyond constraining expected positive rate.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>GE-KL PU learning objective</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Loss = classification loss on positives + λ * KL( E_{x∼U}[g(x)] || π ). Implemented as a soft constraint but can produce biased minibatch gradients when optimized with SGD.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Semi-supervised / posterior-regularization (positive-unlabeled learning)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable but less robust under minibatch SGD compared to GE-binomial; works well when minibatch gradient bias is not critical or when many labeled points exist.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Performed similarly to GE-binomial in many settings; GE-binomial outperformed GE-KL significantly in very-low-label (10 positives on EMPIAR-10096) and in some high-label difficult cases GE-KL occasionally outperformed GE-binomial in the 50–250 positive range on the Shapiro-lab dataset (statistical significance reported p<0.05 for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective as a constraint but sensitive to biased gradient estimates under minibatch SGD; GE-binomial was introduced to address this practical issue.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful baseline GE-based PU objective; highlights the importance of minibatch-aware regularization for deep PU learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared with GE-binomial (minibatch-aware) and NNPU/PN baselines; GE-binomial usually matches or exceeds GE-KL performance in key regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simplicity of expectation matching and connection to GE posterior regularization; however, practical SGD optimization issues limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Expectation-matching via KL provides a principled PU constraint, but minibatch gradient bias in neural network training can limit effectiveness compared to minibatch-aware GE-binomial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2352.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2352.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoencoder regularization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoencoder-augmented classifier (encoder + decoder reconstruction loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid classifier+autoencoder architecture that adds a reconstruction error term on encoder features to regularize the classifier and improve generalization when only few labeled positives are available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>machine learning for image-based particle detection in cryoEM</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve classifier generalization in very-low-label regimes by enforcing that learned feature representations reconstruct input patches, acting as regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed for regimes with very few labeled positives (N <= 250) and abundant unlabeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Image patches; encoder outputs low-dimensional feature maps used by decoder to reconstruct inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Adds optimization complexity (additional decoder network and reconstruction loss) but can reduce overfitting by constraining feature representations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Autoencoders are a mature representation-learning tool; here applied as a regularizer specific to the particle-picking pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: reconstruction objective encourages descriptive features but does not provide causal mechanistic insight.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Hybrid classifier + autoencoder (reconstruction regularizer)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Split classifier into encoder f and linear classifier c such that g(x)=c(f(x)); add decoder d and a reconstruction term γ E[||x - d(f(x))||_2^2] to the GE-binomial objective. γ hyperparameter tuned (γ=10/N recommended for N≤250).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Hybrid semi-supervised / representation-regularized learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and beneficial in very-low-label regimes; recommended when labeled positives are scarce (≤250); can harm performance when too many labeled examples (over-regularization).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Improved average-precision in few-labeled regimes (experiments across EMPIAR-10096 and Shapiro-lab show benefit for γ=1 and γ=10/N when N small). No single AP number tabulated in text, but trends reported and shown in Figure 5b.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides useful regularization that improves classifier generalization with few positives; as labeled data increases benefit decreases and can become detrimental due to over-regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Practically valuable for settings where labeling is expensive and only small positive sets can be obtained; generalizable to other imaging tasks with scarce labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared models with and without autoencoder across label sizes; autoencoder helped for N ≤ 250 (γ=10/N) and hurt for N > 250 compared to no autoencoder.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Constraining encoder features to be descriptive of input reduces overfitting and encourages generalizable features when supervision is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Autoencoder reconstruction as a regularizer improves PU-trained CNN generalization in very-low-label regimes but must be down-weighted as labeled data increases to avoid over-regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2352.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2352.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NNPU (Kiryo et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-negative PU risk estimator (NNPU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent PU learning approach using a non-negative risk estimator designed to reduce overfitting in positive-unlabeled learning for neural networks, used here as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>machine learning methods for learning from positive and unlabeled data</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Estimate PN classification risk from P and U data in an unbiased way and prevent the negative risk estimator from becoming negative (non-negative correction) to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed for settings with many unlabeled examples and some positives; used in benchmarking on cryoEM datasets with simulated labeled subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>General classifier inputs (here image patches).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Method addresses bias/variance trade-offs in risk estimation for neural PU learning; still susceptible to overfitting in some neural training contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established recent approach in PU learning literature (external to this paper); used here as a benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low: method focuses on risk estimation, not interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Non-negative PU risk estimator (NNPU)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Constructs an unbiased estimator of PN risk from P and U, with a non-negative correction to the estimated negative risk to prevent overfitting; used to train neural network classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Semi-supervised / PU learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable as a baseline for PU tasks; in this paper it was outperformed by GE-based methods on cryoEM particle-picking benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Performed worse than GE-binomial and GE-KL across cryoEM experiments in this paper (specific average-precision numbers available in figures, not tabulated in-text); suffered overfitting in neural network settings relative to GE criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>While theoretically principled, NNPU was less robust in these cryoEM image experiments than the GE-criteria approaches when labeled positives were very few.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Serves as an important baseline and demonstrates limitations of unbiased risk estimators in minibatch neural training for noisy imaging tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared to GE-binomial and GE-KL in benchmarking; GE approaches dramatically outperformed NNPU on these cryoEM datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Principled risk estimation reduces some overfitting but may not address minibatch-gradient and domain-specific challenges encountered in cryoEM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Unbiased/non-negative risk estimation for PU learning (NNPU) may still overfit in practical neural network training on noisy imaging domains compared to GE-criteria approaches that explicitly regularize minibatch expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2352.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2352.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PN (naive negative labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PN: Treating unlabeled examples as negative (naive approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple baseline that treats all unlabeled micrograph regions as negatives and trains a standard supervised classifier on the labeled positives and these pseudo-negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>cryoEM particle detection / PU learning baselines</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>When π is small, treating unlabeled as negatives can be used to train classifiers but risks severe bias and overfitting since many unlabeled regions are actually positive.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Many unlabeled regions; works only when positives are rare enough that unlabeled-as-negative assumption approximates truth.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Image patches.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Low conceptual complexity but often fails due to mislabeled positives in the negative pool and class imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Common naive approach used as baseline; known limitations especially in domains where unlabeled positives are non-negligible.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low; straightforward supervised learning with noisy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>PN (treat U as negative) naive supervised training</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Loss: π E_{x∼P}[L(g(x),1)] + (1-π) E_{x∼U}[L(g(x),0)] (implemented directly); used as a baseline and compared in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning with noisy/unreliable labels (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Sometimes effective when π is very small; otherwise prone to overfitting and biased decision boundaries; outperformed by GE-based PU methods in this paper's benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Underperformed relative to GE-binomial and GE-KL on the cryoEM benchmarks; specific AP values shown in figures but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Suffers from overfitting and requires many labeled negatives in practice; not recommended for particle-picking where negative space is diverse.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Limited as a primary method but useful as a simple baseline to evaluate more sophisticated PU methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared and shown to be inferior to GE-based approaches and often inferior to NNPU in certain regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simplicity; may work when positive prevalence in U is extremely low.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Treating unlabeled regions as negatives is a brittle baseline for particle-picking because unlabeled data contains non-negligible positive examples and highly varied negative types, requiring more principled PU methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2352.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2352.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional detection (DoG / template)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difference-of-Gaussians (DoG) and template-based particle picking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical non-ML particle detection methods: DoG detects compact spherical objects of known scale; template matching uses cross-correlation against reference templates to find similar regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>cryoEM particle detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automatic detection of particles in micrographs using matched filters (DoG) or cross-correlation with templates; limited for irregular shapes and can produce many false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>DoG/template methods operate on full micrographs (unlabeled) and may require templates derived from known structures or hand-labeled particles.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>2D image micrographs; templates are image patches.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Lower algorithmic complexity but high failure rate for non-spherical particles and structured background; sensitive to template quality and size assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Long-established baseline methods widely used historically in cryoEM particle picking.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low; transparent matched-filter behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Classical image processing detection (DoG, template matching)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>DoG: bandpass matched filter to detect blob-like particles of known size; template matching: cross-correlate micrographs with templates (from known structures or hand-labeled particles) to find matches.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Classical computer vision / signal-processing</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to roughly spherical, compact particles or when good templates exist; fails for atypical/irregularly shaped particles and yields high false positives without strong post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Not quantified here, but described as limited with high false positives and often requiring substantial manual post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Fast and simple for suitable particle shapes, but inadequate for many modern cryoEM challenges; motivates adoption of learned methods.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Serves as practical baseline for easy datasets but limited generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with ML approaches (CNNs, PU learning) which address shape variability and structured noise better.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simplicity and low computational cost when assumptions hold (spherical/compact particles).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Classical detection methods are insufficient for irregular shapes and low-SNR cryoEM micrographs, motivating learned PU-based CNN approaches like Topaz.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Positive-Unlabeled Learning with Non-Negative Risk Estimator <em>(Rating: 2)</em></li>
                <li>Generalized Expectation Criteria for Semi-Supervised Learning <em>(Rating: 2)</em></li>
                <li>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2352",
    "paper_id": "paper-12541cb75c5a8d0d9a859ff0e5f947568bb8bfed",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Topaz",
            "name_full": "Topaz: Positive-unlabeled convolutional neural networks for particle picking",
            "brief_description": "A particle-picking pipeline for cryo-electron microscopy that uses convolutional neural networks trained with positive-unlabeled (PU) learning (GE-criteria) and optional autoencoder regularization to identify particles from micrographs with very few labeled positives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "cryo-electron microscopy (single-particle cryoEM) particle picking / structural biology",
            "problem_description": "Automatically identify particle coordinates in noisy cryoEM micrographs to produce sufficiently large, accurate particle sets for high-resolution 3D reconstruction; manual particle picking is slow and previous automated methods have high false positive rates or require large labeled negative sets.",
            "data_availability": "Abundant unlabeled data (whole micrographs with many candidate regions) and limited labeled positives; experiments used scenarios with as few as 10 labeled positives up to 1,000+, with published particle sets ranging up to ~200k particles for some datasets; micrographs publicly accessible for many datasets (EMPIAR etc.).",
            "data_structure": "2D image data (electron micrographs), high-dimensional pixel arrays, low signal-to-noise ratio, structured background features (grid bars, gold contaminants), class imbalance (positives rare vs. background), spatial locality important.",
            "problem_complexity": "High: low SNR and structured noise make detection difficult; diverse and irregular particle morphologies (including non-globular or stick-like particles); requires ranking many candidate regions and extracting coordinates accurately; large search space over micrograph pixels; need for scale and orientation invariance.",
            "domain_maturity": "Mature experimental domain (cryoEM) with established downstream reconstruction methods, but automated particle-picking is an active area with limited adoption of ML due to labeling burdens and dataset heterogeneity; manual curation remains a common standard.",
            "mechanistic_understanding_requirements": "Low-to-medium: primary requirement is reliable identification and ranking of particle coordinates rather than mechanistic interpretability of the classifier; downstream scientific validity depends on particle quality rather than model interpretability.",
            "ai_methodology_name": "Convolutional neural networks with positive-unlabeled (PU) learning (Topaz pipeline)",
            "ai_methodology_description": "A three-layer CNN (conv7x7 32 -&gt; conv5x5 64 -&gt; conv5x5 128 with strides, batch normalization, PReLU, final linear output) is trained using a PU learning objective (GE-binomial or GE-KL) that constrains the expected fraction of positives on unlabeled data; optional deconvolutional decoder (autoencoder) reconstructs inputs from encoder features as an auxiliary regularizer. Training uses minibatched stochastic gradient descent and minibatch-aware GE-binomial regularization; prediction is done by convolving the classifier over micrographs to produce dense per-pixel probabilities, followed by non-maximum suppression to extract coordinates.",
            "ai_methodology_category": "Supervised / semi-supervised hybrid (positive-unlabeled learning + deep convolutional networks; hybrid classifier+autoencoder regularization)",
            "applicability": "Highly applicable and appropriate: methodology leverages the abundant unlabeled data in particle-picking, mitigates need for large negative labels, and adapts to varied particle shapes; modifications (GE-binomial minibatch-aware constraint, per-micrograph normalization, downsampling) tailored to cryoEM specifics.",
            "effectiveness_quantitative": "Topaz trained with 1,000 positives: retrieved 3.22x, 1.72x, and 3.68x more particles than curated sets for EMPIAR-10025, EMPIAR-10028, and NYSBC-aldo respectively; improved reconstruction resolution by ~0.15 Å (EMPIAR-10025) and ~0.05 Å (EMPIAR-10028); achieved a 3.0 Å reconstruction on EMPIAR-10028 (reported as best for that dataset), improved 0.1 Å over prior best in EM map challenge; training runs in a few hours on a single GPU; inference for hundreds of micrographs takes minutes.",
            "effectiveness_qualitative": "Worked well across diverse particle shapes including non-globular and stick-like particles; produced well-ranked candidate lists with low false-positive rates even at relaxed probability thresholds; improvements obtained without any ad-hoc postprocessing. Limitations: autoencoder regularization helps only in very-low-label regimes and can over-regularize with larger labeled sets; dependence on an estimate of positive class prior π (though reported as insensitive in practice).",
            "impact_potential": "High practical impact for cryoEM workflows: reduces manual labeling and curation time, enables picking for previously difficult datasets, increases particle yields and reconstruction resolution, and is computationally efficient and reusable across imaging runs for the same particle; potential to be integrated into cryoEM suites (Appion, cryoSPARC workflows).",
            "comparison_to_alternatives": "Compared directly against traditional DoG and template matching (limitations described) and against other PU learning approaches (NNPU and PN) and prior CNN approaches; Topaz's GE-criteria PU training outperformed NNPU and PN baselines on benchmark datasets, and exceeded curated published particle sets in reconstruction results without postprocessing.",
            "success_factors": "Key factors: framing particle-picking as PU learning (leveraging abundant unlabeled data), minibatch-aware GE-binomial regularization to prevent overfitting, per-micrograph normalization, efficient CNN architecture, optional autoencoder regularization for very small labeled sets, and efficient inference+non-maximum suppression to extract coordinates.",
            "key_insight": "Framing cryoEM particle picking as positive-unlabeled learning and using a minibatch-aware GE-binomial constraint plus optional autoencoder regularization enables accurate CNN classifiers from very few labeled positives, increasing particle yield and improving 3D reconstruction quality without extensive manual curation.",
            "uuid": "e2352.0",
            "source_info": {
                "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "GE-binomial",
            "name_full": "Generalized Expectation (GE) criteria — binomial minibatch-aware objective (GE-binomial)",
            "brief_description": "A novel GE-criteria PU learning objective that regularizes a classifier by matching the classifier's distribution over the number of positives in each minibatch to the binomial distribution parameterized by the prior π, implemented with a Gaussian approximation for efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "machine learning for object detection in cryo-electron microscopy (generalizable to other imaging domains)",
            "problem_description": "Train classifiers from positive and unlabeled data without large labeled negative sets, avoiding overfitting in neural networks trained with SGD on minibatches.",
            "data_availability": "Scenario: many unlabeled examples and few labeled positives (experiments down to 10 positives, up to 1,000+); unlabeled minibatches drawn during SGD.",
            "data_structure": "Image patches (micrograph regions) presented in minibatches; stochastic sampling across unlabeled pool.",
            "problem_complexity": "Moderate ML complexity: requires estimating minibatch-level distributions induced by model posteriors and efficiently computing gradients compatible with SGD; must avoid biased gradient estimates from naive expectation constraints.",
            "domain_maturity": "GE-criteria is a known posterior regularization technique (Mann & McCallum); GE-binomial is a novel adaptation for minibatch-aware PU learning in neural networks introduced in this paper.",
            "mechanistic_understanding_requirements": "Low-to-medium: method is a regularization objective; interpretability of the classifier remains secondary to performance, but the prior π must be specified or cross-validated.",
            "ai_methodology_name": "GE-binomial PU learning objective",
            "ai_methodology_description": "Adds a regularization term to the classifier loss equal to the cross-entropy between the classifier's induced distribution q(k) over number of positives in a minibatch and the binomial prior p(k) with parameter π; approximates q(k) with a Gaussian using mean sum g(x_i) and variance sum g(x_i)(1-g(x_i)) for efficiency during minibatched SGD.",
            "ai_methodology_category": "Semi-supervised / posterior-regularization (positive-unlabeled learning)",
            "applicability": "Appropriate for minibatch SGD training of neural networks from positive and unlabeled data; tailored to domains with abundant unlabeled data and sparse positives such as cryoEM.",
            "effectiveness_quantitative": "Outperformed NNPU and PN baselines on cryoEM benchmarks; specifically gave significantly better average-precision on some low-label regimes versus GE-KL (p&lt;0.05) and enabled picking on challenging stick-like particle dataset at 1,000 labeled examples (p&lt;0.05); exact AP numbers are reported in figures (not tabulated in text).",
            "effectiveness_qualitative": "Mitigates overfitting experienced by other PU approaches in neural network settings by modeling minibatch sampling statistics; showed robust performance across both easy and challenging cryoEM datasets and performed especially well in very-low-label or difficult-particle regimes compared to GE-KL and NNPU in select cases.",
            "impact_potential": "High potential to generalize to other object-detection problems in imaging domains where positives are rare and labels incomplete (e.g., light microscopy, medical imaging), by providing a practical minibatch-aware PU objective for training deep models.",
            "comparison_to_alternatives": "Compared to GE-KL (KL between expected classifier output and π), GE-binomial often performed similarly but outperformed GE-KL in two notable cases: extremely small labeled sets (10 positives on an easy dataset) and large labeled sets on a difficult dataset (1,000 positives on Shapiro-lab); also outperformed NNPU and PN baselines overall.",
            "success_factors": "Modeling minibatch-level distributional properties rather than only global expectation reduces biased gradient estimation and overfitting under minibatch SGD; Gaussian approximation makes computation tractable.",
            "key_insight": "Enforcing a minibatch-aware constraint (matching predicted minibatch positive counts to a binomial prior) regularizes neural PU learning better than naive expectation constraints, enabling robust training from few positives in noisy imaging tasks.",
            "uuid": "e2352.1",
            "source_info": {
                "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "GE-KL",
            "name_full": "GE-KL: Generalized Expectation with KL-divergence",
            "brief_description": "A GE-criteria PU learning objective that regularizes the classifier by penalizing the KL-divergence between the expectation of classifier outputs on unlabeled data and the known positive prior π.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "machine learning for cryoEM particle detection",
            "problem_description": "Regularize classifiers trained on P and U by matching expected classifier positive rate on unlabeled data to known π, implemented as a soft constraint in the loss.",
            "data_availability": "Many unlabeled examples with a known or estimated positive prior π; few labeled positives in experiments.",
            "data_structure": "Image patches (micrograph regions) aggregated to compute expectations.",
            "problem_complexity": "Moderate: requires estimating global expectation during minibatch optimization; biased gradient estimates from samples can hurt convergence.",
            "domain_maturity": "GE-KL leverages established GE posterior-regularization ideas (Mann & McCallum) but its minibatch gradient bias is a challenge for neural networks, motivating GE-binomial.",
            "mechanistic_understanding_requirements": "Low-to-medium: used as an implicit regularizer; does not provide interpretability beyond constraining expected positive rate.",
            "ai_methodology_name": "GE-KL PU learning objective",
            "ai_methodology_description": "Loss = classification loss on positives + λ * KL( E_{x∼U}[g(x)] || π ). Implemented as a soft constraint but can produce biased minibatch gradients when optimized with SGD.",
            "ai_methodology_category": "Semi-supervised / posterior-regularization (positive-unlabeled learning)",
            "applicability": "Applicable but less robust under minibatch SGD compared to GE-binomial; works well when minibatch gradient bias is not critical or when many labeled points exist.",
            "effectiveness_quantitative": "Performed similarly to GE-binomial in many settings; GE-binomial outperformed GE-KL significantly in very-low-label (10 positives on EMPIAR-10096) and in some high-label difficult cases GE-KL occasionally outperformed GE-binomial in the 50–250 positive range on the Shapiro-lab dataset (statistical significance reported p&lt;0.05 for comparisons).",
            "effectiveness_qualitative": "Effective as a constraint but sensitive to biased gradient estimates under minibatch SGD; GE-binomial was introduced to address this practical issue.",
            "impact_potential": "Useful baseline GE-based PU objective; highlights the importance of minibatch-aware regularization for deep PU learning.",
            "comparison_to_alternatives": "Directly compared with GE-binomial (minibatch-aware) and NNPU/PN baselines; GE-binomial usually matches or exceeds GE-KL performance in key regimes.",
            "success_factors": "Simplicity of expectation matching and connection to GE posterior regularization; however, practical SGD optimization issues limit performance.",
            "key_insight": "Expectation-matching via KL provides a principled PU constraint, but minibatch gradient bias in neural network training can limit effectiveness compared to minibatch-aware GE-binomial.",
            "uuid": "e2352.2",
            "source_info": {
                "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Autoencoder regularization",
            "name_full": "Autoencoder-augmented classifier (encoder + decoder reconstruction loss)",
            "brief_description": "A hybrid classifier+autoencoder architecture that adds a reconstruction error term on encoder features to regularize the classifier and improve generalization when only few labeled positives are available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "machine learning for image-based particle detection in cryoEM",
            "problem_description": "Improve classifier generalization in very-low-label regimes by enforcing that learned feature representations reconstruct input patches, acting as regularization.",
            "data_availability": "Designed for regimes with very few labeled positives (N &lt;= 250) and abundant unlabeled data.",
            "data_structure": "Image patches; encoder outputs low-dimensional feature maps used by decoder to reconstruct inputs.",
            "problem_complexity": "Adds optimization complexity (additional decoder network and reconstruction loss) but can reduce overfitting by constraining feature representations.",
            "domain_maturity": "Autoencoders are a mature representation-learning tool; here applied as a regularizer specific to the particle-picking pipeline.",
            "mechanistic_understanding_requirements": "Low-to-medium: reconstruction objective encourages descriptive features but does not provide causal mechanistic insight.",
            "ai_methodology_name": "Hybrid classifier + autoencoder (reconstruction regularizer)",
            "ai_methodology_description": "Split classifier into encoder f and linear classifier c such that g(x)=c(f(x)); add decoder d and a reconstruction term γ E[||x - d(f(x))||_2^2] to the GE-binomial objective. γ hyperparameter tuned (γ=10/N recommended for N≤250).",
            "ai_methodology_category": "Hybrid semi-supervised / representation-regularized learning",
            "applicability": "Applicable and beneficial in very-low-label regimes; recommended when labeled positives are scarce (≤250); can harm performance when too many labeled examples (over-regularization).",
            "effectiveness_quantitative": "Improved average-precision in few-labeled regimes (experiments across EMPIAR-10096 and Shapiro-lab show benefit for γ=1 and γ=10/N when N small). No single AP number tabulated in text, but trends reported and shown in Figure 5b.",
            "effectiveness_qualitative": "Provides useful regularization that improves classifier generalization with few positives; as labeled data increases benefit decreases and can become detrimental due to over-regularization.",
            "impact_potential": "Practically valuable for settings where labeling is expensive and only small positive sets can be obtained; generalizable to other imaging tasks with scarce labels.",
            "comparison_to_alternatives": "Compared models with and without autoencoder across label sizes; autoencoder helped for N ≤ 250 (γ=10/N) and hurt for N &gt; 250 compared to no autoencoder.",
            "success_factors": "Constraining encoder features to be descriptive of input reduces overfitting and encourages generalizable features when supervision is weak.",
            "key_insight": "Autoencoder reconstruction as a regularizer improves PU-trained CNN generalization in very-low-label regimes but must be down-weighted as labeled data increases to avoid over-regularization.",
            "uuid": "e2352.3",
            "source_info": {
                "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "NNPU (Kiryo et al.)",
            "name_full": "Non-negative PU risk estimator (NNPU)",
            "brief_description": "A recent PU learning approach using a non-negative risk estimator designed to reduce overfitting in positive-unlabeled learning for neural networks, used here as a baseline for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "machine learning methods for learning from positive and unlabeled data",
            "problem_description": "Estimate PN classification risk from P and U data in an unbiased way and prevent the negative risk estimator from becoming negative (non-negative correction) to avoid overfitting.",
            "data_availability": "Designed for settings with many unlabeled examples and some positives; used in benchmarking on cryoEM datasets with simulated labeled subsets.",
            "data_structure": "General classifier inputs (here image patches).",
            "problem_complexity": "Method addresses bias/variance trade-offs in risk estimation for neural PU learning; still susceptible to overfitting in some neural training contexts.",
            "domain_maturity": "Established recent approach in PU learning literature (external to this paper); used here as a benchmark.",
            "mechanistic_understanding_requirements": "Low: method focuses on risk estimation, not interpretability.",
            "ai_methodology_name": "Non-negative PU risk estimator (NNPU)",
            "ai_methodology_description": "Constructs an unbiased estimator of PN risk from P and U, with a non-negative correction to the estimated negative risk to prevent overfitting; used to train neural network classifiers.",
            "ai_methodology_category": "Semi-supervised / PU learning",
            "applicability": "Applicable as a baseline for PU tasks; in this paper it was outperformed by GE-based methods on cryoEM particle-picking benchmarks.",
            "effectiveness_quantitative": "Performed worse than GE-binomial and GE-KL across cryoEM experiments in this paper (specific average-precision numbers available in figures, not tabulated in-text); suffered overfitting in neural network settings relative to GE criteria.",
            "effectiveness_qualitative": "While theoretically principled, NNPU was less robust in these cryoEM image experiments than the GE-criteria approaches when labeled positives were very few.",
            "impact_potential": "Serves as an important baseline and demonstrates limitations of unbiased risk estimators in minibatch neural training for noisy imaging tasks.",
            "comparison_to_alternatives": "Directly compared to GE-binomial and GE-KL in benchmarking; GE approaches dramatically outperformed NNPU on these cryoEM datasets.",
            "success_factors": "Principled risk estimation reduces some overfitting but may not address minibatch-gradient and domain-specific challenges encountered in cryoEM.",
            "key_insight": "Unbiased/non-negative risk estimation for PU learning (NNPU) may still overfit in practical neural network training on noisy imaging domains compared to GE-criteria approaches that explicitly regularize minibatch expectations.",
            "uuid": "e2352.4",
            "source_info": {
                "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "PN (naive negative labeling)",
            "name_full": "PN: Treating unlabeled examples as negative (naive approach)",
            "brief_description": "A simple baseline that treats all unlabeled micrograph regions as negatives and trains a standard supervised classifier on the labeled positives and these pseudo-negatives.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "cryoEM particle detection / PU learning baselines",
            "problem_description": "When π is small, treating unlabeled as negatives can be used to train classifiers but risks severe bias and overfitting since many unlabeled regions are actually positive.",
            "data_availability": "Many unlabeled regions; works only when positives are rare enough that unlabeled-as-negative assumption approximates truth.",
            "data_structure": "Image patches.",
            "problem_complexity": "Low conceptual complexity but often fails due to mislabeled positives in the negative pool and class imbalance.",
            "domain_maturity": "Common naive approach used as baseline; known limitations especially in domains where unlabeled positives are non-negligible.",
            "mechanistic_understanding_requirements": "Low; straightforward supervised learning with noisy labels.",
            "ai_methodology_name": "PN (treat U as negative) naive supervised training",
            "ai_methodology_description": "Loss: π E_{x∼P}[L(g(x),1)] + (1-π) E_{x∼U}[L(g(x),0)] (implemented directly); used as a baseline and compared in experiments.",
            "ai_methodology_category": "Supervised learning with noisy/unreliable labels (baseline)",
            "applicability": "Sometimes effective when π is very small; otherwise prone to overfitting and biased decision boundaries; outperformed by GE-based PU methods in this paper's benchmarks.",
            "effectiveness_quantitative": "Underperformed relative to GE-binomial and GE-KL on the cryoEM benchmarks; specific AP values shown in figures but not enumerated in text.",
            "effectiveness_qualitative": "Suffers from overfitting and requires many labeled negatives in practice; not recommended for particle-picking where negative space is diverse.",
            "impact_potential": "Limited as a primary method but useful as a simple baseline to evaluate more sophisticated PU methods.",
            "comparison_to_alternatives": "Directly compared and shown to be inferior to GE-based approaches and often inferior to NNPU in certain regimes.",
            "success_factors": "Simplicity; may work when positive prevalence in U is extremely low.",
            "key_insight": "Treating unlabeled regions as negatives is a brittle baseline for particle-picking because unlabeled data contains non-negligible positive examples and highly varied negative types, requiring more principled PU methods.",
            "uuid": "e2352.5",
            "source_info": {
                "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Traditional detection (DoG / template)",
            "name_full": "Difference-of-Gaussians (DoG) and template-based particle picking",
            "brief_description": "Classical non-ML particle detection methods: DoG detects compact spherical objects of known scale; template matching uses cross-correlation against reference templates to find similar regions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "cryoEM particle detection",
            "problem_description": "Automatic detection of particles in micrographs using matched filters (DoG) or cross-correlation with templates; limited for irregular shapes and can produce many false positives.",
            "data_availability": "DoG/template methods operate on full micrographs (unlabeled) and may require templates derived from known structures or hand-labeled particles.",
            "data_structure": "2D image micrographs; templates are image patches.",
            "problem_complexity": "Lower algorithmic complexity but high failure rate for non-spherical particles and structured background; sensitive to template quality and size assumptions.",
            "domain_maturity": "Long-established baseline methods widely used historically in cryoEM particle picking.",
            "mechanistic_understanding_requirements": "Low; transparent matched-filter behavior.",
            "ai_methodology_name": "Classical image processing detection (DoG, template matching)",
            "ai_methodology_description": "DoG: bandpass matched filter to detect blob-like particles of known size; template matching: cross-correlate micrographs with templates (from known structures or hand-labeled particles) to find matches.",
            "ai_methodology_category": "Classical computer vision / signal-processing",
            "applicability": "Applicable to roughly spherical, compact particles or when good templates exist; fails for atypical/irregularly shaped particles and yields high false positives without strong post-processing.",
            "effectiveness_quantitative": "Not quantified here, but described as limited with high false positives and often requiring substantial manual post-processing.",
            "effectiveness_qualitative": "Fast and simple for suitable particle shapes, but inadequate for many modern cryoEM challenges; motivates adoption of learned methods.",
            "impact_potential": "Serves as practical baseline for easy datasets but limited generalizability.",
            "comparison_to_alternatives": "Contrasted with ML approaches (CNNs, PU learning) which address shape variability and structured noise better.",
            "success_factors": "Simplicity and low computational cost when assumptions hold (spherical/compact particles).",
            "key_insight": "Classical detection methods are insufficient for irregular shapes and low-SNR cryoEM micrographs, motivating learned PU-based CNN approaches like Topaz.",
            "uuid": "e2352.6",
            "source_info": {
                "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs",
                "publication_date_yy_mm": "2018-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Positive-Unlabeled Learning with Non-Negative Risk Estimator",
            "rating": 2
        },
        {
            "paper_title": "Generalized Expectation Criteria for Semi-Supervised Learning",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "rating": 1
        }
    ],
    "cost": 0.017263999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Positive-unlabeled convolutional neural networks for particle picking in cryoelectron micrographs</h1>
<p>Tristan Bepler ${ }^{1,2}$, Andrew Morin ${ }^{2,3}$, Julia Brasch ${ }^{4}$, Lawrence Shapiro ${ }^{4}$, Alex J. Noble ${ }^{5, <em>}$, and Bonnie Berger ${ }^{2,3, * </em>}$<br>${ }^{1}$ Computational and Systems Biology, MIT, Cambridge, MA, USA<br>${ }^{2}$ Computer Science and Artificial Intelligence Laboratory, MIT, Cambridge, MA, USA<br>${ }^{3}$ Department of Mathematics, MIT, Cambridge, MA, USA<br>${ }^{4}$ Department of Biochemistry and Molecular Biophysics, Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, NY, NY, USA<br>${ }^{5}$ National Resource for Automated Molecular Microscopy, Simons Electron Microscopy Center, New York Structural Biology Center, NY, NY, USA<br><em> Corresponding author for cryoEM experiments: anoble@nysbc.org<br></em>* Corresponding author: bab@mit.edu</p>
<h4>Abstract</h4>
<p>Cryo-electron microscopy (cryoEM) is an increasingly popular method for protein structure determination. However, identifying a sufficient number of particles for analysis (often $&gt;100,000$ ) can take months of manual effort. Current computational approaches are limited by high false positive rates and require significant ad-hoc post-processing, especially for unusually shaped particles. To address this shortcoming, we develop Topaz, an efficient and accurate particle picking pipeline using neural networks trained with few labeled particles by newly leveraging the remaining unlabeled particles through the framework of positive-unlabeled (PU) learning. Remarkably, despite using minimal labeled particles, Topaz allows us to improve reconstruction resolution by up to $0.15 \AA$ over published particles on three public cryoEM datasets without any post-processing. Furthermore, we show that our novel generalizedexpectation criteria approach to PU learning outperforms existing general PU learning approaches when applied to particle detection, especially for challenging datasets of nonglobular proteins. We expect Topaz to be an essential component of cryoEM analysis.</p>
<h1>Introduction</h1>
<p>Transmission electron microscopy (TEM) of purified proteins vitrified on thin metal EM grids, called single particle cryo-electron microscopy (cryoEM), is an emerging technology capable of resolving high resolution structures of proteins in near-native states. CryoEM projection images, or micrographs, can contain hundreds or thousands of individual protein projections, called particles. By identifying a sufficient number of particles in a sufficient number of orientations, a 3D reconstruction of the purified protein can be solved by aligning the particles in 3D Fourier space ${ }^{1}$. However, due to the low signal-to-noise ratio of cryoEM images, large numbers of observations are required for accurate reconstruction. Studies have shown a monotonicallyincreasing linear dependence between the logarithm of the number of particles included in a single particle alignment and the inverse of the resolution of the resulting 3D reconstruction ${ }^{2}$. The concentration of protein on EM grids, the efficiency of data collection, and the completeness and accuracy of particle identification are all factors affecting the total number of particles that can be used for downstream reconstruction and hence the achievable resolution. In particular, accurate particle identification, also known as particle picking, is a major bottleneck, often taking weeks or even months with current workflows for small or non-globular particles, due to variability in particle shapes and structured noise in cryoEM micrographs.</p>
<p>A variety of methods have been developed for automation of particle picking. The most common are Difference of Gaussians (DoG) and template-based approaches ${ }^{3-7}$. DoG is a detection method limited to spherical, compact particles of known size since it matches these features directly. Template-based approaches, on the other hand, search micrographs for regions matching available particle templates based on cross correlation. These templates can be derived from known structures or using hand-labeled particles. Template derivation from known</p>
<p>structures, in particular, can produce large numbers of false positives or fail for atypical particles ${ }^{8-10}$.</p>
<p>Recently, to address the shortcomings of these techniques, new methods based on convolutional neural networks (CNNs) have been proposed ${ }^{11-13}$. These methods use positive and negative labeled micrograph regions to train CNN classifiers which are used to predict labels for the remaining regions. However, due to factors like low signal-to-noise ratio, structured background, and the distribution of particle morphologies, researchers must label a large number of regions for training - a non-trivial and time-consuming task. Moreover, the diverse characteristics of negative data make it difficult to manually label a representative set of negative examples, and hence the number of labeled negatives must be an order of magnitude larger than the number of positives to achieve acceptable performance ${ }^{14}$. Thus, these methods have seen only limited adoption by the cryoEM community and hand-labeling remains the gold standard.</p>
<p>To overcome the challenges inherent in current automatic particle picking methods, we newly frame the problem of particle-picking as a positive-unlabeled (PU) learning problem in which we seek to learn a classifier using a small number of labeled positive regions and the remaining unlabeled micrograph regions. PU learning has proved to be an effective paradigm when working with partially labeled data, primarily with application to document classification ${ }^{15}$, time series classification ${ }^{16}$, and anomaly detection ${ }^{17}$. Recent work in machine learning has generally explored PU learning for neural network models ${ }^{18}$ based on estimating the true positive-negative risk, but overfitting remains a challenging problem for PU learning. In order to address this, we approach PU learning as a constrained optimization problem in which we wish to find classifier parameters to minimize classification errors on the labeled data subject to a constraint on the expectation over the unlabeled data. By imposing this constraint softly with</p>
<p>a novel generalized expectation (GE) criteria, we are able to mitigate overfitting and train high accuracy particle classifiers using very few labeled data points. Furthermore, by combining our PU learning method with autoencoder-based regularization, we can further reduce the amount of labeled data required for high performance.</p>
<p>Here, we present Topaz, a pipeline for particle picking using convolutional neural networks with PU learning. We demonstrate that by using Topaz with only 1,000 labeled examples, we are able to improve the 3D structure resolution of several publicly available cryoEM datasets by up to $0.15 \AA$ over the published particle sets, which were constructed based on significant manual curation efforts. Remarkably, this improvement is without any ad hoc post-processing typically required for high resolution structures; we feed Topaz predictions directly into alignment and reconstruction. We also find that Topaz achieves a remarkably low false positive rate even when using a relaxed predicted probability threshold to retrieve large particle sets. Finally, we compare our GE based PU learning techniques with other recent PU learning methods and demonstrate that our PU learning approach enables particle picking with minimal labeled examples, even on a challenging dataset containing stick-like particles with low signal-to-noise.</p>
<p>Our source code is freely available for academic (https://github.com/tbepler/topaz) and the program runs efficiently on a single GPU computer. Topaz is currently being integrated into Appion ${ }^{20}$ and may be integrated into other cryoEM software suites.</p>
<h1>Results</h1>
<h2>1. The Topaz Pipeline</h2>
<p>The Topaz particle picking pipeline is composed of three main steps (Figure 2): (1) whole micrograph preprocessing with a mixture model newly designed to capture micrograph statistics, (2) neural network classifier training with our PU learning framework, and (3) micrograph segmentation and particle coordinate extraction by non-maximum suppression. Our implementation of this pipeline is freely available for academic use at https://github.com/tbepler/topaz.</p>
<h2>Micrograph preprocessing</h2>
<p>Prior to classifier training, micrographs must first be downsampled and normalized.
Downsampling is critical for reducing pixel level noise and making training and particle extraction more efficient. Micrographs are then normalized using a per-micrograph scaled twocomponent Gaussian mixture model (see Methods). This captures and corrects for large-scale intensity differences between micrographs and the bi-modality of pixel values caused by micrographs containing dark grid sections.</p>
<h2>Classifier training from positive and unlabeled data</h2>
<p>The core advancement of Topaz is our ability to leverage unlabeled data when training particle classifiers. We frame particle picking as a PU learning problem in which we seek to learn a classifier that discriminates between particle and non-particle micrograph regions given a small number of labeled particles and many unlabeled micrograph regions. CNN classifiers are trained using minibatched stochastic gradient descent with a novel objective function, GE-binomial (see</p>
<p>Methods for details), which explicitly models the sampling statistics of minibatch training to regularize the classifier's posterior over the unlabeled data. Combining this with an optional autoencoder module allows high-accuracy classifiers to be trained despite using very few positive examples (Methods). This approach allows us to overcome overfitting problems associated with recent PU learning methods developed for neural networks in domains other than cryoEM analysis and to effectively pick particles in challenging cryoEM datasets as we demonstrate in our experiments.</p>
<h1>Micrograph segmentation and particle extraction</h1>
<p>Given a trained CNN particle classifier, we extract predicted particle coordinates and their associated predicted probabilities. First, we calculate the per pixel predicted probabilities by convolving the classifier over each micrograph. Then, to extract coordinates from these dense predictions, we use the well known non-maximum suppression algorithm to greedily select high scoring pixels and remove their neighbors from consideration as particle centers. This yields a list of predicted particle coordinates and their associated model scores for each micrograph.</p>
<h2>2. Topaz improves structure resolutions with no postprocessing</h2>
<p>Because the end goal of single-particle cryoEM is to produce a high resolution 3D structure, we evaluate the full Topaz particle picking pipeline by utilizing picked particles in reconstructions for three cryoEM datasets containing T20S proteasome (EMPIAR-10025), 80S ribosome</p>
<p>(EMPIAR-10028), and rabbit muscle aldolase (NYSBC-aldo). Each of these datasets already has a curated set of particles giving high quality reconstructions which we compare with particles predicted by Topaz trained with 1,000 positives based on reconstruction quality (Methods). We standardize the reconstruction procedure by using cryoSPARC homogeneous refinement on the raw Topaz particle sets (i.e. no postprocessing was applied) and published particle sets with identical settings for each dataset (Methods). By considering the reconstruction resolution at decreasing probability thresholds (increasing numbers of particles) predicted by Topaz, we select the particle set that optimizes the resolution for each dataset. We find that Topaz is able to retrieve significantly more good particles than were present in the curated particle sets, finding 3.22, 1.72, and 3.68 times more particles in EMPIAR-10025, EMPIAR-10028, and NYSBC-aldo respectively. Remarkably, this led to an improvement in the reconstruction resolution by $\sim 0.15 \AA$ for EMPIAR-10025 and $\sim 0.05 \AA$ for EMPIAR-10028 (Figure 3) despite performing no postprocessing (i.e. particle filtering with 2D or 3D class averaging or iterative reconstructions removing particles poorly fitting the 3D model) on the Topaz particle sets. All predicted particle coordinates were fed directly into the reconstruction pipeline. Furthermore, the $3.0 \AA$ structure we report for EMPIAR-10028 is, to our knowledge, the highest resolution structure ever reported for this dataset. We improve the resolution by $0.1 \AA$ over the best structure reported in the EM map challenge ${ }^{21}$ simply by picking particles with Topaz trained using 1,000 initial examples. For NYSBC-aldo, although Topaz finds many more particles than were in the published dataset, both particle sets achieve the same reconstruction resolution ( $2.63 \AA$ by $\mathrm{FSC}_{0.143}$ ), suggesting that the $\sim 200 \mathrm{k}$ particles in the published set is already sufficient to reach the resolution limit of the data given standard reconstuction methods. We verify that the additional particles found by Topaz are good particles by performing reconstructions using only the newly picked particles (i.e. we</p>
<p>remove the entire published particle set from the Topaz particle set and perform reconstruction) and we find nearly identical structures (Figure 3).</p>
<h1>3. Topaz particle predictions are well-ranked and contain few false positives</h1>
<p>We next quantify the quality of the particles predicted by Topaz over varying predicted probability thresholds by calculating the reconstruction resolution and estimating the number of false positive particles based on 2D class averaging. For each dataset, reconstructions are calculated using particles predicted by Topaz at decreasing probability cutoffs (Figure 4a). We find that the resolution of Topaz structures increases as we include more good particles and then drops once the threshold becomes small and too many false positives are included as demonstrated by the dip in resolution for the last threshold of EMPIAR-10025. Furthermore, we compare these curves with those obtained by randomly subsampling the published particle sets and find that Topaz particles quickly match the resolution of the published particles for the proteasome and ribosome datasets. For the aldolase dataset, we see that more Topaz particles are required to match and then exceed the resolution of the curated particle set. This could be because Topaz does not find enough side views of the particle until the probability is sufficiently lowered whereas the curated dataset has been filtered to be enriched for these views (Supplementary Figure 5).</p>
<p>We also classified the particle sets at each threshold into ten classes and manually examined the class averages to determine whether each class represented true particles or false positives. As expected, we find that as the probability threshold is decreased, the fraction of false positives increases (Figure 4b). However, the number of false positives remains remarkably low even at relaxed thresholds. Furthermore, particles appear to be well-ranked in that noisy or</p>
<p>unusual particle classes only start to appear at low thresholds. For example, the T20S proteasome dataset is contaminated with gold particles which appear as dark spots in the micrographs. Particles in close proximity to gold are only selected as the probability threshold is decreased (Figure 4c). Similar trends can be observed in the ribosome (Supplementary Figure 4) and aldolase (Supplementary Figure 5) class averages.</p>
<h1>4. Our GE criteria based PU learning framework outperforms other PU learning methods on cryoEM datasets</h1>
<h2>Comparison of PU learning methods</h2>
<p>We consider two generalized expectation-based approaches to PU learning. Given the positive class prior, $\pi$, we constrain the classifier using the unlabeled data by matching the classifier's expectation to $\pi$ using the KL-divergence (GE-KL) or using the cross entropy between the classifier's posterior and the Binomial distribution prior over the number of positives in each minibatch (GE-binomial) (Methods). To evaluate the effectiveness of our GE-based PU learning methods, we benchmark against the recent non-negative risk estimator approach of Kiryu et al. ${ }^{18}$ (NNPU) and the naive approach in which unlabeled data are considered as negative for classifier training (PN) on two additional cryoEM datasets. This is important to keep our PU learning methods development separate from the full Topaz evaluation above. The first dataset, EMPIAR10096, is a publicly available dataset containing influenza hemagglutinin trimer particles and the second, Shapiro-lab, is a challenging dataset provided by the Shapiro lab containing a stick-like particle with low signal-to-noise. For purposes of comparison, we simulated positively labeled</p>
<p>datasets of varying sizes by randomly subsampling the set of the all positive examples within the training set of each dataset.</p>
<p>We find that across all experiments, classifiers trained with our GE criteria-based objective functions dramatically outperform those trained with the NNPU or PN methods. Generally, GE-binomial and GE-KL classifiers display similar performance with a few important exceptions where GE-binomial gives better results. For the dataset with more compact particles, EMPIAR-10096, GE-binomial gives significantly ( $\mathrm{p}&lt;0.05$ by Student's paired t-test) better test set average-precision scores than GE-KL when the number of data points is tiny ( 10 positive examples; Figure 5a). At larger numbers of positives, both methods are statistically equivalent. On the challenging Shapiro-lab dataset, GE-binomial significantly outperforms GE-KL at 1,000 labeled examples ( $\mathrm{p}&lt;0.05$ ) whereas GE-KL gives better results ( $\mathrm{p}&lt;0.05$ ) within the 50-250 range of labeled examples. These results indicate that our GE based PU learning approaches dramatically outperform previous PU learning methods, enabling particle picking despite few labeled positives on the challenging Shapiro-lab dataset and substantially improving picking quality on the easier EMPIAR-10096. Although GE-binomial and GE-KL perform similarly in this experiment, we do find that GE-binomial outperforms GE-KL in the two important cases of 10 easy particles and 1,000 difficult particles.</p>
<h1>Augmentation with autoencoder</h1>
<p>We next consider whether classifier performance can be improved when few labeled data points are available by introducing a generator network with corresponding reconstruction error term in the objective to form a hybrid classifier+autoencoder network (Methods). We hypothesized that including this reconstruction component would improve the generalizability of the classifier</p>
<p>when few labeled data points are available by requiring that the feature vectors given by the encoder network be descriptive of the input - acting as a sort of machine learning technique known as regularization.</p>
<p>We test this by training classifiers with different settings of the autoencoder weight, $\gamma$, and varying numbers of labeled data points, $N$, on the EMPIAR-10096 and Shapiro-lab datasets. We compare models trained with $\gamma=0$ (no autoencoder), $\gamma=1$, and $\gamma=10 / N$. For each setting of $\gamma$ and $N$, we train 10 models with different sets of $N$ randomly sampled positives and calculate the average-precision score for each model on the test split of each dataset. We find that including the decoder network with reconstruction error term in the objective ( $\gamma=1$ and $\gamma=$ $10 / N$ ) improves classifier performance in the few labeled data points regime (Figure 5b). As the number of data points increases, the benefit of using the autoencoder decreases and then hurts classifier performance due to over-regularization. Our results from both datasets suggest that using the autoencoder with $\gamma=10 / N$ gives best results when $N \leq 250$ and that not using the autoencoder is best for $N&gt;250$. Combined with PU learning, autoencoder-based regularization is an effective method to further improve classifier performance when few labeled positives are available.</p>
<h1>Discussion</h1>
<p>CryoEM is revolutionizing structural biology with widespread applications ranging from basic biology to the understanding of disease-linked proteins and the development of novel therapeutics. Fully realizing the promise of cryoEM and achieving rapid turnaround from imaging to structure determination requires state-of-the-art computational methods. To this end, we present Topaz, a particle picking pipeline using PU learning - a framework naturally suited to the task of particle picking where only a small number of labeled positive examples are available for training. We show empirically that our GE-criteria-based approach outperforms other PU learning techniques for particle detection. In addition, augmenting the neural network classifier with an autoencoder further improves performance in the regime of very few labeled data points $(\mathrm{n}&lt;500)$. Topaz enables particle picking for unusually shaped particles with low signal-to-noise and, remarkably, allows us to improve structure resolution without any time consuming and adhoc post-processing.</p>
<p>Although we use a simple CNN architecture with reasonable default hyperparameters and show that it performs well on these datasets, any model architecture that can be trained with gradient descent can use our GE-criteria objective functions to learn from positive and unlabeled data. Furthermore, additional hyperparameter tuning, such as L2 or dropout regularization, can improve model performance. The only hyperparameter introduced by our objective function, and other positive-unlabeled objectives that we consider, is the unknown positive class prior. Although this parameter should also be chosen by cross validation, we observed that our results were relatively insensitive to its choice (Supplemental Figure 3). Our novel GE-binomial PU learning method could also have widespread utility for object detection in other domains, for example in light microscopy or medical imaging where positive labels are frequently incomplete.</p>
<p>Additionally, although we proposed GE-binomial for positive-unlabeled learning, it is straightforward to extend to the full semi-supervised case (where some labeled negative regions are provided) by taking the expectation of the loss over all labeled data in the first term.</p>
<p>By reporting a predicted probability of being a particle in addition to particle coordinates, Topaz allows researchers to take particle sets of varying size by choosing a probability threshold at which to select particles. This allows particles to be included iteratively by lowering the threshold until reconstruction resolution stops improving. However, it is also possible for reconstruction algorithms to explicitly take these probabilities into account when determining 3D structures.</p>
<p>Topaz requires researchers to label far fewer particles to achieve high quality predictions. It performs well independently of particle shape, opening automated picking to a wide selection of proteins previously too difficult to locate computationally. In addition, our pipeline is computationally efficient - training in a few hours on a single GPU and producing predictions for hundreds of micrographs in only minutes. Furthermore, once a model is trained for a specific particle, it can be applied to new imaging runs of the same particle. Topaz greatly expedites structure determination by cryoEM, enabling particle picking for previously difficult datasets and reducing the manual effort required to achieve high resolution structures.</p>
<h1>Methods</h1>
<h2>1. Dataset description</h2>
<p>Aligned and summed micrographs and star files containing published particle sets were retrieved from EMPIAR for datasets EMPIAR-10025, EMPIAR-10028, and EMPIAR-10096. Aligned and summed micrographs and hand labeled particle coordinates were provided by the Shapiro lab for the Shapiro-lab dataset. Aligned and summed micrographs and a curated in-house particle set were provided by the New York Structural Biology Center for the NYSBC-aldo dataset. Micrographs for each dataset were downsampled to the resolution specified in table 1 and normalized as described in the following section. Each dataset was then split into training and test sets at the micrograph level. The number of micrographs and labeled particles in each split are also reported in table 1.</p>
<h2>2. Micrograph normalization</h2>
<p>Images were then normalized using a per-image scaled two component Gaussian mixture model. Given $K$ images, each pixel is modeled as being drawn from a two component Gaussian mixture model, parameterized by $\pi$, the mixing parameter, $\mu_{0}, \sigma_{0}, \mu_{1}$, and $\sigma_{1}$, the means and standard deviations of the Gaussian distributions, with a scalar multiplier for each image, $\alpha_{1 _K}$. Let $x_{i, j, k}$ be the value of the pixel at position $i, j$ in image $k$, it is distributed according to</p>
<p>$$
\begin{gathered}
z_{i, j, k} \sim \operatorname{Bernoulli}(\pi) \
x_{i, j, k} \mid z_{i, j, k} \sim \operatorname{Gaussian}\left(\alpha_{k} \mu_{z_{i, j, k}}\left(\alpha_{k} \sigma_{z_{i, j, k}}\right)^{2}\right)
\end{gathered}
$$</p>
<p>where $z_{i, j, k}$ is a random variable denoting the component membership of the pixel. The maximum likelihood values of the parameters $\pi, \mu_{0}, \mu_{1}, \sigma_{0}, \sigma_{1}$ and $\alpha_{1 _K}$ are found by expectation-</p>
<p>maximization for each data set. Then, the pixels are normalized by first dividing by the image scaling factor and then standardizing to the dominant mixture component. Let $\mu^{\prime}, \sigma^{\prime}$ be $\mu_{0}, \sigma_{0}$ if $\pi&lt;0.5$ and $\mu_{1}, \sigma_{1}$ otherwise, then the normalized pixel values $x_{i, j, k}^{\prime}$ are given by</p>
<p>$$
x_{i, j, k}^{\prime}=\left(\frac{x_{i, j, k}}{\alpha_{k}}-\mu^{\prime}\right) / \sigma^{\prime}
$$</p>
<h1>3. PU learning baselines</h1>
<p>Let P be the set of labeled positive micrograph regions (centered on a particle), and U be the set of unlabeled micrograph regions where $\pi$ is the fraction of positive examples within $U$. Then, the task is to learn a classifier $(g)$ that discriminates between positive and negative regions given $P$ and $U$. When $\pi$ is small, treating the unlabeled examples as negatives for the purposes of classifier training with the following standard loss minimization objective can be effective</p>
<p>$$
\pi E_{x \sim P}[L(g(x), 1)]+(1-\pi) E_{x \sim U}[L(g(x), 0)]
$$</p>
<p>However, in general, this approach suffers from overfitting due to poor specification of the classification objective - it is minimized when positives are perfectly separated from unlabeled data points. To address this, Kiryo et al. ${ }^{18}$ recently proposed an unbiased estimator of the true positive-negative classification objective for positive and unlabeled data with known $\pi$ and a non-negative estimator (PU) which is shown to reduce overfitting still present in the unbiased estimator.</p>
<h2>4. PU learning with generalized expectation criteria</h2>
<p>Here, we adopt an alternative approach to positive-unlabeled learning not based on estimating the PN misclassification risk. Instead, we observe that the unlabeled data with known $\pi$ can be</p>
<p>used to constrain a classifier such that it minimizes the classification loss on the labeled data and matches the expectation ( $\pi$ ) over the unlabeled data. In other words, we wish to find the classifier, $g$, that minimizes $E_{x \sim P}[L(g(x), 1)]$ subject to the constraint $E_{x \sim U}[g(x)]=\pi$. This constraint can be imposed "softly" through a regularization term in the objective function with weight $\lambda$ :</p>
<p>$$
E_{x \sim P}[L(g(x), 1)]+\lambda K L\left(E_{x \sim U}[g(x)] | \pi\right)
$$</p>
<p>In this objective function, we impose the constraint through the KL-divergence between the expectation of the classifier over the unlabeled data and the known fraction of positives which is minimized when these terms are equal. This approach is an instance of a general class of posterior regularization called generalized expectation (GE) criteria, as specifically proposed by Mann and McCallum ${ }^{19}$. However, because we wish for our classifier to be a neural network and to optimize the objective using minibatched stochastic gradient descent, the gradient of the objective must be approximating using samples from the data. Estimates of the gradient of the GE-KL objective from samples are biased, which could cause SGD to find a suboptimal solution.</p>
<p>To address this issue, we propose an alternative GE criteria, GE-binomial, defined so as to minimize the difference between the distribution over the number of positives in the minibatch and the binomial distribution parameterized by $\pi$. The number of positive data points, k , in a minibatch of N samples from U follows the binomial distribution with parameter $\pi$. Furthermore, the classifier $g$ also describes a distribution over the number of positives in the minibatch as</p>
<p>$$
q(k)=\sum_{y \in Y(k)} \prod_{i=1}^{N} g\left(x_{i}\right)^{y_{i}}\left(1-g\left(x_{i}\right)^{\left(1-y_{i}\right)}\right)
$$</p>
<p>where x is a micrograph region, y is an indicator vector $\left(y_{i} \in{0,1}\right)$ denoting which data points are positive $\left(y_{i}=1\right)$ and negative $\left(y_{i}=0\right)$ and $\mathrm{Y}(\mathrm{k})$ is the set of all such vectors summing to k .</p>
<p>This allows us to define the new GE criteria as the cross entropy between these two distributions $\sum_{k=1}^{N} q(k) \log p(k)$ giving the full GE-binomial objective function</p>
<p>$$
E_{x \sim P}[L(g(x), 1)]+\lambda \sum_{k=1}^{N} q(k) \log p(k)
$$</p>
<p>In practice, because computing exact $q(k)$ is slow, we make a Gaussian approximation with mean $\sum_{i=1}^{N} g\left(x_{i}\right)$ and variance $\sum_{i=1}^{N} g\left(x_{i}\right)\left(1-g\left(x_{i}\right)\right)$ and substitute the Gaussian PDF with these parameters for $q$ in the above equation.</p>
<h1>5. Autoencoder-based classifier regularization</h1>
<p>When including the autoencoder component, we break our classifier network into two components: an encoder network composed of all layers except the final linear layer and the linear classifier layer. We denote these networks as $f$ and $c$, respectively, with the full network, $g$, being given by $g(x)=c(f(x))$. Furthermore, we introduce a deconvolutional (also called transposed convolutional, see next section) decoder network, $d$, which takes the output of the feature extractor network and returns a reconstruction of the input image, $x^{\prime}=d(f(x))$. The objective function is then modified to include a term penalizing the expected reconstruction error over all images in the dataset, $D$, with weight $\gamma$</p>
<p>$$
E_{x \sim P}[L(c(f(x)), 1)]+\lambda \sum_{k=1}^{N} q(k) \log p(k)+\gamma E_{x \sim D}\left[|x-d(f(x))|_{2}^{2}\right]
$$</p>
<p>This forms the full GE-binomial objective function with autoencoder component used in Topaz.</p>
<h2>6. Classifier and autoencoder architectures and hyperparameters</h2>
<p>We use a simple three-layer convolutional neural network with striding, batch normalization ${ }^{22}$, and parametric rectified linear units ( PReLU ) as the classifier in this work. The model is</p>
<p>organized as 32 conv7x7 filters with batch normalization and PReLU, stride by 2, 64 conv5x5 filters with batch normalization and PReLU, stride by 2, 128 conv5x5 filters with batch normalization and PReLU, and a final fully connected layer with a single output.</p>
<p>When augmenting with an autoencoder, we use a decoder structure similar to that of DCGAN ${ }^{23}$. The d-dimensional representation output by the final convolutional layer of the classifier network is projected to a small spatial dimension but large feature dimension representation. This is repeatedly projected into larger spatial dimension and smaller feature dimension representations until the final output is of the original input image size. Specifically, this model is structured as repeated transpose convolutions with batch normalization and leaky ReLU activations. Let z be the representation output by the final convolutional layer of the classifier and $\mathrm{X}^{\prime}$ be the image reconstruction given by the decoder, the decoder structure is $\mathrm{z}-&gt;$ transpose conv4x4 128-d, batch normalization, leaky ReLU -&gt; transpose conv4x4 64-d, stride 2, batch normalization, leaky ReLU -&gt; transpose conv4x4 32-d, stride 2, batch normalization, leaky ReLU -&gt; transpose conv3x3 1-d, stride $2-&gt;\mathrm{X}^{\prime}$.</p>
<h1>7. PU learning benchmarking</h1>
<p>To compare classifiers trained with the different objective functions, we simulate hand labeling with various amounts of effort by randomly sampling varying numbers of particles from the training sets to treat as the positive examples. All other particles are considered unlabeled. We use cross entropy loss for the labeled particles. The values of $\pi$ used for training are specified in table 1. For GE-KL we set the GE criteria weight, $\lambda$, to 10 as recommended by Mann and McCallum ${ }^{19}$. For GE-binomial, we set this parameter to 1 . The classifier is then trained with those positives and evaluated by average-precision score (see next section for description of</p>
<p>classifier evaluation) on the test set micrographs. This is repeated with 10 independent samples of particles for each number of positives.</p>
<h1>8. Classifier evaluation</h1>
<p>Classifiers were evaluating by average-precision score. This score is a measure of how well ranked the micrograph regions were when ordered by the predicted probability of containing a particle and corresponds to the area under the precision-recall curve. It is calculated as the sum over the ranked micrograph regions of the precision at $k$ elements times the change in recall</p>
<p>$$
\sum_{k=1}^{n} \operatorname{Pr}(k)(R(k)-R(k-1))
$$</p>
<p>where precision $(\operatorname{Pr})$ is the fraction of predictions that are correct and recall $(\operatorname{Re})$ is the fraction of labeled particles that are retrieved in the top $k$ predictions</p>
<p>$$
\begin{gathered}
T P(k)=\sum_{i=1}^{k} \sum y_{i} \
\operatorname{Pr}(k)=\operatorname{TP}(k) / k \
R(k)=\operatorname{TP}(k) / \sum_{i=1}^{n} y_{i}
\end{gathered}
$$</p>
<p>This measure is commonly used in information retrieval.</p>
<h2>9. Non-maximum suppression algorithm for extracting particle coordinates</h2>
<p>Non-maximum suppression chooses coordinates and their corresponding predicted probabilities of being a particle greedily starting from the highest scoring region. In order to prevent nearby pixels from also being considered particle candidates, all pixels within a second user-defined radius are excluded when a coordinate is selected. We set this radius to be the half major-axis</p>
<p>length of the particle, however, smaller radii may give better results for closely packed, irregularly shaped particles.</p>
<h1>10. Micrograph pre-processing</h1>
<p>For EMPIAR-10025 and $-10096^{24,25}$, the aligned and summed micrographs along with CTF estimates were taken directly from the public data release on EMPIAR. For EMPIAR-10028 ${ }^{26}$, frames were aligned and summed without dose compensation using MotionCor2. Whole micrograph CTF estimates provided with the public release were used for this dataset.</p>
<p>For the clustered protocadherin dataset (Shapiro-lab), single particle micrographs were collected on a Titan Krios (Thermo Fisher Scientific) equipped with a K2 counting camera (Gatan, Inc.); the microscope was operated at 300 kV with a calibrated pixel size of $1.061 \AA .10$ secs exposures were collected ( 40 frames/micrograph), for a total dose of $68 \mathrm{e}^{-} / \AA^{2}$ with a defocus range of 1 to $4 \mu \mathrm{~m}$. A total of 896 micrographs were collected using Leginon ${ }^{27}$. Frames were aligned using MotionCor2 ${ }^{28} .1,540$ particles were picked manually using Appion Manual Picker ${ }^{20}$ from 87 micrographs and used as a training dataset for Topaz.</p>
<p>The rabbit muscle aldolase dataset (NYSBC-aldo) was collected on a Titan Krios (Thermo Fisher Scientific) equipped with a K2 counting camera (Gatan, Inc.) in super-resolution mode; the microscope was operated at 300 kV with a calibrated super-resolution pixel size of $0.416 \AA .6$ secs exposures were collected ( 30 frames $/$ micrograph), for a total dose of $70.32 \mathrm{e}^{-} / \AA^{2}$ with a defocus range of 1 to $2 \mu \mathrm{~m}$. A total of 1,052 micrographs were collected using Leginon ${ }^{27}$. Frames were aligned, Fourier binned by a factor of 2, and dose compensated using MotionCor2 ${ }^{28}$. Whole-image CTF estimation was performed using CTFFIND4 ${ }^{29}$.</p>            </div>
        </div>

    </div>
</body>
</html>
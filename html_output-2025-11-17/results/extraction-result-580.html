<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-580 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-580</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-580</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-83b44a37ddf121e660c873e207c2909b79f23469</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/83b44a37ddf121e660c873e207c2909b79f23469" target="_blank">Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting</a></p>
                <p><strong>Paper Venue:</strong> IEEE transactions on intelligent transportation systems (Print)</p>
                <p><strong>Paper TL;DR:</strong> A novel deep learning framework, Traffic Graph Convolutional Long Short-Term Memory Neural Network (TGC-LSTM), to learn the interactions between roadways in the traffic network and forecast the network-wide traffic state and shows that the proposed model outperforms baseline methods on two real-world traffic state datasets.</p>
                <p><strong>Paper Abstract:</strong> Traffic forecasting is a particularly challenging application of spatiotemporal forecasting, due to the time-varying traffic patterns and the complicated spatial dependencies on road networks. To address this challenge, we learn the traffic network as a graph and propose a novel deep learning framework, Traffic Graph Convolutional Long Short-Term Memory Neural Network (TGC-LSTM), to learn the interactions between roadways in the traffic network and forecast the network-wide traffic state. We define the traffic graph convolution based on the physical network topology. The relationship between the proposed traffic graph convolution and the spectral graph convolution is also discussed. An L1-norm on graph convolution weights and an L2-norm on graph convolution features are added to the model’s loss function to enhance the interpretability of the proposed model. Experimental results show that the proposed model outperforms baseline methods on two real-world traffic state datasets. The visualization of the graph convolution weights indicates that the proposed framework can recognize the most influential road segments in real-world traffic networks.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e580.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e580.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TGC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traffic Graph Convolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-convolution operator designed for traffic networks that multiplies trainable per-edge weight matrices elementwise with k-hop adjacency and a free-flow reachability mask to extract localized, physically-plausible features for each sensing location.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Traffic Graph Convolution (TGC)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>TGC computes features as GC_t^k = (W_{gc_k} ⊙ \bar{A}^k ⊙ FFR) x_t for each hop k, where W_{gc_k} is an N×N trainable weight matrix, \bar{A}^k is the clipped k-hop neighborhood matrix, FFR is a binary free-flow reachable matrix, and x_t is the vector of node signals at time t. Features from k=1..K are concatenated to form the input to downstream layers. The Hadamard masking ensures the receptive field is both k-localized and physically reachable under free-flow assumptions; sparsity of parameters is encouraged with L1 regularization and feature-smoothness with an L2 term across adjacent hops.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (graph-based convolution)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>graph signal processing / machine learning / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>transportation engineering / network-scale traffic forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Introduced a domain-specific mask (Free-Flow Reachable matrix) to limit receptive fields by physical travelability; replaced generic adjacency or Laplacian-only formulations with elementwise (Hadamard) product W ⊙ \bar{A}^k ⊙ FFR to give per-edge learnable weights only where physically meaningful; allowed multiple k-hop orders concatenated to enrich features; applied L1 regularization on W and an L2 consistency penalty across hop features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - as part of the TGC-LSTM model the TGC operator led to state-of-the-art forecasting on two real-world datasets: LOOP (TGC-LSTM MAE 2.57±0.10 mph vs best baseline SGC+LSTM 2.64±0.12) and INRIX (TGC-LSTM MAE 1.02±0.07 mph vs SGC+LSTM 1.07±0.08); improvements occurred across MAE, MAPE and RMSE.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Higher parameter and computational cost (O(N^2) weight matrices) relative to some spectral/localized methods; need to choose K (hop size) to trade off richness vs cost; without regularization learned weights could be noisy and hard to interpret; FFR requires accurate distance/free-flow data.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of road-network topology (adjacency), precise distances and free-flow speed data to build FFR; underlying graph-structured data matches graph-convolution framework; existing graph-conv theory/frameworks enabled rapid adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Must have adjacency matrix, distance matrix, and free-flow speeds for nodes/segments; choose time quantum Δt and horizon parameter m to compute FFR; sufficient compute (GPU) to handle N×N parameters and training; hyperparameter tuning (K, λ1, λ2, learning rate).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately generalizable — method can be applied to other spatiotemporal networked systems that have meaningful physical edge metrics and reachability (e.g., power distribution, pipeline networks, pedestrian flows) but requires designing an appropriate domain-specific reachability mask analogous to FFR.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (computational method augmented with domain physical constraints); instrumental/technical skills to implement and tune.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e580.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e580.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TGC-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traffic Graph Convolutional Long Short-Term Memory Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based recurrent architecture whose input is concatenated multi-hop TGC features and which includes a neighborhood-aware cell-state gate that aggregates neighboring cell states via an FFR-masked adjacency to model spatiotemporal traffic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Traffic Graph Convolutional LSTM (TGC-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>TGC-LSTM replaces the vanilla LSTM input with GC^{(K)}_t (concatenated K-hop TGC features) and retains the standard LSTM gates mapping from the graph features and previous hidden states. It adds a cell-state neighbor gate: C_{t-1}^t = W_N ⊙ (A^K ⊙ FFR) · C_{t-1}, i.e., previous cell states are aggregated from neighbors masked by K-hop adjacency and FFR and weighted by W_N, then used in the recurrent update C_t = f_t ⊙ C_{t-1}^t + i_t ⊙ \tilde{C}_t. The output h_t is computed as usual. Training minimizes MSE with optional L1/L2 regularizers on GC weights/features.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / machine learning model adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>sequence modelling / deep learning (RNN/LSTM methods)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>transportation engineering / spatiotemporal graph forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (hybrid approach combining graph convolution and RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Input space changed from raw time-series to concatenated graph-convolutional features; added cell-state neighbor gate that uses masked adjacency (A^K ⊙ FFR) and W_N to incorporate neighborhood cell-state influences; adjusted weight dimensions to accept KN-sized inputs; incorporated GC-specific regularizations to stabilize interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - outperformed a set of baselines (ARIMA, SVR, FNN, LSTM, DiffGRU, Conv+LSTM, LSGC+LSTM, SGC+LSTM) on two datasets. Example metrics: LOOP MAE 2.57±0.10 (TGC-LSTM) vs 2.64±0.12 (SGC+LSTM); INRIX MAE 1.02±0.07 vs 1.07±0.08.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Increased training time versus vanilla LSTM (≈2× per epoch); additional hyperparameters (K, λ1, λ2, m) to tune; requirement for adjacency/FFR/distance data; potential for overfitting without regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Compatibility of LSTM with sequential traffic data; availability of graph features from TGC; ability to encode physical neighbor influence through FFR; GPU for training.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Graph structure (A, A^k), FFR, per-node signals, chosen K and m, and compute resources to handle larger input dimensionality (KN).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable to other spatiotemporal forecasting tasks on networks (e.g., electrical load, water flow, epidemic spread) provided analogous graph topology and meaningful neighbor reachability definitions exist.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural adaptation (architectural changes), instrumental/technical skills for implementation and tuning, and interpretive framework for neighbor influence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e580.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e580.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spectral Graph Convolution (and Localized Spectral Graph Convolution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph convolution defined in the graph Fourier domain via Laplacian eigendecomposition (spectral filters) and a computationally efficient localized polynomial approximation (LSGC) that sums powers of the Laplacian to achieve K-localized filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Spectral Graph Convolution / Localized Spectral Graph Convolution (LSGC)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Spectral graph convolution defines filtering as h_θ *_G x = U diag(θ) U^T x, where U and Λ are Laplacian eigenvectors/values. Localized spectral conv approximates filters with polynomials in Λ so h_{θ'} *_G x = Σ_{j=0}^{K-1} θ'_j L^j x, avoiding explicit eigendecomposition and yielding K-hop localization.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational/analytical method (graph signal processing)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>graph signal processing / applied mathematics / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic forecasting (used as feature extractor prior to LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with computational adaptation (polynomial localization)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Used localized polynomial filters (LSGC) to avoid eigendecomposition; stacked a single spectral/LSGC layer before an LSTM for sequence forecasting; selected K=3 for experiments to match receptive-field comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - SGC+LSTM improved over vanilla LSTM in experiments (e.g., SGC+LSTM MAE 2.64 vs LSTM 2.70 on LOOP), demonstrating spectral filtering helps, but lacked incorporation of physical edge properties (distances, free-flow reachability) and interpretability compared to TGC.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires selection/tuning of K; full spectral SGC requires eigen-decomposition (computational cost); multiple spectral layers can reduce interpretability; does not natively incorporate physical edge attributes like distance or free-flow reachability.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Solid mathematical foundation, prior literature and implementations, and availability of adjacency to form Laplacian.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Graph Laplacian L (or approximation via polynomials), choice of polynomial order K, compute resources for multiplications or eigendecomposition when used without approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>General approach applicable to many graph-structured prediction problems, but may require additional modifications to incorporate domain-specific physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles (graph Fourier analysis) and explicit computational procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e580.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e580.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conv-Image</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traffic-as-Images Conversion for Convolutional Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-representation technique that maps traffic-network states into 2D/3D images or matrices so that standard 2D/3D convolutional neural networks developed for vision can be applied to extract spatial(-temporal) features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning traffic as images: a deep convolutional neural network for large-scale transportation network speed prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Traffic-to-image conversion + CNN / 3D-CNN</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Traffic sensor/link signals are arranged into 2D grids or 3D tensors (adding channels or a temporal dimension) to feed conventional CNNs. 1D/2D/3D convolution kernels are used to capture local spatial (and temporal) patterns as if the traffic state lived on an image; often followed by LSTM for sequence modelling. The paper cites prior work that used 2D/3D CNNs but notes that converting inherently non-Euclidean networks to grids introduces noise and spurious spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data representation technique / computational method (CNN application)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision / image processing / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>transportation network forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with representational adaptation (mapping graph to grid)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Mapped graph nodes/links to positions in a matrix; used multiple channels or kernel sizes to capture multi-scale features; some works use 3D convolutions to include the temporal axis. The paper notes these mappings inevitably introduce noise / spurious relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful in prior literature: CNN-based pipelines sometimes perform well but are suboptimal for arbitrary road network topologies; in this paper Conv+LSTM baseline performed similarly to vanilla LSTM but was outperformed by graph-based methods that respect topology.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Mismatch between Euclidean-grid convolutions and non-Euclidean road graphs; mapping distortions produce noisy/spurious spatial relationships; loss of physical adjacency structure.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Wide availability and maturity of CNN methods and tooling; large volumes of spatiotemporal data that can be arranged into grids.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>A mapping from graph nodes to image/grid positions, design of channels/time-aggregation, kernel sizes, and significant data to train high-parameter CNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Limited: works best when the network can be embedded into a grid with low distortion or when approximate Euclidean structure is acceptable; less suitable for arbitrary or complex graph topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and data-representation know-how (tacit mapping choices affect performance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e580.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e580.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FFR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Free-Flow Reachable Matrix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary matrix that indicates whether a vehicle can traverse from node i to node j within m time-steps at free-flow speed, used to mask graph convolutions so receptive fields respect physical travelability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Free-Flow Reachable Matrix (FFR) for masking convolution receptive fields</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute Dist_{i,j} (roadway distance) and free-flow speed S_{i,j}; for chosen Δt (time quantum) and m (number of time steps), set FFR_{i,j}=1 if S_{i,j} * m * Δt - Dist_{i,j} ≥ 0 else 0; diagonals set to 1. The FFR is elementwise-multiplied (Hadamard) with k-hop adjacency and weight matrices to ensure only physically reachable neighbors within the temporal horizon contribute to convolution/neighbor aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational/data-preprocessing technique embedding domain physical constraints</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>transportation engineering / traffic flow theory (free-flow speed, cell transmission conceptual foundations)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>graph convolution design / machine learning for traffic forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (embedding physical reachability into ML operator)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Formalized domain physical travelability into a binary mask and integrated it into the convolution operator via Hadamard product; selected m and Δt to control temporal horizon; used to constrain both feature extraction and neighbor cell-state aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - contributed to improved localization, interpretability and forecasting accuracy of the TGC-TGC-LSTM approach relative to purely topological spectral baselines; FFR also allowed more physically meaningful weight visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires accurate per-edge distances and sensible free-flow speed assignments; choice of m and Δt strongly affects receptive field; for very large k the masked adjacency converges to FFR limiting benefit of further hops.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of physical roadway attributes (lengths, speed limits); established traffic flow concepts that directly map to reachability.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Distance matrix, reliable free-flow speed for segments, chosen time quantum and horizon, and integration into the convolution computation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to other physical flow networks where a notion of uncongested travel time exists (e.g., freight corridors, pedestrian networks) but not applicable to abstract graphs lacking physical distance/time semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural knowledge and theoretical principles from transportation engineering applied to ML.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e580.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e580.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GC-Regularization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolution Weight and Feature Regularization (L1 & L2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of L1 regularization to TGC weight matrices to induce sparsity and an L2 penalty on differences between adjacent-hop TGC features to enforce feature consistency and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>L1 weight sparsity and L2 inter-hop feature regularization for graph convolution</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Adds two regularization terms to the loss: R^{[1]} = Σ_{i=1}^K |W_{gc_i}| (L1-norm of per-hop weight matrices) to encourage sparse, interpretable weights; and R^{[2]} = ||GC_T^{(K)}||_2 defined as sqrt(Σ_{i=1}^{K-1} (GC^i - GC^{i+1})^2) to penalize large differences between features extracted from adjacent hops. Loss = MSE + λ1 R^{[1]} + λ2 R^{[2]} with empirically chosen λ1=λ2=0.01 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>regularization technique / model-stabilization method</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / statistics (regularization concepts)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>graph-based traffic forecasting interpretability and stability</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with adaptation (feature-level L2 redefined across hop differences)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied conventional L1 sparsity to per-edge/ per-hop GC weight matrices and defined an L2 penalty specifically to measure and reduce differences between adjacent-hop graph convolution features; empirically tuned penalty strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - increased sparsity and feature consistency, improving interpretability; produced a small accuracy trade-off (MAE increased by ~0.02) but yielded more stable/meaningful learned weights suitable for visualization of influential road segments.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Trade-off between interpretability and marginal drop in predictive accuracy; requires tuning λ1/λ2; if overly strong, harms performance.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Well-understood behavior of L1/L2 in ML; presence of per-hop weights/features that can be regularized; availability of interpretability objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to GC weights and features for computing penalties; hyperparameter search to balance accuracy vs interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Broadly generalizable to other graph-conv models where interpretability/stability of learned edge weights or multi-hop features is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural knowledge (regularization formulas) and interpretive framing for model explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e580.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e580.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Conv (DiffGRU baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Convolution (as used in Diffusion Convolutional Recurrent Neural Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph convolution based on diffusion processes on graphs previously applied to traffic forecasting in DCRNN; in this paper an adjusted DiffGRU baseline was used, replacing diffusion conv with spectral graph conv because the study's graph is undirected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Diffusion convolution applied in recurrent networks (DiffGRU / DCRNN)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Diffusion convolution models information propagation as a diffusion process along graph edges and integrates it into recurrent cells (e.g., GRU) to capture spatiotemporal dynamics. The original DCRNN uses directed random-walk diffusion operators; in this paper the authors implemented an adjusted DiffGRU baseline replacing diffusion convolution with spectral graph convolution due to the undirected nature of their graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / graph neural network technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>graph neural networks / diffusion processes / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (modified operator to fit undirected graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Replaced original diffusion convolution (which assumes directionality) with spectral graph convolution to fit the undirected graph setting and combined it with GRU gates; used as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - DiffGRU baseline performed worse than LSTM and other baselines on experiments (e.g., LOOP MAE 4.64 compared to LSTM 2.70), leading authors to suggest that the GRU (without cell state) and the operator choice may limit performance in their tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Mismatch between diffusion operator assumptions (directed diffusion) and undirected graph data; possible limitations of GRU compared to LSTM for storing long-term cell-state information; operator choice affects performance.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Prior success of diffusion convolution in other traffic forecasting works; ability to swap convolution operators in recurrent architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Graph directionality must match operator assumptions; design choice of GRU vs LSTM; availability of adjacency and potential diffusion transition matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Diffusion convolution is well-suited to directed diffusion-like processes and can be adapted for various network flow forecasting tasks, but operator selection must respect graph properties (directionality) and temporal-memory needs.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles (diffusion on graphs) and explicit computational procedures; instrumental skills to implement model adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering <em>(Rating: 2)</em></li>
                <li>Spectral Networks and Locally Connected Networks on Graphs <em>(Rating: 2)</em></li>
                <li>Semi-supervised classification with graph convolutional networks <em>(Rating: 2)</em></li>
                <li>Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting <em>(Rating: 2)</em></li>
                <li>Diffusion-convolutional neural networks <em>(Rating: 1)</em></li>
                <li>Learning traffic as images: a deep convolutional neural network for large-scale transportation network speed prediction <em>(Rating: 2)</em></li>
                <li>Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-580",
    "paper_id": "paper-83b44a37ddf121e660c873e207c2909b79f23469",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "TGC",
            "name_full": "Traffic Graph Convolution",
            "brief_description": "A graph-convolution operator designed for traffic networks that multiplies trainable per-edge weight matrices elementwise with k-hop adjacency and a free-flow reachability mask to extract localized, physically-plausible features for each sensing location.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Traffic Graph Convolution (TGC)",
            "procedure_description": "TGC computes features as GC_t^k = (W_{gc_k} ⊙ \\bar{A}^k ⊙ FFR) x_t for each hop k, where W_{gc_k} is an N×N trainable weight matrix, \\bar{A}^k is the clipped k-hop neighborhood matrix, FFR is a binary free-flow reachable matrix, and x_t is the vector of node signals at time t. Features from k=1..K are concatenated to form the input to downstream layers. The Hadamard masking ensures the receptive field is both k-localized and physically reachable under free-flow assumptions; sparsity of parameters is encouraged with L1 regularization and feature-smoothness with an L2 term across adjacent hops.",
            "procedure_type": "computational method / data analysis technique (graph-based convolution)",
            "source_domain": "graph signal processing / machine learning / computer science",
            "target_domain": "transportation engineering / network-scale traffic forecasting",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Introduced a domain-specific mask (Free-Flow Reachable matrix) to limit receptive fields by physical travelability; replaced generic adjacency or Laplacian-only formulations with elementwise (Hadamard) product W ⊙ \\bar{A}^k ⊙ FFR to give per-edge learnable weights only where physically meaningful; allowed multiple k-hop orders concatenated to enrich features; applied L1 regularization on W and an L2 consistency penalty across hop features.",
            "transfer_success": "successful - as part of the TGC-LSTM model the TGC operator led to state-of-the-art forecasting on two real-world datasets: LOOP (TGC-LSTM MAE 2.57±0.10 mph vs best baseline SGC+LSTM 2.64±0.12) and INRIX (TGC-LSTM MAE 1.02±0.07 mph vs SGC+LSTM 1.07±0.08); improvements occurred across MAE, MAPE and RMSE.",
            "barriers_encountered": "Higher parameter and computational cost (O(N^2) weight matrices) relative to some spectral/localized methods; need to choose K (hop size) to trade off richness vs cost; without regularization learned weights could be noisy and hard to interpret; FFR requires accurate distance/free-flow data.",
            "facilitating_factors": "Availability of road-network topology (adjacency), precise distances and free-flow speed data to build FFR; underlying graph-structured data matches graph-convolution framework; existing graph-conv theory/frameworks enabled rapid adaptation.",
            "contextual_requirements": "Must have adjacency matrix, distance matrix, and free-flow speeds for nodes/segments; choose time quantum Δt and horizon parameter m to compute FFR; sufficient compute (GPU) to handle N×N parameters and training; hyperparameter tuning (K, λ1, λ2, learning rate).",
            "generalizability": "Moderately generalizable — method can be applied to other spatiotemporal networked systems that have meaningful physical edge metrics and reachability (e.g., power distribution, pipeline networks, pedestrian flows) but requires designing an appropriate domain-specific reachability mask analogous to FFR.",
            "knowledge_type": "explicit procedural steps and theoretical principles (computational method augmented with domain physical constraints); instrumental/technical skills to implement and tune.",
            "uuid": "e580.0",
            "source_info": {
                "paper_title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "TGC-LSTM",
            "name_full": "Traffic Graph Convolutional Long Short-Term Memory Neural Network",
            "brief_description": "An LSTM-based recurrent architecture whose input is concatenated multi-hop TGC features and which includes a neighborhood-aware cell-state gate that aggregates neighboring cell states via an FFR-masked adjacency to model spatiotemporal traffic dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Traffic Graph Convolutional LSTM (TGC-LSTM)",
            "procedure_description": "TGC-LSTM replaces the vanilla LSTM input with GC^{(K)}_t (concatenated K-hop TGC features) and retains the standard LSTM gates mapping from the graph features and previous hidden states. It adds a cell-state neighbor gate: C_{t-1}^t = W_N ⊙ (A^K ⊙ FFR) · C_{t-1}, i.e., previous cell states are aggregated from neighbors masked by K-hop adjacency and FFR and weighted by W_N, then used in the recurrent update C_t = f_t ⊙ C_{t-1}^t + i_t ⊙ \\tilde{C}_t. The output h_t is computed as usual. Training minimizes MSE with optional L1/L2 regularizers on GC weights/features.",
            "procedure_type": "computational method / machine learning model adaptation",
            "source_domain": "sequence modelling / deep learning (RNN/LSTM methods)",
            "target_domain": "transportation engineering / spatiotemporal graph forecasting",
            "transfer_type": "adapted/modified for new context (hybrid approach combining graph convolution and RNN)",
            "modifications_made": "Input space changed from raw time-series to concatenated graph-convolutional features; added cell-state neighbor gate that uses masked adjacency (A^K ⊙ FFR) and W_N to incorporate neighborhood cell-state influences; adjusted weight dimensions to accept KN-sized inputs; incorporated GC-specific regularizations to stabilize interpretability.",
            "transfer_success": "successful - outperformed a set of baselines (ARIMA, SVR, FNN, LSTM, DiffGRU, Conv+LSTM, LSGC+LSTM, SGC+LSTM) on two datasets. Example metrics: LOOP MAE 2.57±0.10 (TGC-LSTM) vs 2.64±0.12 (SGC+LSTM); INRIX MAE 1.02±0.07 vs 1.07±0.08.",
            "barriers_encountered": "Increased training time versus vanilla LSTM (≈2× per epoch); additional hyperparameters (K, λ1, λ2, m) to tune; requirement for adjacency/FFR/distance data; potential for overfitting without regularization.",
            "facilitating_factors": "Compatibility of LSTM with sequential traffic data; availability of graph features from TGC; ability to encode physical neighbor influence through FFR; GPU for training.",
            "contextual_requirements": "Graph structure (A, A^k), FFR, per-node signals, chosen K and m, and compute resources to handle larger input dimensionality (KN).",
            "generalizability": "Likely generalizable to other spatiotemporal forecasting tasks on networks (e.g., electrical load, water flow, epidemic spread) provided analogous graph topology and meaningful neighbor reachability definitions exist.",
            "knowledge_type": "explicit procedural adaptation (architectural changes), instrumental/technical skills for implementation and tuning, and interpretive framework for neighbor influence.",
            "uuid": "e580.1",
            "source_info": {
                "paper_title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "SGC",
            "name_full": "Spectral Graph Convolution (and Localized Spectral Graph Convolution)",
            "brief_description": "Graph convolution defined in the graph Fourier domain via Laplacian eigendecomposition (spectral filters) and a computationally efficient localized polynomial approximation (LSGC) that sums powers of the Laplacian to achieve K-localized filtering.",
            "citation_title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
            "mention_or_use": "use",
            "procedure_name": "Spectral Graph Convolution / Localized Spectral Graph Convolution (LSGC)",
            "procedure_description": "Spectral graph convolution defines filtering as h_θ *_G x = U diag(θ) U^T x, where U and Λ are Laplacian eigenvectors/values. Localized spectral conv approximates filters with polynomials in Λ so h_{θ'} *_G x = Σ_{j=0}^{K-1} θ'_j L^j x, avoiding explicit eigendecomposition and yielding K-hop localization.",
            "procedure_type": "computational/analytical method (graph signal processing)",
            "source_domain": "graph signal processing / applied mathematics / machine learning",
            "target_domain": "traffic forecasting (used as feature extractor prior to LSTM)",
            "transfer_type": "direct application with computational adaptation (polynomial localization)",
            "modifications_made": "Used localized polynomial filters (LSGC) to avoid eigendecomposition; stacked a single spectral/LSGC layer before an LSTM for sequence forecasting; selected K=3 for experiments to match receptive-field comparisons.",
            "transfer_success": "partially successful - SGC+LSTM improved over vanilla LSTM in experiments (e.g., SGC+LSTM MAE 2.64 vs LSTM 2.70 on LOOP), demonstrating spectral filtering helps, but lacked incorporation of physical edge properties (distances, free-flow reachability) and interpretability compared to TGC.",
            "barriers_encountered": "Requires selection/tuning of K; full spectral SGC requires eigen-decomposition (computational cost); multiple spectral layers can reduce interpretability; does not natively incorporate physical edge attributes like distance or free-flow reachability.",
            "facilitating_factors": "Solid mathematical foundation, prior literature and implementations, and availability of adjacency to form Laplacian.",
            "contextual_requirements": "Graph Laplacian L (or approximation via polynomials), choice of polynomial order K, compute resources for multiplications or eigendecomposition when used without approximation.",
            "generalizability": "General approach applicable to many graph-structured prediction problems, but may require additional modifications to incorporate domain-specific physical constraints.",
            "knowledge_type": "theoretical principles (graph Fourier analysis) and explicit computational procedures.",
            "uuid": "e580.2",
            "source_info": {
                "paper_title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Conv-Image",
            "name_full": "Traffic-as-Images Conversion for Convolutional Neural Networks",
            "brief_description": "A data-representation technique that maps traffic-network states into 2D/3D images or matrices so that standard 2D/3D convolutional neural networks developed for vision can be applied to extract spatial(-temporal) features.",
            "citation_title": "Learning traffic as images: a deep convolutional neural network for large-scale transportation network speed prediction",
            "mention_or_use": "mention",
            "procedure_name": "Traffic-to-image conversion + CNN / 3D-CNN",
            "procedure_description": "Traffic sensor/link signals are arranged into 2D grids or 3D tensors (adding channels or a temporal dimension) to feed conventional CNNs. 1D/2D/3D convolution kernels are used to capture local spatial (and temporal) patterns as if the traffic state lived on an image; often followed by LSTM for sequence modelling. The paper cites prior work that used 2D/3D CNNs but notes that converting inherently non-Euclidean networks to grids introduces noise and spurious spatial relations.",
            "procedure_type": "data representation technique / computational method (CNN application)",
            "source_domain": "computer vision / image processing / deep learning",
            "target_domain": "transportation network forecasting",
            "transfer_type": "direct application with representational adaptation (mapping graph to grid)",
            "modifications_made": "Mapped graph nodes/links to positions in a matrix; used multiple channels or kernel sizes to capture multi-scale features; some works use 3D convolutions to include the temporal axis. The paper notes these mappings inevitably introduce noise / spurious relationships.",
            "transfer_success": "partially successful in prior literature: CNN-based pipelines sometimes perform well but are suboptimal for arbitrary road network topologies; in this paper Conv+LSTM baseline performed similarly to vanilla LSTM but was outperformed by graph-based methods that respect topology.",
            "barriers_encountered": "Mismatch between Euclidean-grid convolutions and non-Euclidean road graphs; mapping distortions produce noisy/spurious spatial relationships; loss of physical adjacency structure.",
            "facilitating_factors": "Wide availability and maturity of CNN methods and tooling; large volumes of spatiotemporal data that can be arranged into grids.",
            "contextual_requirements": "A mapping from graph nodes to image/grid positions, design of channels/time-aggregation, kernel sizes, and significant data to train high-parameter CNNs.",
            "generalizability": "Limited: works best when the network can be embedded into a grid with low distortion or when approximate Euclidean structure is acceptable; less suitable for arbitrary or complex graph topologies.",
            "knowledge_type": "explicit procedural steps and data-representation know-how (tacit mapping choices affect performance).",
            "uuid": "e580.3",
            "source_info": {
                "paper_title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "FFR",
            "name_full": "Free-Flow Reachable Matrix",
            "brief_description": "A binary matrix that indicates whether a vehicle can traverse from node i to node j within m time-steps at free-flow speed, used to mask graph convolutions so receptive fields respect physical travelability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Free-Flow Reachable Matrix (FFR) for masking convolution receptive fields",
            "procedure_description": "Compute Dist_{i,j} (roadway distance) and free-flow speed S_{i,j}; for chosen Δt (time quantum) and m (number of time steps), set FFR_{i,j}=1 if S_{i,j} * m * Δt - Dist_{i,j} ≥ 0 else 0; diagonals set to 1. The FFR is elementwise-multiplied (Hadamard) with k-hop adjacency and weight matrices to ensure only physically reachable neighbors within the temporal horizon contribute to convolution/neighbor aggregation.",
            "procedure_type": "computational/data-preprocessing technique embedding domain physical constraints",
            "source_domain": "transportation engineering / traffic flow theory (free-flow speed, cell transmission conceptual foundations)",
            "target_domain": "graph convolution design / machine learning for traffic forecasting",
            "transfer_type": "adapted/modified for new context (embedding physical reachability into ML operator)",
            "modifications_made": "Formalized domain physical travelability into a binary mask and integrated it into the convolution operator via Hadamard product; selected m and Δt to control temporal horizon; used to constrain both feature extraction and neighbor cell-state aggregation.",
            "transfer_success": "successful - contributed to improved localization, interpretability and forecasting accuracy of the TGC-TGC-LSTM approach relative to purely topological spectral baselines; FFR also allowed more physically meaningful weight visualization.",
            "barriers_encountered": "Requires accurate per-edge distances and sensible free-flow speed assignments; choice of m and Δt strongly affects receptive field; for very large k the masked adjacency converges to FFR limiting benefit of further hops.",
            "facilitating_factors": "Availability of physical roadway attributes (lengths, speed limits); established traffic flow concepts that directly map to reachability.",
            "contextual_requirements": "Distance matrix, reliable free-flow speed for segments, chosen time quantum and horizon, and integration into the convolution computation.",
            "generalizability": "Generalizable to other physical flow networks where a notion of uncongested travel time exists (e.g., freight corridors, pedestrian networks) but not applicable to abstract graphs lacking physical distance/time semantics.",
            "knowledge_type": "explicit procedural knowledge and theoretical principles from transportation engineering applied to ML.",
            "uuid": "e580.4",
            "source_info": {
                "paper_title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "GC-Regularization",
            "name_full": "Graph Convolution Weight and Feature Regularization (L1 & L2)",
            "brief_description": "Application of L1 regularization to TGC weight matrices to induce sparsity and an L2 penalty on differences between adjacent-hop TGC features to enforce feature consistency and interpretability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "L1 weight sparsity and L2 inter-hop feature regularization for graph convolution",
            "procedure_description": "Adds two regularization terms to the loss: R^{[1]} = Σ_{i=1}^K |W_{gc_i}| (L1-norm of per-hop weight matrices) to encourage sparse, interpretable weights; and R^{[2]} = ||GC_T^{(K)}||_2 defined as sqrt(Σ_{i=1}^{K-1} (GC^i - GC^{i+1})^2) to penalize large differences between features extracted from adjacent hops. Loss = MSE + λ1 R^{[1]} + λ2 R^{[2]} with empirically chosen λ1=λ2=0.01 in experiments.",
            "procedure_type": "regularization technique / model-stabilization method",
            "source_domain": "machine learning / statistics (regularization concepts)",
            "target_domain": "graph-based traffic forecasting interpretability and stability",
            "transfer_type": "direct application with adaptation (feature-level L2 redefined across hop differences)",
            "modifications_made": "Applied conventional L1 sparsity to per-edge/ per-hop GC weight matrices and defined an L2 penalty specifically to measure and reduce differences between adjacent-hop graph convolution features; empirically tuned penalty strengths.",
            "transfer_success": "successful - increased sparsity and feature consistency, improving interpretability; produced a small accuracy trade-off (MAE increased by ~0.02) but yielded more stable/meaningful learned weights suitable for visualization of influential road segments.",
            "barriers_encountered": "Trade-off between interpretability and marginal drop in predictive accuracy; requires tuning λ1/λ2; if overly strong, harms performance.",
            "facilitating_factors": "Well-understood behavior of L1/L2 in ML; presence of per-hop weights/features that can be regularized; availability of interpretability objectives.",
            "contextual_requirements": "Access to GC weights and features for computing penalties; hyperparameter search to balance accuracy vs interpretability.",
            "generalizability": "Broadly generalizable to other graph-conv models where interpretability/stability of learned edge weights or multi-hop features is desired.",
            "knowledge_type": "explicit procedural knowledge (regularization formulas) and interpretive framing for model explanation.",
            "uuid": "e580.5",
            "source_info": {
                "paper_title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Diffusion Conv (DiffGRU baseline)",
            "name_full": "Diffusion Convolution (as used in Diffusion Convolutional Recurrent Neural Networks)",
            "brief_description": "A graph convolution based on diffusion processes on graphs previously applied to traffic forecasting in DCRNN; in this paper an adjusted DiffGRU baseline was used, replacing diffusion conv with spectral graph conv because the study's graph is undirected.",
            "citation_title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
            "mention_or_use": "use",
            "procedure_name": "Diffusion convolution applied in recurrent networks (DiffGRU / DCRNN)",
            "procedure_description": "Diffusion convolution models information propagation as a diffusion process along graph edges and integrates it into recurrent cells (e.g., GRU) to capture spatiotemporal dynamics. The original DCRNN uses directed random-walk diffusion operators; in this paper the authors implemented an adjusted DiffGRU baseline replacing diffusion convolution with spectral graph convolution due to the undirected nature of their graphs.",
            "procedure_type": "computational method / graph neural network technique",
            "source_domain": "graph neural networks / diffusion processes / machine learning",
            "target_domain": "traffic forecasting",
            "transfer_type": "adapted/modified for new context (modified operator to fit undirected graphs)",
            "modifications_made": "Replaced original diffusion convolution (which assumes directionality) with spectral graph convolution to fit the undirected graph setting and combined it with GRU gates; used as a baseline for comparison.",
            "transfer_success": "partially successful - DiffGRU baseline performed worse than LSTM and other baselines on experiments (e.g., LOOP MAE 4.64 compared to LSTM 2.70), leading authors to suggest that the GRU (without cell state) and the operator choice may limit performance in their tasks.",
            "barriers_encountered": "Mismatch between diffusion operator assumptions (directed diffusion) and undirected graph data; possible limitations of GRU compared to LSTM for storing long-term cell-state information; operator choice affects performance.",
            "facilitating_factors": "Prior success of diffusion convolution in other traffic forecasting works; ability to swap convolution operators in recurrent architectures.",
            "contextual_requirements": "Graph directionality must match operator assumptions; design choice of GRU vs LSTM; availability of adjacency and potential diffusion transition matrices.",
            "generalizability": "Diffusion convolution is well-suited to directed diffusion-like processes and can be adapted for various network flow forecasting tasks, but operator selection must respect graph properties (directionality) and temporal-memory needs.",
            "knowledge_type": "theoretical principles (diffusion on graphs) and explicit computational procedures; instrumental skills to implement model adaptations.",
            "uuid": "e580.6",
            "source_info": {
                "paper_title": "Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
                "publication_date_yy_mm": "2018-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
            "rating": 2
        },
        {
            "paper_title": "Spectral Networks and Locally Connected Networks on Graphs",
            "rating": 2
        },
        {
            "paper_title": "Semi-supervised classification with graph convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
            "rating": 2
        },
        {
            "paper_title": "Diffusion-convolutional neural networks",
            "rating": 1
        },
        {
            "paper_title": "Learning traffic as images: a deep convolutional neural network for large-scale transportation network speed prediction",
            "rating": 2
        },
        {
            "paper_title": "Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting",
            "rating": 2
        }
    ],
    "cost": 0.02508525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Traffic Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting</h1>
<p>Zhiyong Cui, Kristian Henrickson, Ruimin Ke, Ziyuan Pu, and Yinhai Wang*</p>
<h4>Abstract</h4>
<p>Traffic forecasting is a particularly challenging application of spatiotemporal forecasting, due to the time-varying traffic patterns and the complicated spatial dependencies on road networks. To address this challenge, we learn the traffic network as a graph and propose a novel deep learning framework, Traffic Graph Convolutional Long Short-Term Memory Neural Network (TGC-LSTM), to learn the interactions between roadways in the traffic network and forecast the network-wide traffic state. We define the traffic graph convolution based on the physical network topology. The relationship between the proposed traffic graph convolution and the spectral graph convolution is also discussed. An L1-norm on graph convolution weights and an L2-norm on graph convolution features are added to the model's loss function to enhance the interpretability of the proposed model. Experimental results show that the proposed model outperforms baseline methods on two real-world traffic state datasets. The visualization of the graph convolution weights indicates that the proposed framework can recognize the most influential road segments in real-world traffic networks.</p>
<p>Index Terms- Traffic forecasting, Spatial-temporal, Graph convolution, LSTM, Recurrent neural network</p>
<h2>I. INTRODUCTION</h2>
<p>TRAFFIC forecasting is one of the most challenging components of Intelligent Transportation Systems (ITS). The goal of traffic forecasting is to predict future traffic states in the traffic network given a sequence of historical traffic states and the physical roadway network. Since the volume and variety of traffic data has been increasing in recent years, datadriven traffic forecasting methods have shown considerable promise in their ability to outperform conventional and simulation-based methods [1].</p>
<p>Previous work [2][3][4][5] on this topic roughly categorizes existing models into two categories: classical statistical methods and machine learning models. Most of the studies focusing on traffic forecasting using statistical methods were developed when traffic systems were less complex, and the sizes of traffic datasets were relatively small. However, statistical models' capability of handling high dimensional time series data is quite limited. With the more recent rapid</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>development in computational power, as well as growth in traffic data volume, much of the more recent work on this topic focuses on machine learning methods for traffic forecasting.</p>
<p>Machine learning methods with the capability of capturing complex non-linear relationships, like support vector regression (SVR) [6], tend to outperform the statistical methods, such as autoregressive integrated moving average (ARIMA) [7] and its variants, with respect to handling complex traffic forecasting problems [8]. However, the full potential of artificial intelligence approaches to traffic forecasting was not exploited until the rise of deep neural network (NN) models (also referred to as deep learning models). Following early works [2], [9] applying NNs to the traffic prediction problem, many NN-based methods have been adopted for traffic forecasting.</p>
<p>Deep learning models for traffic forecasting, such as deep belief networks (DBN) [10] and stacked auto-encoders [11], can effectively learn high dimensional features and achieve good forecasting performance. Recurrent neural network (RNN) and its variants, including long short-term memory (LSTM) [12] and gated recurrent unit (GRU) [13] networks, have also shown great potential for solving traffic forecasting problems [8], [14], [15], [16]. Although RNN-based methods can learn the spatial dependencies, they tend to be overcomplex and inevitably capture a certain amount of noise and spurious relationships which likely do not represent the true causal structure in a physical traffic network. Moreover, interpreting the network parameters in terms of real-world spatial dependencies is most often impossible. To address this, other works [5], [17], [18] attempt to model spatial dependencies with convolutional neural network (CNN). However, conventional CNNs are most appropriate for spatial relationships in the Euclidean space as represented by twodimensional (2D) matrices or images. Thus, spatial features learned in CNN are not optimal for representing the traffic network structure [19][20].</p>
<p>Recently, substantial research has focused on extending the convolution operator to more general, graph-structured data, which can be applied to capture the spatial relationships present in a traffic network. There are two primary ways to conduct graph convolution. The first class of methods [21], [22], [23],</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>[24] makes use of spectral graph theory, by designing spectral filter/convolutions based on the graph Laplacian matrix. Spectral-based graph convolution has been adopted and combined with RNN [20] and CNN [1] to forecast traffic states. These models successfully apply convolution to graphstructured data, but they do not fully capture the unique properties of graphs [25], like traffic networks. These models [23], [26] usually adopt multiple graph convolution layers, and thus, their learned spatial dependencies are hard to interpret. The other form of graph convolution proposed in several newlypublished studies is conducted on graph data dynamically, for example, the dynamic edge-conditioned filters in graph convolution [27], the high-order adaptive graph convolutional network [25][28]. Still, these methods are not capable of fully accommodating the physical specialties of traffic networks.</p>
<p>One of the deficiencies of the previous graph convolutionbased models is that the receptive field of the convolution operators is not confined in the graph according to the real structure of the traffic network. The traffic states of two locations far apart from each other in the traffic network should not be influenced by each other in a short time period. Though the spectral graph convolution models [20],[23] can capture features from K-localized neighbors of a vertex in the graph, how to choose the value of K and whether the localized neighbors truly affect the vertex are still questions to be answered. Thus, we propose a free-flow reachable matrix based on the free-flow speed of the real traffic and apply it on the graph convolution operator to learn features from the truly influential neighborhood in the traffic network.</p>
<p>In this study, we learn the traffic network as a graph and conduct convolution on the traffic network-based graph. To learn localized features and incorporate roadway physical characteristics, we proposed a traffic graph convolution operator. Base on this operator, we propose a traffic graph convolutional LSTM (TGC-LSTM) to model the dynamics of the traffic flow and capture the spatial dependencies. Evaluation results show that the proposed TGC-LSTM outperforms multiple state-of-the-art traffic forecasting baselines. More importantly, the proposed model turns out to be capable of identifying the most influential roadway segments in the realworld traffic networks. The main contributions of our work include:</p>
<ol>
<li>A traffic graph convolution operator is proposed to accommodate physical specialties of traffic networks and extract comprehensive features.</li>
<li>A traffic graph convolutional LSTM neural network is proposed to learn the complex spatial and dynamic temporal dependencies presented in traffic data.</li>
<li>To make learned localized graph convolution features more consistent and interpretable, we proposed two regularization terms, including an L1-norm on traffic the graph convolution weights and an L2-norm on the traffic graph convolution features, that can be optionally added to the model's loss function.</li>
<li>The real-world traffic speed data, including the graph structure of the traffic network, used in this study is published via a publicly available website ${ }^{1}$ to facilitate further research on this problem.</li>
</ol>
<h2>II. Literature REVIEW</h2>
<h2>A. Deep Learning based Traffic Forecasting</h2>
<p>Deep learning models have shown their superior capabilities of capturing nonlinear spatiotemporal effects for traffic forecasting [29]. Ever since the precursory study [30] using the feed-forward NN for vehicle travel time estimation was proposed, many other NN-based models, including fuzzy NN [31], recurrent NN [9], convolution NN [5][18], deep belief networks [10][32], auto-encoders [11][33], generative adversarial networks [34][35], and combinations of these models have been applied to forecast traffic states. With the capability of capturing temporal dependencies, the recurrent NN or its variants, like LSTM [12] and GRU [13], was widely adopted as a component of a traffic forecasting model to forecast traffic speed [8], travel time [36], and traffic flow [37].</p>
<p>Further, in most recent years, various novel deep learningbased traffic forecasting models have been proposed through adjusting classical neural network model, combining existing methods, and incorporating auxiliary data. Multiple novel LSTM based models, such as bidirectional LSTM [14], deep LSTM [15], shared hidden LSTM [38], and nested LSTM [39], have been designed via reorganizing and combing single LSTM models and applied to capture comprehensive temporal dependencies for traffic prediction. In addition, sequence-tosequence (seq2seq) architecture based models [20],[33] have also been used for traffic state sequence forecasting. To deal with different types of features, multi-stream deep learning models [15][40][41][42] have also been well studied and tested for traffic forecasting problems. To improve the prediction performance, multiple deep learning based models also incorporate various traffic-related auxiliary data, including roadway geographical attribute data [33], accident data [15], and weather data [43].</p>
<p>To capture spatial relationships present in traffic networks, many forecasting models [5], [44] incorporating CNNs to extract spatial features from 2D spatial-temporal traffic data. Due to the traffic structure is hard to be depicted by 2D spatialtemporal data, studies [18] tried to convert traffic network structures to images and use CNNs to learn spatial features. However, these converted images have a certain amount of noise, inevitably resulting in spurious spatial relationships captured by CNNs. Recent studies [42][45][46] also attempted to convert traffic state data into three-dimensional (3D) matrices and use the 3D convolutional network to extract more effective features. However, conventional CNN based methods still cannot inherently deal with the topological structure and the physical attributes of the traffic network. To solve this problem, studies [1], [20] attempted to learn the traffic network as a graph and adopt the graph-based convolution operator to extract features from the graph-structured traffic network.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>B. Graph Convolution Networks</h2>
<p>Traffic networks have already been analyzed as graphs for dynamic shortest path routing [47], traffic congestion analysis [48], and dynamic traffic assignment [49]. In the last couple of years, many studies attempt to generalize neural networks to work on arbitrarily structured graphs by designing graph convolutional networks. Generally, the graph convolutional networks utilize the adjacency matrix or the Laplacian matrix to depict the structure of a graph. The Laplacian matrix based graph convolution [22], [26] are designed based on the spectral graph theory [50]. As an extension, a localized spectral graph convolution [23] is also proposed to reduce the learning complexity. The adjacency matrix based graph convolution neural networks [24], [25] incorporate the adjacency matrix and their network structures are more flexible. The traffic network can be considered as a graph consisting of nodes and edges, and thus, several graph convolution neural network based models, including the spectral graph convolution [1] and the diffusion graph convolution [21], are proposed to fulfill network-wide traffic forecasting. Several studies [51][52] also incorporated multi-scale graph convolution operations into their proposed models to learn traffic features. Although these existing methods can extract spatial features from neighborhoods in the traffic network, the physical specialties of roadways, like length, speed limits, and the number of lanes, are normally neglected.</p>
<h2>III. Methodology</h2>
<h2>A. Notions</h2>
<p>1) Traffic Network based Graph</p>
<p>Normally, a graph consists of nodes (vertices) and edges. The graph representing a traffic network is distinct from social network graphs, document citation graphs, or molecule graphs, in several respects: 1) there are no isolated nodes/edges in traffic network based graphs and the traffic network structure seldom changes; 2) the traffic status of each road in a traffic network varies over time; and 3) the roads in a traffic network have meaningful physical characteristics, such as the length, type, speed limit, and lane numbers of a road. Further, traffic state data is collected by different types of sensors such that some types of data detect location-based traffic states, but others may measure road segment based averaged traffic states. Due to traffic states vary over time, it is better to let the graph nodes possess the varying traffic states and keep the graph structure fixed. Thus, to ensure the consistency of the definition in a graph, we use nodes to represent the traffic sensing locations, which can be sensor stations or road segments. Then, the edges in a graph represent the intersections or road segments connecting those traffic sensing locations.</p>
<p>The traffic network and the relationship between traffic locations can be represented by an undirected graph $G$ where $G=(\mathcal{V}, \mathcal{E})$ with $N$ nodes $v_{i} \in \mathcal{V}$ and edges $\left(v_{i}, v_{j}\right) \in \mathcal{E}$. Even though some roads are directed in the reality, due to the impact of traffic congestions occurring on these roads will be bi-
directionally propagated to upstream and downstream roads [14], we take the bidirectional impact into account and thus let $G$ be an undirected graph.
2) Adjacency Matrix and Neighborhood Matrix</p>
<p>The connectedness of nodes in $G$ is represented by an adjacency matrix $A \in \mathbb{R}^{N \times N}$, in which each element $A_{i, j}=1$ if there is an edge connecting node $i$ and node $j$ and $A_{i, j}=0$ otherwise $\left(A_{i, i}=0\right)$. Based on the adjacency matrix, the degree matrix of $G$, which measures the number of edges attached to each vertex, can be defined as $D \in \mathbb{R}^{N \times N}$ in which $D_{i i}=$ $\sum_{j} A_{i j} . D$ is a diagonal matrix and all non-diagonal elements are zeros.</p>
<p>Based on the adjacency matrix, an edge counting function $d\left(v_{i}, v_{j}\right)$ can be defined as counting the minimum number of edges traversed from node $i$ to node $j$. Then, the set of $k$-hop ( $k$-th order) neighborhood of each node $i$, including node $i$ itself, can be defined as $\left{v_{j} \in \mathcal{V} \mid d\left(v_{i}, v_{j}\right) \leq k\right}$. However, since the traffic states are time series data and the current traffic state on a road will definitely influence the future state, we consider the all roads are self-influenced. Thus, we consider the neighborhood of a node contains the node itself and a neighborhood matrix to characterize the one-hop neighborhood relationship of the whole graph, denoted as</p>
<p>$$
\bar{A}=A+I
$$</p>
<p>where $I$ is the identity matrix. Then, the $k$-hop neighborhood relationship of the graph nodes can be characterized by $(A+I)^{k}$. However, some elements in $(A+I)^{k}$ will inevitably exceed one. Owing to the $k$-hop neighborhood of a node is only used for describing the existence of all the $k$-hop neighbors, it is not necessary to make a node's $k$-hop neighbors weighted by the number of hops. Thus, we clip the values of all elements in $(A+I)^{k}$ to be in ${0,1}$ and define a new $k$-hop neighborhood matrix $\bar{A}^{k}$, in which each element $\bar{A}_{i, j}^{k}$ satisfies</p>
<p>$$
\bar{A}<em i_="i," j="j">{i, j}^{k}=\min \left((A+I)</em>, 1\right)
$$}^{k</p>
<p>where min refers to minimum. In this case, $\bar{A}^{1}=A^{1}=A$. An intuitive example of $k$-hop neighborhood with respect to a node (a red star) is illustrated by blue points on the left side of Fig. 1. 3) Free-Flow Reachable Matrix</p>
<p>Based on the length of each road in the traffic network, we define a distance matrix Dist $\in \mathbb{R}^{N \times N}$, where each element Dist $<em i="i" i_="i,">{i, j}$ represents the real roadway distance from node $i$ to $j$ (Dist $</em>=0$ ). When taking the underlying physics of vehicle traffic on a road network into consideration, we need to understand that the impact of a roadway segment on adjacent segments is transmitted in two primary ways: 1) slowdowns and/or blockages propagating upstream; and 2) driver behavior and vehicle characteristics associated with a particular group of vehicles traveling downstream. Thus, for a traffic networkbased graph or other similar graphs, the traffic impact transmission between non-adjacent nodes cannot bypass the intermediate node/nodes, and thus, we need to consider the reachability of the impact between adjacent and nearby node</p>
<p>pairs. To ensure the traffic impact transmission between k-hop adjacent nodes follow the established traffic flow theory [53], we define a free-flow reachable matrix, $\mathcal{F F R} \in \mathbb{R}^{N \times N}$, that</p>
<p>$$
\mathcal{F F} R_{i, j}= \begin{cases}1, S_{i, j}^{\mathcal{F F}} m \Delta t-\text { Dist }<em i="i">{i, j} \geq 0 \ 0, &amp; \text { otherwise }\end{cases}, \forall v</em>
$$}, v_{j} \in \mathcal{V</p>
<p>where $S_{i, j}^{\mathcal{F F}}$ is the free-flow speed between node $i$ and $j$, and free-flow speed [54] refers to the average speed that a motorist would travel if there were no congestion or other adverse conditions (such as severe weather). $\Delta t$ is the duration of time quantum and $m$ is a number counting how many time intervals are considered to calculate the distance travelled under freeflow speed. Thus, $m$ determines the temporal influence of formulating the $\mathcal{F F R}$. Each element $\mathcal{F F} R_{i, j}$ equals one if vehicles can traverse from node $i$ to $j$ in $m$ time-step, $m \cdot \Delta t$, with free-flow speed, and $\mathcal{F F} R_{i, j}=0$ otherwise. Intuitively, the $\mathcal{F F} R_{i, j}$ measures whether a vehicle can travel from node $i$ to node $j$ with the free-flow speed under a specific time interval. We consider each road is self-reachable, and thus, all diagonal values of $\mathcal{F F R}$ are set as one. An example $\mathcal{F F R}$ with respect to a node (a red star) is shown by green lines on the left side of Fig. 1.</p>
<h2>B. Traffic Forecasting Problem</h2>
<p>Traffic forecasting refers to predicting future traffic states, such as traffic speed, travel time, or volume, given previously observed traffic states from a road network. In this study, the traffic network is converted into a graph consisting of all $N$ nodes, representing $N$ traffic sensing locations, and a set of edges. During a period of time $t$, the signals of these nodes representing the collected traffic states, can be denoted as $x_{t} \in$ $\mathbb{R}^{N}$.</p>
<p>To formulate the traffic forecasting problem, the main aforementioned notations are summarized in the following list:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$G$</th>
<th style="text-align: left;">Traffic network-based graph $G=(\mathcal{V}, \mathcal{E})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathcal{V}$</td>
<td style="text-align: left;">Set of vertices in $G$ with the size of $</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{E}$</td>
<td style="text-align: left;">Set of edges in $G$ with the size of $</td>
</tr>
<tr>
<td style="text-align: left;">$A \in \mathbb{R}^{N \times N}$</td>
<td style="text-align: left;">Adjacency matrix of $G$</td>
</tr>
<tr>
<td style="text-align: left;">$D \in \mathbb{R}^{N \times N}$</td>
<td style="text-align: left;">Degree matrix of $G$</td>
</tr>
<tr>
<td style="text-align: left;">$\bar{A} \in \mathbb{R}^{N \times N}$</td>
<td style="text-align: left;">Neighborhood matrix defined by (1)</td>
</tr>
<tr>
<td style="text-align: left;">$\bar{A}^{k} \in \mathbb{R}^{N \times N}$</td>
<td style="text-align: left;">$k$-hop neighborhood matrix defined by (2)</td>
</tr>
<tr>
<td style="text-align: left;">Dist $\in \mathbb{R}^{N \times N}$</td>
<td style="text-align: left;">Distance matrix</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{F F R} \in \mathbb{R}^{N \times N}$</td>
<td style="text-align: left;">Free-flow reachable matrix by (3)</td>
</tr>
<tr>
<td style="text-align: left;">$x_{t} \in \mathbb{R}^{N}$</td>
<td style="text-align: left;">Vector of speed of all graph nodes at time $t$</td>
</tr>
</tbody>
</table>
<p>The short-term traffic forecasting problem aims to learn a function $F(\cdot)$ to map $T$ time steps of historical graph signals, i.e. $\boldsymbol{X}<em 1="1">{T}=\left[x</em>$, and the formulation of $F(\cdot)$ is defined as}, \ldots, x_{t}, \ldots, x_{T}\right]$, to the graph signals in the subsequent one or multiple time steps. In this study, the function attempts to forecast the graph signals in the subsequent one step, i.e. $x_{T+1</p>
<p>$$
F\left(\left[x_{1}, \ldots, x_{t}, \ldots, x_{T}\right] ; G\left(\mathcal{V}, \mathcal{E}, \bar{A}^{k}, \mathcal{F F R}\right)\right)=x_{T+1}
$$</p>
<p>Further, another goal of this study is to learn the traffic impact transmission between adjacent and neighboring nodes in
a traffic network-based graph by learning the weight parameters in the function $F(\cdot)$.</p>
<h2>C. Traffic Graph Convolution</h2>
<p>Previous work [24][25][28] has defined the graph convolution based the adjacency matrix. The core idea of a convolution layer in a neural network is to extract localized features from input data in a 2D or 3D matrices structure. The localized region of the input space which affects the convolution operation results is called receptive field. Analogously, the core idea of a graph convolution layer is to extract localized features from input data in a graph structure. Thus, the product of the neighborhood matrix $\bar{A}$, the input data $x_{t}$, and a trainable weight matrix $W$, i.e. $\bar{A} x_{t} W$, can be considered as a graph convolution operation to extract features from one-hop neighborhood [24][25]. Then, the receptive field of the graph convolution operation on a node is the one-hop neighborhood.</p>
<p>However, in this way, the receptive field is confined, and it only concentrates on one-hop neighboring nodes. To overcome this shortcoming, we extend the receptive field of graph convolution by replacing the one-hop neighborhood matrix $\bar{A}$ with the $k$-hop neighborhood matrix $\bar{A}^{k}$. Meanwhile, existing studies either neglect the properties of the edges in a graph, such as the distances between different sensing locations (the lengths of the graph edges) and the free-flow reachability defined in (3), or fail to consider high-order neighborhood of nodes in the graph. Hence, to comprehensively solve the network-wide forecasting problem, we consider both graph edge properties and high-order neighborhood in the traffic network-based graph. Hence, we define the $k$-order ( $k$-hop) Traffic Graph Convolution (TGC) operation as</p>
<p>$$
G C_{t}^{k}=\left(W_{\theta C_{k}} \odot \bar{A}^{k} \odot \mathcal{F F R}\right) x_{t}
$$</p>
<p>where $\odot$ is the Hadamard product operator, i.e. the elementwise matrix multiplication operator, and $x_{t} \in \mathbb{R}^{N}$ is the vector of traffic states (speed) of all nodes at time $t$. The $W_{\theta C, k} \in$ $\mathbb{R}^{N \times N}$ is a trainable weight matrix for the $k$-order traffic graph convolution and the $G C^{k} \in \mathbb{R}^{N}$ is the extracted $k$-order traffic graph convolution feature. Due to $\bar{A}^{k}$ and $\mathcal{F F R}$ are both sparse matrices only containing 0 and 1 elements, the result of $W_{\theta C_{k}} \odot \bar{A}^{k} \odot \mathcal{F F R}$ is also sparse. Further, the trained weight $W_{\theta C, k}$ has the potential to measure the interactive influence between graph nodes, and thus, enhance the interpretability of the model.</p>
<p>In Equation (5), $k$ should be a positive integer. The larger the order $k$ is, the larger the size of the receptive field of the TGC is, and then the more neighborhood-based features can be extracted from the graph. However, $k$ is not infinite, and it can be easily proved that, for a specific graph, when increasing the value of $k, \bar{A}^{k} \odot \mathcal{F F R}$ will eventually converge to $\mathcal{F F R}$ such that $k=K_{\max }$ and $\bar{A}^{K_{\max }} \odot \mathcal{F F R}=\mathcal{F F R}$. It should be noted that, while extracting traffic graph convolution features to solve real traffic prediction problems, it is not necessary to set $k$ as the max value $K_{\max }$. The trade-off between the prediction</p>
<p>accuracy and the feature richness, which is directly related to the computational cost, should be considered and balanced.</p>
<p>Let $K \leq K_{\max }$ denote the largest hop for traffic graph convolution in this study, and the corresponding traffic graph convolution feature is $G C_{t}^{K}$ with respect to input data $x_{t}$. Different hops of neighborhood in TGC will result in different extracted features. To enrich the feature space, the features extracted from different orders (from 1 to $K$ ) of traffic graph convolution with respect to $X_{t}$ are concatenated together as a vector defined as follows</p>
<p>$$
\boldsymbol{G} \boldsymbol{C}<em t="t">{t}^{(K)}=\left[G C</em>\right]
$$}^{1}, G C_{t}^{2}, \ldots, G C_{t}^{K</p>
<p>The $\boldsymbol{G} \boldsymbol{C}<em t="t">{t}^{(K)} \in \mathbb{R}^{N \times K}$ contains all the $K$ orders of traffic graph convolutional features, as intuitively shown in the left part of Fig. 1. In this study, after operating the TGC on input data $x</em>$ will be fed into the following layer in the proposed neural network structure described in the following section.}$, the generated $\boldsymbol{G} \boldsymbol{C}_{t}^{(K)</p>
<h2>D. Comparing TGC with Spectral Graph Convolution</h2>
<p>The proposed traffic graph convolution is based on adjacency matrix $A$, but the spectral graph convolution (SGC) is defined in the Fourier domain [50] based on the Laplacian matrix $L$, which equals</p>
<p>$$
L=D-A
$$</p>
<p>where $D$ is the degree matrix as introduced in Section III.A.2. The Laplacian matrix $L$ is symmetric positive semi-definite such that it can be diagonalized via eigen-decomposition as</p>
<p>$$
L=U \Lambda U^{T}
$$</p>
<p>where $\Lambda$ is a diagonal matrix containing the eigenvalues, $U$ consists of the eigenvectors, and $U^{T}$ is the transpose of $U$.</p>
<p>The spectral convolution on graph is defined as the multiplication of a signal $x_{t} \in \mathbb{R}^{N}$ with a filter $h_{\theta}=\operatorname{diag}(\theta)$ parameterized by $\theta \in \mathbb{R}^{N}[24]$. The $\operatorname{diag}(\theta)$ is the diagonalized matrix given $\theta$. The spectral graph convolution operation can be described as</p>
<p>$$
h_{\theta} *<em t="t">{\mathcal{G}} x</em>
$$}=U h_{\theta} U^{T} x_{t}=U \operatorname{diag}(\theta) U^{T} x_{t</p>
<p>where $*<em _theta="\theta">{\mathcal{G}}$ is the spectral graph convolution operator. The filter $h</em>$ that can be considered as a learnable convolutional kernel
weight.
Further, for saving computational cost, the localized spectral graph convolution (LSGC) is proposed by employing a polynomial filter $h_{\theta^{\prime}}=\sum_{j=0}^{k-1} \theta_{j}^{\prime} \Lambda^{j}$ [23] and the learnable parameter $\theta^{\prime} \in \mathbb{R}^{K}$. Then $K$-hop localized spectral graph convolution can be formulated as:</p>
<p>$$
h_{\theta^{\prime}} *<em t="t">{\mathcal{G}} x</em>
$$}=U \sum_{j=0}^{K-1} \theta_{j}^{\prime} \Lambda^{j} U^{T} x_{t}=\sum_{j=0}^{K-1} \theta_{j}^{\prime} L^{j} x_{t</p>
<p>The advantages of the LSGC is that it only has $K$ parameters and does not need eigen-decomposition. It is well spatial localized and each convolution operation on a centered vertex extracts the summed weighted feature of the vertex's $K$-hop neighbors. The details of SGC and LSGC can be found in the literature [22][23][24].</p>
<p>The comparison between TGC, SGC, and LSGC in terms of the number of parameters, computational time, and localized feature extraction, is shown in TABLE I. Comparing to SGC and LSGC, the TGC is better in terms of spatial localization because it can extract local features based on physical properties of roadways by incorporating the $\mathcal{F} \mathcal{F} R$. TGC with more parameters has better capabilities of representing the relationships between connected nodes in the graph. Further, SGC and LSGC normally need multiple convolutional layers, which leads the SGC and LSGC to lose their interpretability. However, TGC only needs one convolution layer and its parameters can be better interpreted.</p>
<h2>E. Traffic Graph Convolutional LSTM</h2>
<p>We propose a Traffic Graph Convolutional LSTM (TGCLSTM) recurrent neural network, as shown on the right side of the Fig. 1, which learns both the complex spatial dependencies and the dynamic temporal dependencies presented in traffic data. In this model, the gates structure in the vanilla LSTM [12] and the hidden state are unchanged, but the input is replaced by the graph convolution features, which are reshaped into a vector $\boldsymbol{G} \boldsymbol{C}^{(K)} \in \mathbb{R}^{K N}$. The forget gate $\mathrm{f}<em t="t">{t}$, the input gate $\mathrm{i}</em>}$, the output gate $\mathrm{o<em t="t">{t}$, and the input cell state $\tilde{C}</em>$ in terms of time step $t$ are defined as follows</p>
<p>$$
\mathrm{f}<em g="g">{t}=\sigma</em>}\left(W_{f} \cdot \boldsymbol{G} \boldsymbol{C<em f="f">{t}^{(K)}+U</em>\right)
$$} \cdot h_{t-1}+b_{f</p>
<p>TABLE I
COMPARISON BETWEEN TGC, SGC, AND LSGC</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Graph convolution definition</th>
<th style="text-align: left;">$K$-hop TGC</th>
<th style="text-align: left;">SGC</th>
<th style="text-align: left;">$K$-hop LSGC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Graph convolution on signal $x_{t}$</td>
<td style="text-align: left;">$\left(W_{g c_{k}} \odot \hat{A}^{K} \odot \mathcal{F} \mathcal{F} R\right)$</td>
<td style="text-align: left;">$U \operatorname{diag}(\theta) U^{T}$</td>
<td style="text-align: left;">$\sum_{j=0}^{K-1} \theta_{j}^{\prime} L^{j}$</td>
</tr>
<tr>
<td style="text-align: left;">Weight parameters</td>
<td style="text-align: left;">$W_{g c_{k}} \in \mathbb{R}^{N \times N}$</td>
<td style="text-align: left;">$\theta \in \mathbb{R}^{N}$</td>
<td style="text-align: left;">$\theta^{\prime} \in \mathbb{R}^{K}$</td>
</tr>
<tr>
<td style="text-align: left;">Computational time complexity</td>
<td style="text-align: left;">$\mathrm{O}\left(N^{2}\right)$</td>
<td style="text-align: left;">$\mathrm{O}\left(N^{2}\right)[23]$</td>
<td style="text-align: left;">$\mathrm{O}(K \mid \mathcal{E}])[23]$</td>
</tr>
<tr>
<td style="text-align: left;">Extract Localized features</td>
<td style="text-align: left;">Yes. It is $k$-localized <br> incorporating roadway <br> physical properties.</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes. It is exactly $k$-localized.</td>
</tr>
</tbody>
</table>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The architecture of the proposed Traffic Graph Convolution LSTM is shown on the right side. The traffic graph convolution (TGC) as a component of the proposed model is shown on the left side in detail by unfolding the traffic graph convolution at time <strong>t</strong>, in which <strong>A</strong><sup>R</sup>s and <strong>FFR</strong> with respect to a red star node are demonstrated.</p>
<p>$$
i_t = \sigma_g(W_i \cdot GC_t^{(K)} + U_i \cdot h_{t-1} + b_i) \tag{12}
$$</p>
<p>$$
o_t = \sigma_g(W_o \cdot GC_t^{(K)} + U_o \cdot h_{t-1} + b_o) \tag{13}
$$</p>
<p>$$
C_t = \tanh(W_c \cdot GC_t^{(K)} + U_c \cdot h_{t-1} + b_c) \tag{14}
$$</p>
<p>where · is the matrix multiplication operator. <strong>W</strong><sub>f</sub>, <strong>W</strong><sub>i</sub>, <strong>W</strong><sub>o</sub>, and <strong>W</strong><sub>c</sub> ∈ ℝ<sup>N×KN</sup> are the weight matrices, mapping the input to the three gates and the input cell state, while <strong>U</strong><sub>f</sub>, <strong>U</strong><sub>i</sub>, <strong>U</strong><sub>o</sub>, and <strong>U</strong><sub>c</sub> ∈ ℝ<sup>N×N</sup> are the weight matrices for the preceding hidden state. <strong>b</strong><sub>f</sub>, <strong>b</strong><sub>i</sub>, <strong>b</strong><sub>o</sub>, and <strong>b</strong><sub>c</sub> ∈ ℝ<sup>N</sup> are four bias vectors. The σ<sub>g</sub> is the gate activation function, which typically is the sigmoid function, and tanh is the hyperbolic tangent function.</p>
<p>Due to each node in a traffic network graph is influenced by the preceding states of itself and its neighboring nodes, the LSTM cell state of each node in the graph should also be affected by neighboring cell states. Thus, a cell state gate is designed and added in the LSTM cell. The cell state gate, as shown in Fig. 1, is defined as follows</p>
<p>$$
C_{t-1}^t = W_N \Theta (A^K \Theta FFR) \cdot C_{t-1} \tag{15}
$$</p>
<p>where <strong>W</strong><sub>N</sub> is a weight matrix to measure the contributions of neighboring cell states. To correctly reflect the traffic network structure, the <strong>W</strong><sub>N</sub> is constrained by multiplying a <strong>FFR</strong> based <strong>K</strong>-hop adjacency matrix, <strong>A</strong><sup>K</sup>⊕ <strong>FFR</strong>. With this gate, the influence of neighboring cell states will be considered when the cell state is recurrently input to the subsequent time step. Then, the final cell state and the hidden state are calculated as follows</p>
<p>$$
C_t = f_t \Theta C_{t-1}^t + i_t \Theta C_t \tag{16}
$$</p>
<p>$$
h_t = o_t \Theta \tanh(C_t) \tag{17}
$$</p>
<p>At the final time step <strong>T</strong>, the hidden state <strong>h</strong><sub>T</sub> is the output of TGC-LSTM, namely the predicted value <strong>ŷ</strong><sub>T</sub> = <strong>h</strong><sub>T</sub>. Let <strong>y</strong><sub>T</sub> ∈ ℝ<sup>N</sup> denote the label of the input data <strong>X</strong><sub>T</sub> ∈ ℝ<sup>N×N</sup>. For the sequence prediction problem in this study, the label of time step <strong>T</strong> is the input of the next time step (<strong>T</strong> + 1) such that <strong>y</strong><sub>T</sub> = <strong>x</strong><sub>T+1</sub>. Then the loss during the training process is defined as</p>
<p>$$
\text{Loss} = L(y_T, ŷ<em T_1="T+1">T) = L(x</em>
$$}, h_T) \tag{18</p>
<p>where <strong>L</strong>(·) is a function to calculate the residual between the predicted value <strong>ŷ</strong><sub>T</sub> and the true value <strong>y</strong><sub>T</sub>. Normally, the <strong>L</strong>(·) function is a Mean Squared Error (MSE) function for predicting continuous values.</p>
<p>To explain the proposed method in a clearer way, a pseudo-code of the TGC-LSTM algorithm is presented in Algorithm 1. Given the traffic state data <strong>X</strong><sub>T</sub> and the graph related matrices as input, the pseudo-code mainly describes the process of generating the final output <strong>h</strong><sub>T</sub> after <strong>T</strong> steps of iteration. For simplicity, the pseudo-code does not include the mini-batch gradient descent process and the backpropagation-based parameter updating process. In Algorithm 1, Eq. is short for Equation and the function TGC-LSTM(·) refers to the whole calculation process described in Equation (11-17) in this section.</p>
<h1><em>F. Traffic Graph Convolution Regularization</em></h1>
<p>Since the proposed model contains a traffic graph convolution operation, the generated set of TGC features <strong>GC</strong><sup>K</sup><sub>t</sub> and the learned TGC weights <strong>W</strong><sub>gc1</sub>, <strong>..., W</strong><sub>gcK</sub> provide an opportunity to make the proposed model interpretable via analyzing the learned TGC weights. To confine the graph convolution features within a reasonable scale and make the learned weights more stable and interpretable, we propose two</p>
<p>Algorithm 1 Calculation the output of the TGC-LSTM layer</p>
<p><strong>Inputs:</strong> <strong>X</strong><sub>T</sub> = {<strong>x</strong><sub>1</sub>, <strong>...</strong>, <strong>x</strong><sub>T</sub>}, <strong>A</strong><sup>1</sup>, <strong>..., A</strong><sup>K</sup>}, <strong>FFR</strong></p>
<p><strong>Parameters:</strong> <strong>W</strong><sub>gc1</sub>, <strong>..., W</strong><sub>gcK</sub>}, <strong>Ws</strong>, <strong>Us</strong>, and <strong>bs</strong> in Eq. (11-14)</p>
<p><em>W</em><sub>N</sub> in Eq. (15)</p>
<p><strong>Initialize:</strong> <strong>h</strong><sub>0</sub> = 0 ∈ ℝ<sup>N</sup>, <strong>C</strong><sub>0</sub> = 0 ∈ ℝ<sup>N</sup></p>
<p><strong>for</strong> t = 1 <strong>to</strong> T <strong>do</strong></p>
<p><strong>for</strong> k = 1 <strong>to</strong> K <strong>do</strong></p>
<p><em>GC</em><sub>t</sub><sup>k</sup> ← (<strong>W</strong><sub>gck</sub> ⊗ <strong>A</strong><sup>k</sup>⊗ <strong>FFR</strong>) <strong>x</strong><sub>t</sub></p>
<p><strong>end for</strong></p>
<p><strong>GC</strong><sup>K</sup><sub>t</sub> ← [<strong>GC</strong><sub>t</sub><sup>1</sup>, GC<sub>t</sub><sup>2</sup>, ..., GC<sub>t</sub><sup>K</sup>]</p>
<p><strong>h</strong><sub>t</sub>, <strong>C</strong><sub>t</sub> = TGC-LSTM( <strong>x</strong><sub>t</sub>, <strong>GC</strong><sup>K</sup><sub>t</sub>, <strong>h</strong><sub>t−1</sub>, <strong>C</strong><sub>t−1</sub>)</p>
<p><strong>end for</strong></p>
<p><strong>Return:</strong> <strong>h</strong><sub>T</sub></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. (a) LOOP dataset covering the freeway network in Seattle area; (b) INRIX dataset covering the downtown Seattle area, where traffic segments are plotted with colors.
optional regularization terms that can be added to the loss function described in Equation (18).</p>
<h2>1) Regularization on Graph Convolution weights</h2>
<p>Because the graph convolution weights are not confined to be positive and each node's extracted features are influenced by multiple neighboring nodes, the graph convolution weights can vary a lot while training. Ideally, the convolution weights would be themselves informative, so that the relationships between different nodes in the network could be interpreted and visualized by plotting the convolution weights. This is not likely to be possible without regularization, because very high or low weights tend to appear somewhat randomly, with the result that high/low weights tend to cancel each other out. In combination, such weights can still represent informative features for the network, but they cannot reflect the true relationship between nodes in the graph. Thus, we add L1-norm of the graph convolution weight matrices to the loss function as a regularization term to make these weight matrices as sparse as possible. The L1 regularization term is defined as follows</p>
<p>$$
R^{{1}}=\left|\boldsymbol{W}<em 1="1">{g c}\right|</em>\right|
$$}=\sum_{i=1}^{K}\left|W_{g c_{i}</p>
<p>In this way, the trained graph convolution weight can be sparse and stable, and thus, it will be more intuitive to distinguish which neighboring node or group of nodes contribute most.</p>
<h2>2) Regularization on Graph Convolution features</h2>
<p>Considering that the impact of neighboring nodes with respect to a specific node must be transmitted through all nodes between the node of interest and the influencing node, features extracted from different hops in the graph convolution should not vary dramatically. Thus, to restrict the difference between features extracted from adjacent hops of graph convolution, an L2-norm based TGC feature regularization term is added on the loss function at each time step. The regularization term is defined as follows</p>
<p>$$
R^{{2}}=\left|\boldsymbol{G} \boldsymbol{C}<em 2="2">{T}^{{K}}\right|</em>
$$}=\sqrt{\sum_{i=1}^{K-1}\left(G C_{T}^{i}-G C_{T}^{{+1}}\right)^{2}</p>
<p>In this way, the features extracted from adjacent hops of graph convolution should not differ dramatically, and thus, the graph convolution operator should be more in keeping with the physical realities of the relationships present in a traffic network.</p>
<p>Then, the total loss function at time $t$ can be defined as follows</p>
<p>$$
\operatorname{Loss}=\mathrm{L}\left(h_{T}-x_{T+1}\right)+\lambda_{1} R^{{1}}+\lambda_{2} R^{{2}}
$$</p>
<p>where $\lambda_{1}$ and $\lambda_{2}$ are penalty terms to control the weight magnitude of the regularization terms on graph convolution weights and features.</p>
<h2>IV. EXPERIMENTS</h2>
<h2>A. Dataset Description</h2>
<p>In this study, two real-world network-scale traffic speed datasets are utilized. The first contains data collected from inductive loop detectors deployed on four connected freeways (I-5, I-405, I-90, and SR-520) in the Greater Seattle Area, shown in Fig. 2 (a). This dataset, which is publicly accessible ${ }^{2}$, contains traffic state data from 323 sensor stations over the entirety of 2015 at 5-minute intervals. The second contains road link-level traffic speeds aggregated from GPS probe data collected by commercial vehicle fleets and mobile apps provided by the company INRIX. The INRIX traffic network covers the Seattle downtown area, shown in Fig. 2 (b). This dataset describes the traffic state at 5-minute intervals for 1014 road segments and covers the entire year of 2012. We use LOOP data and INRIX data to denote these two datasets, respectively, in this study.</p>
<p>We adopt the speed limit as the free-flow speed, which for the segments in the LOOP traffic network is 60 mph in all cases. The INRIX traffic network contains freeways, ramps, arterials, and urban corridors, and so the free-flow speeds of INRIX traffic network range from 20 mph to 60 mph . The distance adjacency matrices Dist and free-flow reachable matrices $\mathcal{F} \mathcal{F} R$ for both datasets are calculated based on the roadway characteristics and topology.</p>
<h2>B. Experimental Settings</h2>
<h2>1) Baselines</h2>
<p>We compare TGC-LSTM with the following baseline models: (1) ARIMA: Auto-Regressive Integrated Moving Average model [7]; (2) SVR: Support Vector Regression [6]; (3) FNN: Feed forward neural network with two hidden layers, i.e. the multilayer perceptron, whose hidden layer size is N; (4) LSTM: Long Short-Term Memory recurrent neural network [12]; (5) DiffGRU [20]: an adjusted version of diffusion convolutional gated recurrent network [20] whose gate units are defined based on diffusion convolution. Since the graph is undirected in this study, we replace the diffusion convolution with spectral graph convolution in DiffGRU; (6) Conv+LSTM: a one-dimensional (1D) convolution layer with two channels followed by an</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>TABLE II
PERFORMANCE COMPARISON OF DIFFERENT APPROACHES. (THE NUMBER OF HOPS K IS SET AS 3 IN THE GRAPH CONVOLUTION RELATED MODEL)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">LOOPf Data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">INRIX Data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE <br> (mph) $\pm$ STD</td>
<td style="text-align: center;">MAPE</td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">MAE <br> (mph) $\pm$ STD</td>
<td style="text-align: center;">MAPE</td>
<td style="text-align: center;">RMSE</td>
</tr>
<tr>
<td style="text-align: center;">ARIMA</td>
<td style="text-align: center;">$6.10 \pm 1.09$</td>
<td style="text-align: center;">$13.85 \%$</td>
<td style="text-align: center;">10.65</td>
<td style="text-align: center;">$4.80 \pm 0.32$</td>
<td style="text-align: center;">$13.51 \%$</td>
<td style="text-align: center;">10.85</td>
</tr>
<tr>
<td style="text-align: center;">SVR</td>
<td style="text-align: center;">$6.85 \pm 1.17$</td>
<td style="text-align: center;">$14.39 \%$</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">$4.78 \pm 0.37$</td>
<td style="text-align: center;">$13.37 \%$</td>
<td style="text-align: center;">10.44</td>
</tr>
<tr>
<td style="text-align: center;">FNN</td>
<td style="text-align: center;">$4.45 \pm 0.81$</td>
<td style="text-align: center;">$10.19 \%$</td>
<td style="text-align: center;">7.83</td>
<td style="text-align: center;">$2.31 \pm 0.17$</td>
<td style="text-align: center;">$8.35 \%$</td>
<td style="text-align: center;">5.92</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">$2.70 \pm 0.18$</td>
<td style="text-align: center;">$6.83 \%$</td>
<td style="text-align: center;">4.97</td>
<td style="text-align: center;">$1.14 \pm 0.09$</td>
<td style="text-align: center;">$3.88 \%$</td>
<td style="text-align: center;">2.43</td>
</tr>
<tr>
<td style="text-align: center;">DiffGRU</td>
<td style="text-align: center;">$4.64 \pm 0.38$</td>
<td style="text-align: center;">$11.18 \%$</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">$2.44 \pm 0.09$</td>
<td style="text-align: center;">$8.91 \%$</td>
<td style="text-align: center;">6.34</td>
</tr>
<tr>
<td style="text-align: center;">Conv+LSTM</td>
<td style="text-align: center;">$2.71 \pm 0.12$</td>
<td style="text-align: center;">$6.79 \%$</td>
<td style="text-align: center;">5.02</td>
<td style="text-align: center;">$1.13 \pm 0.08$</td>
<td style="text-align: center;">$3.80 \%$</td>
<td style="text-align: center;">2.37</td>
</tr>
<tr>
<td style="text-align: center;">LSGC+LSTM</td>
<td style="text-align: center;">$3.16 \pm 0.23$</td>
<td style="text-align: center;">$7.51 \%$</td>
<td style="text-align: center;">6.18</td>
<td style="text-align: center;">$1.38 \pm 0.12$</td>
<td style="text-align: center;">$4.54 \%$</td>
<td style="text-align: center;">2.82</td>
</tr>
<tr>
<td style="text-align: center;">SGC+LSTM</td>
<td style="text-align: center;">$2.64 \pm 0.12$</td>
<td style="text-align: center;">$6.52 \%$</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">$1.07 \pm 0.08$</td>
<td style="text-align: center;">$3.74 \%$</td>
<td style="text-align: center;">2.28</td>
</tr>
<tr>
<td style="text-align: center;">TGC-LSTM</td>
<td style="text-align: center;">$2.57 \pm 0.10$</td>
<td style="text-align: center;">6.01\%</td>
<td style="text-align: center;">4.63</td>
<td style="text-align: center;">$1.02 \pm 0.07$</td>
<td style="text-align: center;">3.28\%</td>
<td style="text-align: center;">2.18</td>
</tr>
</tbody>
</table>
<p>LSTM layer, the 1D CNN is conducted on $x_{t}$ with two output channels (kernel size=5 and stride=2); (7) SGC+LSTM: stacking a one-layer spectral graph convolution layer [26] with an LSTM layer; (8) LSGC+LSTM: stacking a one-layer localized spectral graph convolution layer [23] whose $K=3$ and an LSTM layer. All the LSTM/GRU layers have the same weight dimensions. The baseline models do not include autoencoder based models and pure CNN based models, due to the core ideas of these methodologies are totally different from the tested baseline models which are mostly single RNN layerbased models. All the neural networks are implemented based on PyTorch 1.0.1 and they are trained and evaluated on a single NVIDIA GeForce GTX 1080 Ti with 11GB memory.</p>
<h2>2) TGC-LSTM Model</h2>
<p>For both datasets, the dimensions of the hidden states of the TGC-LSTM are set as the amount of the nodes in the traffic network graphs. The size of hops in the graph convolution can vary, but we set it as $3, K=3$, for the model evaluation and comparison in this experiment. In this case, the $\mathcal{F} \mathcal{F} R$ is calculated based on three time steps. The two regularization terms ( $R^{[1]}$ and $R^{[2]}$ ) can not only confine the learnt graph convolution weights, they also can avoid overfitting causing the decrease of the prediction accuracy. Thus, there is a trade-off between the prediction accuracy and the scale of the penalty terms ( $\lambda_{1}$ and $\lambda_{2}$ ). Based on empirically adjusting the regularization rates, the values of the $\lambda_{1}$ and $\lambda_{2}$ are both set as 0.01 . We train our model by minimizing the mean square error with the batch size of 10 and the initial learning rate of $10^{-5}$. Since the RMSProp [55] can solve the gradient exploding and vanishing problems, it is used as the gradient descent optimizer whose alpha (smoothing constant) is set as 0.99 and epsilon (the term added to the denominator to improve numerical stability) is set as $10^{-8}$.</p>
<h2>3) Evaluation</h2>
<p>In this study, the samples of the input are traffic time series data with 10 time steps. The output/label is the next subsequent data of the input sequence. The performance of the proposed and the compared models are evaluated by three commonly used metrics in traffic forecasting, including 1) Mean Absolute Error
(MAE), 2) Mean Absolute Percentage Error (MAPE), and 3) Root Mean Squared Error (RMSE).</p>
<p>$$
\begin{gathered}
M A E=\frac{1}{n} \sum_{i=1}^{n}\left|y_{T}-\hat{y}<em i="1">{T}\right| \
M A P E=\frac{1}{n} \sum</em>}^{n}\left|\frac{y_{T}-\hat{y<em T="T">{T}}{y</em>\right| * 100 \% \
R M S E=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(y_{T}-\hat{y}_{T}\right)^{2}}
\end{gathered}
$$}</p>
<h2>C. Experimental Results</h2>
<p>TABLE II demonstrates the results of the TGC-LSTM and other baseline models on the two datasets. The proposed method outperforms other models with all the three metrics on the two datasets. The ARIMA and SVR cannot compete with other methods, which suggest that non-neural-network approaches are less appropriate for this network-wide prediction task, due to the complex spatiotemporal dependencies and the high dimension features in the datasets. The basic FNN does not perform well on predicting spatialtemporal sequence. The DiffGRU performs nearly the same as
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Histogram of performance comparison for the influence of orders (hops) of graph convolution in the TGC-LSTM on INRIX and LOOP datasets.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. (a) Validation loss versus training epoch (batch size = 40 and early stopping patience = 10 epochs). (b) Histogram of model's training time per epochs. (c) Compare training efficiency with different <em>K</em> hops of TGC: training loss versus training iteration (batch size = 40). (The figures are generated based on the LOOP data.)</p>
<p>The FNN. The reason might be that GRU has the no cell state to store historical information in its gate units comparing to LSTM. This can reduce the prediction capability of DiffGRU. Both LSTM and Conv+LSTM work well and they have similar performance. The SGC+LSTM performs better than vanilla LSTM, which demonstrates the feature extraction by using spectral graph convolution is beneficial for traffic forecasting. However, the LSGC+LSTM does not outperform LSTM resulting from utilizing one-layer LSGC, whose parameters are not enough for representing the network features. The proposed TGC-LSTM, which captures graph-based features while accommodating the physical specialties of traffic networks, performs better than all other approaches. It should be noted that, for the INRIX data, during the nighttime or off-peak hours when there are no observed speed values on specific roads, the missing speed values are comprehensively imputed by the data provider. Thus, there are few variations at the non-peak hours in the INRIX data. Further, the speed values in the INRIX data are all integers. Therefore, the calculated errors of the INRIX data are less than that of the LOOP data and the evaluated performance on INRIX data is inflated somewhat.</p>
<p>Fig. 3 shows a histogram of performance comparison on the effects of orders (hops) of the graph convolution in the TGCLSTM. The model performance is improved when the value of <em>K</em> increases. For the LOOP data, the performance improves slightly when <em>K</em> is gradually increased. But for the INRIX data, there is a big improvement in when <em>K</em> increases to two from one. The complex structure and the various road types in the INRIX traffic network could be the main reason for this performing difference. Further, when <em>K</em> is larger than two, the improvement of the prediction is quite limited. This is also the reason why we choose <em>K</em>=3 in the model comparison part, as shown in TABLE II.</p>
<h4><em>D. Training Efficiency</em></h4>
<p>In this subsection, we compare the training efficiency of the proposed model and other LSTM-based models. Fig. 4 (a) shows the validation loss curves versus the training epoch. Due to the early stopping mechanism used in the training process, the numbers of training epochs are different. The TGC-LSTM needs less epochs to converge than the SGC+LSTM and the LSGC+LSTM. In addition, the loss of the TGC-LSTM decreases fastest among the compared models. Fig. 4 (b) shows the comparison of the training time per epoch of different models. The training cost of Conv+LSTM is between that of LSTM and SGC+LSTM. TGC-LSTM costs twice as much as LSTM does. The time required for SGC+LSTM is less than that for TGC-LSTM, while LSGC+LSTM costs slightly more than TGC-LSTM. Fig. 4 (c) shows the training losses of TGC-LSTM with different hops of graph convolution components. The rate of convergence increases when increasing the number of hops, <em>k</em>. In our experiments, when <em>k</em> is larger than 3, the training and validation results improve only marginally for both INRIX and LOOP datasets.</p>
<h4><em>E. Effect of Regularization</em></h4>
<p>The model's loss function can add regularization terms to avoid overfitting. The proposed L1-norm on the graph convolution weights and L2-norm on the graph convolutional features can further help the model to confine the learned weights and features. However, there is a trade-off between the prediction accuracy and the scale of the penalty terms (λ<sub>1</sub> and λ<sub>2</sub>). As tested, by adding the regularization terms to the loss function with the penalty rates setting as 0.01, the MAEs of the proposed model tested on the two datasets increase around 0.02, which are still superior to baseline models. Meanwhile, the TGC weight sparsity is increased and the value of the feature regularization <em>R</em><sup>[2]</sup> is lower than that of the proposed model without regularization terms in the loss function, which means the TGC features' consistency is enhanced. Thus, it is worth adding these regularization terms to the loss function to help the trained model to be more interpretable. Fig. 5 (a) and (b) show portions of the averaged graph convolution weight matrices for the INRIX data and the LOOP data, respectively, where <em>K</em> = 3 and the average weight is calculated by $$\frac{1}{K} \sum_{i=1}^{K} W_i \bigodot \tilde{A}^i \bigodot \mathcal{F}\mathcal{F} R.$$</p>
<p>The road segment names, which are not displayed, are aligned on the vertical and horizontal axes with the same order in each figure. The colored dots in the matrices in Fig. 5 (a) and (b) illustrate the weight of the contribution of a single node to its neighboring nodes. Since we align the traffic states of roadway segments based on their interconnectedness in the training data, most of the weights are distributed around the diagonal line of the weight matrix. The INRIX network is more complex and</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. (a) Visualization of a proportion of the INRIX GC weight matrix, in which three representative weight areas are tagged. (b) Visualization of a proportion of the LOOP GC weight matrix, in which four representative weight areas are tagged. (c) Visualization of the INRIX graph convolution weight on the real traffic network using colored lines. (d) Visualization of the four tagged weight areas in the LOOP graph convolution weight on the Seattle freeway network using colorful circles.</p>
<p>The average degree of nodes in the INRIX graph is higher than that in the LOOP graph. Hence, the dots in the average weight matrix of the INRIX graph convolution are more scattered. But these dots still form multiple clusters demonstrating the weights of several nearby or connected road segments. Considering roadway segments are influenced by their neighboring or nearby connected segments, the nodes with the large absolute weight in a cluster are very likely to be key road segments in the local traffic network. In this way, we can infer the bottlenecks of the traffic network from the traffic graph convolution weight matrices.</p>
<h3><em>F. Model Interpretation and Visualization</em></h3>
<p>To better understand the contribution of the graph convolution weight, we mark seven groups of representative weights in Fig. 5 (a) and (b) and visualize their physical locations on the real map in Fig. 5 (c) and (d), by highlighting them with Roman numerals and red boxes. The influence of these marked weights on neighboring nodes in the INRIX and LOOP data are visualized by lines and circles, respectively, considering the INRIX traffic network is too dense to use circles. The darkness of the green and pink colors and the sizes of the circles represent the magnitude of influence. It should be noted that the darkness of colors on lines on the INRIX map and the size of the circles on the LOOP map will change when the model is trained with different scales of regularization terms (λ₁ and λ₂).</p>
<p>From Fig. 5 (c), we can find the marked areas with dark colors in the INRIX GC weight matrix, (I), (II), and (III), are all located at very busy and congested freeway entrance and exit ramps in Seattle downtown area. In Fig. 5 (d), the area tagged with (IV) is quite representative because the two groups of circles are located at the intersections between freeways and two main corridors that represent the entrances to an island (Mercer Island). Areas (V) and (VI) are the intersections between I-90 and I-405 and between I-5 and SR-520, respectively. The VII area located on SR-520 contains a frequent-congested ramp connecting to the city of Bellevue, the location of which is highlighted by the biggest green circle. Additionally, there are many other representative areas in the graph convolution weight matrix, but we cannot show all of them due to space limits. By comparing the weight matrix with</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Traffic time series forecasting visualization for LOOP and INRIX datasets on two randomly selected days.
the physical realities of the traffic network, it can be shown that the proposed method effectively captures spatial dependencies and helps to identify the most influential points/segments in the traffic network.</p>
<p>Fig. 6 visualizes the predicted traffic speed sequences and the ground truth of two locations selected from the LOOP and INRIX dataset. Though the traffic networks of the two datasets are very different, the curves demonstrate that the trends of the traffic speed are predicted well at both peak traffic and off-peak hours.</p>
<h2>V. CONCLUSION</h2>
<p>In this paper, we learn the traffic network as a graph and define a traffic graph convolution operation to capture spatial features from the traffic network. The traffic graph convolution incorporates the adjacency matrix and the proposed free-flow reachable matrix to extract localized features from the graph. We propose a traffic graph convolutional LSTM neural network to forecast network-wide traffic states. We also design two regularization terms on the TGC weights and TGC features, respectively, that can be added to the model's loss function to help the learned TGC weight to be more stable and interpretable. By evaluating on two real-world traffic datasets, our approach is proved to be superior to the compared baseline models. In addition, the learned TGC weight can help to identify the most influential roadways, and thus, enhance the interpretability of the proposed model.</p>
<p>For future work, we will move forward to improve the model's prediction performance in terms of accuracy and robustness, and further investigate how to conduct the convolution on both spatial and temporal dimensions to make the neural network more interpretable.</p>
<h2>REFERENCES</h2>
<p>[1] B. Yu, H. Yin, and Z. Zhu, "Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting," arXiv Prepr.
arXiv1709.04875, 2017.
[2] D. Park and L. R. Rilett, "Forecasting freeway link travel times with a multilayer feedforward neural network," Comput. Civ. Infrastruct. Eng., vol. 14, no. 5, pp. 357-367, 1999.
[3] E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, "Short-term traffic forecasting: Where we are and where we're going," Transp. Res. Part C Emerg. Technol., vol. 43, pp. 3-19, Jun. 2014.
[4] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, "Long short-term memory neural network for traffic speed prediction using remote microwave sensor data," Transp. Res. Part C Emerg. Technol., vol. 54, pp. 187-197, May 2015.
[5] X. Ma, Z. Dai, Z. He, J. Ma, Y. Wang, and Y. Wang, "Learning traffic as images: a deep convolutional neural network for large-scale transportation network speed prediction," Sensors, vol. 17, no. 4, p. 818, 2017.
[6] A. J. Smola and B. Schölkopf, "A tutorial on support vector regression," Stat. Comput., vol. 14, no. 3, pp. 199-222, 2004.
[7] M. M. Hamed, H. R. Al-Masaeid, and Z. M. B. Said, "Short-term prediction of traffic volume in urban arterials," J. Transp. Eng., vol. 121, no. 3, pp. 249254, 1995.
[8] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, "Long short-term memory neural network for traffic speed prediction using remote microwave sensor data," Transp. Res. Part C Emerg. Technol., vol. 54, pp. 187-197, 2015.
[9] J. Van Lint, S. Hoogendoorn, and H. Van Zuylen, "Freeway travel time prediction with state-space neural networks: modeling state-space dynamics with recurrent neural networks," Transp. Res. Rec. J. Transp. Res. Board, no. 1811, pp. 30-39, 2002.
[10] W. Huang, G. Song, H. Hong, and K. Xie, "Deep Architecture for Traffic Flow Prediction: Deep Belief Networks With Multitask Learning.," IEEE Trans. Intell. Transp. Syst., vol. 15, no. 5, pp. 2191-2201, 2014.
[11] Y. Lv, Y. Duan, W. Kang, Z. Li, F.-Y. Wang, and others, "Traffic flow prediction with big data: A deep learning approach.," IEEE Trans. Intell. Transp. Syst., vol. 16, no. 2, pp. 865-873, 2015.
[12] S. Hochreiter and J. Schmidhuber, "Long Short-Term Memory," Neural Comput., vol. 9, no. 8, pp. 17351780, Nov. 1997.
[13] K. Cho et al., "Learning phrase representations using RNN encoder-decoder for statistical machine translation," arXiv Prepr. arXiv1406.1078, 2014.
[14] Z. Cui, R. Ke, and Y. Wang, "Deep Stacked Bidirectional and Unidirectional LSTM Recurrent Neural Network for Network-wide Traffic Speed Prediction," in 6th International Workshop on Urban Computing (UrbComp 2017), 2016.
[15] R. Yu, Y. Li, C. Shahabi, U. Demiryurek, and Y. Liu, "Deep learning: A generic approach for extreme condition traffic forecasting," in Proceedings of the 2017 SIAM International Conference on Data Mining,</p>
<p>2017, pp. 777-785.
[16] F. Kong, J. Li, B. Jiang, T. Zhang, and H. Song, "Big data-driven machine learning-enabled traffic flow prediction," Trans. Emerg. Telecommun. Technol., p. e3482.
[17] J. Zhang, Y. Zheng, and D. Qi, "Deep SpatioTemporal Residual Networks for Citywide Crowd Flows Prediction.," in AAAI, 2017, pp. 1655-1661.
[18] H. Yu, Z. Wu, S. Wang, Y. Wang, and X. Ma, "Spatiotemporal recurrent convolutional networks for traffic prediction in transportation networks," Sensors, vol. 17, no. 7, p. 1501, 2017.
[19] B. Yu, M. Li, J. Zhang, and Z. Zhu, "3D Graph Convolutional Networks with Temporal Graphs: A Spatial Information Free Framework For Traffic Forecasting," Mar. 2019.
[20] Y. Li, R. Yu, C. Shahabi, and Y. Liu, "Diffusion Convolutional Recurrent Neural Network: DataDriven Traffic Forecasting," in International Conference on Learning Representations (ICLR '18), 2018.
[21] J. Atwood and D. Towsley, "Diffusion-convolutional neural networks," in Advances in Neural Information Processing Systems, 2016, pp. 1993-2001.
[22] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, "Spectral Networks and Locally Connected Networks on Graphs," arXiv Prepr. arXiv1312.6203, Dec. 2013.
[23] M. Defferrard, X. Bresson, and P. Vandergheynst, "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering," in Advances in Neural Information Processing Systems, 2016, pp. 38443852.
[24] T. N. Kipf and M. Welling, "Semi-supervised classification with graph convolutional networks," in International Conference on Learning Representations (ICLR '17), 2017.
[25] Z. Zhou and X. Li, "Convolution on Graph: A HighOrder and Adaptive Approach," arXiv Prepr. arXiv1706.09916, 2018.
[26] M. Henaff, J. Bruna, and Y. LeCun, "Deep Convolutional Networks on Graph-Structured Data," arXiv Prepr. arXiv1506.05163, Jun. 2015.
[27] M. Simonovsky and N. Komodakis, "Dynamic edgeconditioned filters in convolutional neural networks on graphs," in Proc. CVPR, 2017.
[28] S. Abu-El-Haija et al., "MixHop: Higher-Order Graph Convolution Architectures via Sparsified Neighborhood Mixing," Int. Conf. Mach. Learn., 2019.
[29] N. G. Polson and V. O. Sokolov, "Deep learning for short-term traffic flow prediction," Transp. Res. Part C Emerg. Technol., vol. 79, pp. 1-17, 2017.
[30] J. Hua and A. Faghri, "Applcations of Artificial Neural Networks to Intelligent Vehicle-Highway Systems," Record (Washington)., vol. 1453, p. 83, 1994.
[31] H. Yin, S. Wong, J. Xu, and C. K. Wong, "Urban traffic flow prediction using a fuzzy-neural approach," Transp. Res. Part C Emerg. Technol., vol. 10, no. 2, pp. 85-98, 2002.
[32] F. Kong, J. Li, B. Jiang, and H. Song, "Short-term traffic flow prediction in smart multimedia system for Internet of Vehicles based on deep belief network," Futur. Gener. Comput. Syst., vol. 93, pp. 460-472, 2019.
[33] B. Liao et al., "Deep Sequence Learning with Auxiliary Information for Traffic Prediction," in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 2018, pp. 537-546.
[34] Y. Liang, Z. Cui, Y. Tian, H. Chen, and Y. Wang, "A Deep Generative Adversarial Architecture for Network-Wide Spatial-Temporal Traffic-State Estimation," Transp. Res. Rec. J. Transp. Res. Board, p. 036119811879873, Oct. 2018.
[35] Y. Lin, X. Dai, L. Li, and F.-Y. Wang, "Pattern Sensitive Prediction of Traffic Flow Based on Generative Adversarial Framework," IEEE Trans. Intell. Transp. Syst., no. 99, pp. 1-6, 2018.
[36] Y. Duan, Y. Lv, and F.-Y. Wang, "Travel time prediction with LSTM neural network," in Intelligent Transportation Systems (ITSC), 2016 IEEE 19th International Conference on, 2016, pp. 1053-1058.
[37] Z. Zhao, W. Chen, X. Wu, P. C. Y. Chen, and J. Liu, "LSTM network: a deep learning approach for shortterm traffic forecast," IET Intell. Transp. Syst., vol. 11, no. 2, pp. 68-75, 2017.
[38] X. Song, H. Kanasugi, and R. Shibasaki, "DeepTransport: Prediction and Simulation of Human Mobility and Transportation Mode at a Citywide Level.," in IJCAI, 2016, pp. 2618-2624.
[39] X. Ma, Y. Li, Z. Cui, and Y. Wang, "Forecasting Transportation Network Speed Using Deep Capsule Networks with Nested LSTM Models," arXiv Prepr. arXiv1811.04745, 2018.
[40] Y. Wu, H. Tan, L. Qin, B. Ran, and Z. Jiang, "A hybrid deep learning based traffic flow prediction method and its understanding," Transp. Res. Part C Emerg. Technol., vol. 90, pp. 166-180, May 2018.
[41] R. Ke, W. Li, Z. Cui, and Y. Wang, "Two-Stream Multi-Channel Convolutional Neural Network (TMCNN) for Multi-Lane Traffic Speed Prediction Considering Traffic Volume Impact," arXiv Prepr. arXiv1903.01678, 2019.
[42] C. Zhang and P. Patras, "Long-term mobile traffic forecasting using deep spatio-temporal neural networks," in Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing, 2018, pp. 231-240.
[43] D. Zhang and M. R. Kabuka, "Combining weather condition data to predict traffic flow: a GRU-based deep learning approach," IET Intell. Transp. Syst., vol. 12, no. 7, pp. 578-585, 2018.
[44] W. Jin, Y. Lin, Z. Wu, and H. Wan, "Spatio-Temporal Recurrent Convolutional Networks for Citywide Short-term Crowd Flows Prediction," in Proceedings of the 2nd International Conference on Compute and Data Analysis, 2018, pp. 28-35.
[45] C. Chen et al., "Exploiting Spatio-Temporal Correlations with Multiple 3D Convolutional Neural</p>
<p>Networks for Citywide Vehicle Flow Prediction," in 2018 IEEE International Conference on Data Mining (ICDM), 2018, pp. 893-898.
[46] S. Guo, Y. Lin, S. Li, Z. Chen, and H. Wan, "Deep Spatial-Temporal 3D Convolutional Neural Networks for Traffic Data Forecasting," IEEE Trans. Intell. Transp. Syst., 2019.
[47] Y. Sun, X. Yu, R. Bie, and H. Song, "Discovering time-dependent shortest path on traffic graph for drivers towards green driving," J. Netw. Comput. Appl., vol. 83, pp. 204-212, 2017.
[48] H. Sun, J. Wu, D. Ma, and J. Long, "Spatial distribution complexities of traffic congestion and bottlenecks in different network topologies," Appl. Math. Model., vol. 38, no. 2, pp. 496-505, 2014.
[49] G. Kalafatas and S. Peeta, "An exact graph structure for dynamic traffic assignment: Formulation, properties, and computational experience," 2007.
[50] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains," IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83-98, 2013.
[51] X. Geng et al., "Spatiotemporal multi-graph convolution network for ride-hailing demand forecasting," in 2019 AAAI Conference on Artificial Intelligence (AAAI'19), 2019.
[52] Q. Zhang, Q. Jin, J. Chang, S. Xiang, and C. Pan, "Kernel-Weighted Graph Convolutional Network: A Deep Learning Approach for Traffic Forecasting," in 2018 24th International Conference on Pattern Recognition (ICPR), 2018, pp. 1018-1023.
[53] C. F. Daganzo, "The cell transmission model: A dynamic representation of highway traffic consistent with the hydrodynamic theory," Transp. Res. Part B Methodol., vol. 28, no. 4, pp. 269-287, 1994.
[54] K. Hunter-Zaworski et al., "Transportation engineering online lab manual." 2003.
[55] T. Tieleman and G. Hinton, "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude," COURSERA Neural networks Mach. Learn., vol. 4, no. 2, pp. 26-31, 2012.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/zhiyongc/Seattle-Loop-Data&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>K. Henrickson is with the INRIX, Inc., Kirkland WA 98033, USA (e-mail: Kristian.Henrickson@inrix.com)&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
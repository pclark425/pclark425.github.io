<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1717 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1717</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1717</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-258352761</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.14317v2.pdf" target="_blank">ICE-Score: Instructing Large Language Models to Evaluate Code</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments. Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our metric on two different aspects (human preference and execution success) and four programming languages. Our results demonstrate that our metric surpasses state-of-the-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation metric and datasets available to the public, encouraging further research in evaluating code intelligence tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1717.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1717.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICE-Score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ICE-Score (Instructing Large Language Models to Evaluate Code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reference-free, instruction-prompted LLM-based automatic metric that instructs LLMs to rate generated code on multidimensional criteria (usefulness and execution-based correctness) using a 0–4 rubric; evaluated primarily with GPT-3.5 and ablated with GPT-4 and chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (reference-free and reference-enhanced variants), compared to automated metrics (BLEU, ROUGE-L, METEOR, CodeBLEU, RUBY, CodeBERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5-turbo (primary); GPT-4 (ablation/comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Single-rating with explicit rubric/template-based steps (assign 0–4 Likert-style score for a specified evaluation aspect); optionally augmented with Zero-Shot Chain-of-Thought (step-by-step evaluation) and a rule-based parser to extract scores</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated code snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python (CoNaLa human-preference experiment); multiple languages including Python, Java, C, C++, JavaScript (HumanEval-X functional-correctness experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human-rated usefulness (0–4 Likert-style rubric) and execution-based functional correctness (0–4 rubric guided by unit-test pass/fail and logic checks)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Experienced software developers graded generated code (CoNaLa) on a 0–4 usefulness scale; for functional correctness experiments, automated execution-based ground truth (HumanEval / HumanEval-X unit tests and reference solutions) was used as the reference</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>4.5 (average annotators per snippet on CoNaLa)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>experienced software developers (human annotators for CoNaLa)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's Tau (τ), Pearson correlation (r_p), Spearman correlation (r_s)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Against human usefulness (CoNaLa), ICE-Score reported: example-level Kendall τ = 0.556, Pearson r = 0.613, Spearman r_s = 0.594; corpus-level Kendall τ = 0.546, Pearson r = 0.649, Spearman r_s = 0.635. (Functional-correctness correlations versus execution-based ground truth were reported in Tables 2/3; ICE-Score/Ref-ICE-Score qualitatively outperform baselines across languages, exact numeric values for those tables are reported in the paper but not reproduced verbatim in the text excerpt.)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Clear, task-agnostic evaluation criteria and template prompts (explicit rubric); use of chain-of-thought prompting and reasoning; more capable backbone LLM (GPT-4) improved alignment; reference-free evaluation already strongly aligned when references are low-quality.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Poor or biased reference code (reference-enhanced variant sometimes offers no improvement or can introduce bias); limited/opaque training data (possible contamination) is a concern; reliance on low-quality references reduced benefit of reference-enhanced scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not explicitly quantified numerically; authors note code evaluation requires deeper programming understanding and imply harder/complex programming concepts make alignment more challenging (i.e., LLMs may perform less reliably on more complex code), but no direct per-complexity breakdown is reported in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clarity and specificity of the rubric + template improved alignment; adding Zero-Shot Chain-of-Thought (step-by-step evaluation) increased correlation with human judgments (see CoT ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>CoNaLa: 2,860 annotated code snippets (5 generations × 472 examples, average 4.5 annotators per snippet). HumanEval-X: authors sampled 20 generated samples per problem (exact aggregated total for the multilingual subset not specified in the excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>LLM-human agreement reported (see correlations above) demonstrating ICE-Score outperforms traditional automatic metrics and achieves substantial correlation with human preference; direct comparison of LLM-human agreement to inter-human agreement is not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No fine-tuning against the human labels reported; ICE-Score uses instruction prompts (template and explicit criteria). For stability they set model temperature to 0 and used rule-based score extraction rather than training on human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICE-Score (LLM-as-a-judge) achieves substantially higher correlation with human usefulness judgments than traditional string-based and several neural baselines (e.g., BLEU, ROUGE-L, CodeBERTScore variants). Reference-free ICE-Score already performs strongly; Ref-ICE-Score (with reference code) does not reliably improve performance when references are imperfect. Augmenting prompts with Zero-Shot Chain-of-Thought and using a stronger backbone (GPT-4) further increases agreement with human ratings. For execution-based correctness, ICE-Score variants outperform or match state-of-the-art automatic metrics across multiple languages (example-level and corpus-level results reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Possible data contamination for some datasets (CoNaLa, HumanEval Python) given closed-source LLM training data; dependence on closed-source/high-cost LLM APIs; reference code quality can bias reference-enhanced evaluation; score extraction for CoT is rule-based and may not be optimal; inter-human agreement statistics are not reported (preventing direct comparison to human–human reliability).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ICE-Score: Instructing Large Language Models to Evaluate Code', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1717.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1717.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZS-CoT evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Chain-of-Thought (ZS-CoT) evaluation prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that appends a step-by-step evaluation instruction to the LLM prompt so the model performs a sequence of reasoning steps before emitting a final rubric score; implemented here with a rule-based parser to extract the numeric score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge with chain-of-thought reasoning (step-by-step reasoning in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5-turbo (ZS-CoT experiments run on CoNaLa; authors note GPT-4 also evaluated elsewhere but ZS-CoT ablation used GPT-3.5 due to resources)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Zero-shot Chain-of-Thought: add an instruction like 'Step-by-step Evaluation:' prompting the model to produce intermediate reasoning and then a final score; scores extracted via a rule-based parser</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated code snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python (CoNaLa experiment where ZS-CoT ablation was performed)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Usefulness (0–4 rubric) and optionally functional correctness following the same rubric template</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same CoNaLa human annotation setup (experienced developers rated usefulness on 0–4; used as human ground truth for correlation). ZS-CoT evaluation compared LLM outputs to these human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>4.5 (average annotators per snippet on CoNaLa)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>experienced software developers</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's Tau (τ), Pearson (r_p), Spearman (r_s)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>ZS-CoT (CoT-ICE-Score) reported improved correlations vs non-CoT: example-level Kendall τ = 0.561, Pearson r = 0.628, Spearman r_s = 0.600; corpus-level Kendall τ = 0.579, Pearson r = 0.703, Spearman r_s = 0.665 (see Table 4 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Adding step-by-step reasoning (ZS-CoT) increased reliability/alignement with human judgments; reference-enhanced evaluations benefit more from CoT than they did without reasoning (Ref-ICE-Score gains via ZS-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Without CoT, reference-enhanced and reference-free variants had similar performance; CoT was required to unlock further gains from references. Rule-based score extraction is brittle and may limit improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not directly quantified for CoT runs; authors show CoT improves overall alignment on CoNaLa but do not provide per-complexity breakdown in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>CoT prompting combined with explicit rubric/template improved alignment; step-by-step prompts give the LLM structured reasoning that makes the final score better aligned with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>CoNaLa: 2,860 annotated snippets (CoT ablation evaluated on CoNaLa only due to resource limits).</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>ZS-CoT increased LLM-human correlation relative to the baseline ICE-Score (see numerical improvements above); direct comparison to inter-human agreement not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No supervised calibration on human labels; used rule-based parser to extract numeric scores from LLM reasoning outputs; temperature set to 0 for deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting meaningfully improves the agreement between LLM-based evaluations and human usefulness judgments on CoNaLa. It also enables reference-enhanced variants to better exploit reference code (Ref-ICE-Score benefits more under CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Score extraction was implemented with a simple rule-based parser that may not be optimal; CoT evaluations are more expensive and were only tested on CoNaLa due to resource constraints; improvements may depend on the LLM backbone capability (GPT-4 could further improve performance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ICE-Score: Instructing Large Language Models to Evaluate Code', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-EVAL <em>(Rating: 2)</em></li>
                <li>CodeBERTScore: Evaluating code generation with pretrained models of code <em>(Rating: 2)</em></li>
                <li>Out of the bleu: how should we assess quality of the code generation models? <em>(Rating: 2)</em></li>
                <li>GPTScore: Evaluate as you desire <em>(Rating: 1)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1717",
    "paper_id": "paper-258352761",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "ICE-Score",
            "name_full": "ICE-Score (Instructing Large Language Models to Evaluate Code)",
            "brief_description": "A reference-free, instruction-prompted LLM-based automatic metric that instructs LLMs to rate generated code on multidimensional criteria (usefulness and execution-based correctness) using a 0–4 rubric; evaluated primarily with GPT-3.5 and ablated with GPT-4 and chain-of-thought.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (reference-free and reference-enhanced variants), compared to automated metrics (BLEU, ROUGE-L, METEOR, CodeBLEU, RUBY, CodeBERTScore)",
            "llm_judge_model": "GPT-3.5-turbo (primary); GPT-4 (ablation/comparison)",
            "llm_judge_prompt_approach": "Single-rating with explicit rubric/template-based steps (assign 0–4 Likert-style score for a specified evaluation aspect); optionally augmented with Zero-Shot Chain-of-Thought (step-by-step evaluation) and a rule-based parser to extract scores",
            "artifact_type": "source code (generated code snippets)",
            "artifact_domain": "Python (CoNaLa human-preference experiment); multiple languages including Python, Java, C, C++, JavaScript (HumanEval-X functional-correctness experiments)",
            "evaluation_criteria": "Human-rated usefulness (0–4 Likert-style rubric) and execution-based functional correctness (0–4 rubric guided by unit-test pass/fail and logic checks)",
            "human_evaluation_setup": "Experienced software developers graded generated code (CoNaLa) on a 0–4 usefulness scale; for functional correctness experiments, automated execution-based ground truth (HumanEval / HumanEval-X unit tests and reference solutions) was used as the reference",
            "human_expert_count": "4.5 (average annotators per snippet on CoNaLa)",
            "human_expert_expertise": "experienced software developers (human annotators for CoNaLa)",
            "agreement_metric": "Kendall's Tau (τ), Pearson correlation (r_p), Spearman correlation (r_s)",
            "agreement_score": "Against human usefulness (CoNaLa), ICE-Score reported: example-level Kendall τ = 0.556, Pearson r = 0.613, Spearman r_s = 0.594; corpus-level Kendall τ = 0.546, Pearson r = 0.649, Spearman r_s = 0.635. (Functional-correctness correlations versus execution-based ground truth were reported in Tables 2/3; ICE-Score/Ref-ICE-Score qualitatively outperform baselines across languages, exact numeric values for those tables are reported in the paper but not reproduced verbatim in the text excerpt.)",
            "high_agreement_conditions": "Clear, task-agnostic evaluation criteria and template prompts (explicit rubric); use of chain-of-thought prompting and reasoning; more capable backbone LLM (GPT-4) improved alignment; reference-free evaluation already strongly aligned when references are low-quality.",
            "low_agreement_conditions": "Poor or biased reference code (reference-enhanced variant sometimes offers no improvement or can introduce bias); limited/opaque training data (possible contamination) is a concern; reliance on low-quality references reduced benefit of reference-enhanced scoring.",
            "artifact_complexity_effect": "Not explicitly quantified numerically; authors note code evaluation requires deeper programming understanding and imply harder/complex programming concepts make alignment more challenging (i.e., LLMs may perform less reliably on more complex code), but no direct per-complexity breakdown is reported in the excerpt.",
            "criteria_clarity_effect": "Clarity and specificity of the rubric + template improved alignment; adding Zero-Shot Chain-of-Thought (step-by-step evaluation) increased correlation with human judgments (see CoT ablation).",
            "sample_size": "CoNaLa: 2,860 annotated code snippets (5 generations × 472 examples, average 4.5 annotators per snippet). HumanEval-X: authors sampled 20 generated samples per problem (exact aggregated total for the multilingual subset not specified in the excerpt).",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "LLM-human agreement reported (see correlations above) demonstrating ICE-Score outperforms traditional automatic metrics and achieves substantial correlation with human preference; direct comparison of LLM-human agreement to inter-human agreement is not provided in the excerpt.",
            "calibration_or_training": "No fine-tuning against the human labels reported; ICE-Score uses instruction prompts (template and explicit criteria). For stability they set model temperature to 0 and used rule-based score extraction rather than training on human judgments.",
            "key_findings": "ICE-Score (LLM-as-a-judge) achieves substantially higher correlation with human usefulness judgments than traditional string-based and several neural baselines (e.g., BLEU, ROUGE-L, CodeBERTScore variants). Reference-free ICE-Score already performs strongly; Ref-ICE-Score (with reference code) does not reliably improve performance when references are imperfect. Augmenting prompts with Zero-Shot Chain-of-Thought and using a stronger backbone (GPT-4) further increases agreement with human ratings. For execution-based correctness, ICE-Score variants outperform or match state-of-the-art automatic metrics across multiple languages (example-level and corpus-level results reported in the paper).",
            "limitations_noted": "Possible data contamination for some datasets (CoNaLa, HumanEval Python) given closed-source LLM training data; dependence on closed-source/high-cost LLM APIs; reference code quality can bias reference-enhanced evaluation; score extraction for CoT is rule-based and may not be optimal; inter-human agreement statistics are not reported (preventing direct comparison to human–human reliability).",
            "uuid": "e1717.0",
            "source_info": {
                "paper_title": "ICE-Score: Instructing Large Language Models to Evaluate Code",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ZS-CoT evaluation",
            "name_full": "Zero-Shot Chain-of-Thought (ZS-CoT) evaluation prompting",
            "brief_description": "A prompting strategy that appends a step-by-step evaluation instruction to the LLM prompt so the model performs a sequence of reasoning steps before emitting a final rubric score; implemented here with a rule-based parser to extract the numeric score.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge with chain-of-thought reasoning (step-by-step reasoning in prompt)",
            "llm_judge_model": "GPT-3.5-turbo (ZS-CoT experiments run on CoNaLa; authors note GPT-4 also evaluated elsewhere but ZS-CoT ablation used GPT-3.5 due to resources)",
            "llm_judge_prompt_approach": "Zero-shot Chain-of-Thought: add an instruction like 'Step-by-step Evaluation:' prompting the model to produce intermediate reasoning and then a final score; scores extracted via a rule-based parser",
            "artifact_type": "source code (generated code snippets)",
            "artifact_domain": "Python (CoNaLa experiment where ZS-CoT ablation was performed)",
            "evaluation_criteria": "Usefulness (0–4 rubric) and optionally functional correctness following the same rubric template",
            "human_evaluation_setup": "Same CoNaLa human annotation setup (experienced developers rated usefulness on 0–4; used as human ground truth for correlation). ZS-CoT evaluation compared LLM outputs to these human annotations.",
            "human_expert_count": "4.5 (average annotators per snippet on CoNaLa)",
            "human_expert_expertise": "experienced software developers",
            "agreement_metric": "Kendall's Tau (τ), Pearson (r_p), Spearman (r_s)",
            "agreement_score": "ZS-CoT (CoT-ICE-Score) reported improved correlations vs non-CoT: example-level Kendall τ = 0.561, Pearson r = 0.628, Spearman r_s = 0.600; corpus-level Kendall τ = 0.579, Pearson r = 0.703, Spearman r_s = 0.665 (see Table 4 in paper).",
            "high_agreement_conditions": "Adding step-by-step reasoning (ZS-CoT) increased reliability/alignement with human judgments; reference-enhanced evaluations benefit more from CoT than they did without reasoning (Ref-ICE-Score gains via ZS-CoT).",
            "low_agreement_conditions": "Without CoT, reference-enhanced and reference-free variants had similar performance; CoT was required to unlock further gains from references. Rule-based score extraction is brittle and may limit improvements.",
            "artifact_complexity_effect": "Not directly quantified for CoT runs; authors show CoT improves overall alignment on CoNaLa but do not provide per-complexity breakdown in the excerpt.",
            "criteria_clarity_effect": "CoT prompting combined with explicit rubric/template improved alignment; step-by-step prompts give the LLM structured reasoning that makes the final score better aligned with human judgments.",
            "sample_size": "CoNaLa: 2,860 annotated snippets (CoT ablation evaluated on CoNaLa only due to resource limits).",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "ZS-CoT increased LLM-human correlation relative to the baseline ICE-Score (see numerical improvements above); direct comparison to inter-human agreement not reported.",
            "calibration_or_training": "No supervised calibration on human labels; used rule-based parser to extract numeric scores from LLM reasoning outputs; temperature set to 0 for deterministic outputs.",
            "key_findings": "Zero-shot Chain-of-Thought prompting meaningfully improves the agreement between LLM-based evaluations and human usefulness judgments on CoNaLa. It also enables reference-enhanced variants to better exploit reference code (Ref-ICE-Score benefits more under CoT).",
            "limitations_noted": "Score extraction was implemented with a simple rule-based parser that may not be optimal; CoT evaluations are more expensive and were only tested on CoNaLa due to resource constraints; improvements may depend on the LLM backbone capability (GPT-4 could further improve performance).",
            "uuid": "e1717.1",
            "source_info": {
                "paper_title": "ICE-Score: Instructing Large Language Models to Evaluate Code",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-EVAL",
            "rating": 2
        },
        {
            "paper_title": "CodeBERTScore: Evaluating code generation with pretrained models of code",
            "rating": 2,
            "sanitized_title": "codebertscore_evaluating_code_generation_with_pretrained_models_of_code"
        },
        {
            "paper_title": "Out of the bleu: how should we assess quality of the code generation models?",
            "rating": 2,
            "sanitized_title": "out_of_the_bleu_how_should_we_assess_quality_of_the_code_generation_models"
        },
        {
            "paper_title": "GPTScore: Evaluate as you desire",
            "rating": 1,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 1,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        }
    ],
    "cost": 0.015882499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ICE-Score: Instructing Large Language Models to Evaluate Code</p>
<p>Terry Yue terry.zhuo@monash.edu 
CSIRO's Data61
Monash University</p>
<p>ICE-Score: Instructing Large Language Models to Evaluate Code
4DCCA8CE51E77D025413CF512520A5DD
Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text.Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement.The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment.Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks.Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources.To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments.Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references.We evaluate the efficacy of our metric on two different aspects (human preference and execution success) and four programming languages.Our results demonstrate that our metric surpasses state-ofthe-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks.We also make our evaluation metric and datasets available to the public 1 , encouraging further research in evaluating code intelligence tasks.</p>
<p>Introduction</p>
<p>Natural language generation (NLG) systems have seen significant progress in developing large language models (LLMs).These models have shown great promise in generating high-quality and diverse texts that can be difficult to distinguish from human-written texts (Ouyang et al., 2022).However, evaluating the quality of NLG systems remains a challenging task, primarily due to the limitations of traditional evaluation metrics.Tokenmatching-based metrics, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), have been widely used to evaluate NLG systems but have demonstrated poor correlation with human judgment and a lack of ability to capture semantic meanings (Kocmi et al., 2021).Furthermore, these metrics require reference output, which can be challenging to obtain for new tasks and low-resource domains (Liu et al., 2023).</p>
<p>In recent years, the use of LLMs as referencefree evaluators for Natural Language Generation (NLG) tasks has gained attention among researchers.This approach is strongly aligned with human preferences, even when reference texts are unavailable (Liu et al., 2023;Fu et al., 2023).The underlying assumption behind this approach is that LLMs possess a profound understanding of humangenerated text and task instructions, enabling them to evaluate various NLG tasks through prompts.The exceptional performance of LLMs in contextual understanding and natural language generation, as evidenced by studies (Brown et al., 2020), further supports this assumption.Moreover, LLMs trained on both textual and code-based data have showcased remarkable capabilities in diverse downstream tasks related to source code, including code generation (OpenAI, 2023;Allal et al., 2023;Li et al., 2023).While a performance gap still exists between LLMs and human developers in coderelated tasks, recent research has illustrated that LLMs can be enhanced to handle various source code tasks with appropriate guidance (Chen et al., 2023;Madaan et al., 2023).This indicates the significant potential of LLMs in comprehending and working with source code.</p>
<p>Code evaluation presents unique challenges, requiring a deeper understanding of programming concepts and more complex syntax than natural language generation (Hindle et al., 2016).Traditional reference-based evaluation metrics for code generation, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and chrF (Popović, 2015), rely on token matching to assess performance automatically.However, these metrics have demonstrated poor correlation with human evaluation (Evtikhiev et al., 2023) since they often underestimate the variety of outputs with the same semantic logic.While some studies have incorporated programming features to improve these metrics, they have shown limited gains and poor correlation with functional correctness (Eghbali and Pradel, 2022;Tran et al., 2019).Alternatively, researchers have proposed using well-designed test suites to objectively evaluate code generation performance at the function level (Chen et al., 2021;Zheng et al., 2023;Cassano et al., 2023).However, developing these test suites requires programming expertise, which can be impractical and costly in low-resource scenarios.Additionally, executing model-generated code poses a security risk and must be run in an isolated sandbox, which is technically cumbersome.</p>
<p>More recently, CodeBERTScore (Zhou et al., 2023), a neural-model-based evaluation metric, has been proposed, showing a higher correlation with functional correctness and human preferences by capturing the semantic information of reference code and generated code.However, CodeBERTScore still relies on high-quality references that can be difficult and expensive to obtain.Moreover, the limited performance of the Code-BERT (Feng et al., 2020) backbone suggests that it has not yet reached a human-level understanding of source code, limiting the effectiveness of CodeBERTScore.Therefore, more advanced evaluation metrics are needed so that they can better capture the complex syntax and semantics of code intelligence tasks.</p>
<p>To address these challenges, we propose a novel evaluation metric based on LLMs trained on both text and code, shown in Figure 1.Specifically, we Instruct LLMs to perform human-like multidimensional Code Evaluation, where the metric is denoted as ICE-Score.Our metric leverages the recent NLG metric, G-EVAL (Liu et al., 2023), but achieves superior correlations with subjective human preferences and objective functional correctness, both at the example and corpus levels.Different from G-EVAL, ICE-Score only relies on assessment criteria and evaluation step template, without the need for instruction generation and weighted scoring function.</p>
<p>Based on our extensive evaluation, we have summarized our contributions as follows:</p>
<p>• We designed the first multi-dimensional and reference-free automatic evaluation metric for code intelligence tasks via large language models.</p>
<p>• We conducted extensive experiments to demonstrate the efficacy of ICE-Score on four programming languages (Java, Python, C, C++, and JavaScript) from two aspects (human-based usefulness and execution-based functional correctness).</p>
<p>• We further discussed several aspects that can improve the performance of ICE-Score, including the backbone model performance and reasoning capability.</p>
<p>Method</p>
<p>Our evaluation metric ICE-Score, inspired by G-EVAL (Liu et al., 2023), consists of two main components: 1) task definition, evaluation criteria, and detailed evaluation steps, and 2) a given problem and generated code snippet for evaluation.Different from G-EVAL, we only require the input of evaluation criteria and template-based evaluation steps, without the need for generation from LLMs.</p>
<p>In addition, As we set the model temperature to 0, our evaluation metric no longer needs a weighted scoring function after iterative score generation.These two differences suggest that ICE-Score is more cost-friendly and efficient.</p>
<p>Instructions for Code Evaluation</p>
<p>The evaluation of code quality involves two main aspects: 1) human judgment of code usefulness and 2) execution-based functional correctness.To provide a comprehensive evaluation, we adopt the design of G-EVAL for the general task instruction, as follows:</p>
<p>You will be given the code snippet for a problem.Your task is to rate the code snippet only on one metric.Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.</p>
<p>Regarding the task-agnostic prompt, we have designed the evaluation criteria for assessing code usefulness, as shown in Appendix A.1.These criteria are aligned with previous human evaluations of code quality (Evtikhiev et al., 2023).To evaluate functional correctness, we emphasize the importance of considering unit tests during the evaluation process.We present the following criteria for evaluating functional correctness, as provided in Appendix A.2.</p>
<p>For the instruction of evaluation steps, we provide a template-based prompt:</p>
<p>Evaluation Steps: 1. Read the problem carefully and identify the required functionalities of the implementation.</p>
<ol>
<li>Read the code snippet and compare it to the problem.Check if the code snippet covers all required functionalities of the problem, and if it aligns with the Evaluation Criteria.3. Assign a score for [Evaluation Aspect] on a scale of 0 to 4, where 0 is the lowest and 4 is the highest based on the Evaluation Criteria.</li>
</ol>
<p>Here, we define [Evaluation Aspect] as any aspects that are emphasized during the evaluation.In our paper, we consider code usefulness and functional correctness.</p>
<p>Inputs of Code Evaluation</p>
<p>It is worth noting that most code generative models do not take formatting into account, resulting in unformatted code that requires post-processing of code formatting to be understood, compiled, and executed (Zheng et al., 2023).Additionally, automatic evaluation metrics for code generation, such as CodeBLEU (Ren et al., 2020) and RUBY (Tran et al., 2019), still rely on language-specific program parsers2 .However, based on prior findings that LLMs can robustly understand input data (Huang et al., 2022;Zhuo et al., 2023;Zhu et al., 2023), we hypothesize that LLMs can also understand programming context without proper formatting.Therefore, for evaluation, we input the problems and generated code (and reference code, if provided).When the reference code is provided, we slightly modify the evaluation steps in the prompt to incorporate it.</p>
<p>Experiment Setup</p>
<p>We evaluate the effectiveness of ICE-Score using GPT-3.5 (GPT-3.5-turbo3 ) as the backbone across multiple datasets and programming languages.We conduct two experiments to investigate the correlation between ICE-Score and human preference and functional correctness, respectively.We compare the performance of LLMbased evaluations against 7 predominant automatic evaluation metrics, including the state-of-the-art CodeBERTScore (Zhou et al., 2023).To measure the correlation with human preference, we use the CoNaLa dataset (Yin et al., 2018) and corresponding human annotation on the generated code from various models trained on the dataset (Evtikhiev et al., 2023).To measure the correlation with functional correctness, we use the HumanEval-X dataset (Zheng et al., 2023).We do not consider distinguishability as an evaluation option, as prior work (Zhou et al., 2023) has shown it to be an unreliable meta-metric that cannot substitute for execution-based or human-based ratings.</p>
<p>Automatic Evaluation Metric Baselines</p>
<p>The baseline metrics we include can be classified into two groups: string-based and neural-modelbased evaluation.</p>
<p>String-based Evaluation Most evaluation metrics in code generation have been adapted from natural language generation (NLG) and rely on comparing the generated code to reference code.The most commonly used metric is BLEU (Papineni et al., 2002), which computes the overlaps of n-grams in the generated output with those in the reference, where the n-grams are tokenized using a language-specific tokenizer (Post, 2018).Other metrics include ROUGE-L (Lin, 2004), a recall-oriented metric that looks for the longest common subsequence between the reference and the generated code, and METEOR (Banerjee and Lavie, 2005), which is based on unigram matching between the generated code and the reference.However, studies have shown that BLEU may yield similar results for models with different quality levels from the perspective of human graders in code generation (Evtikhiev et al., 2023), leading to the proposal of new evaluation metrics such as RUBY (Tran et al., 2019).RUBY takes the code structure into account and compares the program dependency graphs (PDG) of the reference and the candidate.If the PDG is impossible to build, the metric falls back to comparing the abstract syntax tree (AST), and if the AST is also impossible to build, it compares the weighted string edit distance between the tokenized reference and candidate sequence.Another recent metric is CodeBLEU (Ren et al., 2020), which is a composite metric that computes a weighted average of four sub-metrics treating code differently: as a data-flow graph, as an abstract syntax tree, and as text.CodeBLEU is designed to evaluate the quality of generated code for code generation, code translation, and code refinement tasks.Neural-model-based Evaluation Neural-modelbased evaluation is becoming increasingly important for evaluating the quality of code generated by deep learning models.CodeBERTScore (Zhou et al., 2023) is one of the latest approaches that leverages pre-trained code models like Code-BERT (Feng et al., 2020) and best practices from natural language generation evaluation to assess the quality of generated code.CodeBERTScore encodes the generated code and reference code independently and considers the natural language context, contextual information of each token, and implementation diversity.It enables the comparison of code pairs that are lexically different and calculates precision and recall based on the bestmatching token vector pairs.This approach provides an effective way to evaluate the effectiveness of deep learning models for code intelligence tasks.Note that the authors of CodeBERTScore provided both F1 and F3 scores, with the optional source input.Therefore, we use these four language-specific variants of CodeBERTScore in our experiments.</p>
<p>Datasets and Evaluation Aspects</p>
<p>Human-based Usefulness Experiments Similar to (Zhou et al., 2023), we conduct an evaluation on the CoNaLa benchmark (Yin et al., 2018), which is a widely used dataset for natural language context to Python code generation.To measure the correlation between each evaluation metric and human preference, we utilize the human annotations provided by (Evtikhiev et al., 2023).Specifically, for each example in the dataset, experienced software   Table 2: Example-level Kendall-Tau (τ ) and Spearman (r s ) correlations with the execution-based functional correctness on HumanEval.ICE-Score: without reference code inputs, or reference-free; Ref-ICE-Score: with reference code inputs, or reference-enhanced.The best performance is bold.The second-best performance is underlined.</p>
<p>BLEU</p>
<p>developers were asked to grade the generated code snippets from five different models.The grading scale ranges from zero to four, with zero indicating that the generated code is irrelevant and unhelpful, and four indicating that the generated code solves the problem accurately.The dataset comprises a total of 2,860 annotated code snippets (5 generations × 472 examples) with each snippet being graded by 4.5 annotators on average.</p>
<p>Execution-based Functional Correctness Experiments</p>
<p>We conduct an evaluation of functional correctness using the HumanEval benchmark (Chen et al., 2021), which provides natural language goals, input-output test cases, and reference solutions written by humans for each example.The benchmark originally consists of 164 coding problems in Python, and has been extended by (Cassano et al., 2023) to 18 other programming languages, including Java, C++, Python, and JavaScript.We chose to evaluate our models on these languages, as they are among the most popular programming languages.The translated examples also include the predictions of code-davinci-002 and their corresponding functional correctness scores.Inspired by (Zhou et al., 2023), we obtain them from the HumanEval-X dataset (Zheng et al., 2023).As each problem has nearly 200 generated code samples on average, it would be computationally expensive to evaluate them all using LLMs.Therefore, we randomly select 20 samples from each problem, and collect all samples from problems where no more than 20 versions of code were generated.</p>
<p>Correlation Metrics To measure the correlation between each metric's scores and the references, we follow best practices in natural language evaluation and used Kendall-Tau (τ ), Pearson (r p ), and Spearman (r s ) coefficients.4 .To systematically study the efficacy of each automatic evaluation metric, we compute both example-level and corpuslevel correlations.The example-level correlation is the average correlation of each problem example, while the corpus-level correlation is the correlation of all aggregated examples in the task.</p>
<p>Results</p>
<p>Human-based Usefulness Table 1 shows the correlation between different metrics with human preference.We compare two variants of our evaluation approach, reference-free and reference-enhanced evaluations, with 10 baseline metrics and their variants.We find that ICE-Score outperform these metrics by a significant margin, regarding both example-and corpus-level correlations.Our observation is consistent with the work of CodeBER-Score, where the variants of CodeBERScore mostly outperform the strong baselines like chrF and ROUGE-L.For example, ICE-Score achieves 0.556 and 0.546 measured by Spearman correlation on example level and corpus level, respectively.In contrast, prior evaluation metrics barely reach a score of 0.5.In addition, we find that Ref-ICE-Score does not significantly improve the performance, indicating the reference code may not be optimized.Our further analysis of the human rating of CoNaLa reference code complies   Table 3: Corpus-level Kendall-Tau (τ ) and Spearman (r s ) correlations with the execution-based functional correctness on HumanEval.ICE-Score: without reference code inputs, or reference-free; Ref-ICE-Score: with reference code inputs, or reference-enhanced.The best performance is bold.The second-best performance is underlined.</p>
<p>BLEU</p>
<p>with this implication, where the average score of the reference code only achieves 3.4 out of 4, suggesting that not all human practitioners consider the reference fully useful.</p>
<p>Execution-based Functional Correctness Table 2 and Table 3 present the results of example-and corpus-level functional correctness, respectively.From Table 2, we observe that both reference-free and reference-enhanced Ref-ICE-Scoresconsistently outperform the other baselines across all four programming languages on the example level.ICE-Score even outperforms the reference-enhanced one, suggesting potential bias in some reference code.Additionally, we find that METEOR and CodeBLEU receive better correlations than all variants of CodeBERTScore, indicating that they are still strong baselines compared to the recent neuralmodel-based evaluators in code generation.In Table 3, we observe that our Ref-ICE-Score achieves state-of-the-art performance among all evaluation metrics.When compared to other baselines, ICE-Score still achieves comparable results to the source-free CodeBERTScore-F3.</p>
<p>Ablation Study</p>
<p>Does reasoning help the code evaluation?Prior work (Wei et al.; Kojima et al.) has demonstrated that the performance of LLMs can be significantly improved via Chain-of-Thought (CoT) and Zero-Shot-Chain-of-Thought (ZS-CoT), where the prompts instruct LLMs to perform the task in a step-by-step manner.Here, we explore the zeroshot reasoning ability of LLMs in evaluating code generation.Specifically, we instruct GPT-3.5 to  perform CoT-evaluation by adding "Step-by-step Evaluation:" at the end of the prompt.An example of the zero-shot-CoT prompt is shown in Figure 2. Instead of using LLMs to extract the evaluation score from the reasoning steps, like the original metric of zero-shot-CoT via multiple queries, we design a rule-based parser to extract scores.Due to limited resources, we only evaluate on CoNaLa in Table 4.Our results show that ZS-CoT can significantly improve the reliability of code evaluation.Additionally, we find that Ref-ICE-Score can achieve better results than reference-free ones via ZS-CoT, even though their performances are similar without CoT processing.This suggests that LLMs can exploit the use of reference code through reasoning.</p>
<p>Does more-capable backbone LLM yield better performance on code evaluation?As shown in previous studies (OpenAI, 2023;Bubeck et al., 2023), GPT-4 significantly outperforms GPT-3.5 on various tasks.Therefore, we use GPT-4 as the backbone model for ICE-Score and evalu-  ate its performance on CoNaLa.The results in Table 5 indicate that GPT-4 consistently surpasses GPT-3.5-turbo on evaluating code, suggesting it has the superior capability of code comprehension.We also note that using a more capable model like GPT-4 can guarantee even better performance, compared to using ZS-CoT techniques in Table 4.</p>
<p>Discussion</p>
<p>Data Contamination Evaluations on recent closed-source LLMs have been criticized for the possibility of data contamination (Aiyappa et al., 2023), where the model may have already seen the evaluation datasets during training, due to the opaque training details of these models.For instance, Kocmi and Federmann (2023) conducted an empirical study on a few closed-source LLMs, including GPT-3.5, and suggested that LLMs are the state-of-the-art evaluators of translation qual-ity, based on the evaluation of the WMT22 Metric Shared Task (Freitag et al., 2022).However, as most of the evaluated models were trained on data prior to 20225 , it is highly likely that these models have been trained with some human-rated translation quality data.Similarly, G-EVAL (Liu et al., 2023) shows that GPT-3.5 and GPT-4 are the stateof-the-art evaluators of natural language generation (NLG) with the evaluation of three NLG datasets.However, as these human-annotated datasets were released before 2021, it is probable that they were included in the training data of GPT-3.5 and GPT-4.In contrast, our work is minimally impacted by data contamination, as we report the data release year in Table 6.Our analysis suggests that only CoNaL and HumanEval (Python) datasets may have been contaminated, and it is unlikely that GPT-3.5 has seen any human annotation or generated code during training.</p>
<p>Human-aligned Evaluation Beyond Code Generation While our study has shown that LLMs can achieve state-of-the-art performance in evaluating the functional correctness and usefulness of generated source code, the question remains as to whether LLMs can be utilized to evaluate code intelligence tasks beyond code generation.Allamanis et al. (2018) have identified several downstream applications such as code translation, commit message generation, and code summarization.While some studies have investigated the human evaluation of these tasks, none of them have released the annotation data or fully described the human evaluation criteria.This presents a challenge for analyzing if ICE-Score can be adapted to these tasks.For example, Hu et al. (2022) proposed a human evaluation metric for code documentation generation quality, which is specifically designed for code comment generation and commit message generation.Their metric includes three aspects: Language-related, Content-related, and Effectiveness-related, with detailed task descriptions and explanations of assigned scores.We propose that the information provided in their metric can be used to create prompts for LLM-based evaluation and enable human-aligned evaluation of code documentation generation.</p>
<p>Related Work</p>
<p>Large Language Models for Code.LLMs pretrained on large-scale code data have demonstrated strong capabilities in code intelligence tasks, such as code completion (Li et al., 2023;Luo et al., 2023;Rozière et al., 2023), code summarization (Ahmed and Devanbu, 2022;Sun et al., 2023) and program repair (Surameery and Shakor, 2023;Sobania et al., 2023).However, they remain unreliable, particularly in scenarios that require an understanding of natural language.Recent studies (Muennighoff et al., 2023b;Ma et al.) show that pretraining on both text and code results in the optimal model performance on natural language and code understanding.Furthermore, in order to make LLMs more human-aligned and more capable of performing complex tasks, instruction tuning is proposed to enhance the capability of following natural language requirements.In this work, we utilize such instruction-tuned LLMs to conduct multi-dimensional code evaluation via various instructions.</p>
<p>Automatic Evaluation Metrics for Generation.</p>
<p>The quest for reliable and robust automatic evaluation metrics for generated content has been a cornerstone in natural language processing.Tradi-tionally, string-based metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and ME-TEOR (Banerjee and Lavie, 2005) have dominated the landscape, primarily when assessing machine translation or text summarization outputs.While these metrics provide a quick and cost-effective means of evaluating the quality of the generated text, they often fall short of capturing the nuanced intricacies and semantic richness inherent in natural language.To mitigate such drawbacks, a few neural-based multi-dimensional evaluation metrics have been proposed for text generation, such as UniEval (Zhong et al., 2022), GPTScore (Fu et al., 2023) and G-EVAL (Liu et al., 2023).However, when it comes to code generation, where both syntactical correctness and semantic intent are paramount, there are few attempts to address these challenges.Instead, the most dominant metrics still compute the similarity between generated code and reference code.In this work, we introduce ICE-Score, a novel metric that not only addresses the limitations of its predecessors but also harnesses the capabilities of LLMs, setting a new benchmark for the evaluation of code generation tasks.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel evaluation metric based on large language models trained on both text and code, which can better capture the complex syntax and semantics of code intelligence tasks.</p>
<p>Our metric achieves superior correlations with subjective human preferences and objective functional correctness, both at the example and corpus levels, without reference and test suites.We conduct an extensive evaluation of four programming languages (Java, Python, C, C++, and JavaScript) and demonstrate the effectiveness of our proposed method on human-based usefulness and execution-based functional correctness.We have publicly released our evaluation metrics and datasets to encourage the development of more accurate and effective evaluation metrics for tasks involving source code.</p>
<p>Our proposed evaluation metric is based on the assumption that LLMs can follow the instructions to evaluate the code snippets.The backbone models we investigated are closed-source state-of-the-art LLMs from OepnAI.As we noticed that there is a huge performance gap between current closedsource and open-source LLMs, it is possible that ICE-Score can be adapted with an open-source LLM trained on code and text, such as Wizard-Coder (Luo et al., 2023) and OctoPack (Muennighoff et al., 2023a).Hence, we encourage future investigations on open-source LLMs for code evaluation.In addition, as discussed in Section 6, our experiments only focus on two code generation tasks.There are other code intelligence tasks like program repair and code summarization.However, due to the limited study on human evaluation of these tasks, no open-source dataset is publicly available or documented in detail.Finally, ICE-Score assumes that either model weights or model APIs are available, which is costly for some users.We, therefore, suggest future work on proposing low-cost evaluation metrics.</p>
<p>Figure 1 :
1
Figure1: An illustration of ICE-Score.On the left-hand side, we input the task problems and corresponding generated code snippets.On the right-hand side, ICE-Score outputs the corresponding assessments.</p>
<p>) and Spearman (r s ) correlations with the human preferred usefulness on CoNaLa.ICE-Score: without reference code inputs, or reference-free; Ref-ICE-Score: with reference code inputs, or reference-enhanced.CoT-indicates the use of ZS-CoT.The best performance is bold.</p>
<p>Figure 2 :
2
Figure 2: Example inputs and outputs with (a) ICE-Score, (b) ICE-Score with Zero-Shot Chain-of-Thought.With the step-by-step evaluation, the output assessment is more aligned with human preference.</p>
<p>Table 4 :
4
Example-level and corpus-level Kendall-Tau (τ ), Pearson (r p
MetricτExample r pr sτCorpus r pr sICE-Score.556 .613 .594 .546 .649 .635CoT-ICE-Score.561 .628 .600 .579 .703 .665Ref-ICE-Score.554 .617 .591 .539 .661 .630CoT-Ref-ICE-Score .571 .639 .607 .583 .712 .667</p>
<p>Table 6 :
6
Dataset, Release Year and the likelihood of data contamination for each dataset used in our study.
DatasetRelease Year Likely to be contaminated?CoNaLa2018✓human-annotated CoNaLa w/ generated code2023✗HumanEval (Python)2021✓HumanEval-X (w/o Python)2023✗human-annotated HumanEval-X w/ generated code2023✗
https://github.com/terryyz/ice-score
https://tree-sitter.github.io/
https://platform.openai.com/docs/ models/gpt-3-5
We use the implementations from https://scipy. org/
https://platform.openai.com/docs/ model-index-for-researchers
AcknowledgementsWe thank Haolan Zhan and Yufei Wang for the helpful feedback on the paper.LimitationsA Prompts for Code EvaluationA.1 Code Usefulness Evaluation Criteria: Usefulness (0-4) Usefulness of the code snippet based on the problem description.-A score of 0: Snippet is not at all helpful, it is irrelevant to the problem.-A score of 1: Snippet is slightly helpful, it contains information relevant to the problem, but it is easier to write the solution from scratch.-A score of 2: Snippet is somewhat helpful, it requires significant changes (compared to the size of the snippet), but is still useful.-A score of 3: Snippet is helpful, but needs to be slightly changed to solve the problem.-A score of 4: Snippet is very helpful, it solves the problem.A.2 Functional CorrectnessEvaluation Criteria: Functional Correctness (0-4) -Execution-based quality of the code snippet combined with the problem.The correctness is measured by all possible unit tests and the comparison of the reference code.The combination of the code snippet and the problem should pass all the possible tests based on your understanding of the reference code.The length of the code snippet can not determine the correctness.You need to assess the logic line by line.-A score of 0 (failing all possible tests) means that the code snippet is totally incorrect and meaningless.-A score of 4 (passing all possible tests) means that the code snippet is totally correct and can handle all cases.B Automatic Evaluation Metric BaselinesOur implementations of the automatic evaluation metric baselines except for CodeBERTScore are based on https: //github.com/JetBrains-Research/codegen-metrics.For CodeBERTScore, we adopt the official release at https://github.com/neulab/code-bert-score.C Correlation MetricsFor all correlation metrics, we use the implementation from https://scipy.org/ and call these APIs with the default settings.D Rule-based Score Extraction from Zero-shot Chain Of Thought EvaluationWe demonstrate the general implementation of score extraction:1 import re 2 TASK_KEY_WORD = "usefulness" # or " functional" 3 def get_gpt_answer(raw_content): 13 # Clean up and split the content 14 splits = content.lower().replace("(", "").replace(")", "").split("\n")We note that our extraction process for the evaluation metrics is entirely rule-based and may not be optimized for the best results.
Few-shot training llms for project-specific codesummarization. Toufique Ahmed, Premkumar Devanbu, Proceedings of the 37th. the 37th2022</p>
<p>IEEE/ACM International Conference on Automated Software Engineering. </p>
<p>Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn, arXiv:2303.12767Can we trust the evaluation on chatgpt?. 2023arXiv preprint</p>
<p>Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, arXiv:2301.03988Santacoder: don't reach for the stars!. 2023arXiv preprint</p>
<p>A survey of machine learning for big code and naturalness. Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, Charles Sutton, ACM Computing Surveys (CSUR). 5142018</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>MultiPL-E: A scalable and polyglot approach to benchmarking neural code generation. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, Abhinav Jangda, IEEE Transactions of Software Engineering. 2023TSE</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Crystalbleu: precisely and efficiently measuring the similarity of code. Aryaz Eghbali, Michael Pradel, 37th IEEE/ACM International Conference on Automated Software Engineering. 2022</p>
<p>Out of the bleu: how should we assess quality of the code generation models?. Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, Timofey Bryksin, Journal of Systems and Software. 2031117412023</p>
<p>Codebert: A pre-trained model for programming and natural languages. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Results of wmt22 metrics shared task: Stop using bleu-neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, André Ft Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)2022</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>On the naturalness of software. Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, Premkumar Devanbu, Communications of the ACM. 5952016</p>
<p>Correlating automated and human evaluation of code documentation generation quality. Xing Hu, Qiuyuan Chen, Haoye Wang, Xin Xia, David Lo, Thomas Zimmermann, ACM Transactions on Software Engineering and Methodology (TOSEM). 3142022</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, 2023</p>
<p>To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Proceedings of the Sixth Conference on Machine Translation. the Sixth Conference on Machine TranslationOnline. Association for Computational Linguistics2021Hitokazu Matsushita, and Arul Menezes</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. </p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634Gpteval: Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, arXiv:2306.08568Wizardcoder: Empowering code large language models with evolinstruct. 2023arXiv preprint</p>
<p>At which training stage does code data help llms reasoning?. Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, Shanshan Li, </p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.17651arXiv:2308.07124Octopack: Instruction tuning code large language models. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, Shayne Longpre, 2023. 2023aarXiv preprint</p>
<p>Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, arXiv:2305.16264Thomas Wolf, and Colin Raffel. 2023b. Scaling data-constrained language models. arXiv preprint</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>chrf: character n-gram f-score for automatic mt evaluation. Maja Popović, Proceedings of the tenth workshop on statistical machine translation. the tenth workshop on statistical machine translation2015</p>
<p>A call for clarity in reporting BLEU scores. Matt Post, 10.18653/v1/W18-6319Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.102972020arXiv preprint</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>An analysis of the automatic bug fixing performance of chatgpt. Dominik Sobania, Martin Briesch, Carol Hanna, Justyna Petke, arXiv:2301.086532023arXiv preprint</p>
<p>Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, arXiv:2305.12865Automatic code summarization via chatgpt: How far are we?. 2023arXiv preprint</p>
<p>Use chat gpt to solve programming bugs. Shafiq Nigar, Mohammed Y Surameery, Shakor, International Journal of Information Technology &amp; Computer Engineering (IJITC). 2455-52903012023</p>
<p>Does bleu score work for code migration. Ngoc Tran, Hieu Tran, Son Nguyen, Hoan Nguyen, Tien Nguyen, 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC). IEEE2019</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. </p>
<p>Learning to mine aligned code and natural language pairs from stack overflow. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, Graham Neubig, 10.1145/3196398.3196408International Conference on Mining Software Repositories, MSR. ACM2018</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, arXiv:2303.175682023arXiv preprint</p>
<p>Towards a unified multidimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Codebertscore: Evaluating code generation with pretrained models of code. Shuyan Zhou, Uri Alon, Association for Computational Linguistics: EMNLP 2023. 2023Sumit Agarwal, and Graham Neubig</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. 2023arXiv preprint</p>
<p>Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing, Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
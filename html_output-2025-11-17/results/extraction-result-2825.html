<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2825 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2825</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2825</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-272367206</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.00872v2.pdf" target="_blank">Self-evolving Agents with reflective and memory-augmented abilities</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2825.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2825.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-evolving Agents with Reflective and Memory-augmented Abilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-agent LLM framework (User, Assistant, Checker) that augments LLM agents with reflection and a dual-memory system (STM/LTM) managed by MemorySyntax, which uses an Ebbinghaus forgetting-curve-inspired retention model and linguistic optimization to prioritize and retain task-relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAGE Assistant (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Three-role multi-agent architecture: User proposes task, Assistant (LLM) observes environment, generates actions and outputs, performs self-reflection r_t from o_{1:t} and R_{1:t}, and consults a Checker agent which provides iterative feedback f_t; Assistant maintains short-term memory (STM) for recent trajectory (O_t, a_t) and long-term memory (LTM) for self-reflections r_t and prioritized information. The assistant updates policy π_θ via iterative feedback (policy-gradient style) and uses MemorySyntax to decide what to retain/transfer/discard. Memory is consulted during decision-making to condition action generation o_t ∼ π_θ(o_t | s_t, R_t, f_it) together with reflection-derived signals.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>applied to multiple base models in this paper (GPT-3.5, GPT-4, Llama2-7B, CodeLlama-7B, Qwen-1.8B, Qwen-7B, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dual-memory (short-term trajectory STM and long-term self-reflection LTM); retention-strength based memory (human-like forgetting curve)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Dual-memory system: STM stores immediate, recent observations and actions as trajectory T_t = (O_t, a_t) for short-horizon decision-making; LTM stores self-reflections r_t = ref(o_{1:t}, R_{1:t}) and optimized information items I*_t. MemorySyntax computes a retention rate R(I_t, τ)=exp(−τ/S) where S is information strength; linguistic optimization increases strength S* for an optimized I*_t. Memory update uses two thresholds θ1>θ2: if retention ≥ θ1 keep in STM; if retention < θ2 discard; if θ2 ≤ retention < θ1 move to LTM. Retrieval and use: Assistant conditions generation on STM, retrieves relevant LTM reflections and prioritized I*_t based on retention strength and recency (implicit importance-based selection), and integrates reflected insights into planning and action selection. The checker provides iterative feedback which the assistant uses along with memory contents to refine policy.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not given as a fixed numeric limit; theoretical formulation includes a capacity constraint C (∑_t M_t ≤ C) but no concrete memory size reported</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Importance/retention-strength and recency driven (retention S(I_t)=H(I_t) f(t)); retrieval favors items with higher retention strength and recent/linguistically-optimized items; reflections r_t are stored and consulted; selection is threshold/priority-based rather than an explicit vector-similarity index in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Memories are added after reflection steps (r_t ← ref(o_{1:t},R_{1:t}) and M_L ← M_L ∪ {r_t}); new information I_t is linguistically optimized to I*_t, retention rate computed, and then either retained in STM, transferred to LTM, or discarded based on thresholds θ1 and θ2; the assistant incrementally updates memory alongside iterative feedback iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFWorld (as part of AgentBench GAME suite); evaluations also on AgentBench tasks including sequential text-based tasks</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>ALFWorld: sequential, dependency-rich embodied/text-based tasks bridging TextWorld and ALFRED; requires multi-step planning, abstract strategy formation and execution, long-horizon memory of prior actions and world state, variable episode length and multi-step step-completion evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Across models, SAGE (with MemorySyntax) yields substantial gains on sequential/text-game tasks: e.g., SAGE-Mistral-7B on ALFWorld: Task completion 73.8% (± not reported) — a +17.3% improvement over baseline; Step completion accuracy 79.9% (+14.8%). Ablation results (memory enabled) show substantial gains for smaller models in AgentBench tasks (see per-model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (without SAGE memory optimization): e.g., Mistral-7B baseline Task completion 56.5% and Step completion 65.1% (as reported in Table 2); other models show lower baselines in AgentBench (see per-model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Ablation shows memory optimization is a major contributor: enabling memory optimization raises low-performing open-source models dramatically (examples in Table 5): Qwen-1.8B and CodeLlama-7B show large absolute gains across AgentBench tasks (detailed examples below).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper cites prior memory approaches (MemGPT uses FIFO queue; MemoryBank uses insertion-time based forgetting) but does not present a direct empirical head-to-head comparison against those specific systems in text games. Instead, SAGE's MemorySyntax (Ebbinghaus-based + linguistic optimization) is proposed and evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Memory that explicitly models retention strength and stores richer self-reflections improves sequential/text-game performance—especially for smaller models; memory optimization reduces context-limit errors and invalid-action errors, and selective retention (MemorySyntax) is effective in preserving crucial information while discarding noise to reduce cognitive load.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-evolving Agents with reflective and memory-augmented abilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2825.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2825.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGE-Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAGE-applied Mistral-7B Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instantiation of the SAGE assistant using Mistral-7B as the base LLM, evaluated on the ALFWorld sequential text-game tasks; uses the SAGE dual-memory and reflection mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAGE-Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mistral-7B LLM acting as the Assistant within the SAGE three-agent loop: processes observations and trajectories in STM, retrieves LTM self-reflections and prioritized items via retention-strength logic, generates action sequences conditioned on current state s_t, reward signals, and iterative checker feedback, and updates policy with iterative feedback/reflection cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>STM + LTM (self-reflections and prioritized info via MemorySyntax)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Same SAGE MemorySyntax: STM tracks recent trajectory, LTM stores reflections r_t; retention rate computed via exponential forgetting model R(I_t,τ)=exp(−τ/S), linguistic optimization increases strength S*, and thresholds θ1/θ2 govern retention/transfer/discard.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified (theoretical capacity variable C mentioned but no numeric limit)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Priority by retention strength and recency; retrieve self-reflections r_t and high-strength I*_t for planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Self-reflection generated after trajectories and feedback (r_t), appended to LTM; memory items re-evaluated with retention model and moved between STM/LTM/discarded based on thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFWorld (sequential task from AgentBench GAME)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Sequential multi-step embodied tasks with dependencies requiring planning and stepwise execution; performance measured by task completion rate and step completion accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Task completion: 73.8% (reported) which is +17.3% relative to its baseline; Step completion accuracy: 79.9% (+14.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline Mistral-7B (without SAGE memory/reflective loop) Task completion: 56.5%; Step completion accuracy: 65.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Memory + reflection substantially increases both task-level completion and step-level accuracy in ALFWorld; iterative feedback plus selective retention improves planning and reduces logical errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-evolving Agents with reflective and memory-augmented abilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2825.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2825.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGE-Qwen1.8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAGE-applied Qwen-1.8B Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qwen-1.8B used as the Assistant inside the SAGE framework; ablation shows very large relative gains from Memory optimization on AgentBench tasks including ALFWorld (text-game) and web/knowledge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAGE-Qwen-1.8B</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Qwen-1.8B LLM in SAGE: uses STM for immediate trajectory, generates reflections r_t for LTM, integrates checker feedback iteratively, and uses MemorySyntax retention-based memory updates to select and consult relevant memories during generation and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Qwen-1.8B (1.8B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>STM + LTM with retention-strength based selection (MemorySyntax)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>As in SAGE: STM stores recent observations/actions; LTM stores self-reflections and linguistically-optimized information I*_t; retention strength S(I_t)=H(I_t) f(t) used with thresholds θ1, θ2 to manage storage.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>No numeric capacity provided (theoretical capacity C referenced but unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Selection by retention strength and recency; reflections used to guide action generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Memories appended after reflection; information optimized linguistically before evaluating retention and placement in STM/LTM or discard.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFWorld (as part of AgentBench) and other AgentBench tasks</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>ALFWorld: multi-step sequential tasks, requires planning, stepwise execution and memory of past actions/world state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>AgentBench ablation (Table 5): with memory optimization Qwen-1.8B scores — OS: 18.7, DB: 28.3, KG: 45.3, ALF: 10.5, WS: 31.4, M2W: 25.1 (units are task-specific scores/percentages as reported in table). On ALFWorld specifically ALF -> 10.5% (from 0.0% without memory).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without memory optimization (w/o memo) Qwen-1.8B: OS 10.4, DB 22.6, KG 6.8, ALF 0.0, WS 26.6, M2W 5.0 (reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Ablation (Table 5) shows very large relative gains when memory optimization is enabled: KG from 6.8 → 45.3, ALF from 0.0 → 10.5, M2W from 5.0 → 25.1, indicating memory is critical for multi-step/text-game and knowledge-graph tasks for this smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper references other memory approaches (e.g., MemGPT FIFO queue, MemoryBank forgetting-curve variants) in related work but does not run direct empirical comparisons between those and SAGE's MemorySyntax on the same benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Memory optimization (MemorySyntax + reflection) transforms an otherwise failing small model on ALFWorld (0% → 10.5%) and yields large gains on knowledge- and web-based benchmarks; selective retention and reflection are particularly important for small-model agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-evolving Agents with reflective and memory-augmented abilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2825.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2825.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGE-CodeLlama7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAGE-applied CodeLlama-7B Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CodeLlama-7B used as the Assistant inside the SAGE framework; ablation shows large improvements across AgentBench tasks including ALFWorld when memory optimization is enabled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAGE-CodeLlama-7B</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>CodeLlama-7B LLM functions as the Assistant in the SAGE three-agent loop, using STM and LTM plus reflection and MemorySyntax to store and retrieve trajectory history and self-reflections, and iteratively refine outputs with checker feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>CodeLlama-7B</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>STM + LTM (retention-strength based MemorySyntax)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>STM holds recent observations/actions; LTM stores self-reflections r_t and optimized items I*_t; retention computed via Ebbinghaus-style curve and information strength S(I_t)=H(I_t) f(t); thresholds dictate retention/transfer/discard.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified numerically (theoretical capacity C discussed but no concrete size)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Priority-based retrieval using retention strength and recency; reflection items consulted during generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>After each interaction/reflection, r_t is appended to LTM and items re-evaluated under MemorySyntax thresholds to determine placement or discard.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ALFWorld (AgentBench GAME) and other AgentBench tasks</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Sequential multi-step tasks requiring planning, accurate step execution and use of prior knowledge and past actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 5 (w memo) shows CodeLlama-7B scores: OS 23.4, DB 41.3, KG 48.0, ALF 12.5, WS 58.7, M2W 15.0 (task-specific scores/percentages as reported). On ALFWorld ALF -> 12.5% with memory enabled (from 0.0% baseline without memory).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without memory optimization (w/o memo) CodeLlama-7B: OS 9.7, DB 2.7, KG 0.0, ALF 0.0, WS 14.3, M2W 5.0 (reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Ablation shows dramatic improvements when memory is enabled (e.g., DB 2.7 → 41.3, KG 0.0 → 48.0, WS 14.3 → 58.7, ALF 0.0 → 12.5), indicating memory optimization is a dominant factor enabling task success in complex/text-game tasks for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>No direct empirical comparison against alternative memory modules in this paper; prior works (MemGPT FIFO, MemoryBank) are cited in related work but not benchmarked here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Memory optimization and reflection unlock strong gains for a code-focused 7B model across web, knowledge-graph and sequential text-game (ALFWorld) tasks; selective retention and self-reflection particularly improve multi-step execution and reduce basic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-evolving Agents with reflective and memory-augmented abilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 2)</em></li>
                <li>Agentbench: Evaluating llms as agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2825",
    "paper_id": "paper-272367206",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [
        {
            "name_short": "SAGE",
            "name_full": "Self-evolving Agents with Reflective and Memory-augmented Abilities",
            "brief_description": "A three-agent LLM framework (User, Assistant, Checker) that augments LLM agents with reflection and a dual-memory system (STM/LTM) managed by MemorySyntax, which uses an Ebbinghaus forgetting-curve-inspired retention model and linguistic optimization to prioritize and retain task-relevant information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAGE Assistant (framework)",
            "agent_description": "Three-role multi-agent architecture: User proposes task, Assistant (LLM) observes environment, generates actions and outputs, performs self-reflection r_t from o_{1:t} and R_{1:t}, and consults a Checker agent which provides iterative feedback f_t; Assistant maintains short-term memory (STM) for recent trajectory (O_t, a_t) and long-term memory (LTM) for self-reflections r_t and prioritized information. The assistant updates policy π_θ via iterative feedback (policy-gradient style) and uses MemorySyntax to decide what to retain/transfer/discard. Memory is consulted during decision-making to condition action generation o_t ∼ π_θ(o_t | s_t, R_t, f_it) together with reflection-derived signals.",
            "base_llm": "applied to multiple base models in this paper (GPT-3.5, GPT-4, Llama2-7B, CodeLlama-7B, Qwen-1.8B, Qwen-7B, Mistral-7B)",
            "uses_memory": true,
            "memory_type": "dual-memory (short-term trajectory STM and long-term self-reflection LTM); retention-strength based memory (human-like forgetting curve)",
            "memory_architecture": "Dual-memory system: STM stores immediate, recent observations and actions as trajectory T_t = (O_t, a_t) for short-horizon decision-making; LTM stores self-reflections r_t = ref(o_{1:t}, R_{1:t}) and optimized information items I*_t. MemorySyntax computes a retention rate R(I_t, τ)=exp(−τ/S) where S is information strength; linguistic optimization increases strength S* for an optimized I*_t. Memory update uses two thresholds θ1&gt;θ2: if retention ≥ θ1 keep in STM; if retention &lt; θ2 discard; if θ2 ≤ retention &lt; θ1 move to LTM. Retrieval and use: Assistant conditions generation on STM, retrieves relevant LTM reflections and prioritized I*_t based on retention strength and recency (implicit importance-based selection), and integrates reflected insights into planning and action selection. The checker provides iterative feedback which the assistant uses along with memory contents to refine policy.",
            "memory_capacity": "Not given as a fixed numeric limit; theoretical formulation includes a capacity constraint C (∑_t M_t ≤ C) but no concrete memory size reported",
            "memory_retrieval_method": "Importance/retention-strength and recency driven (retention S(I_t)=H(I_t) f(t)); retrieval favors items with higher retention strength and recent/linguistically-optimized items; reflections r_t are stored and consulted; selection is threshold/priority-based rather than an explicit vector-similarity index in the paper",
            "memory_update_strategy": "Memories are added after reflection steps (r_t ← ref(o_{1:t},R_{1:t}) and M_L ← M_L ∪ {r_t}); new information I_t is linguistically optimized to I*_t, retention rate computed, and then either retained in STM, transferred to LTM, or discarded based on thresholds θ1 and θ2; the assistant incrementally updates memory alongside iterative feedback iterations.",
            "text_game_benchmark": "ALFWorld (as part of AgentBench GAME suite); evaluations also on AgentBench tasks including sequential text-based tasks",
            "game_characteristics": "ALFWorld: sequential, dependency-rich embodied/text-based tasks bridging TextWorld and ALFRED; requires multi-step planning, abstract strategy formation and execution, long-horizon memory of prior actions and world state, variable episode length and multi-step step-completion evaluation.",
            "performance_with_memory": "Across models, SAGE (with MemorySyntax) yields substantial gains on sequential/text-game tasks: e.g., SAGE-Mistral-7B on ALFWorld: Task completion 73.8% (± not reported) — a +17.3% improvement over baseline; Step completion accuracy 79.9% (+14.8%). Ablation results (memory enabled) show substantial gains for smaller models in AgentBench tasks (see per-model entries).",
            "performance_without_memory": "Baseline (without SAGE memory optimization): e.g., Mistral-7B baseline Task completion 56.5% and Step completion 65.1% (as reported in Table 2); other models show lower baselines in AgentBench (see per-model entries).",
            "has_ablation_study": true,
            "memory_ablation_results": "Ablation shows memory optimization is a major contributor: enabling memory optimization raises low-performing open-source models dramatically (examples in Table 5): Qwen-1.8B and CodeLlama-7B show large absolute gains across AgentBench tasks (detailed examples below).",
            "comparison_with_other_memory_types": "Paper cites prior memory approaches (MemGPT uses FIFO queue; MemoryBank uses insertion-time based forgetting) but does not present a direct empirical head-to-head comparison against those specific systems in text games. Instead, SAGE's MemorySyntax (Ebbinghaus-based + linguistic optimization) is proposed and evaluated.",
            "key_findings_about_memory_effectiveness": "Memory that explicitly models retention strength and stores richer self-reflections improves sequential/text-game performance—especially for smaller models; memory optimization reduces context-limit errors and invalid-action errors, and selective retention (MemorySyntax) is effective in preserving crucial information while discarding noise to reduce cognitive load.",
            "uuid": "e2825.0",
            "source_info": {
                "paper_title": "Self-evolving Agents with reflective and memory-augmented abilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SAGE-Mistral-7B",
            "name_full": "SAGE-applied Mistral-7B Assistant",
            "brief_description": "An instantiation of the SAGE assistant using Mistral-7B as the base LLM, evaluated on the ALFWorld sequential text-game tasks; uses the SAGE dual-memory and reflection mechanisms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAGE-Mistral-7B",
            "agent_description": "Mistral-7B LLM acting as the Assistant within the SAGE three-agent loop: processes observations and trajectories in STM, retrieves LTM self-reflections and prioritized items via retention-strength logic, generates action sequences conditioned on current state s_t, reward signals, and iterative checker feedback, and updates policy with iterative feedback/reflection cycles.",
            "base_llm": "Mistral-7B",
            "uses_memory": true,
            "memory_type": "STM + LTM (self-reflections and prioritized info via MemorySyntax)",
            "memory_architecture": "Same SAGE MemorySyntax: STM tracks recent trajectory, LTM stores reflections r_t; retention rate computed via exponential forgetting model R(I_t,τ)=exp(−τ/S), linguistic optimization increases strength S*, and thresholds θ1/θ2 govern retention/transfer/discard.",
            "memory_capacity": "Not specified (theoretical capacity variable C mentioned but no numeric limit)",
            "memory_retrieval_method": "Priority by retention strength and recency; retrieve self-reflections r_t and high-strength I*_t for planning",
            "memory_update_strategy": "Self-reflection generated after trajectories and feedback (r_t), appended to LTM; memory items re-evaluated with retention model and moved between STM/LTM/discarded based on thresholds.",
            "text_game_benchmark": "ALFWorld (sequential task from AgentBench GAME)",
            "game_characteristics": "Sequential multi-step embodied tasks with dependencies requiring planning and stepwise execution; performance measured by task completion rate and step completion accuracy.",
            "performance_with_memory": "Task completion: 73.8% (reported) which is +17.3% relative to its baseline; Step completion accuracy: 79.9% (+14.8%).",
            "performance_without_memory": "Baseline Mistral-7B (without SAGE memory/reflective loop) Task completion: 56.5%; Step completion accuracy: 65.1%.",
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Memory + reflection substantially increases both task-level completion and step-level accuracy in ALFWorld; iterative feedback plus selective retention improves planning and reduces logical errors.",
            "uuid": "e2825.1",
            "source_info": {
                "paper_title": "Self-evolving Agents with reflective and memory-augmented abilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SAGE-Qwen1.8B",
            "name_full": "SAGE-applied Qwen-1.8B Assistant",
            "brief_description": "Qwen-1.8B used as the Assistant inside the SAGE framework; ablation shows very large relative gains from Memory optimization on AgentBench tasks including ALFWorld (text-game) and web/knowledge tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAGE-Qwen-1.8B",
            "agent_description": "Qwen-1.8B LLM in SAGE: uses STM for immediate trajectory, generates reflections r_t for LTM, integrates checker feedback iteratively, and uses MemorySyntax retention-based memory updates to select and consult relevant memories during generation and planning.",
            "base_llm": "Qwen-1.8B (1.8B parameters)",
            "uses_memory": true,
            "memory_type": "STM + LTM with retention-strength based selection (MemorySyntax)",
            "memory_architecture": "As in SAGE: STM stores recent observations/actions; LTM stores self-reflections and linguistically-optimized information I*_t; retention strength S(I_t)=H(I_t) f(t) used with thresholds θ1, θ2 to manage storage.",
            "memory_capacity": "No numeric capacity provided (theoretical capacity C referenced but unspecified)",
            "memory_retrieval_method": "Selection by retention strength and recency; reflections used to guide action generation",
            "memory_update_strategy": "Memories appended after reflection; information optimized linguistically before evaluating retention and placement in STM/LTM or discard.",
            "text_game_benchmark": "ALFWorld (as part of AgentBench) and other AgentBench tasks",
            "game_characteristics": "ALFWorld: multi-step sequential tasks, requires planning, stepwise execution and memory of past actions/world state.",
            "performance_with_memory": "AgentBench ablation (Table 5): with memory optimization Qwen-1.8B scores — OS: 18.7, DB: 28.3, KG: 45.3, ALF: 10.5, WS: 31.4, M2W: 25.1 (units are task-specific scores/percentages as reported in table). On ALFWorld specifically ALF -&gt; 10.5% (from 0.0% without memory).",
            "performance_without_memory": "Without memory optimization (w/o memo) Qwen-1.8B: OS 10.4, DB 22.6, KG 6.8, ALF 0.0, WS 26.6, M2W 5.0 (reported in Table 5).",
            "has_ablation_study": true,
            "memory_ablation_results": "Ablation (Table 5) shows very large relative gains when memory optimization is enabled: KG from 6.8 → 45.3, ALF from 0.0 → 10.5, M2W from 5.0 → 25.1, indicating memory is critical for multi-step/text-game and knowledge-graph tasks for this smaller model.",
            "comparison_with_other_memory_types": "Paper references other memory approaches (e.g., MemGPT FIFO queue, MemoryBank forgetting-curve variants) in related work but does not run direct empirical comparisons between those and SAGE's MemorySyntax on the same benchmarks.",
            "key_findings_about_memory_effectiveness": "Memory optimization (MemorySyntax + reflection) transforms an otherwise failing small model on ALFWorld (0% → 10.5%) and yields large gains on knowledge- and web-based benchmarks; selective retention and reflection are particularly important for small-model agent performance.",
            "uuid": "e2825.2",
            "source_info": {
                "paper_title": "Self-evolving Agents with reflective and memory-augmented abilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SAGE-CodeLlama7B",
            "name_full": "SAGE-applied CodeLlama-7B Assistant",
            "brief_description": "CodeLlama-7B used as the Assistant inside the SAGE framework; ablation shows large improvements across AgentBench tasks including ALFWorld when memory optimization is enabled.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAGE-CodeLlama-7B",
            "agent_description": "CodeLlama-7B LLM functions as the Assistant in the SAGE three-agent loop, using STM and LTM plus reflection and MemorySyntax to store and retrieve trajectory history and self-reflections, and iteratively refine outputs with checker feedback.",
            "base_llm": "CodeLlama-7B",
            "uses_memory": true,
            "memory_type": "STM + LTM (retention-strength based MemorySyntax)",
            "memory_architecture": "STM holds recent observations/actions; LTM stores self-reflections r_t and optimized items I*_t; retention computed via Ebbinghaus-style curve and information strength S(I_t)=H(I_t) f(t); thresholds dictate retention/transfer/discard.",
            "memory_capacity": "Not specified numerically (theoretical capacity C discussed but no concrete size)",
            "memory_retrieval_method": "Priority-based retrieval using retention strength and recency; reflection items consulted during generation",
            "memory_update_strategy": "After each interaction/reflection, r_t is appended to LTM and items re-evaluated under MemorySyntax thresholds to determine placement or discard.",
            "text_game_benchmark": "ALFWorld (AgentBench GAME) and other AgentBench tasks",
            "game_characteristics": "Sequential multi-step tasks requiring planning, accurate step execution and use of prior knowledge and past actions.",
            "performance_with_memory": "Table 5 (w memo) shows CodeLlama-7B scores: OS 23.4, DB 41.3, KG 48.0, ALF 12.5, WS 58.7, M2W 15.0 (task-specific scores/percentages as reported). On ALFWorld ALF -&gt; 12.5% with memory enabled (from 0.0% baseline without memory).",
            "performance_without_memory": "Without memory optimization (w/o memo) CodeLlama-7B: OS 9.7, DB 2.7, KG 0.0, ALF 0.0, WS 14.3, M2W 5.0 (reported in Table 5).",
            "has_ablation_study": true,
            "memory_ablation_results": "Ablation shows dramatic improvements when memory is enabled (e.g., DB 2.7 → 41.3, KG 0.0 → 48.0, WS 14.3 → 58.7, ALF 0.0 → 12.5), indicating memory optimization is a dominant factor enabling task success in complex/text-game tasks for this model.",
            "comparison_with_other_memory_types": "No direct empirical comparison against alternative memory modules in this paper; prior works (MemGPT FIFO, MemoryBank) are cited in related work but not benchmarked here.",
            "key_findings_about_memory_effectiveness": "Memory optimization and reflection unlock strong gains for a code-focused 7B model across web, knowledge-graph and sequential text-game (ALFWorld) tasks; selective retention and self-reflection particularly improve multi-step execution and reduce basic errors.",
            "uuid": "e2825.3",
            "source_info": {
                "paper_title": "Self-evolving Agents with reflective and memory-augmented abilities",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "Agentbench: Evaluating llms as agents",
            "rating": 2,
            "sanitized_title": "agentbench_evaluating_llms_as_agents"
        }
    ],
    "cost": 0.0176775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SAGE: Self-evolving Agents with Reflective and Memoryaugmented Abilities
20 Apr 2025</p>
<p>Xuechen Liang 
East China Jiao Tong University</p>
<p>Yangfan He 
University of Minnesota -Twin Cities
3 AutoAgents.ai</p>
<p>Yinghui Xia 
Xinyuan Song 
Emory University</p>
<p>Jianhui Wang 
University of Electronic Science and Technology of China</p>
<p>Meiling Tao 
Guangdong University of Technology
7 Amazon</p>
<p>Li Sun 
Xinhang Yuan 
Washington University</p>
<p>Jiayi Su 
Xiamen University
10 Independent Researcher</p>
<p>Keqin Li 
Jiaqi Chen 
Jinsong Yang 
Siyuan Chen 
Tianyu Shi ty.shi@mail.utoronto.ca 
University of Bristol</p>
<p>University of Toronto Project</p>
<p>SAGE: Self-evolving Agents with Reflective and Memoryaugmented Abilities
20 Apr 20259EC60CBBAA33262EA66EEC458DED68A3arXiv:2409.00872v2[cs.CL]
Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments.To address these issues, this paper proposes an innovative framework-Self-evolving Agents with Reflective and Memory-augmented Abilities (SAGE).The SAGE framework comprises three agents: the User, the Assistant, and the Checker.By integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.The agents, through self-evolution, can adaptively adjust strategies, optimize information storage and transmission, and effectively reduce cognitive load.We evaluate the performance of the SAGE framework on multiple benchmarks and long text tasks.Experimental results show that SAGE significantly improves model performance, achieving a 2.26X improvement on closed-source models and an improvement ranging from 57.7% to 100% on open-source models, with particularly notable effects on smaller models.</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have made significant progress in the field of natural language processing, demonstrating powerful performance in tasks such as dialogue and text generation Brown et al. (2020); He et al. (2025;2024).Recently, there has been growing interest in applying LLMs as autonomous agents (LLM agents), which use language not only for understanding and generation, but also for planning and acting in interactive environments Yao et al. (2023b); Shinn et al. (2023); Liang et al. (2024); Li et al. (2024); Zhou et al. (2024).However, these models still face several challenges: (1) LLM Agents need to continuously make decisions in changing environments and adapt to new situations and tasks.(2) LLM Agents lack long-term memory mechanisms, which is increasingly evident in situations requiring sustained interaction with the environment Graves et al. (2016).The limited context window also hinders the model's ability to handle information over long time spans Rae et al. (2019).</p>
<p>To tackle these challenges, researchers have proposed meta-learning and multi-task learning to enhance the transferability and adaptability of LLM agents.For memory limitations, prior works like MemGPT Packer et al. (2024) use a FIFO queue to manage forgetting, while MemoryBank employs a forgetting curve based on insertion time.However, these approaches are often task-specific, lacking a general framework to systematically improve LLM agents in complex environments.Recent innovations, such as AutoGPT Yang et al. (2023) and BabyAGI Nakajima (2024), leverage LLMs as core controllers, aiming to solve real-world challenges.Yet, multi-agent frameworks still face issues like communication overload, heavily relying on memory to maintain context.As interaction history grows, resource demands and latency increase, limiting efficient deployment in practical scenarios.</p>
<p>In this paper, we propose an innovative framework, Self-evolving Agents with reflective and memory-augmented abilities (SAGE).By enhancing agents' self-adjustment capabilities through reflection, they can more effectively utilize historical information and make efficient decisions when faced with complex and dynamic tasks.From the perspective of self-evolution, we introduce a memory optimization mechanism based on the Ebbinghaus forgetting curv Ebbinghaus (1885).This mechanism helps agents selectively retain key information, optimize information storage and transmission, reduce unnecessary cognitive load, and enhance agents' capabilities in interaction tasks with the environment.Experimental results demonstrate that our approach consistently enhances the performance of both proprietary and open-source LLMs across a wide range of benchmarks.The improvements are especially notable in smaller models, where the gains are more pronounced.On tasks such as multi-source question answering and code generation, our method sets a new standard, outperforming existing techniques and achieving leading benchmarks Etezadi &amp; Shamsfard (2023), including AgentBench Liu et al. (2023b).</p>
<p>The main contributions of our work are as follows:</p>
<p>• We propose a novel framework, SAGE, which introduces a reflection mechanism to enhance agents' self-adjustment capabilities.Without any additional training, this enables agents to utilize historical information more effectively and make better decisions when faced with complex and dynamic tasks.• We introduce a memory optimization mechanism based on the Ebbinghaus forgetting curve.This helps agents selectively retain key information, reducing the issue of information overload in multi-agent systems.• SAGE achieves improvements over strong baselines in multiple challenging real-world tasks and attains state-of-the-art results on benchmarks.This framework can be applied to other LLMs, with particularly strong improvements in smaller models.</p>
<p>Related work</p>
<p>Self-Improvement of Reasoning and Decision-Making</p>
<p>Deep learning has transformed multiple domains including NLP, time series analysis and computer vision Qiu et al. (2025a;b;2024).A lot of research is focused on making large language models (LLMs) better at improving themselves.Some researchers are working on using carefully crafted prompts to help models learn how to get better, although this usually only works for one-off tasks.Others are tweaking how models get feedback during tasks, which helps them get better at thinking things through Huang et al. (2022).There's also work on using strategies like random beam searches to help models make smarter decisions and assess their own work.Most current methods rely on quick, one-off tweaks and learning strategies that need lots of resources and hands-on tech help Tian et al. (2024).This paper introduces a self-reflection mechanism, showing that LLMs can keep getting better and produce higher quality work across different tasks, all without needing extra training.</p>
<p>Memory Mechanism for LLM-based Agents</p>
<p>In LLM-based agents, the memory module stores, processes, and retrieves task-related information, supporting knowledge accumulation, experience handling, and decision-making.</p>
<p>To enhance the self-evolution capabilities of these agents, researchers are focused on designing and optimizing these memory modules Raffel et al. (2020).Past research has covered various designs and implementations of memory modules.This includes integrating information from different trials to boost reasoning abilities or storing information in natural language to enhance the module's interpretability and user-friendliness Wada et al. (2019).Despite progress, self-adjustment and memory management still need improvement to handle complex real-world problems more effectively.</p>
<p>Figure 1: An illustration of the SAGE: a user provides a description and instance to the assistant with short-term (STM) and long-term (LTM) memory.The assistant performs observation, action, reflection, and output, which the checker reviews.The retention rate curve on the right illustrates memory decay over time, with a self-evolving loop guiding continued updates.</p>
<p>Method</p>
<p>In this section, we present the SAGE framework, designed to improve agent performance by leveraging three core mechanisms: iterative feedback, reflection, and MemorySyntax (as shown in Figure 1).The assistant agent A iteratively updates its policy π θ based on feedback f t provided by the checker agent C, optimizing over successive iterations to maximize the expected reward R. The reflection mechanism allows A to incorporate historical observations O t and actions a t , forming a self-reflection r t , which is stored in the memory M L for future decision-making.Finally, MemorySyntax combines the Ebbinghaus forgetting curve with linguistic principles to manage memory decay, dynamically updating the agent's short-term memory M S and long-term memory M L by prioritizing information based on its retention strength S(I t ), thus improving the agent's ability to retain crucial information while discarding less relevant data.The subsequent subsections detail these components.</p>
<p>Iterative Feedback</p>
<p>The iterative feedback mechanism in the SAGE framework enables the assistant agent A to refine its policy π θ through repeated interactions with the checker agent C. At each iteration t, the assistant receives feedback f t based on its current output o t , and adjusts its policy accordingly.This process continues until the checker validates the output or the iteration cap N is reached, ensuring that A incrementally optimizes its decisions to improve task performance over successive iterations.</p>
<p>Initialization Phase</p>
<p>Role Assignment.In the SAGE framework, three agents are introduced: the user U, the assistant A, and the checker C. The user, upon receiving prompt P U , assumes the role of task proposer by specifying a task T U and related constraints C U .The assistant, upon receiving prompt P A , generates a sequence of actions a t based on the observations O t and environment E .The checker C evaluates the output o A produced by the assistant, providing feedback f C based on the discrepancy between o A and the expected result, updating its policy π θ iteratively to minimize this gap.Task Assignment.The task T U provided by the user includes an initial task description d U and an instance i U that serves as the reference for correct output.This forms the input set I A = (d U , i U ) for the assistant to initiate its generative process.The assistant then proceeds by selecting an action a t at each time step t, guided by π θ , with the goal of maximizing the reward R t for completing T U . Figure 2: An example of the assistant's iterative workflow, including checker evaluation, prompt templates for feedback, and reflection processes integrating short-term and longterm memory.</p>
<p>Actual Interaction Phase</p>
<p>Following the role assignment and task definition in the initialization phase, the assistant A transitions into the actual interaction phase to generate outputs aimed at accomplishing the task T U .In this phase, A iteratively produces outputs o t at each time step t based on the task description d U and instance i U provided in the input set I A = (d U , i U ).At each time step t, the assistant selects an action a t by following its policy π θ , which is conditioned on the current state s t , the reward signal R t (the reward score for task performance), and feedback f i t from the checker C.This decision-making process is formalized as:
o t ∼ π θ (o t | s t , R t , f i t ),(1)
where π θ represents the assistant's policy, R t reflects the reward signal based on task performance at time t, and f i t is the feedback provided by the checker during the i-th iteration.</p>
<p>As the interaction progresses, the checker C evaluates each output o t generated by A, comparing it against the expected outcome derived from i U .Based on this comparison, the checker provides iterative feedback f i t to guide A in refining its actions a t and outputs o t .The iterative refinement continues until either the checker validates the output as correct or the iteration limit N is reached.</p>
<p>Theoretical optimality of iterative feedback mechanism.In the SAGE framework, the assistant repeatedly updates its policy through this checker feedback, enabling the outputs to be incrementally refined until either the result is validated or a specified iteration limit is reached.The assistant's utility R A reflects task performance, and the checker's utility R C depends on its feedback.The following theorem indicates that this iterative feedback mechanism leads to strategy stability in the sense of a Nash equilibrium Fudenberg &amp; Tirole (1991).</p>
<p>Theorem 3.1 (Theory for the multi-agent iterative feedback system).Let U , A, C denote the compact, convex strategy spaces of the user (U), assistant (A), and checker (C), respectively.Assume that the utility functions
R U : U × A × C → R, R A : U × A × C → R, and R C : U × A × C → R (2)
are continuous in each player's strategy.Then, by the Debreu-Glicksberg-Fan fixed-point theorem, there exists a Nash equilibrium
(s * U , s * A , s * C ) ∈ U × A × C. (3)
Furthermore, suppose that the assistant's policy π θ is updated via policy gradient methods and that the checker's strategy is refined through convex optimization.Then, the iterative update procedures yield sequences
{π (k) θ } k≥0 and { f (k) } k≥0 ,(4)
which converge to a stable strategy profile (π * θ , f * ), and has:
R A (π * θ , f * ) ≥ R A (π θ , f * ), R C (π * θ , f * ) ≥ R C (π * θ , f ). (5)
This result demonstrates that the iterative feedback mechanism enhances the model's strategy stability by converging to a Nash equilibrium in the three-player game.It provides a stronger justification for the three-agent system versus simpler alternatives (such as two-agent systems).For the detailed theoretical explanation and proof, see A.2.</p>
<p>Evolutionary Goals and Directions</p>
<p>Leveraging the feedback f i t obtained at each iteration t, the assistant A formulates new evolutionary objectives:
G t+1 = (A t+1 , D t+1 ), D t+1 = arg min D t ∈∆ ∑ i∈I t L D D t ; f i t , π t θ ,(6)
where A t+1 represents the updated memory optimization mechanisms, and D t+1 ∈ ∆ refers to the model's self-adjustments to make the RL algorithm converge.These evolutionary objectives guide the assistant in updating its policy π θ for the subsequent iteration.The policy update is governed by the function ψ, which integrates the current policy π t θ with the new evolutionary objectives G t+1 :
θ t+1 = ϕ θ t , G t+1 = θ t + α∇ θ λ A L A θ t , A t+1 + λ D L D θ t , D t+1 . (7)
Here L A (θ, A) and L D (θ, D) are MSE loss functions corresponding to the memoryoptimization and self-adjustment aspects, respectively, and λ A , λ D ≥ 0 are weighting coefficients.The iterative policy refinement enables the assistant A to continuously adapt its strategies based on cumulative feedback and evolving task requirements, thereby improving its overall performance in dynamic environments.</p>
<p>Memory Management</p>
<p>The SAGE framework implements a dual-memory system, consisting of Short-Term Memory (STM) and Long-Term Memory (LTM), to manage task-relevant information and enhance the agent's reasoning and decision-making capabilities (see Figure 2 for a visual representation of this process).</p>
<p>Short-Term Memory (STM).</p>
<p>STM is responsible for storing immediate, task-specific data with limited capacity.It updates rapidly with new observations (O t ) and actions (a t ), maintaining a recent trajectory history T t = (O t , a t ).This allows the agent to make real-time decisions and respond quickly to dynamic changes in the environment Mnih et al. (2015).</p>
<p>Long-Term Memory (LTM).</p>
<p>LTM retains critical information and self-reflections (r t ) over extended periods, enabling the agent to accumulate knowledge from past interactions and apply it to future tasks.Stored as M L = {r t | t ∈ T}, this memory mechanism allows the agent to use prior experiences to improve task performance, particularly in complex environments that require long-span information Graves et al. (2016).</p>
<p>By integrating STM and LTM, the SAGE framework allows the agent to balance immediate task demands with the ability to draw from accumulated knowledge, thereby enhancing its overall decision-making efficiency.</p>
<p>Reflection</p>
<p>Figure 4 illustrates an example of the reflection mechanism applied to a HotpotQA task Yang et al. (2018b).The reflection mechanism equips the assistant A with sparse reward signals, such as binary success/failure states, trajectory T t , and its stored memory M L .The assistant processes these inputs, deriving insights from past performance and storing self-reflections r t for future decision-making.These self-reflections, richer than scalar rewards, enhance the assistant's learning capacity and are incorporated into long-term memory:
r t = ref(o 1:t , R 1:t ),(8)
where ref(•) denotes the reflection function based on the output sequence o 1:t and rewards R 1:t .The derived reflection r t is then added to M L :
M L ← M L ∪ {r t }. (9)
The process gradually enhances the agent's decision-making, allowing it to adapt effectively through accumulated experience.</p>
<p>MemorySyntax</p>
<p>Building upon the reflection mechanism, the MemorySyntax method integrates the Ebbinghaus forgetting curve with linguistic principles to emulate human-like memory processes within the agent's memory management system.Let I t denote the information received at time t, and let R(I t , τ) represent its retention rate after a time interval τ.According to the Ebbinghaus forgetting curve, the retention rate is modeled as:
R(I t , τ) = e − τ S , (10)
where S is the strength of the information, reflecting its importance and complexity.</p>
<p>To enhance retention, MemorySyntax applies linguistic optimization to I t , producing an optimized version I * t with increased strength S * &gt; S. The retention rate for I * t is defined as:
R(I * t , τ) = e − τ S * , if I * t ∈ M S , e − τ S , if I * t ∈ M L ,(11)
where M S and M L represent short-term memory and long-term memory, respectively.The agent updates its memory state M t based on the retention rate of I * t using predefined thresholds θ 1 and θ 2 , with θ 1 &gt; θ 2 .The memory update rule is formalized as:
M t+1 =    M t ∪ {I * t }, if R(I * t , τ) ≥ θ 1 , M t \ {I * t }, if R(I * t , τ) &lt; θ 2 , M t , otherwise.(12)
This update rule operates under the following conditions:
• Retention in Short-Term Memory (M S ): If R(I * t , τ) ≥ θ 1 , the information I * t is retained in short-term memory M S . • Discarding Information: If R(I * t , τ) &lt; θ 2 , the information I * t is considered insignificant and is discarded. • Transfer to Long-Term Memory (M L ): If θ 2 ≤ R(I * t , τ) &lt; θ 1 , the information I * t is transferred to long-term memory M L .
The above MemorySyntax method is optimal and ensures maximal information utility.For theoretical background and detailed proof, see Appendix A.1.</p>
<p>By simulating human memory dynamics, MemorySyntax enables the agent to prioritize essential information, retain critical data in short-term memory M S , store important but less frequently used information in long-term memory M L , and discard irrelevant data.The mechanism addresses memory capacity limitations and enhances the agent's ability to perform complex tasks requiring efficient memory management.</p>
<p>Experiment</p>
<p>To demonstrate the capabilities and performance of SAGE in coordinating autonomous agent groups to work together on tasks, we conduct extensive quantitative experiments on benchmark tasks.We use a public benchmark, AgentBench Liu et al. (2023b), which is a multidimensional evolutionary benchmark from which we select six tasks.These tasks test the reasoning and decision-making abilities of LLMs acting as agents in multi-turn open-ended generation settings.To further assess the agents' long-context understanding, we select four widely adopted tasks related to long-text processing.These tasks reflect the agents' programming abilities(LCCGuo et al. ( 2023 2024), Qwen Bai et al. (2023), andChatGLM2 GLM et al. (2024).Dialogue history exceeding the model length limit is truncated, using greedy decoding.</p>
<p>Results</p>
<p>As shown in Table 1, our method significantly improves model performance on AgentBench, especially for smaller models.GPT-3.5 and GPT-4, despite already high scores, also show notable improvements with SAGE, up to 2.26x in the Database task.Llama2-7b is notably enhanced, showing the method's impact on weaker models.CodeLlama-7b and Qwen-1.8Balso see substantial gains.Qwen-1.8B,after applying our method, performs close to GPT-3.5, highlighting its potential as a general agent.Llama2, previously error-prone, shows a significant reduction in basic errors through feedback and memory optimization, proving that our method not only enhances agent capabilities but also reduces fundamental errors in complex tasks.4.9 14.9 14.9 14.9</p>
<p>Complex Problem-Solving Tasks Evaluation</p>
<p>We evaluated SAGE against baseline models on three tasks: long-form QA Akash et al.</p>
<p>(2023), multi-turn dialog Cui et al. (2020), and sequential task completion Stephens et al. (2012).As shown in Table 2, SAGE outperforms all baselines with significant gains, such as a 20.8% increase in answer accuracy for GPT-3.5 on HotpotQA Yang et al. (2018b) and a 17.3% improvement in task completion for Mistral-7b on ALFWorld Shridhar et al. (2021).Across all tasks, SAGE notably enhances answer accuracy, dialog coherence, and step completion.</p>
<p>Evaluation of Long-Context Tasks</p>
<p>We evaluated the agent's code generation and reasoning on four long-text tasks: LCC Dataset Mohler et al. (2016) 2017a) is a reading comprehension dataset with question-answer pairs and evidence paragraphs (filtered to over 1,000 words), also using answer F1 for evaluation.</p>
<p>We compared two self-refinement methods: Beam Search Kool et al. (2019), which integrates self-assessment through stochastic beam search, and Reflexion Shinn et al. ( 2023), which uses past trial experience in a verbal form.</p>
<p>Evaluation Results:</p>
<p>Code Completion Task: On the LCC dataset (Table 3), SAGE shows a slight improvement in F1 score (79.29) compared to Beam Search and Reflexion.Its memory mechanisms help refine code predictions, but the performance difference is not substantial in simpler tasks like code completion.</p>
<p>Reasoning Tasks: SAGE significantly outperforms Reflexion and Beam Search on HotPotQA and TriviaQA, with F1 scores of 22.06 and 22.76 (Table 3).SAGE's ability to effectively integrate multi-document information through reflection leads to better reasoning accuracy, while Reflexion and Beam Search face challenges in handling complex reasoning tasks.2019), and TriviaQA).RAG with BM25 relies on term-based matching, while RAG with DPR uses learned dense embeddings.TART adopts a transformer-based approach to produce relevant contexts, and FiD fuses multiple retrieved passages through an encoder-decoder design.In contrast, ChatGPT-4 (SAGE) employs a structured reasoning workflow for retrieval and generation, which leads to steady accuracy improvements of 3.6% to 4.7% and cuts memory consumption nearly 50% on some tasks, all without increasing latency.</p>
<p>Evaluation of RAG Agents</p>
<p>Error analysis</p>
<p>As shown in Figure 3, the SAGE framework significantly enhances agent performance across tasks, especially in the WS task for AgentBench, due to its iterative feedback mechanism, which refines outputs through continuous assistant-checker interaction.In OS and DB tasks, Context Limit Exceeded and invalid format errors are nearly eliminated, with a notable reduction in invalid action errors, attributed to the reflection mechanism that helps the assistant learn and reduce logical mistakes.</p>
<p>Ablation Study</p>
<p>We conducted ablation experiments on Qwen-1.8B and CodeLlama-7B to evaluate memory optimization (Table 5).Without memory optimization, both models perform weakly, especially Qwen-1.8B,which improves from 6.8 to 48.0 in KG and from 0.0 to 10.5 in ALF after optimization.Similarly, CodeLlama-7B shows substantial gains, particularly in DB (2.7 to 41.3) and WS (14.3 to 58.7).Overall, CodeLlama-7B performs better than Qwen-1.8B,highlighting the stronger adaptability of models with more parameters in handling complex tasks.</p>
<p>Conclusion</p>
<p>In this paper, we propose the SAGE framework, which enhances agents' self-adjustment and memory management in complex tasks through reflective mechanisms and memory optimization.Experimental results show significant performance improvements across benchmarks, especially in smaller models.In the AgentBench test, SAGE boosts the performance of strong baselines like GPT-3.5 and GPT-4, while also significantly improving open-source models.It effectively reduces basic errors and logical mistakes, particularly enabling smaller models to handle complex tasks.</p>
<p>Ziqi Zhou, Jingyue Zhang, Jingyuan Zhang, Yangfan He, Boyue Wang, Tianyu Shi, and Alaa Khamis.Human-centric reward optimization for reinforcement learning-based automated driving using large language models.arXiv preprint arXiv:2405.04135,2024.</p>
<p>Figure 4: The illustration of an example HotpotQA with SAGE.</p>
<p>A Theoretical Analysis</p>
<p>A.1 An Information-Theoretic Perspective on Memory Optimization Based on the Ebbinghaus Forgetting Curve</p>
<p>The memory optimization mechanism proposed in this work is grounded in the Ebbinghaus forgetting curve.It dynamically manages short-term memory (STM) and long-term memory (LTM) by prioritizing the retention of essential information while discarding irrelevant data.This mechanism can be viewed as computing the retention strength S(I t ) of each information unit based on its information entropy.Theoretically, one can interpret this as maximizing information entropy to show that the mechanism effectively mitigates the issue of information overload Cover &amp; Thomas (2006).That is, by preferentially retaining highentropy (i.e., high-uncertainty) information, the model improves its information processing efficiency.</p>
<p>First, the retention strength of information is defined as:
S(I t ) = H(I t ) f (t) (13)
where the numerator H(I t ) represents the information entropy of the content at time t, and f (t) models the forgetting curve.</p>
<p>The objective of memory optimization is to maximize the utility of retained information under a memory capacity constraint:
max ∑ t M t • S(I t ), s.t. ∑ t M t ≤ C (14)
Here, C represents the total memory capacity, and M t ∈ {0, 1} denotes whether the information at time t is retained (1) or discarded (0).</p>
<p>Using the method of Lagrange multipliers, we construct the Lagrangian function:
L(M t , λ) = ∑ t M t • S(I t ) − λ ∑ t M t − C (15)
Taking the derivative of L, which is the optimal memory retnetion, with respect to M t and setting it to zero yields the optimality condition:
∂L ∂M t = S(I t ) − λ = 0 ⇒ Retain I t if S(I t ) ≥ λ Discard I t if S(I t ) &lt; λ (16)
This rule is consistent with the threshold-based memory retention strategy used in the SAGE framework.</p>
<p>A.2 Iterative Feedback and Game-Theoretic Model</p>
<p>Multi-agent systems often depend on reinforcement learning (RL) methods, making their theoretical basis essential for ensuring that algorithms converge to stable solutions, such as Nash equilibria Fudenberg &amp; Tirole (1991).The subsequent theory provides a formal guarantee of convergence, showing that our proposed algorithm is not randomly designed.Instead, it surpasses standard approaches by establishing robust criteria that confirm both its stability and performance advantages.</p>
<p>A.2.1 Players and Their Strategy Spaces Settings</p>
<p>User (U) : Provides a task description T U and constraints C U .Assistant (A) : Chooses actions a t ∼ π θ (• | s t ), where π θ is the policy.Checker (C) : Generates feedback f t aimed at reducing the error ∥o
t − o * ∥ F ,(17)
where o t is the output of the assistant and o * is the expected (ground-truth) output, ∥ • ∥ F denotes the Frobenius norm.</p>
<p>For the Utility Functions, we have: Assistant's Utility, R A : Measures task performance (e.g., BLEU score, accuracy):</p>
<p>R A = Task Performance Metric of a t .Checker's Utility, R C : Measures feedback accuracy by reduction of error:
R C ∝ ∆∥o t − o * ∥ F .(18)
The overall interaction can be viewed as a three-player game in which: The assistant (A) optimizes its policy π θ to maximize R A , while the checker (C) aims to minimize ∥o t − o * ∥ (i.e., maximize R C ) The user U sets the task and constraints that guide both the assistant and checker's performances.Next we want to show that it can be shown that under a finite strategy space and continuous utility functions, a Nash equilibrium exists in this multi-agent game.</p>
<p>A.2.2 Setup for the proof of theorem 3.1</p>
<p>First, we want to write a road map for this proof.</p>
<p>• It can be shown that under a finite strategy space and continuous utility functions, a Nash equilibrium exists in this multi-agent game.</p>
<p>• According to the Debreu-Glicksberg-Fan Theorem (a generalization of Nash's existence theorem), if each player's strategy space is a compact convex set and the utility functions are continuous and quasi-concave (or concave) in their own strategies, then at least one Nash equilibrium exists.</p>
<p>• Then, the assistant's policy π θ is updated using policy gradient methods.Since R A is a continuous function of π θ , the assistant's utility is continuous.</p>
<p>• After that, the checker's feedback strategy can be obtained via convex optimization (e.g., least-squares regression), and thus R C is a convex function.• Therefore, a Nash equilibrium (π * θ , f * ) exists, at which both the assistant and checker's strategies are stable with respect to each other:
π * θ = arg max π R A (π, f * ), f * = arg max f R C (π * θ , f )(19)
Let's start the proof.Lemma A.1.Let S 1 , S 2 , . . ., S N be finite strategy sets for each of N players, and let u i : S 1 × • • • × S N → R be continuous for each i.Then, there exists a Nash equilibrium in the multi-agent game.</p>
<p>Proof.For each player i, define the set of mixed strategies as
∆(S i ) = x i ∈ R |S i | x i (s) ≥ 0 for all s ∈ S i , ∑ s∈S i x i (s) = 1 . (20) Each set ∆(S i ) is a simplex in R |S i |
, and hence is nonempty, compact, and convex.Define the expected utility function for player i:
U i (x 1 , . . . , x N ) = ∑ (s 1 ,...,s N )∈S 1 ×•••×S N N ∏ j=1 x j (s j ) u i (s 1 , . . . , s N ).(21)
Since the strategy spaces are finite and u i is continuous (and hence bounded on the finite set), the function U i is continuous on the product space ∏ N j=1 ∆(S j ).For each player i, define the best response correspondence as:
BR i (x −i ) = arg max x i ∈∆(S i ) U i (x i , x −i ),(22)
where x −i denotes the mixed strategies of all players other than i.Because ∆(S i ) is compact and U i is continuous and linear in x i (hence concave), the maximum exists and BR i (x −i ) is nonempty, convex, and compact.Now we define the overall best-response correspondence:
BR(x) = BR 1 (x −1 ) × BR 2 (x −2 ) × • • • × BR N (x −N ),(23)
where x = (x 1 , x 2 , . . ., x N ).The product space ∏ N i=1 ∆(S i ) is compact and convex, and the correspondence BR has nonempty, convex, and compact values and is upper hemicontinuous (due to the continuity and linearity of the expected utilities).Hence, by Kakutani's fixed point theorem, there exists a fixed point
x * = (x * 1 , x * 2 , . . . , x * N ) ∈ N ∏ i=1 ∆(S i )(24)
such that x * ∈ BR(x * ).</p>
<p>(25) This fixed point is a Nash equilibrium in mixed strategies.Thus, under a finite strategy space and continuous utility functions, a Nash equilibrium exists.According to the Debreu-Glicksberg-Fan Theorem (a generalization of Nash's existence theorem), if each player's strategy space is a compact convex set and the utility functions are continuous and quasi-concave (or concave) in their own strategies, then at least one Nash equilibrium exists.</p>
<p>Lemma A.2.By Lemma A.1, Let I = {1, 2, . . ., n} be the set of players.For each player i, let X i ⊂ R d i be a compact, convex strategy set, and define X = ∏ n i=1 X i .Suppose each player's utility function u i : X → R is continuous on X and quasi-concave (or concave) in the coordinate x i for fixed x −i .Then, by the Debreu-Glicksberg-Fan theorem, there exists at least one Nash equilibrium
x * = (x * 1 , x * 2 , . . . , x * n ) ∈ X.
differentiable with continuous gradient), the gradient ∇ θ F is well-defined.The policy gradient update is thus a continuous map from θ k to θ k+1 .</p>
<p>Because the update at each iteration is based on the gradient (which is continuous) of the assistant's utility F(θ), any small perturbation in θ results in only a small variation in both the gradient and the updated parameter.This demonstrates that the update procedure is stable in the sense that the assistant's utility, R A (π θ ) = F(θ), continuously varies under the sequence of policy gradient updates Sutton &amp; Barto (2018).</p>
<p>Thus, since R A is a continuous function of π θ and π θ depends continuously on θ, the assistant's utility is continuous, and the policy gradient update is well-defined.</p>
<p>Then we want to prove that, the checker's feedback strategy can be obtained via convex optimization (e.g., least-squares regression), and thus R C is a convex function.</p>
<p>Lemma A.4.Let F be a convex subset of a vector space, and suppose the checker's feedback strategy is given by a function f ∈ F obtained as a solution to the convex optimization problem
min f ∈F J( f ) = n ∑ i=1 y i − f (x i ) 2 , (34)
where x i are given inputs and y i are target outputs.Define the checker's utility function
R C ( f ) = J( f ). (35)
Then R C is a convex function on F ; that is, for any f 1 , f 2 ∈ F and any λ ∈ [0, 1] it holds that
R C λ f 1 + (1 − λ) f 2 ≤ λR C ( f 1 ) + (1 − λ)R C ( f 2 ).(36)
Proof.We prove the convexity of R C by verifying that the squared-error loss is a convex function and that convex combinations preserve convexity.</p>
<p>For each data point i, consider the function
ℓ i ( f ) = y i − f (x i ) 2 . (37)
Since for any real number t, the function t 2 is convex, and the mapping f → y i − f (x i ) is affine (and hence convex), the composition f → y i − f (x i ) 2 is convex.In particular, for any f 1 , f 2 ∈ F and λ ∈ [0, 1],
ℓ i λ f 1 + (1 − λ) f 2 ≤ λℓ i ( f 1 ) + (1 − λ)ℓ i ( f 2 ).(38)
Since the checker's overall loss (and hence utility function)
R C ( f ) = n ∑ i=1 ℓ i ( f )(39)
is a finite sum of convex functions, it follows that R C is convex on F ; that is, for any
f 1 , f 2 ∈ F and λ ∈ [0, 1], R C λ f 1 + (1 − λ) f 2 = n ∑ i=1 ℓ i λ f 1 + (1 − λ) f 2 ≤ λ n ∑ i=1 ℓ i ( f 1 ) + (1 − λ) n ∑ i=1 ℓ i ( f 2 ) = λR C ( f 1 ) + (1 − λ)R C ( f 2 ).(40)
Thus:
R C ( f ) = n ∑ i=1 y i − f (x i ) 2 is convex on F .(41)
Thus, the checker's feedback strategy derived via convex optimization (e.g., least-squares regression) ensures that the corresponding utility function R C is convex.</p>
<p>A.2.3 The formal proof of theorem 3.1</p>
<dl>
<dt>We want to rewrite the theorem 3.1 into the following lemma based on the former lemmas: Lemma A.5 (theorem 3.1).Suppose the assistant's strategy space Π and the checker's strategy space F are nonempty, compact, and convex sets.Assume that: R A</dt>
<dd>Π × F → R is continuous in π for fixed f , R C : Π × F → R is convex in f for fixed π. Then there exists a Nash equilibrium π * θ , f * defined by π * θ = arg max π∈Π R A (π, f * ) and f * = arg max f ∈F R C (π * θ , f ).(42)
Proof.For each fixed f ∈ F define the assistant's best-response set:
BR A ( f ) = {π ∈ Π : R A (π, f ) = max π ′ ∈Π R A (π ′ , f )}. (43)
By hypothesis (i) and the compactness of Π, the maximum is attained and BR A ( f ) is nonempty.(Continuity of R A assures that small changes in f lead to small changes in R A (•, f ).)</dd>
</dl>
<p>Similarly, for each fixed π ∈ Π define the checker's best-response set:
BR C (π) = { f ∈ F : R C (π, f ) = max f ′ ∈F R C (π, f ′ )}. (44)
Since hypothesis (ii) implies that R C is convex in f and F is convex and compact, we conclude that BR C (π) is nonempty, convex, and closed.</p>
<p>Consider the correspondence
H : Π × F → Π × F , H(π, f ) = BR A ( f ) × BR C (π).(45)(π * , f * ) ∈ Π × F such that (π * , f * ) ∈ H(π * , f * ).(46)
By the definition of H, the fixed point satisfies
π * ∈ BR A ( f * ) and f * ∈ BR C (π * ),(47)
which is equivalent to
π * = arg max π∈Π R A (π, f * ) and f * = arg max f ∈F R C (π * , f ).(48)
Thus, no unilateral deviation by either the assistant or the checker increases the respective utility; in other words, the strategy pair (π * , f * ) is a Nash equilibrium.</p>
<p>B Detailed Dataset Information</p>
<p>(1) Operating systems (OS) Integrating LLMs into operating systems has immense potential for automating and optimizing tasks.This integration requires a secure, user-friendly interface to ensure effective LLM-OS interaction.LLMs must accurately understand the OS context for informed operations, prioritizing safety to prevent misuse.Additionally, the system should effectively handle errors and provide clear feedback to users, enhancing overall interaction and control.Addressing these aspects can transform computer interaction and efficiency across various industries.</p>
<p>(2) Database (DB) The ability of LLMs to operate on real databases via SQL is critical due to the importance and complexity of database analysis in everyday activities.</p>
<p>Previous research has highlighted the effectiveness of LLMs in automating database access, such as with T5QL, a new SQL generation method.Furthermore, fine-tuned LLMs (like GPT-3.5)have demonstrated the ability to extract and link complex scientific information from texts, obtaining structured knowledge from unstructured text and building extensive databases.</p>
<p>(3) WebShop (WS) WebShop is an innovative simulation of an e-commerce website environment, featuring 1.18 million real-world products and 12,087 crowd-sourced text instructions.It challenges agents to navigate various types of webpages and perform diverse actions to find, customize, and purchase products based on given instructions.WebShop's challenges include understanding compositional instructions, query (re-)formulation, dealing with noisy text on webpages, and strategic exploration.</p>
<p>(4) Knowledge Graphs (KG) LLMs' utilization in constructing and interacting with knowledge graphs (KG) presents a promising opportunity to enhance semantic understanding and information retrieval.This involves assessing the models' ability to generate and interpret complex interrelations within data, facilitating more intuitive and context-aware responses.Effective LLM performance in this domain could significantly improve AI's reasoning and decision-making capabilities based on structured knowledge.</p>
<p>(5) Mind2Web (M2W) Mind2Web (M2W) is a dataset designed for developing web agents capable of performing complex tasks on real websites via language instructions.It features over 2,000 tasks across 137 sites from 31 domains.M2W's real web environments and diverse user interactions make it a crucial platform for advancing AI navigation capabilities.</p>
<p>(6) ALFWorld (ALF) ALFWorld bridges interactive TextWorld environments with embodied tasks from the ALFRED dataset, enabling agents to learn abstract strategies and apply them to real-world tasks.It supports abstract reasoning and concrete execution, allowing agents to plan actions in a text-based simulator and execute these tasks in a visual environment.This approach enhances agent generalization and problem-solving skills across various domains, such as language understanding and visual navigation, by leveraging a modular design that simplifies research improvements.</p>
<p>C Comparison with Task-Splitting Agents</p>
<p>To compare the SAGE framework with existing agents across various task types and datasets, we conducted a comprehensive evaluation.The results are detailed in Table 6.SAGE demonstrates superior performance in task completion time, answer accuracy, dialog coherence, and step completion accuracy, highlighting its effectiveness in managing complex, multifaceted tasks.</p>
<p>), RepoBench-PLiu et al. (2023a)) and reasoning abilities(HotpotQA Yang et al. (2018b), TriviaQA Joshi et al. (2017b)).4.1 Evaluation onAgentBench Task Description AgentBench includes scenarios from CODE (Knowledge Graph, OS, DB), GAME (ALFWorld) Shridhar et al. (2021), and WEB (WebShop Yao et al. (2023a), Mind2Web Deng et al. (2023)).For more details for the datasets and benchmarks, see Appendix B. Baselines We evaluate commercial models GPT-3.5 Brown et al. (2020) and GPT-4 OpenAI et al. (2024), and open-source models Llama2 Touvron et al. (2023), Codellama Rozière et al. (</p>
<p>Figure 3 :
3
Figure 3: Execution results across six tasks (CLE: Context Limit Exceeded, TLE: Task Limit Exceeded).Task limits are the main cause of incomplete tasks, highlighting LLM agents' limitations under time constraints.</p>
<p>Table 1 :
1
Baseline and SAGE Framework Performance on AgentBench
LLM TypeModelVEROSDBKGALFWSM2WBaseSAGEBaseSAGEBaseSAGEBaseSAGEBaseSAGEBaseSAGEBaseSAGEAPIGPT-4 GPT-3.542.4 31.649.7 38.332.0 15.739.8 35.657.4 25.963.1 37.678.0 17.082.0 23.067.1 64.167.8 72.127.0 16.032.0 28.027.0 16.032.0 28.0Llama2-7B Chat0.08.40.010.20.025.00.05.04.410.40.015.00.015.0CodeLlama-7B Instruct5.718.42.619.20.027.00.012.516.340.20.015.015.015.0OSSQwen1.8B Chat2.718.71.415.16.845.30.010.56.611.40.613.613.613.6Qwen-7B Chat5.622.24.818.00.048.034.038.50.013.60.015.015.015.0ChatGLM2-6B v1.10.015.20.016.30.017.00.05.00.310.3</p>
<p>Table 2 :
2
Evaluation of SAGE and Baseline Models on Three Different Tasks
AgentTask Completion Time (min)Answer Accuracy Dialog Coherence Step Completion (QA)(%) (%) Accuracy (%)GPT-3.5 (Baseline)Long-form QA (HotpotQA)54.1%48.5%62.7%GPT-4 (Baseline)Long-form QA (HotpotQA)61.2%53.8%68.2%Llama2-7b (Baseline)Multi-turn Dialog (MultiWOZ)55.9%50.1%64.8%Codellama-13b (Baseline) Multi-turn Dialog (MultiWOZ)58.4%52.3%66.7%Mistral-7b (Baseline)Sequential Task (ALFWorld)56.5%51.5%65.1%SAGE-GPT-3.5Long-form QA (HotpotQA)74.9% (+20.8%)68.3% (+19.8%)80.6% (+17.9%)SAGE-GPT-4Long-form QA (HotpotQA)78.4% (+17.2%)73.4% (+19.6%)83.9% (+15.7%)SAGE-Llama2-7bMulti-turn Dialog (MultiWOZ) 72.2% (+16.1%)67.9% (+17.8%)78.5% (+13.7%)SAGE-Codellama-13bMulti-turn Dialog (MultiWOZ) 74.7% (+16.3%)71.2% (+18.9%)81.2% (+14.5%)SAGE-Mistral-7bSequential Task (ALFWorld)73.8% (+17.3%)70.5% (+19.0%)79.9% (+14.8%)
Yang et al. (2018a)ng the next line of code from a few initial lines, with Precision, Recall, and F1 as metrics.RepoBench-PLiu et al. (2024)tests retrieval of relevant code snippets from cross-file and within-file contexts to predict the next line, also evaluated with Precision, Recall, and F1.HotPotQAYang et al. (2018a), a Wikipedia-based dataset with 113k question-answer pairs, challenges the agent to reason across multiple documents, evaluated by answer F1.TriviaQAJoshi et al. (</p>
<p>Table 3 :
3
Comparison of Performance Across Different Methods
ModelsLCCRepoBench-PHotpotQA TriviaQAPrecision RecallF1Precision RecallF1F1F1Reflexion77.7281.00 79.2878.7381.86 80.2511.2611.23Beam search78.9879.32 79.1278.7581.02 79.8710.2612.13SAGE78.7679.88 79.2979.2783.28 81.2222.0622.76</p>
<p>Table 4
4compares classical lexical retrieval (RAG with BM25) Robertson &amp; Zaragoza (2009),dense passage retrieval (RAG with DPR) Reichman &amp; Heck (2024), a commercial retrievalsolution (RAG with OpenAI Retrieval) OpenAI (2023), the TART Eisenschlos et al. (2022)sequence-to-sequence retrieval model, and the FiD (Fusion-in-Decoder) method Izacard&amp; Grave (2021), all tested on multi-document and open-domain QA tasks (HotpotQA,Natural Questions Kwiatkowski et al. (</p>
<p>Table 4 :
4
Evaluation of different RAG Agents on Different Tasks and Datasets
AgentAccuracy (QA) (%) Latency (ms) Memory Usage (MB)Task 1: Multi-Document QA (HotpotQA)RAG (BM25)60.8121RAG (DPR)66.3129RAG (OpenAI Retrieval)67.4108TART63.2144FiD (Fusion-in-Decoder)70.1153ChatGPT-4 -Sage74.8 (+4.7)128(-50%)Task 2: Document Retrieval for Contextual Answering (Natural Questions)RAG (BM25)59.9125RAG (DPR)65.5131RAG (OpenAI Retrieval)66.8113TART62.4146FiD (Fusion-in-Decoder)69.8156ChatGPT-4 -Sage73.6 (+3.8)131(-49%)Task 3: Open-Domain QA with Multiple Contexts (TriviaQA)RAG (BM25)62.1124RAG (DPR)67.8129RAG (OpenAI Retrieval)68.9117TART64.7148FiD (Fusion-in-Decoder)71.9155ChatGPT-4 -Sage75.5 (+3.6)134(-47%)</p>
<p>Table 5 :
5
Ablation study for memory optimization on the task of AgentBench
ModelsOS DBKG ALF WS M2WQwen-1.8B (w/o memo)10.4 22.6 6.80.026.65.0Qwen-1.8B (w memo)18.7 28.3 45.3 10.5 31.425.1Codellama-7B (w/o memo) 9.72.70.00.014.35.0Codellama-7B (w memo)23.4 41.3 48.0 12.5 58.715.0</p>
<p>Table 6 :
6
Evaluation of Task-Splitting SOTA Agents on Different Task Types and Datasets
AgentTask Completion Answer Accuracy Dialog Coherence Step Completion Time (min) (QA)(%) (%) Accuracy (%)Task 1: Long-form Question Answering (HotpotQA)BabyAGI7.874.1--AutoGPT8.176.3--HuggingGPT6.779.2--LangChain Agent7.977.0--TaskMatrix.AI6.580.3--SAGE -ChatGPT-410.682.5 (+6.0)--Task 2: Multi-turn Dialog Completion (MultiWOZ)BabyAGI8.2-73.5-AutoGPT8.5-75.6-HuggingGPT7.1-78.1-LangChain Agent8.3-76.5-TaskMatrix.AI6.8-79.9-SAGE -ChatGPT-410.9-81.6 (+5.8)-Task 3: Sequential Task with Dependencies (ALFWorld)BabyAGI8.7--71.5AutoGPT8.3--73.1HuggingGPT7.5--75.2LangChain Agent7.8--72.8TaskMatrix.AI6.6--76.4SAGE -ChatGPT-410.7--82.3 (+5.9)
Proof.For each player i, define the best-response correspondenceSince u i is continuous and X i is compact, the maximum is attained; hence, BR i (x −i ) ̸ = ∅.The quasi-concavity (or concavity) of u i in x i ensures that the setis convex for any α ∈ R. In particular, the set BR i (x −i ) is convex for every fixed x −i .The continuity of u i implies that the best-response correspondence BR i is upper hemicontinuous.This means that for any sequence {x (k)−i } converging to x −i and any corresponding sequence {xNow we define the aggregate best-response correspondence:where(3) The correspondence BR is upper hemicontinuous.By the Debreu-Glicksberg-Fan fixed point theorem, there exists a fixed pointThis means that no player can unilaterally deviate from x * and increase their utility, which is precisely the definition of a Nash equilibrium.Next, based on the above Lemmas, we want to prove that the assistant's policy π θ is updated using policy gradient methodsSutton &amp; Barto (2018).Since R A is a continuous function of π θ , the assistant's utility is continuous.Lemma A.3.Let Θ denote the parameter space for the assistant's policy π θ , and let the assistant's utility be given by Rwhere f : Π → R is a continuous function on the space Π of policies.Assume that the mapping θ → π θ is continuous.Then the composite functionis continuous in θ.Moreover, since the assistant's policy is updated using policy gradient methods, the update rule) employs the gradient of a continuous function.Proof.We divide the proof into two parts:Step 1: Continuity of the Utility Function.Since the mapping θ → π θ is continuous and the function f : Π → R is continuous by hypothesis, their composition F(θ) = f π θ is continuous in θ.Formally, for any ϵ &gt; 0, there exists a δ &gt; 0 such that for all θ, θStep 2: Policy Gradient Updates.In the policy gradient framework, the assistant updates its parameters according to the rule:where α k &gt; 0 is a (possibly diminishing) step size and ∇ θ F(θ k ) denotes the gradient of F evaluated at θ k .Since F is continuous (and under typical smoothness assumptions,D ExamplesCases study on TriviaQA in using GPT with SAGE.User:Answer the question based on the given passages.Only give me the answer and do not output any other words.The following are given passages: History of canals in Great Britain -London Canal Museum History of canals in Great Britain Home &gt; History &gt; Canals in Britain In the history of canals, Britain was not a pioneer.The Chinese can claim that the Grand Canal of China was one of the first, in the tenth century,a. . . . . .Answer the question based on the given passages.Only give me the answer and do not output any other words.Question: Which metal is produced by the Bessemer Process?Answer:Assistant:The content of the file seems to be an HTML document related to the administrative division of Turkey, including population statistics, charts, and maps.This file contains scripts, metadata, and structured data typical for a web page.Checker:Clear Objectives: When answering user questions, the focus should be on directly addressing the question without introducing irrelevant information.Document Handling: Before confirming whether the document content is relevant to the question, detailed aspects of the document should not be directly addressed.Reflection:Issue Description: The user wants to understand why the system outputs irrelevant HTML document content based on user input and seeks detailed feedback and reflection.Reasons for the Error: Misinterpretation of Input: The system mistakenly assumed that the user uploaded a file and attempted to read its content, whereas the user only provided text input.Processing Logic Error: Without clearly determining the user's intent, the system directly parsed and outputted file content.User Experience Problems: Irrelevant Output: The user expects concise, relevant answers, but the system provided lengthy and irrelevant HTML content, affecting the user experience.Poor Communication: Failure to accurately understand and respond to the user's real needs resulted in off-topic responses In-depth Reflection and Improvement Suggestions: Clarify User Intent: Accurate Parsing of User Input: Enhance the system's natural language processing capabilities to accurately distinguish whether the user uploaded a file or simply provided text input.User Feedback Mechanism: Implement a user feedback mechanism to promptly adjust the understanding of user intent.For example, confirm the issue to verify if the user uploaded a file.Optimize Processing Logic: Layered Processing Flow: In system design, separate file processing logic from text response logic to avoid misjudgments.For instance, when a user uploads a file, process the file content separately; when a user inputs text, provide relevant answers directly.Pre-processing Check: Prior to handling files, perform pre-processing checks to confirm if the file content meets expectations. . . . . . .LLMs with SAGE response: Steel.Ground truthSteel.Cases study on LCC in using GPT with SAGE.User: import networkx as nx import pickle from Queue import PriorityQueue import copy import random import string import sys class MultiDiGraphEdgeKey(nx.MultiDiGraph): . . . . . .Assistant:You will be given the history of a past experience where you were given a task and failed to complete it.Reflect on the strategy and actions taken.Checker:The given code is a part of the Camerini algorithm, which is used for ranking edges in a directed graph.This algorithm is particularly useful for problems involving the identification of optimal or near-optimal substructures, like arborescences or spanning trees, in complex networks.The algorithm has two main phases:Collapsing Phase: This phase identifies and collapses cycles in the graph, creating supernodes.This simplification helps in managing cycles more efficiently and prepares the graph for the expansion phase.Expanding Phase: After collapsing cycles into supernodes, this phase expands the supernodes back into their original structure, resulting in a ranked arborescence, where the edges are sorted or prioritized based on certain criteria. . . . . . .LLMsAssistant:To devise a new plan of action, let's analyze the previous mistakes and come up with a solution: Mistake 1: In the KadRefreshOperation class, the execute() method calls the BucketRefreshOperation and ContentRefreshOperation without any error handling or exception handling.This can lead to unexpected errors and failures.Solution: Add proper error handling and exception handling in the execute() method of the KadRefresh-Operation class.This will ensure that any errors or exceptions are properly handled and the execution can continue smoothly. . . . . .Checker:In the 'KadRefreshOperation' class, there is no error handling when the 'execute()' method calls 'BucketRefreshOperation' and 'ContentRefreshOperation'. Analysis: If 'BucketRefreshOperation' or 'ContentRefreshOperation' throws an exception, the entire 'execute()' method will fail, potentially causing the interruption of the entire system's refresh operation.In the 'StoreOperation' class, the 'execute()' method does not handle the case of existing content in the DHT.Analysis: The 'execute()' method always returns 1 as the number of storage nodes, which is clearly incorrect.
Long-form question answering: An iterative planning-retrieval-generation approach. Pritom Saha Akash, Kashob Kumar Roy, Lucian Popa, Kevin Chen, -Chuan Chang, 2023</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, 2023Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, 2020Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. Language models are few-shot learners</p>
<p>Elements of Information Theory. Thomas M Cover, Joy A Thomas, Wiley Series in Telecommunications and Signal Processing. 2006ISBN 0471241954</p>
<p>MuTual: A dataset for multiturn dialogue reasoning. Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, Ming Zhou, 10.18653/v1/2020.acl-main.130Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, 2023</p>
<p>Über das gedächtnis: untersuchungen zur experimentellen psychologie. Hermann Ebbinghaus, Duncker &amp; Humblot1885</p>
<p>TART: Retrieval-augmented generation for task-oriented dialog. Julian Eisenschlos, Yury Zemlyanskiy, Jason Phang, Bhuwan Dhingra, William W Cohen, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL). the 60th Annual Meeting of the Association for Computational Linguistics (ACL)2022</p>
<p>The state of the art in open domain complex question answering: a survey. Romina Etezadi, Mehrnoush Shamsfard, Applied Intelligence. 5342023</p>
<p>Game Theory. Drew Fudenberg, Jean Tirole, ; Team, Glm , Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang, 1991. 2024MIT PressCambridge, MAChatglm: A family of large language models from glm-130b to glm-4 all tools</p>
<p>Hybrid computing using a neural network with dynamic external memory. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi Ńska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Nature. 53876262016</p>
<p>Longcoder: A long-range pre-trained language model for code completion. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, Julian Mcauley, 2023</p>
<p>Enhancing intent understanding for ambiguous prompt: A human-machine co-adaption strategy. Yangfan He, Jianhui Wang, Kun Li, Yijin Wang, Li Sun, Jun Yin, Miao Zhang, Xueqian Wang, 2024Available at SSRN 5119629</p>
<p>Enhancing intent understanding for ambiguous prompts through human-machine co-adaptation. Yangfan He, Jianhui Wang, Kun Li, Yijin Wang, Li Sun, Jun Yin, Miao Zhang, Xueqian Wang, arXiv:2501.151672025arXiv preprint</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 2022</p>
<p>Leveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL). the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL)2021</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. Regina Barzilay, Min-Yen Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 2017a1</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, 2017b</p>
<p>Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement. Wouter Kool, Herke Van Hoof, Max Welling, 2019</p>
<p>Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019</p>
<p>Falcon: Feedback-driven adaptive long/short-term memory reinforced coding optimization system. Zeyuan Li, Yangfan He, Lewei He, Jianhui Wang, Tianyu Shi, Bin Lei, Yuchen Li, Qiuwu Chen, arXiv:2410.213492024arXiv preprint</p>
<p>Cmat: A multi-agent collaboration tuning framework for enhancing small language models. Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, Jingsong Yang, arXiv:2404.016632024arXiv preprint</p>
<p>Repobench: Benchmarking repository-level code auto-completion systems. Tianyang Liu, Canwen Xu, Julian Mcauley, 2023a</p>
<p>Repobench: Benchmarking repositorylevel code auto-completion systems. Tianyang Liu, Canwen Xu, Julian Mcauley, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, arXiv:2308.03688Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. 2023barXiv preprint</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Introducing the LCC metaphor datasets. Michael Mohler, Mary Brunson, Bryan Rink, Marc Tomlinson ; Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). Stelios Odijk, Piperidis, the Tenth International Conference on Language Resources and Evaluation (LREC'16)Asuncion Moreno; Portorož, SloveniaEuropean Language Resources Association (ELRAJan. May 2016</p>
<p>Babyagi: An ai agent that can achieve goals and execute tasks. Yohei Nakajima, OpenAI. Openai cookbook. 2024. 2023</p>
<p>Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Ashvin Mishkin ; Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Kyla Perelman, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Wojciech Wong ; Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. Felipe Petroski Such. Michelle Pokrass, H Vitchyr, Tolly Pong, Alethea Powell, Boris Power, Elizabeth Power, Raul Proehl, Alec Puri, Jack Radford, Aditya Rae, Cameron Ramesh, Francis Raymond, Kendra Real, Carl Rimbach, Bob Ross, Henri Rotsted, Nick Roussez, Mario Ryder, Ted Saltarelli, Shibani Sanders, Girish Santurkar, Heather Sastry, David Schmidt, John Schnurr, Daniel Schulman, Selsam, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,; Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David; Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny; Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek; Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason; Lauren Workman; Sarah Yoo, Kevin Yu2024Filipe de Avila Belbute Peres ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun VijayvergiyaSherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, Memgpt: Towards llms as operating systems. 2024</p>
<p>Tfb: Towards comprehensive and fair benchmarking of time series forecasting methods. Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S Jensen, Zhenli Sheng, Bin Yang, Proc. VLDB Endow. VLDB Endow2024</p>
<p>Easytime: Time series forecasting made easy. Xiangfei Qiu, Xiuwen Li, Ruiyang Pang, Zhicheng Pan, Xingjian Wu, Liu Yang, Jilin Hu, Yang Shu, Xuesong Lu, Chengcheng Yang, Chenjuan Guo, Aoying Zhou, Christian S Jensen, Bin Yang, ICDE. 2025a</p>
<p>Duet: Dual clustering enhanced multivariate time series forecasting. Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, Bin Yang, SIGKDD. 2025b</p>
<p>Anna Jack W Rae, Potapenko, M Siddhant, Timothy P Jayakumar, Lillicrap, arXiv:1911.05507Compressive transformers for long-range sequence modelling. 2019arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Dense passage retrieval: Is it retrieving?. Benjamin Reichman, Larry Heck, 2024</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, 10.1561/1500000019Foundations and Trends® in Information Retrieval. 1554-0669342009</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Simultaneity, sequentiality, and speed: Organizational messages about multiple-task completion. Keri K Stephens, Jaehee K Cho, Dawna I Ballard, Human Communication Research. 3812012</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, 2018MIT PressCambridge, MA2 edition</p>
<p>Toward self-improvement of llms via imagination, searching, and criticizing. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu, 2024</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Unsupervised multilingual word embedding with limited resources using neural language models. Takashi Wada, Tomoharu Iwata, Yuji Matsumoto, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Auto-gpt for online decision making: Benchmarks and additional opinions. Hui Yang, Sifu Yue, Yunzhong He, 2023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober-November 2018a</p>
<p>Hotpotqa: A dataset for diverse, explainable multihop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, 2018b</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, 2023a</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
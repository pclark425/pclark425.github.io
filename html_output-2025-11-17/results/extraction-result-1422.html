<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-2f4c451922e227cbbd4f090b74298445bbd900d0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2f4c451922e227cbbd4f090b74298445bbd900d0" target="_blank">Elucidating the Design Space of Diffusion-Based Generative Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The theory and practice of diffusion-based generative models are currently unnecessarily convoluted and the design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model.</p>
                <p><strong>Paper Abstract:</strong> We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elucidated Diffusion Models (this paper's design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular diffusion-based generative modeling design that disentangles sampling, preconditioning, and training choices to optimize fidelity and sampling efficiency; includes specific sampler, preconditioning, loss-weighting and training-distribution choices producing state-of-the-art FIDs with far fewer network evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Elucidated Diffusion Models (EDM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A score-based / denoising diffusion generative model family in which a neural network D_theta (implemented via a preconditioned raw network F_theta) models the denoiser/score at varying Gaussian noise levels sigma; sampling is performed by solving the probability-flow ODE (or its stochastic Langevin-augmented variant) with a carefully chosen time schedule sigma(t)=t, preconditioning functions c_in/c_out/c_skip/c_noise, and either a 2nd-order Heun integrator (deterministic) or a tailored churn-based stochastic sampler.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent-free score-based generative model (diffusion model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image generation (CIFAR-10, ImageNet-64, FFHQ, AFHQv2) and general generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Fréchet Inception Distance (FID) between generated and real images</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>State-of-the-art reported: CIFAR-10 conditional FID = 1.79 and unconditional FID = 1.97 (32×32, with 35 NFE); ImageNet-64 retrained to FID = 1.36 (64×64). Also improved previously-trained ImageNet-64 model from FID 2.07 → 1.55 by sampler changes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Mostly a black-box neural denoiser; some interpretability arises from (1) analytic denoiser visualization (Figure 1) showing denoised targets as function of sigma, and (2) analysis of ODE solution trajectories (Figure 3) which makes sampling paths and curvature interpretable; no claimed explicit disentangled latent semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of denoiser outputs across noise levels, analysis/visualization of ODE trajectories and curvature; use of probability-flow ODE enables latent inversion (mapping images to noise latents and back). No symbolic or explicit feature-level interpretability methods described.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Sampling costs reported in terms of network evaluations (NFE): high-quality sampling with EDM achieved 35 NFE per image for CIFAR-10 (32×32) and 79 NFE for 64×64; measured throughput ~26.3 images/s on a single NVIDIA V100 for CIFAR-10 with their setup. Training costs not precisely enumerated in FLOPs or GPU-hours; project consumed ~250 MWh on in-house V100 cluster (total training & experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to prior samplers/models, EDM sampling reduces NFE dramatically: sampling improvements alone gave ~7.3× speed-up for VP, ~300× for VE, and ~3.2× for DDIM style baselines (reported by paper). EDM + training improvements produced better FID with far fewer NFE than prior state-of-the-art.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Generative quality measured by FID: CIFAR-10 conditional 1.79, unconditional 1.97; ImageNet-64 retrained 1.36 (64×64). These numbers indicate top-tier perceptual/sample quality relative to prior diffusion/GAN baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High fidelity (low FID) aligns with sampling and training choices: improved samplers reduce NFE while maintaining or improving FID; improved preconditioning and loss weighting improved training stability and quality, sometimes removing the need for stochastic sampling (dataset-dependent). For simpler datasets (CIFAR-10) deterministic sampling with EDM training suffices; for more diverse datasets (ImageNet-64) limited stochasticity still improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Key trade-offs documented: (1) higher-order integrators (Heun) require extra D_theta evaluations per step but reduce truncation error, giving better NFE-to-quality tradeoff; (2) adding stochasticity (Langevin churn) can correct earlier step errors but excessive stochasticity degrades detail and colors, requiring heuristics and per-model tuning; (3) aggressive step allocation (rho) shortens steps near low sigma improving quality but may raise cost elsewhere; (4) preconditioning choices improve training robustness at some implementation complexity cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Principal choices affecting fidelity/interpretability/efficiency/utility: sigma(t)=t and s(t)=1 (trajectory-linearization), Heun 2nd-order integrator for deterministic sampling, churn-based stochastic sampler with parameters {S_churn, S_min, S_max, S_noise} tailored per model, step schedule sigma_i = (A i + B)^rho with rho=7, preconditioning D_theta = c_skip(σ) x + c_out(σ) F_theta(c_in(σ) x; c_noise(σ)), loss weighting lambda(σ)=1/c_out(σ)^2 (and final formula used: lambda = (σ^2 + σ_data^2)/(σ·σ_data)^2 as in Table 1), and log-normal training sampling p_train(σ) to prioritize mid-sigma range.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>EDM evaluated against prior diffusion frameworks (VP, VE, DDIM, iDDPM) and samplers (Euler, predictor-corrector, Euler–Maruyama): EDM's sampler+training pipeline achieves lower FID at far fewer NFEs (examples: turned a model FID 2.07 → 1.55 via sampler changes, and retraining + EDM improved to 1.36). Compared to black-box adaptive RK45 solver, EDM's fixed schedule + Heun was more cost-effective in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>The paper advocates the following practical optimum for image synthesis: use sigma(t)=t and s(t)=1; Heun 2nd-order integrator; step schedule with rho ≈ 7 to concentrate steps near low sigma; preconditioning functions as in Table 1 (c_skip = σ_data^2/(σ^2+σ_data^2), c_out = σ·σ_data/√(σ_data^2+σ^2), c_in = 1/√(σ^2+σ_data^2), c_noise = (1/4) ln(σ)); loss weighting λ(σ)=1/c_out(σ)^2 and log-normal p_train(σ) with parameters given (P_mean, P_std). They also recommend dataset-dependent use of stochastic churn (often unnecessary after their training improvements for small datasets but useful for diverse datasets like ImageNet).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Elucidating the Design Space of Diffusion-Based Generative Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probability-flow ODE (score-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probability-flow ordinary differential equation for score-based generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ODE formulation (from score-based generative modeling) whose solution trajectories map Gaussian-noised samples to data samples by following the score ∇_x log p(x;σ); used here as the basis for deterministic sampling and trajectory analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Score-based generative modeling through stochastic differential equations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Probability-flow ODE / score-based model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Continuous-time ODE d x = -dot{σ}(t) σ(t) ∇_x log p(x;σ(t)) dt (generalized with optional scaling s(t)) that evolves samples so that marginals at time t match p(x;σ(t)); the score is implemented approximately by a neural denoiser D_θ, enabling deterministic inversion and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>continuous-time implicit world model / generative dynamical model (score-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image generation and analysis of sampling trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>FID of final generated samples (indirectly reflects quality of the ODE-based sampling trajectory and score approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Used as foundational model; when combined with EDM samplers and networks achieves FIDs reported above (e.g., CIFAR-10 1.79 conditional after full pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides interpretable continuous trajectories between noise and data; enables inversion (map image to noise) and visualization of how denoiser targets shift with σ, giving some insight into how the model represents data at different noise scales. Nevertheless the internal score estimator (neural network) remains a black box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of ODE solution trajectories and curvature (Figure 3), analytical denoiser visualizations (Figure 1), and use of ODE invertibility to analyze latent paths.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Solving the ODE numerically requires repeated D_theta evaluations; cost measured as NFE. Choice of solver affects per-step cost: Euler (1 eval/step), Heun (2 evals/step), adaptive RK45 (multiple evals), with trade-offs in truncation error vs NFE.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Deterministic ODE sampling with their schedule + Heun outperforms Euler-based solvers in NFE-to-quality tradeoff; adaptive RK45 was more expensive overall in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used with EDM components and Heun solver, ODE-based deterministic sampling achieves high quality (see EDM FIDs), with deterministic sampling sometimes outperforming stochastic variants under improved training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The probability-flow ODE is useful when deterministic inversion (latent mapping) is desired and when the trained denoiser is sufficiently accurate; however, stochastic sampling (SDE-style) can improve final sample quality by correcting discretization/estimation errors, especially for diverse datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Deterministic ODE sampling is simpler and enables inversion but tends to yield worse sample quality than stochastic SDE sampling in some setups; higher-order discrete ODE solvers reduce error but increase per-step eval cost; the best choice depends on network accuracy and dataset diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper recommends choosing sigma(t)=t and s(t)=1 (makes ODE simpler and trajectories more linear), and using a discretization schedule concentrating steps near low sigma (rho≈7). Solver choice (Heun) is recommended for favorable truncation-error vs NFE tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to SDE/predictor-corrector or Euler–Maruyama discretizations, the probability-flow ODE with Heun and appropriate schedule achieved competitive or better FID using fewer NFEs for several evaluated models/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends using sigma(t)=t, s(t)=1, a Heun 2nd-order integrator with the step schedule sigma_i = (A i + B)^rho (rho≈7), and treating D_theta as black-box score estimator; use stochastic churn only when beneficial per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Elucidating the Design Space of Diffusion-Based Generative Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heun sampler (Alg.1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic Heun 2nd-order sampler (Algorithm 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic sampling algorithm using the 2nd-order Heun (explicit trapezoidal) method to integrate the probability-flow ODE; trades one extra neural evaluation per step for O(h^3) local error and substantially better NFE-to-quality performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Heun-based deterministic sampler</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Numerical integrator that at each step evaluates D_theta at current t and at an Euler-predicted x_{i+1} to form a trapezoidal/corrector update (two D_theta evaluations per step except when stepping to σ=0 where Euler is used).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>deterministic numerical solver for score-based generative model dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>sampling/generation in diffusion models (image generation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>FID vs number of neural function evaluations (NFE)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>When combined with EDM time schedule and preconditioning: reached same FID as Euler with considerably lower NFE; example: overall pipeline achieves CIFAR-10 FID 1.79 with 35 NFE.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Method is a numerical integrator — interpretable in numerical-analysis sense; does not change interpretability of underlying neural world model beyond enabling smoother trajectories and more faithful ODE following.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not applicable beyond inspecting trajectory curvature and truncation error; paper analyzes curvature to motivate schedule choice.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Costs roughly 2 neural evaluations per step (one extra than Euler) but reduces number of steps needed; overall sampling NFE decreased (e.g., 35 NFE for CIFAR-10 with Heun + schedule).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Heun provides better truncation-error vs NFE tradeoff than Euler; outperformed adaptive RK45 in wall-clock/qualified cost in the authors' experiments due to solver overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improved sample quality per NFE compared to Euler-based samplers; numerical improvements summarized in Figures/Tables (see Table 3 and plots).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Heun reduces discretization bias, enabling fewer steps at equivalent or better FID — directly improves sampling efficiency and utility in real deployments where D_theta evals dominate cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Extra per-step cost (one more D_theta eval) vs fewer total steps; Heun recommended because net NFE decreased for comparable quality. Stepping to σ=0 requires fallback to Euler to avoid division by zero.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use Heun 2nd-order correction; apply correction except when next σ==0; pair with sigma(t)=t and step schedule concentrating steps near low σ (rho≈7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Euler and adaptive RK45, Heun provided the best practical tradeoff in authors' tests: fewer NFEs to reach high-quality FID than Euler, and lower overhead than RK45 for fixed-budget sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Use Heun with sigma(t)=t, s(t)=1, sigma_i schedule with rho≈7 and clamp scheme for σ→0; choose N (steps) to match desired NFE budget (authors report excellent results with 35 steps on CIFAR-10).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Elucidating the Design Space of Diffusion-Based Generative Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1422.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1422.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Churn stochastic sampler (Alg.2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic churn sampler with controlled Langevin-like noise injection (Algorithm 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tailored stochastic sampling procedure that alternates explicit Langevin-like noise 'churn' (temporary increase of noise level) with an ODE step (using Heun/Euler), governed by per-step churn parameters to trade off error correction and detail preservation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Churn-based stochastic sampler</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>At each step, temporarily raise the current noise level by factor γ_i (clamped), add Gaussian noise with adjustable amplitude S_noise, evaluate D_theta at the increased-noise state, then take an ODE step back to the next σ; parameters S_churn, S_min, S_max, S_noise control where and how much stochasticity is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>stochastic sampler for score-based generative models (hybrid SDE/ODE procedure)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image generation; used to improve sample fidelity when deterministic sampling underperforms</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>FID vs NFE and ablation over churn parameters</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>When optimally tuned for pre-trained models: improved ImageNet-64 pre-trained ADM model from FID 2.07 → 1.55 (Table 4). For various datasets, Algorithm 2 with optimal settings yielded best FIDs in stochastic sampling experiments (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Adds stochastic perturbations; interpretability similar to deterministic ODE but less straightforward due to stochasticity; authors analyze detrimental effects (loss of detail, color drift) empirically and attribute them to nonconservative behavior of practical denoisers.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Empirical ablations and visualization of generated samples and per-sigma denoiser behavior (Figures 4, 5); no explicit explainability method for the stochastic corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Per step cost similar to deterministic Heun when 2nd-order correction is used; additional sampling overhead from random draws and possible extra steps for tuned settings; NFE budgets reported (e.g., optimal settings often require more NFEs than deterministic low-NFE runs but yield better FID for some models).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>When appropriately tuned, stochastic churn provides better final FID than purely deterministic sampling for some pre-trained models and datasets, but requires per-model hyperparameter search (grid search) and sometimes more NFEs; the authors note stochasticity is dataset- and training-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improved sampler quality especially for previously-trained models: e.g., ImageNet-64 ADM pre-trained model: FID from 2.07 → 1.55 with churn sampler; for VE/VP pre-trained models substantial improvements at low step counts were observed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Stochastic churn helps correct earlier discretization/approximation errors and can thus improve perceptual quality; however, excessive churn degrades fine detail and color fidelity, and it requires heuristics (S_min/S_max/S_churn/S_noise) tuned per model, so its utility depends on the reliability of the denoiser and dataset complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Stochasticity trades error-correction ability for potential detail loss and color drift; requires per-model hyperparameter tuning; effective in improving FID of some models but can be detrimental when model training already targets the right sigma regime (e.g., with EDM training on CIFAR-10 deterministic sampling was preferable).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Churn parameters: set γ_i=min(S_churn/N, sqrt(2)-1) within S_min..S_max; noise amplitude S_noise can be tuned >1 to counteract denoiser over-smoothing; apply churn only in a sigma window to avoid color drift; pair with an ODE integrator for the denoising step.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against Euler–Maruyama, predictor-corrector, and previous stochastic samplers, the churn sampler outperformed prior samplers in the authors' experiments (Figure 4/Table 4) when tuned, particularly at low NFEs. However tuning costs and dataset dependence are noted disadvantages.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend grid-search tuning of {S_churn, S_min, S_max, S_noise} per model; use modest stochasticity in a restricted sigma window, clamp γ_i to not exceed existing noise, and select S_noise slightly >1 to counteract denoiser bias. For well-trained EDM models on small datasets, deterministic sampling may be preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Elucidating the Design Space of Diffusion-Based Generative Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1422.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1422.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preconditioned denoiser (D_theta / F_theta)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preconditioned denoiser parameterization D_theta(x;σ) = c_skip(σ) x + c_out(σ) F_theta(c_in(σ) x; c_noise(σ))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A σ-dependent reparameterization of the denoiser that normalizes input/output magnitudes and provides a skip connection to stabilize training across noise levels, enabling more robust optimization and better final sample quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Preconditioned denoiser architecture (D_theta with raw network F_theta)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Denoiser implemented as a mixture of a σ-dependent scaled identity skip and a scaled neural network output: D_θ(x;σ)=c_skip(σ) x + c_out(σ) F_θ(c_in(σ) x; c_noise(σ)). Choices for c_in/c_out/c_skip and noise-conditioning c_noise are derived to make network input/output targets unit-variance and to minimize amplification of F_θ errors across σ.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural network-based conditional denoiser with σ-dependent preconditioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>training of diffusion score models for image generation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>training loss per-σ, aggregate FID after sampling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Using the proposed preconditioning plus loss-weighting and p_train adjustments led to major FID improvements (Table 2): e.g., moving from baseline to full config produced CIFAR-10 conditional FID improvements culminating at 1.79.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Preconditioning improves the numerical stability and interpretability of what the network must predict: it clarifies the effective training target for F_theta (shown in Eq.8) and keeps network inputs/targets near unit variance, but the learned internal representations remain neural black-box features.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Analytic derivation of effective training target (Eq.8) and empirical plots of initial/final per-σ loss (Figure 5a) used to justify choices; visualization of denoiser outputs also used.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No explicit extra inference cost beyond the underlying F_theta network; training stability improvements allow same-capacity models to train effectively. Parameter counts for F_theta architectures are not specified in the paper; authors report network-architecture tweaks (redistribute capacity) but not absolute sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Preconditioning improved training robustness enabling better final quality without larger networks; combining preconditioning with loss-weighting and log-normal p_train achieved larger FID gains than applying loss-weighting alone (citing smaller improvements in concurrent work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Leads to more stable training and better final FID when used with other design choices; Table 2 demonstrates successive gains as preconditioning, loss function, and augmentation are introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Preconditioning reduces extreme per-σ variance in targets and errors, enabling the model to focus capacity where it matters for perceptual quality (mid σ) and helping training to translate to better sampling quality—thus improving task utility without necessarily increasing model size.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Designing c_skip and c_out to minimize amplification of errors helps high-σ behavior but requires careful derivation; choice of c_noise is empirical. The approach trades some implementation complexity (σ-dependent scalings and conditioning) for stability and quality gains.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Authors recommend c_in(σ)=1/√(σ^2+σ_data^2), c_skip(σ)=σ_data^2/(σ^2+σ_data^2), c_out(σ)=σ·σ_data/√(σ_data^2+σ^2), and c_noise(σ)=(1/4) ln(σ) (Table 1); set loss weight λ(σ)=1/c_out(σ)^2 (or equivalent formula used in experiments) and sample σ from a log-normal p_train targeting mid-σ values.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Previous parameterizations predicted scaled noise n or used simpler normalization; the proposed preconditioning reduces error amplification at large σ and improves robustness versus those earlier choices, especially when combined with the other EDM training/sampling recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Use the preconditioning scalings given in Table 1, set λ(σ)=1/c_out(σ)^2 (or the equivalent derived form), and use a log-normal p_train to emphasize mid-σ training; pair with architecture capacity redistribution (boost high-res layers) and augmentation when data is scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Elucidating the Design Space of Diffusion-Based Generative Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Score-based generative modeling through stochastic differential equations <em>(Rating: 2)</em></li>
                <li>Denoising diffusion probabilistic models <em>(Rating: 2)</em></li>
                <li>Improved denoising diffusion probabilistic models <em>(Rating: 2)</em></li>
                <li>Diffusion models beat GANs on image synthesis <em>(Rating: 2)</em></li>
                <li>Denoising diffusion implicit models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1422",
    "paper_id": "paper-2f4c451922e227cbbd4f090b74298445bbd900d0",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "EDM",
            "name_full": "Elucidated Diffusion Models (this paper's design)",
            "brief_description": "A modular diffusion-based generative modeling design that disentangles sampling, preconditioning, and training choices to optimize fidelity and sampling efficiency; includes specific sampler, preconditioning, loss-weighting and training-distribution choices producing state-of-the-art FIDs with far fewer network evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Elucidated Diffusion Models (EDM)",
            "model_description": "A score-based / denoising diffusion generative model family in which a neural network D_theta (implemented via a preconditioned raw network F_theta) models the denoiser/score at varying Gaussian noise levels sigma; sampling is performed by solving the probability-flow ODE (or its stochastic Langevin-augmented variant) with a carefully chosen time schedule sigma(t)=t, preconditioning functions c_in/c_out/c_skip/c_noise, and either a 2nd-order Heun integrator (deterministic) or a tailored churn-based stochastic sampler.",
            "model_type": "latent-free score-based generative model (diffusion model)",
            "task_domain": "image generation (CIFAR-10, ImageNet-64, FFHQ, AFHQv2) and general generative modeling",
            "fidelity_metric": "Fréchet Inception Distance (FID) between generated and real images",
            "fidelity_performance": "State-of-the-art reported: CIFAR-10 conditional FID = 1.79 and unconditional FID = 1.97 (32×32, with 35 NFE); ImageNet-64 retrained to FID = 1.36 (64×64). Also improved previously-trained ImageNet-64 model from FID 2.07 → 1.55 by sampler changes.",
            "interpretability_assessment": "Mostly a black-box neural denoiser; some interpretability arises from (1) analytic denoiser visualization (Figure 1) showing denoised targets as function of sigma, and (2) analysis of ODE solution trajectories (Figure 3) which makes sampling paths and curvature interpretable; no claimed explicit disentangled latent semantics.",
            "interpretability_method": "Visualization of denoiser outputs across noise levels, analysis/visualization of ODE trajectories and curvature; use of probability-flow ODE enables latent inversion (mapping images to noise latents and back). No symbolic or explicit feature-level interpretability methods described.",
            "computational_cost": "Sampling costs reported in terms of network evaluations (NFE): high-quality sampling with EDM achieved 35 NFE per image for CIFAR-10 (32×32) and 79 NFE for 64×64; measured throughput ~26.3 images/s on a single NVIDIA V100 for CIFAR-10 with their setup. Training costs not precisely enumerated in FLOPs or GPU-hours; project consumed ~250 MWh on in-house V100 cluster (total training & experiments).",
            "efficiency_comparison": "Compared to prior samplers/models, EDM sampling reduces NFE dramatically: sampling improvements alone gave ~7.3× speed-up for VP, ~300× for VE, and ~3.2× for DDIM style baselines (reported by paper). EDM + training improvements produced better FID with far fewer NFE than prior state-of-the-art.",
            "task_performance": "Generative quality measured by FID: CIFAR-10 conditional 1.79, unconditional 1.97; ImageNet-64 retrained 1.36 (64×64). These numbers indicate top-tier perceptual/sample quality relative to prior diffusion/GAN baselines.",
            "task_utility_analysis": "High fidelity (low FID) aligns with sampling and training choices: improved samplers reduce NFE while maintaining or improving FID; improved preconditioning and loss weighting improved training stability and quality, sometimes removing the need for stochastic sampling (dataset-dependent). For simpler datasets (CIFAR-10) deterministic sampling with EDM training suffices; for more diverse datasets (ImageNet-64) limited stochasticity still improves results.",
            "tradeoffs_observed": "Key trade-offs documented: (1) higher-order integrators (Heun) require extra D_theta evaluations per step but reduce truncation error, giving better NFE-to-quality tradeoff; (2) adding stochasticity (Langevin churn) can correct earlier step errors but excessive stochasticity degrades detail and colors, requiring heuristics and per-model tuning; (3) aggressive step allocation (rho) shortens steps near low sigma improving quality but may raise cost elsewhere; (4) preconditioning choices improve training robustness at some implementation complexity cost.",
            "design_choices": "Principal choices affecting fidelity/interpretability/efficiency/utility: sigma(t)=t and s(t)=1 (trajectory-linearization), Heun 2nd-order integrator for deterministic sampling, churn-based stochastic sampler with parameters {S_churn, S_min, S_max, S_noise} tailored per model, step schedule sigma_i = (A i + B)^rho with rho=7, preconditioning D_theta = c_skip(σ) x + c_out(σ) F_theta(c_in(σ) x; c_noise(σ)), loss weighting lambda(σ)=1/c_out(σ)^2 (and final formula used: lambda = (σ^2 + σ_data^2)/(σ·σ_data)^2 as in Table 1), and log-normal training sampling p_train(σ) to prioritize mid-sigma range.",
            "comparison_to_alternatives": "EDM evaluated against prior diffusion frameworks (VP, VE, DDIM, iDDPM) and samplers (Euler, predictor-corrector, Euler–Maruyama): EDM's sampler+training pipeline achieves lower FID at far fewer NFEs (examples: turned a model FID 2.07 → 1.55 via sampler changes, and retraining + EDM improved to 1.36). Compared to black-box adaptive RK45 solver, EDM's fixed schedule + Heun was more cost-effective in practice.",
            "optimal_configuration": "The paper advocates the following practical optimum for image synthesis: use sigma(t)=t and s(t)=1; Heun 2nd-order integrator; step schedule with rho ≈ 7 to concentrate steps near low sigma; preconditioning functions as in Table 1 (c_skip = σ_data^2/(σ^2+σ_data^2), c_out = σ·σ_data/√(σ_data^2+σ^2), c_in = 1/√(σ^2+σ_data^2), c_noise = (1/4) ln(σ)); loss weighting λ(σ)=1/c_out(σ)^2 and log-normal p_train(σ) with parameters given (P_mean, P_std). They also recommend dataset-dependent use of stochastic churn (often unnecessary after their training improvements for small datasets but useful for diverse datasets like ImageNet).",
            "uuid": "e1422.0",
            "source_info": {
                "paper_title": "Elucidating the Design Space of Diffusion-Based Generative Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Probability-flow ODE (score-based)",
            "name_full": "Probability-flow ordinary differential equation for score-based generative modeling",
            "brief_description": "An ODE formulation (from score-based generative modeling) whose solution trajectories map Gaussian-noised samples to data samples by following the score ∇_x log p(x;σ); used here as the basis for deterministic sampling and trajectory analysis.",
            "citation_title": "Score-based generative modeling through stochastic differential equations",
            "mention_or_use": "use",
            "model_name": "Probability-flow ODE / score-based model",
            "model_description": "Continuous-time ODE d x = -dot{σ}(t) σ(t) ∇_x log p(x;σ(t)) dt (generalized with optional scaling s(t)) that evolves samples so that marginals at time t match p(x;σ(t)); the score is implemented approximately by a neural denoiser D_θ, enabling deterministic inversion and sampling.",
            "model_type": "continuous-time implicit world model / generative dynamical model (score-based)",
            "task_domain": "image generation and analysis of sampling trajectories",
            "fidelity_metric": "FID of final generated samples (indirectly reflects quality of the ODE-based sampling trajectory and score approximation)",
            "fidelity_performance": "Used as foundational model; when combined with EDM samplers and networks achieves FIDs reported above (e.g., CIFAR-10 1.79 conditional after full pipeline).",
            "interpretability_assessment": "Provides interpretable continuous trajectories between noise and data; enables inversion (map image to noise) and visualization of how denoiser targets shift with σ, giving some insight into how the model represents data at different noise scales. Nevertheless the internal score estimator (neural network) remains a black box.",
            "interpretability_method": "Visualization of ODE solution trajectories and curvature (Figure 3), analytical denoiser visualizations (Figure 1), and use of ODE invertibility to analyze latent paths.",
            "computational_cost": "Solving the ODE numerically requires repeated D_theta evaluations; cost measured as NFE. Choice of solver affects per-step cost: Euler (1 eval/step), Heun (2 evals/step), adaptive RK45 (multiple evals), with trade-offs in truncation error vs NFE.",
            "efficiency_comparison": "Deterministic ODE sampling with their schedule + Heun outperforms Euler-based solvers in NFE-to-quality tradeoff; adaptive RK45 was more expensive overall in experiments.",
            "task_performance": "When used with EDM components and Heun solver, ODE-based deterministic sampling achieves high quality (see EDM FIDs), with deterministic sampling sometimes outperforming stochastic variants under improved training.",
            "task_utility_analysis": "The probability-flow ODE is useful when deterministic inversion (latent mapping) is desired and when the trained denoiser is sufficiently accurate; however, stochastic sampling (SDE-style) can improve final sample quality by correcting discretization/estimation errors, especially for diverse datasets.",
            "tradeoffs_observed": "Deterministic ODE sampling is simpler and enables inversion but tends to yield worse sample quality than stochastic SDE sampling in some setups; higher-order discrete ODE solvers reduce error but increase per-step eval cost; the best choice depends on network accuracy and dataset diversity.",
            "design_choices": "Paper recommends choosing sigma(t)=t and s(t)=1 (makes ODE simpler and trajectories more linear), and using a discretization schedule concentrating steps near low sigma (rho≈7). Solver choice (Heun) is recommended for favorable truncation-error vs NFE tradeoff.",
            "comparison_to_alternatives": "Compared to SDE/predictor-corrector or Euler–Maruyama discretizations, the probability-flow ODE with Heun and appropriate schedule achieved competitive or better FID using fewer NFEs for several evaluated models/datasets.",
            "optimal_configuration": "Paper recommends using sigma(t)=t, s(t)=1, a Heun 2nd-order integrator with the step schedule sigma_i = (A i + B)^rho (rho≈7), and treating D_theta as black-box score estimator; use stochastic churn only when beneficial per dataset.",
            "uuid": "e1422.1",
            "source_info": {
                "paper_title": "Elucidating the Design Space of Diffusion-Based Generative Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Heun sampler (Alg.1)",
            "name_full": "Deterministic Heun 2nd-order sampler (Algorithm 1)",
            "brief_description": "A deterministic sampling algorithm using the 2nd-order Heun (explicit trapezoidal) method to integrate the probability-flow ODE; trades one extra neural evaluation per step for O(h^3) local error and substantially better NFE-to-quality performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Heun-based deterministic sampler",
            "model_description": "Numerical integrator that at each step evaluates D_theta at current t and at an Euler-predicted x_{i+1} to form a trapezoidal/corrector update (two D_theta evaluations per step except when stepping to σ=0 where Euler is used).",
            "model_type": "deterministic numerical solver for score-based generative model dynamics",
            "task_domain": "sampling/generation in diffusion models (image generation)",
            "fidelity_metric": "FID vs number of neural function evaluations (NFE)",
            "fidelity_performance": "When combined with EDM time schedule and preconditioning: reached same FID as Euler with considerably lower NFE; example: overall pipeline achieves CIFAR-10 FID 1.79 with 35 NFE.",
            "interpretability_assessment": "Method is a numerical integrator — interpretable in numerical-analysis sense; does not change interpretability of underlying neural world model beyond enabling smoother trajectories and more faithful ODE following.",
            "interpretability_method": "Not applicable beyond inspecting trajectory curvature and truncation error; paper analyzes curvature to motivate schedule choice.",
            "computational_cost": "Costs roughly 2 neural evaluations per step (one extra than Euler) but reduces number of steps needed; overall sampling NFE decreased (e.g., 35 NFE for CIFAR-10 with Heun + schedule).",
            "efficiency_comparison": "Heun provides better truncation-error vs NFE tradeoff than Euler; outperformed adaptive RK45 in wall-clock/qualified cost in the authors' experiments due to solver overhead.",
            "task_performance": "Improved sample quality per NFE compared to Euler-based samplers; numerical improvements summarized in Figures/Tables (see Table 3 and plots).",
            "task_utility_analysis": "Heun reduces discretization bias, enabling fewer steps at equivalent or better FID — directly improves sampling efficiency and utility in real deployments where D_theta evals dominate cost.",
            "tradeoffs_observed": "Extra per-step cost (one more D_theta eval) vs fewer total steps; Heun recommended because net NFE decreased for comparable quality. Stepping to σ=0 requires fallback to Euler to avoid division by zero.",
            "design_choices": "Use Heun 2nd-order correction; apply correction except when next σ==0; pair with sigma(t)=t and step schedule concentrating steps near low σ (rho≈7).",
            "comparison_to_alternatives": "Compared to Euler and adaptive RK45, Heun provided the best practical tradeoff in authors' tests: fewer NFEs to reach high-quality FID than Euler, and lower overhead than RK45 for fixed-budget sampling.",
            "optimal_configuration": "Use Heun with sigma(t)=t, s(t)=1, sigma_i schedule with rho≈7 and clamp scheme for σ→0; choose N (steps) to match desired NFE budget (authors report excellent results with 35 steps on CIFAR-10).",
            "uuid": "e1422.2",
            "source_info": {
                "paper_title": "Elucidating the Design Space of Diffusion-Based Generative Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Churn stochastic sampler (Alg.2)",
            "name_full": "Stochastic churn sampler with controlled Langevin-like noise injection (Algorithm 2)",
            "brief_description": "A tailored stochastic sampling procedure that alternates explicit Langevin-like noise 'churn' (temporary increase of noise level) with an ODE step (using Heun/Euler), governed by per-step churn parameters to trade off error correction and detail preservation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Churn-based stochastic sampler",
            "model_description": "At each step, temporarily raise the current noise level by factor γ_i (clamped), add Gaussian noise with adjustable amplitude S_noise, evaluate D_theta at the increased-noise state, then take an ODE step back to the next σ; parameters S_churn, S_min, S_max, S_noise control where and how much stochasticity is applied.",
            "model_type": "stochastic sampler for score-based generative models (hybrid SDE/ODE procedure)",
            "task_domain": "image generation; used to improve sample fidelity when deterministic sampling underperforms",
            "fidelity_metric": "FID vs NFE and ablation over churn parameters",
            "fidelity_performance": "When optimally tuned for pre-trained models: improved ImageNet-64 pre-trained ADM model from FID 2.07 → 1.55 (Table 4). For various datasets, Algorithm 2 with optimal settings yielded best FIDs in stochastic sampling experiments (Table 4).",
            "interpretability_assessment": "Adds stochastic perturbations; interpretability similar to deterministic ODE but less straightforward due to stochasticity; authors analyze detrimental effects (loss of detail, color drift) empirically and attribute them to nonconservative behavior of practical denoisers.",
            "interpretability_method": "Empirical ablations and visualization of generated samples and per-sigma denoiser behavior (Figures 4, 5); no explicit explainability method for the stochastic corrections.",
            "computational_cost": "Per step cost similar to deterministic Heun when 2nd-order correction is used; additional sampling overhead from random draws and possible extra steps for tuned settings; NFE budgets reported (e.g., optimal settings often require more NFEs than deterministic low-NFE runs but yield better FID for some models).",
            "efficiency_comparison": "When appropriately tuned, stochastic churn provides better final FID than purely deterministic sampling for some pre-trained models and datasets, but requires per-model hyperparameter search (grid search) and sometimes more NFEs; the authors note stochasticity is dataset- and training-dependent.",
            "task_performance": "Improved sampler quality especially for previously-trained models: e.g., ImageNet-64 ADM pre-trained model: FID from 2.07 → 1.55 with churn sampler; for VE/VP pre-trained models substantial improvements at low step counts were observed.",
            "task_utility_analysis": "Stochastic churn helps correct earlier discretization/approximation errors and can thus improve perceptual quality; however, excessive churn degrades fine detail and color fidelity, and it requires heuristics (S_min/S_max/S_churn/S_noise) tuned per model, so its utility depends on the reliability of the denoiser and dataset complexity.",
            "tradeoffs_observed": "Stochasticity trades error-correction ability for potential detail loss and color drift; requires per-model hyperparameter tuning; effective in improving FID of some models but can be detrimental when model training already targets the right sigma regime (e.g., with EDM training on CIFAR-10 deterministic sampling was preferable).",
            "design_choices": "Churn parameters: set γ_i=min(S_churn/N, sqrt(2)-1) within S_min..S_max; noise amplitude S_noise can be tuned &gt;1 to counteract denoiser over-smoothing; apply churn only in a sigma window to avoid color drift; pair with an ODE integrator for the denoising step.",
            "comparison_to_alternatives": "Compared against Euler–Maruyama, predictor-corrector, and previous stochastic samplers, the churn sampler outperformed prior samplers in the authors' experiments (Figure 4/Table 4) when tuned, particularly at low NFEs. However tuning costs and dataset dependence are noted disadvantages.",
            "optimal_configuration": "Authors recommend grid-search tuning of {S_churn, S_min, S_max, S_noise} per model; use modest stochasticity in a restricted sigma window, clamp γ_i to not exceed existing noise, and select S_noise slightly &gt;1 to counteract denoiser bias. For well-trained EDM models on small datasets, deterministic sampling may be preferable.",
            "uuid": "e1422.3",
            "source_info": {
                "paper_title": "Elucidating the Design Space of Diffusion-Based Generative Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Preconditioned denoiser (D_theta / F_theta)",
            "name_full": "Preconditioned denoiser parameterization D_theta(x;σ) = c_skip(σ) x + c_out(σ) F_theta(c_in(σ) x; c_noise(σ))",
            "brief_description": "A σ-dependent reparameterization of the denoiser that normalizes input/output magnitudes and provides a skip connection to stabilize training across noise levels, enabling more robust optimization and better final sample quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Preconditioned denoiser architecture (D_theta with raw network F_theta)",
            "model_description": "Denoiser implemented as a mixture of a σ-dependent scaled identity skip and a scaled neural network output: D_θ(x;σ)=c_skip(σ) x + c_out(σ) F_θ(c_in(σ) x; c_noise(σ)). Choices for c_in/c_out/c_skip and noise-conditioning c_noise are derived to make network input/output targets unit-variance and to minimize amplification of F_θ errors across σ.",
            "model_type": "neural network-based conditional denoiser with σ-dependent preconditioning",
            "task_domain": "training of diffusion score models for image generation",
            "fidelity_metric": "training loss per-σ, aggregate FID after sampling",
            "fidelity_performance": "Using the proposed preconditioning plus loss-weighting and p_train adjustments led to major FID improvements (Table 2): e.g., moving from baseline to full config produced CIFAR-10 conditional FID improvements culminating at 1.79.",
            "interpretability_assessment": "Preconditioning improves the numerical stability and interpretability of what the network must predict: it clarifies the effective training target for F_theta (shown in Eq.8) and keeps network inputs/targets near unit variance, but the learned internal representations remain neural black-box features.",
            "interpretability_method": "Analytic derivation of effective training target (Eq.8) and empirical plots of initial/final per-σ loss (Figure 5a) used to justify choices; visualization of denoiser outputs also used.",
            "computational_cost": "No explicit extra inference cost beyond the underlying F_theta network; training stability improvements allow same-capacity models to train effectively. Parameter counts for F_theta architectures are not specified in the paper; authors report network-architecture tweaks (redistribute capacity) but not absolute sizes.",
            "efficiency_comparison": "Preconditioning improved training robustness enabling better final quality without larger networks; combining preconditioning with loss-weighting and log-normal p_train achieved larger FID gains than applying loss-weighting alone (citing smaller improvements in concurrent work).",
            "task_performance": "Leads to more stable training and better final FID when used with other design choices; Table 2 demonstrates successive gains as preconditioning, loss function, and augmentation are introduced.",
            "task_utility_analysis": "Preconditioning reduces extreme per-σ variance in targets and errors, enabling the model to focus capacity where it matters for perceptual quality (mid σ) and helping training to translate to better sampling quality—thus improving task utility without necessarily increasing model size.",
            "tradeoffs_observed": "Designing c_skip and c_out to minimize amplification of errors helps high-σ behavior but requires careful derivation; choice of c_noise is empirical. The approach trades some implementation complexity (σ-dependent scalings and conditioning) for stability and quality gains.",
            "design_choices": "Authors recommend c_in(σ)=1/√(σ^2+σ_data^2), c_skip(σ)=σ_data^2/(σ^2+σ_data^2), c_out(σ)=σ·σ_data/√(σ_data^2+σ^2), and c_noise(σ)=(1/4) ln(σ) (Table 1); set loss weight λ(σ)=1/c_out(σ)^2 (or equivalent formula used in experiments) and sample σ from a log-normal p_train targeting mid-σ values.",
            "comparison_to_alternatives": "Previous parameterizations predicted scaled noise n or used simpler normalization; the proposed preconditioning reduces error amplification at large σ and improves robustness versus those earlier choices, especially when combined with the other EDM training/sampling recommendations.",
            "optimal_configuration": "Use the preconditioning scalings given in Table 1, set λ(σ)=1/c_out(σ)^2 (or the equivalent derived form), and use a log-normal p_train to emphasize mid-σ training; pair with architecture capacity redistribution (boost high-res layers) and augmentation when data is scarce.",
            "uuid": "e1422.4",
            "source_info": {
                "paper_title": "Elucidating the Design Space of Diffusion-Based Generative Models",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Score-based generative modeling through stochastic differential equations",
            "rating": 2
        },
        {
            "paper_title": "Denoising diffusion probabilistic models",
            "rating": 2
        },
        {
            "paper_title": "Improved denoising diffusion probabilistic models",
            "rating": 2
        },
        {
            "paper_title": "Diffusion models beat GANs on image synthesis",
            "rating": 2
        },
        {
            "paper_title": "Denoising diffusion implicit models",
            "rating": 1
        }
    ],
    "cost": 0.020671,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Elucidating the Design Space of Diffusion-Based Generative Models</h1>
<p>Tero Karras<br>NVIDIA</p>
<p>Miika Aittala<br>NVIDIA</p>
<p>Timo Aila<br>NVIDIA</p>
<h2>Samuli Laine</h2>
<p>NVIDIA</p>
<h4>Abstract</h4>
<p>We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling ( 35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.</p>
<h2>1 Introduction</h2>
<p>Diffusion-based generative models [46] have emerged as a powerful new framework for neural image synthesis, in both unconditional [16, 37, 49] and conditional [17, 36, 37, 39, 40, 42, 43, 49] settings, even surpassing the quality of GANs [13] in certain situations [9]. They are also rapidly finding use in other domains such as audio [28, 38] and video [19] generation, image segmentation [4, 57] and language translation [35]. As such, there is great interest in applying these models and improving them further in terms of image/distribution quality, training cost, and generation speed.
The literature on these models is dense on theory, and derivations of sampling schedule, training dynamics, noise level parameterization, etc., tend to be based as directly as possible on theoretical frameworks, which ensures that the models are on a solid theoretical footing. However, this approach has a danger of obscuring the available design space - a proposed model may appear as a tightly coupled package where no individual component can be modified without breaking the entire system.
As our first contribution, we take a look at the theory behind these models from a practical standpoint, focusing more on the "tangible" objects and algorithms that appear in the training and sampling phases, and less on the statistical processes from which they might be derived. The goal is to obtain better insights into how these components are linked together and what degrees of freedom are available in the design of the overall system. We focus on the broad class of models where a neural network is used to model the score [22] of a noise level dependent marginal distribution of the training data corrupted by Gaussian noise. Thus, our work is in the context of denoising score matching [54].
Our second set of contributions concerns the sampling processes used to synthesize images using diffusion models. We identify the best-performing time discretization for sampling, apply a higherorder Runge-Kutta method for the sampling process, evaluate different sampler schedules, and analyze the usefulness of stochasticity in the sampling process. The result of these improvements is a significant drop in the number of sampling steps required during synthesis, and the improved sampler can be used as a drop-in replacement with several widely used diffusions models [37, 49].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>(a) Noisy images drawn from ( p(x ; \sigma ) (b) Ideal denoiser outputs ( D(x ; \sigma )</p>
<p>Figure 1: Denoising score matching on CIFAR-10. (a) Images from the training set corrupted with varying levels of additive Gaussian noise. High levels of noise lead to oversaturated colors; we normalize the images for cleaner visualization. (b) Optimal denoising result from minimizing Eq. 2 analytically (see Appendix B.3). With increasing noise level, the result approaches dataset mean.</p>
<p>The third set of contributions focuses on the training of the score-modeling neural network. While we continue to rely on the commonly used network architectures (DDPM [16], NCSN [48]), we provide the first principled analysis of the preconditioning of the networks' inputs, outputs, and loss functions in a diffusion model setting and derive best practices for improving the training dynamics. We also suggest an improved distribution of noise levels during training, and note that non-leaking augmentation [25] — typically used with GANs — is beneficial for diffusion models as well.</p>
<p>Taken together, our contributions enable significant improvements in result quality, e.g., leading to record FIDs of 1.79 for CIFAR-10 [29] and 1.36 for ImageNet [8] in 64×64 resolution. With all key ingredients of the design space explicitly tabulated, we believe that our approach will allow easier innovation on the individual components, and thus enable more extensive and targeted exploration of the design space of diffusion models. Our implementation and pre-trained models are available at <a href="https://github.com/NVlabs/edm">https://github.com/NVlabs/edm</a></p>
<h2>2 Expressing diffusion models in a common framework</h2>
<p>Let us denote the data distribution by ( p_{\text{data}}(x) ), with standard deviation ( \sigma_{\text{data}} ), and consider the family of mollified distributions ( p(x ; \sigma) ) obtained by adding i.i.d. Gaussian noise of standard deviation ( \sigma ) to the data. For ( \sigma_{\text{max}} \gg \sigma_{\text{data}}, p(x ; \sigma_{\text{max}}) ) is practically indistinguishable from pure Gaussian noise. The idea of diffusion models is to randomly sample a noise image ( x_0 \sim \mathcal{N}(\mathbf{0}, \sigma_{\text{max}}^{2}\mathbf{I}) ), and sequentially denoise it into images ( x_i ) with noise levels ( \sigma_0 = \sigma_{\text{max}} &gt; \sigma_1 &gt; \cdots &gt; \sigma_N = 0 ) so that at each noise level ( x_i \sim p(x_i ; \sigma_i) ). The endpoint ( x_N ) of this process is thus distributed according to the data.</p>
<p>Song et al. [49] present a stochastic differential equation (SDE) that maintains the desired distribution ( p ) as sample ( x ) evolves over time. This allows the above process to be implemented using a stochastic solver that both removes and adds noise at each iteration. They also give a corresponding "probability flow" ordinary differential equation (ODE) where the only source of randomness is the initial noise image ( x_0 ). Contrary to the usual order of treatment, we begin by examining the ODE, as it offers a fruitful setting for analyzing sampling trajectories and their discretizations. The insights carry over to stochastic sampling, which we reintroduce as a generalization in Section 4.</p>
<h3>ODE formulation.</h3>
<p>A probability flow ODE [49] continuously increases or reduces noise level of the image when moving forward or backward in time, respectively. To specify the ODE, we must first choose a schedule ( \sigma(t) ) that defines the desired noise level at time ( t ). For example, setting ( \sigma(t) \propto \sqrt{t} ) is mathematically natural, as it corresponds to constant-speed heat diffusion [12]. However, we will show in Section 3 that the choice of schedule has major practical implications and should not be made on the basis of theoretical convenience.</p>
<p>The defining characteristic of the probability flow ODE is that evolving a sample ( x_a \sim p(x_a; \sigma(t_a)) ) from time ( t_a ) to ( t_b ) (either forward or backward in time) yields a sample ( x_b \sim p(x_b; \sigma(t_b)) ). Following previous work [49], this requirement is satisfied (see Appendix B.1 and B.2) by</p>
<p>$$
\dd x = -\dot{\sigma}(t) \sigma(t) \nabla_x \log p(x; \sigma(t)) \dd t, \tag{1}
$$</p>
<p>where the dot denotes a time derivative. ( \nabla_x \log p(x; \sigma) ) is the <em>score function</em> [22], a vector field that points towards higher density of data at a given noise level. Intuitively, an infinitesimal forward step of this ODE nudges the sample away from the data, at a rate that depends on the change in noise level. Equivalently, a backward step nudges the sample towards the data distribution.</p>
<h3>Denoising score matching.</h3>
<p>The score function has the remarkable property that it does not depend on the generally intractable normalization constant of the underlying density function ( p(x; \sigma) ) [22],</p>
<p>Table 1: Specific design choices employed by different model families. $N$ is the number of ODE solver iterations that we wish to execute during sampling. The corresponding sequence of time steps is $\left{t_{0}, t_{1}, \ldots, t_{N}\right}$, where $t_{N}=0$. If the model was originally trained for specific choices of $N$ and $\left{t_{i}\right}$, the originals are denoted by $M$ and $\left{u_{j}\right}$, respectively. The denoiser is defined as $D_{\theta}(\boldsymbol{x} ; \sigma)=c_{\text {skip }}(\sigma) \boldsymbol{x}+c_{\text {out }}(\sigma) F_{\theta}\left(c_{\text {in }}(\sigma) \boldsymbol{x} ; c_{\text {noise }}(\sigma)\right) ; F_{\theta}$ represents the raw neural network layers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VP [49]</th>
<th style="text-align: center;">VE [49]</th>
<th style="text-align: center;">iDDPM [37] + DDIM [47]</th>
<th style="text-align: center;">Ours ("EDM")</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sampling (Section 3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ODE solver</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Euler</td>
<td style="text-align: center;">Euler</td>
<td style="text-align: center;">Euler</td>
<td style="text-align: center;">$2^{\text {nd }}$ order Heun</td>
</tr>
<tr>
<td style="text-align: center;">Time steps</td>
<td style="text-align: center;">$t_{i&lt;N}$</td>
<td style="text-align: center;">$1+\frac{1}{N-1}\left(r_{t}-1\right)$</td>
<td style="text-align: center;">$\sigma_{\text {min }}^{2}\left(\sigma_{\text {min }}^{2} / \sigma_{\text {max }}^{2}\right)^{\frac{1}{N-1}}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; u_{\left\lfloor j_{0}+\frac{M-1}{N-1} i_{0}+\frac{1}{2}\right\rfloor} ; \text { where } \ &amp; u_{M}=0 \cdot \frac{1}{u_{j-1}=\sqrt{\frac{M+1}{2 \text { min }\left(r_{t-1} ; i_{0}, C_{1}\right)}-1}} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \left(\sigma_{\text {max }}\right)^{2}+ \ &amp; \frac{1}{N-1}\left(\sigma_{\text {min }}{ }^{2}-\sigma_{\text {max }}{ }^{2}\right)\right)^{p} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Schedule</td>
<td style="text-align: center;">$\sigma(t)$</td>
<td style="text-align: center;">$\sqrt{e^{\frac{1}{2} \beta_{0} t^{2}+\beta_{\text {max }} t}-1}$</td>
<td style="text-align: center;">$\sqrt{t}$</td>
<td style="text-align: center;">$t$</td>
<td style="text-align: center;">$t$</td>
</tr>
<tr>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">$s(t)$</td>
<td style="text-align: center;">$1 / \sqrt{e^{\frac{1}{2} \beta_{0} t^{2}+\beta_{\text {max }} t}}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Network and preconditioning (Section 5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Architecture of $F_{\theta}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DDPM++</td>
<td style="text-align: center;">NCSN++</td>
<td style="text-align: center;">DDPM</td>
<td style="text-align: center;">(any)</td>
</tr>
<tr>
<td style="text-align: center;">Skip scaling</td>
<td style="text-align: center;">$c_{\text {skip }}(\sigma)$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\sigma_{\text {data }}^{2} /\left(\sigma^{2}+\sigma_{\text {data }}^{2}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">Output scaling $c_{\text {out }}(\sigma)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-\sigma$</td>
<td style="text-align: center;">$\sigma$</td>
<td style="text-align: center;">$-\sigma$</td>
<td style="text-align: center;">$\sigma \cdot \sigma_{\text {data }} / \sqrt{\sigma_{\text {data }}^{2}+\sigma^{2}}$</td>
</tr>
<tr>
<td style="text-align: center;">Input scaling</td>
<td style="text-align: center;">$c_{\text {in }}(\sigma)$</td>
<td style="text-align: center;">$1 / \sqrt{\sigma^{2}+1}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1 / \sqrt{\sigma^{2}+1}$</td>
<td style="text-align: center;">$1 / \sqrt{\sigma^{2}+\sigma_{\text {data }}^{2}}$</td>
</tr>
<tr>
<td style="text-align: center;">Noise cond.</td>
<td style="text-align: center;">$c_{\text {noise }}(\sigma)$</td>
<td style="text-align: center;">$(M-1) \sigma^{-1}(\sigma)$</td>
<td style="text-align: center;">$\ln \left(\frac{1}{2} \sigma\right)$</td>
<td style="text-align: center;">$M-1-\arg \min _{j}\left</td>
<td style="text-align: center;">u_{j}-\sigma\right</td>
</tr>
<tr>
<td style="text-align: center;">Training (Section 5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Noise distribution</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sigma^{-1}(\sigma) \sim \mathcal{U}\left(\epsilon_{t}, 1\right)$</td>
<td style="text-align: center;">$\ln (\sigma) \sim \mathcal{U}\left(\ln \left(\sigma_{\text {min }}\right)\right.$, $\left.\ln \left(\sigma_{\text {max }}\right)\right)$</td>
<td style="text-align: center;">$\sigma=u_{j}, j \sim \mathcal{U}{0, M-1}$</td>
<td style="text-align: center;">$\ln (\sigma) \sim \mathcal{N}\left(P_{\text {mean }}, P_{\text {std }}^{2}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">Loss weighting</td>
<td style="text-align: center;">$\lambda(\sigma)$</td>
<td style="text-align: center;">$1 / \sigma^{2}$</td>
<td style="text-align: center;">$1 / \sigma^{2}$</td>
<td style="text-align: center;">$1 / \sigma^{2} \quad\left(\right.$ note: ${ }^{*}$ )</td>
<td style="text-align: center;">$\left(\sigma^{2}+\sigma_{\text {data }}^{2}\right) /\left(\sigma \cdot \sigma_{\text {data }}\right)^{2}$</td>
</tr>
<tr>
<td style="text-align: center;">Parameters</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\beta_{\delta}=19.9, \beta_{\text {min }}=0.1$</td>
<td style="text-align: center;">$\sigma_{\text {min }}=0.02$</td>
<td style="text-align: center;">$\bar{\alpha}<em 2="2">{j}=\sin ^{2}\left(\frac{2}{4} \frac{j}{M\left(C</em>\right)$}+1\right)</td>
<td style="text-align: center;">$\sigma_{\text {min }}=0.002, \sigma_{\text {max }}=80$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\epsilon_{\mathrm{r}}=10^{-3}, \epsilon_{\mathrm{r}}=10^{-5}$</td>
<td style="text-align: center;">$\sigma_{\text {max }}=100$</td>
<td style="text-align: center;">$C_{1}=0.001, C_{2}=0.008$</td>
<td style="text-align: center;">$\sigma_{\text {data }}=0.5, \rho=7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$M=1000$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$M=1000 ; j_{0}=8^{t}$</td>
<td style="text-align: center;">$P_{\text {min }}=-1.2, P_{\text {std }}=1.2$</td>
</tr>
</tbody>
</table>
<p>${ }^{*}$ iDDPM also employs a second loss term $L_{\text {vib. }}{ }^{\mathrm{t}}$ In our tests, $j_{0}=8$ yielded better FID than $j_{0}=0$ used by iDDPM
and thus can be much easier to evaluate. Specifically, if $D(\boldsymbol{x} ; \sigma)$ is a denoiser function that minimizes the expected $L_{2}$ denoising error for samples drawn from $p_{\text {data }}$ separately for every $\sigma$, i.e.,</p>
<p>$$
\mathbb{E}<em _data="{data" _text="\text">{\boldsymbol{y} \sim p</em>}}} \mathbb{E<em 2="2">{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^{2} \mathbf{I}\right)} | D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y} |</em>
$$}^{2}, \text { then } \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=\left(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}\right) / \sigma^{2</p>
<p>where $\boldsymbol{y}$ is a training image and $\boldsymbol{n}$ is noise. In this light, the score function isolates the noise component from the signal in $\boldsymbol{x}$, and Eq. 1 amplifies (or diminishes) it over time. Figure 1 illustrates the behavior of ideal $D$ in practice. The key observation in diffusion models is that $D(\boldsymbol{x} ; \sigma)$ can be implemented as a neural network $D_{\theta}(\boldsymbol{x} ; \sigma)$ trained according to Eq. 2. Note that $D_{\theta}$ may include additional pre- and post-processing steps, such as scaling $\boldsymbol{x}$ to an appropriate dynamic range; we will return to such preconditioning in Section 5.</p>
<p>Time-dependent signal scaling. Some methods (see Appendix C.1) introduce an additional scale schedule $s(t)$ and consider $\boldsymbol{x}=s(t) \hat{\boldsymbol{x}}$ to be a scaled version of the original, non-scaled variable $\hat{\boldsymbol{x}}$. This changes the time-dependent probability density, and consequently also the ODE solution trajectories. The resulting ODE is a generalization of Eq. 1 :</p>
<p>$$
\mathrm{d} \boldsymbol{x}=\left[\frac{\dot{s}(t)}{s(t)} \boldsymbol{x}-s(t)^{2} \dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p\left(\frac{\boldsymbol{x}}{s(t)} ; \sigma(t)\right)\right] \mathrm{d} t
$$</p>
<p>Note that we explicitly undo the scaling of $\boldsymbol{x}$ when evaluating the score function to keep the definition of $p(\boldsymbol{x} ; \sigma)$ independent of $s(t)$.</p>
<p>Solution by discretization. The ODE to be solved is obtained by substituting Eq. 3 into Eq. 4 to define the point-wise gradient, and the solution can be found by numerical integration, i.e., taking finite steps over discrete time intervals. This requires choosing both the integration scheme (e.g., Euler or a variant of Runge-Kutta), as well as the discrete sampling times $\left{t_{0}, t_{1}, \ldots, t_{N}\right}$. Many prior works rely on Euler's method, but we show in Section 3 that a $2^{\text {nd }}$ order solver offers a better computational tradeoff. For brevity, we do not provide a separate pseudocode for Euler's method applied to our ODE here, but it can be extracted from Algorithm 1 by omitting lines 6-8.</p>
<p>Putting it together. Table 1 presents formulas for reproducing deterministic variants of three earlier methods in our framework. These methods were chosen because they are widely used and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Uncond. CIFAR-10, VP ODE
(b) Uncond. CIFAR-10, VE ODE
(c) Class-cond. ImageNet-64, DDIM</p>
<p>Figure 2: Comparison of deterministic sampling methods using three pre-trained models. For each curve, the dot indicates the lowest NFE whose FID is within 3% of the lowest observed FID.
achieve state-of-the-art performance, but also because they were derived from different theoretical foundations. Some of our formulas appear quite different from the original papers as indirection and recursion have been removed; see Appendix C for details. The main purpose of this reframing is to bring into light all the independent components that often appear tangled together in previous work. In our framework, there are no implicit dependencies between the components - any choices (within reason) for the individual formulas will, in principle, lead to a functioning model. In other words, changing one component does not necessitate changes elsewhere in order to, e.g., maintain the property that the model converges to the data in the limit. In practice, some choices and combinations will of course work better than others.</p>
<h1>3 Improvements to deterministic sampling</h1>
<p>Improving the output quality and/or decreasing the computational cost of sampling are common topics in diffusion model research (e.g., $[10,24,31,32,33,37,44,53,55,56,59])$. Our hypothesis is that the choices related to the sampling process are largely independent of the other components, such as network architecture and training details. In other words, the training procedure of $D_{\theta}$ should not dictate $\sigma(t), s(t)$, and $\left{t_{i}\right}$, nor vice versa; from the viewpoint of the sampler, $D_{\theta}$ is simply a black box [55, 56]. We test this by evaluating different samplers on three pre-trained models, each representing a different theoretical framework and model family. We first measure baseline results for these models using their original sampler implementations, and then bring these samplers into our unified framework using the formulas in Table 1, followed by our improvements. This allows us to evaluate different practical choices and propose general improvements to the sampling process that are applicable to all models.</p>
<p>We evaluate the "DDPM++ cont. (VP)" and "NCSN++ cont. (VE)" models by Song et al. [49] trained on unconditional CIFAR-10 [29] at $32 \times 32$, corresponding to the variance preserving (VP) and variance exploding (VE) formulations [49], originally inspired by DDPM [16] and SMLD [48]. We also evaluate the "ADM (dropout)" model by Dhariwal and Nichol [9] trained on class-conditional ImageNet [8] at $64 \times 64$, corresponding to the improved DDPM (iDDPM) formulation [37]. This model was trained using a discrete set of $M=1000$ noise levels. Further details are given in Appendix C.</p>
<p>We evaluate the result quality in terms of Fréchet inception distance (FID) [15] computed between 50,000 generated images and all available real images. Figure 2 shows FID as a function of neural function evaluations (NFE), i.e., how many times $D_{\theta}$ is evaluated to produce a single image. Given that the sampling process is dominated entirely by the cost of $D_{\theta}$, improvements in NFE translate directly to sampling speed. The original deterministic samplers are shown in blue, and the reimplementations of these methods in our unified framework (orange) yield similar but consistently better results. The differences are explained by certain oversights in the original implementations as well as our more careful treatment of discrete noise levels in the case of DDIM; see Appendix C. Note that our reimplementations are fully specified by Algorithm 1 and Table 1, even though the original codebases are structured very differently from each other.</p>
<p>Discretization and higher-order integrators. Solving an ODE numerically is necessarily an approximation of following the true solution trajectory. At each step, the solver introduces truncation error that accumulates over the course of $N$ steps. The local error generally scales superlinearly with respect to step size, and thus increasing $N$ improves the accuracy of the solution.</p>
<p>The commonly used Euler's method is a first order ODE solver with $\mathcal{O}\left(h^{2}\right)$ local error with respect to step size $h$. Higher-order Runge-Kutta methods [50] scale more favorably but require multiple</p>
<p>Algorithm 1 Deterministic sampling using Heun's $2^{\text {nd }}$ order method with arbitrary $\sigma(t)$ and $s(t)$.
1:procedure HEUNSAMPLER $\left(D_{\theta}(\boldsymbol{x} ; \sigma), \sigma(t), s(t), t_{i \in{0, \ldots, N}}\right)$
2: sample $\boldsymbol{x}<em 0="0">{0} \sim \mathcal{N}\left(\mathbf{0}, \sigma^{2}\left(t</em>$
3: for $i \in{0, \ldots, N-1}$ do
$\triangleright$ Solve Eq. 4 over $N$ time steps
4: $\quad \boldsymbol{d}}\right) s^{2}\left(t_{0}\right) \mathbf{I}\right) \quad \triangleright$ Generate initial sample at $t_{0<em i="i">{i} \leftarrow\left(\frac{\dot{\sigma}\left(t</em>}\right)}{\sigma\left(t_{i}\right)}+\frac{\dot{s}\left(t_{i}\right)}{s\left(t_{i}\right)}\right) \boldsymbol{x<em i="i">{i}-\frac{\dot{\sigma}\left(t</em>}\right) s\left(t_{i}\right)}{\sigma\left(t_{i}\right)} D_{\theta}\left(\frac{\boldsymbol{x<em i="i">{i}}{s\left(t</em>$
5: $\quad \boldsymbol{x}}\right)} ; \sigma\left(t_{i}\right)\right) \quad \triangleright$ Evaluate $\mathrm{d} \boldsymbol{x} / \mathrm{d} t$ at $t_{i<em i="i">{i+1} \leftarrow \boldsymbol{x}</em>}+\left(t_{i+1}-t_{i}\right) \boldsymbol{d<em i="i">{i} \quad \triangleright$ Take Euler step from $t</em>$
6: if $\sigma\left(t_{i+1}\right) \neq 0$ then $\quad \triangleright$ Apply $2^{\text {nd }}$ order correction unless $\sigma$ goes to zero
7: $\quad \boldsymbol{d}}$ to $t_{i+1<em i_1="i+1">{i}^{\prime} \leftarrow\left(\frac{\dot{\sigma}\left(t</em>}\right)}{\sigma\left(t_{i+1}\right)}+\frac{\dot{s}\left(t_{i+1}\right)}{s\left(t_{i+1}\right)}\right) \boldsymbol{x<em i_1="i+1">{i+1}-\frac{\dot{\sigma}\left(t</em>}\right) s\left(t_{i+1}\right)}{\sigma\left(t_{i+1}\right)} D_{\theta}\left(\frac{\boldsymbol{x<em i_1="i+1">{i+1}}{s\left(t</em>$
8: $\quad \boldsymbol{x}}\right)} ; \sigma\left(t_{i+1}\right)\right) \triangleright$ Eval. $\mathrm{d} \boldsymbol{x} / \mathrm{d} t$ at $t_{i+1<em i="i">{i+1} \leftarrow \boldsymbol{x}</em>}+\left(t_{i+1}-t_{i}\right)\left(\frac{1}{2} \boldsymbol{d<em i="i">{i}+\frac{1}{2} \boldsymbol{d}</em>$
9: return $\boldsymbol{x}}^{\prime}\right) \quad \triangleright$ Explicit trapezoidal rule at $t_{i+1<em N="N">{N} \quad \triangleright$ Return noise-free sample at $t</em>$
evaluations of $D_{\theta}$ per step. Linear multistep methods have also been recently proposed for sampling diffusion models [31, 59]. Through extensive tests, we have found Heun's $2^{\text {nd }}$ order method [2] (a.k.a. improved Euler, trapezoidal rule) — previously explored in the context of diffusion models by Jolicoeur-Martineau et al. [24] — to provide an excellent tradeoff between truncation error and NFE. As illustrated in Algorithm 1, it introduces an additional correction step for $\boldsymbol{x}<em i="i">{i+1}$ to account for change in $\mathrm{d} \boldsymbol{x} / \mathrm{d} t$ between $t</em>$ order solvers in Appendix D.2.
The time steps $\left{t_{i}\right}$ determine how the step sizes and thus truncation errors are distributed between different noise levels. We provide a detailed analysis in Appendix D.1, concluding that the step size should decrease monotonically with decreasing $\sigma$ and it does not need to vary on a per-sample basis. We adopt a parameterized scheme where the time steps are defined according to a sequence of noise levels $\left{\sigma_{i}\right}$, i.e., $t_{i}=\sigma^{-1}\left(\sigma_{i}\right)$. We set $\sigma_{i&lt;N}=(A i+B)^{\rho}$ and select the constants $A$ and $B$ so that $\sigma_{0}=\sigma_{\max }$ and $\sigma_{N-1}=\sigma_{\min }$, which gives}$ and $t_{i+1}$. This correction leads to $\mathcal{O}\left(h^{3}\right)$ local error at the cost of one additional evaluation of $D_{\theta}$ per step. Note that stepping to $\sigma=0$ would result in a division by zero, so we revert to Euler's method in this case. We discuss the general family of $2^{\text {nd }</p>
<p>$$
\sigma_{i&lt;N}=\left(\sigma_{\max }^{\frac{1}{\rho}}+\frac{i}{N-1}\left(\sigma_{\min }^{\frac{1}{\rho}}-\sigma_{\max }^{\frac{1}{\rho}}\right)\right)^{\rho} \quad \text { and } \quad \sigma_{N}=0
$$</p>
<p>Here $\rho$ controls how much the steps near $\sigma_{\min }$ are shortened at the expense of longer steps near $\sigma_{\max }$. Our analysis in Appendix D. 1 shows that setting $\rho=3$ nearly equalizes the truncation error at each step, but that $\rho$ in range of 5 to 10 performs much better for sampling images. This suggests that errors near $\sigma_{\min }$ have a large impact. We set $\rho=7$ for the remainder of this paper.
Results for Heun's method and Eq. 5 are shown as the green curves in Figure 2. We observe consistent improvement in all cases: Heun's method reaches the same FID as Euler's method with considerably lower NFE.</p>
<p>Trajectory curvature and noise schedule. The shape of the ODE solution trajectories is defined by functions $\sigma(t)$ and $s(t)$. The choice of these functions offers a way to reduce the truncation errors discussed above, as their magnitude can be expected to scale proportional to the curvature of $\mathrm{d} \boldsymbol{x} / \mathrm{d} t$. We argue that the best choice for these functions is $\sigma(t)=t$ and $s(t)=1$, which is also the choice made in DDIM [47]. With this choice, the ODE of Eq. 4 simplifies to $\mathrm{d} \boldsymbol{x} / \mathrm{d} t=(\boldsymbol{x}-D(\boldsymbol{x} ; t)) / t$ and $\sigma$ and $t$ become interchangeable.
An immediate consequence is that at any $\boldsymbol{x}$ and $t$, a single Euler step to $t=0$ yields the denoised image $D_{\theta}(\boldsymbol{x} ; t)$. The tangent of the solution trajectory therefore always points towards the denoiser output. This can be expected to change only slowly with the noise level, which corresponds to largely linear solution trajectories. The 1D ODE sketch of Figure 3c supports this intuition; the solution trajectories approach linear at both large and small noise levels, and have substantial curvature in only a small region in between. The same effect can be seen with real data in Figure 1b, where the change between different denoiser targets occurs in a relatively narrow $\sigma$ range. With the advocated schedule, this corresponds to high ODE curvature being limited to this same range.
The effect of setting $\sigma(t)=t$ and $s(t)=1$ is shown as the red curves in Figure 2. As DDIM already employs these same choices, the red curve is identical to the green one for ImageNet-64. However, VP and VE benefit considerably from switching away from their original schedules.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A sketch of ODE curvature in 1D where $p_{\text {data }}$ is two Dirac peaks at $\boldsymbol{x}= \pm 1$. Horizontal $t$ axis is chosen to show $\sigma \in[0,25]$ in each plot, with insets showing $\sigma \in[0,1]$ near the data. Example local gradients are shown with black arrows. (a) Variance preserving ODE of Song et al. [49] has solution trajectories that flatten out to horizontal lines at large $\sigma$. Local gradients start pointing towards data only at small $\sigma$. (b) Variance exploding variant has extreme curvature near data and the solution trajectories are curved everywhere. (c) With the schedule used by DDIM [47] and us, as $\sigma$ increases the solution trajectories approach straight lines that point towards the mean of data. As $\sigma \rightarrow 0$, the trajectories become linear and point towards the data manifold.</p>
<p>Discussion. The choices that we made in this section to improve deterministic sampling are summarized in the Sampling part of Table 1. Together, they reduce the NFE needed to reach highquality results by a large factor: $7.3 \times$ for VP, $300 \times$ for VE, and $3.2 \times$ for DDIM, corresponding to the highlighted NFE values in Figure 2. In practice, we can generate 26.3 high-quality CIFAR-10 images per second on a single NVIDIA V100. The consistency of improvements corroborates our hypothesis that the sampling process is orthogonal to how each model was originally trained. As further validation, we show results for the adaptive RK45 method [11] using our schedule as the dashed black curves in Figure 2; the cost of this sophisticated ODE solver outweighs its benefits.</p>
<h1>4 Stochastic sampling</h1>
<p>Deterministic sampling offers many benefits, e.g., the ability to turn real images into their corresponding latent representations by inverting the ODE. However, it tends to lead to worse output quality [47, 49] than stochastic sampling that injects fresh noise into the image in each step. Given that ODEs and SDEs recover the same distributions in theory, what exactly is the role of stochasticity?</p>
<p>Background. The SDEs of Song et al. [49] can be generalized [20, 58] as a sum of the probability flow ODE of Eq. 1 and a time-varying Langevin diffusion SDE [14] (see Appendix B.5):</p>
<p>$$
\mathrm{d} \boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">{ \pm}=\underbrace{-\dot{\sigma}(t) \sigma(t) \nabla</em>}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t<em _boldsymbol_x="\boldsymbol{x">{\text {probability flow ODE (Eq. 1) }} \pm \underbrace{\beta(t) \sigma(t)^{2} \nabla</em>}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t<em t="t">{\text {deterministic noise decay }}+\underbrace{\sqrt{2 \beta(t)} \sigma(t) \mathrm{d} \omega</em>
$$}}_{\text {noise injection }</p>
<p>where $\omega_{t}$ is the standard Wiener process. $\mathrm{d} \boldsymbol{x}<em -="-">{+}$and $\mathrm{d} \boldsymbol{x}</em>(t) / \sigma(t)$, whereby the score vanishes from the forward SDE.
This perspective reveals why stochasticity is helpful in practice: The implicit Langevin diffusion drives the sample towards the desired marginal distribution at a given time, actively correcting for any errors made in earlier sampling steps. On the other hand, approximating the Langevin term with discrete SDE solver steps introduces error in itself. Previous results [3, 24, 47, 49] suggest that non-zero $\beta(t)$ is helpful, but as far as we can tell, the implicit choice for $\beta(t)$ in Song et al. [49] enjoys no special properties. Hence, the optimal amount of stochasticity should be determined empirically.}$are now separate SDEs for moving forward and backward in time, related by the time reversal formula of Anderson [1]. The Langevin term can further be seen as a combination of a deterministic score-based denoising term and a stochastic noise injection term, whose net noise level contributions cancel out. As such, $\beta(t)$ effectively expresses the relative rate at which existing noise is replaced with new noise. The SDEs of Song et al. [49] are recovered with the choice $\beta(t)=\dot{\sigma</p>
<p>Our stochastic sampler. We propose a stochastic sampler that combines our $2^{\text {nd }}$ order deterministic ODE integrator with explicit Langevin-like "churn" of adding and removing noise. A pseudocode is given in Algorithm 2. At each step $i$, given the sample $\boldsymbol{x}<em i="i">{i}$ at noise level $t</em>\right)\right)$, we perform two}\left(=\sigma\left(t_{i</p>
<p>Algorithm 2 Our stochastic sampler with $\sigma(t)=t$ and $s(t)=1$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">1:</th>
<th style="text-align: center;">procedure $\operatorname{StOCHASticSampler}\left(D_{\theta}(\boldsymbol{x} ; \sigma), t_{i \in{0, \ldots, N}}, \gamma_{i \in{0, \ldots, N-1}}, S_{\text {noise }}\right)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2:</td>
<td style="text-align: center;">sample $\boldsymbol{x}<em 0="0">{0} \sim \mathcal{N}\left(\mathbf{0}, t</em>\right)$}^{2} \mathbf{I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3:</td>
<td style="text-align: center;">for $i \in{0, \ldots, N-1}$ do</td>
<td style="text-align: center;">$\triangleright \gamma_{i}= \begin{cases}\min \left(\frac{S_{\text {churn }}}{N}, \sqrt{2}-1\right) &amp; \text { if } t_{i} \in\left[S_{\text {min }}, S_{\text {max }}\right] \ 0 &amp; \text { otherwise }\end{cases}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4:</td>
<td style="text-align: center;">sample $\boldsymbol{\epsilon}<em _noise="{noise" _text="\text">{i} \sim \mathcal{N}\left(\mathbf{0}, S</em>\right)$}}^{2} \mathbf{I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5:</td>
<td style="text-align: center;">$\tilde{t}<em i="i">{i} \leftarrow t</em>$}+\gamma_{i} t_{i</td>
<td style="text-align: center;">$\triangleright$ Select temporarily increased noise level $\tilde{t}_{i}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">6:</td>
<td style="text-align: center;">$\tilde{\boldsymbol{x}}<em i="i">{i} \leftarrow \boldsymbol{x}</em>}+\sqrt{\tilde{t<em i="i">{i}^{2}-t</em>$}^{2}} \boldsymbol{\epsilon}_{i</td>
<td style="text-align: center;">$\triangleright$ Add new noise to move from $t_{i}$ to $\tilde{t}_{i}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7:</td>
<td style="text-align: center;">$\boldsymbol{d}<em i="i">{i} \leftarrow\left(\tilde{\boldsymbol{x}}</em>}-D_{\theta}\left(\tilde{\boldsymbol{x}<em i="i">{i} ; \tilde{t}</em>$}\right)\right) / \tilde{t}_{i</td>
<td style="text-align: center;">$\triangleright$ Evaluate $\mathrm{d} \boldsymbol{x} / \mathrm{d} t$ at $\tilde{t}_{i}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8:</td>
<td style="text-align: center;">$\boldsymbol{x}<em i="i">{i+1} \leftarrow \tilde{\boldsymbol{x}}</em>}+\left(t_{i+1}-\tilde{t<em i="i">{i}\right) \boldsymbol{d}</em>$</td>
<td style="text-align: center;">$\triangleright$ Take Euler step from $\tilde{t}<em i_1="i+1">{i}$ to $t</em>$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9:</td>
<td style="text-align: center;">if $t_{i+1} \neq 0$ then</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10:</td>
<td style="text-align: center;">$\boldsymbol{d}<em i_1="i+1">{i}^{\prime} \leftarrow\left(\boldsymbol{x}</em>}-D_{\theta}\left(\boldsymbol{x<em i_1="i+1">{i+1} ; t</em>$}\right)\right) / t_{i+1</td>
<td style="text-align: center;">$\triangleright$ Apply $2^{\text {nd }}$ order correction</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11:</td>
<td style="text-align: center;">$\boldsymbol{x}<em i="i">{i+1} \leftarrow \tilde{\boldsymbol{x}}</em>}+\left(t_{i+1}-\tilde{t<em i="i">{i}\right)\left(\frac{1}{2} \boldsymbol{d}</em>\right)$}+\frac{1}{2} \boldsymbol{d}_{i}^{\prime</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12:</td>
<td style="text-align: center;">return $\boldsymbol{x}_{N}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>sub-steps. First, we add noise to the sample according to a factor $\gamma_{i} \geq 0$ to reach a higher noise level $\tilde{t}<em i="i">{i}=t</em>}+\gamma_{i} t_{i}$. Second, from the increased-noise sample $\tilde{\boldsymbol{x}<em i="i">{i}$, we solve the ODE backward from $\tilde{t}</em>}$ to $t_{i+1}$ with a single step. This yields a sample $\boldsymbol{x<em i_1="i+1">{i+1}$ with noise level $t</em>$, and the iteration continues. We stress that this is not a general-purpose SDE solver, but a sampling procedure tailored for the specific problem. Its correctness stems from the alternation of two sub-steps that each maintain the correct distribution (up to truncation error in the ODE step). The predictor-corrector sampler of Song et al. [49] has a conceptually similar structure to ours.
To analyze the main difference between our method and Euler-Maruyama, we first note a subtle discrepancy in the latter when discretizing Eq. 6. One can interpret Euler-Maruyama as first adding noise and then performing an ODE step, not from the intermediate state after noise injection, but assuming that $\boldsymbol{x}$ and $\sigma$ remained at the initial state at the beginning of the iteration step. In our method, the parameters used to evaluate $D_{\theta}$ on line 7 of Algorithm 2 correspond to the state after noise injection, whereas an Euler-Maruyama -like method would use $\boldsymbol{x}<em i="i">{i} ; t</em>}$ instead of $\tilde{\boldsymbol{x}<em i="i">{i} ; \tilde{t}</em>$ approaching zero there may be no difference between these choices, but the distinction appears to become significant when pursuing low NFE with large steps.}$. In the limit of $\Delta_{t</p>
<p>Practical considerations. Increasing the amount of stochasticity is effective in correcting errors made by earlier sampling steps, but it has its own drawbacks. We have observed (see Appendix E.1) that excessive Langevin-like addition and removal of noise results in gradual loss of detail in the generated images with all datasets and denoiser networks. There is also a drift toward oversaturated colors at very low and high noise levels. We suspect that practical denoisers induce a slightly nonconservative vector field in Eq. 3, violating the premises of Langevin diffusion and causing these detrimental effects. Notably, our experiments with analytical denoisers (such as the one in Figure 1b) have not shown such degradation.
If the degradation is caused by flaws in $D_{\theta}(\boldsymbol{x} ; \sigma)$, they can only be remedied using heuristic means during sampling. We address the drift toward oversaturated colors by only enabling stochasticity within a specific range of noise levels $t_{i} \in\left[S_{\text {min }}, S_{\text {max }}\right]$. For these noise levels, we define $\gamma_{i}=$ $S_{\text {churn }} / N$, where $S_{\text {churn }}$ controls the overall amount of stochasticity. We further clamp $\gamma_{i}$ to never introduce more new noise than what is already present in the image. Finally, we have found that the loss of detail can be partially counteracted by setting $S_{\text {noise }}$ slightly above 1 to inflate the standard deviation for the newly added noise. This suggests that a major component of the hypothesized non-conservativity of $D_{\theta}(\boldsymbol{x} ; \sigma)$ is a tendency to remove slightly too much noise - most likely due to regression toward the mean that can be expected to happen with any $L_{2}$-trained denoiser [30].</p>
<p>Evaluation. Figure 4 shows that our stochastic sampler outperforms previous samplers [24, 37, 49] by a significant margin, especially at low step counts. Jolicoeur-Martineau et al. [24] use a standard higher-order adaptive SDE solver [41] and its performance is a good baseline for such solvers in general. Our sampler has been tailored to the use case by, e.g., performing noise injection and ODE step sequentially, and it is not adaptive. It is an open question if adaptive solvers can be a net win over a well-tuned fixed schedule in sampling diffusion models.
Through sampler improvements alone, we are able to bring the ImageNet-64 model that originally achieved FID 2.07 [9] to 1.55 that is very close to the state-of-the-art; previously, FID 1.48 has been</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Evaluation of our stochastic sampler (Algorithm 2). The purple curve corresponds to optimal choices for <em>S</em><sub>churn</sub>, <em>S</em><sub>min</sub>, <em>S</em><sub>max</sub>, <em>S</em><sub>noise</sub><em>; orange, blue, and green correspond to disabling the effects of </em>S<em><sub>min,max</sub> and/or </em>S<em><sub>noise</sub>. The red curves show reference results for our deterministic sampler (Algorithm 1), equivalent to setting </em>S*<sub>churn</sub> = 0. The dashed black curves correspond to the original stochastic samplers from previous work: Euler–Maruyama [49] for VP, predictor-corrector [49] for VE, and iDDPM [37] for ImageNet-64. The dots indicate lowest observed FID.</p>
<p>reported for cascaded diffusion [17], 1.55 for classifier-free guidance [18], and 1.52 for StyleGAN-XL [45]. While our results showcase the potential gains achievable through sampler improvements, they also highlight the main shortcoming of stochasticity: For best results, one must make several heuristic choices — either implicit or explicit — that depend on the specific model. Indeed, we had to find the optimal values of {<em>S</em><sub>churn</sub>, <em>S</em><sub>min</sub>, <em>S</em><sub>max</sub>, <em>S</em><sub>noise</sub>} on a case-by-case basis using grid search (Appendix E.2). This raises a general concern that using stochastic sampling as the primary means of evaluating model improvements may inadvertently end up influencing the design choices related to model architecture and training.</p>
<h2>5 Preconditioning and training</h2>
<p>There are various known good practices for training neural networks in a supervised fashion. For example, it is advisable to keep input and output signal magnitudes fixed to, e.g., unit variance, and to avoid large variation in gradient magnitudes on a per-sample basis [5, 21]. Training a neural network to model <em>D</em> directly would be far from ideal — for example, as the input <em>x</em> = <em>y</em> + <em>n</em> is a combination of clean signal <em>y</em> and noise <em>n</em> ∼ <em>N</em>(<strong>0</strong>, <em>σ</em><sup>2</sup><strong>I</strong>), its magnitude varies immensely depending on noise level <em>σ</em>. For this reason, the common practice is to not represent <em>D</em><sub>θ</sub> as a neural network directly, but instead train a different network <em>F</em><sub>θ</sub> from which <em>D</em><sub>θ</sub> is derived.</p>
<p>Previous methods [37, 47, 49] address the input scaling via a <em>σ</em>-dependent normalization factor and attempt to precondition the output by training <em>F</em><sub>θ</sub> to predict <em>n</em> scaled to unit variance, from which the signal is then reconstructed via <em>D</em><sub>θ</sub>(<em>x</em>; <em>σ</em>) = <em>x</em> − <em>σF</em><sub>θ</sub>(·). This has the drawback that at large <em>σ</em>, the network needs to fine-tune its output carefully to cancel out the existing noise <em>n</em> exactly and give the output at the correct scale; note that any errors made by the network are amplified by a factor of <em>σ</em>. In this situation, it would seem much easier to predict the expected output <em>D</em>(<em>x</em>; <em>σ</em>) directly. In the same spirit as previous parameterizations that adaptively mix signal and noise (e.g., [10, 44, 53]), we propose to precondition the neural network with a <em>σ</em>-dependent skip connection that allows it to estimate either <em>y</em> or <em>n</em>, or something in between. We thus write <em>D</em><sub>θ</sub> in the following form:</p>
<p>$$D_{\theta}(\mathbf{x}; \sigma) = c_{\text{skip}}(\sigma) \mathbf{x} + c_{\text{out}}(\sigma) F_{\theta}(c_{\text{in}}(\sigma) \mathbf{x}; c_{\text{noise}}(\sigma)),\tag{7}$$</p>
<p>where <em>F</em><sub>θ</sub> is the neural network to be trained, <em>c</em><sub>skip</sub>(<em>σ</em>) modulates the skip connection, <em>c</em><sub>in</sub>(<em>σ</em>) and <em>c</em><sub>out</sub>(<em>σ</em>) scale the input and output magnitudes, and <em>c</em><sub>noise</sub>(<em>σ</em>) maps noise level <em>σ</em> into a conditioning input for <em>F</em><sub>θ</sub>. Taking a weighted expectation of Eq. 2 over the noise levels gives the overall training loss ℝ<sub>σ, y, n</sub> [λ(σ) ∥<em>D</em>(y + n; σ) − y∥2<sup>2</sup>], where <em>σ</em> ∼ <em>p</em><sub>train</sub>, <em>y</em> ∼ <em>p</em><sub>data</sub>, and <em>n</em> ∼ <em>N</em>(<strong>0</strong>, <em>σ</em><sup>2</sup><strong>I</strong>). The probability of sampling a given noise level <em>σ</em> is given by <em>p</em><sub>train</sub>(<em>σ</em>) and the corresponding weight is given by λ(<em>σ</em>). We can equivalently express this loss with respect to the raw network output <em>F</em><sub>θ</sub> in Eq. 7:</p>
<p>$$\mathbb{E}<em _text_out="\text{out">{\sigma,\mathbf{y},\mathbf{n}} \left[ \underbrace{\lambda(\sigma) c</em>}}(\sigma)^2<em _theta="\theta">{\text{effective weight}} \left|\underbrace{F</em>}(c_{\text{in}}(\sigma) \cdot (\mathbf{y} + \mathbf{n}); c_{\text{noise}}(\sigma))<em _text_out="\text{out">{\text{network output}} - \underbrace{\frac{1}{c</em>}}(\sigma)}(\mathbf{y} - c_{\text{skip}}(\sigma) \cdot (\mathbf{y} + \mathbf{n}))<em 2="2">{\text{effective training target}} \right|</em> \right]. \qquad (8)$$}^{2</p>
<p>This form reveals the effective training target of <em>F</em><sub>θ</sub>, allowing us to determine suitable choices for the preconditioning functions from first principles. As detailed in Appendix B.6, we derive our choices</p>
<p>Table 2: Evaluation of our training improvements. The starting point (config A) is VP \&amp; VE using our deterministic sampler. At the end (configs E,F), VP \&amp; VE only differ in the architecture of $F_{\theta}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CIFAR-10 [29] at $32 \times 32$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FFHQ [27] $64 \times 64$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AFHQv2 [7] $64 \times 64$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conditional</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unconditional</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unconditional</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unconditional</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Training configuration</td>
<td style="text-align: center;">VP</td>
<td style="text-align: center;">VE</td>
<td style="text-align: center;">VP</td>
<td style="text-align: center;">VE</td>
<td style="text-align: center;">VP</td>
<td style="text-align: center;">VE</td>
<td style="text-align: center;">VP</td>
<td style="text-align: center;">VE</td>
</tr>
<tr>
<td style="text-align: center;">A Baseline [49] (* pre-trained)</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">$3.01^{*}$</td>
<td style="text-align: center;">$3.77^{*}$</td>
<td style="text-align: center;">3.39</td>
<td style="text-align: center;">25.95</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">18.52</td>
</tr>
<tr>
<td style="text-align: center;">B + Adjust hyperparameters</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">3.13</td>
<td style="text-align: center;">22.53</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">23.12</td>
</tr>
<tr>
<td style="text-align: center;">C + Redistribute capacity</td>
<td style="text-align: center;">2.08</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">2.83</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">41.62</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">15.04</td>
</tr>
<tr>
<td style="text-align: center;">D + Our preconditioning</td>
<td style="text-align: center;">2.09</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">2.29</td>
<td style="text-align: center;">3.10</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">3.39</td>
<td style="text-align: center;">2.79</td>
<td style="text-align: center;">3.81</td>
</tr>
<tr>
<td style="text-align: center;">E + Our loss function</td>
<td style="text-align: center;">1.88</td>
<td style="text-align: center;">1.86</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">1.99</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">2.81</td>
<td style="text-align: center;">2.29</td>
<td style="text-align: center;">2.28</td>
</tr>
<tr>
<td style="text-align: center;">F + Non-leaky augmentation</td>
<td style="text-align: center;">1.79</td>
<td style="text-align: center;">1.79</td>
<td style="text-align: center;">1.97</td>
<td style="text-align: center;">1.98</td>
<td style="text-align: center;">2.39</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">2.16</td>
</tr>
<tr>
<td style="text-align: center;">NFE</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">79</td>
</tr>
</tbody>
</table>
<p>shown in Table 1 by requiring network inputs and training targets to have unit variance ( $c_{\text {in }}, c_{\text {out }}$ ), and amplifying errors in $F_{\theta}$ as little as possible ( $c_{\text {skip }}$ ). The formula for $c_{\text {noise }}$ is chosen empirically.
Table 2 shows FID for a series of training setups, evaluated using our deterministic sampler from Section 3. We start with the baseline training setup of Song et al. [49], which differs considerably between the VP and VE cases; we provide separate results for each (config A). To obtain a more meaningful point of comparison, we re-adjust the basic hyperparameters (config B) and improve the expressive power of the model (config C) by removing the lowest-resolution layers and doubling the capacity of the highest-resolution layers instead; see Appendix F. 3 for further details. We then replace the original choices of $\left{c_{\text {in }}, c_{\text {out }}, c_{\text {noise }}, c_{\text {skip }}\right}$ with our preconditioning (config D), which keeps the results largely unchanged - except for VE that improves considerably at $64 \times 64$ resolution. Instead of improving FID per se, the main benefit of our preconditioning is that it makes the training more robust, enabling us to turn our focus on redesigning the loss function without adverse effects.</p>
<p>Loss weighting and sampling. Eq. 8 shows that training $F_{\theta}$ as preconditioned in Eq. 7 incurs an effective per-sample loss weight of $\lambda(\sigma) c_{\text {out }}(\sigma)^{2}$. To balance the effective loss weights, we set $\lambda(\sigma)=1 / c_{\text {out }}(\sigma)^{2}$, which also equalizes the initial training loss over the entire $\sigma$ range as shown in Figure 5a (green curve). Finally, we need to select $p_{\text {train }}(\sigma)$, i.e., how to choose noise levels during training. Inspecting the per- $\sigma$ loss after training (blue and orange curves) reveals that a significant reduction is possible only at intermediate noise levels; at very low levels, it is both difficult and irrelevant to discern the vanishingly small noise component, whereas at high levels the training targets are always dissimilar from the correct answer that approaches dataset average. Therefore, we target the training efforts to the relevant range using a simple log-normal distribution for $p_{\text {train }}(\sigma)$ as detailed in Table 1 and illustrated in Figure 5a (red curve).
Table 2 shows that our proposed $p_{\text {train }}$ and $\lambda$ (config E) lead to a dramatic improvement in FID in all cases when used in conjunction with our preconditioning (config D). In concurrent work, Choi et al. [6] propose a similar scheme to prioritize noise levels that are most relevant w.r.t. forming the perceptually recognizable content of the image. However, they only consider the choice of $\lambda$ in isolation, which results in a smaller overall improvement.</p>
<p>Augmentation regularization. To prevent potential overfitting that often plagues diffusion models with smaller datasets, we borrow an augmentation pipeline from the GAN literature [25]. The pipeline consists of various geometric transformations (see Appendix F.2) that we apply to a training image prior to adding noise. To prevent the augmentations from leaking to the generated images, we provide the augmentation parameters as a conditioning input to $F_{\theta}$; during inference we set the them to zero to guarantee that only non-augmented images are generated. Table 2 shows that data augmentation provides a consistent improvement (config F) that yields new state-of-the-art FIDs of 1.79 and 1.97 for conditional and unconditional CIFAR-10, beating the previous records of 1.85 [45] and 2.10 [53].</p>
<p>Stochastic sampling revisited. Interestingly, the relevance of stochastic sampling appears to diminish as the model itself improves, as shown in Figure 5b,c. When using our training setup in CIFAR-10 (Figure 5b), the best results were obtained with deterministic sampling, and any amount of stochastic sampling was detrimental.</p>
<p>ImageNet-64. As a final experiment, we trained a class-conditional ImageNet-64 model from scratch using our proposed training improvements. This model achieved a new state-of-the-art FID of</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (a) Observed initial (green) and final loss per noise level, representative of the the $32 \times 32$ (blue) and $64 \times 64$ (orange) models considered in this paper. The shaded regions represent the standard deviation over 10 k random samples. Our proposed training sample density is shown by the dashed red curve. (b) Effect of $S_{\text {churn }}$ on unconditional CIFAR-10 with 256 steps (NFE $=511$ ). For the original training setup of Song et al. [49], stochastic sampling is highly beneficial (blue, green), while deterministic sampling ( $S_{\text {churn }}=0$ ) leads to relatively poor FID. For our training setup, the situation is reversed (orange, red); stochastic sampling is not only unnecessary but harmful. (c) Effect of $S_{\text {churn }}$ on class-conditional ImageNet-64 with 256 steps (NFE $=511$ ). In this more challenging scenario, stochastic sampling turns out to be useful again. Our training setup improves the results for both deterministic and stochastic sampling.
1.36 compared to the previous record of 1.48 [17]. We used the ADM architecture [9] with no changes, and trained it using our config E with minimal tuning; see Appendix F. 3 for details. We did not find overfitting to be a concern, and thus chose to not employ augmentation regularization. As shown in Figure 5c, the optimal amount of stochastic sampling was much lower than with the pre-trained model, but unlike with CIFAR-10, stochastic sampling was clearly better than deterministic sampling. This suggests that more diverse datasets continue to benefit from stochastic sampling.</p>
<h1>6 Conclusions</h1>
<p>Our approach of putting diffusion models to a common framework exposes a modular design. This allows a targeted investigation of individual components, potentially helping to better cover the viable design space. In our tests this let us simply replace the samplers in various earlier models, drastically improving the results. For example, in ImageNet-64 our sampler turned an average model (FID 2.07) to a challenger (1.55) for the previous SOTA model (1.48) [17], and with training improvements achieved SOTA FID of 1.36. We also obtained new state-of-the-art results on CIFAR-10 while using only 35 model evaluations, deterministic sampling, and a small network. The current high-resolution diffusion models rely either on separate super-resolution steps [17, 36, 40], subspace projection [23], very large networks [9, 49], or hybrid approaches [39, 42, 53] — we believe that our contributions are orthogonal to these extensions. That said, many of our parameter values may need to be re-adjusted for higher resolution datasets. Furthermore, we feel that the precise interaction between stochastic sampling and the training objective remains an interesting question for future work.</p>
<p>Societal impact. Our advances in sample quality can potentially amplify negative societal effects when used in a large-scale system like DALL-E 2, including types of disinformation or emphasizing sterotypes and harmful biases [34]. The training and sampling of diffusion models needs a lot of electricity; our project consumed $\sim 250 \mathrm{MWh}$ on an in-house cluster of NVIDIA V100s.</p>
<p>Acknowledgments. We thank Jaakko Lehtinen, Ming-Yu Liu, Tuomas Kynkäänniemi, Axel Sauer, Arash Vahdat, and Janne Hellsten for discussions and comments, and Tero Kuosmanen, Samuel Klenberg, and Janne Hellsten for maintaining our compute infrastructure.</p>
<h2>References</h2>
<p>[1] B. D. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313-326, 1982.
[2] U. M. Ascher and L. R. Petzold. Computer Methods for Ordinary Differential Equations and DifferentialAlgebraic Equations. Society for Industrial and Applied Mathematics, 1998.</p>
<p>[3] F. Bao, C. Li, J. Zhu, and B. Zhang. Analytic-DPM: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In Proc. ICLR, 2022.
[4] D. Baranchuk, A. Voynov, I. Rubachev, V. Khrulkov, and A. Babenko. Label-efficient semantic segmentation with diffusion models. In Proc. ICLR, 2022.
[5] C. M. Bishop. Neural networks for pattern recognition. Oxford University Press, USA, 1995.
[6] J. Choi, J. Lee, C. Shin, S. Kim, H. Kim, and S. Yoon. Perception prioritized training of diffusion models. In Proc. CVPR, 2022.
[7] Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha. StarGAN v2: Diverse image synthesis for multiple domains. In Proc. CVPR, 2020.
[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In Proc. CVPR, 2009.
[9] P. Dhariwal and A. Q. Nichol. Diffusion models beat GANs on image synthesis. In Proc. NeurIPS, 2021.
[10] T. Dockhorn, A. Vahdat, and K. Kreis. Score-based generative modeling with critically-damped Langevin diffusion. In Proc. ICLR, 2022.
[11] J. R. Dormand and P. J. Prince. A family of embedded Runge-Kutta formulae. Journal of computational and applied mathematics, 6(1):19-26, 1980.
[12] J. B. J. Fourier, G. Darboux, et al. Théorie analytique de la chaleur, volume 504. Didot Paris, 1822.
[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. In Proc. NIPS, 2014.
[14] U. Grenander and M. I. Miller. Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549-581, 1994.
[15] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Proc. NIPS, 2017.
[16] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Proc. NeurIPS, 2020.
[17] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23, 2022.
[18] J. Ho and T. Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.
[19] J. Ho, T. Salimans, A. A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. In Proc. ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022.
[20] C.-W. Huang, J. H. Lim, and A. C. Courville. A variational perspective on diffusion-based generative models and score matching. In Proc. NeurIPS, 2021.
[21] L. Huang, J. Qin, Y. Zhou, F. Zhu, L. Liu, and L. Shao. Normalization techniques in training DNNs: Methodology, analysis and application. CoRR, abs/2009.12836, 2020.
[22] A. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24):695-709, 2005.
[23] B. Jing, G. Corso, R. Berlinghieri, and T. Jaakkola. Subspace diffusion generative models. In Proc. ECCV, 2022.
[24] A. Jolicoeur-Martineau, K. Li, R. Piché-Taillefer, T. Kachman, and I. Mitliagkas. Gotta go fast when generating data with score-based models. CoRR, abs/2105.14080, 2021.
[25] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila. Training generative adversarial networks with limited data. In Proc. NeurIPS, 2020.
[26] T. Karras, M. Aittala, S. Laine, E. Härkönen, J. Hellsten, J. Lehtinen, and T. Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021.
[27] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In Proc. CVPR, 2018.
[28] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro. DiffWave: A versatile diffusion model for audio synthesis. In Proc. ICLR, 2021.
[29] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
[30] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala, and T. Aila. Noise2Noise: Learning image restoration without clean data. In Proc. ICML, 2018.
[31] L. Liu, Y. Ren, Z. Lin, and Z. Zhao. Pseudo numerical methods for diffusion models on manifolds. In Proc. ICLR, 2022.
[32] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Proc. NeurIPS, 2022.
[33] E. Luhman and T. Luhman. Knowledge distillation in iterative generative models for improved sampling speed. CoRR, abs/2101.02388, 2021.
[34] P. Mishkin, L. Ahmad, M. Brundage, G. Krueger, and G. Sastry. DALL-E 2 preview - risks and limitations. OpenAI, 2022.
[35] E. Nachmani and S. Dovrat. Zero-shot translation using diffusion models. CoRR, abs/2111.01471, 2021.</p>
<p>[36] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In Proc. ICML, 2022.
[37] A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In Proc. ICML, volume 139, pages 8162-8171, 2021.
[38] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov. Grad-TTS: A diffusion probabilistic model for text-to-speech. In Proc. ICML, volume 139, pages 8599-8608, 2021.
[39] K. Preechakul, N. Chatthee, S. Wizadwongsa, and S. Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In Proc. CVPR, 2022.
[40] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with CLIP latents. Technical report, OpenAI, 2022.
[41] A. J. Roberts. Modify the improved Euler scheme to integrate stochastic differential equations. CoRR, abs/1210.0933, 2012.
[42] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proc. CVPR, 2022.
[43] C. Saharia, W. Chan, H. Chang, C. A. Lee, J. Ho, T. Salimans, D. J. Fleet, and M. Norouzi. Palette: Image-to-image diffusion models. In Proc. SIGGRAPH, 2022.
[44] T. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. In Proc. ICLR, 2022.
[45] A. Sauer, K. Schwarz, and A. Geiger. StyleGAN-XL: Scaling StyleGAN to large diverse datasets. In Proc. SIGGRAPH, 2022.
[46] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proc. ICML, pages 2256-2265, 2015.
[47] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In Proc. ICLR, 2021.
[48] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In Proc. NeurIPS, 2019.
[49] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In Proc. ICLR, 2021.
[50] E. Süli and D. F. Mayers. An Introduction to Numerical Analysis. Cambridge University Press, 2003.
[51] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception architecture for computer vision. In Proc. CVPR, 2016.
[52] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. T. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Proc. NeurIPS, 2020.
[53] A. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space. In Proc. NeurIPS, 2021.
[54] P. Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661-1674, 2011.
[55] D. Watson, W. Chan, J. Ho, and M. Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In Proc. ICLR, 2022.
[56] D. Watson, J. Ho, M. Norouzi, and W. Chan. Learning to efficiently sample from diffusion probabilistic models. CoRR, abs/2106.03802, 2021.
[57] J. Wolleb, R. Sandkühler, F. Bieder, P. Valmaggia, and P. C. Cattin. Diffusion models for implicit image segmentation ensembles. In Medical Imaging with Deep Learning, 2022.
[58] Q. Zhang and Y. Chen. Diffusion normalizing flow. In Proc. NeurIPS, 2021.
[59] Q. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator. CoRR, abs/2204.13902, 2022.</p>
<h1>Appendices</h1>
<h2>A Additional results</h2>
<p>Figure 6 presents generated images for class-conditional ImageNet-64 [8] using the pre-trained ADM model by Dhariwal and Nichol [9]. The original DDIM [47] and iDDPM [37] samplers are compared to ours in both deterministic and stochastic settings (Sections 3 and 4). Figure 7 shows the corresponding results that we obtain by training the model from scratch using our improved training configuration (Section 5).</p>
<p>The original samplers and training configurations by Song et al. [49] are compared to ours in Figures 8 and 9 (unconditional CIFAR-10 [29]), Figure 10 (class-conditional CIFAR-10), and Figure 11 (FFHQ [27] and AFHQv2 [7]). For ease of comparison, the same latent codes $\boldsymbol{x}_{0}$ are used for each dataset/scenario across different training configurations and ODE choices. Figure 12 shows generated image quality with various NFE when using deterministic sampling.
Tables 3 and 4 summarize the numerical results on deterministic and stochastic sampling methods in various datasets, previously shown as functions of NFE in Figures 2 and 4.</p>
<h2>B Derivation of formulas</h2>
<h2>B. 1 Original ODE / SDE formulation from previous work</h2>
<p>Song et al. [49] define their forward SDE (Eq. 5 in [49]) as</p>
<p>$$
\mathrm{d} \boldsymbol{x}=\boldsymbol{f}(\boldsymbol{x}, t) \mathrm{d} t+g(t) \mathrm{d} \omega_{t}
$$</p>
<p>where $\omega_{t}$ is the standard Wiener process and $\boldsymbol{f}(\cdot, t): \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ and $g(\cdot): \mathbb{R} \rightarrow \mathbb{R}$ are the drift and diffusion coefficients, respectively, where $d$ is the dimensionality of the dataset. These coefficients are selected differently for the variance preserving (VP) and variance exploding (VE) formulations, and $\boldsymbol{f}(\cdot)$ is always of the form $\boldsymbol{f}(\boldsymbol{x}, t)=f(t) \boldsymbol{x}$, where $f(\cdot): \mathbb{R} \rightarrow \mathbb{R}$. Thus, the SDE can be equivalently written as</p>
<p>$$
\mathrm{d} \boldsymbol{x}=f(t) \boldsymbol{x} \mathrm{d} t+g(t) \mathrm{d} \omega_{t}
$$</p>
<p>The perturbation kernels of this SDE (Eq. 29 in [49]) have the general form</p>
<p>$$
p_{0 t}(\boldsymbol{x}(t) \mid \boldsymbol{x}(0))=\mathcal{N}\left(\boldsymbol{x}(t) ; s(t) \boldsymbol{x}(0), s(t)^{2} \sigma(t)^{2} \mathbf{I}\right)
$$</p>
<p>where $\mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}, \boldsymbol{\Sigma})$ denotes the probability density function of $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ evaluated at $\boldsymbol{x}$,</p>
<p>$$
s(t)=\exp \left(\int_{0}^{t} f(\xi) \mathrm{d} \xi\right), \quad \text { and } \quad \sigma(t)=\sqrt{\int_{0}^{t} \frac{g(\xi)^{2}}{s(\xi)^{2}} \mathrm{~d} \xi}
$$</p>
<p>The marginal distribution $p_{t}(\boldsymbol{x})$ is obtained by integrating the perturbation kernels over $\boldsymbol{x}(0)$ :</p>
<p>$$
p_{t}(\boldsymbol{x})=\int_{\mathbb{R}^{d}} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}<em _data="{data" _text="\text">{0}\right) p</em>}}\left(\boldsymbol{x<em 0="0">{0}\right) \mathrm{d} \boldsymbol{x}</em>
$$</p>
<p>Song et al. [49] define the probability flow ODE (Eq. 13 in [49]) so that it obeys this same $p_{t}(\boldsymbol{x})$ :</p>
<p>$$
\mathrm{d} \boldsymbol{x}=\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^{2} \nabla_{\boldsymbol{x}} \log p_{t}(\boldsymbol{x})\right] \mathrm{d} t
$$</p>
<h2>B. 2 Our ODE formulation (Eq. 1 and Eq. 4)</h2>
<p>The original ODE formulation (Eq. 14) is built around the functions $f$ and $g$ that correspond directly to specific terms that appear in the formula; the properties of the marginal distribution (Eq. 12) can only be derived indirectly based on these functions. However, $f$ and $g$ are of little practical interest in themselves, whereas the marginal distributions are of utmost importance in terms of training the model in the first place, bootstrapping the sampling process, and understanding how the ODE behaves in practice. Given that the idea of the probability flow ODE is to match a particular set of marginal</p>
<p>Deterministic, Original sampler (DDIM)
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>FID 2.91 NFE 250
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>FID 2.01 NFE 512</p>
<p>Deterministic, Our sampler (Alg. 1)
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>FID 2.66 NFE 79
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>FID 1.55 NFE 1023</p>
<p>Figure 6: Results for different samplers on class-conditional ImageNet [8] at $64 \times 64$ resolution, using the pre-trained model by Dhariwal and Nichol [9]. The cases correspond to dots in Figures 2c and 4c.</p>
<p>Deterministic, Our sampler \&amp; training configuration
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>FID 2.23 NFE 79
Stochastic, Our sampler \&amp; training configuration
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>FID 1.36 NFE 511
Figure 7: Results for our training configuration on class-conditional ImageNet [8] at $64 \times 64$ resolution, using our deterministic and stochastic samplers.</p>
<p>Deterministic, Original sampler (p.flow), VP
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>FID 2.94 NFE 256
Deterministic, Our sampler (Alg. 1), VP
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>FID 3.01 NFE 35
Stochastic, Original sampler (E-M), VP
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>FID 2.55 NFE 1024
Stochastic, Our sampler (Alg. 2), VP
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>FID 2.27 NFE 511</p>
<p>Deterministic, Original sampler (p.flow), VE
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>FID 5.45 NFE 8192
Deterministic, Our sampler (Alg. 1), VE
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>FID 3.82 NFE 27
Stochastic, Original sampler ( $\mathrm{P}-\mathrm{C}$ ), VE
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>FID 2.46 NFE 2048
Stochastic, Our sampler (Alg. 2), VE
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>FID 2.23 NFE 2047</p>
<p>Figure 8: Results for different samplers on unconditional CIFAR-10 [29] at $32 \times 32$ resolution, using the pre-trained models by Song et al. [49]. The cases correspond to dots in Figures 2a,b and 4a,b.</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 9: Results for different training configurations on unconditional CIFAR-10 [29] at $32 \times 32$ resolution, using our deterministic sampler with the same set of latent codes $\left(\boldsymbol{x}_{0}\right)$ in each case.</p>
<p><img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 10: Results for different training configurations on class-conditional CIFAR-10 [29] at $32 \times 32$ resolution, using our deterministic sampler with the same set of latent codes $\left(\boldsymbol{x}_{0}\right)$ in each case.</p>
<p>FFHQ, Original training (config A), VP
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>FID 3.39 NFE 79
FFHQ, Our training (config F), VP
<img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>FID 2.39 NFE 79
AFHQv2, Original training (config A), VP
<img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>FID 2.58 NFE 79
AFHQv2, Our training (config F), VP
<img alt="img-24.jpeg" src="img-24.jpeg" /></p>
<p>FID 1.96 NFE 79</p>
<p>FFHQ, Original training (config A), VE
<img alt="img-25.jpeg" src="img-25.jpeg" /></p>
<p>FID 2.53 NFE 79
AFHQv2, Original training (config A), VE
<img alt="img-26.jpeg" src="img-26.jpeg" /></p>
<p>FID 18.52 NFE 79
AFHQv2, Our training (config F), VE
<img alt="img-27.jpeg" src="img-27.jpeg" /></p>
<p>FID 2.16 NFE 79</p>
<p>Figure 11: Results for different training configurations on FFHQ [27] and AFHQv2 [7] at $64 \times 64$ resolution, using our deterministic sampler with the same set of latent codes $\left(\boldsymbol{x}_{0}\right)$ in each case.</p>
<p><img alt="img-28.jpeg" src="img-28.jpeg" /></p>
<p>Figure 12: Image quality and FID as a function of NFE using our deterministic sampler. At $32 \times 32$ resolution, reasonable image quality is reached around NFE $=13$, but FID keeps improving until NFE $=35$. At $64 \times 64$ resolution, reasonable image quality is reached around NFE $=19$, but FID keeps improving until NFE $=79$.</p>
<p>Table 3: Evaluation of our improvements to deterministic sampling. The values correspond to the curves shown in Figure 2. We summarize each curve with two key values: the lowest observed FID for any NFE ("FID"), and the lowest NFE whose FID is within 3\% of the lowest FID ("NFE"). The values marked with "-" are identical to the ones above them, because our sampler uses the same $\sigma(t)$ and $s(t)$ as DDIM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Unconditional CIFAR-10 at $32 \times 32$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Class-conditional</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">VP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">VE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ImageNet-64</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sampling method</td>
<td style="text-align: center;">FID $\downarrow$</td>
<td style="text-align: center;">NFE $\downarrow$</td>
<td style="text-align: center;">FID $\downarrow$</td>
<td style="text-align: center;">NFE $\downarrow$</td>
<td style="text-align: center;">FID $\downarrow$</td>
<td style="text-align: center;">NFE $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Original sampler [49, 9]</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">5.45</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td style="text-align: left;">Our Algorithm 1</td>
<td style="text-align: center;">$\mathbf{2 . 7 9}$</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">4.78</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">384</td>
</tr>
<tr>
<td style="text-align: left;">+ Heun \&amp; our $t_{i}$</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">255</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">191</td>
<td style="text-align: center;">$\mathbf{2 . 6 4}$</td>
<td style="text-align: center;">$\mathbf{7 9}$</td>
</tr>
<tr>
<td style="text-align: left;">+ Our $\sigma(t) \&amp; s(t)$</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">$\mathbf{3 5}$</td>
<td style="text-align: center;">3.73</td>
<td style="text-align: center;">$\mathbf{2 7}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Black-box RK45</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">$\mathbf{3 . 6 9}$</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">131</td>
</tr>
</tbody>
</table>
<p>Table 4: Evaluation and ablations of our improvements to stochastic sampling. The values correspond to the curves shown in Figure 4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Unconditional CIFAR-10 at $32 \times 32$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Class-conditional</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">VP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">VE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ImageNet-64</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sampling method</td>
<td style="text-align: center;">FID $\downarrow$</td>
<td style="text-align: center;">NFE $\downarrow$</td>
<td style="text-align: center;">FID $\downarrow$</td>
<td style="text-align: center;">NFE $\downarrow$</td>
<td style="text-align: center;">FID $\downarrow$</td>
<td style="text-align: center;">NFE $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Deterministic baseline (Alg. 1)</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">$\mathbf{3 5}$</td>
<td style="text-align: center;">3.73</td>
<td style="text-align: center;">$\mathbf{2 7}$</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">$\mathbf{7 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Alg. 2, $S_{\min , \max }=[0, \infty], S_{\text {noise }}=1$</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2.97</td>
<td style="text-align: center;">383</td>
<td style="text-align: center;">1.86</td>
<td style="text-align: center;">383</td>
</tr>
<tr>
<td style="text-align: left;">Alg. 2, $S_{\min , \max }=[0, \infty]$</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">511</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">767</td>
</tr>
<tr>
<td style="text-align: left;">Alg. 2, $S_{\text {noise }}=1$</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2.84</td>
<td style="text-align: center;">191</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">255</td>
</tr>
<tr>
<td style="text-align: left;">Alg. 2, Optimal settings</td>
<td style="text-align: center;">$\mathbf{2 . 2 7}$</td>
<td style="text-align: center;">383</td>
<td style="text-align: center;">$\mathbf{2 . 2 3}$</td>
<td style="text-align: center;">767</td>
<td style="text-align: center;">$\mathbf{1 . 5 5}$</td>
<td style="text-align: center;">511</td>
</tr>
<tr>
<td style="text-align: left;">Previous work [49, 9]</td>
<td style="text-align: center;">2.55</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">2.46</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">2.01</td>
<td style="text-align: center;">384</td>
</tr>
</tbody>
</table>
<p>distributions, it makes sense to treat the marginal distributions as first-class citizens and define the ODE directly based on $\sigma(t)$ and $s(t)$, eliminating the need for $f(t)$ and $g(t)$.
Let us start by expressing the marginal distribution of Eq. 13 in closed form:</p>
<p>$$
\begin{aligned}
p_{t}(\boldsymbol{x}) &amp; =\int_{\mathbb{R}^{d}} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}<em _data="{data" _text="\text">{0}\right) p</em>}}\left(\boldsymbol{x<em 0="0">{0}\right) \mathrm{d} \boldsymbol{x}</em> \
&amp; =\int_{\mathbb{R}^{d}} p_{\text {data }}\left(\boldsymbol{x}<em 0="0">{0}\right)\left[\mathcal{N}\left(\boldsymbol{x} ; s(t) \boldsymbol{x}</em>}, s(t)^{2} \sigma(t)^{2} \mathbf{I}\right)\right] \mathrm{d} \boldsymbol{x<em _mathbb_R="\mathbb{R">{0} \
&amp; =\int</em>}^{d}} p_{\text {data }}\left(\boldsymbol{x<em 0="0">{0}\right)\left[s(t)^{-d} \mathcal{N}\left(\boldsymbol{x} / s(t) ; \boldsymbol{x}</em>}, \sigma(t)^{2} \mathbf{I}\right)\right] \mathrm{d} \boldsymbol{x<em _mathbb_R="\mathbb{R">{0} \
&amp; =s(t)^{-d} \int</em>}^{d}} p_{\text {data }}\left(\boldsymbol{x<em 0="0">{0}\right) \mathcal{N}\left(\boldsymbol{x} / s(t) ; \boldsymbol{x}</em> \
&amp; =s(t)^{-d}\left}, \sigma(t)^{2} \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_{0<a href="\boldsymbol{x} / s(t)">p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^{2} \mathbf{I}\right)\right</a>
\end{aligned}
$$</p>
<p>where $p_{a} * p_{b}$ denotes the convolution of probability density functions $p_{a}$ and $p_{b}$. The expression inside the brackets corresponds to a mollified version of $p_{\text {data }}$ obtained by adding i.i.d. Gaussian noise to the samples. Let us denote this distribution by $p(\boldsymbol{x} ; \sigma)$ :</p>
<p>$$
p(\boldsymbol{x} ; \sigma)=p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^{2} \mathbf{I}\right) \quad \text { and } \quad p_{t}(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))
$$</p>
<p>We can now express the probability flow ODE (Eq. 14) using $p(\boldsymbol{x} ; \sigma)$ instead of $p_{t}(\boldsymbol{x})$ :</p>
<p>$$
\begin{aligned}
\mathrm{d} \boldsymbol{x} &amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^{2} \nabla_{\boldsymbol{x}} \log \left[p_{t}(\boldsymbol{x})\right]\right] \mathrm{d} t \
&amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^{2} \nabla_{\boldsymbol{x}} \log \left[s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))\right]\right] \mathrm{d} t \
&amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^{2}\left[\nabla_{\boldsymbol{x}} \log s(t)^{-d}+\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right]\right] \mathrm{d} t \
&amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^{2} \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t
\end{aligned}
$$</p>            </div>
        </div>

    </div>
</body>
</html>
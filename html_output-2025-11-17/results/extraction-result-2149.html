<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2149 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2149</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2149</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-278537074</p>
                <p><strong>Paper Title:</strong> From data extraction to analysis: a comparative study of ELISE capabilities in scientific literature</p>
                <p><strong>Paper Abstract:</strong> The exponential growth of scientific literature presents challenges for pharmaceutical, biotechnological, and Medtech industries, particularly in regulatory documentation, clinical research, and systematic reviews. Ensuring accurate data extraction, literature synthesis, and compliance with industry standards require AI tools that not only streamline workflows but also uphold scientific rigor. This study evaluates the performance of AI tools designed for bibliographic review, data extraction, and scientific synthesis, assessing their impact on decision-making, regulatory compliance, and research productivity. The AI tools assessed include general-purpose models like ChatGPT and specialized solutions such as ELISE (Elevated LIfe SciencEs), SciSpace/Typeset, Humata, and Epsilon. The evaluation is based on three main criteria: Extraction, Comprehension, and Analysis with Compliance and Traceability (ECACT) as additional dimensions. Human experts established reference benchmarks, while AI Evaluator models ensure objective performance measurement. The study introduces the ECACT score, a structured metric assessing AI reliability in scientific literature analysis, regulatory reporting and clinical documentation. Results demonstrate that ELISE consistently outperforms other AI tools, excelling in precise data extraction, deep contextual comprehension, and advanced content analysis. ELISE’s ability to generate traceable, well-reasoned insights makes it particularly well-suited for high-stakes applications such as regulatory affairs, clinical trials, and medical documentation, where accuracy, transparency, and compliance are paramount. Unlike other AI tools, ELISE provides expert-level reasoning and explainability, ensuring AI-generated insights align with industry best practices. ChatGPT is efficient in data retrieval but lacks precision in complex analysis, limiting its use in high-stakes decision-making. Epsilon, Humata, and SciSpace/Typeset exhibit moderate performance, with variability affecting their reliability in critical applications. In conclusion, while AI tools such as ELISE enhance literature review, regulatory writing, and clinical data interpretation, human oversight remains essential to validate AI outputs and ensure compliance with scientific and regulatory standards. For pharmaceutical, biotechnological, and Medtech industries, AI integration must strike a balance between automation and expert supervision to maintain data integrity, transparency, and regulatory adherence.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2149.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2149.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELISE (Elevated LIfe SciencEs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized AI literature-analysis system developed by Biolevate combining advanced NLP, retrieval strategies and a custom parser (Matsu) to extract, comprehend, analyze and provide traceable outputs for biomedical and regulatory documents. Evaluated in this study and consistently outperformed other tested tools across Extraction, Comprehension, Analysis, Compliance and Traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ELISE</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid NLP/LLM + retrieval + custom parser</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical literature analysis, regulatory/clinical documentation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>structured data extraction (metadata, tables), scientific summaries, critical analyses, traceable insights for regulatory reporting</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>comparison to human-expert reference answers; automated evaluation by independent AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview); built-in traceability and compliance checks that highlight source document sections used to produce outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not formalized numerically; assessed implicitly by performance on challenging/unfamiliar tasks (graph/table parsing, inlay detection) and stability of scores with and without human reference (small score variation used as proxy for robustness to novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction: 87.50% global average; Comprehension: 8.23/10; Analysis: 7.98/10. Performance was high and stable across articles and languages relative to peers, and remained stable whether or not human reference answers were provided (Comprehension 8.32–8.42; Analysis 7.98–8.13).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>High — uses explicit traceability to source text and compliance checks; AI Evaluator models and human experts rated ELISE outputs as closely aligned with expert answers. No numeric false-positive/negative rates reported for validation, but demonstrably fewer incorrect extractions in case studies (e.g., only tool to correctly extract HR and exclusion counts).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not numerically reported; qualitatively low in study (few hallucinations observed compared to peers)</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not numerically reported; qualitatively low (ELISE retrieved expected values where others failed)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Minimal — ELISE's validation/alignment with human references changed little when evaluated without human reference answers, indicating robust validation for more novel/challenging tasks in this corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Small asymmetry: ELISE both generated and validated outputs effectively with minimal gap, attributed to parser + traceability + RAG-style retrieval; evidence suggests smaller generation-validation gap than for other systems.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Better-than-others on out-of-distribution/challenging items (graph interpretation, table/inlay parsing). No formal OOD metrics provided; case examples (Kaplan–Meier EFS, HR extraction) show ELISE succeeded where others failed.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified numerically; qualitatively better calibration implied by ELISE indicating margins of error and providing traceable source citations, suggesting more calibrated uncertainty communication than peers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported in study; described methods include additional retrieval and parsing steps implying higher cost than baseline LLM generation but no measurements provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Custom parser (Matsu), retrieval-augmented generation (RAG) approach, built-in traceability and compliance mechanisms, and human-in-the-loop evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ELISE consistently outperforms general-purpose and other specialized tools on extraction, comprehension and analysis in biomedical literature, and its built-in traceability and parser improve alignment with human expert references, reducing the generation-validation gap especially on novel/challenging tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2149.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2149.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A versatile general-purpose large language model evaluated for literature extraction, comprehension and analysis; strong at metadata extraction and fluent summarization but weaker at precise analytical tasks and traceability in regulated contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning and literature summarization (applied to biomedical papers in this study)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>conversational summaries, metadata extraction, hypothesis generation, literature synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>outputs compared to human expert reference answers; also assessed by independent AI Evaluator models; traceability was limited (model often did not cite document sections), and an example shows ChatGPT produced a DOI not present in the document (hallucination) indicating weak provenance-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not formalized; proxy comparisons include performance on tasks requiring structured parsing (graph/table) where ChatGPT performed poorly relative to ELISE, implying lower robustness on novel/structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction: 86.99% global average (high); Comprehension ~7.48/10 (moderate); Analysis ~7.10/10. Performance dropped on high-rigor analytical tasks (graph/table reading) and lacked consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Moderate/variable: AI Evaluator models sometimes scored ChatGPT leniently (notably divergence among evaluators observed), but human-aligned validation exposed hallucinations (e.g., DOI not present). No numeric validation metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not numerically reported; qualitative evidence of hallucination (providing an external DOI not in source) indicates non-negligible false positives in provenance-sensitive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported; qualitative evidence shows failures to extract correct numerical values from graphs/tables (false negatives for expected facts).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation scores for ChatGPT increased when human reference answers were not used (0.3–0.9 point increases for several models), suggesting AI Evaluator models may overestimate ChatGPT outputs in autonomous evaluation, and ChatGPT is less reliable on novel/structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Notable asymmetry: strong at fluent generation and retrieval-like tasks but weaker at producing objectively verifiable, traceable outputs; validation (traceability/provenance) often lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Worse on OOD/challenging tasks such as Kaplan–Meier curve reading and table-specific HR extraction — produced incorrect numerical values in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; evidence shows poor provenance calibration (hallucinated DOI) and variable evaluator agreement, implying calibration degrades on novel, structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human-in-the-loop verification, supplementary retrieval/traceability systems (RAG) recommended; study notes general LLMs lack built-in traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ChatGPT is excellent at metadata extraction and fluent summaries but exhibits hallucinations and poor traceability for provenance-sensitive outputs; its autonomous evaluation scores can be inflated by AI Evaluator models, especially when human references are absent, indicating a generation-validation gap on novel/structured tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2149.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2149.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Epsilon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epsilon (v2.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized AI literature-analysis tool focused on identifying research gaps and suggesting directions; in this study it showed moderate comprehension ability but weak structured data extraction performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Epsilon</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>specialized literature-analysis NLP system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific literature gap identification and summarization (biomedical focus in study)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>comprehension summaries, research-gap identification, suggestions for directions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>assessed against human reference answers and by AI Evaluator models; compliance and traceability evaluations included and penalized Epsilon for guideline violations and poor traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>no explicit novelty metric; novelty inferred from variation in scores between human-referenced and autonomous evaluation and performance on structured extraction tasks (poor).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction: 37.41% (low); Comprehension: 7.56/10 (moderate); Analysis: 6.98/10 (moderate); showed score increases when human references not used (suggests evaluator-inflation effect).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Moderate but inconsistent; failed many structured extraction tasks and violated compliance guidelines leading to reduced ECACT when traceability/compliance were considered.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not given numerically; qualitative indication of guideline violations and verbose/less reliable responses increased false acceptance risk.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation scores increased when human references were absent (suggests AI Evaluator models gave lenient autonomous assessments), indicating potential overestimation on novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Present — better at higher-level comprehension/gap identification than at precise, verifiable extraction; performance and validation diverge on structured/novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performed poorly on structured extraction and OOD-like tasks (e.g., graph reading); no numeric OOD metric.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; qualitative evidence of over-verbosity and guideline violations suggests calibration weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human oversight, enforcing compliance/traceability protocols; integration with retrieval/parsing methods recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Epsilon provides moderate comprehension but poor structured data extraction and fails compliance/traceability checks; autonomous AI evaluation may overestimate its outputs, widening the generation-validation gap on novel/structured tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2149.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2149.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Humata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Humata</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI-driven text analysis tool aimed at summarizing large volumes of documents; in this study it showed variable comprehension and analysis performance and weak extraction, especially across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Humata</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>specialized NLP summarization system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>document summarization and analysis (biomedical papers in study)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>summaries, text analyses, extraction of narrative elements</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>evaluated against human reference answers and AI Evaluator models; lacked robust traceability leading to lower ECACT scores and more variability between human-referenced and autonomous evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>inferred from variability across tasks and languages (performed better in English than French for some tasks), no explicit novelty metric.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction: 52.00% (moderate/low); Comprehension: variable ~7.46/10 globally; Analysis: 6.86/10 with high variability across articles and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Lower and inconsistent; greater variability indicates less reliable validation of generated outputs, especially on structured/challenging items.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported; qualitative variability suggests elevated risk of incorrect or inconsistent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance and validation varied more across languages and without human references, indicating sensitivity to novelty and format changes (e.g., French vs English).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Present — variable generation quality and inconsistent validation alignment, particularly on extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poorer and more variable on out-of-distribution/challenging tasks; no numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; evidence of inconsistent outputs and language sensitivity implies calibration issues.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human-in-the-loop verification, improved multilingual adaptation and traceability features recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Humata is effective for high-level summarization but shows variable and sometimes poor extraction and validation capabilities, especially across languages and on structured/challenging tasks, highlighting a generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2149.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2149.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciSpace/Typeset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciSpace (ex-Typeset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature exploration and analysis platform with search and highlighting capabilities; in this study it performed poorly on structured data extraction but moderately on comprehension/analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciSpace/Typeset</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>specialized literature-analysis/search platform (NLP-enabled)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific literature exploration and summarization (multi-domain, tested on biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>search-enhanced summaries, retrieval of document content, some extraction of entities</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>assessed against human reference answers and AI Evaluator models; lacked in extraction traceability leading to low Extraction scores and reduced ECACT when traceability/compliance considered.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not formalized; performance drops on structured or nonstandard document elements were used as proxy for low novelty robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction: 49.10% (low); Comprehension/Analysis: ~7.10/10 (moderate) with variable performance across tasks and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Moderate for comprehension but weak for extractive validation; failed to provide required exact exclusion counts and struggled with numerical/table reading.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not provided; qualitative evidence of failures in extraction suggests potential false negatives and some incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance degraded on nonstandard/structured tasks (graphs, tables) and language variants; validation degraded accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Present — reasonable comprehension but poor extractive validation and traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Weaker on OOD/structured tasks; examples show inability to provide exact excluded article numbers or numerical table values.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; traceability and compliance weaknesses indicate poor calibration for provenance-sensitive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Integration with stronger parsers and RAG, and human oversight to confirm extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SciSpace/Typeset can aid comprehension but struggles with precise extraction and traceability, resulting in a significant generation-validation gap for provenance- or number-sensitive tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2149.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2149.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Evaluator models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Independent high-performing LLMs used to objectively assess outputs from the evaluated AI tools by scoring them against human references and assessing autonomous interpretability; employed to reduce human bias in scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude 3.5 Sonnet; GPT-4o; o1 Preview</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models used as automated evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>evaluation and scoring of AI-generated scientific outputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generate evaluation scores, justifications, and comparisons between AI tool outputs and human reference answers</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>apply a standardized 10-point rubric to AI tool responses, compare anonymized vs identified outputs to check bias, and average scores across models to mitigate evaluator-specific bias; also used to autonomously assess responses without human references.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not explicit; novelty effect probed by comparing evaluator scores with and without human references and checking sensitivity to anonymization and tool identity.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (they evaluate rather than produce scientific discoveries); measured to be consistent across anonymized/identified conditions with minimal bias, except some evaluator divergence when judging ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provided structured scores and justifications; however, evidence shows evaluators can be more lenient when human reference answers are absent (leading to score inflation for some tools).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not applicable in standard sense; but qualitative evidence indicates AI evaluators sometimes over-score generated outputs (false acceptance of incorrect answers) particularly when human references were not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Evaluator assessments tended to be more lenient on autonomous (no-human-reference) evaluations, inflating scores for several tools; divergence among evaluator models occurred for some tool responses (ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Evaluators can mask generation-validation gaps by overestimating outputs when human references are absent, producing an asymmetry between apparent validation and true accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Evaluator agreement remained mostly stable across anonymized vs identified conditions; specific divergence noted for ChatGPT evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantitatively reported; qualitative issues include leniency in autonomous settings and occasional inter-evaluator disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Averaging multiple evaluator models; anonymized testing to check bias; pairing evaluator assessments with human reference answers to reduce overestimation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI Evaluator models provide scalable, relatively unbiased scoring when used in aggregate, but can overestimate AI-generated answers in autonomous settings and sometimes disagree, indicating they should complement — not replace — human validation for novel or high-stakes outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2149.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2149.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach combining retrieval mechanisms with LLM generation to ground outputs in retrieved, document-level evidence, recommended in the paper to reduce hallucinations and improve factual accuracy and traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid retrieval + generative model architecture</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>document-grounded generation for literature analysis and regulatory content</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>context-aware, evidence-grounded natural language generation (summaries, answers) relying on retrieved documents/paragraphs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>improves validation by grounding generations in retrieved sources and enabling traceability to source passages; validation still requires human checks for regulatory use but reduces unsupported hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not formalized here; novelty robustness argued qualitatively because retrieval reduces reliance on stale/pretrained knowledge and anchors responses to present documents.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not empirically measured in this paper but posited as an improvement over vanilla LLM generation for accuracy and traceability; ELISE uses retrieval strategies consistent with RAG principles to achieve better results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Expected improvement in factuality and provenance; paper reports ELISE (which employs retrieval strategies) had fewer provenance errors and better alignment with human references.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically; RAG described as reducing false positives/hallucinations relative to non-retrieval baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>RAG reduces novelty-related validation failures by grounding outputs in document evidence, but human oversight remains necessary for novel scientific inference.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>RAG narrows the asymmetry by providing source grounding, improving validation of generated outputs compared to ungrounded LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Improved expected robustness for document-local OOD content (i.e., content not strongly represented in LLM pretraining) because retrieval provides the needed evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; described qualitatively as improving factual calibration by providing source citations and context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported; implied additional cost due to retrieval and indexing operations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Grounding generation in retrieved documents, traceability of citations, combining retrieval with custom parsing and human-in-the-loop checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG-style retrieval combined with LLMs is recommended to reduce hallucinations and improve traceability, and is credited in part for ELISE's superior performance in the study.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2149.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2149.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Matsu (ELISE parser)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matsu (ELISE parser by Biolevate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom document parser developed for ELISE used to parse unstructured documents, tables, formulas and inlays; reported to have lower Normalized Edit Distance (better performance) than several alternatives on textual content normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Matsu parser</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>custom document parser / text normalization engine</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>unstructured biomedical document parsing (tables, figures, inlays)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not 'generate' discoveries but generates structured parsing outputs (normalized textual elements, extracted table values) that enable downstream generation and validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>performance compared to other parsers using Normalized Edit Distance (NED) on textual content; enables traceable extraction by linking parsed elements to source positions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>NED used as proxy for parser quality; lower NED indicates closer match to expected normalized outputs and better handling of unusual/unstructured content.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>NED on global textual performance: Matsu (ELISE) 0.490463 vs Megaparse 0.558241, Llama 0.622417, Unstructured 0.574559 — indicating superior parsing/normalization performance in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improved validation downstream by more accurately extracting numerical and tabular data used to validate generated claims; case studies show ELISE (with Matsu) correctly extracted HR and exclusion counts where others failed.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not provided numerically; lower NED implies fewer parsing errors that could lead to false positives in downstream extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Better parser quality (lower NED) improves robustness to novel/unstructured document formatting, reducing validation failures on atypical layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Parser reduces asymmetry by improving fidelity of extracted facts used to validate generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Matsu showed relatively better performance across heterogeneous textual inputs compared to other parsers per NED metric, implying stronger OOD robustness for document formats.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable directly; contributes to downstream system calibration by reducing extraction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Improved parsing accuracy (lower NED) combined with RAG and traceability; enables better automated validation of generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A higher-quality parser (Matsu) materially improves extraction accuracy and downstream validation, contributing to ELISE's ability to generate verifiable outputs on structured and novel document elements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating literature reviews conducted by humans versus ChatGPT: comparative study <em>(Rating: 2)</em></li>
                <li>Quality and effectiveness of AI tools for students and researchers for scientific literature review and analysis <em>(Rating: 2)</em></li>
                <li>Augmenting research: The role of artificial intelligence in recognizing topics and ideas. <em>(Rating: 1)</em></li>
                <li>Retrieval-Augmented Generation and its applications in scientific document grounding <em>(Rating: 1)</em></li>
                <li>Guidelines for clinical trials using artificial intelligence - SPIRIT-AI and CONSORT-AI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2149",
    "paper_id": "paper-278537074",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "ELISE",
            "name_full": "ELISE (Elevated LIfe SciencEs)",
            "brief_description": "A specialized AI literature-analysis system developed by Biolevate combining advanced NLP, retrieval strategies and a custom parser (Matsu) to extract, comprehend, analyze and provide traceable outputs for biomedical and regulatory documents. Evaluated in this study and consistently outperformed other tested tools across Extraction, Comprehension, Analysis, Compliance and Traceability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ELISE",
            "system_type": "hybrid NLP/LLM + retrieval + custom parser",
            "domain": "biomedical literature analysis, regulatory/clinical documentation",
            "generation_capability": "structured data extraction (metadata, tables), scientific summaries, critical analyses, traceable insights for regulatory reporting",
            "validation_method": "comparison to human-expert reference answers; automated evaluation by independent AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview); built-in traceability and compliance checks that highlight source document sections used to produce outputs",
            "novelty_measure": "not formalized numerically; assessed implicitly by performance on challenging/unfamiliar tasks (graph/table parsing, inlay detection) and stability of scores with and without human reference (small score variation used as proxy for robustness to novelty)",
            "generation_performance": "Extraction: 87.50% global average; Comprehension: 8.23/10; Analysis: 7.98/10. Performance was high and stable across articles and languages relative to peers, and remained stable whether or not human reference answers were provided (Comprehension 8.32–8.42; Analysis 7.98–8.13).",
            "validation_performance": "High — uses explicit traceability to source text and compliance checks; AI Evaluator models and human experts rated ELISE outputs as closely aligned with expert answers. No numeric false-positive/negative rates reported for validation, but demonstrably fewer incorrect extractions in case studies (e.g., only tool to correctly extract HR and exclusion counts).",
            "false_positive_rate": "not numerically reported; qualitatively low in study (few hallucinations observed compared to peers)",
            "false_negative_rate": "not numerically reported; qualitatively low (ELISE retrieved expected values where others failed)",
            "novelty_effect_on_validation": "Minimal — ELISE's validation/alignment with human references changed little when evaluated without human reference answers, indicating robust validation for more novel/challenging tasks in this corpus.",
            "generation_validation_asymmetry": "Small asymmetry: ELISE both generated and validated outputs effectively with minimal gap, attributed to parser + traceability + RAG-style retrieval; evidence suggests smaller generation-validation gap than for other systems.",
            "out_of_distribution_performance": "Better-than-others on out-of-distribution/challenging items (graph interpretation, table/inlay parsing). No formal OOD metrics provided; case examples (Kaplan–Meier EFS, HR extraction) show ELISE succeeded where others failed.",
            "calibration_quality": "Not quantified numerically; qualitatively better calibration implied by ELISE indicating margins of error and providing traceable source citations, suggesting more calibrated uncertainty communication than peers.",
            "validation_computational_cost": "Not reported in study; described methods include additional retrieval and parsing steps implying higher cost than baseline LLM generation but no measurements provided.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Custom parser (Matsu), retrieval-augmented generation (RAG) approach, built-in traceability and compliance mechanisms, and human-in-the-loop evaluation.",
            "evidence_type": "supports",
            "key_findings": "ELISE consistently outperforms general-purpose and other specialized tools on extraction, comprehension and analysis in biomedical literature, and its built-in traceability and parser improve alignment with human expert references, reducing the generation-validation gap especially on novel/challenging tasks.",
            "uuid": "e2149.0"
        },
        {
            "name_short": "ChatGPT (GPT-4o)",
            "name_full": "ChatGPT (GPT-4o)",
            "brief_description": "A versatile general-purpose large language model evaluated for literature extraction, comprehension and analysis; strong at metadata extraction and fluent summarization but weaker at precise analytical tasks and traceability in regulated contexts.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT (GPT-4o)",
            "system_type": "large language model (LLM)",
            "domain": "general scientific reasoning and literature summarization (applied to biomedical papers in this study)",
            "generation_capability": "conversational summaries, metadata extraction, hypothesis generation, literature synthesis",
            "validation_method": "outputs compared to human expert reference answers; also assessed by independent AI Evaluator models; traceability was limited (model often did not cite document sections), and an example shows ChatGPT produced a DOI not present in the document (hallucination) indicating weak provenance-based validation.",
            "novelty_measure": "not formalized; proxy comparisons include performance on tasks requiring structured parsing (graph/table) where ChatGPT performed poorly relative to ELISE, implying lower robustness on novel/structured tasks.",
            "generation_performance": "Extraction: 86.99% global average (high); Comprehension ~7.48/10 (moderate); Analysis ~7.10/10. Performance dropped on high-rigor analytical tasks (graph/table reading) and lacked consistency.",
            "validation_performance": "Moderate/variable: AI Evaluator models sometimes scored ChatGPT leniently (notably divergence among evaluators observed), but human-aligned validation exposed hallucinations (e.g., DOI not present). No numeric validation metrics provided.",
            "false_positive_rate": "not numerically reported; qualitative evidence of hallucination (providing an external DOI not in source) indicates non-negligible false positives in provenance-sensitive outputs.",
            "false_negative_rate": "not reported; qualitative evidence shows failures to extract correct numerical values from graphs/tables (false negatives for expected facts).",
            "novelty_effect_on_validation": "Validation scores for ChatGPT increased when human reference answers were not used (0.3–0.9 point increases for several models), suggesting AI Evaluator models may overestimate ChatGPT outputs in autonomous evaluation, and ChatGPT is less reliable on novel/structured tasks.",
            "generation_validation_asymmetry": "Notable asymmetry: strong at fluent generation and retrieval-like tasks but weaker at producing objectively verifiable, traceable outputs; validation (traceability/provenance) often lacking.",
            "out_of_distribution_performance": "Worse on OOD/challenging tasks such as Kaplan–Meier curve reading and table-specific HR extraction — produced incorrect numerical values in case studies.",
            "calibration_quality": "Not quantified; evidence shows poor provenance calibration (hallucinated DOI) and variable evaluator agreement, implying calibration degrades on novel, structured tasks.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human-in-the-loop verification, supplementary retrieval/traceability systems (RAG) recommended; study notes general LLMs lack built-in traceability.",
            "evidence_type": "supports",
            "key_findings": "ChatGPT is excellent at metadata extraction and fluent summaries but exhibits hallucinations and poor traceability for provenance-sensitive outputs; its autonomous evaluation scores can be inflated by AI Evaluator models, especially when human references are absent, indicating a generation-validation gap on novel/structured tasks.",
            "uuid": "e2149.1"
        },
        {
            "name_short": "Epsilon",
            "name_full": "Epsilon (v2.5)",
            "brief_description": "A specialized AI literature-analysis tool focused on identifying research gaps and suggesting directions; in this study it showed moderate comprehension ability but weak structured data extraction performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Epsilon",
            "system_type": "specialized literature-analysis NLP system",
            "domain": "scientific literature gap identification and summarization (biomedical focus in study)",
            "generation_capability": "comprehension summaries, research-gap identification, suggestions for directions",
            "validation_method": "assessed against human reference answers and by AI Evaluator models; compliance and traceability evaluations included and penalized Epsilon for guideline violations and poor traceability.",
            "novelty_measure": "no explicit novelty metric; novelty inferred from variation in scores between human-referenced and autonomous evaluation and performance on structured extraction tasks (poor).",
            "generation_performance": "Extraction: 37.41% (low); Comprehension: 7.56/10 (moderate); Analysis: 6.98/10 (moderate); showed score increases when human references not used (suggests evaluator-inflation effect).",
            "validation_performance": "Moderate but inconsistent; failed many structured extraction tasks and violated compliance guidelines leading to reduced ECACT when traceability/compliance were considered.",
            "false_positive_rate": "not given numerically; qualitative indication of guideline violations and verbose/less reliable responses increased false acceptance risk.",
            "false_negative_rate": "not reported.",
            "novelty_effect_on_validation": "Validation scores increased when human references were absent (suggests AI Evaluator models gave lenient autonomous assessments), indicating potential overestimation on novel tasks.",
            "generation_validation_asymmetry": "Present — better at higher-level comprehension/gap identification than at precise, verifiable extraction; performance and validation diverge on structured/novel tasks.",
            "out_of_distribution_performance": "Performed poorly on structured extraction and OOD-like tasks (e.g., graph reading); no numeric OOD metric.",
            "calibration_quality": "Not quantified; qualitative evidence of over-verbosity and guideline violations suggests calibration weaknesses.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human oversight, enforcing compliance/traceability protocols; integration with retrieval/parsing methods recommended.",
            "evidence_type": "supports",
            "key_findings": "Epsilon provides moderate comprehension but poor structured data extraction and fails compliance/traceability checks; autonomous AI evaluation may overestimate its outputs, widening the generation-validation gap on novel/structured tasks.",
            "uuid": "e2149.2"
        },
        {
            "name_short": "Humata",
            "name_full": "Humata",
            "brief_description": "An AI-driven text analysis tool aimed at summarizing large volumes of documents; in this study it showed variable comprehension and analysis performance and weak extraction, especially across languages.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Humata",
            "system_type": "specialized NLP summarization system",
            "domain": "document summarization and analysis (biomedical papers in study)",
            "generation_capability": "summaries, text analyses, extraction of narrative elements",
            "validation_method": "evaluated against human reference answers and AI Evaluator models; lacked robust traceability leading to lower ECACT scores and more variability between human-referenced and autonomous evaluations.",
            "novelty_measure": "inferred from variability across tasks and languages (performed better in English than French for some tasks), no explicit novelty metric.",
            "generation_performance": "Extraction: 52.00% (moderate/low); Comprehension: variable ~7.46/10 globally; Analysis: 6.86/10 with high variability across articles and languages.",
            "validation_performance": "Lower and inconsistent; greater variability indicates less reliable validation of generated outputs, especially on structured/challenging items.",
            "false_positive_rate": "not reported; qualitative variability suggests elevated risk of incorrect or inconsistent outputs.",
            "false_negative_rate": "not reported.",
            "novelty_effect_on_validation": "Performance and validation varied more across languages and without human references, indicating sensitivity to novelty and format changes (e.g., French vs English).",
            "generation_validation_asymmetry": "Present — variable generation quality and inconsistent validation alignment, particularly on extraction tasks.",
            "out_of_distribution_performance": "Poorer and more variable on out-of-distribution/challenging tasks; no numeric metrics.",
            "calibration_quality": "Not quantified; evidence of inconsistent outputs and language sensitivity implies calibration issues.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human-in-the-loop verification, improved multilingual adaptation and traceability features recommended.",
            "evidence_type": "supports",
            "key_findings": "Humata is effective for high-level summarization but shows variable and sometimes poor extraction and validation capabilities, especially across languages and on structured/challenging tasks, highlighting a generation-validation gap.",
            "uuid": "e2149.3"
        },
        {
            "name_short": "SciSpace/Typeset",
            "name_full": "SciSpace (ex-Typeset)",
            "brief_description": "A literature exploration and analysis platform with search and highlighting capabilities; in this study it performed poorly on structured data extraction but moderately on comprehension/analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "SciSpace/Typeset",
            "system_type": "specialized literature-analysis/search platform (NLP-enabled)",
            "domain": "scientific literature exploration and summarization (multi-domain, tested on biomedical)",
            "generation_capability": "search-enhanced summaries, retrieval of document content, some extraction of entities",
            "validation_method": "assessed against human reference answers and AI Evaluator models; lacked in extraction traceability leading to low Extraction scores and reduced ECACT when traceability/compliance considered.",
            "novelty_measure": "not formalized; performance drops on structured or nonstandard document elements were used as proxy for low novelty robustness.",
            "generation_performance": "Extraction: 49.10% (low); Comprehension/Analysis: ~7.10/10 (moderate) with variable performance across tasks and languages.",
            "validation_performance": "Moderate for comprehension but weak for extractive validation; failed to provide required exact exclusion counts and struggled with numerical/table reading.",
            "false_positive_rate": "not provided; qualitative evidence of failures in extraction suggests potential false negatives and some incorrect outputs.",
            "false_negative_rate": "not provided.",
            "novelty_effect_on_validation": "Performance degraded on nonstandard/structured tasks (graphs, tables) and language variants; validation degraded accordingly.",
            "generation_validation_asymmetry": "Present — reasonable comprehension but poor extractive validation and traceability.",
            "out_of_distribution_performance": "Weaker on OOD/structured tasks; examples show inability to provide exact excluded article numbers or numerical table values.",
            "calibration_quality": "Not quantified; traceability and compliance weaknesses indicate poor calibration for provenance-sensitive outputs.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Integration with stronger parsers and RAG, and human oversight to confirm extraction.",
            "evidence_type": "supports",
            "key_findings": "SciSpace/Typeset can aid comprehension but struggles with precise extraction and traceability, resulting in a significant generation-validation gap for provenance- or number-sensitive tasks.",
            "uuid": "e2149.4"
        },
        {
            "name_short": "AI Evaluator models",
            "name_full": "AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview)",
            "brief_description": "Independent high-performing LLMs used to objectively assess outputs from the evaluated AI tools by scoring them against human references and assessing autonomous interpretability; employed to reduce human bias in scoring.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude 3.5 Sonnet; GPT-4o; o1 Preview",
            "system_type": "large language models used as automated evaluators",
            "domain": "evaluation and scoring of AI-generated scientific outputs",
            "generation_capability": "generate evaluation scores, justifications, and comparisons between AI tool outputs and human reference answers",
            "validation_method": "apply a standardized 10-point rubric to AI tool responses, compare anonymized vs identified outputs to check bias, and average scores across models to mitigate evaluator-specific bias; also used to autonomously assess responses without human references.",
            "novelty_measure": "not explicit; novelty effect probed by comparing evaluator scores with and without human references and checking sensitivity to anonymization and tool identity.",
            "generation_performance": "Not applicable (they evaluate rather than produce scientific discoveries); measured to be consistent across anonymized/identified conditions with minimal bias, except some evaluator divergence when judging ChatGPT.",
            "validation_performance": "Provided structured scores and justifications; however, evidence shows evaluators can be more lenient when human reference answers are absent (leading to score inflation for some tools).",
            "false_positive_rate": "Not applicable in standard sense; but qualitative evidence indicates AI evaluators sometimes over-score generated outputs (false acceptance of incorrect answers) particularly when human references were not provided.",
            "false_negative_rate": "Not reported.",
            "novelty_effect_on_validation": "Evaluator assessments tended to be more lenient on autonomous (no-human-reference) evaluations, inflating scores for several tools; divergence among evaluator models occurred for some tool responses (ChatGPT).",
            "generation_validation_asymmetry": "Evaluators can mask generation-validation gaps by overestimating outputs when human references are absent, producing an asymmetry between apparent validation and true accuracy.",
            "out_of_distribution_performance": "Evaluator agreement remained mostly stable across anonymized vs identified conditions; specific divergence noted for ChatGPT evaluations.",
            "calibration_quality": "Not quantitatively reported; qualitative issues include leniency in autonomous settings and occasional inter-evaluator disagreement.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Averaging multiple evaluator models; anonymized testing to check bias; pairing evaluator assessments with human reference answers to reduce overestimation.",
            "evidence_type": "mixed",
            "key_findings": "AI Evaluator models provide scalable, relatively unbiased scoring when used in aggregate, but can overestimate AI-generated answers in autonomous settings and sometimes disagree, indicating they should complement — not replace — human validation for novel or high-stakes outputs.",
            "uuid": "e2149.5"
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "An approach combining retrieval mechanisms with LLM generation to ground outputs in retrieved, document-level evidence, recommended in the paper to reduce hallucinations and improve factual accuracy and traceability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_type": "hybrid retrieval + generative model architecture",
            "domain": "document-grounded generation for literature analysis and regulatory content",
            "generation_capability": "context-aware, evidence-grounded natural language generation (summaries, answers) relying on retrieved documents/paragraphs",
            "validation_method": "improves validation by grounding generations in retrieved sources and enabling traceability to source passages; validation still requires human checks for regulatory use but reduces unsupported hallucinations.",
            "novelty_measure": "not formalized here; novelty robustness argued qualitatively because retrieval reduces reliance on stale/pretrained knowledge and anchors responses to present documents.",
            "generation_performance": "Not empirically measured in this paper but posited as an improvement over vanilla LLM generation for accuracy and traceability; ELISE uses retrieval strategies consistent with RAG principles to achieve better results.",
            "validation_performance": "Expected improvement in factuality and provenance; paper reports ELISE (which employs retrieval strategies) had fewer provenance errors and better alignment with human references.",
            "false_positive_rate": "Not reported numerically; RAG described as reducing false positives/hallucinations relative to non-retrieval baselines.",
            "false_negative_rate": "Not reported.",
            "novelty_effect_on_validation": "RAG reduces novelty-related validation failures by grounding outputs in document evidence, but human oversight remains necessary for novel scientific inference.",
            "generation_validation_asymmetry": "RAG narrows the asymmetry by providing source grounding, improving validation of generated outputs compared to ungrounded LLMs.",
            "out_of_distribution_performance": "Improved expected robustness for document-local OOD content (i.e., content not strongly represented in LLM pretraining) because retrieval provides the needed evidence.",
            "calibration_quality": "Not quantified; described qualitatively as improving factual calibration by providing source citations and context.",
            "validation_computational_cost": "Not reported; implied additional cost due to retrieval and indexing operations.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Grounding generation in retrieved documents, traceability of citations, combining retrieval with custom parsing and human-in-the-loop checks.",
            "evidence_type": "supports",
            "key_findings": "RAG-style retrieval combined with LLMs is recommended to reduce hallucinations and improve traceability, and is credited in part for ELISE's superior performance in the study.",
            "uuid": "e2149.6"
        },
        {
            "name_short": "Matsu (ELISE parser)",
            "name_full": "Matsu (ELISE parser by Biolevate)",
            "brief_description": "A custom document parser developed for ELISE used to parse unstructured documents, tables, formulas and inlays; reported to have lower Normalized Edit Distance (better performance) than several alternatives on textual content normalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Matsu parser",
            "system_type": "custom document parser / text normalization engine",
            "domain": "unstructured biomedical document parsing (tables, figures, inlays)",
            "generation_capability": "does not 'generate' discoveries but generates structured parsing outputs (normalized textual elements, extracted table values) that enable downstream generation and validation",
            "validation_method": "performance compared to other parsers using Normalized Edit Distance (NED) on textual content; enables traceable extraction by linking parsed elements to source positions",
            "novelty_measure": "NED used as proxy for parser quality; lower NED indicates closer match to expected normalized outputs and better handling of unusual/unstructured content.",
            "generation_performance": "NED on global textual performance: Matsu (ELISE) 0.490463 vs Megaparse 0.558241, Llama 0.622417, Unstructured 0.574559 — indicating superior parsing/normalization performance in this evaluation.",
            "validation_performance": "Improved validation downstream by more accurately extracting numerical and tabular data used to validate generated claims; case studies show ELISE (with Matsu) correctly extracted HR and exclusion counts where others failed.",
            "false_positive_rate": "not provided numerically; lower NED implies fewer parsing errors that could lead to false positives in downstream extractions.",
            "false_negative_rate": "not reported numerically.",
            "novelty_effect_on_validation": "Better parser quality (lower NED) improves robustness to novel/unstructured document formatting, reducing validation failures on atypical layouts.",
            "generation_validation_asymmetry": "Parser reduces asymmetry by improving fidelity of extracted facts used to validate generated outputs.",
            "out_of_distribution_performance": "Matsu showed relatively better performance across heterogeneous textual inputs compared to other parsers per NED metric, implying stronger OOD robustness for document formats.",
            "calibration_quality": "Not applicable directly; contributes to downstream system calibration by reducing extraction errors.",
            "validation_computational_cost": "Not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Improved parsing accuracy (lower NED) combined with RAG and traceability; enables better automated validation of generated outputs.",
            "evidence_type": "supports",
            "key_findings": "A higher-quality parser (Matsu) materially improves extraction accuracy and downstream validation, contributing to ELISE's ability to generate verifiable outputs on structured and novel document elements.",
            "uuid": "e2149.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating literature reviews conducted by humans versus ChatGPT: comparative study",
            "rating": 2
        },
        {
            "paper_title": "Quality and effectiveness of AI tools for students and researchers for scientific literature review and analysis",
            "rating": 2
        },
        {
            "paper_title": "Augmenting research: The role of artificial intelligence in recognizing topics and ideas.",
            "rating": 1
        },
        {
            "paper_title": "Retrieval-Augmented Generation and its applications in scientific document grounding",
            "rating": 1
        },
        {
            "paper_title": "Guidelines for clinical trials using artificial intelligence - SPIRIT-AI and CONSORT-AI",
            "rating": 1
        }
    ],
    "cost": 0.018362999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From data extraction to analysis: a comparative study of ELISE capabilities in scientific literature
12 May 2025</p>
<p>Maxime Gobin maxime@biolevate.com 
Muriel Gosnat 
Seindé Toure 
Lina Faik 
Joel Belafa 
Antoine Villedieu De Torcy 
Florence Armstrong 
Pengfei Zhang 
Jie Wang </p>
<p>Biolevate, ParisFrance</p>
<p>Chengdu University of Traditional Chinese Medicine
China</p>
<p>Southwest Jiaotong University
China</p>
<p>Cq Tan
Chengdu University of Traditional Chinese Medicine
China</p>
<p>From data extraction to analysis: a comparative study of ELISE capabilities in scientific literature
12 May 20255843DD62E7966D9CDD055A1CC854E5E510.3389/frai.2025.1587244RECEIVED 04 March 2025 ACCEPTED 18 April 2025scientific literaturesystematic reviewdata extractionAI toolElevated LIfe SciencE solution
The exponential growth of scientific literature presents challenges for pharmaceutical, biotechnological, and Medtech industries, particularly in regulatory documentation, clinical research, and systematic reviews.Ensuring accurate data extraction, literature synthesis, and compliance with industry standards require AI tools that not only streamline workflows but also uphold scientific rigor.This study evaluates the performance of AI tools designed for bibliographic review, data extraction, and scientific synthesis, assessing their impact on decision-making, regulatory compliance, and research productivity.The AI tools assessed include generalpurpose models like ChatGPT and specialized solutions such as ELISE (Elevated LIfe SciencEs), SciSpace/Typeset, Humata, and Epsilon.The evaluation is based on three main criteria: Extraction, Comprehension, and Analysis with Compliance and Traceability (ECACT) as additional dimensions.Human experts established reference benchmarks, while AI Evaluator models ensure objective performance measurement.The study introduces the ECACT score, a structured metric assessing AI reliability in scientific literature analysis, regulatory reporting and clinical documentation.Results demonstrate that ELISE consistently outperforms other AI tools, excelling in precise data extraction, deep contextual comprehension, and advanced content analysis.ELISE's ability to generate traceable, well-reasoned insights makes it particularly well-suited for high-stakes applications such as regulatory affairs, clinical trials, and medical documentation, where accuracy, transparency, and compliance are paramount.Unlike other AI tools, ELISE provides expert-level reasoning and explainability, ensuring AI-generated insights align with industry best practices.ChatGPT is efficient in data retrieval but lacks precision in complex analysis, limiting its use in high-stakes decision-making.Epsilon, Humata, and SciSpace/Typeset exhibit moderate performance, with variability affecting their reliability in critical applications.In conclusion, while AI tools such as ELISE enhance literature review, regulatory writing, and clinical data interpretation, human oversight remains essential to validate AI outputs and ensure compliance with scientific and regulatory standards.For pharmaceutical, biotechnological, and Medtech industries, AI integration must strike a balance between automation and expert supervision to maintain data integrity, transparency, and regulatory adherence.</p>
<p>Introduction</p>
<p>In recent years, the integration of artificial intelligence (AI) into scientific research has significantly transformed academic publishing, drug development, and biomedical innovation.AI-driven literature analysis tools, or AI tools, have become essential, offering groundbreaking capabilities in processing, analyzing, and summarizing large volumes of scientific papers (Agrawal Gobin et al. 10.3389/frai.2025.1587244Frontiers in Artificial Intelligence 02 frontiersin.orget al., 2024;Danler et al., 2024;Khalifa and Ibrahim, 2024).These tools leverage advanced natural language processing (NLP) techniques to streamline traditionally manual tasks, such as literature reviews and data extraction (Fabiano et al., 2024).The development of sophisticated AI algorithms, fueled by increased computational power and growing data availability, has revolutionized how researchers and industry professionals interact with scientific literature (Zahra et al., 2024).AI-driven literature analysis tools can be broadly categorized into general-purpose models and specialized scientific models, each serving distinct roles in research.General-purpose AI models, such as OpenAI's ChatGPT, are designed for versatility, able to handle a wide array of tasks across various domains.These models excel in summarizing and interpreting general information, offering researchers a flexible tool for diverse inquiries (Ning et al., 2024).In contrast, specialized tools like ELISE (the tool we developed at Biolevate), SciSpace (ex-Typeset), Humata, and Epsilon are tailored to meet the specific needs of scientific and biomedical research, including pharmaceutical, biotech and Medtech applications (Zahra et al., 2024;Tsirmpas et al., 2024).These applications focus on precision and relevance, employing advanced NLP techniques to extract and analyze data with a high level of specificity for regulated and highly technical fields.While general-purpose tools provide broad applicability, specialized tools ensure accuracy and domain relevance, crucial for rigorous academic inquiry.</p>
<p>More specifically, ChatGPT excels in conversational interactions, aiding in literature reviews and hypothesis generation (Khalifa and Ibrahim, 2024).SciSpace/Typeset enhances literature exploration with intuitive interfaces and robust search capabilities (SciSpace, 2024).Humata focuses on AI-driven text analysis, efficiently summarizing large volumes of data (Humata, 2024).Epsilon identifies research gaps, suggesting innovative directions (Epsilon, 2024).Meanwhile, ELISE advances document analysis with sophisticated NLP and data extraction techniques, addressing the growing need for efficient data management in research (ELISE, 2024).</p>
<p>Together, these AI tools enhance research quality and efficiency, facilitating a more comprehensive understanding of complex scientific literature and accelerating the pace of discovery by reducing the time required for knowledge synthesis (Borah et al., 2017;Chakraborty et al., 2024;Mehta et al., 2024).They support key industry processes, from early-stage drug discovery to regulatory documentation preparation, helping companies navigate vast and evolving scientific landscape (Agrawal et al., 2024).For pharmaceutical, biotech, and Medtech companies, AI tools can improve literature surveillance, support regulatory submissions (e.g., EMA/FDA filings), and optimize knowledge management for evidence-based decision-making.</p>
<p>Despite these advantages, integrating AI into scientific research is not without challenges, particularly in ensuring the reliability and consistency of AI-generated insights.A significant issue is the variability in AI-generated responses, which stems from the diverse methodologies and algorithms employed by different tools (Bolaños et al., 2024).This inconsistency complicates the standardization of evaluations, as outputs can vary in quality and relevance.Additionally, the lack of traceability in AI-generated insights raises concerns about the validity of interpretations, particularly in fields requiring high precision, such as biomedical and clinical research (Agrawal et al., 2024;Danler et al., 2024).Furthermore, the absence of a standardized framework for evaluating AI tools poses a threat to research reliability and validity (Ning et al., 2024;Sharma and Ruikar, 2024).</p>
<p>Therefore, there is a pressing need for rigorous and standardized evaluations to ensure that AI tools contribute effectively and reliably to scientific research, safeguarding the rigor and integrity of research outcomes.</p>
<p>This study aims to provide the scientific community and industry stakeholders with a clearer understanding of the potential and limitations of AI-driven literature analysis technologies.It also introduces a structured and independent methodology based on three main criteria (Extraction, Comprehension, and Analysis), to evaluate these applications, including ELISE, a novel AI tool integrating NLP and retrieval strategies to enhance workflows, improve regulatory compliance and optimize R&amp;D processes for pharma, biotech and Medtech organizations.</p>
<p>Methods</p>
<p>Selected articles</p>
<p>To ensure a comprehensive and exhaustive evaluation, a diverse selection of scientific articles was chosen, covering different disciplines and study types.The selected articles are detailed in Table 1.</p>
<p>The current selection emphasized experimental and applied biomedical literature to stress-test AI tools in high-rigor contexts.Future work will include broader disciplinary representation to assess generalizability.This initial dataset includes articles in both English and French, allowing us to test cross-lingual consistency of the ECACT scoring framework.However, broader validation across other languages and domains remains a priority for future work.</p>
<p>The sample was restricted to nine articles to allow for a controlled proof-of-concept analysis.This scale enabled full evaluation across five criteria and three evaluators per article.However, broader validation will require expansion to larger datasets.</p>
<p>AI models used for evaluation</p>
<p>AI tools</p>
<p>Among the most popular, several AI-driven literature analysis tools were selected based on their ability to extract, comprehend, and analyze scientific content.Table 2A summarizes their main characteristics.</p>
<p>AI evaluator models</p>
<p>To assess the performance of the AI tools, independent AI Evaluator models were used, as described in Table 2B.To ensure a neutral and reproducible evaluation process, we selected three independent, high-performing AI models to act as evaluators: GPT-4o (OpenAI), Claude 3.5 Sonnet (Anthropic), and o1 Preview (OpenAI).</p>
<p>These models were intentionally chosen from distinct technical ecosystems and were not used at any stage in the development, training, or fine-tuning of the ELISE engine.This separation was critical to avoid any potential overlap or bias arising from shared components.</p>
<p>The selected models are known for their advanced reasoning and summarization capabilities, making them suitable for structured judgment across multiple scientific domains.Their inclusion was further motivated by public availability, multilingual support, and Each model performed evaluations independently, using randomized prompts under both anonymized and non-anonymized conditions, and their scores were subsequently averaged to control for evaluator-specific bias.</p>
<p>Bias identification and model selection</p>
<p>To identify potential biases in AI-generated evaluations, we conducted a control assessment using Article 5 (Jones et al., 2021) on the Comprehension and Analysis criteria.</p>
<p>-AI tools were evaluated under two conditions: with their identities revealed and anonymized.-A comparison of the scoring results under both conditions allowed us to assess any bias in evaluator decisions.</p>
<p>To ensure transparency, AI tool responses were evaluated under both identified and anonymized conditions.For anonymized evaluations, tool names were replaced with neutral codes (Tool A, Tool B, etc.), and the order of responses was randomized.The evaluators were not informed of the anonymization.This protocol was repeated for all three evaluator models, and scoring results were then compared across both conditions to assess potential bias.Supplementary Table 1 presents the comparative results.</p>
<p>Evaluation criteria</p>
<p>To objectively assess the performance of AI tools, three primary criteria were selected: Extraction, Comprehension, and Analysis.Each criterion was evaluated through a structured questionnaire described in Table 3.</p>
<p>To limit subjectivity in open-ended tasks such as summarization, each question was paired with an expected response profile.Evaluators followed a standardized 10-point rubric (see Supplementary Table 3), and were prompted to assess semantic fidelity, relevance, and completeness rather than superficial similarity.For comparison-based judgments, evaluators used structured prompts to evaluate content quality and alignment with the source article.</p>
<p>Evaluation protocol and scoring</p>
<p>Before evaluation, the questions listed in Table 3 were submitted to each AI tool for every article and every evaluation criterion.The queries were conducted between October 2024 and January 2025, and all responses were collected.The full evaluation process was carried out in January 2025.</p>
<p>Scoring methodology 2.4.1.1 Extraction criterion</p>
<p>-Each response was manually checked by a human reviewer against expected data.-Each answer was scored out of 1, with partial credit assigned proportionally based on the number of correctly retrieved elements.</p>
<p>For instance, if an article was authored by three individuals and the AI tool retrieved only one name, the response received a score of 0.33/1.Similarly, if a study had four different sponsors but only two were correctly identified, the response was scored 0.5/1.-For each article and each AI tool, a percentage score was calculated by summing the points obtained across all questions (see Figure 1A).-First, three independent human experts answered the questions, without knowing the answers of the AIs, to establish a reference baseline.-Second, two different evaluations were performed using automated evaluation prompts provided in Supplementary Figures 1A,B: i Each AI Evaluator model (presented in Table 2B) assessed AI-generated responses against human-defined reference answers.ii AI Evaluator models independently assessed the responses based solely on document content, without human answers, to evaluate autonomous interpretability of AI-generated content.</p>
<p>-Each response was scored out of 10 by the AI Evaluator models and justified by them.-For each article and each AI tool, a percentage score was calculated by summing the points obtained across all questions (see Figure 1B).</p>
<p>Final performance assessment</p>
<p>-To assess global performance, the average score obtained across all evaluated articles was calculated for each AI tool.-Additionally, the standard deviation of scores was determined for each AI tool to measure variability in performance across different articles.</p>
<p>Compliance and traceability scoring</p>
<p>-Compliance score (0-10): evaluates if AI responses strictly adhered to the document content and followed the required guidelines.-Traceability score (0-10): assesses whether AI tools correctly highlighted the relevant sections used in their answers.</p>
<p>Final ECACT score calculation</p>
<p>A weighed global score (ECACT score) proposition was created to reflect the importance of each evaluation criterion:
( ) ( ) ( ) ( ) ( ) = * + * + * + * + * 1.0 1.5 2 1.5 2.0 comp ECACT E C A C T
Where E = Extraction, C = Comprehension, A = Analysis, C comp = Compliance, and T = Traceability, leading to a final score out of 80 points.The weighting system was designed to reflect the relative importance of each criterion in regulated scientific contexts.Analysis and Traceability were prioritized (×2.0) due to their implications for interpretability and compliance, particularly in biomedical and regulatory workflows.A sensitivity analysis exploring alternative weighting schemes is provided in Supplementary Table 2.</p>
<p>Statistical analysis</p>
<p>To ensure statistical robustness and objectivity in comparing the performance of AI tools, a comprehensive statistical framework was applied.All statistical analyses were conducted by a trained statistician (LF, co-author).</p>
<p>First, we performed a one-way ANOVA for each evaluation criterion (Extraction, Comprehension, Analysis) to determine whether differences in performance scores between AI tools were statistically significant.The assumption of homogeneity of variances was assessed using Levene's test, which evaluates whether the variances across groups are equal.A non-significant result (p &gt; 0.05) confirmed that the assumption was met, allowing the use of standard ANOVA procedures.</p>
<p>Second, a two-way ANOVA was carried out to investigate both the main effects and their interaction effects between AI tool identity and evaluation criteria, thereby assessing whether certain models performed differently depending on the evaluation criterion.</p>
<p>Third, we applied post-hoc Tukey Honestly Significant Difference (HSD) tests to identify which pairs of AI tools showed statistically significant differences.This test was chosen for its robustness to multiple comparisons and its suitability for evaluating grouped means.</p>
<p>Additionally, to support transparency and explore the robustness of the ECACT scoring framework, a sensitivity analysis was conducted.Alternative weighting schemes (e.g., equal weights, compliance-prioritized, comprehension-focused) were applied to assess how different weight configurations impacted the final AI tool rankings.Results showed that while some mid-ranking positions Extraction (A) and comprehension/analysis (B) evaluation protocol.shifted, the top-performing (ELISE) and lowest-performing tools remained consistent across weighting conditions (see Supplementary Table 2 and Supplementary Figure 4).All analyses were performed using Python version 3.11.9 and validated independently (statsmodels v.0.14.4 and scipy v.1.14.1 libraries).A p-value threshold of 0.05 was used for significance across all tests.</p>
<p>The full evaluation dataset, question prompts, and scoring rubrics are available in the accompanying GitHub repository. 1</p>
<p>Results</p>
<p>To provide the scientific community with a clearer understanding of the capabilities and limitations of AI-driven literature analysis applications, we developed a structured evaluation methodology based on three key criteria (Extraction, Comprehension and Analysis).Each of these criteria plays a critical role in assessing the overall effectiveness of AI tools in scientific research.</p>
<p>The evaluation process was conducted by analyzing the responses provided by each AI tool to a predefined set of questions (Table 3) across multiple scientific articles (Table 1).The articles were selected to represent a diverse range of disciplines and study types, ensuring a broad assessment of AI tool performance in different editorial contexts.Additionally, two equivalent articles (one in French and the other in English) were included in the dataset to investigate potential variations in AI performance due to language differences.</p>
<p>Extraction performance</p>
<p>The first evaluation criterion, Extraction, assessed the ability of AI tools to accurately identify and retrieve key bibliographic elements such as author names, publication dates, study types, and other fundamental metadata.These elements are essential for organizing, referencing, and citing scientific work.The questions related to this criterion and their expected responses are detailed in Table 3, with answers consisting exclusively of factual data.For each article and each question, the expected number of correct data points was predefined and AI-generated responses were manually evaluated for completeness and accuracy.Each response was assigned a score of 1 point, with proportional credit awarded when only a subset of the expected data was correctly retrieved (data not shown).Then a percentage score was calculated by summing the points obtained across all questions for each article and a global score is obtained for each AI tool.</p>
<p>The results of the Extraction evaluation are presented in Figure 2, illustrating the performance of each AI tool across different articles as well as the global average performance.ChatGPT (Figure 2A) and ELISE (Figure 2B) demonstrate the highest extraction efficiency, consistently achieving scores above 80%, with a minimal variation across articles.In contrast, Epsilon (Figure 2C), Humata (Figure 2D), and SciSpace/Typeset (Figure 2E) exhibited more variable performance, generally ranging between 60 and 70%, with significant fluctuations 1 https://github.com/Biolevate/SL-EVAL-ECACTdepending on the articles.The global performance average (Figure 2F) demonstrates that ELISE (87.50%) and ChatGPT (86.99%) were the most effective tools in extracting standard metadata, significantly outperforming Humata (52.00%),SciSpace/Typeset (49.10%) and Epsilon (37.41%).Statistical analysis revealed significant differences, with ELISE demonstrating a superior performance compared to Humata (p£0.01),SciSpace/Typeset (p£0.001), and Epsilon (p£0.001).</p>
<p>Selection of evaluation models</p>
<p>To further assess the performance of AI tools beyond simple data extraction, we focused on two complex evaluation criteria: Comprehension and Analysis.These criteria require structured, explanatory and context-aware answers, which introduce significant variability in both quality and quantity.Such complexity makes human evaluation challenging, as scoring responses may be influenced by subjectivity and cognitive biases.To mitigate these risks, we established a set of calibrated guidelines (see Table 3) and opted for an AI-based evaluation approach.</p>
<p>A multi-model, independent evaluation was conducted to systematically assess AI tools performance on the Comprehension and Analysis criteria.To ensure robustness, a single reference article (Article 6 - Jones et al., 2021) was used in this evaluation.The primary objective was to determine whether AI Evaluator models introduced biases when grading responses, particularly by comparing identified and anonymized AI tool responses.</p>
<p>The evaluation was conducted using three AI Evaluator models: Claude 3.5 Sonnet, GPT-4o, and o1 Preview (Table 2B).The responses from AI tools were processed separately by each evaluator, and their scores were then averaged (Figure 3).</p>
<p>Results indicate minimal variation between evaluations of identified and anonymized answers, demonstrating that the AI Evaluator models were not influenced by the identity of the AI tools being assessed.This consistency supports the reliability and objectivity of the evaluation framework.</p>
<p>The comparison between identified and anonymized scoring (see Supplementary Figure 1) revealed minimal variation for most tools, confirming that AI Evaluator models were not significantly influenced by tool identities.This supports the robustness and neutrality of the evaluation framework.</p>
<p>However, one notable discrepancy was observed in the evaluation of ChatGPT's responses, where the scores provided by Claude 3.5 Sonnet diverged from those of GPT-4o and o1 Preview.To maintain fairness and accuracy in scoring, as well as to ensure a balanced evaluation, the final assessment of AI tools was determined by incorporating the average results from all three AI Evaluator models.This approach minimizes potential biases and ensures that the final evaluation reflects a comprehensive and standardized assessment of AI-driven comprehension and analysis capabilities.</p>
<p>Comprehension</p>
<p>The second evaluation criterion, Comprehension, aimed to assess the AI tools' ability to interpret and structure key arguments, conclusions and methodological aspects of scientific articles.This criterion is critical for determining how well AI models can process complex scientific content and provide accurate and coherent summaries.</p>
<p>To ensure an objective evaluation, a reference answer set was established by a panel of three human experts, defining expected responses for each question in every article.Then, the AI-generated responses were assessed by AI Evaluator models, which compared them to the human-defined references.Each response was scored on a 10-point scale, with justifications provided by the AI Evaluator models.The scores obtained for each AI tool, AI Evaluator model, and article were averaged (data not shown).The overall final score for each AI tool and article were averaged across the different evaluators and the results are presented in Figure 4.</p>
<p>ELISE (Figure 4B) demonstrated the highest performance in Comprehension, with scores consistently exceeding 8.0, highlighting its ability to process and synthesize scientific information effectively.In contrast, Epsilon (Figure 4C), ChatGPT (Figure 4A) and SciSpace/ Typeset (Figure 4E) exhibited moderate performance, with scores ranging between 7.0 and 8.0.Humata (Figure 4D) displayed greater variability with scores fluctuating between 5.0 and 8.0, indicating inconsistencies in its comprehension capabilities.The global evaluation (Figure 4F) demonstrated that ELISE and Epsilon outperformed ChatGPT, Humata, and SciSpace/Typeset with final scores of: 8.23, 7.56, 7.48, 7.46 and 6.66, respectively.Statistical analysis demonstrated significant differences, reinforcing ELISE's superior comprehension capabilities compared to all other models.</p>
<p>Analysis</p>
<p>The final evaluation criterion, Analysis, focused on assessing each AI tool's ability to engage in critical reasoning, summarize key findings, identify study limitations, and generate meaningful insights.ELISE (Figure 5B) achieved the highest Analysis performance, with scores ranging between 7.0 and 9.0, demonstrating strong critical reasoning capabilities.In contrast, ChatGPT (Figure 5A), SciSpace/ Typeset (Figure 5E), Epsilon (Figure 5C) and Humata (Figure 5D), exhibited lower efficiency, with scores fluctuating between 4.5 and 8.0.Greater performance variability was observed in Humata, SciSpace/ Typeset and ChatGPT show greater variability in performance compared to Epsilon and ELISE, suggesting inconsistencies in their ability to generate structured and insightful interpretations.</p>
<p>The global performance evaluation (Figure 5F) demonstrated ELISE's superior analytical capabilities, with a final score of 7.98, significantly surpassing ChatGPT (7.10), SciSpace/Typeset (7.10), Epsilon (6.98) and Humata (6.86).Statistical analyses demonstrated significant differences, reinforcing ELISE's effectiveness in analyzing scientific content compared to other models.</p>
<p>Language change and AI tools efficiency</p>
<p>To evaluate the impact of language on AI tools performances, we assessed their ability to process identical articles written in French and English (Article 6 and Article 7 - Lowry et al., 2023a).The evaluation was conducted using the Extraction, Comprehension, and Analysis criteria, and the results are presented in Figure 6.</p>
<p>Performance scores for the Extraction criterion revealed a greater variability across languages.ChatGPT achieved 8.06 in French and 7.78 in English, ELISE 6.67 and 8.33, Epsilon 4.56 and 4.00, Humata 0.00 and 3.61, and SciSpace/Typeset 1.11 and 1.94, respectively.These results suggest that ChatGPT and Epsilon maintained stable performance across both languages, while Humata and ELISE exhibited better results in English.</p>
<p>For Comprehension and Analysis criteria, performance variability across languages was minimal for all AI tools, with no strong preference for one language over the other, except for Humata, which consistently performed better in English.The comprehension scores were 7.25 and 7.21 for ChatGPT, 8.38 and 8.63 for ELISE, 8.17 and 7.94 for Epsilon, 6.67 and 7.88 for Humata, 7.63 and 7.42 for SciSpace/Typeset.The analysis criterion followed a similar trend with 7.33 and 6.94 for ChatGPT, 8.28 and 7.92 for ELISE, 7.50 and 6.94 for Epsilon, 7.11 and 8.06 for Humata, 7.67 and 7.83 for SciSpace/Typeset.</p>
<p>These findings highlight that language influences Extraction performance more than Comprehension and Analysis.While some AI tools perform equally well across languages, others exhibit discrepancies, particularly in data retrieval tasks, emphasizing the need for further linguistic adaptation in AI-driven scientific analysis.</p>
<p>Human expertise and AI tools</p>
<p>To further examine AI tools' capabilities, we assessed their performance in Comprehension and Analysis criteria without providing human-validated reference answers.The goal was to evaluate how AI tools perform autonomously when interpreting scientific texts, and the results are presented in Figure 7.</p>
<p>When comparing AI tools' global evaluation scores with and without human expertise as a reference, ELISE consistently demonstrated the highest alignment with expert-level responses.For Comprehension, ELISE's score remained stable (8.32-8.42),whereas other tools demonstrated greater variations: Epsilon (7.56-8.00),), SciSpace/Typeset (7.43-7.76)and Humata (6.66 and 6.82).In the Analysis criterion, ELISE also maintained minimal variation (7.98-8.13),outperforming ChatGPT (7.10-7.81),SciSpace/Typeset (7.10-7.52),Epsilon (6.98-7.83)and Humata (6.86-6.87).</p>
<p>Notably, ChatGPT, Epsilon, and SciSpace/Typeset exhibited the largest score increases when human expertise was not used as a reference, with variations ranging from 0.3 to 0.9 points.These results suggest that AI evaluators models might overestimate AI-generate answers with a more lenient AI tools-assessments considering also some of the evaluators (GPT-4o and o1-Preview) self-assess by scoring ChatGPT answers.In contrast, ELISE consistently produced reliable responses, with minimal variation between the two evaluation settings, reinforcing its alignment with expert-level reasoning.Humata followed a similar trend, but with significantly lower scores, indicating less overall accuracy and robustness compared to ELISE.</p>
<p>To further validate these observations, specific cases where AI tools exhibited string discrepancies were analyzed.The results are visually represented in Figures 8-10.</p>
<p>In the first case (Figure 8), AI tools were required to extract Event-Free Survival (EFS) percentages at 60 months from a Kaplan-Meier curve.Epsilon, Humata and SciSpace/Typeset failed to generate relevant responses, while ChatGPT provided incorrect values (21 and 30% instead of 37 and 44%).In contrast, ELISE generated the closest approximation (40 and 50%) and explicitly indicated a margin of error, demonstrating a more expert-like approach to data interpretation.</p>
<p>In another example (Figure 9), AI tools had to identify the numbers of excluded articles based on specific selection criteria.</p>
<p>While SciSpace/Typeset was unable to provide an exact number, ChatGPT, Epsilon and Humata misinterpret the exclusion criteria, leading to incorrect responses.Only ELISE successfully differentiated between exclusion categories and provided the correct answer (12 articles excluded due to missing full text, demonstrating its superior ability to recognize complex selection criteria and accurately extract relevant numerical data.</p>
<p>The third case (Figure 10) required AI tools to identify a Hazard Ratio (HR) for a specific population within a data table, necessitating both vertical and horizontal reading to locate the expected value.ELISE was the only AI tool capable of retrieving the correct HR (0.31, 95% CI: 0.13-0.74).Moreover, it provided a response even more precise than the expected answer, showcasing its advanced document parsing capabilities and its ability to accurately interpret structured data, a task where all other AI tools failed.</p>
<p>These findings (Figures 7-10 and Supplementary Tables 1-3) reinforce that ELISE is the AI tool that aligns most closely with human expertise across all tested evaluation criteria.Unlike other models, which exhibited greater variability and inconsistencies, ELISE consistently provided responses that matched expert expectations, particularly in challenging tasks involving graph interpretation, inlay detection, and the comprehension of complex scientific data.</p>
<p>Overall results</p>
<p>To provide a comprehensive overview of the AI tools' evaluation, a detailed comparison of scores across criteria (Figure 11A) and an overall averaged comparison (Figure 11B) were conducted.</p>
<p>The results confirm that ELISE consistently achieved high scores across all criteria, demonstrating minimal variation between Extraction, Comprehension and Analysis.</p>
<p>Among the evaluated tools, ChatGPT performed best in Extraction but exhibited lower efficiency in Comprehension and Analysis, indicating its strength in retrieving structured metadata but its relative weakness in processing and interpreting scientific content.In contrast, SciSpace/Typeset, Humata and Epsilon showed poor performance in Extraction but performed moderately better in Comprehension and Analysis, although their results remained less relevant and less consistent than ELISE's.</p>
<p>The global average comparison further reinforces these observations.ELISE emerges as the most effective AI tool, followed by ChatGPT, then SciSpace/Typeset, Humata and Epsilon, which obtained similar but lower overall scores.These findings underscore the importance of an AI tool's ability to handle the entire research workflow, from accurate data extraction to in-depth comprehension  and critical analysis, ensuring its reliability for scientific literature processing across various fields and study types.</p>
<p>Statistical analysis</p>
<p>To validate the observed performance differences among AI tools, a comprehensive statistical analysis was conducted.Results for each criterion were included in the global average evaluation (Figures 2F,  4F, 5F).</p>
<p>A unidirectional ANOVA test (data not shown) confirmed that AI tool performance varied significantly depending on the evaluation criterion, with some tools excelling in certain task underperforming in others.Additionally, two-way ANOVA tests (data not shown) demonstrated that AI tools differed significantly from each other across all criteria, confirming that no single evaluation metric can fully determine an AI tool's effectiveness in scientific literature analysis.</p>
<p>Interestingly, results indicated no significant interaction effect between AI tools and evaluation criteria, meaning that performance rankings remained consistent regardless of the assessment category.This supports the robustness of the conclusions drawn in this study and validates the methodological soundness of the evaluation framework.</p>
<p>ECACT score</p>
<p>To ensure a rigorous and holistic evaluation framework, an ECACT score was developed, incorporating the Extraction, Comprehension and Analysis criteria alongside two additional dimensions: Compliance and Traceability.These complementary criteria are essential to assessing an AI tool's reliability and adherence to scientific best practices.</p>
<p>The Compliance criterion evaluates whether an AI tool follows predefined guidelines and exclusively relies on documented content to generate responses.The traceability criterion assesses the tool's ability to highlight the relevant data sources that were used to generate its answers.These criteria provide a more nuanced understanding of each AI model's transparency and scientific rigor.</p>
<p>To reflect the importance of each criterion, a weighting system was applied.The Analysis criterion received the highest weight, followed by Comprehension, then Extraction.Similarly, Traceability was weighted equivalently to Analysis, while Compliance was weighted at the same level as Comprehension, ensuring a balanced assessment of AI tools' capabilities.</p>
<p>The results, presented in Figure 12, show that ELISE (Figure 12B) demonstrated the highest performance across all evaluated criteria,    extraction, structuring complex information, and streamlining regulatory and scientific documentation.These tools leverage Named Entity Recognition (NER) to efficiently identify key data points, reducing manual workload and accelerating critical decision-making processes.However, despite these advantages, significant challenges remain, particularly in ensuring the reliability, accuracy, and contextual relevance of AI-generated outputs.Many traditional AI struggle to interpret complex technical content, leading to misinterpretations, inconsistencies, and errors in data extraction.These limitations highlight the need for AI solutions capable of handling industry-specific requirements, where precision, compliance, and traceability are essential for regulatory submissions, clinical trials, and scientific validation.</p>
<p>To address these limitations, the integration of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) offers a promising solution for improving accuracy, reliability, and compliance in AI-driven document analysis.By combining advanced retrieval mechanisms with context-aware generation, this approach reduces dependence on pre-trained datasets, which may contain biases or outdated information, and instead ensures fact-based, real-time content generation (Mostafapour et al., 2024;Doyal et al., 2023).One of the most persistent challenges in regulated industries like pharma and Medtech remains the parsing of unstructured documents, including clinical trial reports, regulatory filings, and research publications.In this study, ELISE's superior performance, closely aligned with human expertise, can be attributed to its advanced parsing capabilities.Unlike other AI tools, ELISE demonstrated a higher efficiency in processing text, formula and table, surpassing industry alternatives such as Megaparse, Llama or Unstructured.Preliminary study demonstrated a Normalized Edit Distance (NED) -how different two elements are by counting the minimum changes (insertions, deletions, substitutions) needed to transform one into the other, normalized by the longest element's length with 0 as identical and 1 completely different-respectively of 0.558241, 0.622417, 0.574559, and 0.490463 for Megaparse, Llama, Unstructured and Matsu (ELISE parser developed by Biolevate) on the global performance metric for textual content.When combined with context-aware modeling, this feature enhances query interpretation, ensure more precise responses and improve data traceability, making ELISE particularly adapted to regulatory and clinical applications.</p>
<p>The reliability of AI-generated outputs remains a key concern for industries where compliance with regulatory framework is non-negotiable.One striking example from this study (detailed in Supplementary Figure 2) highlights a critical issue: ChatGPT provided a DOI for an article when no other AI tools succeeded, but further investigation revealed that this DOI was not present in the original  To ensure a rigorous and unbiased evaluation of AI tools, a reference human answer was established to benchmark AI-generated responses.To further mitigate evaluation biases, AI Evaluator models were incorporated into the scoring methodology.By leveraging LLMs trained on diverse datasets, these evaluators ensure that responses are analyzed based on factual accuracy rather than subjective human biases.This multi-model validation approach minimizes overestimated AI-generated responses and ensures a more reliable assessment of AI performance, particularly in regulatory and clinical settings.</p>
<p>AI tools such as ChatGPT demonstrate remarkable capabilities in processing large volumes of data, offering significant advantages in speed and accessibility.However, their lack of industry-specific knowledge and contextual awareness makes human oversight essential in critical applications.This study addressed this gap by using expert-defined reference answers, ensuring AI-generated content meets the highest standards of accuracy and relevance for pharmaceutical, biotechnological, and medical applications.Moreover, the ethical and regulatory implications of AI-driven research and documentation must be considered, particularly concerning biases, transparency, and compliance with industry standard.While AI can augment scientific workflow, it cannot replace human expertise.Instead, a hybrid model, where AI supports human decision-making while ensuring data integrity and compliance, offers the most reliable and scalable approach for integrating AI into regulated industries (Mehta et al., 2024;Mostafapour et al., 2024;Ahaley et al., 2024;Singh et al., 2024).</p>
<p>One of the key differentiators among AI tools is compliance and traceability.AI solutions like ELISE integrate built-in compliance mechanisms, allowing for systematic verification of extracted data and response relevance.Unlike general-purpose AI models, which lack transparent methodologies, ELISE explicitly highlights source data and provides traceability on how each response was generated.This feature is critical for regulatory bodies and compliance teams in the pharma, biotech, and Medtech industries, where decision-making must be based on verifiable evidence rather than opaque AI-generated summaries.Furthermore, AI tools capable of explaining their reasoning processes, such as ELISE, allow human experts to refine AI queries, optimize search strategies, and improve model training over time, making them more aligned with human expertise (as demonstrated in Supplementary Figure 3).</p>
<p>Despite advancements in Deep Neural Networks (DNNs), Natural Language Processing (NLP), and Transformers architectures, AI models still struggle with high-level analytical reasoning, contextual variation, and long-form document coherence.These challenges impact critical decision-making in pharmaceutical and medical research, where AI-generated insights must be reliable, interpretable, and reproducible.To address these gaps, state-of-the-art techniques such as Pointer-Generator Networks and Sparse Attention Transformers are being implemented to enhance scientific summarization, improve structured data interpretation, and extract meaningful insights from large-scale regulatory of clinical documents (Tsirmpas et al., 2024).</p>
<p>Given the specialized needs of pharma, biotech, and Medtech, AI tools should be designed with modular adaptability, allowing organizations to select and integrate the most suitable models for their specific applications.As AI becomes more deeply embedded in regulatory, clinical, and research workflows, standardized industry guidelines must be established to ensure transparency, compliance, and ethical AI deployment.Human oversight will continue to play a critical role in refining AI-generated insights, ensuring scientific validity, and maintaining alignment with industry regulations, reinforcing the value of a hybrid AI-human approach in optimizing research and clinical decision-making (Bran et al., 2024;Meyer-Szary et al., 2024).AI tools that provide explainability, such as ELISE, play a crucial role in enhancing human-AI collaboration.By offering transparency on how responses are generated, these tools enable users to understand the AI's reasoning process, refine their queries for more precise outputs, and iteratively train the model to align more closely with expert-level expectations (as detailed in the Supplementary Figure 3).This capability is particularly valuable in regulated environments such as pharmaceuticals, biotechnology and Medtech, where interpretability, compliance, and continuous model improvement are essential for integrating AI into decisionmaking workflows.</p>
<p>ECACT score and standardized AI evaluation in regulated industries</p>
<p>The integration of AI tools into pharmaceutical, clinical, and healthcare workflow presents both significant opportunities and operational challenges.While AI optimizes processes such as medical documentation, literature review and regulatory reporting, the variability in AI-generated content quality necessitates a standardized evaluation framework.</p>
<p>Existing regulatory framework, such as SPIRIT-AI and CONSORT-AI, provide essential guidance for AI-driven clinical trials, ensuring transparency, reproducibility, and accountability (McGenity and Treanor, 2021).However, they do not provide standardized methodologies for assessing AI-generated research and regulatory outputs.Similarly, the PRISMA 2020 Checklist, widely used for systematic reviews, lacks specific AI assessment criteria (Page et al., 2021).</p>
<p>To address this gap, this study introduces a dedicated AI evaluation framework based on three core criteria: Extraction, Comprehension, and Analysis, each associated with a structured set of questions.This approach allows progressive assessment from basic data retrieval to advanced contextual analysis, ensuring AI tools are evaluating on their full operational capacity.</p>
<p>Additionally, data transparency in AI-generated response is a critical factor in regulatory compliance.As highlighted in the Danler study, AI models must be assessed not only on their accuracy but also on their ability to justify and trace their responses to verifiable sources.The variability in response quality further reinforces the importance of integrating Compliance and Traceability into AI evaluations (Danler et al., 2024).By incorporating these dimensions into the ECACT score, this study demonstrates that AI tools like ELISE, which follow compliance protocols and ensure traceability, significantly outperform models that do not.For instance, Epsilon, which frequently violates guidelines and generates overly verbose responses, was found to be less reliable despite strong comprehension scores.Similarly, ChatGPT's inability to provide traceable references led to its reassignment from second to last position when these additional criteria were included.</p>
<p>While ChatGPT demonstrates lower performance in traceability and analysis under the ECACT framework, it consistently ranks high in fluency, syntactic clarity, and readability.These traits make it a valuable option for use cases outside regulatory or high-stakes contexts, such as educational summaries, internal research notes, or exploratory drafts.ECACT should thus be interpreted as a scenario-sensitive evaluation tool, guiding tool selection according to task constraints.</p>
<p>Moving forward, the ECACT score should evolve to incorporate additional rating scales to further refine Compliance and Traceability assessments.Moreover, ethical considerations-including data privacy, processing speed, and AI model resource consumptionmust be integrated into AI evaluation frameworks.By establishing a standardized, transparent evaluation methodology, as proposed in this study, AI-driven research and regulatory applications can be optimized while ensuring data integrity and ethical compliance (Danler et al., 2024;Wattanapisit et al., 2023;Tangsrivimol et al., 2025).</p>
<p>The current study is limited to life science and biomedical articles.The sample size (n = 9) was deliberately kept small to allow for detailed, multi-criteria assessment of each tool's performance.However, this limited scale restricts the broader generalizability of the results.Future work will focus on validating the ECACT framework on independent datasets, non-English corpora beyond French-English, and diverse scientific domains such as engineering and social sciences, using larger and more heterogeneous article corpora.</p>
<p>Future versions of ECACT may integrate quantitative metrics of semantic similarity (e.g., cosine distance, ROUGE, BERTScore) to complement evaluator-based assessments and further reduce subjectivity in open-ended tasks.</p>
<p>Conclusion</p>
<p>The study assessed the performance of AI tools in scientific literature analysis, focusing on Extraction, Comprehension, and Analysis criteria while also introducing Compliance and Traceability as critical evaluation dimensions.ELISE emerged as the most effective tool, demonstrating superior performance across all criteria, particularly in data extraction and analytical reasoning, aligning closely with human expertise.ChatGPT exhibited strong efficiency in data retrieval but struggled with deeper comprehension and analysis, limiting its applicability for highly regulated environments.Epsilon, Humata, and SciSpace/Typeset performed moderately, with notable strengths in comprehension but significant weaknesses in structured data extraction, impacting their reliability for complex scientific and regulatory applications.</p>
<p>A key takeaway from this study, is that human oversight remains indispensable in validating AI-generated content, ensuring accuracy, compliance, and contextual relevance, particularly in pharmaceutical, biotechnological, and Medtech applications where data integrity and regulatory adherence are paramount.While AI tools significantly enhance efficiency in literature analysis and knowledge extraction, they must function as augmentative tools rather than standalone solutions.</p>
<p>To address the variability in AI-generated responses and provide a structured evaluation framework, this study introduced the ECACT score, incorporating Extraction, Comprehension, Analysis, Compliance, and Traceability, as key performance indicators.This scoring system ensures that AI tools are assessed not only for their ability to process scientific content but also for their transparency, adherence to guidelines, and ability to justify their outputs.Moving forward, establishing standardized evaluation frameworks such as ECACT will be crucial for integrating AI-driven solutions into research, clinical, and regulatory environments, ensuring that these tools meet the highest standards of scientific rigor, reliability, and ethical compliance.</p>
<p>Publisher's note</p>
<p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers.Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
<p>data's in the document: present in one sentence the global methodology of the study and if it described, present in one sentence different methodologies used in the study) Main results (in one sentence and for each part of the study results section, present the conclusion) Secondary results (in one sentence and for each sub-part of the study results section, if any, present the conclusion) Conclusion of the study (in two sentences) Summary of the abstract (in three sentences) Summary of the study (one sentence for each main section of the study: introduction, method, results, discussions, conclusion) Analysis Interpretation of the mains results (in one sentence for each main result, discuss/enhance results) Limitations of the study (in one sentence for each limitation) Three specific questions (only from data of the document, in one sentence) Prospects for the future (only from data of the document, in one sentence)</p>
<p>FIGURE 2
2
FIGURE 2Extraction performance of AI tools -ELISE and ChatGPT are the most effective tools for extracting standard metadata compared to common AI specialized tools.Results of AI tools evaluations for the Extraction criterion, presented by article: ChatGPT (A), ELISE (B), Epsilon (C), Humata (D), SciSpace/typeset (E) and global average across all AI tools (F).Statistically significant differences with ELISE are marked with asterisks: ns = not significant (p ≥ 0.05), ** = very significant (p ≤ 0.01) and *** = highly significant (p ≤ 0.001).</p>
<p>FIGURE 3 A
3
FIGURE 3A multi-model evaluation ensuring unbiased AI scoring -Global average of AI tools' evaluation by AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview and the average global score (Global) for the Comprehension and Analysis criteria applied to Article 5(Jones et al., 2021).Comparison between identified and anonymized AI tool response.</p>
<p>FIGURE 4 ELISE
4
FIGURE 4 ELISE and Epsilon outperform ChatGPT, Humata, Scispace/Typeset for scientific comprehension.Overall results of AI tools evaluation for the Comprehension criteria: ChatGPT (A), ELISE (B), Epsilon (C), Humata (D), SciSpace/Typeset (E), and Global average of AI tools evaluation (F).Statistically significant differences with ELISE are marked with asterisks: * = significant (p ≤ 0.05), ** = very significant (p ≤ 0.01) and *** = highly significant (p ≤ 0.001).</p>
<p>FIGURE 5 ELISE
5
FIGURE 5 ELISE demonstrates superior analytical capabilities compared to ChatGPT and specialized AI tools.Overall results of AI tools evaluations for the Analysis criterion: ChatGPT (A), ELISE (B), Epsilon (C), Humata (D), SciSpace/Typeset (E) and global average of AI tool evaluations (Global).Statistically significant differences with ELISE are marked with asterisks: ** = very significant (p ≤ 0.01) and *** = highly significant (p ≤ 0.001).</p>
<p>FIGURE 6
6
FIGURE 6Language change can lead to variations in AI tools responses.Overall results of AI tools (ChatGPT, ELISE, Epsilon, Humata and SciSpace/Typeset) for the extraction, comprehension and analysis criteria comparing performance across French and English articles.</p>
<p>FIGURE 7 ELISE
7
FIGURE 7 ELISE's responses align more closely with human expertise than other AI tools.Comparison of AI tool performance in the comprehension (A) and analysis (B) criteria, evaluated with and without human reference answers.</p>
<p>FIGURE 8
8
FIGURE 8Accuracy in extracting event-free survival (EFS) at 60 months from Article 8(Marks et al., 2022).Comparison of AI tool responses, with expected values shown in green, ELISE responses in blue, ChatGPT responses in red, and Other AI tools responses in gray (lines and dots) on Article 8 screenshot with the associated outputs of each AI tools.</p>
<p>FIGURE 9
9
FIGURE 9 Accuracy in identifying articles excluded for missing full text (Article 5 -Jones et al., 2021).Comparison of AI tool responses to the following question: How many articles were excluded for not having full text?With expected values shown in green, ELISE responses in blue and incorrect Other AI tools responses in red on Article 5 screenshot with the associated outputs of each AI tools.</p>
<p>FIGURE 10
10
FIGURE 10Accuracy in identifying hazard ratios (HR) for progression-free survival (Article 2 -Herrera et al., 2024).Comparison of AI tools responses for the question "What is the HR of the Progression-Free Survival in Modified Intent-to-treat Analysis Set for the teenagers?"with expected values shown in green, ELISE responses in blue and Other AI tools responses in gray on Article 2 screenshot with the associated outputs of each AI tools.</p>
<p>FIGURE 12 ECACT
12
FIGURE 12ECACT score evaluation and AI tool comparison.(A-E) Radar charts illustrating AI tools' performance across all evaluation criteria.(F) Global ECACT scores, confirming ELISE's superior reliability and effectiveness across all dimensions.</p>
<p>the literature review, the drafting and structuring of the scientific content.</p>
<p>TABLE 1
1
Overview of selected articles, including title, type, discipline, and citation.
ArticleTitleno.</p>
<p>TABLE 2
2
Overview of selected AI tools (A) and AI evaluator models (B).
AAI toolsProviderRelease dateCitations capacityHighlighting capacityChatGPT (GPT-4o)OpenAINovember 2024NoNoELISE 2.0BiolevateDecember 2024YesYesEpsilon 2.5EpsilonApril 2024YesYesHumataTilda technologiesNovember 2024YesYesSciSpace/Typeset 1.4.12SciSpace (ex-Typeset)November 2024YesYesBAI evaluator modelsCompanyLaunching dateReasoning modelClaude 3.5 SonnetAnthropicJune 2024NOGPT-4oOpenAIMay 2024NOO1 PreviewOpenAIDecember 2024YES
Frontiers in Artificial Intelligence 05 frontiersin.orgFIGURE 1</p>
<p>TABLE 3
3
Criteria and evaluation questions.</p>
<p>ChatGPT had sourced it externally, violating strict data integrity guidelines.Such discrepancies underscore the importance of traceability and compliance features in AI tools, particularly in clinical research, regulatory submissions, and drug development workflows, where data provenance must be verifiable and reproducible.
10.3389/frai.2025.1587244document,Frontiers in Artificial Intelligence17frontiersin.org
Data availability statementThe original contributions presented in the study are publicly available.This data can be found here: https://github.com/Biolevate/SL-EVAL-ECACT.FundingThe author(s) declare that no financial support was received for the research and/or publication of this article.A significant shift in AI tools rankings was observed when Compliance and Traceability were factored into the evaluation, further reinforcing the need for transparency and guideline adherence in AI-driven research tools.Notably, ChatGPT's performance declined significantly due to its lack of traceability, while Epsilon's overall score decreased due to its failure to comply with evaluation guidelines.Conversely, ELISE remained consistently at the top (Figure12F), demonstrating that it is not only the most performant AI tool for scientific literature analysis but also the most reliable in terms of transparency and methodological rigor.Sensitivity analysis of the ECACT scoreTo assess the robustness of the ECACT framework, a sensitivity analysis was performed by modifying the relative weights of each criterion (e.g., equal weights; Compliance and Traceability prioritized).While minor fluctuations were observed in the ranking of mid-performing tools, the top and bottom positions remained consistent.ELISE systematically outperformed other tools across all tested schemes (see Supplementary Table2and Supplementary Figure4), confirming the resilience of the ECACT methodology.4 Discussion and future directions AI tools are increasingly transforming workflows in pharmaceuticals, biotechnology, and Medtech by automating dataConflict of interestMaG, MuG, ST, LF, JB, AV, and FA were employed by the Biolevate, the developer of the AI tool ELISE analyzed in this study.Generative AI statementThe authors declare that Gen AI was used in the creation of this manuscript.This article was partially prepared with the assistance of ELISE, an artificial intelligence tool developed by Biolevate, to supportSupplementary materialThe Supplementary material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frai.2025.1587244/full#supplementary-material
Augmenting research: The role of artificial intelligence in recognizing topics and ideas. V Agrawal, S Bhardwaj, N Pathak, J Dixit, S Agarwal, M Momin, S S Ahaley, A Pandey, S K Juneja, T S Gupta, S Vijayakumar, 10.4018/979-8-3693-1798-3.ch003Utilizing AI Tools Acad. Res. Writing. 20242024. 2024</p>
<p>ChatGPT in medical writing: a game-changer or a gimmick?. 10.4103/picr.picr_167_23Perspect. Clin. Res. 15</p>
<p>Functions of double-stranded RNA-binding domains in nucleocytoplasmic transport. S Banerjee, P Barraud, 10.4161/15476286.2014.972856RNA Biol. 112014</p>
<p>Artificial intelligence for literature reviews: opportunities and challenges. F Bolaños, A Salatino, F Osborne, E Motta, 10.1007/s10462-024-10902-3Artif. Intell. Rev. 572592024</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. R Borah, A W Brown, P L Capers, K A Kaiser, 10.1136/bmjopen-2016-012545BMJ Open. 7e0125452017</p>
<p>. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, 2024</p>
<p>Augmenting large language models with chemistry tools. 10.1038/s42256-024-00832-8Nat. Mach Intell. 6</p>
<p>The changing scenario of drug discovery using AI to deep learning: recent advancement, success stories, collaborations, and challenges. C Chakraborty, M Bhattacharya, S S Lee, Z H Wen, Y H Lo, 10.1016/j.omtn.2024.102295Mol. Ther. Nucleic Acids. 351022952024</p>
<p>Quality and effectiveness of AI tools for students and researchers for scientific literature review and analysis. Stud. M Danler, W O Hackl, S B Neururer, B Pfeifer, A S Doyal, D Sender, M Nanda, R A Serrano, 10.7759/cureus.43292doi: 10.7759/cureusHealth Technol. Inform. 313e432922024. 2023Cureus.</p>
<p>. Elise , 2024. accessed February 17, 2025. 2024. accessed February 17, 2025</p>
<p>How to optimize the systematic review process using AI tools. N Fabiano, A Gupta, N Bhambra, B Luu, S Wong, M Maaz, 10.1002/jcv2.12234JCPP Adv. 4e122342024</p>
<p>. A Gatin, P Duchambon, G V D Rest, I Billault, C Sicard-Roselli, 2022</p>
<p>Protein dimerization via Tyr residues: highlight of a slow process with co-existence of numerous intermediates and final products. 10.3390/ijms23031174Int. J. Mol. Sci. 231174</p>
<p>Nivolumab+AVD in advanced-stage classic Hodgkin's lymphoma. A F Herrera, M Leblanc, S M Castellino, H Li, S C Rutherford, A M Evens, 10.1056/NEJMoa2405888N. Engl. J. Med. 3912024</p>
<p>Extracellular HMGB1 blockade inhibits tumor growth through profoundly remodeling immune microenvironment and enhances checkpoint inhibitor-based immunotherapy. P Hubert, P Roncarati, S Demoulin, C Pilard, M Ancion, C Reynders, 10.1136/jitc-2020-001966J. Immunother. Cancer. 9e0019662021. 2024. February 17, 2025</p>
<p>Impact of COVID-19 on mental health in adolescents: a systematic review. E A K Jones, A K Mitra, A R Bhuiyan, 10.3390/ijerph18052470Int. J. Environ. Res. Public Health. 1824702021</p>
<p>Artificial intelligence (AI) and ChatGPT involvement in scientific and medical writing, a new concern for researchers. A scoping review. A A Khalifa, M A Ibrahim, 10.1108/AGJSR-09-2023-0423Arab Gulf J. Sci. Res. 422024</p>
<p>Le système de surveillance des anomalies congénitales de l' Alberta: compte rendu des données sur 40 ans avec prévalence et tendances de certaines anomalies congénitales entre. R B Lowry, T Bedard, X Grevers, S Crawford, S C Greenway, M E Brindle, 10.24095/hpcdp.43.1.04fPromot. Santé Prév. Mal. Chron. Au. Can. 432023a. 1997 et 2019</p>
<p>The Alberta congenital anomalies surveillance system: a 40-year review with prevalence and trends for selected congenital anomalies. R B Lowry, T Bedard, X Grevers, S Crawford, S C Greenway, M E Brindle, 10.24095/hpcdp.43.1.04Health Promot. Chronic Dis. Prev. Can. 432023b. 1997-2019</p>
<p>Addition of four doses of rituximab to standard induction chemotherapy in adult patients with precursor B-cell acute lymphoblastic leukaemia (UKALL14): a phase 3, multicentre, randomised controlled trial. D I Marks, A A Kirkwood, C J Rowntree, M Aguiar, K E Bailey, B Beaton, 10.1016/S2352-3026(22)00038-2Lancet Haematol. 92022</p>
<p>Guidelines for clinical trials using artificial intelligence -SPIRIT-AI and CONSORT-AI †. C Mcgenity, D Treanor, 10.1002/path.5565J. Pathol. 2532021</p>
<p>AI-dependency in scientific writing. V Mehta, V Thomas, A Mathur, 10.1016/j.oor.2024.100269Oral Oncol. Rep. 101002692024</p>
<p>Scientific writing at the dawn of AI. J Meyer-Szary, M Jaguszewski, S Mikulski, 10.5603/cj.94335Cardiol. J. 312024</p>
<p>. M Mostafapour, J H Fortier, K Pacheco, H Murray, G Garber, 2024</p>
<p>Evaluating literature reviews conducted by humans versus ChatGPT: comparative study. 10.2196/56537Jmir Ai. 3e56537</p>
<p>. Y Ning, S Teixayavong, Y Shang, J Savulescu, V Nagaraj, D Miao, 2024</p>
<p>Generative artificial intelligence and ethical considerations in health care: a scoping review and ethics checklist. 10.1016/S2589-7500(24)00143-2Lancet Digit. Health. 6</p>
<p>The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, 10.1371/journal.pmed.1003583PLoS Med. 18e10035832021</p>
<p>. Scispace, 2024. February 17, 2025</p>
<p>Artificial intelligence at the pen's edge: exploring the ethical quagmires in using artificial intelligence models like ChatGPT for assisted writing in biomedical research. H Sharma, M Ruikar, S Singh, R Kumar, V Maharshi, P K Singh, V Kumari, M Tiwari, 10.4103/picr.picr_196_23Perspect. Clin. Res. 152024. 2024</p>
<p>Harnessing artificial intelligence for advancing medical manuscript composition: applications and ethical considerations. 10.7759/cureus.71744Cureus. 16e71744</p>
<p>Benefits, limits, and risks of ChatGPT in medicine. J A Tangsrivimol, E Darzidehkalani, H U H Virk, Z Wang, J Egger, M Wang, 10.3389/frai.2025.1518049Front. Artif. Intell. 815180492025</p>
<p>Neural natural language processing for long texts: a survey on classification and summarization. D Tsirmpas, I Gkionis, G T Papadopoulos, I Mademlis, 10.1016/j.engappai.2024.108231Eng. Appl. Artif. Intell. 1331082312024</p>
<p>Research progresses and applications of knowledge graph embedding technique in chemistry. C Wang, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysY Yang, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysJ Song, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysNan , Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysX Wattanapisit, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysA Photia, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysA Wattanapisit, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysS , Malays Fam. Phys. Off. J. Acad. Fam. Phys. Malays10.51866/lte.483doi: 10.51866/lte.483J. Chem. Inf. Model. 64692024. 2023Should ChatGPT be considered a medical writer?</p>
<p>The synergy of artificial intelligence and personalized medicine for the enhanced diagnosis, treatment, and prevention of disease. M A Zahra, A Al-Taher, M Alquhaidan, T Hussain, I Ismail, I Raya, 10.1515/dmpt-2024-0003Drug Metab. Pers. Ther. 392024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1203 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1203</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1203</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-267782742</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14281v2.pdf" target="_blank">A Landmark-Aware Visual Navigation Dataset for Map Representation Learning</a></p>
                <p><strong>Paper Abstract:</strong> Map representations learned by expert demonstrations have shown promising research value. However, the field of visual navigation still faces challenges due to the lack of real-world human-navigation datasets that can support efficient, supervised, representation learning of environments. We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised learning of human-centric exploration policies and map building. We collect RGBD observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space. The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or graph building and localization. These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments. Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors. We releaseour dataset with detailed documentation at https://huggingface.co/datasets/visnavdataset/lavn (DOI: l0.57967/hf/2386) and a plan for long-term preservation.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1203.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1203.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAVN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Landmark-Aware Visual Navigation dataset (LAVN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-annotated visual navigation dataset (virtual + real) that records RGB-D observations, odometry, per-step human waypoint point-clicks and landmark annotations, and saves a sequential graph (nodes per timestep, edges between consecutive nodes) for supervised learning of waypoint/landmark-informed exploration and topological map representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat-based virtual scenes (Gibson & Matterport) and multiple real-world indoor/outdoor scenes</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic indoor and outdoor navigation environments captured in Habitat (Gibson and Matterport datasets) and ten real-world scenes (campus buildings, hallways, walkways); domain: embodied visual navigation for robots/humans with ego-centric RGB-D camera observations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>By-default sequential chain connectivity: nodes added at every action and edges drawn only between consecutive nodes (path graphs per trajectory); loop-closure can be performed post-hoc using provided odometry to increase connectivity; landmarks provide sparse long-range connectors when used to build topological graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Dataset-level: 310 trajectories, 103,998 frames (nodes), 14,281 landmark annotations across 300 virtual + 10 real environments; per-trajectory max length 500 actions/frames; traversed trajectory lengths in virtual scenes range ~1.16 m to 85.38 m.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>human annotator (expert)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A human-guided navigator who issues discrete movement actions by clicking on ego-view images (move forward ~0.25m, turn left/right ~15°) and annotates landmarks with right-clicks; these human point-clicks serve as supervised waypoints and landmark examples for training agents.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Suggested/used metrics in text: coverage (rooms/space visited), trajectory length (meters), frames or steps per trajectory, sample efficiency; also supervised waypoint prediction loss (image-coordinate L2) and latent-space proximity (contrastive distance) for map representation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Human-centric waypoint/landmark-informed, topological/memory-based policies are hypothesized to be most effective (i.e., waypoint predictors + local low-level controllers or topological graph planners leveraging sparse landmark nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Reported/argued relationships: sparse, distinct landmarks act as high-value nodes that create structured global connections across otherwise sequential node chains, improving localization and navigation in complex/long-horizon trajectories; supervised human waypoints provide helpful subgoals that can reduce sample complexity and permit simpler network architectures; sequential-only graphs (edges between consecutive timesteps) provide limited global connectivity unless loop-closure or landmark links are added, which affects map quality and downstream planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Findings/suggestions in paper: policies that incorporate human-provided waypoints and landmark predictions can be simpler and more sample-efficient than purely learned exploration policies; landmark-augmented topological maps facilitate global connections and localization and thus favor graph-search or topological planning components in policy structure; long-horizon / high-distance trajectories motivate latent-space map representations and memory (contrastive proximity predictors) rather than purely reactive policies.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper is a dataset contribution: it provides graph-construction details (nodes per timestep, edges consecutive only, odometry for loop closure), landmark annotations, and suggested supervised training formulations (waypoint regression and contrastive latent proximity). It does not report controlled experiments measuring topology metrics (diameter, clustering coefficient, dead-end counts) or quantitative agent performance comparisons across different graph topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Landmark-Aware Visual Navigation Dataset for Map Representation Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural topological slam for visual navigation <em>(Rating: 2)</em></li>
                <li>Topological semantic graph memory for image-goal navigation <em>(Rating: 2)</em></li>
                <li>Learning exploration policies for navigation <em>(Rating: 2)</em></li>
                <li>Learning to explore using active neural slam <em>(Rating: 2)</em></li>
                <li>From cognitive maps to cognitive graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1203",
    "paper_id": "paper-267782742",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "LAVN",
            "name_full": "Landmark-Aware Visual Navigation dataset (LAVN)",
            "brief_description": "A human-annotated visual navigation dataset (virtual + real) that records RGB-D observations, odometry, per-step human waypoint point-clicks and landmark annotations, and saves a sequential graph (nodes per timestep, edges between consecutive nodes) for supervised learning of waypoint/landmark-informed exploration and topological map representation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Habitat-based virtual scenes (Gibson & Matterport) and multiple real-world indoor/outdoor scenes",
            "environment_description": "Photorealistic indoor and outdoor navigation environments captured in Habitat (Gibson and Matterport datasets) and ten real-world scenes (campus buildings, hallways, walkways); domain: embodied visual navigation for robots/humans with ego-centric RGB-D camera observations.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "By-default sequential chain connectivity: nodes added at every action and edges drawn only between consecutive nodes (path graphs per trajectory); loop-closure can be performed post-hoc using provided odometry to increase connectivity; landmarks provide sparse long-range connectors when used to build topological graphs.",
            "environment_size": "Dataset-level: 310 trajectories, 103,998 frames (nodes), 14,281 landmark annotations across 300 virtual + 10 real environments; per-trajectory max length 500 actions/frames; traversed trajectory lengths in virtual scenes range ~1.16 m to 85.38 m.",
            "agent_name": "human annotator (expert)",
            "agent_description": "A human-guided navigator who issues discrete movement actions by clicking on ego-view images (move forward ~0.25m, turn left/right ~15°) and annotates landmarks with right-clicks; these human point-clicks serve as supervised waypoints and landmark examples for training agents.",
            "exploration_efficiency_metric": "Suggested/used metrics in text: coverage (rooms/space visited), trajectory length (meters), frames or steps per trajectory, sample efficiency; also supervised waypoint prediction loss (image-coordinate L2) and latent-space proximity (contrastive distance) for map representation.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Human-centric waypoint/landmark-informed, topological/memory-based policies are hypothesized to be most effective (i.e., waypoint predictors + local low-level controllers or topological graph planners leveraging sparse landmark nodes).",
            "topology_performance_relationship": "Reported/argued relationships: sparse, distinct landmarks act as high-value nodes that create structured global connections across otherwise sequential node chains, improving localization and navigation in complex/long-horizon trajectories; supervised human waypoints provide helpful subgoals that can reduce sample complexity and permit simpler network architectures; sequential-only graphs (edges between consecutive timesteps) provide limited global connectivity unless loop-closure or landmark links are added, which affects map quality and downstream planning.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Findings/suggestions in paper: policies that incorporate human-provided waypoints and landmark predictions can be simpler and more sample-efficient than purely learned exploration policies; landmark-augmented topological maps facilitate global connections and localization and thus favor graph-search or topological planning components in policy structure; long-horizon / high-distance trajectories motivate latent-space map representations and memory (contrastive proximity predictors) rather than purely reactive policies.",
            "notes": "The paper is a dataset contribution: it provides graph-construction details (nodes per timestep, edges consecutive only, odometry for loop closure), landmark annotations, and suggested supervised training formulations (waypoint regression and contrastive latent proximity). It does not report controlled experiments measuring topology metrics (diameter, clustering coefficient, dead-end counts) or quantitative agent performance comparisons across different graph topologies.",
            "uuid": "e1203.0",
            "source_info": {
                "paper_title": "A Landmark-Aware Visual Navigation Dataset for Map Representation Learning",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural topological slam for visual navigation",
            "rating": 2,
            "sanitized_title": "neural_topological_slam_for_visual_navigation"
        },
        {
            "paper_title": "Topological semantic graph memory for image-goal navigation",
            "rating": 2,
            "sanitized_title": "topological_semantic_graph_memory_for_imagegoal_navigation"
        },
        {
            "paper_title": "Learning exploration policies for navigation",
            "rating": 2,
            "sanitized_title": "learning_exploration_policies_for_navigation"
        },
        {
            "paper_title": "Learning to explore using active neural slam",
            "rating": 2,
            "sanitized_title": "learning_to_explore_using_active_neural_slam"
        },
        {
            "paper_title": "From cognitive maps to cognitive graphs",
            "rating": 1,
            "sanitized_title": "from_cognitive_maps_to_cognitive_graphs"
        }
    ],
    "cost": 0.00973225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Landmark-Aware Visual Navigation Dataset for Map Representation Learning
22 Feb 2025</p>
<p>Faith Johnson faith.johnson@rutgers.edu 
Kristin Dana kristin.dana@rutgers.edu 
Bryan Bo Cao boccao@cs.stonybrook.edu 
Shubham Jain jain@cs.stonybrook.edu 
Ashwin Ashok aashok@gsu.edu </p>
<p>Rutgers University New Brunswick
NJUSA</p>
<p>Stony Brook University Stony Brook
NYUSA</p>
<p>Georgia State University Atlanta
GAUSA</p>
<p>A Landmark-Aware Visual Navigation Dataset for Map Representation Learning
22 Feb 20257F571167477506FFE3C0256D984B2A9B10.57967/hf/2386)arXiv:2402.14281v2[cs.CV]Human-in-the-LoopVisual NavigationMap RepresentationGraph RepresentationGaze Behavior GenerationImplicit Behavior CloningLandmarkDataset
Map representations learned by expert demonstrations have shown promising research value.However, the field of visual navigation still faces challenges due to the lack of real-world human-navigation datasets that can support efficient, supervised, representation learning of environments.We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised learning of human-centric exploration policies and map building.We collect RGBD observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space.The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or graph building and localization.These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments.Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors.We release our dataset with detailed documentation at https://huggingface.co/datasets/visnavdataset/lavn (DOI: 10.57967/hf/2386) and a plan for long-term preservation.</p>
<p>I. INTRODUCTION</p>
<p>Robotic navigation remains a research problem of interest as advances in AI make intelligent mobile agents, such as delivery bots, more feasible.Many approaches have been proposed from SLAM [1,2,3], to graph-based methods [4,5,6,7], to reinforcement learning (RL) [8] to enable robots to effectively move around a space in a goal-oriented manner.However, these methods suffer from high computational complexity, high memory usage, and low sample efficiency respectively.Visual navigation [9,10,11] methods circumvent several of these challenges by using neural networks to implicitly learn scene geometry and an optimal navigation policy without necessitating direct environment interaction.</p>
<p>Typically, visual navigation methods are evaluated with the image goal task, where an agent is given a goal image of some target and tasked with finding and reaching it in the environment.The success of state of the art (SOTA) methods at this task comes down to an agent's ability to efficiently explore its environment.Several works focus on explicitly learning optimal exploration policies [12,13,14,15].However, they either abstract exploration out of the problem all together by * Both authors contributed equally to this work.assuming that the environment has been seen by the agent prior to test time [10], with the hope that their networks will implicitly learn an adequate exploration strategy from the data due to the specific task structure [16], or explicitly define the exploration strategy for the agent by hard coding it into the architecture [11].</p>
<p>Of those methods that learn to explore, several niches have arisen such as adding an exploration term to RL reward functions with [13] and without [17] behavioral cloning, building metric maps or graphs with [14] and without [18] semantics, and learning to propose exploration waypoints that provide helpful subgoals during navigation for local policies [15].These methods suffer from similar downsides as those mentioned previously with the added potential for also requiring full ground truth semantic labels of an environment, which can be extremely costly to acquire.Instead, a much simpler approach would be to explicitly learn good exploration techniques in a human-centric manner.Learning human-centric exploration strategies in a supervised manner would allow for much simpler networks to effectively solve navigation problems, thus reducing the computational complexity, high memory usage, and low sample efficiency issues of SOTA.However, there is very limited data that would allow for this kind of learning.</p>
<p>In this work, we present a Landmark-Aware Visual Navigation (LAVN) dataset of human-guided environment exploration videos collected from the photorealistic Habitat virtual environment as well as several real world environments.We collect RGB image observations and human point click pairs as a human guides an agent in exploring each environment.These point clicks represent subgoals that guide the agent incrementally though the space and can be used for supervised learning of human-centric waypoints for environment exploration.Additionally, we expect that having landmarks when exploring has the potential to improve current SOTA performance on the map-building and localization tasks inherent in navigation.To this end, the human annotator also provides examples for helpful navigational landmarks, which will allow for supervised training of a landmark prediction network as well.In this way, it becomes possible to directly train a network to explore an environment in a human-centric manner under multiple SOTA paradigms as demonstrated by the recent work [19,20] in the visual navigation task.In addition, learning from such waypoint and landmark annotations allows for aligning the cognitive mindset of human navigation with mobile robots, enabling intuitive interactions with people to convey mobile robots' intent in navigation via gaze behavior generation [21,22,23,24,25].</p>
<p>This paper is organized as follows: we summarize our dataset's key differences from existing works in Section II.Then we present the details of data acquisition both in the virtual and real-world environments in Section III and IV, respectively.We conclude our work in Section V.</p>
<p>II. RELATED WORK</p>
<p>A. Map Representation for Navigation</p>
<p>The success of navigation relies on an effective learning and understanding of the environment.Two main approaches for this are map representation learning in geometry-based [1,2,3] and learning-based [8,10,12,26,27] domains.However, the former requires careful and robust hand-engineered feature design and requires intensive computation (e.g.keyframes in SLAM).The advancements of Deep RL in the second category are partially attributed to the emergence of simulators designed for photorealistic simulation, including Habitat AI [28,29] with datasets Gibson [30] and Matterport [31].While these simulators enable large scale model training, they are also expensive to build, can run slowly enough to mitigate their ability to produce large numbers of training samples, and introduce the sim2real problem when deploying models in the real world.To combat this, methods like NRNS [11] demonstrate the effectiveness of using expert videos to train robots without interacting with environments.</p>
<p>B. Human-Robot Interaction Datasets</p>
<p>There is a large body of datasets or benchmarks for Human-Robot Interaction, ranging from industrial [32] to social [33,34,35,36,37,38,39,40] environments.As one of the fundamental tasks for robots, researchers have collected datasets for learning visual navigation with socially compliant behaviors [21,23,33,34,35,37,38,39,40], including intuitively conveying mobile robot's navigation intent [22,24]
✓ ✓ ✗ ✗ ✓ ✓ ✗ RUGD [42] ✓ ✗ ✓ ✓ ✓ ✗ ✗ RealEstate20k [48] ✓ ✗ ✓ ✓ ✓ ✓ ✗ EmbodiedQA [49] ✓ ✓ ✗ ✓ ✗ ✓ ✗ SCAND [33] ✓ ✗ ✓ ✓ ✓ ✓ ✗ LAVN (Ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓
to humans.Apart from learning from other robots collectively [41], robotic visual navigation can benefit from human feedback via visual cues [42], language [43], gesture [25,44], multimodal instructions [38,45], or demonstrations [46].Our work extends the line of efficient map representation using topological graphs [6,7].Similar to how a human [47] can quickly learn to navigate in a new environment by implicitly treating landmarks as nodes in their mental graph [47], we hypothesize that a robot can acquire the ability to navigate in novel places efficiently by intelligently injecting landmarks into its own topological graph.However, little attention has been paid to such datasets, resulting in a lack of data that includes landmark information for this research in the literature.Our dataset hereby aims to fill this gap by providing a series of human-expert-annotated videos with landmarks both in virtual and real-world environments, including indoor and outdoor scenes.We summarize the key characteristics of recent datasets in Table II
-B.</p>
<p>III. VIRTUAL DATASET</p>
<p>The main objective of the data collection is to provide human navigation demonstrations for robots in a real-world environment.As video has been proven to be an effective way to teach robots to accomplish different tasks [11], we hereby collect trajectories in video format.Below, we detail the process of data collection for the virtual dataset.</p>
<p>A. Environments</p>
<p>We provide human-click annotations for a selection of the Matterport [31] and Gibson environments [30] in Habitat-Sim [28,29] to facilitate virtual environment exploration.We provide a full list of annotated environments with the data.Habitat is already used to train many visual navigation methods, so having in-domain, landmark-aware data will allow for direct extensions of these methods for greater ease of model comparison.Additionally, it allows for greater data collection volume and lower data processing time.</p>
<p>We aim at randomly selecting rooms evenly distributed by room IDs to ensure diversity in room types and sizes, as detailed in the Appendix.</p>
<p>B. Data Collection and Representation</p>
<p>To start, the human navigator is placed at a random, navigable point in an environment, and tasked with visiting as many rooms as possible while identifying distinct landmarks in each room.The current first person observation of the environment is shown on a screen, and the human moves through the environment by left clicking on this image towards the direction they would like to move.There are three discrete moving action choices: moving forward 0.25m, turning left 15 • , and turning right 15 • .To specify a landmark, the navigator right clicks on the observation image over the object or region of space they choose.We provide a human pointclick supervisory signal for each time step of the trajectory, but these signals could easily be masked to generate data to train temporally abstracted waypoint predictors.</p>
<p>A graph is built as the navigator moves through the space, where nodes are added each time an action is taken.Each node contains one RGB observation, one depth image, the human navigational point click, and the ground truth odometry (including ground truth location and orientation).Edges are drawn between consecutive nodes only, however it is possible to perform loop closure on the graph using the odometry provided.Each graph also contains a sequential list of the actions taken to move through the environment.A landmark is defined as a unique and distinct object, distinguishable from other common objects in the scene.A list of pairs of landmark point-click image coordinates and their corresponding nodes is also provided.Each human-guided trajectory is terminated after 500 actions or when the annotator has navigated through the entirety of the space, whichever comes first.</p>
<p>IV. REAL WORLD DATASET A. Environments</p>
<p>Real-world data is recorded in a broad spectrum of environments.Indoor scenes include laboratories and hallways inside a building, while outdoor environments contain avenues, walkways, and campus buildings with different architectures.A wide range of trajectories with various complexities are captured in both regular shapes such as rectangles in corridors inside a building, as well as irregular shapes on roads surrounded by trees or near a lake.Objects in common environments are included, ranging from chairs, desks, doors, and computers in indoor rooms, to trees, lakes, stones, bikes, bricks and so forth in open spaces outside a building.</p>
<p>B. Data Collection and Representation</p>
<p>In order to collect consistent data between the real-world data and simulation, the human annotator is asked to follow the same protocol as in the virtual environment data collection described in Section III-B.During navigation, the annotator holds a camera (iPhone13Mini [50]'s built-in camera) naturally around 1.5m high off the ground facing forwards.Unlike in simulation where agents can move with exact distances or turning angles, real-world environments require that each action taken by the human navigator is inherently inexact.Therefore, the navigator is instructed to follow the same discrete action choices by their own judgement.Specifically, the expert takes a picture (saved in JPEG) in every step (around 0.7m), or in every left or right turn (roughly 15 • ).This also presents the real-world data with complex noise or perturbations, which can improve downstream method robustness.</p>
<p>Resolution in the original images was 4032x3024.To promote consistency between the virtual and real-world data, the  images are resized to be the same resolution of 640x480 as in the simulation and saved in JPEG format with quality 85%.Landmarks are annotated offline after the videos are saved.Samples from our dataset are visualized in Fig. 2. Both virtual and real-world images are photorealistic and can benefit downstream research in the real world.Also notice that trajectories are recorded in ten diverse scenes in both indoor and outdoor environments shown in Fig. 8.Our proposed dataset provides a unique benchmark for both learning and evaluation to serve the visual navigation community.Our dataset statistics are summarized in Table IV-B and Fig. 4.</p>
<p>Observe that most trajectories reach the maximum number of frames (500).On the other hand, a wide variety of trajectory horizon lengths (#F rames/T raj.) and number of landmarks (#LM/T raj.) is covered which benefit robot learning in both short [51] and long horizon [52,53,54] tasks.For both datasets, the graph is saved as a json file.We detail the file organization and examples of dataset usage in  the Appendix.</p>
<p>Real-world Relevance.The photorealistic images from virtual datasets (Gibson and Matterport), combined with our real dataset, support robot visual navigation learning tailored for real-world deployment.</p>
<p>V. CONCLUSION</p>
<p>In this work, we introduce the first Landmark-Aware Visual Navigation (LAVN) dataset with human point-click annotations, providing exploration waypoint and environment landmark labels that allow for supervised training for exploration and map building tasks.We provide trajectories in 300 virtual and real-world environments with RGB observations, depth images, odometry, and human point-clicks for navigation waypoint supervision and environment landmark supervision.With this dataset, it will be possible to improve visual navigation models that exhibit human-centric exploration policies on the image-goal task, paving the way for autonomous embodied agents in real-world applications.Future work includes incorporating dynamic objects in the scene.XI.DATASET ORGANIZATION After downloading and unzipping the zip files, please reorganize the files in the following tructure:</p>
<p>where the main landmark annotation scripts makeData virtual.pyand makeData real.py are in folder (1) src.(2) Virtual and (3) Real store trajectories collected in the simulation and real world, respectively.Each trajectory's data is collected in the following format:</p>
<p>We present RGB observations and the corresponding depth image samples in Fig. 6.Fig. 6.Samples of pairs of RGB and depth images in virtual environments in two rows.</p>
<p>Statistics of trajectory lengths that a robot has traversed over all virtual environments recorded in meters (m) is shown in Fig. 7.The wide spread of trajectory lengths (ranging from 1.16m to 85.38m) provides a significant temporal diversity that enriches a robot's learning recipe with both short-and long-term dependencies.Trajectories are recorded in ten diverse scenes in both indoor and outdoor environments shown in Fig. 8.</p>
<p>Virtual Real</p>
<p>Fig. 8.Samples of RGB images in virtual (1st and 2nd rows in blue) and real-world (3rd and 4th rows in green) environments with landmarks and waypoints annotated by red and orange dots, respectively.Trajectories are recorded in comprehensive environments both indoors and outdoors.</p>
<p>XIV. DATASHEETS FOR DATASETS</p>
<p>We present the answers to the questions of datasheets for datasets in this sub-sections.</p>
<p>A. Motivation</p>
<p>• For what purpose was the dataset created?Answer: LAVN is created to assist efficient representation learning in embodied visual navigation.• Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?Answer: The authors in this paper.• Who funded the creation of the dataset?If there is an associated grant, please provide the name of the grantor and the grant name and number.Answer: We will release the funding source upon acceptance.</p>
<p>B. Composition</p>
<p>• What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?Answer: LAVN consists of RGB and depth images, landmark and waypoint annotations saved in image coordinate formats in a json files.The collected environments include virtual in Habitat AI [29] and real world scenes without any human involvement.</p>
<p>The details are presented in the main paper.• Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?Answer: N/A.• Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?Answer: No.</p>
<p>C. Collection Process</p>
<p>• How was the data associated with each instance acquired?Answer: For virtual environments, we run the simulator Habitat AI on a desktop.In each step, it displays the current frame and a human annotates the waypoint by left click or landmark (if exists) by right click on the frame.The robot will move according to the action occurred by waypoint.Then the corresponding data (RGB, depth, waypoint and landmark) will be saved on disk.For real-world scenes, a human use a</p>
<p>• If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?Answer: N/A.This dataset is not related to people.• Will older versions of the dataset continue to be supported/hosted/maintained?Answer: We plan to maintain the latest version.• If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?Answer: Individual contributors can contact the authors of this paper.</p>
<p>Fig. 1 .
1
Fig. 1.We propose a novel Landmark-Aware Visual Navigation (LAVN) dataset for map representation research.LAVN provides unique landmark information (in red) in specific scenes to aid model navigation and exploration performance and map representation learning.In this figure, a human expert in Room A (in blue) provides a video-based training trajectory (in green lines) for navigation given a goal image (green box).Rooms are depicted in different colors and represented with nodes.Our dataset facilitates topological map representation learning by injecting landmarks into a robot's observation.</p>
<p>Fig. 2 .
2
Fig.2.Samples of RGB images in our dataset captured from an ego-centric camera.Video frames start from left to right.Landmarks are visualized with red dots.Samples in the virtual dataset (1st and 2nd rows in blue) are visually realistic compared to the real-world observations (3rd and 4th rows in green), which makes landmark data acquisition scalable to serve real-world research purposes.</p>
<p>Fig. 3 .Fig. 4 .
34
Fig. 3. Samples of RGB images in the real world environments with landmarks annotated by red dots.Trajectories are recorded in comprehensive environments both indoors and outdoors.</p>
<p>Fig. 5 .
5
Fig. 5. (a) Stacked bar chart of actions in virtual and real-world environments.(b) Waypoints heatmap over the entire dataset.</p>
<p>Fig. 7 .
7
Fig. 7. Histogram of traversed trajectory lengths in all virtual scenes in meters (m).LAVN consists of trajectories in diverse lengths.</p>
<p>TABLE I SUMMARY
I
OF DATASET CHARACTERISTICS FOR VISUAL NAVIGATION.WE PROVIDE A UNIQUE DATASET WITH BENEFICIAL NAVIGATIONAL LANDMARK LABELS IN HUMAN TRAJECTORY DEMONSTRATIONS IN BOTH REAL WORLD AND SIMULATED ENVIRONMENTS.ADD.: ADDITIONAL; MOD: MODALITY; SEM: SEMANTICS; LM: LANDMARK.
DatasetHuman Traj. DemoSim RealSmall ScaleNo Add. Mod.No Sem.LMHabitat-Web [46]</p>
<p>TABLE II STATISTICS
II
OF THE LAVN DATASET.</p>
<h1>Traj.TotalTotal#Virt. #Real World#Frames #LandmarksEnvsEnvs310103,99814,28130010</h1>
<p>[29]w many instances are there in total (of each type, if appropriate)?Answer: There are 310 trajectories, 103,998 frames (RGB and depth images), 14,281 landmarks in 300 virtual and 10 real environments in this dataset.•Doesthedatasetcontainall possible instances or is it a sample (not necessarily random) of instances from a larger set?Answer: It contains all possible instances.•Whatdatadoeseach instance consist of?Answer: Each instance consists of an RGB and depth image, the corresponding waypoint in image coordinate annotated by human that indicates the direction to move, the corresponding action inferred by the waypoint as well as the landmark in image coordinate if recognized by a human.•Istherealabel or target associated with each instance?Answer: Yes, the label includes the waypoint, landmark if exists.•Isanyinformationmissing from individual instances?If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).This does not include intentionally removed information, but might include, e.g., redacted text.Answer: N/A.• Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?Answer: N/A.• Are there recommended data splits (e.g., training, development/validation, testing)?Answer: No. Users may choose any train/val split strategies.•Arethereany errors, sources of noise, or redundancies in the dataset?Answer: We checked the correctness of our dataset.If such errors exist, we will further correct our dataset.•Isthedataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?Answer: It is self-contained.Users can use it directly without the use of the virtual environment such as Habitat AI[29].•Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?Answer: N/A.• Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?Answer: No. • Does the dataset relate to people?Answer: No. • Does the dataset identify any subpopulations (e.g., by age, gender)?Answer: No.</p>
<p>ACKNOWLEDGMENT This research was supported by the National Science Foundation (NSF) NRT NRT-FW-HTF: Socially Cognizant Robotics for a Technology Enhanced Socienty (SOCRATES), No. DGE-2021628 and Grant Nos.CNS-2055520, CNS-1901355, CNS-1901133.Published as a short contribution paper at HRI 2025The full list of scenes in Matterport[31]is listed in TableIV: 00000-kfPV7w3FaU5 00001-UVdNNRcVyV1 00002-FxCkHAfgh7A 00003-NtVbfPCkBFy 00004-VqCaAuuoeWk 00005-yPKGKBCyYx8 00007-UQuchpekHRJ 00008-VYnUX657cVo 00010-DBjEcHFg4oq 00011-1W61QJVDBqe 00012-kDgLKdMd5X8 00013-sfbj7jspYWj 00014-nYYcLpSzihC 00015-LPwS1aEGXBb 00017-oEPjPNSPmzL 00018-as8Y8AYx6yW 00019-AfKhsVmG8L4 00020-XYyR54sxe6b 00021-yQESfVcg18k 00022-gmuS7Wgsbrx 00023-zepmXAdrpjR 00031-Wo6kuutE9i7 00032-jTTGECZYKRA 00033-oPj9qMxrDEa 00034-6imZUJGRUq4 00035-3XYAD64HpDr 00036-41FNXLAZZgC 00037-oKFJo8jpzRW 00038-aJg466zMSNt 00039-ANmWrL7Kz7h 00040-ZB8o8rMmPdB 00102-r77mpaAYUEc 00103-gUqgeUmUagL 00104-KJxdMPgweZG 00105-xWvSkKiWQpC 00106-ZVScmfktNQ1 00107-Y6WjWkVEUks 00109-GTV2Y73Sn5t 00111-AMEM2eWycTq 00112-r38SGhq8aJr 00113-3goH1WRaCYC 00114-Coer9RdivP7 00115-NBWrHFXBF5p 00116-xp4FyfQ6Wr5 00117-2NwLiyeKcrK 00118-F5j7ZLfMm1n 00120-eAUmfFLZDR3 00121-D2PqRE5ZvyQ 00122-QDtpZSqaeyW 00123-C3ifY177Ldq 00201-k7vRbGpz44m 00202-yVbpFay8gTU 00203-VoVGtfYrpuQ 00204-gxttMtT5ZGK 00205-NEVASPhcrxR 00206-uhkqDVMtEnn 00207-FRQ75PjD278 00209-C5RbHBQ76DE 00210-j2EJhFEQGCL 00211-hmRxh2mmzNC 00212-bAdy4hKf1a1 00213-mkvHBa3mEEk 00214-WeyCwVzL53K 00215-zWydhyFhvcj 00216-6EMViBCA2N7 00217-qz3829g1Lzf 00218-fQHGxvurx9L 00220-KAzjXJvZtR3 00221-zJEEFaNaRbB 00222-g8Xrdbe9fir 00301-JiHGQpwKUvd 00302-JFgrz9MNz4b 00303-ghWQ5kHV97i 00304-X6Pct1msZv5 00305-W3J8ruZTQic 00306-Y4L8fjz2yH7 00307-vDfkYo5VqEQ 00309-VKmpsujnc5t 00310-WnvnMQh4eEa 00311-mHXUEKEV6gR 00312-UrFKpVJpvHi 00313-PE6kVEtrxtj 00314-NwG7cpZnRZb 00315-We1N7vBtyGm 00316-LqsTKpxKVP2 00317-P8XJUpcAUkf 00318-6qJyEsZNuey 00319-sjH1uaR68XQ 00320-nicaPonCxvC 00321-JWWJBQWHv64 00401-H8rQCnvBgo6 00402-zR6kPe1PsyS 00404-QN2dRqwd84J 00406-n2Tt2eJdqnT 00407-NPHxDe6VeCc 00408-RamZzGBBPbT 00409-rxGLNxH6eoJ 00410-v7DzfFFEpsD 00411-o4tckGBtaxz 00412-mDPCxA7W1WN 00413-YM4nG4pSAEJ 00414-77mMEyxhs44 00415-rBmEe6ab5VP 00416-zCMdfYaW9iF 00417-nGhNxKrgBPb 00419-fbUcgfPMBDr 00420-R6Byftz8wRN 00421-gamLwhSzHci 00422-8wJuSPJ9FXG 00423-bEdki9cbHDG 00501-N7YVmJQ8sAu 00502-nMeXfQU4PMS 00504-frThKkhTwFT 00505-ZwnLFNzxASM 00506-QVAA6zecMHu 00507-RfNGMBdVbAZ 00508-4vwGX7U38Ux 00509-gDDiZeyaVc2 00510-JSgMy8tTACD 00511-8uSpPmctPXC 00601-PjnDyQJJ3eM 00604-W4r5JssudHR 00605-T22dejNjHK7 00606-W16Bm4ysK8v 00607-JXdzHne1mRo 00608-j2Nms3h9XJv 00609-x1pTUWx9DPr 00611-PXAfUkZGMdU 00612-GsQBY83r3hb 00613-s19Uyn7AWwv 00614-ki6Cu76pWzF 00615-PUNuHY5M7MS 00616-zhzot8MvSjF 00617-AENiMBDjVFb 00618-T4G9hTR5WSv 00619-R9fYpvCUkV7 00620-AUkcTmUs8mw 00622-bxwHR9ipFG8 00701-tpxKD3awofe 00702-wCqnXzoru3X 00703-xvDx98avcwd 00704-qnKYFQsjnHf 00705-2XVvKEDd54w 00706-YHmAkqgwe2p 00707-XVSZJAtHKdi 00708-eUJx9a4u63E 00709-8LLjiNrWzJ9 00710-DGXRxHddGAWTABLE IV FULL LIST OF SCENE IDS IN MATTERPORT USED IN LAVN.VI. DATASET DOCUMENTATION AND INTENDED USESThe datasheet for our dataset can be found at https://huggingface.co/datasets/visnavdataset/lavn along with the data.We include samples from each trajectory in the supplemental material, available in the LAVN Samples.zipfile.The intended uses of this dataset include, but are not limited to:• image representation learning • visual landmark prediction • human navigation policy learning • offline reinforcement learning• robot navigation intent visualization The intended practical applications of this dataset include, but are not limited to:• last-mile delivery • factory/warehouse/hospital item transportation • robotic assistance in retail environments• search-and-rescue operations in hazardous terrainsVII. ACCESSING THE DATAThe dataset is available at https://huggingface.co/datasets/visnavdataset/lavn.Zipped files can be downloaded at https://huggingface.co/datasets/visnavdataset/lavn/tree/main.VIII. CROISSANT METADATAThe croissant metadata can be found at https://huggingface.co/api/datasets/visnavdataset/lavn/croissant.IX. AUTHOR STATEMENT OF RESPONSIBILITYWe, the authors, take full and total responsibility in case of violation of rights inherent in our dataset submission.X. HOSTING, LICENSING, AND MAINTENANCE PLAN.We choose to host our dataset on huggingface, where we detail the maintenance plan, licensing details (MIT license in this case), and other relevant information.We will conduct a long-term maintenance plan to ensure the accessibility and quality for future research:• Data Standards: Data formats will be checked regularly with scripts to validate data consistency.• Data Cleaning: Data in incorrect formats, missing data or contains invalid values will be removed.• Scheduled Updates: We set up montly schedule for data updates.• Storage Solutions: HuggingFace, with DOI (doi:10.57967/hf/2386), is provided as a public repository for online storage.A second copy will be stored in a private cloud server while a third copy will be stored in a local drive.• Data Backup: Once one of the copies in the aforementioned storage approach is detected inaccessible, it will be restored by one of the other two copies immediately.• Documentation: Our documentation will be updated regularly reflecting feedback from users.worker graph.jsonstores the meta data in dictionary in Python saved in json file with the following format:where [&lt; LOC X &gt;, &lt; LOC Y &gt;, &lt; LOC Z &gt;] is the 3-axis location vector, &lt; ORIEN T &gt; is the orientation only in simulation.[&lt; COOR X &gt;, &lt; COOR Y &gt;] are the image coordinates of landmarks.ACTION NAME stores the action of the robot take from the current frame to the next frame.XII. DATASET USAGEThis section highlights two detailed examples of efficient visual navigation training.The visual navigation task can be formulated as various types of problems, including but not limited to:1. Supervised Learning by mapping visual observations (RGBD) to waypoints (image coordinates).A developer can design a vision network whose input (X) is RGBD and output (Y ) is image coordinate, specified by img path, depth path and click point [&lt; COOR X &gt;, &lt; COOR Y &gt;] in the worker graph.jsonfile in the dataset.The loss function can be designed to minimize the discrepancy between the predicted image coordinate (Y pred) and the ground truth (Y ), e.g.loss = ||Y pred − Y ||.Then Y pred can be simply translated to a robot's moving action, such as Y pred in the center or top region of an image means moving forward while lef t/right regions represent turning left or right.2. Map Representation Learning in the latent space of a neural network.One can train this latent space to represent two observations' proximity by contrastive learning.The objective is to learn a function h() that predicts the distance given two observations (X 1 ) and (X 2 ): dist = h(X 1 , X 2 ).Note that dist() can be a cosine or distance-based function, depending on the design choice.The positive samples can be nodes (a node includes information at a timestep such as RGBD data and image coordinates) nearby while further nodes can be treated as negative samples.A landmark is a sparse and distinct object or scene in the dataset that facilitates a more structured and global connection between nodes, which further assists in navigation in more complex or longer trajectories.Published as a short contribution paper at HRI 2025 XIII.EXTENDED DATASET DETAILS We aim to randomly select rooms, evenly distributed by room IDs, to ensure diversity in room types and sizes, e.g.covering Room ID starting with letter A-Z in the Gibson dataset, digits 000-007 in the Matterport.This randomness results in diverse sets of rooms without bias in terms of room type (restroom, kitchen, hallway, office, garage, etc. in Fig.6) and room size (shown in the diverse traversed trajectory length ranging from around 5 to 85 meters in Fig.7).We present the full list of scene IDs in Gibson[30]cellphone's camera acting as a robot to traverse in the environment.Real-world data collection process follows the same procedure in the virtual environment, except that waypoints and landmarks are annotated during post-processing of videos.• What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)?Answer: We use Habitat AI[29]run on a desktop in Ubuntu 20.04 LST equipped with two NVIDIA RTX 1080 GPUs.An iPhone 13 mini[50]is used to collect real-world data.• If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?Answer: For virtual data, we collect images in Gibson[30]and Matterport[31]datasets in a deterministic way.• Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?Answer: The authors of this paper volunteered to help with the data collection process.• Over what timeframe was the data collected?Answer: Data was collected in September 2023.• Were any ethical review processes conducted (e.g., by an institutional review board)?Answer: N/A.Not required as no human involved.• Does the dataset relate to people?Answer: No.D. Preprocessing/cleaning/labeling• Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-ofspeech tagging, SIFT feature extraction, removal of instances, processing of missing values)?Answer: We removed trajectories that contain incorrect labels (waypoints or landmarks).• Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?Answer: Yes.• Is the software that was used to preprocess/clean/label the data available?Answer: We use Google Sheets and Python scripts written by ourselves.E. Uses• Has the dataset been used for any tasks already?Answer: Yes, for improving embodied visual navigation representation learning.• Is there a repository that links to any or all papers or systems that use the dataset?Answer: We will release the papers and systems that use the dataset upon acceptance.• What (other) tasks could the dataset be used for?Answer: N/A.• Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?Answer: Combing RGB and depth images with landmarks may help improve visual navigation in the future.• Are there tasks for which the dataset should not be used?Answer: No.F. Distribution• Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?Answer: No. • How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?Answer: Yes.We release our dataset at https://huggingface.co/datasets/visnavdataset/lavn.• When will the dataset be distributed?Answer: Already distributed.• Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?Answer: The dataset is distributed under MIT License.• Have any third parties imposed IP-based or other restrictions on the data associated with the instances?Answer:No. • Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?Answer:No.G. Maintenance• Who will be supporting/hosting/maintaining the dataset?Answer: The authors of this paper.• How can the owner/curator/manager of the dataset be contacted (e.g., email address)?Answer: The authors of this paper.The email address will be released upon acceptance.• Is there an erratum?Answer: No.• Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?Answer: If such update is needed, we will correct the dataset and release a new version.
Orb-slam: a versatile and accurate monocular slam system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE transactions on robotics. 3152015</p>
<p>Orb-slam2: an open-source slam system for monocular, stereo and rgb-d cameras. R Mur-Artal, J Tardos, arXiv:1610.064752016. 2017arXiv preprint</p>
<p>Orb-slam3: An accurate open-source library for visual, visualinertial, and multimap slam. C Campos, R Elvira, J J G Rodríguez, J M Montiel, J D Tardós, IEEE Transactions on Robotics. 3762021</p>
<p>Faster optimization in s-graphs exploiting hierarchy. H Bavle, J L Sanchez-Lopez, J Civera, H Voos, arXiv:2308.112422023arXiv preprint</p>
<p>H Bavle, J L Sanchez-Lopez, M Shaheer, J Civera, H Voos, arXiv:2212.11770S-graphs+: Real-time localization and mapping leveraging hierarchical representations. 2022arXiv preprint</p>
<p>Topological semantic graph memory for image-goal navigation. N Kim, O Kwon, H Yoo, Y Choi, J Park, S Oh, Conference on Robot Learning. PMLR2023</p>
<p>Metric-free exploration for topological mapping by task and motion imitation in feature space. Y He, I Fang, Y Li, R B Shah, C Feng, arXiv:2303.091922023arXiv preprint</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, 2017 IEEE international conference on robotics and automation (ICRA). IEEE2017</p>
<p>Poni: Potential functions for objectgoal navigation with interaction-free learning. S K Ramakrishnan, D S Chaplot, Z Al-Halah, J Malik, K Grauman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202218900</p>
<p>One-4-all: Neural potential fields for embodied navigation. S Morin, M Saavedra-Ruiz, L Paull, arXiv:2303.040112023arXiv preprint</p>
<p>No rl, no simulation: Learning to navigate without navigating. M Hahn, D Chaplot, S Tulsiani, M Mukadam, J Rehg, A Gupta, 2021</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, arXiv:2004.051552020arXiv preprint</p>
<p>Auxiliary tasks and exploration enable objectgoal navigation. J Ye, D Batra, A Das, E Wijmans, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Seal: Self-supervised embodied active learning using exploration and 3d consistency. D S Chaplot, M Dalal, S Gupta, J Malik, R R Salakhutdinov, Advances in neural information processing systems. 202134</p>
<p>Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D P Gandhi, A Gupta, R R Salakhutdinov, Advances in Neural Information Processing Systems. 202033</p>
<p>Waypoint models for instruction-guided navigation in continuous environments. J Krantz, A Gokaslan, D Batra, S Lee, O Maksymets, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision202115171</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, arXiv:1903.019592019arXiv preprint</p>
<p>Neural topological slam for visual navigation. D S Chaplot, R Salakhutdinov, A Gupta, S Gupta, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202012884</p>
<p>Feudal networks for visual navigation. F Johnson, B B Cao, K Dana, S Jain, A Ashok, arXiv:2402.124982024arXiv preprint</p>
<p>Memory proxy maps for visual navigation. F Johnson, B B Cao, A Ashok, S Jain, K Dana, arXiv:2411.098932024arXiv preprint</p>
<p>Exploring levels of control for a navigation assistant for blind travelers. V Ranganeni, M Sinclair, E Ofek, A Miller, J Campbell, A Kolobov, E Cutrell, Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction. the 2023 ACM/IEEE International Conference on Human-Robot Interaction2023</p>
<p>Your way or my way: improving human-robot co-navigation through robot intent and pedestrian prediction visualisations. X Yu, M Hoggenm "uller, M Tomitsch, Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction. the 2023 ACM/IEEE International Conference on Human-Robot Interaction2023</p>
<p>User-designed human-uav interaction in a social indoor environment. A Bevins, S Kunde, B A Duncan, Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction. the 2024 ACM/IEEE International Conference on Human-Robot Interaction2024</p>
<p>I need to pass through! understandable robot behavior for passing interaction in narrow environment. Y Fujioka, Y Liu, T Kanda, Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction. the 2024 ACM/IEEE International Conference on Human-Robot Interaction2024</p>
<p>(gestures vaguely): The effects of robots' use of abstract pointing gestures in large-scale environments. A Huang, A Ranucci, A Stogsdill, G Clark, K Schott, M Higger, Z Han, T Williams, Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction. the 2024 ACM/IEEE International Conference on Human-Robot Interaction2024</p>
<p>Ving: Learning open-world navigation with visual goals. D Shah, B Eysenbach, G Kahn, N Rhinehart, S Levine, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE202113222</p>
<p>Feudal networks for hierarchical reinforcement learning. A S Vezhnevets, S Osindero, T Schaul, N Heess, M Jaderberg, D Silver, K Kavukcuoglu, International Conference on Machine Learning. 2017</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, A Chang, Z Kira, V Koltun, J Malik, M Savva, D Batra, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Habitat: A Platform for Embodied AI Research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Gibson env: real-world perception for embodied agents. F Xia, A R Zamir, Z.-Y He, A Sax, J Malik, S Savarese, Computer Vision and Pattern Recognition (CVPR). IEEE2018. 2018</p>
<p>Matterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, International Conference on 3D Vision (3DV). 2017</p>
<p>Hri30: An action recognition dataset for industrial human-robot interaction. F Iodice, E De Momi, A Ajoudani, 2022 26th International Conference on Pattern Recognition (ICPR). IEEE2022</p>
<p>Socially compliant navigation dataset (scand): A large-scale dataset of demonstrations for social navigation. H Karnan, A Nair, X Xiao, G Warnell, S Pirk, A Toshev, J Hart, J Biswas, P Stone, IEEE Robotics and Automation Letters. 742022</p>
<p>Th "or: Human-robot navigation data collection and accurate motion trajectories dataset. A Rudenko, T P Kucner, C S Swaminathan, R T Chadalavada, K O Arras, A J , IEEE Robotics and Automation Letters. 522020</p>
<p>A Day, I Karamouzas, arXiv:2307.08668A study in zucker: Insights on human-robot interactions. 2023arXiv preprint</p>
<p>Handmethat: Human-robot communication in physical and social environments. Y Wan, J Mao, J Tenenbaum, Advances in Neural Information Processing Systems. 202235</p>
<p>Modelling social interaction between humans and service robots in large public spaces. B Anvari, H A Wurdemann, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>The magni human motion dataset: Accurate, complex, multi-modal, natural, semantically-rich and contextualized. T Schreiter, T R De Almeida, Y Zhu, E G Maestro, L Morillo-Mendez, A Rudenko, T P Kucner, O M Mozos, M Magnusson, L Palmieri, arXiv:2208.149252022arXiv preprint</p>
<p>Learning social affordance for human-robot interaction. T Shu, M S Ryoo, S.-C Zhu, arXiv:1604.036922016arXiv preprint</p>
<p>Human mobile robot interaction in the retail environment. Y Chen, Y Luo, C Yang, M O Yerebakan, S Hao, N Grimaldi, S Li, R Hayes, B Hu, Scientific Data. 916732022</p>
<p>Gnm: A general navigation model to drive any robot. D Shah, A Sridhar, A Bhorkar, N Hirose, S Levine, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>A rugd dataset for autonomous navigation and visual perception in unstructured outdoor environments. in 2019 ieee. M Wigness, S Eum, J G Rogers, D Han, H Kwon, RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Vision-anddialog navigation. J Thomason, M Murray, M Cakmak, L Zettlemoyer, Conference on Robot Learning. PMLR2020</p>
<p>Diffframenet: A deep learning method for intuitive robot navigation. B Cao, 2018University of Colorado at BoulderPh.D. dissertation</p>
<p>Natsgd: A dataset with speech, gestures, and demonstrations for robot learning in natural human-robot interaction. S Shrestha, Y Zha, G Gao, C Fermuller, Y Aloimonos, 2023</p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>From cognitive maps to cognitive graphs. E R Chrastil, W H Warren, PloS one. 911e1125442014</p>
<p>Stereo magnification: Learning view synthesis using multiplane images. T Zhou, R Tucker, J Flynn, G Fyffe, N Snavely, arXiv:1805.098172018arXiv preprint</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>iphone. </p>
<p>. Mini, 2021</p>
<p>Is long horizon rl more difficult than short horizon rl?. R Wang, S S Du, L Yang, S Kakade, Advances in Neural Information Processing Systems. 202033</p>
<p>Learning composable behavior embeddings for long-horizon visual navigation. X Meng, Y Xiang, D Fox, IEEE Robotics and Automation Letters. 622021</p>
<p>Long-horizon visual planning with goal-conditioned hierarchical predictors. K Pertsch, O Rybkin, F Ebert, S Zhou, D Jayaraman, C Finn, S Levine, Advances in Neural Information Processing Systems. 202033333</p>
<p>Scene memory transformer for embodied agents in long-horizon tasks. K Fang, A Toshev, L Fei-Fei, S Savarese, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>            </div>
        </div>

    </div>
</body>
</html>
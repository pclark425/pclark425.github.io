<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1562 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1562</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1562</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-c52dddfbb4a3af4cc5e72849fe965c62801539e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c52dddfbb4a3af4cc5e72849fe965c62801539e7" target="_blank">Counting to Explore and Generalize in Text-based Games</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments and observes that the agent learns policies that generalize to unseen games of greater difficulty.</p>
                <p><strong>Paper Abstract:</strong> We propose a recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments. We show promising results on a set of generated text-based games of varying difficulty where the goal is to collect a coin located at the end of a chain of rooms. In contrast to previous text-based RL approaches, we observe that our agent learns policies that generalize to unseen games of greater difficulty.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1562.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1562.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Difficulty-distributed training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training on distributions of generated text-based games with varying difficulty levels and counts (no explicit staged curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper trains RL agents (DQN / DRQN variants) on sets of procedurally generated TextWorld 'coin-collector' games that vary in difficulty (easy/medium/hard) and level (L5..L30) and in number of distinct training games, and evaluates zero-shot generalization to unseen, harder games; no explicit curriculum schedule is used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN / DRQN and their variants with cumulative (MODEL+) and episodic (MODEL++) count-based bonuses (referred to as DQN++, DRQN++)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline LSTM-DQN (representation generator Φ_R with embedding + encoder LSTM + mean-pool; action scorer Φ_A with two MLPs predicting verb and object Q-values) and an LSTM-DRQN variant that replaces the shared MLP in Φ_A with an LSTM cell for recurrence; agents use ε-greedy DQN training with prioritized replay and intrinsic discovery bonuses (cumulative and episodic) added to environmental reward during training.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Coin Collector chain games / generated chain experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated deterministic text-based games (TextWorld) where the agent perceives textual room descriptions and issues two-word commands (verb, object) from a small action set (go north/east/south/west, take coin). Games are chains of rooms with a coin at the chain end; modes add distractor (dead-end) rooms. Interactions are textual observations and discrete text commands.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures (navigation / exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>navigate along a chain of rooms to collect a coin; avoid reversing direction on the optimal path; enter and reverse out of distractor dead-ends; follow a consistent exit-order strategy (e.g., anti-clockwise) to traverse rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are composed of sequences of primitive verb-object actions (two-word commands). Complexity arises from longer action sequences (longer chains), branching due to distractor rooms, and the requirement to remember previous actions/observations; no explicit hierarchical decomposition or subtasks beyond these sequential primitives is introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Levels L5 to L30 (denoted L5, L10, L15, L20, L25, L30); modes: easy (mode 0, no distractors), medium (mode 1, one distractor per optimal-step), hard (mode 2, two distractors per optimal-step). Training-set sizes vary: single game or sets of {2,5,10,30,50,100,500} L10 games in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative experimental results: episodic-count discovery bonus combined with recurrence (DRQN++) substantially improves learning and generalization compared with cumulative counting bonus or non-recurrent models. When pre-trained on large training sets (e.g., 500 L10 easy games), both DQN++ and DRQN++ nearly perfectly solved unseen L30 easy games. For hard-mode zero-shot tests, DQN++ and DRQN++H (history-concatenated variant) generalized better, with DQN++ performing near-perfectly on unseen hard games when trained on 500 L10 games. Cumulative bonus (MODEL+) helps less than episodic bonus (MODEL++), and recurrence is crucial for harder modes. Exact numeric metrics (e.g., average reward values or exact success rates) are presented in figures but not tabulated as exact numbers in text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Yes — zero-shot generalization was evaluated: models trained on many L10 games were tested on unseen games of levels L5, L10, L15, L20, L30. Large training-set size and episodic discovery bonus increased generalization: training on 500 L10 easy games produced near-perfect success on unseen L30 easy games; for hard-mode maps, DQN++ generalized near-perfectly to unseen hard games when trained on 500 L10 hard-mode games (after validation-based selection to avoid overfitting). Some models showed overfitting on hard mode (test reward decreased while training reward increased), so validation-based early selection was necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) An episodic count-based discovery bonus (resetting per-episode) combined with a recurrent action scorer (LSTM-DRQN) substantially improves both learning and zero-shot generalization across unseen, harder text-based navigation games compared to cumulative-count bonuses or non-recurrent models. 2) Training on larger numbers of distinct games (up to 500 L10 games) yields much better generalization to longer/unseen levels; with sufficient diversity the agents learn general strategies (e.g., exploring exits in an anti-clockwise order) that transfer to unseen maps. 3) Overfitting occurs on hard-mode games without validation-based model selection; concatenating a short history (previous 4 observations) can help some variants. 4) No explicit staged curriculum (e.g., increasing difficulty schedule) was used — experiments vary difficulty and dataset size but do not implement a curriculum learning protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counting to Explore and Generalize in Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Deep exploration via bootstrapped dqn <em>(Rating: 1)</em></li>
                <li>Count-based exploration with neural density models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1562",
    "paper_id": "paper-c52dddfbb4a3af4cc5e72849fe965c62801539e7",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Difficulty-distributed training",
            "name_full": "Training on distributions of generated text-based games with varying difficulty levels and counts (no explicit staged curriculum)",
            "brief_description": "The paper trains RL agents (DQN / DRQN variants) on sets of procedurally generated TextWorld 'coin-collector' games that vary in difficulty (easy/medium/hard) and level (L5..L30) and in number of distinct training games, and evaluates zero-shot generalization to unseen, harder games; no explicit curriculum schedule is used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DQN / DRQN and their variants with cumulative (MODEL+) and episodic (MODEL++) count-based bonuses (referred to as DQN++, DRQN++)",
            "agent_description": "Baseline LSTM-DQN (representation generator Φ_R with embedding + encoder LSTM + mean-pool; action scorer Φ_A with two MLPs predicting verb and object Q-values) and an LSTM-DRQN variant that replaces the shared MLP in Φ_A with an LSTM cell for recurrence; agents use ε-greedy DQN training with prioritized replay and intrinsic discovery bonuses (cumulative and episodic) added to environmental reward during training.",
            "agent_size": null,
            "environment_name": "TextWorld (Coin Collector chain games / generated chain experiment)",
            "environment_description": "Procedurally generated deterministic text-based games (TextWorld) where the agent perceives textual room descriptions and issues two-word commands (verb, object) from a small action set (go north/east/south/west, take coin). Games are chains of rooms with a coin at the chain end; modes add distractor (dead-end) rooms. Interactions are textual observations and discrete text commands.",
            "procedure_type": "commonsense procedures (navigation / exploration)",
            "procedure_examples": "navigate along a chain of rooms to collect a coin; avoid reversing direction on the optimal path; enter and reverse out of distractor dead-ends; follow a consistent exit-order strategy (e.g., anti-clockwise) to traverse rooms.",
            "compositional_structure": "Tasks are composed of sequences of primitive verb-object actions (two-word commands). Complexity arises from longer action sequences (longer chains), branching due to distractor rooms, and the requirement to remember previous actions/observations; no explicit hierarchical decomposition or subtasks beyond these sequential primitives is introduced.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Levels L5 to L30 (denoted L5, L10, L15, L20, L25, L30); modes: easy (mode 0, no distractors), medium (mode 1, one distractor per optimal-step), hard (mode 2, two distractors per optimal-step). Training-set sizes vary: single game or sets of {2,5,10,30,50,100,500} L10 games in experiments.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Qualitative experimental results: episodic-count discovery bonus combined with recurrence (DRQN++) substantially improves learning and generalization compared with cumulative counting bonus or non-recurrent models. When pre-trained on large training sets (e.g., 500 L10 easy games), both DQN++ and DRQN++ nearly perfectly solved unseen L30 easy games. For hard-mode zero-shot tests, DQN++ and DRQN++H (history-concatenated variant) generalized better, with DQN++ performing near-perfectly on unseen hard games when trained on 500 L10 games. Cumulative bonus (MODEL+) helps less than episodic bonus (MODEL++), and recurrence is crucial for harder modes. Exact numeric metrics (e.g., average reward values or exact success rates) are presented in figures but not tabulated as exact numbers in text.",
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Yes — zero-shot generalization was evaluated: models trained on many L10 games were tested on unseen games of levels L5, L10, L15, L20, L30. Large training-set size and episodic discovery bonus increased generalization: training on 500 L10 easy games produced near-perfect success on unseen L30 easy games; for hard-mode maps, DQN++ generalized near-perfectly to unseen hard games when trained on 500 L10 hard-mode games (after validation-based selection to avoid overfitting). Some models showed overfitting on hard mode (test reward decreased while training reward increased), so validation-based early selection was necessary.",
            "key_findings": "1) An episodic count-based discovery bonus (resetting per-episode) combined with a recurrent action scorer (LSTM-DRQN) substantially improves both learning and zero-shot generalization across unseen, harder text-based navigation games compared to cumulative-count bonuses or non-recurrent models. 2) Training on larger numbers of distinct games (up to 500 L10 games) yields much better generalization to longer/unseen levels; with sufficient diversity the agents learn general strategies (e.g., exploring exits in an anti-clockwise order) that transfer to unseen maps. 3) Overfitting occurs on hard-mode games without validation-based model selection; concatenating a short history (previous 4 observations) can help some variants. 4) No explicit staged curriculum (e.g., increasing difficulty schedule) was used — experiments vary difficulty and dataset size but do not implement a curriculum learning protocol.",
            "uuid": "e1562.0",
            "source_info": {
                "paper_title": "Counting to Explore and Generalize in Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Deep exploration via bootstrapped dqn",
            "rating": 1
        },
        {
            "paper_title": "Count-based exploration with neural density models",
            "rating": 1
        }
    ],
    "cost": 0.00784175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Counting to Explore and Generalize in Text-based Games</h1>
<p>Xingdi Yuan ${ }^{<em> 1}$ Marc-Alexandre Côté ${ }^{</em> 1}$ Alessandro Sordoni ${ }^{1}$ Romain Laroche ${ }^{1}$ Remi Tachet des Combes ${ }^{1}$<br>Matthew Hausknecht ${ }^{1}$ Adam Trischler ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We propose a recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments. We show promising results on a set of generated text-based games of varying difficulty where the goal is to collect a coin located at the end of a chain of rooms. In contrast to previous text-based RL approaches, we observe that our agent learns policies that generalize to unseen games of greater difficulty.</p>
<h2>1. Introduction</h2>
<p>Text-based games like Zork (Infocom, 1980) are complex, interactive simulations. They use natural language to describe the state of the world, to accept actions from the player, and to report subsequent changes in the environment. The player works toward goals which are seldom specified explicitly and must be discovered through exploration. The observation and action spaces in text games are both combinatorial and compositional, and players must contend with partial observability, since descriptive text does not communicate complete, unambiguous information about the underlying game state.</p>
<p>In this paper, we study several methods of exploration in text-based games. Our basic task is a deterministic textbased version of the chain experiment (Osband et al., 2016; Plappert et al., 2017) with distractor nodes that are off-chain: the agent must navigate a path composed of discrete locations (rooms) to the goal, ideally without revisiting dead ends. We propose a DQN-based recurrent model for solving text-based games, where the recurrence gives the model the capacity to condition its policy on historical state information. To encourage exploration, we extend count-based exploration approaches (Ostrovski et al., 2017; Tang et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2017), which assign an intrinsic reward derived from the count of state visitations during learning, across episodes. Specifically, we propose an episodic count-based exploration scheme, where state counts are reset at the beginning of each episode. This reward plays the role of an episodic memory (Gershman \&amp; Daw, 2017) that pushes the agent to visit states not previously encountered within an episode. Although the recurrent policy architecture has the capacity to solve the task by remembering and avoiding previously visited locations, we hypothesize that exploration rewards will help the agent learn to utilize its memory.</p>
<p>We generate a set of games of varying difficulty (measured with respect to the path length and the number of off-chain rooms) with a text-based game generator (Côté et al., 2018). We observe that, in contrast to a baseline model and standard count-based exploration methods, the recurrent model with episodic bonus learns policies that not only complete multiple training games at same time successfully but also generalize to unseen games of greater difficulty.</p>
<h2>2. Text-based Games as POMDPs</h2>
<p>Text-based games are sequential decision-making problems that can be described naturally by the Reinforcement Learning (RL) setting. Fundamentally, text-based games are partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998) where the environment state is never observed directly. To act optimally, an agent must keep track of all observations. Formally, a text-based game is a discrete-time POMDP defined by $(S, T, A, \Omega, O, R, \gamma)$, where $\gamma \in[0,1]$ is the discount factor.</p>
<p>Environment States $(S)$ : The environment state at turn $t$ in the game is $s_{t} \in S$. It contains the complete internal information of the game, much of which is hidden from the agent. When an agent issues a command $\boldsymbol{c}<em t_1="t+1">{t}$ (defined next), the environment transitions to state $s</em>}$ with probability $T\left(s_{t+1} \mid s_{t}, \boldsymbol{c<em t="t">{t}\right)$.
Actions $(A)$ : At each turn $t$, the agent issues a text command $\boldsymbol{c}</em>$. The interpreter can accept any sequence of characters but will only recognize a tiny subset thereof. Furthermore, only a fraction of recognized commands will actually change the state of the world. The resulting action space</p>
<p>is enormous and intractable for existing RL algorithms. In this work, we make the following two simplifying assumptions. (1) Word-level Each command is a two-word sequence where the words are taken from a fixed vocabulary $V$. (2) Command syntax Each command is a (verb, object) pair (direction words are considered objects).</p>
<p>Observations ( $\Omega$ ): The text information perceived by the agent at a given turn $t$ in the game is the agent's observation, $o_{t} \in \Omega$, which depends on the environment state and the previous command with probability $O\left(o_{t} \mid s_{t}, \boldsymbol{c}_{t-1}\right)$. Thus, the function $O$ selects from the environment state what information to show to the agent given the last command.</p>
<p>Reward Function $(R)$ : Based on its actions, the agent receives reward signals $r_{t}=R\left(s_{t}, a_{t}\right)$. The goal is to maximize the expected discounted sum of rewards $E\left[\sum_{t} \gamma^{t} r_{t}\right]$.</p>
<h2>3. Method</h2>
<h3>3.1. Model Architecture</h3>
<p>In this work, we adopt the LSTM-DQN (Narasimhan et al., 2015) model as baseline. It has two modules: a representation generator $\Phi_{R}$, and an action scorer $\Phi_{A} . \Phi_{R}$ takes observation strings $o$ as input, after a stacked embedding layer and LSTM (Hochreiter \&amp; Schmidhuber, 1997) encoder, a mean-pooling layer produces a vector representation of the observation. This feeds into $\Phi_{A}$, in which two MLPs, sharing a lower layer, predict the Q-values over all verbs $w_{v}$ and object words $w_{o}$ independently. The average of the two resulting scores gives the Q-values for the composed actions. The LSTM-DQN does not condition on previous actions or observations, so it cannot deal with partial observability. We concatenate the previous command $\boldsymbol{c}<em t="t">{t-1}$ to the current observation $o</em>$ to lessen this limitation.</p>
<p>To further enhance the agent's capacity to remember previous states, we replace the shared MLP in $\Phi_{A}$ by an LSTM cell. This model is inspired by (Hausknecht \&amp; Stone, 2015; Lample \&amp; Chaplot, 2016) and we call it LSTM-DRQN. The LSTM cell in $\Phi_{A}$ takes the representation generated by $\Phi_{R}$ together with history information $h_{t-1}$ from the previous game step as input. It generates the state information at the current game step, which is then fed into the two MLPs as well as passed forward to next game step. Figure 1 shows the LSTM-DRQN architecture.</p>
<h3>3.2. Discovery Bonus</h3>
<p>To promote exploration we use an intrinsic reward by counting state visits (Kolter \&amp; Ng, 2009; Tang et al., 2017; Martin et al., 2017; Ostrovski et al., 2017). We investigate two approaches to counting rewards. The first is inspired by (Kolter $\&amp; \mathrm{Ng}, 2009$ ), where we define the cumulative counting bonus as $r^{+}\left(o_{t}\right)=\beta \cdot n\left(o_{t}\right)^{-1 / 3}$, where $n\left(o_{t}\right)$ is the num-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. LSTM-DRQN processes textual observations word-byword to generate a fixed-length vector representation. This representation is used by the recurrent policy to estimate Q-values for all verbs $Q(s, v)$ and objects $Q(s, o)$.
ber of times the agent has observed $o_{t}$ since the beginning of training (across episodes), and $\beta$ is the bonus coefficient. During training, as the agent observes new states more and more, the cumulative counting bonus gradually converges to 0 .</p>
<p>The second approach is the episodic discovery bonus, which encourages the agent to discover unseen states by assigning a positive reward whenever it sees a new state. It is defined as: $r^{++}\left(o_{t}\right)=\left{\begin{array}{ll}\beta &amp; \text { if } n\left(o_{t}\right)=1 \ 0.0 &amp; \text { otherwise }\end{array}\right.$, where $n(\cdot)$ is reset to zero at the beginning of each episode. Taking inspiration from (Gershman \&amp; Daw, 2017), we hope this behavior pushes the agent to visit states not previously encountered in the current episode and teaches the agent how to use its memory for this purpose so it may generalize to unseen environments.</p>
<h2>4. Related Work</h2>
<p>RL Applied to Text-based Games: Narasimhan et al. (2015) test their LSTM-DQN in two text-based environments: Home World and Fantasy World. They report the quest completion ratio over multiple runs but not how many steps it takes to complete them. He et al. (2015) introduce the Deep Reinforcement Relevance Network (DRRN) for tackling choice-based (as opposed to parser-based) text games, evaluating the DRRN on one deterministic game and one larger-scale stochastic game. The DRRN model converges on both games; however, this model must know in advance the valid commands at each state. Fulda et al. (2017) propose a method to reduce the action space for parserbased games by training word embeddings to be aware of verb-noun affordances. One drawback of this approach is it requires pre-trained embeddings.</p>
<p>Count-based Exploration: The Model Based Interval Estimation-Exploration Bonus (MBIE-EB) (Strehl \&amp; Littman, 2008) derives an intrinsic reward by counting stateaction pairs with a table $n(s, a)$. Their exploration bonus has the form $\beta / \sqrt{n(s, a)}$ to encourage exploring less-visited pairs. In this work, we use $n(s)$ rather than $n(s, a)$, since the majority of actions leave the agent in the same state</p>
<p>(i.e., unrecognized commands). Using the latter would reward the agent for trying invalid commands, which is not sensible in our setting.</p>
<p>Tang et al. (2017) propose a hashing function for countbased exploration in order to discretize high-dimensional, continuous state spaces. Their exploration bonus $r^{+}=$ $\beta / \sqrt{n(\phi(s))}$, where $\phi(\cdot)$ is a hashing function that can either be static or learned. This is similar to the cumulative counting bonus defined above.</p>
<p>Deep Recurrent Q-Learning: Hausknecht \&amp; Stone (2015) propose the Deep Recurrent Q-Networks (DRQN), adding a recurrent neural network (such as an LSTM (Hochreiter \&amp; Schmidhuber, 1997)) on top of the standard DQN model. DRQN estimates $Q\left(o_{t}, h_{t-1}, a_{t}\right)$ instead of $Q\left(o_{t}, a_{t}\right)$, so it has the capacity to memorize the state history. Lample \&amp; Chaplot (2016) use a model built on the DRQN architecture to learn to play FPS games.</p>
<p>A major difference between the work presented in this paper and the related work is that we test on unseen games and train on a set of similar (but not identical) games rather than training and testing on the same game.</p>
<h2>5. Experiments</h2>
<h3>5.1. Coin Collector Game Setup</h3>
<p>To evaluate the two models described above and the proposed discovery bonus, we designed a set of simple textbased games inspired by the chain experiment (Osband et al., 2016; Plappert et al., 2017). Each game contains a given number of rooms that are randomly connected to each other to form a chain (see figures in Appendix C). The goal is to find and collect a "coin" placed in one of the rooms. The player's initial position is at one end of the chain and the coin is at the other. These games have deterministic state transitions.</p>
<p>Games stop after a set number of steps or after the player has collected the coin. The game interpreter understands only five commands (go north, go east, go south, go west and take coin), while the action space is twice as large: ${\mathrm{go}$, take $} \times{$ north, south, east, west, coin $}$. See Figure 12, Appendix C for an example of what the agent observes in-game.</p>
<p>Our games have 3 modes: easy (mode 0 ), there are no distractor rooms (dead ends) along the path; medium (mode 1), each room along the optimal trajectory has one distractor room randomly connected to it; hard (mode 2), each room on the path has two distractor rooms, i.e., within a room on the optimal trajectory, all 4 directions lead to a connected room. We use difficulty levels to indicate the optimal trajectory's length of a game.</p>
<p>To solve easy games, the agent must learn to recall its previous directional action and to issue the command that does not reverse it (e.g., if the agent entered the current room by going east, do not now go west). Conversely, to solve medium and hard games, the agent must reverse its previous action when it enters distractor rooms to return to the chain, and also recall farther into the past to track which exits it has already passed through. Alternatively, since there are no cycles, it can learn a less memory intensive "wall-following" strategy by, e.g., taking exits in a clockwise order from where it enters a room.</p>
<p>We refer to models with the cumulative counting bonus as MODEL+, and models with episodic discovery bonus as MODEL++, where MODEL $\in{\mathrm{DQN}, \mathrm{DRQN}}^{1}$ (implementation details in Appendix A). In this section we cover part of the experiment results, the full extent of our experiment results are provided in Appendix B.</p>
<h3>5.2. Solving Training Games</h3>
<p>We first investigate whether the variant models can learn to solve single games with different difficulty modes (easy, medium, hard) and levels ${L 5, L 10, L 15, L 20, L 25, L 30}^{2}$. As shown in Figure 2 (top row), when the games are simple, vanilla DQN and DRQN already fail to learn. Adding the cumulative bonus helps somewhat and models perform similarly with and without recurrence. When the games become harder, the cumulative bonus helps less, while episodic bonus remains very helpful and recurrence in the model becomes very helpful.</p>
<p>Next, we are interested to see whether models can learn to solve a distribution of games. Note that each game has its own counting memory, i.e., the states visited in one game do not affect the counters for other games. Here, we fix the game difficulty level to 10 , and randomly generate training sets that contain ${2,5,10,30,50,100}$ games in each mode. As shown in Figure 2 (bottom row), when the game mode becomes harder, the episodic bonus has an advantage over the cumulative bonus, and recurrence becomes more crucial for memorizing the game distribution. It is also clear that the episodic bonus and recurrence help significantly when more training games are provided.</p>
<h3>5.3. Zero-shot Evaluation</h3>
<p>Finally, we want to see if a pre-trained model can generalize to unseen games. The generated training set contains ${1,2,5,10,30,50,100,500} \mathrm{L} 10$ games for each mode. Then, for each corresponding mode the test set contains 10 unseen ${L 5, L 10, L 15, L 20, L 30}$ games. There is no</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Model performance on single games (top row) and multiple games (bottom row).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Zero-shot evaluation: Average rewards of DQN++ (left) and DRQN++ (right) as a function of the number of games in the training set.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Average rewards and steps used corresponding to best validation performance in hard games.
overlap between training and test games in either text descriptions or optimal trajectories. At test time, the counting modules are disabled, the agent is not updated, and its generates verb and noun actions based on the $\operatorname{argmax}$ of their Q-values.</p>
<p>As shown in Figure 3, when the game mode is easy, both models with and without recurrence can generalize well on unseen games by training on a large training set. It is worth noting that by training on 500 L10 easy games, both models can almost perfectly solve level 30 unseen easy games. We also observe that models with recurrence are able to generalize better when trained on fewer games.</p>
<p>When testing on hard mode games, we observe that both models suffer from overfitting (after a certain number of episodes, average test reward starts to decrease while training reward increases). Therefore, we further generated a validation set that contains 10 L10 hard games, and report test results corresponding to best validation performance. In
addition, we investigated what happens when concatenating the previous 4 steps' history observation into the input. In Figure 4, we add $H$ to model names to indicate this variant.</p>
<p>As shown in Figure 4, all models can memorize the 500 training games, while DQN++ and DRQN++H are able to generalize better on unseen games. In particular, the former performs near perfectly on test games. To investigate this, we looked into all the bi-grams of generated commands (i.e., two commands from adjacent game steps) from DQN++ model. Surprisingly, except for moving back from dead end rooms, the agent always explores exits in anti-clockwise order. This means the agent has learned a general strategy that does not require history information beyond the previous command. This strategy generalizes perfectly to all possible hard games because there are no cycles in the maps.</p>
<h2>6. Final Remarks</h2>
<p>We propose an RL model with a recurrent component, together with an episodic count-based exploration scheme that promotes the agent's discovery of the game environment. We show promising results on a set of generated text-based games of varying difficulty. In contrast to baselines, our approach learns policies that generalize to unseen games of greater difficulty.</p>
<p>In future work, we plan to experiment on games with more complex topology, such as cycles (where the "wallfollowing" strategy will not work). We would like to explore games that require multi-word commands (e.g., unlock red door with red key), necessitating a model that generates sequences of words. Other interesting directions include agents that learn to map or to deal with stochastic transitions in text-based games.</p>
<h2>References</h2>
<p>Côté, Marc-Alexandre, Kádár, Ákos, Yuan, Xingdi, Kybartas, Ben, Barnes, Tavian, Fine, Emery, Moore, James, Hausknecht, Matthew, Asri, Layla El, Adada, Mahmoud, Tay, Wendy, and Trischler, Adam. Textworld: A learning environment for text-based games. Computer Games Workshop at IJCAI 2018, Stockholm, 2018.</p>
<p>Fulda, Nancy, Ricks, Daniel, Murdoch, Ben, and Wingate, David. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429, 2017.</p>
<p>Gershman, Samuel J and Daw, Nathaniel D. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annual review of psychology, 68:101-128, 2017.</p>
<p>Hausknecht, Matthew J. and Stone, Peter. Deep recurrent q-learning for partially observable mdps. CoRR, abs/1507.06527, 2015. URL http://arxiv.org/ abs/1507.06527.</p>
<p>He, Ji, Chen, Jianshu, He, Xiaodong, Gao, Jianfeng, Li, Lihong, Deng, Li, and Ostendorf, Mari. Deep reinforcement learning with a natural language action space. arXiv preprint arXiv:1511.04636, 2015.</p>
<p>Hochreiter, Sepp and Schmidhuber, Jürgen. Long shortterm memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997. 9.8.1735. URL http://dx.doi.org/10.1162/ neco.1997.9.8.1735.</p>
<p>Infocom. Zork I, 1980. URL http://ifdb.tads. org/viewgame?id=0dbnusxunq7fw5ro.</p>
<p>Kaelbling, Leslie Pack, Littman, Michael L, and Cassandra, Anthony R. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99134, 1998.</p>
<p>Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Kolter, J Zico and Ng, Andrew Y. Near-bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 513520. ACM, 2009.</p>
<p>Lample, Guillaume and Chaplot, Devendra Singh. Playing FPS games with deep reinforcement learning. CoRR, abs/1609.05521, 2016. URL http://arxiv.org/ abs/1609.05521.</p>
<p>Martin, Jarryd, Sasikumar, Suraj Narayanan, Everitt, Tom, and Hutter, Marcus. Count-based exploration in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017.</p>
<p>Narasimhan, Karthik, Kulkarni, Tejas, and Barzilay, Regina. Language understanding for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.</p>
<p>Osband, Ian, Blundell, Charles, Pritzel, Alexander, and Van Roy, Benjamin. Deep exploration via bootstrapped dqn. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 4026-4034. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/ 6501-deep-exploration-via-bootstrapped-dqn. pdf.</p>
<p>Ostrovski, Georg, Bellemare, Marc G, Oord, Aaron van den, and Munos, Rémi. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.</p>
<p>Paszke, Adam, Gross, Sam, Chintala, Soumith, Chanan, Gregory, Yang, Edward, DeVito, Zachary, Lin, Zeming, Desmaison, Alban, Antiga, Luca, and Lerer, Adam. Automatic differentiation in pytorch. In NIPS-W, 2017.</p>
<p>Plappert, Matthias, Houthooft, Rein, Dhariwal, Prafulla, Sidor, Szymon, Chen, Richard Y, Chen, Xi, Asfour, Tamim, Abbeel, Pieter, and Andrychowicz, Marcin. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.</p>
<p>Strehl, Alexander L and Littman, Michael L. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74 (8):1309-1331, 2008.</p>
<p>Tang, Haoran, Houthooft, Rein, Foote, Davis, Stooke, Adam, Chen, Xi, Duan, Yan, Schulman, John, DeTurck, Filip, and Abbeel, Pieter. # exploration: A study of countbased exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2750-2759, 2017.</p>
<h1>A. Implementation Details</h1>
<p>Implementation details of our neural baseline agent are as follows ${ }^{3}$. In all experiments, the word embeddings are initialized with 20-dimensional random matrices; the number of hidden units of the encoder LSTM is 100. In the nonrecurrent action scorer we use a 1-layer MLP which has 64 hidden units, with $\operatorname{ReLU}$ as non-linear activation function, in the recurrent action scorer, we use an LSTM cell which hidden size is 64 .</p>
<p>In replay memory, we used a memory with capacity of 500000 , a mini-batch gradient update is performed every 4 steps in the gameplay, the mini-batch size is 32 . We apply prioritized sampling in all experiments, in which, we used $\rho=0.25$. In LSTM-DQN and LSTM-DRQN model, we used discount factor $\gamma=0.9$, in all models with discovery bonus, we used $\gamma=0.5$.</p>
<p>When updating models with recurrent components, we follow the update strategy in (Lample \&amp; Chaplot, 2016), i.e., we randomly sample sequences of length 8 from the replay memory, zero initialize hidden state and cell state, use the first 4 states to bootstrap a reliable hidden state and cell state, and then update on rest of the sequence.</p>
<p>We anneal the $\epsilon$ for $\epsilon$-greedy from 1 to 0.2 over 1000 epochs, it remains at 0.2 afterwards. In both cumulative and episodic discovery bonus, we use coefficient $\beta$ of 1.0.</p>
<p>When zero-shot evaluating hard games, we use max_train_step $=100$, in all other experiments we use max_train_step $=50$; during test, we always use max_test_step $=200$.</p>
<p>We use adam (Kingma \&amp; Ba, 2014) as the step rule for optimization. The learning rate is $1 e^{-3}$. The model is implemented using PyTorch (Paszke et al., 2017).</p>
<p>All games are generated using TextWorld framework (Côté et al., 2018), we used the house grammar.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>B. More Results</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Model performance on single games.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Model performance on multiple games.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Model performance on unseen easy test games when pre-trained on easy games.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Model performance on unseen medium test games when pre-trained on medium games.</p>
<h1>C. Text-based Chain Experiment</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Examples of the games used in the experiments: level 10, easy
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Examples of the games used in the experiments: level 10, medium</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Examples of the games used in the experiments: level 10, hard</p>
<p>I hope your ready to go into rooms and interact with objects, because you've just entered TextWorld! take the coin that's in the launderette.
-= Cookhouse =-
Fancy seeing you here. Here, by the way, being the cookhouse.</p>
<p>There is an unguarded exit to the south.
$&gt;$ go south
-= Study =-
You've entered a study. The room seems oddly familiar, as though it were only superficially different from the other rooms in the building.</p>
<p>You need an unguarded exit? You should try going west. You need an unguarded exit? You should try going north.
$&gt;$</p>
<p>Figure 12. Text the agent gets to observe for one of the level 10 easy games.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Our implementation is publicly available at https://github.com/xingdi-eric-yuan/ TextWorld-Coin-Collector.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
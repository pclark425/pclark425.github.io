<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1363 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1363</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1363</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-529ec0c00c46f549335ada62cebb4f5dba11be56</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/529ec0c00c46f549335ada62cebb4f5dba11be56" target="_blank">Complex Structures and Semantics in Free Word Association</a></p>
                <p><strong>Paper Venue:</strong> Advances in Complex Systems</p>
                <p><strong>Paper TL;DR:</strong> This work analyzes in details two large datasets resulting from two very different experiments: on the one hand the massive multiplayer web-based Word Association Game known as Human Brain Cloud, and on the other hand the South Florida Free Association Norms experiment.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1363.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1363.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Brain Cloud (Word Association Game)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale, web-based free word association dataset represented as a directed, weighted word-association graph built from human players' cue→target associations; used as a proxy for a cognitive/semantic navigation space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Human Brain Cloud word-association graph</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A semantic/conceptual space constructed from a massively multiplayer web-based free-association game (cue→target). Domain: cognitive/linguistic word-association space (not a physical text game, but treated as a navigable graph of concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Filtered dataset yields a strongly connected directed weighted graph; small-world property reported (average shortest-path distance ~4); slight weighted assortativity r_w = 0.1; in-degree distribution heavy-tailed (power-law), out-degree peaked (Gamma-like).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>88,747 nodes; 3,013,125 directed edges (6,097,806 registered associations before filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Human players (free-association) / random-walk modeling (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Human participants produce target words in response to cues (empirical, behavioral 'agent'); the paper also references a random-walk model of collective exploration (memoryless local transitions) used in prior work to model annotation/association sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Average shortest-path distance (mean node distance) and vocabulary-growth/coverage metrics (Heaps'-law-like growth k_out(s_out)); frequency-rank (Zipf) behavior of target lists.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Average node distance ≈ 4 (so most nodes reachable in ~5 steps); measured k_out(s_out) follows limited Heaps' law (Eq. 2) with fitted parameters B ≈ 0.92 and V ≈ 74 describing asymptotic vocabulary size per cue.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Empirically, human associations behave like local associative policies (short semantic hops, preferential patterns); prior modeling suggests simple random-walk-like policies reproduce observed aggregate behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Small-world (mean distance ≈ 4) implies fast, efficient exploration/reachability; peaked out-degree (typical k_out ≈ 28, asymptotic V ≈ 74) imposes a bounded local semantic context that limits branching and shapes coverage growth (k_out(s_out) sublinear, limited Heaps' law). Heavy-tailed in-degree (power-law exponent β ≈ 1.76) creates hubs (words reachable from many cues) which affect navigability and semantic retrieval asymmetrically (many cues map to few popular targets). Slight assortativity (r_w = 0.1) reported but its direct impact on exploration is not quantified beyond being noted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Human-generated association sequences preferentially follow short semantic steps and hierarchical patterns (e.g., 'brother' pattern: up one hypernym then down to sibling hyponym). For 2-step and 3-step WN mappings, 'brother' sequences occur ~31% and ~51% respectively, indicating policies that maintain level-of-specificity rather than long random wandering. Causal relations are overrepresented relative to WordNet frequencies, indicating humans preferentially navigate certain semantic-edge types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Structures and Semantics in Free Word Association', 'publication_date_yy_mm': '2012-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1363.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1363.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WordNet (lexical semantic graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large lexical database of English where words (lemmas) are nodes and many typed semantic relations (synonymy, hypernymy, meronymy, causal, etc.) form directed edges; used as the underlying semantic graph to map HBC associations into semantic paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Wordnet: A lexical database for english.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>WordNet semantic graph</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Lexical/semantic network (lemmas as nodes, typed semantic relations as directed edges) representing general semantic relations in English; used here as an underlying graph for mapping free-association links to semantic paths.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Directed graph of semantic relations; paper reports node and edge counts but does not report global connectivity statistics (they compute shortest-paths between mapped word pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>143,963 nodes; 1,345,801 edges (as reported for their WN snapshot).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Shortest-path mapping procedure (analysis algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mapping method: for each HBC association (word pair) compute shortest path(s) in WN between their lemmas; if multiple shortest paths exist, treat them as equiprobable and split weight accordingly; analyze 1-, 2-, and 3-step path compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Path length (number of semantic steps) between cue and target in WN; frequency of specific relation-sequence patterns in short paths (1–3 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Not reported as a single scalar efficiency value; however, many HBC links map into 1-step WN edges (74,903 such associations) and for mapped multi-step paths there is an empirical predominance of short (≤3) paths and structured patterns (e.g., 'brother').</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Mapping shows human associations preferentially use certain WN edge types (synonymy, hyperonymy, hyponymy) and overuse causal edges relative to their corpus frequency in WN; humans thus navigate WN non-uniformly, biasing toward specific semantic relations and short hierarchical moves (up then down).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Analysis of shortest-path decompositions reveals recurrent exploration motifs (e.g., 'brother', 'grandparent', 'grandchild'); these motifs indicate that human association policies favor brief hierarchical excursions (up to a general term then down to a sibling) rather than long random traversals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Structures and Semantics in Free Word Association', 'publication_date_yy_mm': '2012-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1363.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1363.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-walk model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-walk model of collective semantic exploration (as in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple model referenced by the paper in which social annotation or free-association sequences are modeled as random walks on an underlying semantic graph; used to reproduce vocabulary growth and emergent network structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Collective dynamics of social annotation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Implicit semantic graph (modeled environment) used in random-walk analyses</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Abstract semantic/concept graph (model substrate) on which agents perform random-walk-type transitions to emulate sequences of word associations or social annotation events.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Assumed complex-network topology (in referenced modeling work): typically small-world/scale-free-like graphs; the paper cites that such random-walk models on graphs reproduce observed annotation vocabulary growth and network features.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random-walk (memoryless local policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent performs unbiased (or slightly biased) local transitions along semantic edges; no explicit memory/planning beyond local neighborhood; used as a mesoscale model to reproduce emergent statistics of human annotation and association.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Vocabulary growth (Heaps' law / k(s) scaling), coverage rate of nodes as a function of draws, and reproduction of degree/strength distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Random-walk-like local policies can reproduce aggregate features; the paper references that such policies account for Heaps-like vocabulary growth and network statistics but does not claim optimality for specific navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Random-walk dynamics on small-world/scale-free semantic graphs yield fast coverage and reproduce empirical statistics (sublinear vocabulary growth, degree distributions); finite-size effects and rank-frequency (Zipf) of local targets lead to a limited-Heaps behaviour (k_out(s_out) with parameters B and V) that constrains exploration/coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Referenced modeling suggests that simple local/memoryless exploration on small-world semantic graphs explains many aggregate empirical patterns (fast reachability due to short paths, sublinear growth of distinct outputs per cue due to rank-frequency and finite vocabulary per context). The paper's empirical findings (bounded local branching V ≈ 74, dominance of short semantic motifs) align with such local policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Structures and Semantics in Free Word Association', 'publication_date_yy_mm': '2012-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Collective dynamics of social annotation <em>(Rating: 2)</em></li>
                <li>The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth <em>(Rating: 2)</em></li>
                <li>The university of south florida free association, rhyme, and word fragment norms <em>(Rating: 1)</em></li>
                <li>Wordnet: A lexical database for english. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1363",
    "paper_id": "paper-529ec0c00c46f549335ada62cebb4f5dba11be56",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "HBC",
            "name_full": "Human Brain Cloud (Word Association Game)",
            "brief_description": "A large-scale, web-based free word association dataset represented as a directed, weighted word-association graph built from human players' cue→target associations; used as a proxy for a cognitive/semantic navigation space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Human Brain Cloud word-association graph",
            "environment_description": "A semantic/conceptual space constructed from a massively multiplayer web-based free-association game (cue→target). Domain: cognitive/linguistic word-association space (not a physical text game, but treated as a navigable graph of concepts).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": false,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Filtered dataset yields a strongly connected directed weighted graph; small-world property reported (average shortest-path distance ~4); slight weighted assortativity r_w = 0.1; in-degree distribution heavy-tailed (power-law), out-degree peaked (Gamma-like).",
            "environment_size": "88,747 nodes; 3,013,125 directed edges (6,097,806 registered associations before filtering).",
            "agent_name": "Human players (free-association) / random-walk modeling (referenced)",
            "agent_description": "Human participants produce target words in response to cues (empirical, behavioral 'agent'); the paper also references a random-walk model of collective exploration (memoryless local transitions) used in prior work to model annotation/association sequences.",
            "exploration_efficiency_metric": "Average shortest-path distance (mean node distance) and vocabulary-growth/coverage metrics (Heaps'-law-like growth k_out(s_out)); frequency-rank (Zipf) behavior of target lists.",
            "exploration_efficiency_value": "Average node distance ≈ 4 (so most nodes reachable in ~5 steps); measured k_out(s_out) follows limited Heaps' law (Eq. 2) with fitted parameters B ≈ 0.92 and V ≈ 74 describing asymptotic vocabulary size per cue.",
            "success_rate": null,
            "optimal_policy_type": "Empirically, human associations behave like local associative policies (short semantic hops, preferential patterns); prior modeling suggests simple random-walk-like policies reproduce observed aggregate behavior.",
            "topology_performance_relationship": "Small-world (mean distance ≈ 4) implies fast, efficient exploration/reachability; peaked out-degree (typical k_out ≈ 28, asymptotic V ≈ 74) imposes a bounded local semantic context that limits branching and shapes coverage growth (k_out(s_out) sublinear, limited Heaps' law). Heavy-tailed in-degree (power-law exponent β ≈ 1.76) creates hubs (words reachable from many cues) which affect navigability and semantic retrieval asymmetrically (many cues map to few popular targets). Slight assortativity (r_w = 0.1) reported but its direct impact on exploration is not quantified beyond being noted.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Human-generated association sequences preferentially follow short semantic steps and hierarchical patterns (e.g., 'brother' pattern: up one hypernym then down to sibling hyponym). For 2-step and 3-step WN mappings, 'brother' sequences occur ~31% and ~51% respectively, indicating policies that maintain level-of-specificity rather than long random wandering. Causal relations are overrepresented relative to WordNet frequencies, indicating humans preferentially navigate certain semantic-edge types.",
            "uuid": "e1363.0",
            "source_info": {
                "paper_title": "Complex Structures and Semantics in Free Word Association",
                "publication_date_yy_mm": "2012-05"
            }
        },
        {
            "name_short": "WN",
            "name_full": "WordNet (lexical semantic graph)",
            "brief_description": "A large lexical database of English where words (lemmas) are nodes and many typed semantic relations (synonymy, hypernymy, meronymy, causal, etc.) form directed edges; used as the underlying semantic graph to map HBC associations into semantic paths.",
            "citation_title": "Wordnet: A lexical database for english.",
            "mention_or_use": "use",
            "environment_name": "WordNet semantic graph",
            "environment_description": "Lexical/semantic network (lemmas as nodes, typed semantic relations as directed edges) representing general semantic relations in English; used here as an underlying graph for mapping free-association links to semantic paths.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Directed graph of semantic relations; paper reports node and edge counts but does not report global connectivity statistics (they compute shortest-paths between mapped word pairs).",
            "environment_size": "143,963 nodes; 1,345,801 edges (as reported for their WN snapshot).",
            "agent_name": "Shortest-path mapping procedure (analysis algorithm)",
            "agent_description": "Mapping method: for each HBC association (word pair) compute shortest path(s) in WN between their lemmas; if multiple shortest paths exist, treat them as equiprobable and split weight accordingly; analyze 1-, 2-, and 3-step path compositions.",
            "exploration_efficiency_metric": "Path length (number of semantic steps) between cue and target in WN; frequency of specific relation-sequence patterns in short paths (1–3 steps).",
            "exploration_efficiency_value": "Not reported as a single scalar efficiency value; however, many HBC links map into 1-step WN edges (74,903 such associations) and for mapped multi-step paths there is an empirical predominance of short (≤3) paths and structured patterns (e.g., 'brother').",
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Mapping shows human associations preferentially use certain WN edge types (synonymy, hyperonymy, hyponymy) and overuse causal edges relative to their corpus frequency in WN; humans thus navigate WN non-uniformly, biasing toward specific semantic relations and short hierarchical moves (up then down).",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Analysis of shortest-path decompositions reveals recurrent exploration motifs (e.g., 'brother', 'grandparent', 'grandchild'); these motifs indicate that human association policies favor brief hierarchical excursions (up to a general term then down to a sibling) rather than long random traversals.",
            "uuid": "e1363.1",
            "source_info": {
                "paper_title": "Complex Structures and Semantics in Free Word Association",
                "publication_date_yy_mm": "2012-05"
            }
        },
        {
            "name_short": "Random-walk model",
            "name_full": "Random-walk model of collective semantic exploration (as in prior work)",
            "brief_description": "A simple model referenced by the paper in which social annotation or free-association sequences are modeled as random walks on an underlying semantic graph; used to reproduce vocabulary growth and emergent network structure.",
            "citation_title": "Collective dynamics of social annotation",
            "mention_or_use": "mention",
            "environment_name": "Implicit semantic graph (modeled environment) used in random-walk analyses",
            "environment_description": "Abstract semantic/concept graph (model substrate) on which agents perform random-walk-type transitions to emulate sequences of word associations or social annotation events.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Assumed complex-network topology (in referenced modeling work): typically small-world/scale-free-like graphs; the paper cites that such random-walk models on graphs reproduce observed annotation vocabulary growth and network features.",
            "environment_size": null,
            "agent_name": "Random-walk (memoryless local policy)",
            "agent_description": "Agent performs unbiased (or slightly biased) local transitions along semantic edges; no explicit memory/planning beyond local neighborhood; used as a mesoscale model to reproduce emergent statistics of human annotation and association.",
            "exploration_efficiency_metric": "Vocabulary growth (Heaps' law / k(s) scaling), coverage rate of nodes as a function of draws, and reproduction of degree/strength distributions.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Random-walk-like local policies can reproduce aggregate features; the paper references that such policies account for Heaps-like vocabulary growth and network statistics but does not claim optimality for specific navigation tasks.",
            "topology_performance_relationship": "Random-walk dynamics on small-world/scale-free semantic graphs yield fast coverage and reproduce empirical statistics (sublinear vocabulary growth, degree distributions); finite-size effects and rank-frequency (Zipf) of local targets lead to a limited-Heaps behaviour (k_out(s_out) with parameters B and V) that constrains exploration/coverage.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Referenced modeling suggests that simple local/memoryless exploration on small-world semantic graphs explains many aggregate empirical patterns (fast reachability due to short paths, sublinear growth of distinct outputs per cue due to rank-frequency and finite vocabulary per context). The paper's empirical findings (bounded local branching V ≈ 74, dominance of short semantic motifs) align with such local policies.",
            "uuid": "e1363.2",
            "source_info": {
                "paper_title": "Complex Structures and Semantics in Free Word Association",
                "publication_date_yy_mm": "2012-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Collective dynamics of social annotation",
            "rating": 2
        },
        {
            "paper_title": "The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth",
            "rating": 2
        },
        {
            "paper_title": "The university of south florida free association, rhyme, and word fragment norms",
            "rating": 1
        },
        {
            "paper_title": "Wordnet: A lexical database for english.",
            "rating": 1
        }
    ],
    "cost": 0.011601,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Complex structures and semantics in free word association</h1>
<p>Pietro Gravino<br>Dipartimento di Fisica, Sapienza Università di Roma, Piazzale Aldo Moro 5, 00185 Roma, Italy.<br>Dipartimento di Fisica, "Alma Mater Studiorum" Università di Bologna,<br>Viale Berti Pichat 6/2 40127, Bologna, Italy<br>pietro.gravino@gmail.com</p>
<p>Vito D. P. Servedio<br>Dipartimento di Fisica, Sapienza Università di Roma, Piazzale Aldo Moro 5, 00185 Roma, Italy.<br>vito.servedio@roma1.infn.it</p>
<p>Alain Barrat
Centre de Physique Théorique (CNRS UMR 6207), Luminy, 13288 Marseille Cedex 9, France Institute for Scientific Interchange (ISI), Torino, Italy. Alain.Barrat@cpt.univ-mrs.fr</p>
<p>Vittorio Loreto
Dipartimento di Fisica, Sapienza Università di Roma, Piazzale Aldo Moro 5, 00185 Roma, Italy.
Institute for Scientific Interchange (ISI), Torino, Italy. vittorio.loreto@roma1.infn.it</p>
<p>Received (received date)
Revised (revised date)</p>
<p>We investigate the directed and weighted complex network of free word associations in which players write a word in response to another word given as input. We analyze in details two large datasets resulting from two very different experiments: on the one hand the massive multiplayer web-based Word Association Game known as Human Brain Cloud, and on the other hand the South Florida Free Association Norms experiment. In both cases the networks of associations exhibit quite robust properties like the small world property, a slight assortativity and a strong asymmetry between in-degree and outdegree distributions. A particularly interesting result concerns the existence of a typical scale for the word association process, arguably related to specific conceptual contexts for each word. After mapping the Human Brain Cloud network onto the WordNet semantics network, we point out the basic cognitive mechanisms underlying word associations when they are represented as paths in an underlying semantic network.</p>
<p>Keywords: Complex Networks, Language Dynamics, Word Association, Semantic Network, WordNet</p>
<h1>1. Introduction</h1>
<p>In the last years, the physicists' toolbox has become increasingly used for the study of complex systems in areas traditionally far from the pure realm of physics. Many works have concerned the interdisciplinary field of complex networks [8, 21, 3, 1], as well as the statistical physics of social dynamics [4] where the collective properties of population of human individuals are investigated. From this perspectives a lot of emphasis has been put on opinion, cultural and language dynamics, crowd behavior, hierarchy formation, human dynamics, and social spreading phenomena, just to quote the most important examples.</p>
<p>In social phenomena, the basic constituents are not particles but humans [2]. Though in many cases one can neglect the intrinsic complexity of human beings, as for instance for collective phenomena like traffic or crowd behaviour [12, 18], the detailed behavior of each of them is already the complex outcome of many cognitive, psychological and physiological processes, still largely unknown. A very interesting example is represented by social annotations processes [13, 10] through which users annotate resources (web pages, bibliographic references, digital photographs, etc.) with free-form text keywords, known as tags. The emergent data structures are complex networks that represent an externalization of semantic structures (networks of concepts [22]) grounded in cognition and typically hard to access. In addition these networks are collectively built by the uncoordinated activity of thousands to millions of users, entangling semantics and user behaviour into the so-called technosocial systems.</p>
<p>In [5] it has been shown that the process of social annotation can be seen as a collective exploration of a semantic space, modeled as a graph, through a series of random walks that should represent sequences of word associations in an hypothetical conceptual space. This simple approach reproduce several aspects of social annotation, among which the peculiar growth of the size of the vocabulary used by the community [11] and its complex network structure [6].</p>
<p>In [5] the semantic space was modeled as a graph. Though very reasonable, this hypothesis has never been tested in a quantitative way. Since the elementary cognitive processes underlying social annotation are word associations, it is quite natural to investigate the structures of our conceptual spaces by looking at the experiments where word associations have been measured in a quantitative way. This is precisely the point of view we take in this paper and we perform a thorough analysis of the two most important word associations databases obtained by collecting responses (target words) given by humans to specific input words (cue words). We consider in particular the Human Brain Cloud database and the University of South Florida Free Association Norms database [19].</p>
<p>Human Brain Cloud (HBC) represents the largest available word association database. It was obtained through the implementation of a massively multiplayer online game. As HBC was not conceived with a specific scientific purpose in mind and may thus suffer from uncontrolled biases, we also consider the smaller, though</p>
<p>scientifically more controlled, database known as the University of South Florida Free Association Norms [19]. We analyze the graphs built from both databases though we focus more specifically on HBC. We derive in particular an expression describing the growth of the HBC graph and we highlight the existence of a typical scale for the word association process. Next we present an analysis aimed at grounding the observations made in the word association graphs by a direct comparison with WordNet (WN) [15, 17], the largest lexical database of words and semantic relations. The aim here is to associate a semantic value to each node of the graph as well as labeling the word associations in terms of semantically charged cognitive paths on the WordNet graph. The ensemble of these results is very valuable since they help in shedding light on the cognitive processes underlying free word associations.</p>
<h1>2. Free Word Associations: data and networks</h1>
<p>Word associations have been experimentally studied in details, especially by linguists and cognitive scientists [7, 19]. An interesting point of view, not yet fully explored, is to look at the ensemble of words and associations as a complex network, where nodes are words and links are associations, and to analyze it as such [23, 14]. Typical word association experiments involve a relatively limited number of subjects $(100-1,000)$, in controlled conditions. A word (called cue word) is presented to the recruited subjects, who are asked to write the first related word that come to their minds (called target word). Due to high costs in terms of time and money, the number of cue-target associations gathered in these classical experiments has been relatively limited, at maximum of the order of 100,000 .</p>
<p>The experimental data we analyze were obtained from the "massively multiplayer word association game" Human Brain Cloud (HBC) ${ }^{a}$. HBC was designed as a web-based game in English language that simply proposed a cue word to the player, asking for a target word (e.g., volcano-lava, house-roof, dog-cat, etc.). The cue was taken from an internal self-consistent dictionary, constructed by gathering the answered target words at the end of game sessions. With respect to usual experiments, no control is performed on the number of players, the number of cuetarget associations given by each player, etc... On the other hand, the obtained dataset is considerably larger than that of previous experiments, and consists of approximately 600,000 words and $7,000,000$ associations, gathered within a period of one year (while pre-existing experiments involved teams of specialists for much longer periods of time). As each player could enter whichever word or set of words, the data contain a certain volume of inconsistent words, which have to be discarded. After a suitable filtering procedure, which we describe in Appendix A, we obtain a strongly connected directed weighted graph with almost 90,000 words and $6,000,000$ associations, i.e., a dataset still considerably larger than those of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>previous experiments.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Graphical representation of the word association network obtained from the HBC dataset. Labeled nodes represent the first 5,000 words entered in the system while arcs represent word associations. Label colors distinguish different parts of speech (nouns, verbs, etc). Arcs' width codes the weight of the corresponding association, i.e., how many people ever made that association.</p>
<p>In order to detect possible systematic biases in HBC, we also compare some properties of the HBC dataset with the dataset of the most important word association experiment, the Nelson et al. "University of South Florida Free Association Norms" database (SF) [19]. SF is the outcome of a great effort started back in 1973 and lasted almost thirty years. consists of 5,000 words and 700,000 associations. Each dataset yields a graph whose nodes are the words and whose edges correspond to the associations between words made by the players/subjects. Each edge is moreover weighted by the number of times that the corresponding association has been made. A snapshot of a part of the HBC graph is shown in Fig. 1. We compare in Fig. 2 the statistical properties of the two networks. It turns out that almost all $(99 \%)$ the words used as cues in SF were present in HBC and $72 \%$ of the SF</p>
<p>associations were present in the HBC dataset. To deepen our analysis, we consider for each word $w$ the set of associations in which $w$ was proposed as a cue word to players, and define the out-strength $s_{\text {out }}(w)$ of the corresponding node as the total number of associations in which $w$ was the cue, and the out-degree $k_{\text {out }}(w)$ as the number of distinct target words answered in response to $w$. Analogously, we define the in-strength $s_{i n}(w)$ and the in-degree $k_{i n}(w)$ by referring to the associations in which $w$ was answered as target: $s_{i n}(w)$ is the total number of times that $w$ appears as a target, and $k_{i n}(w)$ is the number of distinct other words which yielded $w$ as an answer.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The log-binned degree distributions for the word association networks of HBC and SF, in log-log scale. Since we are matching datasets of different sizes we divided the degree by the average degree value. In red, the in-degree distributions for HBC (filled circles) and for SF (empty circles). In blue, the out-degree distributions for HBC (filled squares) and for SF (empty squares). Both in-degree distributions show a power law form (a straight line in log-log). Both out-degree distributions have a scale-rich shape. The different in shape at lower degrees is a consequence of the filtering procedure described in Appendix A.</p>
<p>Figure 2 shows the distribution of in- and out-degrees in both HBC and SF, as a function of $k /\langle k\rangle$ (where $\langle k\rangle$ is the average of the distribution). The distribution of</p>
<p>out-degrees is narrow, which means that the number of distinct answers given by distinct persons to a given cue is rather limited, as could be expected. On the other hand, the distribution of in-degrees is broad: the number of distinct cues from which a given target can be obtained ranges from 1 to $10\langle k\rangle$ with $\langle k\rangle_{\text {in }}^{\text {HBC }}=\langle k\rangle_{\text {out }}^{\text {HBC }}=34$, $\langle k\rangle_{\text {in }}^{S F}=36.5$ and $\langle k\rangle_{\text {out }}^{S F}=6.15$ in and out averages HAVE to be equal, except if the nodes with $\mathbf{k}=\mathbf{0}$ are not counted...!. The degree distributions of both HBC and SF exhibit very similar shapes, except for a difference caused by the filtering procedure (described in Appendix A) for small in-degrees in HBC. It is worth noticing that even though the systems have different sizes, the out-degrees distributions show a peak at $k /\langle k\rangle$, a clear indication of an intrinsic similarity of this specific kind of networks, despite their different origin.</p>
<p>Note that the difference of distributions between in- and out-degrees is not a surprise: if one chooses at random targets taken from a list of targets obeying a Zipf's distribution, the number of distinct targets obtained from a given cue will turn out to have a narrow distribution.
Figure 3 reports the strength distributions for HBC and SF, as a function of $s /\langle s\rangle$, with $\langle s\rangle_{\text {in }}^{\text {HBC }}=\langle s\rangle_{\text {out }}^{\text {HBC }}=68.7,\langle s\rangle_{\text {in }}^{S F}=144$ and $\langle s\rangle_{\text {out }}^{S F}=24.3$ once again in and out averages have to be equal. The out-strength distributions are completely determined by the way in which the cue words were chosen. Both in-strength distributions show a power law form (a straight line in log-log). The different shape at lower degrees is a consequence of the filtering procedure described in Appendix A.</p>
<p>In order to quantify more precisely the similarity between SF and HBC we computed the cosine similarity between the neighborhoods of each word. Given a word $w_{i}$ which belongs to both SF and HBC , we are interested in how much the associations in our two datasets having $w_{i}$ as a cue are similar. We define $l_{i j}^{S F}$ (resp. $l_{i j}^{H B C}$ ) as the number of times that the association between $w_{i}$ and $w_{j}$ has been made in the SF (resp. HBC) dataset. The cosine similarity between the associations made with $w_{i}$ in the two datasets is then defined as</p>
<p>$$
C S_{i}=\frac{\sum_{j} l_{i j}^{H B C} \cdot l_{i j}^{S F}}{\sqrt{\sum_{j} l_{i j}^{H B C^{2}} \cdot \sum_{j} l_{i j}^{S F^{2}}}}
$$</p>
<p>where the sums run over all common words between the two datasets. The cosine similarity ranges between 0 (no common word is associated to $w_{i}$ in SF and HBC), and 1 , if all the $l_{i j}$ are equal. The average cosine similarity turns out to be 0.851
${ }^{\text {b }}$. This very large value demonstrates a very strong correlation between the data obtained in the SF controlled experiment and in the web-based HBC game. This is an important information pointing to the reliability of the HBC dataset.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. The log-binned strength distributions for the word association networks of HBC and SF, in log-log scale. Since we are matching datasets of different sizes we divided the strength with the average strength value. In red, the in-strength distributions for HBC (filled circles) and for SF (empty circles). In blue, the out-strength distributions for HBC (filled squares) and for SF (empty squares). Both in-strength distributions show a power law form (a straight line in log-log). The different shape at lower degrees is a consequence of the filtering procedure described in Appendix A. The out-strength distributions have a peaked but different shape, because of the different ways in which cue words were chosen.</p>
<h1>3. Human Brain Cloud in depth</h1>
<p>Once acquired enough confidence on the reliability of the HBC dataset, let us now deepen our analysis only focusing on HBC, as the largest available Word Association dataset. The directed graph of HBC is composed by 88,747 nodes and $3,013,125$ edges in total, corresponding to $6,097,806$ registered associations. As we have seen in Fig. 2, while the in-degree distribution follows a power-law, the out-degree distribution is peaked. We measure a power-law in-degree distribution of HBC with an exponent $\beta \simeq 1.76$ smaller than 2 , which implies that the average value of the in-degree $\left\langle k_{i n}\right\rangle$ is not well defined, diverging with the size of the system: the number of cues that can yield a given target word as outcome has no typical value. On the</p>
<p>contrary, the out-degree shows a Gamma-like distribution peaked around a typical value $k_{\text {out }} \simeq 28$, corresponding therefore to a well-defined characteristic number of distinct words obtained as targets for each word inserted in the game as a cue. The distribution drops very rapidly at large $k_{\text {out }}$ with a sharp cut-off around 100, indicating that the number of distinct words that humans spontaneously associate to a given cue is quite restricted. For a better understanding of the origin of this $k_{\text {out }}$ scale, we can study the average growth of $k_{\text {out }}$ as a function of $s_{\text {out }}$, i.e., the average number of different target words obtained from a given cue word as a function of the number of times the given cue was extracted for a game (see Fig. 4).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Average $\left\langle k_{\text {out }}\right\rangle$ as a function of $s_{\text {out }}$ (red), obtained by averaging the $k_{\text {out }}$ values of all those cue words with the same $s_{\text {out }}$ value. The fitted curve obtained from Eq. (2) is shown in black (parameters: $V \simeq 74$ and $B \simeq 0.92$ ). Inset: same data in log-log scale together with a fit to a pure sub-linear power-law, to highlight the deviation from such a law at large $s_{\text {out }}$.</p>
<p>The inset of Fig. 4 shows that $\left\langle k_{\text {out }}\right\rangle$ can be approximately described by a sublinear power-law with exponent of $\sim 0.87$, but a deviation from this law is observed at large $s_{\text {out }}$. The sub-linear power-law behavior is well known to appear in the dictionary growth of texts and is known as Heaps' law in this framework [11]. The</p>
<p>Heaps' law is symptomatic of an underlying generalized Zipf's law [24], which in our case should appear in the marginal distribution of the target word frequencies, given a fixed cue word. Figure 5 shows the average frequency ranks for different values of $s_{\text {out }}$, confirming the presence of a Zipf's law, which becomes better defined as $s_{\text {out }}$ grows.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. For each cue word we calculate the frequency-rank (FR) plot of its target words. Each curve in the picture corresponds to an averaged FR plot obtained by averaging the FR curves corresponding to cue words with the same out-strength value. When $s_{\text {out }}$ is sufficiently large (i.e., we have sufficiently large statistics) we obtain a power-law with an exponent close to (minus) unity, followed by a flat tail.</p>
<p>To estimate the functional shape of $k_{\text {out }}\left(s_{\text {out }}\right)$, we investigate in Appendix B the impact of finite size effects combined with the above mentioned Zipf's law. We define $B$ as the exponent of the frequency rank of the target to a given cue and assume that there exists a maximum number, $V$, of different target words that can be associated to a cue word. In this way we are implicitly assuming that $k_{\text {out }}$ will converge asymptotically to $V$ (possibly with a residual logarithmic growth that we neglect here). With these hypotheses in mind we derive an expression (see Appendix B) for the behaviour of $k_{\text {out }}$ as a function of $s_{\text {out }}$ that reads:</p>
<p>$$
k_{\text {out }}\left(s_{\text {out }}\right)=V \frac{B s_{\text {out }}^{\prime 1 / B}-s_{\text {out }}^{\prime}}{B-1}
$$</p>
<p>where:</p>
<p>$$
s_{\text {out }}^{\prime}=\frac{s_{\text {out }}}{s_{\text {out }}^{l i m}} \quad s_{\text {out }}^{l i m}=\frac{\left(V^{B}-V\right)}{(B-1)}
$$</p>
<p>We also defined the auxiliary parameter $s_{\text {out }}^{\text {lim }}$, it is the value of $s_{\text {out }}$ for which $k_{\text {out }}=V$.</p>
<p>We can use Eq. (2) to fit the experimental data to check whether out finite-size analysis holds in this case and deducing an estimate for $B$ and $V$. The best fit is shown in Fig. 4, leading to the values $B \sim 0.92$ and $V \simeq 74$. It is worth to note that, while Eq. (2) is significantly different from a typical Heaps' law, one recovers the pure Heaps' behaviour for $V&gt;&gt;s_{\text {out }}$. In this limit, one has that for small $s_{\text {out }}$ and for $B&gt;1$ the behaviour of $k_{\text {out }}$ can be approximated by a power law with exponent $1 / B$. If instead $B \leq 1$ the dominant term of the Eq. (2) will be the linear one (Note that the growth cannot be more than linear, since $k_{\text {out }} \leq s_{\text {out }}$ ).</p>
<p>It is important to remark that we cannot exclude that the actual formula fitting the data of Fig. 4 contains logarithmic corrections implying that $k_{\text {out }}$ would keep growing asymptotically with $s_{\text {out }}$ though with a logarithmic or sub-logarithmic law. Here we are neglecting this possibility.</p>
<p>The value of $V$ obtained is somewhat surprising since it implies that the asymptotic number of different targets obtained for a given cue is indeed several orders of magnitude smaller than the total number of words in the system ( $\sim 90,000$ ). Hence, we find that on average, there is a limited number of target words for a given cue, reflecting the existence of a limited semantic context associated by humans to each word. This is in contrast with the fact that the number of words (cues) that yield a given target does not show any particular scale: the semantic context is thus not a "symmetric" concept in terms of free word associations.</p>
<p>Other measures can help in characterizing in a more detailed way the topology of the graph. A measure of the distances between the nodes of the graph, i.e., the shortest path between nodes, indicates that the filtered strongly connected component of the HBC network satisfies the "small world" property with an average node distance of approximately 4 . This means that with path of around 5 steps we can reach almost every node of the graph, so that we can easily and quickly explore it. This is not a surprise as most complex networks have been shown to share this property $[21,3,1]$.</p>
<p>We also analyzed the mixing patterns [20] of the HBC graph, in order to measure the tendency of nodes with similar degree to preferably connect to each other. The assortativity coefficient $r$ is defined as the Pearson's correlation coefficient calculated between the out-degrees of the nodes linked by an edge. If, for each edge, $k_{\text {out }}^{\text {cue }}$ is</p>
<p>the out-degree of the cue and $k_{\text {out }}^{\text {target }}$ is the out-degree of the target, we have:</p>
<p>$$
r=\frac{\left\langle k_{\text {out }}^{\text {cue }} \cdot k_{\text {out }}^{\text {target }}\right\rangle-\left\langle k_{\text {out }}\right\rangle^{2}}{\left\langle k_{\text {out }}^{2}\right\rangle-\left\langle k_{\text {out }}\right\rangle^{2}}
$$</p>
<p>where the averages are calculated on the ensemble of edges. If we consider the number of times a given cue has been associated with a given target as the weight of that edge, we can weight the averages and measure the weighted assortativity coefficient $r_{w}=0.1$, which points to a slight assortativity.</p>
<p>These results give us an overview of the properties of the HBC network. Using the HBC word association network as a proxy of the way in which our mind stores and organizes all words and related meanings, we observe that it has indeed the properties we should expect from such a network: every word defines a limited context; we can explore the network in a fast and efficient way, for example to recover meanings; and the network is still connected even in case we forget some word or if we do not know it. While this first analysis has concerned the network in itself, considering nodes and edges as abstract entities, in the next section we shall deal with their semantic content.</p>
<h1>4. Introducing semantics</h1>
<p>The nodes of the HBC network are words, and as such, they have a semantic value. Let us therefore aim at measuring quantitatively what kind of semantic relations are used while associating two words. In order to analyze these semantic connections, a database of semantic relations between words is needed. We choose to use WordNet (WN) [15, 17], a large lexical database developed at Princeton University. In WordNet words are related to each other according to their mutual semantic relations, a list of which is given in Appendix C. WordNet allows us to built another directed graph in which nodes (143963) are again words but edges (1345801) represent now the semantic relations between them. In order to find to what kind of relation corresponds to a given free association between words (i.e., a link in the HBC graph), we can examine the shortest paths in the WN graph between every pair of words linked by an association in the HBC graph, as shown in Fig. 6.</p>
<p>In this mapping procedure, we have to take into account that words of the WN database are actually lemmas, i.e., are reported in their dictionary form, while HBC words are subject to any kind of morphological derivation (third person, plurals, etc). For this reason, in performing the mapping we assign to two words of HBC the path existing between their relative lemmas. For example since the WN connection between "buy" and "purchase" is of the synonymy type, we consider also the couple "bought" and "purchased" as synonyms.</p>
<p>It may moreover happen that two words associated in HBC correspond to multiple shortest paths with the same length in WN. In such a case, we consider all paths as equiprobable by assigning to each possibility a normalized weight. For example "oak" and "pine" are linked by an association in HBC. In the WN graph</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. The mapping of HBC onto WN: given a couple of words linked by an association in the HBC graph (the blue arrow) we consider for the shortest path (black arrows and nodes) between the same words in the WN graph (gray graph in the background). In this example a HBC association maps into a 3 -steps WN path.
to go from "oak" to "pine" we can go either through "tree" or "forest". The two alternative semantic paths would be: oak $\models$ hyperonymy ${ }^{\mathrm{e}} \Rightarrow$ tree $\models$ hyponymy ${ }^{\mathrm{d}} \Rightarrow$ pine or oak $\models$ holonymy ${ }^{\mathrm{e}} \Rightarrow$ forest $\models$ meronymy ${ }^{\mathrm{f}} \Rightarrow$ pine; we consider both by assigning to each of them a weight 0.5 .</p>
<p>We first analyze the 74903 HBC associations that map into one step path on WN. They correspond to the directed edges which are present in both the WN and HBC graphs. Figure 7 shows the normalized distribution of their semantic relations.</p>
<p>The distribution shows that synonymy ${ }^{\mathrm{g}}(32 \%$ of the common links between HBC and WN), hyperonymy ( $26 \%$ ) and hyponymy ( $18 \%$ ) are the most used semantic relations for an association. In Fig. 7 we also show the distribution of the semantic relations in the whole WN graph for comparison. We notice that, for some relations there is a substantial difference between their occurrence in HBC and WN. For example, the causal (CAU) nexus normalized occurrence is much larger in the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Red bars represent (in logarithmic scale) the normalized distribution of the semantic relations characterizing those HBC associations that map into a one step path on the WN network. Blue bars represent the semantic relations distribution in the whole WN graph.
mapped HBC (1.6\%) than in the WN graph ( $0.05 \%$ ). This means that if a given cue word has a causal link in WN then players will be more oriented to follow it while making the association instead of following other kinds of semantic relations. It is then clear that we need to quantify the occurrences of semantic relations in HBC with respect to our reference system of WN by analyzing the effective possibility of players of doing an association in HBC following a certain semantic relation of WN. If we look at all associations starting from a cue word $w$ we could measure the ratio of associations following the synonymy relation or the hyperonimy relation. In general, we could measure all the ratios for any kind of semantic relation. If we have enough statistics, we may assume these ratios are the probabilities of doing these associations starting from $w$. Let us consider the example case in which $w$ has $k_{\text {out }}=5$ and $s_{\text {out }}=10$. Consider also that 4 links are synonymy links and the other one is a causal link. Finally, consider also that, of the 10 associations, 4 follow a causal link and 6 one of the synonymy links. Even though there are overall more synonymy associations it is clear that the causal link is more used than we could expect by just looking at the links of $w$. This means than human beings construct associations with probabilities that could strongly deviate from what would be ex-</p>
<p>pected from the pure statistical structure of WN. This is a very interesting feature to further quantify. Sticking on the example of the causal link, it is evident that in order to quantify its actual relevance, we should not estimate the probability of making a causal link association, but rather the probability of making a causal link association conditioned to the existence of a certain number of available causal links for $w$. In this case it would be one causal link out of the 5 total links, i.e., $P(c a u \mid(1$ cau; 5 syn)).</p>
<p>In general, the fraction of associations of a given type represents the probability of choosing a certain semantic link given the underlying WN structure of the possible available links. In order to quantify the relative importance of a given semantic association we normalize its frequency of occurrence in WN. To this end, we define $\rho_{i}$ as the percentage of relations of kind $i$ in one step paths and $p_{i}$ as the percentage of relations of kind $i$ in WN and we define a normalized effective probability as:</p>
<p>$$
\pi_{i}=\frac{\frac{\rho_{i}}{p_{i}}}{\sum \frac{\rho_{j}}{p_{j}}}
$$</p>
<p>$\pi_{i}$ represents thus the probability of performing the association $i$ as if at each step all the semantic associations were equally possible. The results of this computation is presented in Fig. 8, where we also report the information about the size of set of each type of semantic association, to give a qualitative and quantitative idea of the reliability of these results.</p>
<p>The analysis reported in Fig. 8 demonstrates how the word association process in HBC features important deviations with respect to WN. If the semantic relation occurrences in HBC were exactly the same as in WN, all symbols would lie on the dashed line. The ones which lie below occur less frequently and viceversa. Deviations are in both directions. For example, hyperonymy is slightly more frequent than expected while hyponymy is less frequent. This means that the target word tends to be a more general term instead of a more specific one. The already mentioned causal link is largely overrepresented in HBC. This means that, if the cue word have a causal link, while making an associations we will tend to prefer it. On the other hand there are associations poorly represented in HBC like "is member" (ISMEM) or "has member" (HASMEM). We also see how many not purely semantic relations (as "see also" or "keyword") are chosen frequently.</p>
<p>We also considered the HBC associations that map into a path of two or three steps in WN, as reported in Fig. 9. The first positions in the distribution correspond again to combinations of hyperonymy, hyponymy and synonymy, which were already the most frequent relations in the previously discussed case of one step paths.</p>
<p>By normalizing as we did before for the one step path case, and retaining the most significant results (i.e., those corresponding to the largest statistics, such as the yellow and the red ones in Fig. 8) we obtain the situation shown in Fig. 9. These probabilities seem to show the emergence of a particular kind of exploration paths of the WN graph. Many of the WN relations are hierarchical. For example,</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Normalized "effective" probabilities of the different types of WN semantic relations with error bars. The dashed line represents the probabilities of the associations in WN. So, if the semantic relation occurrences in the mapped HBC were exactly the same as in WN, all symbols would lie on the dashed line. To give an idea of the significance of the values, for each value we draw two triangles whose colors indicate the logarithm of the number of relations of the given type and therefore give an idea of the measure accuracy (blue and violet values correspond to smaller sample sizes than yellow and red ones). Upward triangles refer to the mapped HBC with unity length paths, while downward triangles refer to the whole WN.
hyperonymy binds a specific term to a more general one (e.g., tree is the hyperonym of oak). Other examples are holonymy (the relation between a whole and a part: tree is the holonym of bark) or geographical collocation. In Fig. 9 and 10 we can see how in the sequences exploring these hierarchical structures there is a recurrent pattern, made of one step towards a more general term followed by one step towards a more specific term. We call this pattern the "brother" pattern, because, starting from a given node it takes us to another child (specific term) of the same parent (general term). In order to study the importance of this pattern we measure its occurrence together with two other patterns: grandparent (two steps towards more general terms) and grandchild (two steps towards more specific terms). For paths of two and three steps, we find:</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. The top ten occurrence ranking of the semantic relations sequences for those HBC associations that map onto a path of two (top chart) and three (bottom chart) steps in WN.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">pattern</th>
<th style="text-align: right;">2 steps</th>
<th style="text-align: right;">3 steps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">brother</td>
<td style="text-align: right;">$31 \%$</td>
<td style="text-align: right;">$51 \%$</td>
</tr>
<tr>
<td style="text-align: left;">grandparent</td>
<td style="text-align: right;">$8 \%$</td>
<td style="text-align: right;">$18 \%$</td>
</tr>
<tr>
<td style="text-align: left;">grandchild</td>
<td style="text-align: right;">$5 \%$</td>
<td style="text-align: right;">$14 \%$</td>
</tr>
</tbody>
</table>
<p>These results confirm the existence of preferential patterns of exploration of hierarchical structures in the process of word association, in a way that most often maintains the same level of specificity between cue and target. Note that we limit the analysis to paths of up to three steps because the distribution for longer paths is practically equal to the distribution occurring in an uncorrelated artificial system built associating randomly extracted words.</p>
<h1>5. Conclusions</h1>
<p>In this paper, we have presented the results of the analysis performed on the Word Association Graph constructed in two different experiments: Human Brain Cloud (HBC) and the South Florida Free Association Norms (SF). Word association graphs are quite interesting because they represent a proxy of the way in which our mind stores and organizes all words and related meanings. The HBC dataset</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. The 5 most significant normalized semantic relations sequences for those HBC associations that map onto a path of two and three steps in WN.
is substantially larger than any previously available datasets and it has been constructed through a web-based game while the South Florida Free Association Norms is the outcome of a controlled linguistic experiment. The comparison between the two databases has been an important preliminary step in our analysis in order to be sure that no major biases were present in the HBC database. After a filtering procedure we ended up with a HBC database whose statistical properties exhibit strong correlations with that of SF, giving us confidence on the generality and reliability of the approach. In this way for the first time the huge database of the Human Brain Cloud experiment has been brought to the attention of the scientific community as a valuable tool to shed light on the possible mechanisms of word and meaning retrieval processes underlying human language skills.</p>
<p>Our results have shown that the associations to a given cue word tend to result in a limited set of words, whose size is considerably smaller than that of the system, while the number of words that yield a given target can fluctuate enormously. Hence, the input of a word seems to define a sort of "semantic context" as an output, which can be subject of further analysis. It is worth to point out how the existence of these contexts should influence the realization of future word association experiment. Equation (2) may be used to understand how much statistics is needed in order to fully explore these semantic contexts, or at least their most significant</p>
<p>part. On the other hand, the size of the context which leads to a given word target has unbounded fluctuations.</p>
<p>We found that the HBC network is robust and can be efficiently explored. In the framework of the hypothesis of the existence of a cognitive networked structure revealed by the free word association games or experiments, the robustness and navigational efficiency of such a network are thus compatible to what we should expect from our mental "meanings management system".</p>
<p>We further extended our analysis by grounding the observations made on the HBC graph through a direct comparison with the largest lexical database of words and semantic relations, WordNet (WN) [15, 17]. Through WordNet, we classified the semantic character of word associations collected in HBC. This classification leads to a preliminary understanding of the cognitive processes underlying word association. The most used semantic relations result to be synonymy, hyperonymy and hyponymy. By comparison with the overall number of semantic relations present in WordNet, we have shown that other types of less common relations are in fact important, such as for instance the causal nexus. Moreover, when the association corresponds to a sequence of semantic steps, preferential combinations of semantic relations emerge, with an overall tendency to keep the same level of specificity between the cue word and the target.</p>
<h1>Acknowledgments</h1>
<p>The authors wish to thank Francesca Tria for very interesting discussions. This research has been partly supported by the EveryAware project funded by the Future and Emerging Technologies program of the European Commission under Grant Agreement Number 265432.</p>
<h2>References</h2>
<p>[1] Barrat, A., Barthélemy, M., and Vespignani, A., Dynamical processes on complex networks (Cambridge University Press, 2008).
[2] Buchanan, M., The social atom (Bloomsbury, New York, NY, USA, 2007).
[3] Caldarelli, G., Scale-Free Networks (Oxford University Press, 2007).
[4] Castellano, C., Fortunato, S., and Loreto, V., Statistical physics of social dynamics, Rev. Mod. Phys. 81 (2009) 591-646.
[5] Cattuto, C., Barrat, A., Baldassarri, A., Schehr, G., and Loreto, V., Collective dynamics of social annotation, pnas 106 (2009) 10511-10515.
[6] Catutto, C., Schmitz, C., Baldassarri, A., Servedio, V. D. P., Loreto, V., Hotho, A., Grahl, M., and Stumme, G., Network properties of folksonomies, AI Communications Journal, Special Issue on 'Network Analysis in Natural Sciences and Engineering' (2007).
[7] Church, K. and Hanks, P., Word association norms, mutual information, and lexicography, Computational Linguistics 16 (1990) 22-29.
[8] Dorogovtsev, S. N. and Mendes, J. F. F., Evolution of Networks: From Biological Nets to the Internet and WWW (Oxford University Press, 2003).
[9] Gabler, K., Kyle gabler's web page, http://kylegabler.com/.</p>
<p>[10] Golder, S. and Huberman, B. A., The structure of collaborative tagging systems, Journal of Information Science 32 (2006) 198-208.
[11] Heaps, H. S., Information Retrieval: Computational and Theoretical Aspects (Academic Press, Inc., Orlando, FL, USA, 1978).
[12] Helbing, D., Traffic and related self-driven many-particle systems, Rev. Mod. Phys. 73 (2001) 1067-1141.
[13] Mathes, A., Folksonomies - Cooperative Classification and Communication Through Shared Metadata (2004), http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html.
[14] Mehler, A., Large text networks as an object of corpus linguistic studies, in Corpus Linguistics. An International Handbook of the Science of Language and Society, eds. Lüdeling, A. and Kytö, M. (de Gruyter, Berlin/New York, 2007).
[15] Miller, G. A., Wordnet - about us, http://wordnet.princeton.edu.
[16] Miller, G. A., Wordnet: A lexical database for english., Commun. ACM 38 (1995) $39-41$.
[17] Miller, G. A. and Fellbaum, C., WordNet: An electronic lexical database (MIT Press, Cambridge, MA, 1998).
[18] Nagatani, T., The physics of traffic jams, Rep. Prog. in Phys. 65 (2002) 1331-1386.
[19] Nelson, D., McEvoy, C., and Schreiber, T., The university of south florida free association, rhyme, and word fragment norms, Behavior Research Methods 36 (2004) $402-407$.
[20] Newman, M. E. J., Assortative mixing in networks, Physical Review Letters 89 (2002) 208701.
[21] Pastor-Satorras, R. and Vespignani, A., Evolution and Structure of the Internet: A Statistical Physics Approach (Cambridge University Press, New York, NY, USA, 2004).
[22] Sowa, J. F., Conceptual structures: information processing in mind and machine (Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1984).
[23] Steyvers, M. and Tenenbaum, J. B., The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth, Cognitive Science 29 (2005) $41-78$.
[24] Zipf, G. K., Human behavior and the principle of least effort (Hafner, New York, 1965).</p>
<h1>Appendix A. Filtering Human Brain Cloud data</h1>
<p>Human Brain Cloud was conceived as a game, and not designed for scientific purposes. Nevertheless, the system included a "quality control" for the word dataset based on two factors: word popularity, i.e., a word used often was assumed to be a "valid word", and reports of users about misspelled or offensive words. These two factors contributed to a sort of quality score. A word was used as cue by the system only if its quality score was above a given threshold. We used the same strategy to filter words in the first part of our study. In the second part, the matching procedure with WordNet itself provided a filtering mechanism, as invalid words do not have a match in WordNet.</p>
<h1>Appendix B. A limited Heaps' law</h1>
<p>To obtain Eq. (2), we start from the assumption that target words to a given cue have a power law frequency rank. This assumption is justified by the measured average frequency rank displayed in Fig. 5.</p>
<p>Let us now we describe the problem in an abstract way. We consider a set of $V$ events, each with a given probability. These probabilities, arranged in decreasing order, form a power law of the form:</p>
<p>$$
f_{t h}(r)=A(V, B) \cdot r^{-B}
$$</p>
<p>where $r$ is the rank, $A(V, B)$ is the normalization coefficient and $B$ is a parameter of the distribution. We perform $s$ extractions of such events, and we want to compute the number $k$ of distinct events obtained. After $s$ extractions, the minimum finite value for the frequency of an event is $1 / s$. The empirically obtained frequency rank will therefore give a curve such as the ones of Fig. 5, i.e., a power-law decay followed by a plateau at $1 / s$. Only the probabilities larger than $1 / s$ can thus be correctly estimated. The number of distinct events with probability larger than $1 / s, n_{&gt;1 / s}$, is given by the rank corresponding to the probability $1 / s$, i.e., from Eq. (B.1):</p>
<p>$$
f(r)=A(V, B) \cdot r^{-B} \simeq 1 / s \Rightarrow n_{&gt;1 / s} \simeq r(1 / s)=(s \cdot A)^{1 / B}
$$</p>
<p>We also have to estimate the number of distinct events which have been extracted although their probability is lower than $1 / s$, as the cumulative probability</p>
<p>$$
P_{&lt;1 / s}=\sum_{r_{1 / s}}^{V} f(r)
$$</p>
<p>can be larger than $1 / s$.
We can reasonably assume that these events are extracted at most once, and estimate their number $n_{&lt;1 / s}$ as:</p>
<p>$$
n_{&lt;1 / s} \simeq P_{&lt;1 / s} \cdot s
$$</p>
<p>The total number of distinct events observed $k$ after $s$ extractions is then obtained by summing (B.2) and (B.4):</p>
<p>$$
k(s) \simeq n_{&lt;1 / s}+n_{&gt;1 / s}=P_{&lt;1 / s} \cdot s+(s \cdot A)^{1 / B}
$$</p>
<p>After straightforward computations we obtain</p>
<p>$$
k(s)=V \cdot \frac{B\left(s / s_{l i m}\right)^{1 / B}-s / s_{l i m}}{B-1}
$$</p>
<p>where $s_{l i m}=V^{B} / A=(V^{B}-V) /(B-1)$, which is equivalent to Eq. 2.
The treatment presented so far does not include the case $B=1$. In this case it is easy to find:</p>
<p>$$
k(s)=V \frac{s}{s_{l i m}}\left(1-\log \left(\frac{s}{s_{l i m}}\right)\right)
$$</p>
<p>It is worth to note that, if $V \gg s$ one recovers the Heaps' law, which is a particular case of Eq. 2. In this limit, in fact, $s / s_{\text {lim }}&lt;&lt;1$ and the dominant term in B. 6 is the one with exponent $1 / B$ for $B&gt;1$ and the linear one for $B&lt;1$. For $B&gt;1$ one recovers the right behaviour with a power-law with an exponent $1 / B$ :</p>
<p>$$
k(s) \sim V \frac{B}{B-1} \frac{s^{1 / B}}{s_{\text {lim }}}
$$</p>
<p>while for $B&lt;1$ one recover a linear behavior:</p>
<p>$$
k(s) \sim \frac{V}{1-B} \frac{s}{s_{\text {lim }}}
$$</p>
<p>Of course in the limit $V \rightarrow \infty$ one is able to observe these behaviours in a very large range of values for $s$.</p>
<h1>Appendix C. Wordnet semantic relations abbreviations</h1>
<p>Here we report a list of the semantic relations recognized by Wordnet with their abbreviation. For further information we refer to the Wordnet documentation [15, $17,16]$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Semantic relation</th>
<th style="text-align: left;">Abbreviation</th>
<th style="text-align: left;">E.g.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">keyword</td>
<td style="text-align: left;">KEY</td>
<td style="text-align: left;">"cold" is the keyword for "icy"</td>
</tr>
<tr>
<td style="text-align: left;">synonym</td>
<td style="text-align: left;">SYN</td>
<td style="text-align: left;">"auto" is a synonym of "car"</td>
</tr>
<tr>
<td style="text-align: left;">antonym</td>
<td style="text-align: left;">ANT</td>
<td style="text-align: left;">"up" is an antonym of "down"</td>
</tr>
<tr>
<td style="text-align: left;">hypernym</td>
<td style="text-align: left;">HYPER</td>
<td style="text-align: left;">"tree" is an hypernym of "oak"</td>
</tr>
<tr>
<td style="text-align: left;">hyponym</td>
<td style="text-align: left;">HYPO</td>
<td style="text-align: left;">"trout" is an hyponym of "fish"</td>
</tr>
<tr>
<td style="text-align: left;">entails</td>
<td style="text-align: left;">ENT</td>
<td style="text-align: left;">"dream" entails "sleep"</td>
</tr>
<tr>
<td style="text-align: left;">similar</td>
<td style="text-align: left;">SIM</td>
<td style="text-align: left;">"mistaken" is similar to "wrong"</td>
</tr>
<tr>
<td style="text-align: left;">is member</td>
<td style="text-align: left;">ISMEM</td>
<td style="text-align: left;">"juror" is member of "jury"</td>
</tr>
<tr>
<td style="text-align: left;">is material</td>
<td style="text-align: left;">ISMAT</td>
<td style="text-align: left;">"iron" is material of "steel"</td>
</tr>
<tr>
<td style="text-align: left;">is part</td>
<td style="text-align: left;">ISPAR</td>
<td style="text-align: left;">"month" is part of an "year"</td>
</tr>
<tr>
<td style="text-align: left;">has member</td>
<td style="text-align: left;">HASMEM</td>
<td style="text-align: left;">"fleet" has "ship" as member</td>
</tr>
<tr>
<td style="text-align: left;">has material</td>
<td style="text-align: left;">HASMAT</td>
<td style="text-align: left;">"air" has "oxygen" as material</td>
</tr>
<tr>
<td style="text-align: left;">has part</td>
<td style="text-align: left;">HASPAR</td>
<td style="text-align: left;">"hour" has "minute" as part</td>
</tr>
<tr>
<td style="text-align: left;">cause to</td>
<td style="text-align: left;">CAU</td>
<td style="text-align: left;">"teach" cause to "learn"</td>
</tr>
<tr>
<td style="text-align: left;">is participle</td>
<td style="text-align: left;">PRTC</td>
<td style="text-align: left;">"forced" is participle of "force"</td>
</tr>
<tr>
<td style="text-align: left;">see also</td>
<td style="text-align: left;">SEE</td>
<td style="text-align: left;">"race" see also "speed"</td>
</tr>
<tr>
<td style="text-align: left;">refers to</td>
<td style="text-align: left;">REF</td>
<td style="text-align: left;">"italian" refers to "Italy"</td>
</tr>
<tr>
<td style="text-align: left;">is attribute</td>
<td style="text-align: left;">ATTR</td>
<td style="text-align: left;">"height" is the attribute of "tall"</td>
</tr>
<tr>
<td style="text-align: left;">verb group</td>
<td style="text-align: left;">VERBG</td>
<td style="text-align: left;">"incinerate" belongs to <br> the verb group of "burn"</td>
</tr>
<tr>
<td style="text-align: left;">derivation</td>
<td style="text-align: left;">DER</td>
<td style="text-align: left;">"sailor" derives from "sail"</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{e}$ Hyperonymy is the link between a word with a particular meaning and another word with a more general meaning which includes the first one; e.g. "red" is the hyperonym of "scarlet".
${ }^{d}$ Hyponymy is the inverse relation of the hyperonymy.
${ }^{\text {e }}$ Holonymy is the link between a word denoting a part of a whole and the word denoting the whole itself; e.g. "hand" is holonym of "finger".
${ }^{\mathrm{f}}$ Meronymy is the inverse relation of the holonymy.
${ }^{\mathrm{g}}$ Synonymy is the link between two words with the same meaning; e.g., "student" and "pupil".&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
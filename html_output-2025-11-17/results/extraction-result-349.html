<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-349 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-349</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-349</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-256390546</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2301.13166v3.pdf" target="_blank">ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation</a></p>
                <p><strong>Paper Abstract:</strong> The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method improves significantly over baselines, and achieves new state-of-the-art results for zero-shot object navigation (e.g., 288% relative Success Rate improvement than CoW on MP3D).</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e349.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e349.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deberta</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa v3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained transformer-based language model used here for zero-shot commonsense reasoning about object-room and object-object co-occurrence; it is prompted with natural-language questions and returns real-valued scores representing likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa v3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer (DeBERTa v3) with replaced-token-detection and disentangled attention; pretraining included commonsense QA data (CSQA) and the model is used without fine-tuning to score candidate objects/rooms given a goal via two short question templates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot Object Goal Navigation (MP3D / HM3D / RoboTHOR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent must find an instance of a specified object category in an unseen indoor environment using egocentric RGB-D and pose/GPS when available; here DeBERTa supplies commonsense priors (S(G|o_i), S(G|r_i)) about where goal objects are likely to appear and near which objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora and CSQA-style commonsense QA (zero-shot prompting at inference)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (two question templates for object-level and room-level likelihoods)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit knowledge in model weights elicited as real-valued co-occurrence scores S(Goal|Object) and S(Goal|Room) in [0,1]; these scores are used as soft predicate values (IsCooccur) in PSL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) and Success weighted by inverse Path Length (SPL) of the full ESC agent using DeBERTa for commonsense</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used as the LLM in ESC (zero-shot, no navigation training): MP3D ESC (DeBERTa) SPL=14.2, SR=28.7; HM3D ESC SPL=22.3, SR=39.2; RoboTHOR ESC SPL=22.2, SR=38.1. (Compared to GLIP-only baseline GoW on MP3D: SPL=12.4, SR=25.6; CoW (CLIP) MP3D: SPL=3.7, SR=7.4.)</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provided useful object-room co-occurrence priors that (a) when combined with map context led to selection of frontiers that were on average closer to goal objects (FrontierDist reduced from 8.2m to 7.6m on HM3D) and (b) reduced exploration errors; room-level reasoning produced larger gains than object-level in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>LLM priors are probabilistic and not deterministic—incorrect or noisy co-occurrence scores can mislead exploration; largest remaining failure mode in experiments was detection error (vision), not the LLM's scores; ESC performance depends on accuracy of GLIP detections that are inputs to LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GLIP w/o commonsense (GoW) on MP3D: GoW SPL=12.4 SR=25.6; CLIP w/o commonsense (CoW) MP3D SPL=3.7 SR=7.4. ESC with DeBERTa improves over GoW and substantially over CoW.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing room-level or object-level commonsense: ESC w/o Room (MP3D) SPL=13.8 SR=28.3; ESC w/o Object (MP3D) SPL=13.7 SR=27.8. Using both object+room yields best overall on MP3D/HM3D (small but consistent gains).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A general LLM (DeBERTa) stores useful spatial and object-relational priors that can be elicited zero-shot via simple prompts and converted to numeric likelihoods; these priors, when encoded as continuous soft predicates, improve exploration decisions and navigation success without any navigation-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e349.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e349.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (conversational LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational large language model used as an alternative LLM to produce room- and object-level likelihood scores for where a goal object is likely to be; performs comparably to DeBERTa in the ESC pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained conversational LLM (OpenAI ChatGPT); used zero-shot with prompts asking which candidate rooms/objects are likely for a given goal and to output scores; no fine-tuning for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot Object Goal Navigation (HM3D experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same object-goal navigation task as above; ChatGPT is queried to provide likelihood scores for (goal,object) and (goal,room) pairs used by PSL to guide frontier selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora and instruction tuning (zero-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (natural language questions requesting scores)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in model weights; elicited as numeric likelihood scores (used as IsCooccur values in PSL) or textual judgments converted to normalized scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR and SPL of full ESC agent when ChatGPT supplies the co-occurrence priors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported as comparable to DeBERTa on HM3D: object+room reasoning with ChatGPT gave similar SPL/SR (e.g., Obj+Room ChatGPT ~ SPL=22.4, SR~39.0 on HM3D), indicating near parity with DeBERTa in this pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>ChatGPT provided informative priors even without dedicated commonsense fine-tuning; room-level judgments were effective for guiding exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Same limitations as other LLMs: probabilistic/uncertain outputs; sensitivity to prompt phrasing and to quality of upstream visual detections.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against DeBERTa in ablation (Table 3): both significantly outperform GLIP-only GoW baseline; ChatGPT's performance is similar to DeBERTa's in the ESC pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using ChatGPT for only room-level or only object-level reasoning yields improvements over no-commonsense baseline, similar pattern to DeBERTa (room-level often producing larger gains).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large conversational LMs can supply usable spatial and object-relational priors zero-shot; choice between DeBERTa and ChatGPT had small impact, suggesting commonsense priors are robust across different LLM families when prompted appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e349.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e349.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IsCooccur / S(G|·)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-derived co-occurrence scores (IsCooccur predicates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Real-valued scores in [0,1] produced by LLM prompts representing the likelihood that a goal object co-occurs with a particular object or appears in a particular room; these scores are used as soft logical predicates in PSL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-derived co-occurrence scores (IsCooccur)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a representation: numeric outputs from LLM prompts (e.g., normalized DeBERTa or ChatGPT scores) that quantify commonsense spatial/object-relational associations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Frontier selection inside ESC for zero-shot object navigation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Scores S(G|o_i) and S(G|r_i) encode the LLM's belief that the goal object G is near object o_i or in room r_i; these are input observed variables X to PSL ground atoms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>elicited from LLMs via zero-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot natural-language questions; predicted scores are linearly normalized to [0,1]</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Continuous scalar predicates (IsCooccur) that feed into PSL rules; they represent probabilistic commonsense relations rather than hard symbolic facts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Indirect — their effectiveness is measured by improvements in navigation SR/SPL when used by PSL to select frontiers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using these scores in PSL produced ESC improvements vs GLIP-only: e.g., MP3D SPL improved from 12.4 (GoW) to 14.2 (ESC with both object+room scores).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Scores reliably raised preference for frontiers within/near semantically-relevant rooms or objects, reducing mean distance to target frontiers and lowering exploration error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Noisy or overly confident scores can bias exploration incorrectly; scores depend on the LLM and prompt quality and do not incorporate immediate sensory uncertainty beyond what GLIP supplies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to not using scores (GoW) and to heuristics that pick closest frontier, the use of S(G|·) leads to better frontier choices and higher SR/SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Dropping either object-level or room-level scores reduces performance (see ESC w/o Room and ESC w/o Object entries); room-level removal often hurts more than object-level removal.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transforming LLM judgments into continuous predicates provides a practical interface between language priors and probabilistic decision rules, enabling uncertain commonsense knowledge to be combined with geometric map information for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e349.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e349.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Soft Logic (PSL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-valued, first-order-logic-based probabilistic framework (hinge-loss MRFs) used to convert LLM-derived soft commonsense rules and geometric features into a single scoring objective for frontier selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hinge-loss markov random fields and probabilistic soft logic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Probabilistic Soft Logic (PSL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Declarative template language that encodes weighted first-order logical rules as hinge-loss potentials under Łukasiewicz continuous-valued logic; inference is convex optimization over continuous atom values in [0,1] (authors also use a one-hot constrained solver for final discrete frontier choice).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Frontier selection for exploration in zero-shot object navigation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>PSL ingests observed atoms (IsCooccur scores from LLMs, IsNearObj from GLIP map) and target atoms (ChooseFrontier) and optimizes a weighted sum of rule-penalty potentials to pick the best frontier to explore next.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit soft logic rules combining LLM priors, object proximity from GLIP, and geometric distance heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>optimization-based inference (HL-MRF convex optimization or one-hot enumeration of frontier choices)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit continuous symbolic predicates (IsCooccur, IsNearObj, ShortDist, ChooseFrontier) encoded as weighted logical clauses and hinge-loss potentials; represents uncertain relational knowledge numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Agent SR / SPL when PSL is used to select frontiers; also inference time per decision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>PSL-based frontier selection in ESC yields higher SR/SPL than heuristic closest-frontier: e.g., ESC (one-hot PSL) MP3D SPL=13.7 SR=27.8 versus ADMM PSL MP3D SPL=12.9 SR=27.0; one-hot solver inference time 0.25s vs ADMM 2.13s per inference.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Soft logic allowed uncertain LLM priors to be combined with geometric priors (distance) and object proximity to prefer semantically promising frontiers; one-hot constrained search matched the discrete decision problem and ran faster.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>PSL's effectiveness limited by quality of observed inputs (GLIP detections, LLM scores) and by fixed rule weights; current ESC uses fixed weights (e.g., doubling distance weight when both commonsense levels used) which may not be optimal across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to vanilla frontier-based exploration (closest-frontier heuristic) and to using PSL solved with ADMM, the one-hot-constrained PSL used in ESC gave slightly better navigation metrics and much faster inference.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Switching solver from ADMM to one-hot improved SPL (ADMM SPL=12.9 -> One-hot SPL=13.7 on MP3D) and reduced inference time (2.13s -> 0.25s). Removing object or room rules reduces agent performance (see ESC ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoding LLM-derived soft commonsense as continuous logical predicates in PSL is an effective way to convert probabilistic linguistic priors into actionable navigation preferences; combining these with geometric constraints yields measurable gains in zero-shot exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e349.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e349.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLIP-L (Grounded Language-Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grounded vision-language model used to perform open-vocabulary prompt-based object and room detection from egocentric RGB images; detections are projected into a semantic map used by PSL and the LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grounded language-image pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLIP-L</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language grounding model that aligns image regions with text prompts to produce open-world detections (object and room labels with bounding boxes and confidence scores); used with prompt lists of candidate objects and rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-world semantic scene understanding for object navigation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given egocentric RGB (and depth for projection), GLIP detects objects and rooms and outputs labels and bounding boxes; these are projected to 2D semantic/navigation maps and provide IsNearObj and room occupancy evidence used by PSL and the LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation / perception</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (detection and spatial localization of objects/rooms)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on large image-text datasets (open-vocab grounding via prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompt-based grounding using text prompts (union of common indoor objects and goal objects; common rooms prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Detected object/room labels and bounding boxes with confidence scores, projected into a 3D voxel then a 2D navigation/semantic map (object center projected, room pixels projected), which yields IsNearObj and IsInRoom evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used indirectly by navigation metrics (SR/SPL); GLIP-based GoW baseline provides ablation comparison to CLIP-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GLIP w/o commonsense (GoW) substantially outperforms CLIP-based baseline (CoW): on MP3D GoW SPL=12.4 SR=25.6 vs CoW SPL=3.7 SR=7.4; integrating GLIP detections with PSL and LLMs yields further gains (ESC).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Open-vocabulary detection generalized to novel objects and provided rich scene context (objects and rooms) that LLMs could reason over; projection to semantic map enabled spatial reasoning about object proximity to frontiers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Detection errors (false negatives or false positives) are the single largest cause of agent failures in experiments; GLIP thresholds and depth-based projection limitations (e.g., depth range) affect map accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to CLIP-based Grad-CAM localization used in CoW, GLIP provided stronger open-world grounding and led to much higher navigation success even without commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing GLIP with CLIP (CoW) yields a large drop in SPL/SR; GLIP plus vanilla frontier heuristic (GoW) already gives substantial improvement over CoW, indicating the importance of richer visual-language grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing structured, open-vocabulary visual-semantic observations (objects and rooms) is necessary for LLMs to supply meaningful spatial/object-relational priors; GLIP's outputs form the bridge from raw vision to language-based commonsense priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation <em>(Rating: 2)</em></li>
                <li>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings <em>(Rating: 2)</em></li>
                <li>Grounded language-image pre-training <em>(Rating: 2)</em></li>
                <li>Hinge-loss markov random fields and probabilistic soft logic <em>(Rating: 2)</em></li>
                <li>Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing <em>(Rating: 1)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 1)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-349",
    "paper_id": "paper-256390546",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Deberta",
            "name_full": "DeBERTa v3",
            "brief_description": "A pretrained transformer-based language model used here for zero-shot commonsense reasoning about object-room and object-object co-occurrence; it is prompted with natural-language questions and returns real-valued scores representing likelihoods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTa v3",
            "model_size": null,
            "model_description": "Pretrained transformer (DeBERTa v3) with replaced-token-detection and disentangled attention; pretraining included commonsense QA data (CSQA) and the model is used without fine-tuning to score candidate objects/rooms given a goal via two short question templates.",
            "task_name": "Zero-shot Object Goal Navigation (MP3D / HM3D / RoboTHOR)",
            "task_description": "Agent must find an instance of a specified object category in an unseen indoor environment using egocentric RGB-D and pose/GPS when available; here DeBERTa supplies commonsense priors (S(G|o_i), S(G|r_i)) about where goal objects are likely to appear and near which objects.",
            "task_type": "navigation",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "pre-training on large text corpora and CSQA-style commonsense QA (zero-shot prompting at inference)",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting (two question templates for object-level and room-level likelihoods)",
            "knowledge_representation": "Implicit knowledge in model weights elicited as real-valued co-occurrence scores S(Goal|Object) and S(Goal|Room) in [0,1]; these scores are used as soft predicate values (IsCooccur) in PSL.",
            "performance_metric": "Success Rate (SR) and Success weighted by inverse Path Length (SPL) of the full ESC agent using DeBERTa for commonsense",
            "performance_result": "When used as the LLM in ESC (zero-shot, no navigation training): MP3D ESC (DeBERTa) SPL=14.2, SR=28.7; HM3D ESC SPL=22.3, SR=39.2; RoboTHOR ESC SPL=22.2, SR=38.1. (Compared to GLIP-only baseline GoW on MP3D: SPL=12.4, SR=25.6; CoW (CLIP) MP3D: SPL=3.7, SR=7.4.)",
            "success_patterns": "Provided useful object-room co-occurrence priors that (a) when combined with map context led to selection of frontiers that were on average closer to goal objects (FrontierDist reduced from 8.2m to 7.6m on HM3D) and (b) reduced exploration errors; room-level reasoning produced larger gains than object-level in many settings.",
            "failure_patterns": "LLM priors are probabilistic and not deterministic—incorrect or noisy co-occurrence scores can mislead exploration; largest remaining failure mode in experiments was detection error (vision), not the LLM's scores; ESC performance depends on accuracy of GLIP detections that are inputs to LLM prompts.",
            "baseline_comparison": "Compared to GLIP w/o commonsense (GoW) on MP3D: GoW SPL=12.4 SR=25.6; CLIP w/o commonsense (CoW) MP3D SPL=3.7 SR=7.4. ESC with DeBERTa improves over GoW and substantially over CoW.",
            "ablation_results": "Removing room-level or object-level commonsense: ESC w/o Room (MP3D) SPL=13.8 SR=28.3; ESC w/o Object (MP3D) SPL=13.7 SR=27.8. Using both object+room yields best overall on MP3D/HM3D (small but consistent gains).",
            "key_findings": "A general LLM (DeBERTa) stores useful spatial and object-relational priors that can be elicited zero-shot via simple prompts and converted to numeric likelihoods; these priors, when encoded as continuous soft predicates, improve exploration decisions and navigation success without any navigation-specific training.",
            "uuid": "e349.0",
            "source_info": {
                "paper_title": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (conversational LLM)",
            "brief_description": "A conversational large language model used as an alternative LLM to produce room- and object-level likelihood scores for where a goal object is likely to be; performs comparably to DeBERTa in the ESC pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_size": null,
            "model_description": "Pretrained conversational LLM (OpenAI ChatGPT); used zero-shot with prompts asking which candidate rooms/objects are likely for a given goal and to output scores; no fine-tuning for navigation.",
            "task_name": "Zero-shot Object Goal Navigation (HM3D experiments reported)",
            "task_description": "Same object-goal navigation task as above; ChatGPT is queried to provide likelihood scores for (goal,object) and (goal,room) pairs used by PSL to guide frontier selection.",
            "task_type": "navigation",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "pre-training on large text corpora and instruction tuning (zero-shot prompting)",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting (natural language questions requesting scores)",
            "knowledge_representation": "Implicit in model weights; elicited as numeric likelihood scores (used as IsCooccur values in PSL) or textual judgments converted to normalized scores.",
            "performance_metric": "SR and SPL of full ESC agent when ChatGPT supplies the co-occurrence priors",
            "performance_result": "Reported as comparable to DeBERTa on HM3D: object+room reasoning with ChatGPT gave similar SPL/SR (e.g., Obj+Room ChatGPT ~ SPL=22.4, SR~39.0 on HM3D), indicating near parity with DeBERTa in this pipeline.",
            "success_patterns": "ChatGPT provided informative priors even without dedicated commonsense fine-tuning; room-level judgments were effective for guiding exploration.",
            "failure_patterns": "Same limitations as other LLMs: probabilistic/uncertain outputs; sensitivity to prompt phrasing and to quality of upstream visual detections.",
            "baseline_comparison": "Compared against DeBERTa in ablation (Table 3): both significantly outperform GLIP-only GoW baseline; ChatGPT's performance is similar to DeBERTa's in the ESC pipeline.",
            "ablation_results": "Using ChatGPT for only room-level or only object-level reasoning yields improvements over no-commonsense baseline, similar pattern to DeBERTa (room-level often producing larger gains).",
            "key_findings": "Large conversational LMs can supply usable spatial and object-relational priors zero-shot; choice between DeBERTa and ChatGPT had small impact, suggesting commonsense priors are robust across different LLM families when prompted appropriately.",
            "uuid": "e349.1",
            "source_info": {
                "paper_title": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "IsCooccur / S(G|·)",
            "name_full": "LLM-derived co-occurrence scores (IsCooccur predicates)",
            "brief_description": "Real-valued scores in [0,1] produced by LLM prompts representing the likelihood that a goal object co-occurs with a particular object or appears in a particular room; these scores are used as soft logical predicates in PSL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM-derived co-occurrence scores (IsCooccur)",
            "model_size": null,
            "model_description": "Not a model but a representation: numeric outputs from LLM prompts (e.g., normalized DeBERTa or ChatGPT scores) that quantify commonsense spatial/object-relational associations.",
            "task_name": "Frontier selection inside ESC for zero-shot object navigation",
            "task_description": "Scores S(G|o_i) and S(G|r_i) encode the LLM's belief that the goal object G is near object o_i or in room r_i; these are input observed variables X to PSL ground atoms.",
            "task_type": "navigation",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "elicited from LLMs via zero-shot prompts",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot natural-language questions; predicted scores are linearly normalized to [0,1]",
            "knowledge_representation": "Continuous scalar predicates (IsCooccur) that feed into PSL rules; they represent probabilistic commonsense relations rather than hard symbolic facts.",
            "performance_metric": "Indirect — their effectiveness is measured by improvements in navigation SR/SPL when used by PSL to select frontiers.",
            "performance_result": "Using these scores in PSL produced ESC improvements vs GLIP-only: e.g., MP3D SPL improved from 12.4 (GoW) to 14.2 (ESC with both object+room scores).",
            "success_patterns": "Scores reliably raised preference for frontiers within/near semantically-relevant rooms or objects, reducing mean distance to target frontiers and lowering exploration error rates.",
            "failure_patterns": "Noisy or overly confident scores can bias exploration incorrectly; scores depend on the LLM and prompt quality and do not incorporate immediate sensory uncertainty beyond what GLIP supplies.",
            "baseline_comparison": "Compared to not using scores (GoW) and to heuristics that pick closest frontier, the use of S(G|·) leads to better frontier choices and higher SR/SPL.",
            "ablation_results": "Dropping either object-level or room-level scores reduces performance (see ESC w/o Room and ESC w/o Object entries); room-level removal often hurts more than object-level removal.",
            "key_findings": "Transforming LLM judgments into continuous predicates provides a practical interface between language priors and probabilistic decision rules, enabling uncertain commonsense knowledge to be combined with geometric map information for exploration.",
            "uuid": "e349.2",
            "source_info": {
                "paper_title": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "PSL",
            "name_full": "Probabilistic Soft Logic (PSL)",
            "brief_description": "A continuous-valued, first-order-logic-based probabilistic framework (hinge-loss MRFs) used to convert LLM-derived soft commonsense rules and geometric features into a single scoring objective for frontier selection.",
            "citation_title": "Hinge-loss markov random fields and probabilistic soft logic",
            "mention_or_use": "use",
            "model_name": "Probabilistic Soft Logic (PSL)",
            "model_size": null,
            "model_description": "Declarative template language that encodes weighted first-order logical rules as hinge-loss potentials under Łukasiewicz continuous-valued logic; inference is convex optimization over continuous atom values in [0,1] (authors also use a one-hot constrained solver for final discrete frontier choice).",
            "task_name": "Frontier selection for exploration in zero-shot object navigation",
            "task_description": "PSL ingests observed atoms (IsCooccur scores from LLMs, IsNearObj from GLIP map) and target atoms (ChooseFrontier) and optimizes a weighted sum of rule-penalty potentials to pick the best frontier to explore next.",
            "task_type": "navigation",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "explicit soft logic rules combining LLM priors, object proximity from GLIP, and geometric distance heuristics",
            "has_direct_sensory_input": false,
            "elicitation_method": "optimization-based inference (HL-MRF convex optimization or one-hot enumeration of frontier choices)",
            "knowledge_representation": "Explicit continuous symbolic predicates (IsCooccur, IsNearObj, ShortDist, ChooseFrontier) encoded as weighted logical clauses and hinge-loss potentials; represents uncertain relational knowledge numerically.",
            "performance_metric": "Agent SR / SPL when PSL is used to select frontiers; also inference time per decision",
            "performance_result": "PSL-based frontier selection in ESC yields higher SR/SPL than heuristic closest-frontier: e.g., ESC (one-hot PSL) MP3D SPL=13.7 SR=27.8 versus ADMM PSL MP3D SPL=12.9 SR=27.0; one-hot solver inference time 0.25s vs ADMM 2.13s per inference.",
            "success_patterns": "Soft logic allowed uncertain LLM priors to be combined with geometric priors (distance) and object proximity to prefer semantically promising frontiers; one-hot constrained search matched the discrete decision problem and ran faster.",
            "failure_patterns": "PSL's effectiveness limited by quality of observed inputs (GLIP detections, LLM scores) and by fixed rule weights; current ESC uses fixed weights (e.g., doubling distance weight when both commonsense levels used) which may not be optimal across domains.",
            "baseline_comparison": "Compared to vanilla frontier-based exploration (closest-frontier heuristic) and to using PSL solved with ADMM, the one-hot-constrained PSL used in ESC gave slightly better navigation metrics and much faster inference.",
            "ablation_results": "Switching solver from ADMM to one-hot improved SPL (ADMM SPL=12.9 -&gt; One-hot SPL=13.7 on MP3D) and reduced inference time (2.13s -&gt; 0.25s). Removing object or room rules reduces agent performance (see ESC ablations).",
            "key_findings": "Encoding LLM-derived soft commonsense as continuous logical predicates in PSL is an effective way to convert probabilistic linguistic priors into actionable navigation preferences; combining these with geometric constraints yields measurable gains in zero-shot exploration.",
            "uuid": "e349.3",
            "source_info": {
                "paper_title": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "GLIP",
            "name_full": "GLIP-L (Grounded Language-Image Pre-training)",
            "brief_description": "A grounded vision-language model used to perform open-vocabulary prompt-based object and room detection from egocentric RGB images; detections are projected into a semantic map used by PSL and the LLM prompts.",
            "citation_title": "Grounded language-image pre-training",
            "mention_or_use": "use",
            "model_name": "GLIP-L",
            "model_size": null,
            "model_description": "Vision-language grounding model that aligns image regions with text prompts to produce open-world detections (object and room labels with bounding boxes and confidence scores); used with prompt lists of candidate objects and rooms.",
            "task_name": "Open-world semantic scene understanding for object navigation",
            "task_description": "Given egocentric RGB (and depth for projection), GLIP detects objects and rooms and outputs labels and bounding boxes; these are projected to 2D semantic/navigation maps and provide IsNearObj and room occupancy evidence used by PSL and the LLM prompts.",
            "task_type": "navigation / perception",
            "knowledge_type": "spatial+object-relational (detection and spatial localization of objects/rooms)",
            "knowledge_source": "pretraining on large image-text datasets (open-vocab grounding via prompts)",
            "has_direct_sensory_input": true,
            "elicitation_method": "prompt-based grounding using text prompts (union of common indoor objects and goal objects; common rooms prompt)",
            "knowledge_representation": "Detected object/room labels and bounding boxes with confidence scores, projected into a 3D voxel then a 2D navigation/semantic map (object center projected, room pixels projected), which yields IsNearObj and IsInRoom evidence.",
            "performance_metric": "Used indirectly by navigation metrics (SR/SPL); GLIP-based GoW baseline provides ablation comparison to CLIP-based baseline.",
            "performance_result": "GLIP w/o commonsense (GoW) substantially outperforms CLIP-based baseline (CoW): on MP3D GoW SPL=12.4 SR=25.6 vs CoW SPL=3.7 SR=7.4; integrating GLIP detections with PSL and LLMs yields further gains (ESC).",
            "success_patterns": "Open-vocabulary detection generalized to novel objects and provided rich scene context (objects and rooms) that LLMs could reason over; projection to semantic map enabled spatial reasoning about object proximity to frontiers.",
            "failure_patterns": "Detection errors (false negatives or false positives) are the single largest cause of agent failures in experiments; GLIP thresholds and depth-based projection limitations (e.g., depth range) affect map accuracy.",
            "baseline_comparison": "Compared to CLIP-based Grad-CAM localization used in CoW, GLIP provided stronger open-world grounding and led to much higher navigation success even without commonsense reasoning.",
            "ablation_results": "Replacing GLIP with CLIP (CoW) yields a large drop in SPL/SR; GLIP plus vanilla frontier heuristic (GoW) already gives substantial improvement over CoW, indicating the importance of richer visual-language grounding.",
            "key_findings": "Providing structured, open-vocabulary visual-semantic observations (objects and rooms) is necessary for LLMs to supply meaningful spatial/object-relational priors; GLIP's outputs form the bridge from raw vision to language-based commonsense priors.",
            "uuid": "e349.4",
            "source_info": {
                "paper_title": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation",
            "rating": 2,
            "sanitized_title": "cows_on_pasture_baselines_and_benchmarks_for_languagedriven_zeroshot_object_navigation"
        },
        {
            "paper_title": "ZSON: Zero-shot object-goal navigation using multimodal goal embeddings",
            "rating": 2,
            "sanitized_title": "zson_zeroshot_objectgoal_navigation_using_multimodal_goal_embeddings"
        },
        {
            "paper_title": "Grounded language-image pre-training",
            "rating": 2,
            "sanitized_title": "grounded_languageimage_pretraining"
        },
        {
            "paper_title": "Hinge-loss markov random fields and probabilistic soft logic",
            "rating": 2,
            "sanitized_title": "hingeloss_markov_random_fields_and_probabilistic_soft_logic"
        },
        {
            "paper_title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
            "rating": 1,
            "sanitized_title": "debertav3_improving_deberta_using_electrastyle_pretraining_with_gradientdisentangled_embedding_sharing"
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 1,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        }
    ],
    "cost": 0.017548,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation</p>
<p>Kaiwen Zhou 
Kaizhi Zheng 
Connor Pryor 
Yilin Shen 
Hongxia Jin 
Lise Getoor 
Xin Eric Wang 
ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation</p>
<p>The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for openworld prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D (Chang et al.,  2017), HM3D (Ramakrishnan et al., 2021), and  RoboTHOR (Deitke et al., 2020)  benchmarks show that our ESC method improves significantly over baselines, and achieves new state-of-theart results for zero-shot object navigation (e.g., 288% relative Success Rate improvement than CoW (Gadre et al., 2023) on MP3D). 1</p>
<p>Introduction</p>
<p>Object navigation (ObjNav) is a task in which an embodied agent must navigate to a specific goal object within an unknown environment   Figure 1. Commonsense reasoning in object navigation. In object navigation, our agent first does a semantic understanding of the current scene (red text in the figure) and then performs commonsense reasoning (blue text in the figure). The agent reasons that a fireplace is likely to be in a living room, so it decides to explore the unobserved part of the living room (the frontier adjacent to the observed part of the living room).</p>
<p>is fundamental to other navigation-based embodied tasks because navigating to a goal object is the preliminary for the agent to interact with it. While current state-of-the-art methods for object navigation achieve good results when trained on specific datasets with limited goal objects and similar environments, they often perform poorly when faced with novel objects or environments due to distribution shifts. Real-world situations often involve diverse objects and varied environments, making it difficult and costly to collect extensive, annotated trajectory data. As a result, generalized zero-shot object navigation, in which the navigation agent can adapt to novel objects and environments without additional training, is a crucial area of study.</p>
<p>Successfully navigating to a goal object requires two key abilities, (1) semantic scene understanding, which involves identifying objects and rooms in the environment, and (2) commonsense reasoning, which involves making logical inferences about the location of the goal object based on commonsense knowledge. For example, as in Fig. 1, a fireplace is very likely in a living room, so the agent decides to explore the unseen area in the living room to find a fireplace. However, current zero-shot object navigation methods have not yet effectively addressed this requirement and often lack commonsense reasoning abilities. Existing methods require training on other goal-oriented navigation tasks and environments (Majumdar et al., 2022;Al-Halah et al., 2022), or use simple heuristics for exploration (Gadre et al., 2023).</p>
<p>Recent studies (He et al., 2021;Radford et al., 2021;Kojima et al., 2022;Li<em> et al., 2022) show that large pre-trained models have a strong generalization and reasoning ability for novel tasks under zero-shot scenarios. Building upon this success, in this work, we propose a zero-shot object navigation framework, named Exploration with Soft Commonsense constraints (ESC), that leverages these pre-trained models and can seamlessly generalize to unseen environments and novel object types. As shown in Fig. 1, we first use a prompt-based vision-and-language grounding model GLIP (Li</em> et al., 2022) for open-world object grounding and scene understanding, which can infer the object and room information of current agent views. Benefiting from large-scale image-text pre-training, GLIP can easily generalize to new objects via prompting. Then, we utilize a pre-trained commonsense reasoning language model that takes the room and object information as context to infer the correspondence between rooms and objects.</p>
<p>However, there still remains a gap in converting the commonsense knowledge inferred from large language models (LLMs) into executable actions. In addition, the relationship between entities is usually uncertain, e.g., the book has a high probability in the living room, but it is not deterministic. To address these challenges, our ESC method models "soft" commonsense constraints using Probabilistic Soft Logic (PSL) (Bach et al., 2017), a declarative templating language that defines a special class of Markov random fields with first-order logical rules. Those soft commonsense constraints are then incorporated into a classic exploration method, frontier-based exploration (FBE), to determine which frontier to explore next in a zero-shot manner. Unlike previous methods that rely on implicit training of commonsense using neural networks (Yang et al., 2019;Chaplot et al., 2020a), our method explicitly uses soft logic predicates to represent knowledge in a continuous value space, which is then assigned to each frontier, enabling more effective exploration.</p>
<p>We demonstrate the effectiveness of our framework on three object goal navigation benchmarks, MP3D (Chang et al., 2017), HM3D (Ramakrishnan et al., 2021), and RoboTHOR (Deitke et al., 2020), with different house sizes, styles, texture features, and object types. Compared with CoW (Gadre et al., 2023) that has the same setting as ours, our method achieves around 285% relative improvement in success rate weighted by length (SPL) and success rate (SR) on MP3D and 35% relative improvement in SPL and SR on RoboTHOR. Compared with ZSON (Majumdar et al., 2022) that requires training on the HM3D dataset, our method outperforms it by 196% relative SPL on MP3D and 85% relative SPL on HM3D. Note that on the MP3D dataset, our zero-shot method is comparable with previous state-of-theart supervised methods and achieves the best SPL.</p>
<p>In summary, our contributions are threefold:</p>
<p>• We propose the Exploration with Soft Commonsense constraints (ESC) method for zero-shot object navigation, which leverages pre-trained vision and language models for open-world scene understanding and objectlevel and room-level commonsense reasoning. • Our ESC approach models soft commonsense constraints and seamlessly converts them into navigation actions using Frontier-based Exploration and Probabilistic Soft Logic, which is training-free. • We achieve state-of-the-art results on zero-shot object goal navigation and outperform baseline methods by a large margin across three object navigation datasets and benchmarks.</p>
<p>Problem Definition</p>
<p>In the conventional task of object navigation, an agent is randomly placed within an unseen environment E with a specified object category G as a goal to find (e.g., chair, fireplace, or cabinet). The agent's objective is to navigate to any instance of the object that belongs to the aforementioned category. At each time step t, the agent is presented with an observation O, which consists of an egocentric RGB-D image I t and in some benchmarks, pose readings P t . The agent needs to select an action a from the action space A, which includes a 'STOP' action to terminate the navigation process. The navigation is considered successful if the agent stops within d s meters of the object and the object is visible without further moving.</p>
<p>In contrast to supervised object navigation, which trains the agent on the objects and environments it will navigate, this work focuses on zero-shot object navigation: given a new set of environments {E new } and a new set of goal objects {o new } that the agent has not seen before, the agent is required to perform object goal navigation in {E new } for {o new } without training on object goal related labels. Furthermore, we target an even more challenging zero-shot scenario-the agent performs zero-shot object navigation without training on any navigation data.</p>
<p>Our ESC Approach</p>
<p>In this section, we outline our Exploration with Soft Commonsense constraints (ESC) framework for zero-shot object navigation. As in Fig. 2, the ESC framework first converts the input image into a semantic understanding of the scene and projects it to a semantic map (Sec. 3.1). Then it leverages large language models to perform commonsense rea-  Figure 2. The ESC framework. During navigation, the agent performs scene understanding based on RGB observations and prompts. Meanwhile, the Mapping module constructs a semantic map containing room, object, and frontier information. Conditioned on the goal object and semantic scene information, the agent will then perform commonsense reasoning via a LLM to infer the probable location of the goal object, and select a frontier to explore using PSL.</p>
<p>soning for the spatial relations between the goal object and common objects and rooms (Sec. 3.2). Lastly, it combines frontier-based exploration with semantic scene understanding and commonsense reasoning via PSL (Sec. 3.3).</p>
<p>Open-World Semantic Scene Understanding</p>
<p>Prompt-Based Scene Grounding To leverage large language models for navigation inference, we need to transform the input RGB images into semantic context in language form. To achieve this, we leverage a pre-trained grounded language-image model GLIP (Li* et al., 2022) using a text prompt. Unlike traditional object detection models such as Mask-RCNN (He et al., 2017), which is limited to fixed classes, GLIP formulates the detection task as a grounding problem by aligning the proposed image region with phrases in the text prompt and predicting the score of region-text alignment. Benefiting from large-scale image-text pretraining, GLIP can detect common indoor concepts (e.g. object, room) in an open-world setting. And it is easy to generalize to different environments and object goals to perform open-world object navigation.</p>
<p>We first define a set of common indoor objects {o c } 2 , then we take the union of {o c } and all the possible goal objects {o g } to generate an object prompt for object grounding. goal in an unseen environment, they will usually consider higher-level contexts (e.g., which room is likely to contain the goal?). Therefore, we define a set of common rooms {r c } in indoor environments for room prompt P r to detect room information.</p>
<p>By inputting these prompts and an egocentric image into the GLIP model as in Fig. 2, we can get the detected objects o t,i , rooms r t,i and bounding boxes from the current scene:
{o t,i , b o t,i } = GLIP(I t , P o ) (1) {r t,i , b r t,i } = GLIP(I t , P r )(2)
where b o t,i and b r t,i are the bounding boxes of the objects and rooms. Notice that these prompts can be easily extended to generalize to new test data to perform open-world semantic scene understanding.</p>
<p>Semantic Map Construction Based on the depth input D t , agent location, and camera parameters, we can transform the pixels in a 2D image into 3D space, which is stored in a 3D voxel, where transformed pixels close to the floor are considered free space. Then we project the 3D voxel along the height dimension and obtain a 2D navigation map as shown in Fig. 2, which will be maintained during navigation. Through the navigation map, we can obtain the frontiers in the current map as explained in Sec. 3.3.1.</p>
<p>Furthermore, as shown in Fig. 2, we can project the detected room and object location into a semantic map. For object detection, we take the center of a bounding box and project it to a 2D location. For room detection, we project all the pixels in a bounding box into a 2D map and record the projected locations as the corresponding room.</p>
<p>Commonsense Reasoning for ObjNav via LLM</p>
<p>In an in-door environment, a goal object will appear in certain rooms and near certain objects more frequently, and this kind of common sense is helpful for the agent to search for a goal object. Therefore, after detecting the room and object information in the current scene, we can leverage pre-trained large language models to perform commonsense reasoning conditioned on the goal object and semantic scene information via text prompt. Specifically, for object-level and room-level inference, the large language models can reason on whether a goal object G is likely to be near each object o i in the object prompt P o and whether it is likely to be in each room r i in the room prompt P r . The prediction output of the large language models will be the real-value scores S(G|o i ), S(G|r i ) ∈ [0, 1] of each (goal, object) pair and (goal, room) pair. How to get the scores from language models and the text prompt for the language model can vary between different LLMs. We mainly use Deberta v3 (He et al., 2021) in our method for its effectiveness and accessibility. Details about the LLMs can be found in Appendix A.3.</p>
<p>Commonsense Guided Exploration</p>
<p>FRONTIER-BASED EXPLORATION</p>
<p>In object goal navigation, exploring the environment efficiently is very important to find the target object, as the object is often not seen in the agent's initial location. Gadre et al. (2023) use a heuristic exploration method, Frontierbased Exploration (FBE), to explore the environment, which shows superior performance compared with learning-based exploration methods. As shown in Fig. 1, a frontier in a map is defined as the border between the free area and the unseen area. Free area is defined as the area that the agent has seen and is not occupied by obstacles. One common strategy for frontier selection is to choose the closest frontier (with a distance threshold d f ) as the next subgoal (Gadre et al., 2023). However, choosing the closest frontier as a subgoal to explore may not be optimal in semantic-rich environments and may be against commonsense. For example, the agent might check the frontiers behind the couch in a living room to search for a bed. Therefore, we propose to introduce commonsense knowledge in LLMs into frontier-based exploration. Our goal is to make the frontier selection decision based on not only the distances d i from the agent but also object o t and room r t information around the frontiers:
P (F ) = P (F |d i , o t , r t )(3)
Intuitively, we are more likely to choose a frontier close to an object near which the goal object is likely to appear, or a frontier in a room in which the goal object should be.</p>
<p>Notice that this kind of rule represents a concept that is not always correct, as there may be multiple frontiers satisfying potentially many rules, and the conditions within these rules are continuously valued. Therefore, we need a system that can express these soft rules and logic well, i.e., Probabilistic Soft Logic (PSL).</p>
<p>SOFT COMMONSENSE CONSTRAINTS</p>
<p>Now we describe how we combine commonsense reasoning with frontier-based exploration mentioned above via PSL and enable frontier selection with soft commonsense constraints. Probabilistic Soft Logic (PSL) (Bach et al., 2017) is a probabilistic programming language defining hinge-loss Markov random fields (HL-MRF) using a syntax based on first-order logic. Specifically, PSL models dependencies between relations and attributes of entities in a domain, defined as atoms, which are encoded with weighted first-order logical clauses and linear arithmetic inequalities referred to as rules. We define four following rules for object navigation.</p>
<p>a. Object reasoning We first consider object-level reasoning. To encourage the agent to explore those frontiers near some objects that are likely to appear around the goal object, we have the rule:
w : IsCooccur(Goal, Object) ∧ IsNearObj(Frontier, Object) −→ ChooseFrontier(Frontier)(4)
The parameter w is the weight of the rule, quantifying its relative importance in the model. This rule includes three atoms: IsCooccur(Goal, Object), IsNearObj(Frontier, Object), and ChooseFrontier(Frontier). The value of IsCooccur(Goal, Object) is the co-occurrence score S(G|o i ) for (Goal, Object) pair predicted by the language models. The value of IsNearObj(Frontier, Object) is the confidence of the object prediction by GLIP if the object is within d o meters of the frontier according to the semantic map from Sec. 3.1; otherwise, IsNearObj(Frontier, Object) = 0. Furthermore, to discourage the agent from going to those frontiers near some objects that are unlikely to be around the goal object, we have a corresponding negative rule:
w : !IsCooccur(Goal, Object) ∧ IsNearObj(Frontier, Object) −→ !ChooseFrontier(Frontier)(5)
b. Room reasoning Similar to object reasoning, we encourage the agent to explore the frontiers near or in a room where the goal object is likely to appear, and discourage it from exploring the frontiers near or in a room where the goal object is unlikely to appear. Thus, we have two rules for room reasoning similar to Eq. (4) and Eq. (5) where 'Object' is substituted with 'Room'.</p>
<p>c. Distance constraint The vanilla frontier-based exploration method (Gadre et al., 2023) chooses the frontier with the shortest distance from the agent, which encourages the agent to continue exploring one area until there is nothing to explore. We also add a shortest-distance rule to encourage the agent to explore nearby frontiers:
w : ShortDist(Frontier) −→ ChooseFrontier(Frontier)(6)
d. Sum constraint We adapt a PSL hard constraint to limit the sum of the scores of choosing all the frontiers to one:
ChooseFrontier(+Frontier) = 1(7)
This constraint prevents the degenerated solution where all the target variables are equal to one and encourages the frontiers to compete with each other.</p>
<p>PSL Inference During PSL inference, atoms will be instantiated with data and referred to as ground atoms. Taking equation 4 as an example. Ground atoms are mapped to either an observed variable X, like IsCooccur and IsNearObj, or a target variable Y , like ChooseFrontier. Then, valid combinations of ground atoms substituted in the rules create ground rules. Each ground rule creates one or more hinge-loss potentials defined over logical rules, which are relaxed using Łukasiewicz continuous valued logical semantics:
φ(Y, X) = [max(0, l(Y, X))] p(8)
where l is a linear penalty function 3 defined by PSL. φ(Y, X) represents the distance to satisfaction of this ground rule. The values of X, Y are in the range [0, 1], and p ∈ {1, 2} optionally squares the potentials.</p>
<p>Given all the observed variables X and target variables Y , PSL defines an HL-MRF over the target variables:
P (Y |X) = 1 Z(Y ) exp(− m i=1 w i φ i (Y, X)) (9) Z(Y ) = Y exp(− m i=1 w i φ i (Y, X))(10)
where m denotes the number of potential functions, φ i is the i th potential function, w i is the weight of the template rule for φ i .</p>
<p>Therefore, the optimization for the distribution can be converted to a convex optimization problem:
Y * = argmin Y m i=1 w i φ(Y, X)(11)
3 Details of the penalty function can be found in Appendix A.2.</p>
<p>One-hot constraint PSL solver Normally, a PSL program can use convex optimization algorithms such as ADMM (Boyd et al., 2011) to find a solution. However, since the final choice of the agent is only one frontier, we further limit the solution space to one hot encoding space, where each one-hot encoding represents selecting one frontier. Our one-hot constraint solver is performed by calculating the violation of constraints for each one-hot encoding.</p>
<p>The encoding with the lowest loss, representing the frontier with the lowest value of violated constraints, will be chosen. This approach can help us save the iteration time of optimization compared with convex optimization algorithms.</p>
<p>Navigation Policy The agent adapts a simple navigation policy with commonsense reasoning. It will choose a new frontier based on PSL inference after it reaches a frontier. After the agent detect the goal object, it will directly navigate to it. The agent is equipped with a deterministic policy as in Fig. 2  Metrics On all three benchmarks, the number of maximum navigation steps is 500. We report and compare Success Rate (SR) and Success Rate weighted by inverse path Length (SPL) (Anderson et al., 2018), of which SPL is the primary metric used in the Habitat and RoboTHOR challenges. In ablation studies, we also report SoftSPL (Datta et al., 2020), which reflects the navigation progress made by the agent considering navigation efficiency.</p>
<p>Agent Configurations</p>
<p>The agent has a height of 0.88m, with a radius of 0.18m. The agent receives 640 × 480 RGB-D egocentric views from a camera with 79 • HFoV placed 0.88m from the ground. All the agents have action space of A = {MoveForward, RotateRight, RotateLeft, LookUp, LookDown, Stop}. The moving step is 0.25m, and each rotation turns the agent by 30 • . In MP3D and HM3D datasets, the agent will receive its GPS location at each step.  (Chang et al., 2017), HM3D (Ramakrishnan et al., 2021), and RoboTHOR (Deitke et al., 2020) benchmarks. Notice that our method and CoW (Gadre et al., 2023) are the only two zero-shot methods with no navigation training experience. Our method significantly outperforms previous zero-shot methods. * The training environment of ProcTHOR is similar to RoboTHOR using the same simulator. </p>
<p>Baselines</p>
<p>We compare our ESC method with the following two stateof-the-art (SOTA) methods for zero-shot object navigation:</p>
<p>• ZSON (Majumdar et al., 2022) uses a CLIP encoder to project the object and image goal into a same embedding space and feed the object goal embedding into an image goal navigation (Mezghani et al., 2021) network. • CLIP on Wheels (CoW) (Gadre et al., 2023) use gradient-based visualization technique (Grad-CAM (Selvaraju et al., 2017)) on CLIP to localize goal object in egocentric view, and a frontier-based exploration technique (Yamauchi, 1997) for zero-shot object goal navigation.</p>
<p>In addition, we also compare our method with the following supervised methods:</p>
<p>• PONI  proposes a modular method that predicts goal object potential and explorable area potential to select a temporary goal from the semantic map, achieving SOTA results on MP3D. • ProcTHOR (Deitke et al., 2022) synthesizes 10k indoor environments and performs large-scale ObjectNav training on those environments. Then it fine-tunes the agent on each specific dataset, achieving SOTA results on HM3D and RoboTHOR. We also compare with its zero-shot version in Table 1.</p>
<p>Implementation Details</p>
<p>There are several hyper-parameters in the ESC method. For the distance threshold d f for selecting the closest frontier to explore, we use d f = 1.6m in all the experiments. For the threshold d o determining whether a frontier is near an object, we fix d o = 1.6m. For the threshold d r determining whether a frontier is in a room, we fix d r = 0.6m. We applied a weight of 1.0 for all PSL rules when only one of commonsense reasoning (object or room) was utilized. Moreover, we double the weight for the shortest distance rule in Eq. 6 to 2.0 when both levels of commonsense reasoning are employed.  </p>
<p>Results and Analysis</p>
<p>Result Comparison with SOTA Methods</p>
<p>We compare the performance of our method with other zero-shot and supervised state-of-the-art (SOTA) methods in Table 1. Our ESC method significantly outperforms previous zero-shot methods on both SR and SPL metrics, e.g., with 284% SPL and 288% SR relative improvements on MP3D over CoW that has the same evaluation setting with our method. Compared with ZSON (Majumdar et al., 2022), which trains the agent on HM3D datasets for the image-goal navigation task at a large scale, our method still outperforms it by a large margin. The zero-shot version of the ProcTHOR model is trained on ProcTHOR data and adapts to other datasets directly. Since the texture, style, and layout of the pre-training data of ProcTHOR are similar to RoboTHOR with the same simulator, the ProcTHOR agent achieves a much higher success rate on RoboTHOR dataset. However, ProcTHOR suffers from a severe performance drop on HM3D dataset, which is reconstructed from realworld architectures with photo-realistic images and on the Habitat simulator. ESC outperforms ProcTHOR on average on SR and SPL on these two datasets.</p>
<p>Moreover, all three other zero-shot methods suffer from in- This shows that their scene understanding and navigation policy are not generalized enough. Our ESC method, in contrast, performs consistently well on three datasets, demonstrating its strong generalizability.</p>
<p>What is more, our ESC method significantly reduces the gap between zero-shot methods and supervised methods on HM3D and RoboTHOR datasets, and even outperforms the supervised THDA method  on MP3D, which shows the great potential of zero-shot methods on object goal navigation tasks. Fig. 3 illustrates a category-wise comparison between CoW and ESC on the MP3D dataset. It is evident that ESC outperforms CoW consistently on all the goal object types. Note that CoW fails on some goal objects with a strong tendency to appear or not to appear in specific rooms or in proximity to particular objects (e.g., 'toilet' and 'bed'), while our agent performs much better in those cases, indicating the efficacy of commonsense reasoning. 4</p>
<p>Ablation Study</p>
<p>To demonstrate the efficacy of semantic scene understanding and commonsense reasoning, we design GLIP on Wheel (GoW). It uses a GLIP model for object detection and the vanilla fronter-based exploration method for exploration. As a replacement for commonsense reasoning in ESC, GoW always selects the closest frontier 1.6 meters away during exploration. Notice that GoW has the same navigation policy as ESC methods except the frontier selection policy.</p>
<p>Effect of semantic scene understanding and common-4 There are only 7 'TV monitor' examples on MP3D, so its performance is not representative. ESC's SR for 'TV monitor' on HM3D is 21.7%, which has 281 'TV monitor' examples. sense reasoning. In Table 2, GoW surpasses CoW on all metrics on MP3D and RoboTHOR, which demonstrates the effectiveness of open-world object grounding of GLIP. Furthermore, ESC further outperforms GoW on all the datasets and metrics, showing the superiority of commonsense reasoning compared with pure heuristic exploration.</p>
<p>Effect of different levels of commonsense reasoning. In Table 2, we also compare the performance of different levels of reasoning on three datasets. We remove object/room level common sense for comparison. From the results, we observe that both room and object reasoning improves over GoW, and room reasoning usually brings larger improvement than object reasoning. Using both room and object reasoning provides better results on MP3D and HM3D datasets. Due to the more random placement of objects in the RoboTHOR dataset, exploration without object reasoning performs the best, and incorporating object reasoning hurts the performance slightly.</p>
<p>Effect of different LLMs. Table 3 compares the performance of different LLMs for commonsense reasoning on HM3D. Both LLMs significantly improve the performance over GoW. ChatGPT performs similarly to Deberta, except for using room-level commonsense even without specific commonsense training. We mainly use Deberta in our framework due to its accessibility. See Appendix A.3 for more implementation details.</p>
<p>How commonsense reasoning helps exploration. In Table 4, we compare the exploration ability of GoW and ESC with different frontier selection strategies. GoW selects the closest frontier 1.6 meters away from the agent, while ESC selects the frontier based on the commonsense knowledge inferred from LLMs. First, we calculate the average distance of all the chosen frontiers to the closest target object of different methods. As shown in the first column of Table 4, the frontiers chosen by our ESC method are closer to the goal object on average, which indicates our method can perform better exploration consistently and help the agent get closer to the goal object.</p>
<p>Second, we demonstrate the error analysis of GoW and ESC models in Table 4. For failure navigation, we define three kinds of errors: Detection error is defined as the goal appearing in the vision, but the agent didn't correctly detect it, or the goal never appears, but the agent thought it detected a goal. Planning error is defined as the agent successfully detecting the target object but failing, or the agent never detecting the goal object and stuck within 1 meter for at least 400 steps, which indicates low-level navigation ability. Exploration error is an error that is not a planning or detection error, which means the agent never saw the goal object without stuck or false detection. The exploration error rate evaluates the ability to get close to the goal object.</p>
<p>In Table 4, we observe that the exploration error of our ESC method has the most decrease compared with GoW. This validates that our ESC method helps the agent better explore the environment and get close to the object. We also observe that most of the errors from both methods are detection errors, this indicates that leveraging limited labels to improve the zero-shot pre-trained VL models and a better strategy to transform the detection results into action are potential directions to improve zero-shot methods.</p>
<p>Effect of different PSL solvers. We compare the performance between ADMM and one-hot constraint for solving PSL optimization in Table 5. We find that the results of the one-hot constraint are slightly better than ADMM, and it runs much faster for frontier selection (0.25 vs 2.13 seconds for each PSL inference). Since the final choice of the agent is one-hot, the target variables that optimize Eq. 11 in the one-hot space achieve the best satisfaction of the rules among all the possible choices of the agent.</p>
<p>Related Work</p>
<p>Object Goal Navigation Recently, there have been mainly two lines of work in object goal navigation. Most of the current SOTA methods use a pre-trained visual encoder (He et al., 2016;Radford et al., 2021) to encode the egocentric images into feature vectors, then feed them into a navigation agent network trained by large-scale imitation learning or reinforcement learning (Ye et al., 2021;Maksymets et al., 2021;Khandelwal et al., 2022;Ramrakhya et al., 2022;Deitke et al., 2022;Chen et al., 2022b). The second line of work is to explicitly construct a semantic map and train a navigation policy based on the constructed semantic map from the training dataset to infer goal object location (Chaplot et al., 2020b;a;Min et al., 2022;Zheng et al., 2022). Compared with these methods, our visual understanding and navigation policy does not need data from a specific environment for training. Instead, we leverage the promptbased text-image grounding model for scene understanding and commonsense knowledge in large language models to reason on both object and room levels in a zero-shot manner.</p>
<p>To solve the problems of supervised methods on generalization to new objects and environments, four recent works aimed at zero-shot object goal navigation (Gadre et al., 2023;Majumdar et al., 2022;Al-Halah et al., 2022;Deitke et al., 2022). Majumdar et al. (2022); Al-Halah et al. (2022) both train an image-goal navigation agent at scale in target environments and map the object goal to image-goal embedding space, which may not be generalized well to new datasets. Gadre et al. (2023) use GradCAM (Selvaraju et al., 2017) with CLIP to localize goal objects, and frontier-based exploration (Yamauchi, 1997) for zero-shot object goal navigation. But it only uses CLIP to localize the goal object, and the exploration decision is not conditioned on the scene context. In contrast, our work leverages a grounded visionand-language pre-trained model to recognize all common objects and rooms. Our exploration and navigation decisions are made conditioned on these scene contexts via a large language model in a zero-shot manner.</p>
<p>Commonsense Reasoning in Embodied Agents Commonsense reasoning is an essential ability for AI to perform human-level intelligence in various tasks and has been introduced into embodied AI tasks. Chaplot et al. (2020b); Chen et al. (2022c); Chaplot et al. (2020a); Min et al. (2022) used in-domain data to train a navigation policy on a semantic map to help find objects in an environment. External object relation knowledge was also used for object navigation in small environments (Yang et al., 2019;Zeng et al., 2021) and embodied procedural planning . For household task completion, Zheng et al. (2022) incorporate different levels of commonsense into task completion process, and Sarch et al. (2022) leverage in-domain semantic prior for room rearrangement. In our work, we aim to transfer the commonsense knowledge in large language models into the object goal navigation task in a zero-shot manner by expressing the commonsense knowledge as first-order rules and encoding them into a declarative templating language-Probablistic Soft Logic to help better exploration.</p>
<p>Large Pre-trained Models for Embodied Agents Benefiting from large-scale pre-training, large pre-trained models have been shown to excel in vision and language tasks (Radford et al., 2021;Li* et al., 2022;Khashabi et al., 2022), and recently have been used for embodied AI tasks, including object navigation (Khandelwal et al., 2022;Gadre et al., 2023;Majumdar et al., 2022), task planning (Ahn et al., 2022;Chen et al., 2022a;Huang et al., 2022b;a;Min et al., 2022;Blukis et al., 2022), and vision-and-language navigation (Shen et al., 2021;Shah et al., 2022). Among these works, Shen et al. (2021); Khandelwal et al. (2022) utilize CLIP (Radford et al., 2021) vision embedding to improve the performance of object goal navigation and vision-andlanguage navigation. Majumdar et al. (2022) leverage CLIP to project image and text into one goal embedding space for zero-shot object navigation. For language models, Min et al. (2022); Blukis et al. (2022); Sharma et al. (2022);Zheng et al. (2022) fine-tune a pre-trained language model on annotated (task, sub-tasks) pairs to teach the language model for sub-task planning. Ahn et al. (2022); Chen et al. (2022a); Huang et al. (2022b;a) feed several examples as prompt to language models to decompose a high-level goal into executable steps for a robot. Our work, instead, focuses on efficiently finding a goal object in unseen environments. And we leverage the pre-trained location-related commonsense in large language models and object and room-level context detected by a pre-trained grounded vision-and-language model to guide exploration in a zero-shot manner.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we propose a zero-shot object navigation framework, ESC, that leverages the pre-trained knowledge of the language-image grounding model and large language model. We introduce commonsense into frontier-based exploration as a soft constraint via PSL. The experiment results illustrate the efficacy and generalizability of our methods from different perspectives.</p>
<p>Our work establishes new state-of-the-art and explores the direction of using pre-trained commonsense knowledge in LLMs for object navigation. Future work can try to acquire more commonsense from LLMs, like the spatial relation between rooms for object navigation, and acquire different knowledge from LLMs for other embodied AI tasks. What is more, ESC uses a fixed strategy to combine commonsense knowledge. Improving the fixed strategy or relaxing the zero-shot constraint to limited finetuning to learn a frontier selection strategy is also a potential direction.</p>
<p>A. Implementation Details</p>
<p>A.1. GLIP Implementation Details</p>
<p>We use pre-trained GLIP-L (Li* et al., 2022) for all our experiments. For both object detection and room detection, we use 0.61 as the threshold of GLIP. For object detection in MP3D and HM3D dataset, we define the common indoor objects O c as all the goal objects in MP3D dataset, which has included the goal objects in the HM3D dataset. Therefore, the object detection prompt for both datasets will be 'chair.  (Deitke et al., 2020) dataset. These objects, combined with the 12 goal objects will constitute the object prompt on the RoboTHOR dataset. For room detection on three datasets, we define the room prompt as 'bedroom. living room. bathroom. kitchen. dining room. office room. gym. lounge. laundry room.'.</p>
<p>A.2. PSL detailed explanation</p>
<p>During PSL inference, each ground rule creates one or more hinge-loss potentials defined over logical rules, which are relaxed using Łukasiewicz continuous valued logical semantics:
φ(Y, X) = [max(0, l(Y, X))] p(12)
where l is a linear penalty function defined by PSL. We explain how the potential function is calculated and help determine unobserved variables through the example rule r in Eq. 4:
IsCooccur(Goal, Object) ∧ IsNearObj(Frontier, Object) −→ ChooseFrontier(Frontier)(13)
This can be transformed into the following logic form:
¬(IsCooccur(Goal, Object) ∧ IsNearObj(Frontier, Object) ∨ChooseFrontier(Frontier)(14)
Given two grounded atoms A 1 , A 2 ∈ [0, 1], the formulas for the Łukasiewicz relaxation of the logical conjunction (∧), disjunction (∨), and negation (¬) are as follows:
A 1∧ A 2 = max{0, A 1 + A 2 − 1} A 1∨ A 2 = min{A 1 + A 2 , 1} ¬A 1 = 1 − A 1(15)
From Eq. 15, by noting the ground atom as
IsCooccur(Goal, Object) = x 1 IsNearObj(Frontier, Object) = x 2 ChooseFrontier(Frontier) = y 1(16)
we can calculate the true value of a ground rule as :
min{1, (1 − (x 1 + x 2 − 1)) + y 1 } = min{1, 2 − (x 1 + x 2 ) + y 1 }(17)
The distance to satisfaction of the rule is defined as
φ(x 1 , x 2 , y 1 ) = 1 − min{1, 2 − (x 1 + x 2 ) + y 1 } = max{0, x 1 + x 2 − y1 − 1}(18)
For instance, when x 1 = 0.8 and x 2 = 0.8, φ(x 1 , x 2 , y 1 ) = 0 only when y 1 &gt; 0.6. And this will force the agent to choose a frontier when it's next to an object that the goal object is likely to be near.</p>
<p>A.3. LLM Details</p>
<p>For commonsense reasoning in object navigation, we choose the following two language models for zero-shot navigation inference.</p>
<p>Deberta v3 (He et al., 2021) (which we will refer to as Deberta for simplicity hereafter) utilizes replaced token detection objective and disentangled attention mechanism for pretraining and achieved SOTA performance on a wide range of natural language understanding tasks. The Deberta v3 is first pre-trained on a commonsense reasoning QA (CSQA) dataset. During pre-training, each 'question+candidate answer' pair in the dataset is fed into the Deberta model and gets the embedding v ∈ R d of [CLS] token from the output, which is then projected to a score s i with learned weights.</p>
<p>During inference, we design two questions: 'What is a GoalObject likely to be near?' and 'If you want to find a GoalObject, where should you go?' for object reasoning and room-level reasoning. The candidate rooms and objects in {o g } {o c } will be the candidate answers. The predicted scores s i will be linearly normalized into 0, 1 and be the values of IsCooccur in the rules.</p>
<p>ChatGPT ( 
. while not GoalDetected do {o t,i , b o t,i } = GLIP(I t , P o ) {r t,i , b r t,i } = GLIP(I t , P r ) M sem = MAP({o t,i , b o t,i }, {r t,i , b r t,i }, M sem ) M nav = MAP({I t,i }, M nav ) if G in {o t,i } then GoalDetected = True break end if if reached F or F is N one then F = PSL({R o G,i }, {R r G,i }, M sem , M nav ) else Navigate to F end if end while if GoalDetected then
Navigate to G end if of likelihood to find a GoalObject inside / nearby?'. Here P r , P o stands for room prompts and object prompts of GLIP respectively. The model will generate a series of scores that can be directly used as the values of IsCooccur.</p>
<p>A.4. Navigation Algorithm and Deterministic Policy</p>
<p>The navigation policy is illustrated in Alg. 1. The agent will first perform object-level and room-level reasoning about the goal object. Then it will look around and initialize the semantic map and navigation map. During the navigation process, it will perform semantic scene understanding with GLIP at each step and update the information on the semantic/navigation map. It will select frontiers using PSL based on commonsense reasoning, semantic map, and navigation map. After the agent detect a goal object, it will directly navigate toward it.</p>
<p>Under the general navigation policy, we also design several local policies to address the specific constraints encountered in different datasets and benchmarks. For MP3D and HM3D, the depth input is limited to 5 meters. Therefore, we design a long-distance goal policy to deal with the situation where a detected object is 5 meters away. If the object is a goal object, the agent will keep navigating in its direction until it's within 5 meters. If it's not a goal object, it will not be recorded in the semantic map.</p>
<p>For RoboTHOR, there is no GPS input to the agent. Therefore, when the agent is facing a wall and takes a moveforward action, it will believe it moves 0.25 meters while it actually stays in the original place. To mitigate this, we calculate the difference of the depth input of the last step and the current step to judge if the agent is moving.</p>
<p>B. Dataset Details</p>
<p>In MP3D (Chang et al., 2017) dataset, there are 21 target objects:</p>
<p>chair, table, picture, cabinet, cushion, sofa, bed, chest of drawers, plant, sink, toilet, stool, towel, tv monitor, shower, bathtub, counter, fireplace, gym equipment, seating, clothes.</p>
<p>In HM3D (Ramakrishnan et al., 2021) dataset, there are 6 target objects: chair, sofa, plant, bed, toilet, and tv monitor. In the RoboTHOR dataset, there are 12 target objects: Alarm-Clock, Apple, BaseballBat, BasketBall, Bowl, GarbageCan, HousePlant, Laptop, Mug, SprayBottle, Television, Vase.</p>
<p>C. More Results</p>
<p>C.1. Results per category</p>
<p>We demonstrate the success rate of each goal category on HM3D and RoboTHOR datasets here and compare our ESC with CoW method in Fig. 4. From the results, we first observe that ESC performs well in all the categories in HM3D dataset. ESC performs better than CoW on most of the object goals on RoboTHOR, except houseplant, basketball and spray bottle. There are mainly two reasons. First, CLIP still has great object localization ability on certain objects. Second, the smaller exploration space of RoboTHOR requires less exploration ability, so our method has less advantage.</p>
<p>C.2. Case study</p>
<p>To give a more intuitive view of how our model work in the object navigation process, we visualize an example in Fig. 5. The agent chooses 3 frontiers during the navigation process as the green points show. First, the agent detects a kitchen and performs commonsense reasoning that the toilet is not likely in the kitchen. Therefore it selects a frontier with a certain distance from the kitchen. When it gets out of the kitchen, it detects a table and several chairs, which are also not likely to be near a toilet. So, the agent selects two frontiers far from them, which avoids useless exploration and helps it find the toilet. U: This is a </p>
<p>Goal: Sofa</p>
<p>Scene Understanding and Commonsense Reasoning</p>
<p>The location of agent The location of chosen frontier Toilet</p>
<p>Steps: 3, 18, 41, end Table   Kitchen Table Table   Figure 5. An example shows how commonsense reasoning helps the agent choose better frontiers that lead the agent to the goal 'toilet'. 'U' means scene understanding and 'R' means commonsense reasoning.</p>
<p>Figure 3 .
3Comparison of the success rate of each goal category on MP3D between CoW(Gadre et al., 2023) and ESC.</p>
<p>Figure 4 .
4A demonstration of the success rate of each goal category on HM3D (left) and RoboTHOR (right) datasets of ESC method and CoW. Kitchen U: This is a Kitchen R: Is a toilet likely in a kitchen? No.</p>
<p>. This task Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 1 https://sites.google.com/ucsc.edu/escnav/home1 University of California, Santa Cruz 2 Samsung Re-
search America. 
Correspondence to: Xin Eric Wang 
<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#120;&#119;&#97;&#110;&#103;&#51;&#54;&#54;&#64;&#117;&#99;&#115;&#99;&#46;&#101;&#100;&#117;">&#120;&#119;&#97;&#110;&#103;&#51;&#54;&#54;&#64;&#117;&#99;&#115;&#99;&#46;&#101;&#100;&#117;</a>. </p>
<p>There is a sofa in the living room. -&gt; 
Fireplace is likely to be in a living room. 
-&gt; 
There are some unexplored space in the 
living room. -&gt; 
I should explore there to find a fireplace. </p>
<p>Goal: Find Fireplace </p>
<p>Living 
room </p>
<p>L iv in g r o o m </p>
<p>free 
frontier 
Obstacle </p>
<p>S o f a </p>
<p>Sofa </p>
<p>Fireplace </p>
<p>Table 1 .
1Zero-shot object navigation results on MP3D</p>
<p>Table 2 .
2Comparison between different detection models and different levels of commonsense reasoning on three datasets.Table 3. Comparison between Deberta and ChatGPT on HM3D.MP3D 
HM3D 
RoboTHOR </p>
<p>Commonsense Reasoning 
SPL SR SoftSPL SPL SR SoftSPL SPL 
SR </p>
<p>CLIP w/o Commonsense (CoW) 
3.7 
7.4 
-
-
-
-
16.9 26.7 
GLIP w/o Commonsense (GoW) 12.4 25.6 
22.8 
18.8 33.1 
27.0 
21.6 36.1 
ESC w/o Room 
13.8 28.3 
23.7 
21.8 39.3 
30.6 
21.4 35.6 
ESC w/o Object 
13.7 27.8 
24.1 
22.3 39.3 
30.2 
23.1 39.3 
ESC 
14.2 28.7 
23.8 
22.3 39.2 
31.1 
22.2 38.1 </p>
<p>Deberta 
ChatGPT </p>
<p>Model 
SPL SR SoftSPL SPL SR SoftSPL </p>
<p>Object 
21.8 39.3 
30.6 
22.1 38.8 
30.8 
Room 
22.3 39.3 
30.2 
21.6 38.0 
29.7 
Obj+Room 22.3 39.2 
31.1 
22.4 39.0 
31.1 </p>
<p>consistent performance when adapting to different datasets. </p>
<p>Table 4 .
4Comparison of exploration efficiency with and without commonsense reasoning on HM3D dataset. FrontierDist measures the average distance between chosen frontiers and the closest goal object. The other three metrics is the error rate of different error types.Model 
FrontierDist 
(meter) </p>
<p>Exploration 
(%) </p>
<p>Detection 
(%) </p>
<p>Planning 
(%) </p>
<p>GoW 
8.2 
14.3 
40.6 
12.1 
ESC 
7.6 
10.6 
40.8 
9.5 </p>
<p>Table 5. Comparison of results from ADMM and one-hot con-
straint for solving PSL program on MP3D dataset. We use object 
reasoning in the comparison. </p>
<p>Solvers 
SPL SR SoftSPL Infer time </p>
<p>ADMM 12.9 27.0 
22.1 
2.13 
One-hot 13.7 27.8 
24.1 
0.25 </p>
<p>Ouyang et al., 2022) is a recently release pretrained conversational LLM. It is capable of answering questions in a reasonable way. For ChatGPT, we input the language prompt as 'Among P r / P o , can you give the scoresAlgorithm 1 Navigation algorithm Input: Goal G Initialize: frontier F = N one, navigation map M nav , semantic map M sem , map update module MAP, GoalDetected Object reasoning {R o G,i = LLM(G, o i , P LLM o )} Room reasoning {R r G,i = LLM(G, r i , P LLM r )} Look around and initialize M sem , M nav</p>
<p>Table R :
RIs a toilet likely near a table? No. U: I detected chairs and a table near me. R: Is a toilet likely near a table and chair? No. U: Detect a toilet.
AcknowledgementsWe thank Xuehai He, Jialu Wang, and the anonymous reviewers for their valuable feedback on this work.
Do as i can and not as i say. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N Joshi, R Julian, D Kalashnikov, Y Kuang, K.-H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, A Zeng, arXiv:2204.01691Grounding language in robotic affordances. In arXiv preprintAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Zero experience required: Plug &amp; play modular transfer learning for semantic visual navigation. Z Al-Halah, S K Ramakrishnan, K Grauman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Al-Halah, Z., Ramakrishnan, S. K., and Grauman, K. Zero experience required: Plug &amp; play modular transfer learn- ing for semantic visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 17031-17041, June 2022.</p>
<p>On evaluation of embodied navigation agents. P Anderson, A X Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, A R Zamir, abs/1807.06757CoRRAnderson, P., Chang, A. X., Chaplot, D. S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., and Zamir, A. R. On evaluation of embodied navigation agents. CoRR, abs/1807.06757, 2018. URL http://arxiv.org/abs/1807.06757.</p>
<p>Hinge-loss markov random fields and probabilistic soft logic. S H Bach, M Broecheler, B Huang, L Getoor, 1532-4435J. Mach. Learn. Res. 181Bach, S. H., Broecheler, M., Huang, B., and Getoor, L. Hinge-loss markov random fields and probabilistic soft logic. J. Mach. Learn. Res., 18(1):3846-3912, jan 2017. ISSN 1532-4435.</p>
<p>Objectnav revisited: On evaluation of embodied agents navigating to objects. CoRR, abs. D Batra, A Gokaslan, A Kembhavi, O Maksymets, R Mottaghi, M Savva, A Toshev, E Wijmans, Batra, D., Gokaslan, A., Kembhavi, A., Maksymets, O., Mottaghi, R., Savva, M., Toshev, A., and Wijmans, E. Objectnav revisited: On evaluation of embodied agents navigating to objects. CoRR, abs/2006.13171, 2020. URL https://arxiv.org/abs/2006.13171.</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. V Blukis, C Paxton, D Fox, A Garg, Artzi , Y , arXiv:2107.05612In arXiv preprintBlukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high-level natural language instruction execution. In arXiv preprint arXiv:2107.05612, 2022.</p>
<p>Distributed optimization and statistical learning via the alternating direction method of multipliers. S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, 10.1561/2200000016Found. Trends Mach. Learn. 31Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3(1):1-122, jan 2011. ISSN 1935- 8237. doi: 10.1561/2200000016. URL https://doi. org/10.1561/2200000016.</p>
<p>A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, Matterport3d, Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV). Chang, A., Dai, A., Funkhouser, T., Halber, M., Niess- ner, M., Savva, M., Song, S., Zeng, A., and Zhang, Y. Matterport3D: Learning from RGB-D data in indoor envi- ronments. International Conference on 3D Vision (3DV), 2017.</p>
<p>Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D Gandhi, A Gupta, R Salakhutdinov, Neural Information Processing Systems (NeurIPS). Chaplot, D. S., Gandhi, D., Gupta, A., and Salakhutdinov, R. Object goal navigation using goal-oriented semantic exploration. In In Neural Information Processing Systems (NeurIPS), 2020a.</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, International Conference on Learning Representations (ICLR). Chaplot, D. S., Gandhi, D., Gupta, S., Gupta, A., and Salakhutdinov, R. Learning to explore using active neural slam. In International Conference on Learning Represen- tations (ICLR), 2020b.</p>
<p>Open-vocabulary queryable scene representations for real world planning. B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Stone, D Kappler, arXivpreprintarXiv:2209.09874Chen, B., Xia, F., Ichter, B., Rao, K., Gopalakrishnan, K., Ryoo, M. S., Stone, A., and Kappler, D. Open-vocabulary queryable scene representations for real world planning. In arXiv preprint arXiv:2209.09874, 2022a.</p>
<p>Learning active camera for multiobject navigation. P Chen, D Ji, K Lin, W Hu, W Huang, T H Li, M Tan, C Gan, Advances in Neural Information Processing Systems. Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K.Chen, P., Ji, D., Lin, K., Hu, W., Huang, W., Li, T. H., Tan, M., and Gan, C. Learning active camera for multi- object navigation. In Oh, A. H., Agarwal, A., Bel- grave, D., and Cho, K. (eds.), Advances in Neural In- formation Processing Systems, 2022b. URL https: //openreview.net/forum?id=iH4eyI5A7o.</p>
<p>Weakly-supervised multi-granularity map learning for vision-and-language navigation. P Chen, D Ji, K Lin, R Zeng, T H Li, M Tan, C Gan, Advances in Neural Information Processing Systems. Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K.Chen, P., Ji, D., Lin, K., Zeng, R., Li, T. H., Tan, M., and Gan, C. Weakly-supervised multi-granularity map learning for vision-and-language navigation. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022c. URL https://openreview.net/forum? id=gyZMZBiI9Cw.</p>
<p>Integrating egocentric localization for more realistic point-goal navigation agents. CoRL. S Datta, O Maksymets, J Hoffman, S Lee, D Batra, D Parikh, Datta, S., Maksymets, O., Hoffman, J., Lee, S., Batra, D., and Parikh, D. Integrating egocentric localization for more realistic point-goal navigation agents. CoRL, 2020.</p>
<p>Robothor: An open simulation-to-real embodied ai platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, L Weihs, M Yatskar, A Farhadi, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Deitke, M., Han, W., Herrasti, A., Kembhavi, A., Kolve, E., Mottaghi, R., Salvador, J., Schwenk, D., VanderBilt, E., Wallingford, M., Weihs, L., Yatskar, M., and Farhadi, A. Robothor: An open simulation-to-real embodied ai platform. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.</p>
<p>Procthor: Large-scale embodied ai using procedural generation. M Deitke, E Vanderbilt, A Herrasti, L Weihs, J Salvador, K Ehsani, W Han, E Kolve, A Farhadi, A Kembhavi, R Mottaghi, Deitke, M., VanderBilt, E., Herrasti, A., Weihs, L., Salvador, J., Ehsani, K., Han, W., Kolve, E., Farhadi, A., Kembhavi, A., and Mottaghi, R. Procthor: Large-scale embodied ai using procedural generation, 2022. URL https:// arxiv.org/abs/2206.06994.</p>
<p>Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. S Y Gadre, M Wortsman, G Ilharco, L Schmidt, S Song, CVPRGadre, S. Y., Wortsman, M., Ilharco, G., Schmidt, L., and Song, S. Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. CVPR, 2023.</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.902016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016. doi: 10.1109/CVPR.2016.90.</p>
<p>Mask rcnn. K He, G Gkioxari, P Dollár, R Girshick, 10.1109/ICCV.2017.3222017 IEEE International Conference on Computer Vision (ICCV). He, K., Gkioxari, G., Dollár, P., and Girshick, R. Mask r- cnn. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988, 2017. doi: 10.1109/ICCV. 2017.322.</p>
<p>Improving deberta using electra-style pre-training with gradientdisentangled embedding sharing. P He, J Gao, Chen , W Debertav3, He, P., Gao, J., and Chen, W. Debertav3: Improving deberta using electra-style pre-training with gradient- disentangled embedding sharing, 2021.</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, arXiv:2201.07207arXiv preprintHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. Lan- guage models as zero-shot planners: Extracting action- able knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022a.</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, N Brown, T Jackson, L Luu, S Levine, K Hausman, B Ichter, arXiv:2207.05608In arXiv preprintHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner monologue: Embodied reasoning through planning with language models. In arXiv preprint arXiv:2207.05608, 2022b.</p>
<p>Simple but effective: Clip embeddings for embodied ai. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi, A. Simple but effective: Clip embeddings for embod- ied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.</p>
<p>Unifiedqa-v2: Stronger generalization via broader cross-format training. D Khashabi, Y Kordi, H Hajishirzi, arXiv:2202.12359arXiv preprintKhashabi, D., Kordi, Y., and Hajishirzi, H. Unifiedqa-v2: Stronger generalization via broader cross-format training. arXiv preprint arXiv:2202.12359, 2022.</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.11916arXiv preprintKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Grounded language-image pre-training. * Li, L H Zhang, * , P Zhang, * , H Yang, J Li, C Zhong, Y Wang, L Yuan, L Zhang, L Hwang, J.-N Chang, K.-W Gao, J , CVPR. 2022Li<em>, L. H., Zhang</em>, P., Zhang*, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.- N., Chang, K.-W., and Gao, J. Grounded language-image pre-training. In CVPR, 2022.</p>
<p>Neuro-symbolic causal language planning with commonsense prompting. Y Lu, W Feng, W Zhu, W Xu, X E Wang, M Eckstein, W Y Wang, abs/2206.02928ArXiv. Lu, Y., Feng, W., Zhu, W., Xu, W., Wang, X. E., Eck- stein, M., and Wang, W. Y. Neuro-symbolic causal lan- guage planning with commonsense prompting. ArXiv, abs/2206.02928, 2022.</p>
<p>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B S Devnani, J Hoffman, D Batra, Advances in Neural Information Processing Systems. Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K.Majumdar, A., Aggarwal, G., Devnani, B. S., Hoffman, J., and Batra, D. ZSON: Zero-shot object-goal navigation us- ing multimodal goal embeddings. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=VY1dqOF2RjC.</p>
<p>Thda: Treasure hunt data augmentation for semantic navigation. O Maksymets, V Cartillier, A Gokaslan, E Wijmans, W Galuba, S Lee, D Batra, 10.1109/ICCV48922.2021.015092021 IEEE/CVF International Conference on Computer Vision (ICCV). Maksymets, O., Cartillier, V., Gokaslan, A., Wijmans, E., Galuba, W., Lee, S., and Batra, D. Thda: Trea- sure hunt data augmentation for semantic navigation. In 2021 IEEE/CVF International Conference on Com- puter Vision (ICCV), pp. 15354-15363, 2021. doi: 10.1109/ICCV48922.2021.01509.</p>
<p>Memoryaugmented reinforcement learning for image-goal navigation. L Mezghani, S Sukhbaatar, T Lavril, O Maksymets, D Batra, P Bojanowski, Alahari , K , arXiv:2101.05181arXiv preprintMezghani, L., Sukhbaatar, S., Lavril, T., Maksymets, O., Batra, D., Bojanowski, P., and Alahari, K. Memory- augmented reinforcement learning for image-goal navi- gation. arXiv preprint arXiv:2101.05181, 2021.</p>
<p>FILM: Following instructions in language with modular methods. S Y Min, D S Chaplot, P K Ravikumar, Y Bisk, R Salakhutdinov, International Conference on Learning Representations. Min, S. Y., Chaplot, D. S., Ravikumar, P. K., Bisk, Y., and Salakhutdinov, R. FILM: Following instructions in language with modular methods. In International Confer- ence on Learning Representations, 2022. URL https: //openreview.net/forum?id=qI4542Y2s1D.</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Gray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, Advances in Neural Information Processing Systems. Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instruc- tions with human feedback. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=TG8KACxEON.</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, ICML. 2021Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In ICML, 2021.</p>
<p>Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J M Turner, E Undersander, W Galuba, A Westbury, A X Chang, M Savva, Y Zhao, D Batra, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 2Ramakrishnan, S. K., Gokaslan, A., Wijmans, E., Maksymets, O., Clegg, A., Turner, J. M., Undersander, E., Galuba, W., Westbury, A., Chang, A. X., Savva, M., Zhao, Y., and Batra, D. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Thirty-fifth Conference on Neural Information Process- ing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum? id=-v4OuqNs5P.</p>
<p>Potential functions for objectgoal navigation with interaction-free learning. S K Ramakrishnan, D S Chaplot, Z Al-Halah, J Malik, K Grauman, Poni, Computer Vision and Pattern Recognition (CVPR), 2022 IEEE Conference on. IEEERamakrishnan, S. K., Chaplot, D. S., Al-Halah, Z., Ma- lik, J., and Grauman, K. Poni: Potential functions for objectgoal navigation with interaction-free learning. In Computer Vision and Pattern Recognition (CVPR), 2022 IEEE Conference on. IEEE, 2022.</p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, CVPR. 2022Ramrakhya, R., Undersander, E., Batra, D., and Das, A. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In CVPR, 2022.</p>
<p>Tidee: Tidying up novel rooms using visuo-semantic commonsense priors. G Sarch, Z Fang, A W Harley, P Schydlo, M J Tarr, S Gupta, K Fragkiadaki, S Avidan, G Brostow, M Cissé, 978-3-031-19842-7Computer Vision -ECCV 2022. Hassner, T.ChamSpringer Nature SwitzerlandSarch, G., Fang, Z., Harley, A. W., Schydlo, P., Tarr, M. J., Gupta, S., and Fragkiadaki, K. Tidee: Tidying up novel rooms using visuo-semantic commonsense priors. In Avidan, S., Brostow, G., Cissé, M., Farinella, G. M., and Hassner, T. (eds.), Computer Vision -ECCV 2022, pp. 480-496, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-19842-7.</p>
<p>Grad-cam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, 10.1109/ICCV.2017.742017 IEEE International Conference on Computer Vision (ICCV). Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 618-626, 2017. doi: 10.1109/ICCV.2017.74.</p>
<p>Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. D Shah, B Osinski, B Ichter, S Levine, Shah, D., Osinski, B., Ichter, B., and Levine, S. Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. 2022. URL https://arxiv. org/abs/2207.04429.</p>
<p>Skill induction and planning with latent language. P Sharma, A Torralba, Andreas , J , 10.18653/v1/2022.acl-long.120Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland1Association for Computational LinguisticsSharma, P., Torralba, A., and Andreas, J. Skill induction and planning with latent language. In Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pp. 1713- 1726, Dublin, Ireland, May 2022. Association for Com- putational Linguistics. doi: 10.18653/v1/2022.acl-long. 120. URL https://aclanthology.org/2022. acl-long.120.</p>
<p>How much can clip benefit vision-and-language tasks?. S Shen, L H Li, H Tan, M Bansal, A Rohrbach, K.-W Chang, Z Yao, K Keutzer, arXiv:2107.06383arXiv preprintShen, S., Li, L. H., Tan, H., Bansal, M., Rohrbach, A., Chang, K.-W., Yao, Z., and Keutzer, K. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383, 2021.</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, 10.1109/CIRA.1997.613851Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and AutomationYamauchi, B. A frontier-based approach for autonomous exploration. In Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation', pp. 146-151, 1997. doi: 10.1109/CIRA.1997.613851.</p>
<p>Visual semantic navigation using scene priors. W Yang, X Wang, A Farhadi, A Gupta, R Mottaghi, ICLR. Yang, W., Wang, X., Farhadi, A., Gupta, A., and Mottaghi, R. Visual semantic navigation using scene priors. In ICLR, 2019.</p>
<p>Auxiliary tasks and exploration enable objectgoal navigation. J Ye, D Batra, A Das, E Wijmans, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Ye, J., Batra, D., Das, A., and Wijmans, E. Auxiliary tasks and exploration enable objectgoal navigation. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision (ICCV), pp. 16117-16126, October 2021.</p>
<p>Semantic linking maps for active visual object search. Z Zeng, A Röfer, O C Jenkins, extended abstractZeng, Z., Röfer, A., and Jenkins, O. C. Semantic linking maps for active visual object search (extended abstract).</p>
<p>10.24963/ijcai.2021/667Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21Sister Conferences Best Papers82021In Proceedings of the Thirtieth International Joint Confer- ence on Artificial Intelligence, IJCAI-21, pp. 4864-4868, 8 2021. doi: 10.24963/ijcai.2021/667. URL https: //doi.org/10.24963/ijcai.2021/667. Sister Conferences Best Papers.</p>
<p>K Zheng, K Zhou, J Gu, Y Fan, J Wang, Z Li, X He, X E Wang, Jarvis, arXiv:2208.13266A neuro-symbolic commonsense reasoning framework for conversational embodied agents. arXiv preprintZheng, K., Zhou, K., Gu, J., Fan, Y., Wang, J., Li, Z., He, X., and Wang, X. E. Jarvis: A neuro-symbolic commonsense reasoning framework for conversational embodied agents. arXiv preprint arXiv:2208.13266, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-398 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-398</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-398</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-212748158</p>
                <p><strong>Paper Title:</strong> A Systematic Study of Unsupervised Domain Adaptation for Robust Human-Activity Recognition</p>
                <p><strong>Paper Abstract:</strong> Wearable sensors are increasingly becoming the primary interface for monitoring human activities. However, in order to scale human activity recognition (HAR) using wearable sensors to million of users and devices, it is imperative that HAR computational models are robust against real-world heterogeneity in inertial sensor data. In this paper, we study the problem of wearing diversity which pertains to the placement of the wearable sensor on the human body, and demonstrate that even state-of-the-art deep learning models are not robust against these factors. The core contribution of the paper lies in presenting a first-of-its-kind in-depth study of unsupervised domain adaptation (UDA) algorithms in the context of wearing diversity -- we develop and evaluate three adaptation techniques on four HAR datasets to evaluate their relative performance towards addressing the issue of wearing diversity. More importantly, we also do a careful analysis to learn the downsides of each UDA algorithm and uncover several implicit data-related assumptions without which these algorithms suffer a major degradation in accuracy. Taken together, our experimental findings caution against using UDA as a silver bullet for adapting HAR models to new domains, and serve as practical guidelines for HAR practitioners as well as pave the way for future research on domain adaptation in HAR.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e398.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e398.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UDA->HAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised Domain Adaptation techniques applied to Human Activity Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application and adaptation of unsupervised domain adaptation (UDA) methods from the machine learning / computer vision literature to the problem of wearing diversity in inertial-sensor-based human activity recognition (HAR). The paper implements, engineers, and evaluates UDA pipelines on IMU time-series data from multiple body positions and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Unsupervised Domain Adaptation (UDA) for IMU-based HAR</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Given a labeled source dataset (sensor data + activity labels) and an unlabeled target dataset (sensor data from a different body position), adapt a pre-trained deep neural network so that its learned features become invariant across source and target. The paper implements three practical approaches: (1) Data augmentation of the labeled dataset with IMU-specific transforms; (2) Feature matching via minimizing Maximum Mean Discrepancy (MMD) between source and target feature distributions; (3) Confusion maximization (domain-adversarial training) using a domain discriminator trained adversarially against the encoder. Training proceeds by alternating steps: supervised classification loss on labeled source batches, and domain-invariance updates using either MMD minimization or adversarial discriminator updates on unlabeled source/target batches. Implementations include a 6-layer 1-D CNN encoder producing 100-dim features, classifier head, MMD with Gaussian-RBF kernels, and a two-layer discriminator for adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer learning protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / computer vision (UDA literature)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>wearable sensing / human activity recognition (IMU time-series)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Re-engineered architectures and training pipelines for IMU time-series: replaced max-pooling with strided convolutional layers; tuned 1-D convolution kernel sizes for 0.5-3 Hz human movement; used LeakyReLU (alpha=0.3), instance normalization, dropout; global average pooling to 100-dim feature vector; MMD implemented with a specific set of Gaussian RBF kernel widths (C = {0.0001,0.001,0.01,0.1,1,5,10,15,20,25,30,100}); discriminator composed of two fully-connected layers with 100 hidden units; training hyperparameters: Adam lr=1e-3, beta1=0.9, beta2=0.999, batch size=125; pre-processing to 50Hz and 3s windows, normalization to [-1,1]; alternating mini-batch procedure to combine labeled source and unlabeled target batches.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — results depend on method and scenario. Feature Matching (MMD) improved target accuracy in 93.4% of experiments and produced gains up to +53 percentage points (example: from F1 0.14→0.67), while preserving source performance (source drop <5% in 99% of cases). Data Augmentation produced comparable adaptability in many upper-torso adaptations (best method in ~47% of cases) but tended to reduce source persistence by ~5–10%. Confusion Maximization (adversarial) was weaker overall (improvement in ~74.5% of pairs) but outperformed others in extreme domain-shift cases (e.g., FOOT→TORSO) and improved with large unlabeled datasets. Combining FM+DA often improved adaptability further (FM+DA best in ~57.5% of pairs) but sometimes worsened persistence (~5–10% drop).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Large domain shifts between source and target body positions; sensitivity to class-distribution mismatch (class collapse of minority activities); discriminator-based (adversarial) method needs large amounts of unlabeled data and is unstable with small target sets; data augmentation can introduce unrealistic perturbations that degrade source-domain performance; requirement for representation-learning models (deep nets) — shallow classifiers are incompatible; privacy/operational barrier of uploading unlabeled target traces to cloud.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of unlabeled target data; structural similarity between source and target (e.g., torso-to-torso); heterogeneous labeled datasets containing multiple limbs and high-degree-of-freedom positions (improves generalization); IMU-specific augmentation transforms; explicit feature-alignment loss (MMD) that works effectively even with small unlabeled sets (~500 samples); careful architecture/tuning choices (kernel sizes, normalization).</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires deep neural network encoders trained on raw IMU time-series; pre-processing to standardized sampling rate (50Hz), 3s non-overlapping windows; availability of unlabeled target-domain traces; computational resources to train alternating minimization (cloud/centralized training assumed); hyperparameter tuning for kernels and adversarial training; awareness of class-distribution in unlabeled data (to avoid collapse).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately generalizable within HAR: Feature Matching generalizes well across many body-position pairs and datasets, especially when body positions are structurally similar or when labeled data includes heterogeneous positions; Data Augmentation generalizes well for orientation/session-level variability and upper-torso transfers; Confusion Maximization can generalize in extreme-shift cases but is less reliable and requires more unlabeled data. Authors caution UDA is not a universal solution and performance depends on dataset properties (class balance, shift magnitude, quantity of unlabeled data).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and computational/technical know-how, plus theoretical transfer-learning principles</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e398.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e398.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature Matching (MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature Matching via Maximum Mean Discrepancy (MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit feature-alignment UDA technique that minimizes a kernel-based distance (MMD) between source and target feature distributions to learn domain-invariant representations; implemented here with Gaussian RBF kernels and applied to CNN features from IMU data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>MMD-based Feature Matching for HAR</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute features E(x) from a neural encoder for source and target batches and add an MMD loss term to the training objective that penalizes discrepancy between source and target feature distributions. The MMD is computed using a sum of Gaussian RBF kernels (set of kernel widths hyperparameterized by C). During training, alternate between minimizing supervised cross-entropy on labeled source batches and minimizing MMD on labeled+unlabeled batches to align feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / statistical domain-alignment</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning (domain adaptation / kernel two-sample testing)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>wearable sensing / HAR</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied MMD to 1-D CNN-extracted IMU feature vectors (100-dim); selected a wide set of Gaussian kernel widths C = {0.0001,...,100} borrowed from TensorFlow MMD implementation; integrated MMD loss into alternating minimization training with labeled source batches and unlabeled target batches; tuned batch sampling and feature extractor architecture appropriate for 3s IMU windows.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in most cases: improved target F1 in 93.4% of experimental pairs, sometimes dramatically (+53 percentage points in one reported case), while retaining source performance in the vast majority of cases (source accuracy drop <5% in 99% of experiments). Also effective with small unlabeled target sets (~500 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Performance degrades under class-distribution mismatch between source and target; when the geometric arrangement of feature distributions differs (activity distributions inside shapes differ), MMD alignment alone may not produce semantically correct alignment; large domain shifts can render MMD less informative.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Works well when feature clusters exist for activities and when source and target distributions share semantic structure; small amounts of unlabeled data suffice; careful kernel-width selection and stable encoder architecture help.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires a differentiable feature extractor (deep neural network), synchronous mini-batch sampling from both domains, choice/tuning of kernel widths, and monitoring for class collapse; suitable preprocessing (50Hz, 3s windows) and normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within HAR tasks where activities form separable feature clusters and class overlap is similar between domains; less general when label spaces or class proportions differ strongly — additional techniques needed to handle partial label mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational method and explicit procedural steps</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e398.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e398.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confusion Maximization (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confusion Maximization via Domain-Adversarial Training (Domain-Adversarial Neural Network, DANN-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial UDA approach where a domain discriminator is trained to distinguish source/target features while the encoder is trained to 'confuse' the discriminator, thus producing domain-invariant features; implemented with a two-layer discriminator and adversarial updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Domain-Adversarial (Confusion Maximization) Training for HAR</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Augment the encoder with a domain discriminator h_psi that predicts domain labels (source vs target) from encoder features. Alternate: (a) train discriminator to minimize binary cross-entropy distinguishing domains; (b) update encoder parameters with reversed gradient sign to maximize discriminator loss (i.e., make features domain-invariant), while also minimizing supervised classification loss on labeled source data. Implemented discriminator: two fully-connected layers with 100 hidden units.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / adversarial training</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / domain-adversarial training (computer vision, NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>wearable sensing / HAR</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Designed a discriminator of two FC layers (100 hidden units) for 100-dim IMU features; integrated adversarial gradient reversal in alternating training schedule with CNN encoder trained on 3s IMU windows; empirically assessed required unlabeled dataset sizes and sensitivity to class mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — overall weaker than Feature Matching for most wearing-diversity scenarios (outperformed others in ~some extreme-shift cases); improved adaptation in ~74.5% of pairs, and specifically performed better than FM and DA when adapting from FOOT to upper-torso positions; performs poorly with small unlabeled datasets but improves as unlabeled size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Poor performance when target unlabeled dataset is small; sensitive to class distribution mismatch; can destabilize feature clusters and worsen source persistence in some settings; adversarial training instability requires careful tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Large unlabeled target datasets, heterogeneous body-position adaptation scenarios, and situations with very large domain shift where distribution-shape alignment is insufficient — adversarial training can produce more uniform features across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires adversarial training support (gradient reversal), sufficient unlabeled target data to train discriminator, and stable encoder architecture; monitoring for class collapse and early stopping are important.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Limited relative to MMD-based FM for HAR; can be useful in specific extreme-shift cases and when more unlabeled data is available, but not a universally superior transfer method.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational method and explicit procedural steps</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e398.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e398.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMU Data Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IMU-specific Data Augmentation (rescaling, axis rotation, time warping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of IMU-appropriate input-space transformations (rescaling, axis rotation, time warping) to enlarge labeled training data and encourage invariant representations across sensor placement/orientation differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>IMU Data Augmentation for HAR</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Apply deterministic or probabilistic transformations on raw IMU traces to simulate common sensor perturbations: (1) Rescaling each axis by random coefficient c ~ N(1,0.1^2) to model IMU scaling errors; (2) Axis rotation via random roll/pitch/yaw to simulate different sensor orientations/placements; (3) Time warping by deforming the time axis and resampling to simulate sampling-rate instabilities. Augmented labeled dataset is used to train encoder/classifier; can be combined with UDA methods.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data-preparation technique / domain generalization</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>wearable-sensor augmentation literature and machine learning data-augmentation practice</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>HAR adaptation to wearing diversity</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application of IMU augmentation methods, integrated with UDA pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Selected three IMU-suitable transforms from prior work (Um et al.) and applied them on 3s, 50Hz windows; tuned rescaling variance and implemented axis rotations/time-warping compatible with CNN encoder inputs; used augmentation either alone or preceding UDA steps (FM+DA, CM+DA).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>effective in many scenarios: achieved top adaptation performance on ~47% of experimental settings (particularly effective within upper-torso body positions); when combined with FM increased adaptability (FM+DA best in ~57.5% of pairs). Trade-off: tends to decrease source persistence (5–10% drop on average).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Augmented perturbations may not perfectly reflect real human/sensor behaviors, which can degrade source accuracy; cannot address large structural domain shifts alone.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simple to implement, works with limited labeled data, effective for orientation/session-level variability, complements UDA methods when realistic transforms are used.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires careful selection/tuning of augmentation parameters and compatibility with preprocessing (sampling rate, windowing).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for handling orientation and session-level sensor variability and for upper-torso wearing positions; less effective for very large cross-limb shifts unless combined with UDA.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental technical know-how</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e398.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e398.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MotionTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MotionTransformer: Transferring Neural Inertial Tracking Between Domains</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that builds a translator to convert traces from new wearable sensors to resemble traces collected from smartphone sensors, using a confusion-maximization-style adversarial approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MotionTransformer: Transferring Neural Inertial Tracking Between Domains.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>MotionTransformer translator (adversarial transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Construct a neural translator that maps sensor traces from one device/domain into the distribution of another device/domain (e.g., convert smartwatch traces to smartphone-like traces) using adversarial objectives so the translated traces fool a discriminator or match a target distribution; used for transferring inertial tracking models across devices.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / sensor-domain translation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>neural inertial tracking / sensor-domain transfer (machine learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>wearable sensor adaptation / HAR-like sensor cross-domain conversion</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>analogical transfer with domain-translation model (adversarial)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Referenced as using confusion-maximization/adversarial training to build a translator — specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>not evaluated in this paper (referenced as prior work); described as an approach that transfers models between device domains in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Not detailed in this paper; likely similar adversarial training stability and data requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Adversarial translator can be effective when paired data or unlabeled target traces are available.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>See referenced MotionTransformer paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Presented in referenced work as a method for converting traces across sensor/device domains; relevance to HAR is direct but details and success metrics are in the source paper.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational method / explicit procedural description (in referenced paper)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e398.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e398.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HDCNN (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HDCNN (feature-matching adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior method that used a feature-matching approach (minimizing feature discrepancy) with Kullback-Leibler divergence at multiple network layers to adapt models across devices (smartphone→smartwatch).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>HDCNN feature-matching adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Reduce discrepancy between source and target feature distributions after each convolutional and fully-connected layer using a divergence measure (KL divergence in the referenced work) to adapt a model trained on one device to another without target labels.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / multi-layer feature alignment</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / HAR cross-device adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>wearable-sensor adaptation (smartphone to smartwatch)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified method (feature-matching across layers)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied KL-divergence based discrepancy minimization layer-wise; exact architecture/hyperparameters not provided in this paper (referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>cited as prior successful approach for smartphone-to-smartwatch adaptation in referenced literature; details are in the original HDCNN work.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Not detailed here; likely similar concerns about class mismatch and distributional differences.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Multi-layer alignment helps align hierarchical features across devices.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Deep network architectures with multiple representational layers suitable for layerwise alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Demonstrated for device-to-device adaptation in prior work; applicability to body-position wearing diversity is related but requires evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational method (explicit procedural steps in referenced work)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e398.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e398.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Akbari et al. approach</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder + Feature Matching for sensor adaptation (Akbari et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced method that first learns stochastic feature representations using a variational autoencoder (VAE) and then applies a feature-matching adaptation (KL divergence) to align source and target for new wearable sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transferring activity recognition models for new wearable sensors with deep generative domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>VAE-based feature extraction + feature-matching adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Train a variational autoencoder on labeled data to obtain stochastic latent features, then align distributions between labeled source and unlabeled target features using a distance/divergence metric (KL), enabling transfer to a new sensor/environment.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / generative feature adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>generative modeling / domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>wearable sensor activity recognition adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified generative transfer</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Referenced approach combines VAE-based stochastic encodings with feature-matching adaptation; exact implementation details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Described in cited work as an explored technique; not experimentally evaluated in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Not detailed here; likely issues include VAE reconstruction/tradeoffs and mismatch sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Stochastic latent representations can help capture variability across sensors and make alignment more flexible.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Generative-model training, careful hyperparameterization, and available unlabeled target data.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Promising for cross-device adaptation in prior work; applicability to wearing-diversity requires empirical evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational method and explicit procedural knowledge (in referenced paper)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MotionTransformer: Transferring Neural Inertial Tracking Between Domains <em>(Rating: 2)</em></li>
                <li>Transferring activity recognition models for new wearable sensors with deep generative domain adaptation <em>(Rating: 2)</em></li>
                <li>Domain-adversarial training of neural networks <em>(Rating: 2)</em></li>
                <li>Deep domain confusion: Maximizing for domain invariance <em>(Rating: 1)</em></li>
                <li>Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring Using Convolutional Neural Networks <em>(Rating: 2)</em></li>
                <li>A kernel two-sample test <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-398",
    "paper_id": "paper-212748158",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "UDA-&gt;HAR",
            "name_full": "Unsupervised Domain Adaptation techniques applied to Human Activity Recognition",
            "brief_description": "Application and adaptation of unsupervised domain adaptation (UDA) methods from the machine learning / computer vision literature to the problem of wearing diversity in inertial-sensor-based human activity recognition (HAR). The paper implements, engineers, and evaluates UDA pipelines on IMU time-series data from multiple body positions and datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Unsupervised Domain Adaptation (UDA) for IMU-based HAR",
            "procedure_description": "Given a labeled source dataset (sensor data + activity labels) and an unlabeled target dataset (sensor data from a different body position), adapt a pre-trained deep neural network so that its learned features become invariant across source and target. The paper implements three practical approaches: (1) Data augmentation of the labeled dataset with IMU-specific transforms; (2) Feature matching via minimizing Maximum Mean Discrepancy (MMD) between source and target feature distributions; (3) Confusion maximization (domain-adversarial training) using a domain discriminator trained adversarially against the encoder. Training proceeds by alternating steps: supervised classification loss on labeled source batches, and domain-invariance updates using either MMD minimization or adversarial discriminator updates on unlabeled source/target batches. Implementations include a 6-layer 1-D CNN encoder producing 100-dim features, classifier head, MMD with Gaussian-RBF kernels, and a two-layer discriminator for adversarial training.",
            "procedure_type": "computational method / transfer learning protocol",
            "source_domain": "machine learning / computer vision (UDA literature)",
            "target_domain": "wearable sensing / human activity recognition (IMU time-series)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Re-engineered architectures and training pipelines for IMU time-series: replaced max-pooling with strided convolutional layers; tuned 1-D convolution kernel sizes for 0.5-3 Hz human movement; used LeakyReLU (alpha=0.3), instance normalization, dropout; global average pooling to 100-dim feature vector; MMD implemented with a specific set of Gaussian RBF kernel widths (C = {0.0001,0.001,0.01,0.1,1,5,10,15,20,25,30,100}); discriminator composed of two fully-connected layers with 100 hidden units; training hyperparameters: Adam lr=1e-3, beta1=0.9, beta2=0.999, batch size=125; pre-processing to 50Hz and 3s windows, normalization to [-1,1]; alternating mini-batch procedure to combine labeled source and unlabeled target batches.",
            "transfer_success": "partially successful — results depend on method and scenario. Feature Matching (MMD) improved target accuracy in 93.4% of experiments and produced gains up to +53 percentage points (example: from F1 0.14→0.67), while preserving source performance (source drop &lt;5% in 99% of cases). Data Augmentation produced comparable adaptability in many upper-torso adaptations (best method in ~47% of cases) but tended to reduce source persistence by ~5–10%. Confusion Maximization (adversarial) was weaker overall (improvement in ~74.5% of pairs) but outperformed others in extreme domain-shift cases (e.g., FOOT→TORSO) and improved with large unlabeled datasets. Combining FM+DA often improved adaptability further (FM+DA best in ~57.5% of pairs) but sometimes worsened persistence (~5–10% drop).",
            "barriers_encountered": "Large domain shifts between source and target body positions; sensitivity to class-distribution mismatch (class collapse of minority activities); discriminator-based (adversarial) method needs large amounts of unlabeled data and is unstable with small target sets; data augmentation can introduce unrealistic perturbations that degrade source-domain performance; requirement for representation-learning models (deep nets) — shallow classifiers are incompatible; privacy/operational barrier of uploading unlabeled target traces to cloud.",
            "facilitating_factors": "Availability of unlabeled target data; structural similarity between source and target (e.g., torso-to-torso); heterogeneous labeled datasets containing multiple limbs and high-degree-of-freedom positions (improves generalization); IMU-specific augmentation transforms; explicit feature-alignment loss (MMD) that works effectively even with small unlabeled sets (~500 samples); careful architecture/tuning choices (kernel sizes, normalization).",
            "contextual_requirements": "Requires deep neural network encoders trained on raw IMU time-series; pre-processing to standardized sampling rate (50Hz), 3s non-overlapping windows; availability of unlabeled target-domain traces; computational resources to train alternating minimization (cloud/centralized training assumed); hyperparameter tuning for kernels and adversarial training; awareness of class-distribution in unlabeled data (to avoid collapse).",
            "generalizability": "Moderately generalizable within HAR: Feature Matching generalizes well across many body-position pairs and datasets, especially when body positions are structurally similar or when labeled data includes heterogeneous positions; Data Augmentation generalizes well for orientation/session-level variability and upper-torso transfers; Confusion Maximization can generalize in extreme-shift cases but is less reliable and requires more unlabeled data. Authors caution UDA is not a universal solution and performance depends on dataset properties (class balance, shift magnitude, quantity of unlabeled data).",
            "knowledge_type": "explicit procedural steps and computational/technical know-how, plus theoretical transfer-learning principles",
            "uuid": "e398.0"
        },
        {
            "name_short": "Feature Matching (MMD)",
            "name_full": "Feature Matching via Maximum Mean Discrepancy (MMD)",
            "brief_description": "An explicit feature-alignment UDA technique that minimizes a kernel-based distance (MMD) between source and target feature distributions to learn domain-invariant representations; implemented here with Gaussian RBF kernels and applied to CNN features from IMU data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "MMD-based Feature Matching for HAR",
            "procedure_description": "Compute features E(x) from a neural encoder for source and target batches and add an MMD loss term to the training objective that penalizes discrepancy between source and target feature distributions. The MMD is computed using a sum of Gaussian RBF kernels (set of kernel widths hyperparameterized by C). During training, alternate between minimizing supervised cross-entropy on labeled source batches and minimizing MMD on labeled+unlabeled batches to align feature spaces.",
            "procedure_type": "computational method / statistical domain-alignment",
            "source_domain": "machine learning (domain adaptation / kernel two-sample testing)",
            "target_domain": "wearable sensing / HAR",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Applied MMD to 1-D CNN-extracted IMU feature vectors (100-dim); selected a wide set of Gaussian kernel widths C = {0.0001,...,100} borrowed from TensorFlow MMD implementation; integrated MMD loss into alternating minimization training with labeled source batches and unlabeled target batches; tuned batch sampling and feature extractor architecture appropriate for 3s IMU windows.",
            "transfer_success": "successful in most cases: improved target F1 in 93.4% of experimental pairs, sometimes dramatically (+53 percentage points in one reported case), while retaining source performance in the vast majority of cases (source accuracy drop &lt;5% in 99% of experiments). Also effective with small unlabeled target sets (~500 samples).",
            "barriers_encountered": "Performance degrades under class-distribution mismatch between source and target; when the geometric arrangement of feature distributions differs (activity distributions inside shapes differ), MMD alignment alone may not produce semantically correct alignment; large domain shifts can render MMD less informative.",
            "facilitating_factors": "Works well when feature clusters exist for activities and when source and target distributions share semantic structure; small amounts of unlabeled data suffice; careful kernel-width selection and stable encoder architecture help.",
            "contextual_requirements": "Requires a differentiable feature extractor (deep neural network), synchronous mini-batch sampling from both domains, choice/tuning of kernel widths, and monitoring for class collapse; suitable preprocessing (50Hz, 3s windows) and normalization.",
            "generalizability": "High within HAR tasks where activities form separable feature clusters and class overlap is similar between domains; less general when label spaces or class proportions differ strongly — additional techniques needed to handle partial label mismatch.",
            "knowledge_type": "computational method and explicit procedural steps",
            "uuid": "e398.1"
        },
        {
            "name_short": "Confusion Maximization (DANN)",
            "name_full": "Confusion Maximization via Domain-Adversarial Training (Domain-Adversarial Neural Network, DANN-style)",
            "brief_description": "An adversarial UDA approach where a domain discriminator is trained to distinguish source/target features while the encoder is trained to 'confuse' the discriminator, thus producing domain-invariant features; implemented with a two-layer discriminator and adversarial updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Domain-Adversarial (Confusion Maximization) Training for HAR",
            "procedure_description": "Augment the encoder with a domain discriminator h_psi that predicts domain labels (source vs target) from encoder features. Alternate: (a) train discriminator to minimize binary cross-entropy distinguishing domains; (b) update encoder parameters with reversed gradient sign to maximize discriminator loss (i.e., make features domain-invariant), while also minimizing supervised classification loss on labeled source data. Implemented discriminator: two fully-connected layers with 100 hidden units.",
            "procedure_type": "computational method / adversarial training",
            "source_domain": "machine learning / domain-adversarial training (computer vision, NLP)",
            "target_domain": "wearable sensing / HAR",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Designed a discriminator of two FC layers (100 hidden units) for 100-dim IMU features; integrated adversarial gradient reversal in alternating training schedule with CNN encoder trained on 3s IMU windows; empirically assessed required unlabeled dataset sizes and sensitivity to class mismatch.",
            "transfer_success": "partially successful — overall weaker than Feature Matching for most wearing-diversity scenarios (outperformed others in ~some extreme-shift cases); improved adaptation in ~74.5% of pairs, and specifically performed better than FM and DA when adapting from FOOT to upper-torso positions; performs poorly with small unlabeled datasets but improves as unlabeled size increases.",
            "barriers_encountered": "Poor performance when target unlabeled dataset is small; sensitive to class distribution mismatch; can destabilize feature clusters and worsen source persistence in some settings; adversarial training instability requires careful tuning.",
            "facilitating_factors": "Large unlabeled target datasets, heterogeneous body-position adaptation scenarios, and situations with very large domain shift where distribution-shape alignment is insufficient — adversarial training can produce more uniform features across domains.",
            "contextual_requirements": "Requires adversarial training support (gradient reversal), sufficient unlabeled target data to train discriminator, and stable encoder architecture; monitoring for class collapse and early stopping are important.",
            "generalizability": "Limited relative to MMD-based FM for HAR; can be useful in specific extreme-shift cases and when more unlabeled data is available, but not a universally superior transfer method.",
            "knowledge_type": "computational method and explicit procedural steps",
            "uuid": "e398.2"
        },
        {
            "name_short": "IMU Data Augmentation",
            "name_full": "IMU-specific Data Augmentation (rescaling, axis rotation, time warping)",
            "brief_description": "Use of IMU-appropriate input-space transformations (rescaling, axis rotation, time warping) to enlarge labeled training data and encourage invariant representations across sensor placement/orientation differences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "IMU Data Augmentation for HAR",
            "procedure_description": "Apply deterministic or probabilistic transformations on raw IMU traces to simulate common sensor perturbations: (1) Rescaling each axis by random coefficient c ~ N(1,0.1^2) to model IMU scaling errors; (2) Axis rotation via random roll/pitch/yaw to simulate different sensor orientations/placements; (3) Time warping by deforming the time axis and resampling to simulate sampling-rate instabilities. Augmented labeled dataset is used to train encoder/classifier; can be combined with UDA methods.",
            "procedure_type": "data-preparation technique / domain generalization",
            "source_domain": "wearable-sensor augmentation literature and machine learning data-augmentation practice",
            "target_domain": "HAR adaptation to wearing diversity",
            "transfer_type": "direct application of IMU augmentation methods, integrated with UDA pipelines",
            "modifications_made": "Selected three IMU-suitable transforms from prior work (Um et al.) and applied them on 3s, 50Hz windows; tuned rescaling variance and implemented axis rotations/time-warping compatible with CNN encoder inputs; used augmentation either alone or preceding UDA steps (FM+DA, CM+DA).",
            "transfer_success": "effective in many scenarios: achieved top adaptation performance on ~47% of experimental settings (particularly effective within upper-torso body positions); when combined with FM increased adaptability (FM+DA best in ~57.5% of pairs). Trade-off: tends to decrease source persistence (5–10% drop on average).",
            "barriers_encountered": "Augmented perturbations may not perfectly reflect real human/sensor behaviors, which can degrade source accuracy; cannot address large structural domain shifts alone.",
            "facilitating_factors": "Simple to implement, works with limited labeled data, effective for orientation/session-level variability, complements UDA methods when realistic transforms are used.",
            "contextual_requirements": "Requires careful selection/tuning of augmentation parameters and compatibility with preprocessing (sampling rate, windowing).",
            "generalizability": "High for handling orientation and session-level sensor variability and for upper-torso wearing positions; less effective for very large cross-limb shifts unless combined with UDA.",
            "knowledge_type": "explicit procedural steps and instrumental technical know-how",
            "uuid": "e398.3"
        },
        {
            "name_short": "MotionTransformer",
            "name_full": "MotionTransformer: Transferring Neural Inertial Tracking Between Domains",
            "brief_description": "A referenced work that builds a translator to convert traces from new wearable sensors to resemble traces collected from smartphone sensors, using a confusion-maximization-style adversarial approach.",
            "citation_title": "MotionTransformer: Transferring Neural Inertial Tracking Between Domains.",
            "mention_or_use": "mention",
            "procedure_name": "MotionTransformer translator (adversarial transfer)",
            "procedure_description": "Construct a neural translator that maps sensor traces from one device/domain into the distribution of another device/domain (e.g., convert smartwatch traces to smartphone-like traces) using adversarial objectives so the translated traces fool a discriminator or match a target distribution; used for transferring inertial tracking models across devices.",
            "procedure_type": "computational method / sensor-domain translation",
            "source_domain": "neural inertial tracking / sensor-domain transfer (machine learning)",
            "target_domain": "wearable sensor adaptation / HAR-like sensor cross-domain conversion",
            "transfer_type": "analogical transfer with domain-translation model (adversarial)",
            "modifications_made": "Referenced as using confusion-maximization/adversarial training to build a translator — specifics not detailed in this paper.",
            "transfer_success": "not evaluated in this paper (referenced as prior work); described as an approach that transfers models between device domains in prior literature.",
            "barriers_encountered": "Not detailed in this paper; likely similar adversarial training stability and data requirements.",
            "facilitating_factors": "Adversarial translator can be effective when paired data or unlabeled target traces are available.",
            "contextual_requirements": "See referenced MotionTransformer paper.",
            "generalizability": "Presented in referenced work as a method for converting traces across sensor/device domains; relevance to HAR is direct but details and success metrics are in the source paper.",
            "knowledge_type": "computational method / explicit procedural description (in referenced paper)",
            "uuid": "e398.4"
        },
        {
            "name_short": "HDCNN (ref)",
            "name_full": "HDCNN (feature-matching adaptation)",
            "brief_description": "Referenced prior method that used a feature-matching approach (minimizing feature discrepancy) with Kullback-Leibler divergence at multiple network layers to adapt models across devices (smartphone→smartwatch).",
            "citation_title": "",
            "mention_or_use": "mention",
            "procedure_name": "HDCNN feature-matching adaptation",
            "procedure_description": "Reduce discrepancy between source and target feature distributions after each convolutional and fully-connected layer using a divergence measure (KL divergence in the referenced work) to adapt a model trained on one device to another without target labels.",
            "procedure_type": "computational method / multi-layer feature alignment",
            "source_domain": "machine learning / HAR cross-device adaptation",
            "target_domain": "wearable-sensor adaptation (smartphone to smartwatch)",
            "transfer_type": "adapted/modified method (feature-matching across layers)",
            "modifications_made": "Applied KL-divergence based discrepancy minimization layer-wise; exact architecture/hyperparameters not provided in this paper (referenced).",
            "transfer_success": "cited as prior successful approach for smartphone-to-smartwatch adaptation in referenced literature; details are in the original HDCNN work.",
            "barriers_encountered": "Not detailed here; likely similar concerns about class mismatch and distributional differences.",
            "facilitating_factors": "Multi-layer alignment helps align hierarchical features across devices.",
            "contextual_requirements": "Deep network architectures with multiple representational layers suitable for layerwise alignment.",
            "generalizability": "Demonstrated for device-to-device adaptation in prior work; applicability to body-position wearing diversity is related but requires evaluation.",
            "knowledge_type": "computational method (explicit procedural steps in referenced work)",
            "uuid": "e398.5"
        },
        {
            "name_short": "Akbari et al. approach",
            "name_full": "Variational Autoencoder + Feature Matching for sensor adaptation (Akbari et al.)",
            "brief_description": "Referenced method that first learns stochastic feature representations using a variational autoencoder (VAE) and then applies a feature-matching adaptation (KL divergence) to align source and target for new wearable sensors.",
            "citation_title": "Transferring activity recognition models for new wearable sensors with deep generative domain adaptation.",
            "mention_or_use": "mention",
            "procedure_name": "VAE-based feature extraction + feature-matching adaptation",
            "procedure_description": "Train a variational autoencoder on labeled data to obtain stochastic latent features, then align distributions between labeled source and unlabeled target features using a distance/divergence metric (KL), enabling transfer to a new sensor/environment.",
            "procedure_type": "computational method / generative feature adaptation",
            "source_domain": "generative modeling / domain adaptation",
            "target_domain": "wearable sensor activity recognition adaptation",
            "transfer_type": "adapted/modified generative transfer",
            "modifications_made": "Referenced approach combines VAE-based stochastic encodings with feature-matching adaptation; exact implementation details are in the cited work.",
            "transfer_success": "Described in cited work as an explored technique; not experimentally evaluated in this paper beyond citation.",
            "barriers_encountered": "Not detailed here; likely issues include VAE reconstruction/tradeoffs and mismatch sensitivity.",
            "facilitating_factors": "Stochastic latent representations can help capture variability across sensors and make alignment more flexible.",
            "contextual_requirements": "Generative-model training, careful hyperparameterization, and available unlabeled target data.",
            "generalizability": "Promising for cross-device adaptation in prior work; applicability to wearing-diversity requires empirical evaluation.",
            "knowledge_type": "computational method and explicit procedural knowledge (in referenced paper)",
            "uuid": "e398.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MotionTransformer: Transferring Neural Inertial Tracking Between Domains",
            "rating": 2,
            "sanitized_title": "motiontransformer_transferring_neural_inertial_tracking_between_domains"
        },
        {
            "paper_title": "Transferring activity recognition models for new wearable sensors with deep generative domain adaptation",
            "rating": 2,
            "sanitized_title": "transferring_activity_recognition_models_for_new_wearable_sensors_with_deep_generative_domain_adaptation"
        },
        {
            "paper_title": "Domain-adversarial training of neural networks",
            "rating": 2,
            "sanitized_title": "domainadversarial_training_of_neural_networks"
        },
        {
            "paper_title": "Deep domain confusion: Maximizing for domain invariance",
            "rating": 1,
            "sanitized_title": "deep_domain_confusion_maximizing_for_domain_invariance"
        },
        {
            "paper_title": "Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring Using Convolutional Neural Networks",
            "rating": 2,
            "sanitized_title": "data_augmentation_of_wearable_sensor_data_for_parkinsons_disease_monitoring_using_convolutional_neural_networks"
        },
        {
            "paper_title": "A kernel two-sample test",
            "rating": 1,
            "sanitized_title": "a_kernel_twosample_test"
        }
    ],
    "cost": 0.021669,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Systematic Study of Unsupervised Domain Adaptation for Robust Human-Activity Recognition</p>
<p>Youngjae Chang yjchang@nclab.kaist.ac.kr@southkorea 
South Kaist kaist@southkorea 
Akhil Korea 
Mathur 
Anton Isopoussu isopoussu@gmail.com 
KAISTJunehwa Song junesong@nclab.kaist.ac.kr@southkorea 
South Korea 
Fahim Kawsar fahimkawsar@nokia-bell-labs.com </p>
<p>University College London and Nokia Bell Labs
United Kingdom</p>
<p>Nokia Bell Labs, United Kingdom and TU
Nokia Bell Labs
DelftUnited Kingdom, Netherlands</p>
<p>Nokia Bell Labs, United Kingdom; Anton Isopoussu, anton
Nokia Bell Labs, United Kingdom; Junehwa Song
Nokia Bell Labs, United Kingdom and TU
University College London
DelftNetherlands</p>
<p>A Systematic Study of Unsupervised Domain Adaptation for Robust Human-Activity Recognition</p>
<p>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol
413910.1145/3380985Publication date: March 2020.39 ACM Reference Format: Youngjae Chang, Akhil Mathur, Anton Isopoussu, Junehwa Song, and Fahim Kawsar. 2020. A Systematic Study of Unsupervised Domain Adaptation for Robust Human-Activity Recognition. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 1, Article 39 (March 2020), 30 pages. https://doi.org/10.1145/3380985 * Both authors contributed equally to this research. † This work was done in part when the author was on an internship at Nokia Bell Labs, UK. Authors' addresses: Youngjae Chang, 2474-9567/2020/3-ART39 $15.00 39:2 • Chang et al.CCS Concepts:Human-centered computing → Ubiquitous and mobile computing;Computing methodologies → Machine learning Additional Key Words and Phrases: Human Activity Recognition, Unsupervised Domain Adaptation, Wearing Diversity
Wearable sensors are increasingly becoming the primary interface for monitoring human activities. However, in order to scale human activity recognition (HAR) using wearable sensors to million of users and devices, it is imperative that HAR computational models are robust against real-world heterogeneity in inertial sensor data. In this paper, we study the problem of wearing diversity which pertains to the placement of the wearable sensor on the human body, and demonstrate that even state-of-the-art deep learning models are not robust against these factors. The core contribution of the paper lies in presenting a first-of-its-kind in-depth study of unsupervised domain adaptation (UDA) algorithms in the context of wearing diversitywe develop and evaluate three adaptation techniques on four HAR datasets to evaluate their relative performance towards addressing the issue of wearing diversity. More importantly, we also do a careful analysis to learn the downsides of each UDA algorithm and uncover several implicit data-related assumptions without which these algorithms suffer a major degradation in accuracy. Taken together, our experimental findings caution against using UDA as a silver bullet for adapting HAR models to new domains, and serve as practical guidelines for HAR practitioners as well as pave the way for future research on domain adaptation in HAR.</p>
<p>INTRODUCTION</p>
<p>Wearable devices equipped with inertial sensors have become a key driver for sensing user activities and context such as physical activities [17,24,35], transportation mode [11,30], body gestures [47] and even eating episodes [1]. These advancements can be attributed to the years of research in the area of human-activity recognition (HAR) [4,8,15,31] as well as the advances in computational models that process raw inertial data to infer human activities. More recently, deep learning-based approaches [17,35] have been proposed to infer human activities from inertial sensors, which outperform the performance of traditional shallow classifiers.</p>
<p>Despite these advances, challenges remain to make HAR models robust against a number of diversities exhibited in the real-world. One of the most prominent forms of variability in wearable sensor data is caused by the positioning of the wearable sensors on the human body [24]. For example, inertial sensors can be worn on the wrist (in a smartwatch), on the ear (in an earbud), or be placed inside a user's trouser and shirt pockets (smartphones). More critically, the sensor placement is not always static and may change rapidly during the course of an activity based on users' preference -for instance, a smartphone may move from a pocket to a user's hand, and then to the ear and then go in a handbag -all while the user is engaged in a certain physical activity. As such, it is critical that HAR models are robust to wearing diversity and can provide accurate predictions across multiple wearing positions.</p>
<p>From a machine learning perspective, the challenge of wearing diversity in HAR models can be formulated as a domain shift problem. A fundamental assumption in supervised learning algorithms that are used to train HAR models is that the distribution of the data remains the same during training and testing stages. However, the variability induced in the data by different wearing positions causes this assumption to be violated, in that they lead to a discrepancy (or shift) between the training and test data distributions, a phenomenon known as domain shift. Therefore, if a classifier is trained on data collected from one domain (e.g., a body position such as wrist) and tested on another domain (a new body position), it is likely to perform poorly.</p>
<p>Indeed, a straightforward way to address this challenge is to collect labeled training data from all possible body positions in which a wearable device can be worn, and thereafter either train an HAR model per body position [41], or train a generic model which can work across all body positions [5]. However, collecting large amounts of labeled human-activity training data for different body positions is both expensive and time-consuming, thereby limiting the practicality of these solutions. Ideally, we desire a solution which can generalize an HAR classifier to a new wearing position using zero or minimal amount of labeled data from that position.</p>
<p>To this end, deep unsupervised domain adaptation (UDA) has emerged as a promising technique to adapt deep learning models across domains using only unlabeled data from the target domain. At a high-level, the idea behind UDA is as follows: given a pre-trained classifier for a source domain and an unlabeled dataset from a target domain, how can we adapt the weights of the source model such that it shows better performance in the target domain. While UDA has been an active area of research in the computer vision community, its application to human activity recognition is currently in a nascent stage [3,21] and to the best of our knowledge, there is no work which systematically studies the applicability of various UDA techniques proposed in the machine learning literature to the problem of wearing diversity in HAR. We argue that as UbiComp and HAR researchers, it is imperative that we carefully analyze the pros, cons, assumptions, and constraints of technique(s) proposed in the machine learning literature, before they are applied to the domain of HAR.</p>
<p>In this paper, we take the first step towards an in-depth study of unsupervised domain adaptation to tackle the problem of wearing diversity in wearables. Specifically, our analysis revolves around four pillars:</p>
<p>• Choice of UDA algorithms. Broadly, UDA algorithms that aim to adapt a classifier from a source domain to a target domain can be divided into two categories: i) Feature Matching and ii) Confusion Maximization. Both algorithms focus on aligning the feature spaces of source and target domains -Feature Matching does it explicitly by minimizing a distance metric in the feature space and Confusion Maximization uses adversarial learning to achieve this objective. While variants of Feature Matching have been developed for HAR [21], to the best of our knowledge Confusion Maximization has not been applied to the problem of wearing diversity. In this work, we develop two UDA algorithms based on Feature Matching and Confusion Maximization to address wearing diversity, and compare their performance in the context of HAR tasks.</p>
<p>Our key objectives are to understand if a particular class of algorithm is better suited to the problem of wearing diversity, and to uncover the assumptions and downsides of each algorithm. Further, we develop a Data Augmentation baseline to compare against the UDA algorithms. • Choice of Body Positions. Prior work has primarily studied the applicability of UDA for two body positions, namely thigh (smartphone) and wrist (smartwatch). However wearable devices are not restricted to just these body positions and can be worn in many, e.g., ears [19], fingers [10], neck [13], etc. We provide the first-ever in-depth analysis of the performance of UDA algorithms across a range of 20 body positions and 5 body position groups from 4 different datasets, by showing results for total 108 adaptation scenarios. • Effect of dataset properties. Indeed, the performance of data-driven UDA algorithms heavily depends on how the underlying datasets are structured. We investigate the effect of class mismatch between source and target domains on UDA performance, as well as the amount of labeled and unlabeled data needed from different body positions to obtain optimal adaptation performance. • Evaluation Metrics for UDA. Finally, our work seeks to uncover the most appropriate metrics to compare the performance of various UDA algorithms in the context of HAR models. To this end, in § 4.2 we propose three evaluation metrics -adaptability, persistence, and generalizability for UDA methods.</p>
<p>To the best of our knowledge, there is no work which has developed and analyzed UDA algorithms in the context of wearing diversity from the above four lenses. As such, this work makes a novel contribution to the activity recognition literature and aims to provide clear guidelines for ubicomp practitioners or HAR model developers to use UDA techniques in practice. Our key contributions are as follows:</p>
<p>• Borrowing on the theoretical foundations of UDA from machine learning literature, we developed three adaptation solutions in the context of wearing diversity. • We conduct a systematic study to uncover the performance of state-of-the-art UDA techniques on four HAR datasets collected from various body positions. • We propose Adaptability, Persistence and Generalizability as performance metrics to evaluate a UDA technique's performance on wearing diversity problem. • Through a set of more than 100 experiments on various body position pairs, we derive practical guidelines for UbiComp and HAR practitioners on how to train more accurate and robust HAR models.</p>
<p>Taken together, our experimental analysis paints a comprehensive picture of using unsupervised domain adaptation to address wearing diversity in HAR. Our findings can serve as practical guidelines for ubicomp practitioners as well as pave the way for future research on domain adaptation in HAR.</p>
<p>Quantification of Wearing Diversity</p>
<p>Due to the various form-factors and individual wearing preferences, mobile and wearable devices can be worn or carried by users in diverse ways, for example, a smartphone can be carried in a pocket or held in hand. As shown in Figure 1a, the accelerometer data captured from wearable devices across body positions show significant divergence, which is a strong indication of the domain shift caused by wearing diversity. Next, we analyze the impact of this domain shift on the accuracy of a HAR classifier. We trained a HAR model based on a state-of-the-art deep learning algorithm [5] to classify eight human activities based on accelerometer traces obtained from a wearable sensor placed in a thigh pocket. In Figure 1b, we plot the F 1 score obtained by applying the trained HAR model for thigh on test datasets from various body positions. We observe that the HAR model has an F 1 score of 0.94 when it is tested on the same body position on which it was trained (i.e., there is no domain shift).   However, in the presence of domain shift, the F 1 score of the classifier drastically reduces to as low as 0.05 when the model is tested on IMU data from the forearm.</p>
<p>Finally, in Figure 1c, we visually demonstrate the effect of domain shift on a classification model trained on data from thigh. More specifically, we input data from thigh, forearm and head to this classification model and plot the feature vectors obtained from the model. Principal component analysis (PCA) is used to plot the vectors in 2D space; the two principal axes are chosen considering all feature vectors from thigh, forearm and head. As evident, the features are well clustered for the data from 'thigh' (same as the training body position), which allows for higher classification performance. However, for the other two body positions, classes are not easily separable, resulting in worse classification performance.</p>
<p>RELATED WORK</p>
<p>While efforts to improve human activity recognition (HAR) have been continued for many years, assuring the robustness of the HAR is still an active area of research. Researchers have examined many different sources of variability, i.e., device heterogeneity [4], user variance [25], and wearing variability [40]. In this paper, we focus on wearing diversity problem, especially on building an HAR model that works robustly on multiple body positions.</p>
<p>Enhancing Robustness of Human Activity Recognition on Wearing Diversity</p>
<p>There exist two major approaches in enabling HAR models to perform robustly in diverse body positions. The first approach is to build a body-position-aware model which tries to predict both wearing position and activity, simultaneously. [6] was an early work in this direction, it used support vector machine and hidden markov model to predict wearing position and activity either independently or jointly. [46] suggested a pipelining approach. It first extracted orientation-independent features and passed them to the body position classifier. After body position being identified, the orientation-independent features and the body position information have been used to classify the activity. However, both works focused on a limited number of body positions (∼4 positions) and activities (∼5 activities). Sztyler [41] extended this approach to work with 8 activities across 7 body positions. The major limitation of this approach exists in its computational cost. Two classification model should run sequentially to detect current activity. Also, it consumes more memory as both wearing position classification model and body-position-specific activity classification models should be loaded into memory on runtime. In addition, it is not scalable. When a new wearing diversity arose, the developers have to collect new labeled dataset and rebuild the body position and activity classification model.</p>
<p>The second approach is to build a body-position-independent model by engineering body-position-independent features. Early works [20,38] applied decision tree and kernel function over common features that can be extracted from the accelerometer. However, they were limited in the number of activities and body positions. Nguyen et al. [32] selected effective features for each body position and combined them to generate a body-positionindependent feature set. The classification model based on the optimized feature set has shown 99.13% accuracy on classifying 13 activities over 4 different body positions. However, while feature engineering is effective on classical models that use hand-crafted features, it is hard to be applied to deep learning models where features are learned from the data.</p>
<p>Recently, Almaslukh et al. [5] trained a deep neural network over a dataset collected from multiple body positions to build a body-position-independent model. Our work significantly differs from it as we utilize an unlabeled dataset rather than a labeled dataset to generalize the model to multiple body positions, thereby reducing the cost of data collection.</p>
<p>Unsupervised Domain Adaptation Applied to Human Activity Recognition</p>
<p>There exist several studies that applied unsupervised domain adaptation to adapt the existing model for a smartphone to new wearable sensors.</p>
<p>HDCNN [21] used a feature matching approach to adapt the existing model to a smartwatch with an unlabeled dataset collected from the smartwatch. Specifically, they tried to reduce the discrepancy between two datasets after every convolutional and fully connected layer. They used Kullback-Leibler divergence as a distance measure. MotionTransformer [7] used a confusion maximization approach to build a translator that converts a trace from new wearable sensors to resemble the trace collected from smartphones. Akbari et al. [3] explored two adaptation techniques. They first extracted stochastic features by training variational autoencoder [23]. Then, they applied a feature matching approach to adapt the model to a target environment, using an unlabeled dataset collected from the target environment. Kullback-Leibler divergence is used to measure the distance between the labeled and unlabeled datasets.</p>
<p>Our approach differs from the above research in two aspects. First, the goal is different. We target to build models that work robustly on multiple body positions. In contrast, the above studies aim to build a new model that will only perform on a new wearable sensor attached to a specific body position. Thus, the evaluation of the above research is limited to adaptation performance. Second, our major contribution is on the comparison of adaptation techniques. We concentrate on giving UbiComp practitioners a guideline on applying UDA algorithms to wearing diversity problem.</p>
<p>UNSUPERVISED DOMAIN ADAPTATION FOR WEARING DIVERSITY</p>
<p>As mentioned earlier, the goal of this work is to do a systematic evaluation of various adaptation techniques with respect to the problem of wearing diversity in wearable devices. In this section, we present three classes of algorithms for performing unsupervised domain adaptation (UDA) that we will study in this paper.</p>
<p>Please note that we focus our analysis on HAR models trained with deep neural network architectures for two reasons: (a) in recent times, deep learning-based computational models have outperformed shallow models for HAR tasks [17,35], and (b) ability of neural networks to learn representations from unlabeled data is a key feature of unsupervised domain adaptation. Part (b) is especially important as UDA techniques are built upon this ability to learn representations even in the absence of labeled data. Other machine learning techniques that do not work on the principle of representation learning, e.g., Random Forest or Support Vector Machines, are hence incompatible with these UDA techniques and out of scope of our work.</p>
<p>Problem Formulation</p>
<p>Let us say we are presented with a source domain (or body position) S with input data X S and labels Y S . The input data (X S , Y S ) here corresponds to the labeled accelerometer or gyroscope data collected from a wearable sensor placed at the source body position. Using supervised learning, we can train a deep neural network on this labeled dataset as has been proposed in prior works on HAR [5,17]. A key property of deep neural networks is their ability to automatically extract meaningful feature encoding from the raw data -we denote the learned feature encoder for the source domain as E S : X S −→ F , and a classifier learned on top of the encoded features as
C S : F −→ Y S .
Formally, E S and C S are trained using supervised learning by solving the optimization problem:
min E S ,C S L supervised = −E (x s ,y s )∼(X S ,Y S ) K k =1 1 [k =y s ] <a href="1">log(C S (E S (x s ))</a>
where K denotes the number of activity classes. Indeed, as the feature encoder E S and the classifier C S have been trained solely on data from the source body position (e.g., thigh), it is unlikely that they will provide accurate inferences under domain shift, i.e., when they are tested on a new target body position (e.g., wrist). Therefore, the objective of unsupervised domain adaptation is to adapt the source feature encoder E S and the source classifier C S such that they can be applied to an arbitrary target domain T j for which an input dataset (X T ) is available, but without any labeled observations. Domain adaptation techniques rely on the assumption that there exists a feature representation of the sensor data which is invariant to changes in sensor body position, and is still powerful enough to encode the human activity data.</p>
<p>Unsupervised Domain Adaptation Techniques</p>
<p>Domain adaptation assumes the availability of two types of datasets:</p>
<p>• The source domain labeled dataset D c , which consists of pairs (x s ,y s ) where x s is the sensor data sampled from the IMU and y s is an activity label for x s . We now describe the three adaptation approaches for addressing the wearing diversity problem, namely Data Augmentation, Feature Matching and Confusion Maximization. The three techniques share a common objective: how to learn robust feature representations of inertial sensing data that are invariant to the body position where the sensor is placed. They however differ in how they achieve this objective: as we will describe, Data Augmentation uses the property of deep neural networks to learn robust feature representations from noisy data, Feature Matching achieves feature invariance by explicitly minimizing the distance between features of two domains, and Confusion Maximization achieves this objective using adversarial learning. Therefore, the three techniques are complementary in their approaches and help us in analyzing the performance of UDA techniques from a broad lens.</p>
<p>In the subsequent text, we describe each technique in detail.</p>
<p>Data augmentation. The simplest approach to train neural networks which are robust to domain changes is to perform data augmentation. Data augmentation mocks perturbations that commonly exist in accelerometer and gyroscope traces on a given labeled dataset. This means defining a set of either probabilistic or deterministic mappings X −→ X on the input space to enlarge the labeled dataset D c as depicted in Figure 2a. Let x ∈ R kT be a reasonably smooth k-axis time-varying signal (usually k = 3 or k = 6). Performing affine similarities in the k axes and continuously deforming the time axes with resampling yields a natural transformation group for wearable inertial sensors. Here, we use three transformations -rescaling, axis rotation, and time-warping -selected from IMU-suitable transformations proposed in Um et al. [44], which have shown a good performance.</p>
<p>• Rescaling scales each axis by a random coefficient sampled from c ∼ N (1, 0.1 2 ), which is related to IMU (Inertial Measurement Unit) scaling factor errors [2]. • Axis rotation rotates x with a random roll, pitch, and yaw. This simulates a variability caused by placement of the sensor.</p>
<p>• Time warping mocks deformation in the time axes; it adjusts time interval between data points in x and resamples from it. This perturbation is related to sampling rate instability that occurs when operating systems read IMU signals [4]. Overall, the objective of data augmentation based training is to enable a deep neural network to learn robust feature representations of the IMU data, that are invariant to the perturbations introduced in the data.</p>
<p>Feature matching. In this method for training neural networks, we explicitly add a loss term which minimizes a distance measure between the features extracted from different body positions. If B d ⊂ D d is a batch of data consisting of data B 0 and B 1 coming from body positions 0 and 1, respectively, then we minimize a distance measure between E (B 0 ) and E (B 1 ) where E represents a feature extracting neural network such as a CNN (E : X −→ F ). By minimizing the distance between the features in the training process, we force the model to learn features that are invariant to body positions.</p>
<p>The selection of a proper distance measure is the most important decision when applying feature matching. In our current implementation, we use (and minimize) the maximum mean discrepancy (MMD) [42] using Gaussian kernels as the distance metric between domain-specific feature representations. MMD distance is one of the most widely used distance measures that has been applied and proven its effectiveness in many different domains [18,21]. Equation 2 describes how the MMD distance is computed across two domains d and d ′ . A high-level representation of this method can be found in Figure 2b.</p>
<p>For a batch of data B d ⊂ D d we define the MMD loss as:
L MMD = E (x,d ),(x ′ ,d ′ ) ∈B d d=d ′ k E(x ),E (x ′ ) − E (x,d ),(x ′ ,d ′ ) ∈B d d d ′ k E (x ),E (x ′ ) ,(2)
where k is a sum of Gaussian radial basis function kernels
k (x,x ′ ) = c ∈C e − ∥x −x ′ ∥ 2 2c , C = {0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 25, 30, 100}(3)
The rationale for using the Gaussian radial basis function kernels is to project the feature representation E(x ) into a Reproducing Kernel Hilbert Space (RKHS) [34] which is essential to the computation of the MMD distance [16]. Here C is a hyper-parameter that we borrowed from the official implementation of MMD in TensorFlow.</p>
<p>We now provide intuition on how the MMD loss works. Let us say we are trying to adapt a model trained on data from Wrist to work on Chest. As such, the goal of the adaptation process is to find a feature representation of the data that is common for both Wrist and Chest. Therefore, we first extract the features for each body position using the feature extractor E and then apply the MMD loss to push the difference of the features to 0. In other words, we aim to align the feature spaces of both body positions.</p>
<p>Confusion Maximization. This technique is built upon the principle of domain adversarial training [14] which is also used for training generative adversarial networks. The key idea is to use an additional neural network called the domain discriminator h ψ , to make the feature encoder E domain-invariant using adversarial training. The goal of the domain discriminator is to distinguish data from the source domain and target domain -as such, during the training process, it aims to learn a binary classification model that can accurately separate the data from two domains. On the contrary, the goal of the feature encoder is exactly the opposite -it aims to confuse the domain discriminator by generating features that are invariant to source and target body positions, and hence cannot be easily separated. In this sense, the domain discriminator and feature encoder are adversaries of each other, tasked with competing objectives. They play this adversarial game of trying to defeat the other, and in the process, both become better at their respective tasks. Importantly from the perspective of wearing diversity, the feature encoder learns to generate features which are invariant to the body positions where the sensor is placed.</p>
<p>Please refer Figure 2c to see how the encoder and the discriminator are connected. In our current implementation, the discriminator is composed of two fully connected layers, with 100 hidden nodes in between.</p>
<p>The discriminator loss in the two domain case d ∈ {0, 1} is given by
L discriminator = − 1 |B d | (x,d ) ∈B d d log h ψ (E (x )) + (1 − d ) log 1 − h ψ (E (x )) .(4)
Our Contribution. It is important to highlight that from a theoretical viewpoint, the above three techniques are known in the machine learning literature. However, they are not black-box solutions that can simply be applied to any dataset or architecture to enable domain adaptation. As we will discuss in the subsequent sections, there are a number of factors that dictate the performance of UDA, including class mismatch between source and target domains, the choice of metrics used to compute divergence between the domains, and relative proportion of labeled and unlabeled data. As such, one of our contributions is to build upon the theoretical foundation of these techniques, and develop domain adaptation model architectures and data pipelines specific to the problem of wearing diversity. </p>
<p>Model Training Process
d ⊂ D d ; θ ← θ − Ω (L classification ,θ ,ϕ,B c ); ϕ ← ϕ − Ω (L classification ,θ ,ϕ,B c ); if method is Feature Matching then θ ← θ − Ω (L MMD ,θ ,ϕ,B d ); end if method is Confusion Maximization then ψ ← ψ − Ω (L discriminator ,θ ,ψ ,B d ); θ ← θ + Ω (L discriminator ,θ ,ψ ,B d ); end end
We now discuss how we leverage different UDA techniques to train robust HAR models. The training of the neural network proceeds by alternating minimizing the classification error and maximizing invariance of features.</p>
<p>We first sample a batch of labeled data B c ⊂ D c from the source body positions and a batch of unlabeled data B d ⊂ D d from the target body position.</p>
<p>In the first step, we use the labeled data B c to minimize the cross-entropy loss as in ordinary supervised training of neural networks, in order to train the parameter θ for the encoder and the parameter ϕ for the classifier.</p>
<p>In the second step, we train the encoder towards producing invariant features. In the case of feature matching, we pass the labeled (B c ) and unlabeled (B d ) batches to the encoder, compute the MMD loss in the feature space based on Equation 2 and minimize it in the training process. Alternatively, for Confusion Maximization, we train the parameter ψ for the discriminator neural network by optimizing the loss in Eq 4 and backpropagate this loss through the encoder with the opposite sign since the encoder has the exact opposite objective as the discriminator. Both the training steps are repeated until the training converges or an early stopping criterion is met. Note that Feature Matching and Confusion Maximization are different methods and are not trained simultaneously. We added them in the same algorithm for ease of explanation. In our evaluation, we will implement domain adaptation separately for both of these techniques and compare their performance.</p>
<p>We will describe the neural network architecture for the encoder and classifier in more detail in § 4.1.</p>
<p>STUDY DESIGN</p>
<p>In this section, we present the study design for evaluating the three adaptation techniques from the perspective of wearing diversity.</p>
<p>Experiment Setup</p>
<p>We begin by describing the datasets and pre-processing operations done on the data, and then proceed to discuss the deep neural network used for the HAR task.</p>
<p>Dataset. We selected the following four datasets for evaluating the impact of different wearing positions on HAR classifiers, with and without domain adaptation. RealWorld. The RealWorld HAR dataset [40] includes data recorded from 15 participants performing 7 activities: climbing stairs down and up, jumping, lying, standing, sitting, running/jogging, and walking. Users were instrumented with smartphones and smartwatches placed at 7 different body positions: head, chest, upper arm, waist, forearm, thigh, and shin; and accelerometer and gyroscope data was sampled from the devices simultaneously at a sampling rate of 50 Hz. Each participant performs each activity for 10 minutes, except for jumping (∼1.7 minutes).</p>
<p>There are three major reasons why this dataset is ideal to study the wearing diversity problem: a) the physical activities by the participants were performed in naturalistic settings as opposed to controlled experiments, b) the data across male and female participants are equally distributed which reduces the bias in our evaluation, c) to the best of our knowledge, this is the largest dataset of body position variability that is publicly available which makes it easy for other researchers to replicate our results.</p>
<p>Opportunity. The Opportunity Activity Recognition dataset [37] includes data recorded from 4 participants performing 5 activities: null, standing, walking, sitting, and lying. Users were instrumented with a custom-designed motion jacket and shoes, collecting accelerometer signals from total 19 body positions. We grouped sensors attached to the same body position, which resulted in an updated dataset of 8 different body positions: UPPERARM, LOWERARM, WRIST, HAND, BACK, HIP, KNEE, and FOOT. The accelerometer data were sampled from the devices simultaneously at a sampling rate of 30Hz. The users were asked to conduct pre-defined daily home activities for 15-25 minutes. This is a challenging dataset as sensors were placed on the clothes rather than being tied to the specific body position. Moreover, the dataset for each body position consists of traces from multiple sensors which were attached separately. Finally, there was no control in users' activity and the resulting dataset is not balanced. HHAR. The Heterogeneity Human Activity Recognition (HHAR) dataset [4] includes data recorded from 9 participants performing 6 activities: bike, sit, stairsdown, stairsup, stand, and walk. Users were instrumented with 8 smartphones and 4 smartwatches. All 8 smartphones were placed around waist, and smartwatches were worn on each arm. We named the 2 body position groups, phone and watch. Accelerometer data were sampled from the devices simultaneously at a sampling rate of 50 to 200Hz depending on the device. Each user performed each activity for 5 minutes. However, this dataset is noisy compared to the other datasets as timestamps are not contiguous and sampling rates are unstable. PAMAP2. The PAMAP2 Physical Activity Monitoring dataset [36] includes data recorded from 9 subjects performing 18 different activities. As 6 were optional activities, we only used 12 activities among them: ascending stairs, cycling, descending stairs, ironing, lying, nordic walking, rope jumping, running, sitting, standing, vacuum cleaning, and walking. Users were instrumented with 3 wireless inertial measurement units placed at 3 different body positions: head, chest, ankle; and accelerometer and gyroscope data were sampled from the devices simultaneously at a sampling rate of 100 Hz. Each user performed each activity for up to 3 minutes. We particularly selected this dataset to evaluate performance of domain adaptation under more diverse activity classes.</p>
<p>Data characteristics and pre-processing. The accelerometer traces used in our experiment have a shape of (150, 3), i.e., the 3-axis value of an accelerometer logged for 3 seconds with a sampling rate of 50 Hz. This duration was chosen empirically to align with the duration of various human activities in the dataset. Datasets collected with a different sampling rate are either upsampled and downsampled to match the sampling rate of 50Hz. The accelerometer traces are segmented into time windows of 3 seconds, without any overlap between the samples. If a 3-second-long trace includes an activity transition, timestamp noise, or data points without labels, the trace gets discarded. The whole dataset is normalized to be in the range of -1 and 1.</p>
<p>Every trace in the dataset has two labels: activity and body position that the trace was collected. Other information, i.e., participant ID and sensor type, is discarded.</p>
<p>Classification model. As explained earlier, we focus our analysis on HAR models trained using neural network architectures. As such, we design a convolutional neural network based on the work by Hammerla et al. [17] and Almaslukh et al. [5].</p>
<p>The model consists of two components: a feature extractor (encoder) and a classifier. The feature extractor is a 6-layer deep CNN with temporal (1-D) convolutional and pooling layers. We used LeakyReLU activations [27] with alpha = 0.3 and instance normalization [43] layers between convolutional layers for faster convergence. We also employed dropout regularization to avoid overfitting. The feature extractor takes as input a 3-second frame of motion data sampled at 50 Hz (150 samples) and outputs a 100-dimensional feature vector. This feature vector is then passed as input to the classifier which consists of one fully-connected layer and generates a k dimensional output where k is the number of activity classes (e.g., sitting, walking).</p>
<p>The proposed network architecture ( Figure 3) differs from [5,17] in three aspects. Firstly, all layers in the encoder are convolutional layers. We replaced the traditional max-pooling layers with convolutional layers with a large stride based on recent research [39] which showed that this simple substitution can speed up the training process without any loss in the accuracy. Secondly, kernel size has been engineered to focus on repetitive patterns of human activities that have a frequency of 0.5 to 3Hz, which may be related to whole-body movement [12]. Finally, global average pooling is used instead of a fully-connected layer to encode a feature vector from the  final output of convolutional layers. This is to reduce the number of weights to be learned. A baseline result on RealWorld dataset (See table 4a) showed that the performance is still on par to [5] while having a smaller model size by using a smaller number of filters and fully-connected layers.</p>
<p>Training process and mini-batch generation for UDA. We trained the network with stochastic gradient descent using the Adam gradient update rule [22] with learning rate 10 −3 , β 1 = 0.9 and β 2 = 0.999. We use a mini-batch size of 125 and follow the adaptation strategies as discussed in Section 3. The mini-batches are not time-aligned, and at every epoch, we shuffle the labeled and unlabeled dataset independently, resulting in an extremely low chance of feeding the same labeled-unlabeled mini-batch pairs to domain adaptation training steps.</p>
<p>Evaluation Metrics</p>
<p>To holistically address the wearing diversity problem, we propose three key metrics that any domain adaptation solution should improve, namely adaptability, persistence, and generalizability.</p>
<p>• Adaptability refers to the performance on target body position(s) supplied as an unlabeled dataset. This is an important metric when models need to be adapted between static body positions -for instance, if a source model was trained on wrist-worn sensors and subsequently need to be deployed for chest-worn sensors, we desire a high adaptation performance on Wrist→Chest experiment. • Persistence measures the performance on source body position(s) supplied as a labeled dataset, after it undergoes the domain adaptation process. It is particularly important because we would like the model to retain its performance in the source domain along with becoming better at doing inferences in the target domain. • Generalizability measures how well a model performs on body positions for which it was neither trained nor adapted. This property is critical because in real-world scenarios, wearing positions do not remain static -i.e., users can wear their device in unexpected positions and orientations. Therefore, a technique which produces more generalizable models is desirable. In this paper, we define Generalizability as the aggregate performance on all body positions that are available in a given dataset.</p>
<p>4.2.1</p>
<p>Choice of Macro-averaged F 1 score. To visualize our results, we report a single, macro-averaged F 1 score for each evaluation metric per experimental setting. Yet, we observe that a selection averaging policy can affect the nuances of the results, especially under the existence of a minority class in the data.</p>
<p>For a multi-class classification problem, we can calculate a F 1 score using three different averaging policy: micro-, weighted-macro-and macro-averaging (Equation 5) whereŷ l is true labels for class l.
F micro = F 1 (y,ŷ) , F weighted = 1 l ∈L |ŷ l | l ∈L |ŷ l |F 1 (y l ,ŷ l ) , F macro = 1 |L| l ∈L F 1 (y l ,ŷ l ) .(5)
While the conventional wisdom suggests using micro-averaging of F 1 score in cases of class imbalance, we observe that in our experiments, micro-averaging often overestimates the performance of adaptation. We hypothesize that the adaptation process sometimes leads to a class collapse -i.e., one particular activity class may end up with very poor accuracy after adaptation. Especially, if the collapsed class happens to be a minority class, we observe that micro-averaging tends to significantly overestimate the overall performance compared to macro-averaging. Table 2 depicts an exemplary scenario. Jumping (a minority class with 5% of the samples) collapses after adaptation, but the micro-averaged F 1 score does not reflect this issue as the percentage of correct prediction is still high. Table 2. Exemplary scenario that compares three averaging policies under a minority class collapse. Jumping, a minority class with 5% of the samples, collapses after adaptation, being classified into null class (Table a). Macro-averaging is the only policy that reflect such a collapse in its value (Table b).</p>
<p>(a) Confusion matrix. We believe the choice of F-1 score averaging technique is application-dependent. It is possible that for a certain human activity recognition application, we do not care about the performance of an extreme minority class. But there could be other applications (e.g., fall detection) where the performance of a minority class (e.g., presence of fall) is of primary interest.</p>
<p>In this paper, we primarily present a macro-averaged F 1 score, as we prefer a conservative estimate of the adaptation performance, while accounting for class collapses. However, we also report a comparison of the three F-1 score averaging policies on the Opportunity dataset (See Figure 7); the Opportunity dataset has a minority class -lying -that is 3.8% of the samples. Note that according to our empirical analysis, the selection of averaging policy does not change the relative performance among the adaptation techniques, thereby not affecting our conclusions.</p>
<p>Methodology</p>
<p>Our in-depth empirical analysis revolves around the following aspects of UDA:</p>
<p>• Comparison of adaptation techniques. One of the main focus of the paper is to unravel the performance of adaptation techniques under diverse adaptation scenarios related to the wearing diversity problem. By enumerating over body position pairs as a source and target body position, we try to find out (a) the relationship between body positions, e.g., amount of domain shift between two body positions and (b) how does the performance of adaptation techniques vary depending on the body positions. • Grouping of multiple body positions. Collecting labeled and unlabeled datasets from multiple body positions would be an intuitive choice if we aim to build a body-position-independent model. We experiment by creating intuitive groups of body positions to understand which body positions can lead to more robust HAR models. • Dataset size and class distribution. We further studied the effect of dataset properties on the performance of adaptation algorithms. We systematically altered the size of and the class distribution within the unlabeled dataset and reported the performance of adaptation techniques.</p>
<p>Comparison of Adaptation Techniques.</p>
<p>As discussed above, the purpose of this experiment is twofold: (a) to find the relationship between body positions, e.g., amount of domain shift between two body positions and (b) to study pros and cons of adaptation techniques in relation to body positions supplied as labeled and unlabeled datasets.</p>
<p>For each body position, we first train a CNN model for the HAR task and evaluate its performance on other body positions. This helps in quantifying the impact of wearing diversity for different datasets and serves as a key baseline for domain adaptation approaches. Thereafter, we leverage the three adaptation techniques discussed in § 3 and adapt the pre-trained CNN models for a different unlabeled body position. Based on the various evaluation metrics introduced in § 4.2, we compare the efficacy of different adaptation approaches in solving the problem of wearing diversity.</p>
<p>Use of Multiple Body Positions Together.</p>
<p>Collecting accelerometer traces from two or more body positions is an intuitive approach if we aim to build truly body-position-independent HAR models. For example, our experiment ( Figure 4) showed that if the model is trained with a dataset collected from three body positions, its generalizability performance goes up to 61% compared to 38% when the model is trained with a single body position.  Therefore, we extend our analysis to examine the effect of multiple body positions on unsupervised domain adaptation. More specifically, we ask: How should we pick body positions to get the best performance after applying domain adaptation?</p>
<p>To answer the question, we hand-crafted the mixtures of three body positions (Table 3). We first focus to contrast body positions with high movement and low movement. Body positions have different degrees of freedom and movement intensity based on their position on the limb. For example, the forearm and ankle have larger degrees of freedom compared to the thigh and chest. Also, the acceleration intensity is bigger on the forearm and ankle as they are placed far from the pivots, i.e., shoulder and pelvis. One intuition is that picking body positions placed on LIMB END as a labeled dataset can help the classification model to learn robust features from more challenging accelerometer signals. However, on the other hand, [41] has reported that waist is the best body position to train a body-position-independent model, as the waist is closer to a center of mass of the whole body and is effective in selecting features related to whole-body movement. We thus compared two adaptation scenarios, LIMB END→LIMB MIDDLE and LIMB MIDDLE→LIMB END, wherein LIMB END contains body positions with a higher degree of freedom (e.g., forearm, shin) and LIMB MIDDLE contains positions with a lower degree of freedom (e.g., thigh, chest). We also added body position groups collected from a single limb to test across-the-limb adaptation scenarios.  (Table 3a) is designed to examine the effect of including body positions with high movement within the labeled dataset. The mixtures on the right (Table 3b) is designed to simulate the case that a labeled dataset only comprises traces collected from a single limb, i.e., a training dataset collected for a smartwatch. To study the effect of dataset size, we progressively increase the amount of unlabeled data provided to the UDA algorithms in increments of 500 samples, and study the impact of the number of samples on UDA evaluation metrics.</p>
<p>Secondly, we simulate class distribution mismatch between labeled and unlabeled datasets. There would be two approaches in generating class distribution mismatch: (1) changing the ratio between the activity classes and (2) adding an unseen new activity to the unlabeled dataset. Here we only focus on the first option and left the second option as future work. While there are many options to simulate class distribution mismatch, we decide to reduce samples from the more dynamic activity classes, i.e., running, jumping, climbing up, and climbing down. This is done to match in-the-wild data collection scenarios wherein such activities are likely to have a smaller number of samples than the more static activities such as sitting or standing.</p>
<p>RESULTS</p>
<p>In this section, we present the experimental results of our investigation into unsupervised domain adaptation approaches for wearing diversity in the context of HAR tasks. The highlights from our experimental results include:</p>
<p>• Overall, Feature Matching outperforms other techniques in adaptation performance and in total, FM succeeded to improve HAR accuracy on target body positions in 93.4% of all experiments conducted. Feature Matching was particularly powerful in learning hidden mappings between domains, thereby boosting the target classifier accuracy by as much as 53% (from 14% to 67%). It also preserves the integrity of the source domain model, with less than a 5% decrease observed in persistence of the model in 99% of cases.</p>
<p>• Data Augmentation appears to be a powerful technique when adapting between body positions in the upper torso. However, it suffers a higher ( 10%) accuracy drop in persistence on average. • The adaptation performance of Confusion Maximization is weak, but it can be used for special cases when the domain shift between body positions is too high, e.g., adapting from foot to upper torso. Data augmentation and feature matching fail in such challenging scenarios. • To develop robust body-position-independent HAR models, a collection of labeled and unlabeled datasets from (1) </p>
<p>Baseline Performance</p>
<p>We first report the performance of HAR models when they are trained and tested on the same body position. This represents the ideal scenario with no presence of domain shift, thereby we expect high classification accuracies. Table 4 aggregates and displays baseline performance of each dataset. For instance, in the RealWorld dataset, a model trained on head and chest provide F-1 scores of 0.86 and 0.94 when tested on the same body position on which they were trained. While the classification performance was high on RealWorld, HHAR, and PAMAP2 datasets, we observed that the performance on Opportunity dataset was low, especially on the wrist, hand, and foot. The accuracy drop was due to difficulties in classifying the 'lying' activity from the body positions at the end of body limb where variability within the activity is high. This result is in line with a prior work [41] which also reported that discriminating static activities such as 'lying' is more challenging than dynamic activities.</p>
<p>In Table 4 (right part), we further analyze the baseline performance on body positions other than the trained body position. While the drop in accuracy was severe as discussed in Section 1.1, we were able to find body position groups that the accuracy drop was relatively low. In the case of RealWorld dataset, the body positions near the torso -i.e., head, chest, upper arm -showed low accuracy drop when tested on each other. For example, the model was head suffered a 72% drop when tested on waist, but the drop for chest was much less (22%). On Opportunity dataset, body positions from the same limb -i.e., wrist, hand, and upper arm -showed similar behavior. Finally, in all datasets, foot or ankle showed a significant accuracy drop when tested on other body positions. This implies that the domain shift between the foot and other body positions is bigger than other pairs.</p>
<p>Comparison of the Adaptation Techniques</p>
<p>We now present the results after applying the adaptation techniques in a total of 106 experimental settings, varying the source and target body positions.</p>
<p>Interpreting the heatmaps. Before presenting our in-depth results in the form of heatmaps, we explain how to read and interpret them. We will present a figure comprising 9 heatmaps for each dataset ( Figure 5, 6, 8a, and 8b). Each column corresponds to an adaptation technique; from the left, each column will contain results for the data augmentation, feature matching, and confusion maximization.</p>
<p>A column consists of three heatmaps. The first heatmap depicts results on Adaptability, i.e., accuracy on the target body position supplied as an unlabeled dataset. The higher the adaptability, the better. The second heatmap represents Persistence, accuracy on the source body position after adaptation. As adaptation can potentially degrade the HAR accuracy in the source domain, we would like this accuracy drop to be small. The third heatmap at the bottom reports Generalizability, accuracy on all possible body positions. Each heatmap is composed of cells and each cell denotes one experimental setting. The body position written on the left of the cell denotes the source body position supplied as a labeled dataset. The body position written on the top of the cell denotes the target body position supplied as an unlabeled dataset. By reading the two body positions, we can find out the experiment setting a cell is representing. Delving deeper, the cell has three attributes. First, the color denotes an improvement/decrease compared to the baseline. If the color is green, the F 1 score improved due to adaptation; if the color is pink, the F 1 score decreased (as will be the case for Persistence). Second, the number inside a cell denotes the actual F 1 score for the given metric. Finally, the orange border around a cell shows that the given cell has the best performance on the given experimental setting among three adaptation techniques. Confusion maximization won over other adaptation techniques when adapting from FOOT (See embedding diagram on Figure 9b).</p>
<p>Macro-averaged F₁ score</p>
<p>Micro-averaged F₁ score</p>
<p>Data Augmentation Feature Matching Confusion Maximization</p>
<p>Weighted-averaged F₁ score Fig. 7. Comparison of three averaging policies on Opportunity dataset. We pick adaptation performance and Opportunity dataset as they are especially susceptible to a class collapse. The Opportunity dataset has 5 classes -null, standing, walking, sitting, and lying. The lying class is a minority class taking only 3.8% of the samples. The overall pattern of improvement (depicted as colors) is similar; yet, we observe that micro-and weighted-averaged F 1 scores show much higher values compared to macro-averaged F 1 score. The difference deepens as the domain shift increases. Such a difference is mainly caused by low precision/recall on the lying class. As such, we select macro-averaged F 1 score as the metric in rest of the paper to best reflect the effect of class collapse.  Figure 9a represents the adaptation scenario that feature matching has shown a supreme performance. Confusion maximization fails to discriminate standing and laying. Figure 9b depicts the scenario that confusion maximization performed the best among three adaptation techniques. While all adaptation algorithms failed to formulate clear clusters, confusion maximization generated features that are most uniform across body positions.
(a) HHAR dataset. †1 †1 †1 †3 †2 †2 †1 †5 †6 †3 (b) PAMAP2 dataset
(a) FM+DA on RealWorld.</p>
<p>(b) CM+DA on Opportunity.</p>
<p>(c) Embedding diagram after applying FM+DA. Compare with Figure 9a.   (Figure 10b). Using UDA algorithms with data augmentation clearly outperforms the sole use of adaptation techniques. Nonetheless, the persitence was harmed as if data augmentation is applied. Embedding figures are drawn on the right (Figure 10c and 10d) highlights the effectiveness of using data augmentation with UDA algorithms.</p>
<p>Data augmentation. When it comes to Adaptability, surprisingly, data augmentation shows comparable performance to UDA algorithms. Data augmentation provided the best F 1 score among three adaptation techniques on 50 experimental settings, which is 47% of 106 experimental settings we experimented with. Data augmentation was particularly effective on body positions within the upper torso. This pattern could be found repeatedly on different datasets. In the case of RealWorld dataset, data augmentation outperformed other techniques on the following body positions: head, chest, upper arm, and forearm. Similar patterns can be observed for the Opportunity and PAMAP2 datasets. Data augmentation, however, performs poorly from the viewpoint of Persistence. In most cases, data augmentation result in 5 to 10% accuracy drop on source body position. This is primarily because the statistical perturbations done to enable data augmentation may not correspond to real human behavior, which in turn degrades the accuracy of the source model. On the other hand, the unsupervised domain adaptation techniques use real traces collected from other body positions -as such, we expect UDA techniques to show better performance in terms of Persistence.</p>
<p>Feature matching. Feature matching was overall the most preferable adaptation technique, showing good performance on Adaptability, Persistence, and Generalizability. The application of feature matching technique has improved performance on target body position on 93.4% of all experimental settings. Data augmentation and confusion maximization improved only 90.6% and 74.5% of the pairs, respectively. When it comes to Persistence, the performance was superior; feature matching improved performance on the source body position on 54.7% of the pairs. Moreover, if we count the number of experiments that accuracy drop on the source body position was less then 5%, it added up to 105 pairs, which is 99% of total 106 pairs experimented.</p>
<p>Feature matching was able to learn the semantic similarity between body position pairs. When adapting from shin to body positions in upper torso (in RealWorld dataset), or from knee to other body positions (in Opportunity dataset), feature matching showed the best Adaptability on target body positions. Figure 9a displays the experimental setting, Waist→Chest, in which feature matching shows the best performance across the three adaptation techniques. In the top row, we can observe that the feature embeddings of waist and chest before adaptation are quite different. By learning body-position-independent features using the Feature Matching technique, the embeddings displayed in the second row have a high alignment. Confusion maximization, on the other hand, worsens the feature embeddings in this case.</p>
<p>Confusion maximization. Confusion maximization turned out to be the weakest technique when it comes to Adaptability. This is a particularly interesting result because adversarial learning techniques such as Confusion maximization have shown significant improvements in computer vision literature for domain adaptation. However, for the task of wearing diversity in HAR, it was outperformed by other, arguably much simpler, techniques.</p>
<p>That said, we observed a few unique experimental settings where confusion maximization outperforms data augmentation and feature matching. On Opportunity dataset, when source body position is FOOT and we adapt to body positions in the upper torso, confusion maximization performs better than feature matching. As discussed in the baseline results, FOOT and body positions in the upper torso display a big domain shift between them, because the impact from the ground usually makes accelerometer signals collected from foot different from other upper body positions. This can be verified from the embedding in Figure 9b where the classification model trained on FOOT shows jumbled feature embeddings for WRIST (the first row). In this case, feature matching does not perform well, because the distance measure between two domains does not convey any useful information to the training process. While the distribution of the feature vectors resembles each other after applying feature matching techniques (second row), the distribution of activity within the shape is completely different. On the other hand, the embedding diagram from the confusion maximization approach succeeded in placing null activity traces (colored in blue) in a similar region and provided the best adaptation performance.</p>
<p>Combining UDA algorithms with data augmentation. Data augmentation can be used in conjunction with UDA algorithms. Before applying the UDA algorithms, we may enlarge the labeled dataset by applying data augmentation. We observe that combining these two techniques significantly improve the Adaptability of models ( Figure 10). Feature matching technique combined with data augmentation (FM+DA) showed the best adaptation performance on 57.5% of body position pairs, winning over data augmentation, feature matching, and confusion maximization techniques applied independently. However, the Persistence was worse than feature matching, showing 5 to 10% of accuracy drop on the source body position. Interestingly, the combination of these techniques enhances the Generalizability potential of the model as can be inferred from the rightmost feature embeddings in Figure 10.</p>
<p>Use of Multiple Body Positions Together</p>
<p>The choice of body positions is critical for the success of UDA. In this section, we study whether clustering different body positions into groups enhances the performance of UDA techniques. In other words, instead of adapting models from a single source position to a single target position -if we do the adaptation over a group of positions, can it result in better performance? As shown in Table 3, we cluster the body positions based on skeletal adjacency and similarity in their degrees of freedom. For example, TORSO group comprises of samples collected from head, chest and waist.</p>
<p>Our findings reveal a clear pattern that could be of great practical significance for HAR developers. We observe that if the training dataset contains inertial data traces from (1) multiple body limbs and (2) body positions with high degrees of freedom, it provides the highest chance of building a robust body-position-invariant model that can be used with a variety of body positions.  More specifically, on comparing the generalization performance of LIMB END→LIMB MIDDLE and LIMB MIDDLE→LIMB END, we observe that using LIMB END as a labeled dataset is more beneficial than using LIMB MIDDLE as a labeled dataset. Figure 12 explains this result. The upper row shows embeddings of the source domain, while the lower row shows the embedding of the target domain. While the activity clusters for the source domain (LIMB MIDDLE) look more clean and distinctive in Figure 12b, the model trained on LIMB MIDDLE eventually fails on forearm and shin, which are target domains with high degrees of freedom. This is also confirmed by Figure 11a which shows that feature matching fails to improve the adaptability of LIMB MIDDLE→LIMB END over data augmentation. On the contrary, the embedding diagram Figure 12a) generated by the HAR model trained on LIMB END may look jumbled (top row), but it succeeded in generating clearer activity clusters on LIMB MIDDLE (bottom row). A similar pattern exists on the body position mixtures constrained to a single limb. If we compare the homogeneity within the body position mixture, TORSO will have the least heterogeneity, and ARM and LEG will have larger heterogeneity compared to TORSO. If we focus on the feature matching technique, we found that the generalizability increased when we select ARM or LEG as a labeled dataset. Also when a labeled dataset is fixed, the use of ARM or LEG as an unlabeled dataset further improves the generalizability.</p>
<p>Effect of Dataset Properties</p>
<p>Size of Unlabeled Dataset.</p>
<p>A key consideration in performing domain adaptation is the amount of unlabeled data needed in the target domain. Although the collection of unlabeled data is cheap, it is desirable if we can adapt models with as few data requirements as possible. In this vein, we now study how the size of the unlabeled dataset affects the performance of domain adaptation techniques.</p>
<p>In Figure 13a and 13c, we vary the amount of unlabeled target domain data for the (waist→thigh) adaptation. Our results show that the feature matching technique can provide accuracy improvements in the target domain with just 500 samples of unlabeled data, which suggests that explicitly adjusting feature embeddings of the model (i.e., by minimizing the MMD distance) works effectively with small amounts of data. On the other hand, the Confusion Maximization technique performs poorly with a small amount of unlabeled data, but its performance gradually improves as the size of the unlabeled dataset increases. Figure 13c shows the impact of unlabeled data on Persistence -here both techniques show gradual improvements in their persistence as the amount of unlabeled  data increases. We also observe similar patterns for feature matching and confusion maximization techniques for other combinations of body positions. Next, we study the impact of unlabeled dataset size on adapting HAR classifiers developed using heterogeneous body position data. As illustrated in Figure 13b and 13d, we choose the (LIMB END→LIMB MIDDLE) as an example adaptation task. In contrast to the previous result on adapting for single-body positions (waist→thigh), here we observe that both FM and CM techniques converge with just 500 samples of unlabeled data. The adaptability performance does not show a significant improvement thereafter, however, as more unlabeled data is fed to the model, its persistence improves significantly. In the interest of space, we do not report results of other heterogeneous adaptation tasks, however we observe a similar pattern of both techniques converging much faster for heterogeneous adaptation tasks as compared to single body position adaptation.</p>
<p>Class Distribution Mismatch in Adaptation.</p>
<p>Although the collection of the unlabeled dataset from a target domain is cheap, it is important to note that we have little control over the class distribution in the unlabeled data, that is, we do not know a priori the type and proportion of different activity classes in the unlabeled target data. Therefore, it is critical that we evaluate how well do the UDA techniques work under class distribution mismatch between source and target domains.</p>
<p>We simulate three scenarios of class mismatch: a) no mismatch wherein the unlabeled target data has the same class distribution as the source domain, b) low mismatch wherein there is at least a 5% mismatch for each activity class, and c) severe mismatch which has at least a 10% mismatch for each activity class. The three scenarios were simulated by undersampling from dynamic activities (climbing up/down, jumping, running) and oversampling from static activities (standing, sitting, lying) as illustrated in Figure 14a. Figures 14b and 14c show the results of class mismatch for Feature Matching and Confusion Maximization technique. We observe that as the severity of class mismatch increases, the performance of both UDA techniques degrade for single position adaptation (i.e., waist→thigh and forearm→chest). However, heterogeneous position adaptation is much more robust to class mismatch and particularly for the Confusion Maximization technique, there is a minimal impact of class mismatch on the adaptation performance. We surmise that using heterogeneous body positions inherently introduces significant variabilities in the adaptation process and as such, the model is able to learn robust domain-invariant features even under the presence of class distribution mismatch.</p>
<p>There are two main takeaways from our results: firstly, both the domain adaptation techniques are not robust against class mismatch scenarios and further research is needed to account for class mismatch in the adaptation process. Secondly, model developers can consider using heterogeneous body positions in the adaptation process as a first step towards alleviating the class mismatch problem.</p>
<p>Practical Considerations for Unsupervised Domain Adaptation</p>
<p>Collect training data from diverse body positions. HAR model developers should strive to have position diversity within a labeled dataset. In order to make HAR models generalize better and be body-position invariant, the labeled dataset should be collected from multiple body positions, and preferably, from the body positions with higher degrees of freedom (e.g., forearm). The diversity in the labeled dataset improves the stability and performance of unsupervised domain adaptation, even with a small amount of unlabeled data or class distribution mismatch.</p>
<p>Choice of UDA algorithms. Data augmentation remains a simple yet powerful technique to increase the generalizability of HAR models, and our results suggest that when used in combination with a UDA technique, data augmentation can significantly improve adaptation performance. Further, data augmentation could be useful to address wearing diversity between sessions or users -these diversities are primarily caused by change in sensor orientations [29] and are easy to simulate with data augmentation techniques. Finally, our results show that Feature Matching is a promising UDA technique and often outperforms more complex algorithms such as Confusion Maximization. More importantly, it can increase the performance of the model to a new body position, while maintaining its Persistence in the original position.</p>
<p>Sensitivity to class distribution. UDA is sensitive to class distribution. For a labeled dataset, the model developers should be aware of the existence of a minority class, and in particular watch out for a class collapse of the minority class in the adaptation process. For an unlabeled dataset, the class mismatch with the labeled dataset should be examined before applying a UDA technique, as class mismatch with the labeled dataset degrades the adaptation performance. We also note improving the performance of UDA under class or label space mismatch is an active area of research in the machine learning community [9,28].</p>
<p>UDA is not a silver bullet. It is critical for model developers to understand the environment (e.g., body position) under which an unlabeled dataset is collected. This will help in choosing an appropriate UDA technique and estimate the effectiveness of domain adaptation. Our results show that UDA is not a "silver bullet" solution that can be applied blindly to any HAR model or an unlabeled dataset. It is most effective when body positions undergoing adaptation either have structural similarity or exhibit similar degrees of freedom, e.g., when the body positions are all placed in the upper torso. If there is a high domain shift between the labeled and unlabeled datasets, the improvement could be low or even negative -e.g., the adaptation from FOOT to HAND.</p>
<p>In this section, we discuss the limitations of this paper and outlook future research directions.</p>
<p>Alternative interpretations of wearing diversity. Our paper was limited to exploring wearing diversity when a inertial sensing device is placed on multiple body positions. However, wearing diversity can arise from other real-world scenarios as well. For example, Min et al. [29] explored intra-wearing, inter-wearing, and inter-user wearing variabilities, i.e., those that occur during or in between wearing sessions and users. Although an in-depth study of these variabilities was out of scope of our work, we surmise that carefully designed UDA techniques could be effective in these scenarios as well, because the domain shift induced by the body position variability is bigger compared to that from intra-wearing, inter-wearing or inter-user differences [29,41].</p>
<p>Limited test on model architecture. Our investigation of unsupervised domain adaptation was limited to one model architecture. That said, our model architecture has shown on-par and sometimes better performance compared to the state-of-art CNN model [5] and therefore, it was a reasonable choice for our study. Nevertheless, in the real world, the model structure should be adjusted to match the available computing resources and it is important the understand how UDA performance will be affected by a change in the model structure.</p>
<p>Mobile and Wearable Implementations. In our work, we train the domain adaptation models centrally on the cloud and assume that unlabeled data from mobile and wearable devices will be uploaded to the cloud. Indeed, this raises a number of privacy issues for the users and hence, there is a need to explore distributed privacy-aware solutions for implementing domain adaptation and other transfer learning solutions on edge devices.</p>
<p>Extension to other UDA approaches. Unsupervised Domain Adaptation is a very active area of research for the machine learning community, and new algorithms are being proposed at a rapid pace. It is imperative that other reasonably mature UDA algorithms should be applied to the HAR task and evaluated under the framework of Adaptability, Persistence and Generalizability proposed in this paper.</p>
<p>Comparison with other fully-supervised and domain generalization algorithms. The recent holistic evaluation on semi-supervised learning [33] reported that carefully-tuned fully-supervised baselines which do not use unlabeled dataset at all can achieve similar accuracy with semi-supervised algorithms. Our result also reported cases where data augmentation wins over feature matching and confusion maximization techniques. Future work should expand the comparison to other fully-supervised/domain generalization algorithms, e.g., adversarial data augmentation [45], variational autoencoder [23], episodic training [26].</p>
<p>CONCLUSION</p>
<p>While wearable devices offer opportunities for novel interactive applications and ubiquitous monitoring, the quality of inertial sensing remains a bottleneck due to a number of real-world diversities. In this paper, we studied the potential of using unsupervised domain adaptation (UDA) to address the issue of wearing diversity. Our results show that UDA algorithms can improve the accuracy of HAR classifiers on unlabeled body positions by as much as 53%. We also uncovered several limitations and implicit data-related assumptions without which the UDA algorithms suffer a major degradation in accuracy. We hope that our work can serve as practical guidelines to help HAR practitioners and researchers in developing UDA-based HAR models.</p>
<p>traces collected from a trouser pocket, a chest pocket and an armband for the same physical activity. Variations across body positions are clear and significant.</p>
<p>The F 1 score of the classification model trained on thigh when tested on other body positions.(c) Feature vectors generated by the classification model trained on thigh are projected into 2D space using PCA.</p>
<p>Fig. 1 .
1The wearing diversity is typical in many wearable devices. Such variability results in huge variance in collected accelerometer signals(Figure 1a). Human activity recognition models that do not consider wearing diversity may suffer from significant accuracy drops(Figure 1b). For example, the classification model trained on thigh forms clear clusters on traces from thigh, but traces from forearm and head fail to form a cluster(Figure 1c).</p>
<p>•Fig. 2 .
2The unlabeled dataset D d which consists of pairs (x i ,d ) where d is the body position from which the sensor data x i was recorded. No labels are available for x i . High-level diagram depicting operation of three adaptation techniques. Note that data augmentation does not use unsupervised dataset at all and is considered as a domain generalization technique.</p>
<p>Algorithm 1 :
1Alternating minimization for domain adaptation using Feature Matching and Confusion Maximization. Data Augmentation is not shown in the algorithm as it is typically done apriori on the entire labeled dataset D c input : datasets D c , D d , gradient update rule Ω, method ∈ {data augmentation, FM, CM} output : Trained parameters θ , ϕ, ψ for number of steps do sample batches B c ⊂ D c and B</p>
<p>Fig. 3 .
3The architecture of the deep neural network model used in evaluation. Conv stands for convolutional layer, and FC stands for fully connected layer. The vector under Conv denotes [kernel size, stride] respectively. The exact size of output vectors are written under each layer.</p>
<p>body positions included in the training dataset</p>
<p>Fig. 4 .
4F 1 score improvement on the evaluation dataset collected from all body positions. Having diverse body positions within the training set clearly improves robustness to wearing diversity. The training and test dataset used in this figure is built from RealWorld dataset[40].</p>
<p>Table 3 .
3The composition matrix to build the mixture of the body positions. Body position mixtures on the left</p>
<p>position mixture selected from multiple limbs LIMB END head forearm shin LIMB MIDDLE chest upperarm thigh (b) Body position mixture selected from a single limb TORSO head chest waist ARM chest upperarm forearm LEG waist thigh shin 4.3.3 Dataset Size and Class Distribution. The performance of data-driven unsupervised domain adaptation algorithms heavily depends on how the underlying datasets are structured. We here examine the effect of size and activity distribution of the unlabeled dataset on the performance of UDA algorithms.</p>
<p>multiple body limbs and (2) body positions with high degrees of freedom is crucial. • There is a significant impact of dataset size on UDA techniques. While Feature Matching can provide superior adaptation performance with very small amounts of unlabeled data (500 samples), Confusion Maximization requires large amounts of data to reach the same accuracy levels. • There is a significant adverse impact of Class Distribution Mismatch on the accuracy of domain adaptation when adapting across individual body positions. This impact can be minimized by incorporating heterogeneous body positions in the adaptation process.</p>
<p>Fig. 5 .Fig. 6 .
56Comparison of three adaptation techniques on RealWorld dataset. Each column represents the results for the adaptation techniques written below. From the top, each row depicts improvement/decrease in Adaptability, Persistence and Generalizability. Each of the square cell inside heatmaps corresponds to an experimental setting; a body position written on the left is where the labeled dataset is collected, and a body position written on the top is where the unlabeled dataset is collected. The value written inside the cell denotes the absolute F 1 score. The color visualizes the amount of improvement/decrease compared to the baseline. The border around the cell means that the corresponding adaptation technique showed the best performance among three techniques applied under the same experimental setting. If we focus on the bordered cells, we can find clear patterns; between body position in the upper torso, data augmentation outperformed UDA techniques. However, when adapting between the upper torso and lower body, feature matching showed the best performance. Nevertheless, when it comes to Persistence and Generalizability feature matching championed in most of the experimental settings. Comparison of three adaptation techniques on Opportunity dataset. LikeFigure 5, each column represents different adaptation techniques and each row denotes improvement/decrease in Adaptability, Persistence and Generalizability (from top to bottom).</p>
<p>Fig. 8 .Fig. 9 .
89Comparison of three adaptation techniques on HHAR and PAMAP2 dataset. From left, each column depicts results for Data augmentation, Feature matching, and Confusion maximization. A dagger with a number besides shows the number of classes that collapsed, i.e. classes that no trace has been classified into. PAMAP2 dataset demonstrated occurrences of collapsing classes. Embedding diagram demonstrating the effect of UDA techniques.</p>
<p>( d )
dEmbedding diagram after applying CM+DA. Compare with Figure 9b.</p>
<p>Fig. 10 .
10Performance improvement/decrease of FM+DA on RealWorld (Figure 10a) and CM+DA on Opportunity dataset</p>
<p>Fig. 11 .
11→ ARM TORSO → LEG ARM → TORSO ARM → LEG LEG → TORSO LEG → F 1 score on all body positions (generalization). (a) shows that mixing body positions with high degrees of freedom for a labeled dataset is better for generalization. (b) also demonstrates that the unlabeled dataset collected from limb with high variability result in higher generalization performance. (a) Embedding diagram drawn with an encoder trained with LIMB END body position mixture. (b) Embedding diagram drawn with an encoder trained with LIMB MIDDLE body position mixture.</p>
<p>Fig. 12 .
12Embedding diagram explains why body position mixture with high variability performs better. While activity clusters are more clean and notable inFigure 12b, the encoder trained on LIMB MIDDLE fails to form clear clusters on shin and forearm which are body positions with high variability. Formation of activity clusters is essential for feature matching technique to work.</p>
<p>Fig. 13 .
13F 1 score measured on the source and target body position(s) varying the amount of the unlabeled target dataset.</p>
<p>Fig. 14 .
14F 1 score measured on target body position(s) varying the class distribution of the unlabeled target dataset.</p>
<p>Table 1 .
1Dataset informationName # activity # bodypos # train # eval sampling rateRealWorld [40] 
8 
7 
118616 29652 
50Hz 
Opportunity [37] 
5 
8 
90490 22616 
30Hz 
HHAR [4] 
6 
2 
30280 5342 
50∼200Hz 
PAMAP2 [36] 
12 
3 
30726 7680 
100Hz </p>
<p>Table 4 .
4Baseline F 1 scores for HAR task when the model is trained and tested on the different body positions. This represents the scenario wherein domain shift due to wearing diversity is not considered at all. BP stands for body position.(a) RealWorld </p>
<p>trained BP head chest upperarm forearm waist thigh shin </p>
<p>head 
0.86 
-
0.60 
0.51 
0.26 
0.14 
0.37 
0.23 
chest 
0.94 
0.28 -
0.32 
0.25 
0.30 
0.42 
0.24 
upperarm 
0.89 
0.47 0.58 
-
0.20 
0.18 
0.44 
0.35 
forearm 
0.88 
0.16 0.12 
0.11 
-
0.39 
0.12 
0.09 
waist 
0.93 
0.16 0.22 
0.05 
0.25 
-
0.15 
0.02 
thigh 
0.94 
0.39 0.39 
0.45 
0.05 
0.16 
-
0.40 
shin 
0.92 
0.06 0.29 
0.33 
0.03 
0.09 
0.30 
-</p>
<p>(b) Opportunity </p>
<p>trained BP upperarm lowerarm wrist hand back hip knee foot </p>
<p>upperarm 
0.75 
-
0.40 
0.53 0.42 0.38 0.53 0.44 0.24 
lowerarm 
0.78 
0.33 
-
0.31 0.28 0.33 0.45 0.23 0.22 
wrist 
0.64 
0.42 
0.27 
-
0.43 0.21 0.39 0.37 0.23 
hand 
0.69 
0.35 
0.15 
0.47 -
0.28 0.41 0.37 0.25 
back 
0.77 
0.33 
0.26 
0.25 0.26 -
0.31 0.40 0.22 
hip 
0.74 
0.32 
0.18 
0.33 0.30 0.26 -
0.38 0.11 
knee 
0.80 
0.31 
0.10 
0.32 0.28 0.20 0.24 -
0.08 
foot 
0.51 
0.14 
0.13 
0.16 0.16 0.21 0.17 0.13 -</p>
<p>(c) HHAR </p>
<p>trained BP phone watch </p>
<p>phone 
0.99 
-
0.20 
watch 
0.86 
0.22 
-</p>
<p>(d) PAMAP2 </p>
<p>trained BP head chest ankle </p>
<p>head 
0.86 
-
0.23 
0.08 
chest 
0.88 
0.16 -
0.02 
ankle 
0.86 
0.01 0.02 
-</p>
<p>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 4, No. 1, Article 39. Publication date: March 2020.</p>
<p>New technology tracks food intake by monitoring wrist movements. 2017. New technology tracks food intake by monitoring wrist movements. http://gadgetsandwearables.com/2017/03/29/food-tracking/. Accessed: February 10, 2020.</p>
<p>A standard testing and calibration procedure for low cost MEMS inertial sensors and units. P Aggarwal, X Syed, N Niu, El-Sheimy, The Journal of Navigation. 61P Aggarwal, Z Syed, X Niu, and N El-Sheimy. 2008. A standard testing and calibration procedure for low cost MEMS inertial sensors and units. The Journal of Navigation 61, 2 (2008), 323-336.</p>
<p>Transferring activity recognition models for new wearable sensors with deep generative domain adaptation. Ali Akbari, Roozbeh Jafari, IPSN. Ali Akbari and Roozbeh Jafari. 2019. Transferring activity recognition models for new wearable sensors with deep generative domain adaptation. In IPSN.</p>
<p>Smart Devices are Different: Assessing and MitigatingMobile Sensing Heterogeneities for Activity Recognition. Henrik Allan, Sourav Blunck, Thor Siiger Bhattacharya, Mikkel Prentow, Anind K Baun Kjaergaard, Tobias Dey, Mads Møller Sonne, Jensen, 10.1145/2809695.2809718Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems, SenSys. the 13th ACM Conference on Embedded Networked Sensor Systems, SenSysSeoul, South KoreaAllan, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjaergaard, Anind K. Dey, Tobias Sonne, and Mads Møller Jensen. 2015. Smart Devices are Different: Assessing and MitigatingMobile Sensing Heterogeneities for Activity Recognition. In Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems, SenSys 2015, Seoul, South Korea, November 1-4, 2015. 127-140. https://doi.org/10.1145/2809695.2809718</p>
<p>A Robust Deep Learning Approach for Position-Independent Smartphone-Based Human Activity Recognition. Abdel Bandar Almaslukh, Jalal Artoli, Al-Muhtadi, Sensors. 183726Bandar Almaslukh, Abdel Artoli, and Jalal Al-Muhtadi. 2018. A Robust Deep Learning Approach for Position-Independent Smartphone- Based Human Activity Recognition. Sensors 18, 11 (2018), 3726.</p>
<p>Hand, belt, pocket or bag: Practical activity tracking with mobile phones. A Stephen, Mark V Antos, Konrad P Albert, Kording, 10.1016/j.jneumeth.2013.09.015Motion Capture in Animal Models and Humans. 231Stephen A. Antos, Mark V. Albert, and Konrad P. Kording. 2014. Hand, belt, pocket or bag: Practical activity tracking with mobile phones. Journal of Neuroscience Methods 231 (2014), 22 -30. https://doi.org/10.1016/j.jneumeth.2013.09.015 Motion Capture in Animal Models and Humans.</p>
<p>Phil Blunsom, Changhao Chen, Xiaoxuan Lu, Andrew Markham, Yishu Miao, Agathoniki Trigoni, Linhai Xie, MotionTransformer: Transferring Neural Inertial Tracking Between Domains. Phil Blunsom, Changhao Chen, Xiaoxuan Lu, Andrew Markham, Yishu Miao, Agathoniki Trigoni, and Linhai Xie. 2019. MotionTrans- former: Transferring Neural Inertial Tracking Between Domains.</p>
<p>A tutorial on human activity recognition using body-worn inertial sensors. Andreas Bulling, Ulf Blanke, Bernt Schiele, ACM Computing Surveys (CSUR). 4633Andreas Bulling, Ulf Blanke, and Bernt Schiele. 2014. A tutorial on human activity recognition using body-worn inertial sensors. ACM Computing Surveys (CSUR) 46, 3 (2014), 33.</p>
<p>Partial adversarial domain adaptation. Zhangjie Cao, Lijia Ma, Mingsheng Long, Jianmin Wang, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCVZhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. 2018. Partial adversarial domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV). 135-150.</p>
<p>AnyButton: Unpowered, Modeless and Highly Available Mobile Input Using Unmodified Clothing Buttons. Liwei Chan, Chien-Ting Weng, Rong-Hao Liang, Bing-Yu Chen, 10.1145/2582051.2582075Proceedings of the 5th Augmented Human International Conference. the 5th Augmented Human International ConferenceKobe, Japan; New York, NY, USA, ArticleACM24AH '14)Liwei Chan, Chien-Ting Weng, Rong-Hao Liang, and Bing-Yu Chen. 2014. AnyButton: Unpowered, Modeless and Highly Available Mobile Input Using Unmodified Clothing Buttons. In Proceedings of the 5th Augmented Human International Conference (Kobe, Japan) (AH '14). ACM, New York, NY, USA, Article 24, 2 pages. https://doi.org/10.1145/2582051.2582075</p>
<p>On heterogeneity in mobile sensing applications aiming at representative data collection. Blunck, Proceedings of the 2013 ACM Ubicomp. the 2013 ACM UbicompACMBlunck et al. 2013. On heterogeneity in mobile sensing applications aiming at representative data collection. In Proceedings of the 2013 ACM Ubicomp. ACM, 1087-1098.</p>
<p>Preprocessing techniques for context recognition from accelerometer data. Davide Figo, Pedro C Diniz, R Diogo, João M P Ferreira, Cardoso, 10.1007/s00779-010-0293-9Personal and Ubiquitous Computing. 1401Davide Figo, Pedro C. Diniz, Diogo R. Ferreira, and João M. P. Cardoso. 2010. Preprocessing techniques for context recognition from accelerometer data. Personal and Ubiquitous Computing 14, 7 (01 Oct 2010), 645-662. https://doi.org/10.1007/s00779-010-0293-9</p>
<p>Breeze: Sharing Biofeedback Through Wearable Technologies. Jérémy Frey, May Grabli, Ronit Slyper, Jessica R Cauchard, 10.1145/3173574.3174219Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI '18). the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI '18)New York, NY, USA, ArticleACM645Jérémy Frey, May Grabli, Ronit Slyper, and Jessica R. Cauchard. 2018. Breeze: Sharing Biofeedback Through Wearable Technologies. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI '18). ACM, New York, NY, USA, Article 645, 12 pages. https://doi.org/10.1145/3173574.3174219</p>
<p>Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky, The Journal of Machine Learning Research. 17Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016. Domain-adversarial training of neural networks. The Journal of Machine Learning Research 17, 1 (2016), 2096-2030.</p>
<p>You Are Sensing, but Are You Biased?: A User Unaided Sensor Calibration Approach for Mobile Sensing. Andreas Grammenos, Cecilia Mascolo, Jon A Crowcroft, IMWUT. 226Andreas Grammenos, Cecilia Mascolo, and Jon A Crowcroft. 2018. You Are Sensing, but Are You Biased?: A User Unaided Sensor Calibration Approach for Mobile Sensing. IMWUT 2 (2018), 11:1-11:26.</p>
<p>A kernel two-sample test. Arthur Gretton, M Karsten, Borgwardt, J Malte, Bernhard Rasch, Alexander Schölkopf, Smola, Journal of Machine Learning Research. 13Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. 2012. A kernel two-sample test. Journal of Machine Learning Research 13, Mar (2012), 723-773.</p>
<p>Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables. Nils Y Hammerla, Shane Halloran, Thomas Plötz, IJCAI. Nils Y. Hammerla, Shane Halloran, and Thomas Plötz. 2016. Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables. In IJCAI.</p>
<p>Cross-Subject EEG Signal Recognition Using Deep Domain Adaptation Network. W Hang, W Feng, R Du, S Liang, Y Chen, Q Wang, X Liu, 10.1109/ACCESS.2019.2939288IEEE Access. 7W. Hang, W. Feng, R. Du, S. Liang, Y. Chen, Q. Wang, and X. Liu. 2019. Cross-Subject EEG Signal Recognition Using Deep Domain Adaptation Network. IEEE Access 7 (2019), 128273-128282. https://doi.org/10.1109/ACCESS.2019.2939288</p>
<p>Earables for Personal-Scale Behavior Analytics. F Kawsar, C Min, A Mathur, A Montanari, 10.1109/MPRV.2018.03367740IEEE Pervasive Computing. 173F. Kawsar, C. Min, A. Mathur, and A. Montanari. 2018. Earables for Personal-Scale Behavior Analytics. IEEE Pervasive Computing 17, 3 (Jul 2018), 83-89. https://doi.org/10.1109/MPRV.2018.03367740</p>
<p>Exploratory Data Analysis of Acceleration Signals to Select Light-Weight and Accurate Features for Real-Time Activity Recognition on Smartphones. Adil Khan, Muhammad Siddiqi, Seok-Won Lee, 10.3390/s131013099Sensors. 13Adil Khan, Muhammad Siddiqi, and Seok-Won Lee. 2013. Exploratory Data Analysis of Acceleration Signals to Select Light-Weight and Accurate Features for Real-Time Activity Recognition on Smartphones. Sensors 13, 10 (Sep 2013), 13099âĂŞ13122. https://doi.org/10. 3390/s131013099</p>
<p>Scaling human activity recognition via deep learning-based domain adaptation. Md Abdullah Hafiz, Khan , Nirmalya Roy, Archan Misra, Md Abdullah Hafiz KHAN, Nirmalya Roy, and Archan Misra. 2018. Scaling human activity recognition via deep learning-based domain adaptation. (2018).</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).</p>
<p>Auto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, CoRR abs/13126114Diederik P. Kingma and Max Welling. 2013. Auto-Encoding Variational Bayes. CoRR abs/1312.6114 (2013).</p>
<p>A survey of mobile phone sensing. Emiliano Nicholas D Lane, Hong Miluzzo, Daniel Lu, Tanzeem Peebles, Andrew T Choudhury, Campbell, IEEE Communications magazine. 48Nicholas D Lane, Emiliano Miluzzo, Hong Lu, Daniel Peebles, Tanzeem Choudhury, and Andrew T Campbell. 2010. A survey of mobile phone sensing. IEEE Communications magazine 48, 9 (2010), 140-150.</p>
<p>Enabling large-scale human activity inference on smartphones using community similarity networks (csn). Nicholas D Lane, Ye Xu, Hong Lu, Shaohan Hu, Tanzeem Choudhury, Andrew T Campbell, Feng Zhao, UbiComp. Nicholas D. Lane, Ye Xu, Hong Lu, Shaohan Hu, Tanzeem Choudhury, Andrew T. Campbell, and Feng Zhao. 2011. Enabling large-scale human activity inference on smartphones using community similarity networks (csn). In UbiComp.</p>
<p>Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, arXiv:cs.CV/1902.00113Yi-Zhe Song, and Timothy M. Hospedales. 2019. Episodic Training for Domain Generalization. Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. 2019. Episodic Training for Domain Generalization. arXiv:cs.CV/1902.00113</p>
<p>Rectifier nonlinearities improve neural network acoustic models. L Andrew, Maas, Y Awni, Andrew Y Hannun, Ng, Proc. icml. icml30Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, Vol. 30. 3.</p>
<p>FlexAdapt: Flexible Cycle-Consistent Domain Adaptation. Akhil Mathur, Anton Isopoussu, Fahim Kawsar, Nadia Berthouze, Nicholas Lane, Proceedings of the IEEE International Conference on Machine Learning and Applications. the IEEE International Conference on Machine Learning and ApplicationsAkhil Mathur, Anton Isopoussu, Fahim Kawsar, Nadia Berthouze, and Nicholas Lane. 2019. FlexAdapt: Flexible Cycle-Consistent Domain Adaptation. In Proceedings of the IEEE International Conference on Machine Learning and Applications.</p>
<p>An Early Characterisation of Wearing Variability on Motion Signals for Wearables. Chulhong Min, Akhil Mathur, Alessandro Montanari, Fahim Kawsar, 10.1145/3341163.3347716Proceedings of the 23rd International Symposium on Wearable Computers. the 23rd International Symposium on Wearable ComputersLondon, United Kingdom; New York, NY, USAAssociation for Computing MachineryISWC '19)Chulhong Min, Akhil Mathur, Alessandro Montanari, and Fahim Kawsar. 2019. An Early Characterisation of Wearing Variability on Motion Signals for Wearables. In Proceedings of the 23rd International Symposium on Wearable Computers (London, United Kingdom) (ISWC '19). Association for Computing Machinery, New York, NY, USA, 166-168. https://doi.org/10.1145/3341163.3347716</p>
<p>Nericell: rich monitoring of road and traffic conditions using mobile smartphones. Prashanth Mohan, Ramachandran Venkata N Padmanabhan, Ramjee, Proceedings of the 6th ACM conference on Embedded network sensor systems. the 6th ACM conference on Embedded network sensor systemsACMPrashanth Mohan, Venkata N Padmanabhan, and Ramachandran Ramjee. 2008. Nericell: rich monitoring of road and traffic conditions using mobile smartphones. In Proceedings of the 6th ACM conference on Embedded network sensor systems. ACM, 323-336.</p>
<ol>
<li>I Did Not Smoke 100 Cigarettes Today!: Avoiding False Positives in Real-world Activity Recognition. Le T Nguyen, Ming Zeng, Patrick Tague, Joy Zhang, 10.1145/2750858.2804256Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing. the 2015 ACM International Joint Conference on Pervasive and Ubiquitous ComputingOsaka, Japan; New York, NY, USAACMUbiComp '15)Le T. Nguyen, Ming Zeng, Patrick Tague, and Joy Zhang. 2015. I Did Not Smoke 100 Cigarettes Today!: Avoiding False Positives in Real-world Activity Recognition. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing (Osaka, Japan) (UbiComp '15). ACM, New York, NY, USA, 1053-1063. https://doi.org/10.1145/2750858.2804256</li>
</ol>
<p>Position-Based Feature Selection for Body Sensors regarding Daily Living Activity Recognition. Duong Trong Nhan Duc Nguyen, Phuc Bui, Gu-Min Huu Truong, Jeong, 9762098:1-9762098:13J. Sensors. Nhan Duc Nguyen, Duong Trong Bui, Phuc Huu Truong, and Gu-Min Jeong. 2018. Position-Based Feature Selection for Body Sensors regarding Daily Living Activity Recognition. J. Sensors 2018 (2018), 9762098:1-9762098:13.</p>
<p>Realistic Evaluation of Deep Semi-Supervised Learning Algorithms. Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, Ian Goodfellow, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. 2018. Realistic Evaluation of Deep Semi-Supervised Learning Algorithms. In Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.). Curran Associates, Inc., 3235-3246. http://papers.nips.cc/paper/ 7585-realistic-evaluation-of-deep-semi-supervised-learning-algorithms.pdf</p>
<p>An introduction to the theory of reproducing kernel Hilbert spaces. I Vern, Mrinal Paulsen, Raghupathi, Cambridge University Press152Vern I Paulsen and Mrinal Raghupathi. 2016. An introduction to the theory of reproducing kernel Hilbert spaces. Vol. 152. Cambridge University Press.</p>
<p>AROMA: A Deep Multi-Task Learning Based Simple and Complex Human Activity Recognition Method Using Wearable Sensors. Liangying Peng, Ling Chen, Zhenan Ye, Yi Zhang, IMWUT. 216Liangying Peng, Ling Chen, Zhenan Ye, and Yi Zhang. 2018. AROMA: A Deep Multi-Task Learning Based Simple and Complex Human Activity Recognition Method Using Wearable Sensors. IMWUT 2 (2018), 74:1-74:16.</p>
<p>Introducing a New Benchmarked Dataset for Activity Monitoring. Attila Reiss, Didier Stricker, 10.1109/ISWC.2012.13Proceedings of the 2012 16th Annual International Symposium on Wearable Computers (ISWC) (ISWC '12). the 2012 16th Annual International Symposium on Wearable Computers (ISWC) (ISWC '12)Washington, DC, USAIEEE Computer SocietyAttila Reiss and Didier Stricker. 2012. Introducing a New Benchmarked Dataset for Activity Monitoring. In Proceedings of the 2012 16th Annual International Symposium on Wearable Computers (ISWC) (ISWC '12). IEEE Computer Society, Washington, DC, USA, 108-109. https://doi.org/10.1109/ISWC.2012.13</p>
<p>Collecting complex activity datasets in highly rich networked sensor environments. D Roggen, A Calatroni, M Rossi, T Holleczek, K Fãűrster, G Trãűster, P Lukowicz, D Bannach, G Pirkl, A Ferscha, J Doppler, C Holzmann, M Kurz, G Holl, R Chavarriaga, H Sagha, H Bayati, M Creatura, J D R Millãăn, 10.1109/INSS.2010.5573462Seventh International Conference on Networked Sensing Systems (INSS). D. Roggen, A. Calatroni, M. Rossi, T. Holleczek, K. FÃűrster, G. TrÃűster, P. Lukowicz, D. Bannach, G. Pirkl, A. Ferscha, J. Doppler, C. Holzmann, M. Kurz, G. Holl, R. Chavarriaga, H. Sagha, H. Bayati, M. Creatura, and J. d. R. MillÃăn. 2010. Collecting complex activity datasets in highly rich networked sensor environments. In 2010 Seventh International Conference on Networked Sensing Systems (INSS). 233-240. https://doi.org/10.1109/INSS.2010.5573462</p>
<p>Ready-to-use activity recognition for smartphones. P Siirtola, J Rãűning, 10.1109/CIDM.2013.65972182013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM). P. Siirtola and J. RÃűning. 2013. Ready-to-use activity recognition for smartphones. In 2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM). 59-64. https://doi.org/10.1109/CIDM.2013.6597218</p>
<p>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller, arXiv:cs.LG/1412.6806Striving for Simplicity: The All Convolutional Net. Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for Simplicity: The All Convolutional Net. arXiv:cs.LG/1412.6806</p>
<p>On-body Localization of Wearable Devices: An Investigation of Position-Aware Activity Recognition. Timo Sztyler, Heiner Stuckenschmidt, 10.1109/PERCOM.2016.74565212016 IEEE International Conference on Pervasive Computing and Communications (PerCom). Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body Localization of Wearable Devices: An Investigation of Position-Aware Activity Recognition. In 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE Computer Society, 1-9. https://doi.org/10.1109/PERCOM.2016.7456521 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7456521.</p>
<p>Position-aware activity recognition with wearable devices. Timo Sztyler, Heiner Stuckenschmidt, Wolfgang Petrich, Pervasive and mobile computing. 38Timo Sztyler, Heiner Stuckenschmidt, and Wolfgang Petrich. 2017. Position-aware activity recognition with wearable devices. Pervasive and mobile computing 38 (2017), 281-295.</p>
<p>Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell, arXiv:1412.3474Deep domain confusion: Maximizing for domain invariance. arXiv preprintEric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. 2014. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474 (2014).</p>
<p>Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis. Dmitry Ulyanov, Andrea Vedaldi, S Victor, Lempitsky, CVPR. 1Dmitry Ulyanov, Andrea Vedaldi, and Victor S Lempitsky. 2017. Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis.. In CVPR, Vol. 1. 3.</p>
<p>Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring Using Convolutional Neural Networks. Terry T Um, M J Franz, Daniel Pfister, Satoshi Pichler, Muriel Endo, Sandra Lang, Urban Hirche, Dana Fietzek, Kulić, 10.1145/3136755.3136817Proceedings of the 19th ACM International Conference on Multimodal Interaction. the 19th ACM International Conference on Multimodal InteractionGlasgow, UK; New York, NY, USAACMTerry T. Um, Franz M. J. Pfister, Daniel Pichler, Satoshi Endo, Muriel Lang, Sandra Hirche, Urban Fietzek, and Dana Kulić. 2017. Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring Using Convolutional Neural Networks. In Proceedings of the 19th ACM International Conference on Multimodal Interaction (Glasgow, UK) (ICMI 2017). ACM, New York, NY, USA, 216-220. https://doi.org/10.1145/3136755.3136817</p>
<p>Generalizing to Unseen Domains via Adversarial Data Augmentation. Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, Silvio Savarese, Proceedings of the 32Nd International Conference on Neural Information Processing Systems (Montr&#233;al, Canada) (NIPS'18). the 32Nd International Conference on Neural Information Processing Systems (Montr&#233;al, Canada) (NIPS'18)USACurran Associates IncRiccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, and Silvio Savarese. 2018. Generalizing to Unseen Domains via Adversarial Data Augmentation. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems (Montr&#233;al, Canada) (NIPS'18). Curran Associates Inc., USA, 5339-5349. http://dl.acm.org/citation.cfm?id=3327345.3327439</p>
<p>A novel orientation-and location-independent activity recognition method. Ran Dian Xi Shi, Yuan Wang, Xiaoyun Wu, Jing Mo, Wei, Personal and Ubiquitous Computing. 21Dian xi Shi, Ran Wang, Yuan Wu, Xiaoyun Mo, and Jing Wei. 2017. A novel orientation-and location-independent activity recognition method. Personal and Ubiquitous Computing 21 (2017), 427-441.</p>
<p>Finger-writing with Smartwatch: A Case for Finger and Hand Gesture Recognition Using Smartwatch. Chao Xu, H Parth, Prasant Pathak, Mohapatra, 10.1145/2699343.2699350Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications. the 16th International Workshop on Mobile Computing Systems and ApplicationsSanta Fe, New Mexico, USA; New York, NY, USAACMHotMobile '15)Chao Xu, Parth H. Pathak, and Prasant Mohapatra. 2015. Finger-writing with Smartwatch: A Case for Finger and Hand Gesture Recognition Using Smartwatch. In Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications (Santa Fe, New Mexico, USA) (HotMobile '15). ACM, New York, NY, USA, 9-14. https://doi.org/10.1145/2699343.2699350</p>            </div>
        </div>

    </div>
</body>
</html>
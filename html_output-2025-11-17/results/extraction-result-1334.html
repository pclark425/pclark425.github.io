<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1334 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1334</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1334</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-244527311</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2111.12137v1.pdf" target="_blank">Learning Interactive Driving Policies via Data-driven Simulation</a></p>
                <p><strong>Paper Abstract:</strong> Data-driven simulators promise high data-efficiency for driving policy learning. When used for modelling interactions, this data-efficiency becomes a bottleneck: Small underlying datasets often lack interesting and challenging edge cases for learning interactive driving. We address this challenge by proposing a simulation method that uses in-painted ado vehicles for learning robust driving policies. Thus, our approach can be used to learn policies that involve multi-agent interactions and allows for training via state-of-the-art policy learning methods. We evaluate the approach for learning standard interaction scenarios in driving. In extensive experiments, our work demonstrates that the resulting policies can be directly transferred to a full-scale autonomous vehicle without making use of any traditional sim-to-real transfer techniques such as domain randomization.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1334.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1334.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-driven multi-agent simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent data-driven photorealistic simulator (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom, photorealistic, data-driven multi-agent simulator that synthesizes novel viewpoints from real driving image sequences and renders parametrizable vehicle meshes to enable multi-agent interaction training for end-to-end driving policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Multi-agent data-driven photorealistic simulator (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Constructs virtual 3D worlds by projecting pre-collected real-world images into 3D using approximate depth, places parametrizable vehicle meshes for other agents, renders photorealistic ego-agent views with lighting and harmonization, and simulates kinematic vehicle dynamics for interactive driving tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / robotics (visual control, multi-agent interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity (photorealistic rendering from real-world image data) with medium-fidelity dynamics (continuous kinematic vehicle model, RK3 integration); perceptual realism prioritized over full physical fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>High-fidelity visual observations synthesized from real images and approximate depth; vehicle appearance randomized (meshes, materials, colors); lighting approximated (ambient + directional from egocentric viewpoint) and image-space harmonization applied; vehicle dynamics use continuous kinematic model integrated with third-order Runge-Kutta; collision detection via polygon overlap with dilation; does not model high-fidelity tire dynamics, suspension, aerodynamics, or full-contact physics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>End-to-end visual driving policy (PPO agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agent trained with Proximal Policy Optimization (PPO); CNN encoder for images followed by LSTM for temporal/motion information; outputs steering commands (action = steering angle).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Interactive driving control tasks (car following and overtaking) requiring perception-driven decision-making in multi-agent scenarios (avoidance, passing, lane-stable maneuvers).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Trained with PPO in simulation using ~30 min indoor garage + ~1 hr rural driving data as the environment; offline evaluations: car-following over 100 simulated episodes and overtaking over 1000 simulated episodes; typical in-simulator minimal clearance ~0.20 m (average) for overtaking; intervention rate lower in controlled (garage) data than rural.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Full-scale real-world autonomous vehicle (2019 Lexus RX 450H) for zero-shot deployment</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Zero-shot transfer demonstrated: 63 online trials (~2 hours autonomous). Reported online intervention counts: static front car scenario 0/33 interventions (0.00), dynamic front car 3/30 (0.10); overtake-from-left 0/32, from-right 3/31. Real-world minimal clearance and max deviation/yaw larger than offline, but successful direct transfer without domain randomization or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue photorealistic, data-driven visual fidelity is critical for zero-shot transfer; they prioritize photorealism over full physics fidelity and demonstrate direct transfer without domain randomization, suggesting high visual realism + approximate kinematic dynamics sufficed for their interactive driving tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported failure cases tied to viewpoint/yaw calibration drift (even ~1Â° drift causing ~9 cm lateral error), limited field-of-view (front car can be lost from view), and challenging initial conditions or higher speeds increasing interventions; limitations stem from perceptual/visual synthesis biases and simplified dynamics rather than detailed physical modeling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1334.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARLA: An open urban driving simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used open-source urban driving simulator based on game engines offering high-fidelity visual and traffic simulation for autonomous driving research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CARLA: An open urban driving simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Game-engine-based urban driving simulator providing photorealistic rendered scenes, dynamic traffic participants, sensor models, and controllable environments for training and evaluating driving policies.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity (synthetic photorealistic rendering) with moderate physical fidelity (game-engine physics); synthetic appearance can differ from real-world data.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides rendered RGB, depth, segmentation, lidar-like sensors; realistic urban layouts and actor behaviors, but images are synthetic and frequently require domain adaptation or domain randomization for real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Mentioned as simulator for navigation and driving policy training (general reference).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world (requires domain randomization/style transfer for effective sim-to-real)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes that synthetically generated images from CARLA are often insufficient for zero-shot transfer and typically require domain randomization or style transfer to deploy in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Mentioned deficiency: synthetic visual domains often fail zero-shot transfer without adaptation techniques.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1334.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AirSim: High-fidelity visual and physical simulation for autonomous vehicles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulator toolkit leveraging game engines to provide high-fidelity visual and physics simulation for drones and vehicles, used for perception and control research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AirSim: High-fidelity visual and physical simulation for autonomous vehicles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Game-engine-based simulator providing realistic camera and sensor outputs, vehicle/drone dynamics models, and APIs for RL and perception algorithm development.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous vehicles / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity with moderate physics fidelity; synthetic rather than data-driven visuals.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports rich sensor models (camera, depth, IMU), simulated physics for vehicles/drones; synthetic rendering may not fully match real-world appearance leading to sim-to-real gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Mentioned as example simulator for navigation tasks and RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world (sim-to-real often requires adaptation techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Referenced as representative of physics/visual simulators that still commonly require domain randomization for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Implied: synthetic visuals insufficient for zero-shot transfer without domain adaptation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1334.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FlightGoggles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A photorealistic simulator that uses photogrammetry and VR technology to simulate sensors for perception-driven robotics, focusing on high-fidelity visual realism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>FlightGoggles</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Photorealistic sensor simulator using photogrammetric reconstructions and VR rendering pipelines to create realistic camera observations for drones and robots.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity (photogrammetry-based) with focus on perceptual realism; physical dynamics modeling is present but not the central feature.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Uses real-world-scanned environments to render sensors; good photorealism but environment editing flexibility can be limited compared to purely synthetic engines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Perception-driven control and navigation tasks (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world (aimed to reduce sim-to-real gap via photogrammetry)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Mentioned as a high-visual-fidelity simulator in related work; paper contrasts data-driven photorealism with synthetic engines.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1334.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson Env: Real-World Perception for Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven environment that reconstructs real-world spaces for embodied AI, providing realistic visual inputs for navigation and perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gibson Env: Real-World Perception for Embodied Agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gibson Env</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Reconstructs real indoor spaces from scans to create photorealistic environments for embodied agent research, focusing on perceptual realism over editable synthetic generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity (real-world reconstruction) with limited environment editing; physics fidelity varies and often simplified.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Uses reconstructed geometry and textures from real scans; limited ability to insert arbitrary new objects or massively edit environment dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Embodied navigation and perception (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper groups Gibson with other data-driven simulators that favor photorealism to ease real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1334.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat: A Platform for Embodied AI Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform providing photorealistic, reconstructed environments and fast simulation for training embodied agents on navigation and perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Habitat: A Platform for Embodied AI Research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Habitat</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Provides a fast simulator with many reconstructed indoor/outdoor environments to train and evaluate embodied agents with realistic sensor inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity (reconstructed scenes) with efficient simulation; editing flexibility limited compared to fully synthetic engines.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Real-scene reconstructions produce realistic visuals; physics and interaction models are typically simplified for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Navigation and perception for embodied agents (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1334.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open platform providing photorealistic simulated indoor environments and tasks designed for simulation-to-real embodied AI research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>RoboTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulation platform with photorealistic indoor scenes and object interaction tasks intended for sim-to-real embodied agent research.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity for indoor scenes, with task designs focusing on sim-to-real benchmarking; physical interaction fidelity moderate.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Photorealistic rendering from curated scenes; physics for interactions included but simplified relative to real-contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Embodied AI tasks with sim-to-real focus (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world robots</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1334.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo: A physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance physics engine commonly used for model-based control and reinforcement learning research, offering accurate rigid-body dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MuJoCo: A physics engine for model-based control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Physics engine providing accurate and efficient simulation of articulated rigid-body dynamics for control, locomotion, and robotics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>dynamics / control / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity physics for articulated rigid bodies and contacts; visual rendering is not the central feature.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Accurate dynamics, contact models, and continuous-time integration; typically lacks photorealistic sensor rendering unless paired with separate rendering systems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Continuous control and model-based RL benchmarks (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Physical robots / control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1334.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pybullet, a python module for physics simulation for games, robotics and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python interface to the Bullet physics engine used for simulating rigid-body dynamics, contacts, and robotics scenarios for learning and control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pybullet, a python module for physics simulation for games, robotics and machine learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Python module wrapping Bullet physics for simulations of rigid-body dynamics, collision detection, and robotic mechanisms; commonly used in ML and robotics research.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / dynamics / control</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-to-high physics fidelity for rigid-body dynamics; graphics are typically lower-fidelity unless coupled with other renderers.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Collision detection, contacts, joint dynamics, and support for sensors via plugins; not focused on photorealistic visual realism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learning control policies and simulating robot dynamics (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Physical robots</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1334.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>dm_control</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>dm control: Software and tasks for continuous control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of continuous control tasks and simulation software (by DeepMind) for RL research, built on physics simulation backends.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>dm control: Software and tasks for continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>dm_control</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Collection of continuous control benchmarks and environments built on a physics simulator, used to train and evaluate RL agents.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>control / reinforcement learning / dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-to-high physics fidelity depending on the underlying physics backend; visuals are typically synthetic and task-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides standardized RL tasks with consistent physics; visualization is secondary to accurate dynamics for control benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Continuous control benchmarks for RL (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Control tasks / potentially real robots</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1334.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AADS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AADS: Augmented autonomous driving simulation using data-driven algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven augmented driving simulation system that composes and augments real-world data to produce training scenes for autonomous driving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AADS: Augmented autonomous driving simulation using data-driven algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AADS</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Combines real-world data with algorithmic augmentation to generate realistic driving scenes for training perception and planning systems.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity (data-driven composition) with limited full-environment editing; focuses on realism by leveraging real-world data.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Creates photorealistic training images by augmenting real data; may limit degrees of freedom for environment editing compared to fully synthetic engines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Perception and driving policy training (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world perception and driving systems</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper places AADS in the category of data-driven simulators that trade environment-editability for photorealism to improve transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1334.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulator for photorealistic image synthesis using geometry-aware composition techniques to generate realistic driving scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>GeoSim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Generates photorealistic synthetic images by composing geometry-aware elements to better match real driving scenes for training perception models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity in synthesized images via geometry-aware composition; still synthetic rather than pure data-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Focus on photorealistic composition and geometry alignment to produce better training images for perception; may not simulate full dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Perception training (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world perception tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1334.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e1334.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiDARsim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulator that uses real-world data to generate realistic LiDAR sensor outputs for training and evaluation of LiDAR-based perception systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>LiDARsim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Produces realistic LiDAR point clouds by leveraging and augmenting real-world scans to train perception systems that use LiDAR.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>perception / LiDAR sensing</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High sensor fidelity for LiDAR data via real-world leveraging; focuses on accurate LiDAR returns rather than full environment physics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Generates LiDAR point clouds grounded in real measurements to reduce sim-to-real gap for LiDAR perception; does not necessarily simulate full vehicle dynamics or photorealistic RGB.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>LiDAR perception/model training (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world LiDAR perception systems</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CARLA: An open urban driving simulator. <em>(Rating: 2)</em></li>
                <li>AirSim: High-fidelity visual and physical simulation for autonomous vehicles. <em>(Rating: 2)</em></li>
                <li>FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality. <em>(Rating: 2)</em></li>
                <li>Gibson Env: Real-World Perception for Embodied Agents. <em>(Rating: 1)</em></li>
                <li>Habitat: A Platform for Embodied AI Research. <em>(Rating: 1)</em></li>
                <li>MuJoCo: A physics engine for model-based control. <em>(Rating: 2)</em></li>
                <li>Pybullet, a python module for physics simulation for games, robotics and machine learning. <em>(Rating: 2)</em></li>
                <li>AADS: Augmented autonomous driving simulation using data-driven algorithms. <em>(Rating: 2)</em></li>
                <li>LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World. <em>(Rating: 2)</em></li>
                <li>GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1334",
    "paper_id": "paper-244527311",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Data-driven multi-agent simulator",
            "name_full": "Multi-agent data-driven photorealistic simulator (this work)",
            "brief_description": "A custom, photorealistic, data-driven multi-agent simulator that synthesizes novel viewpoints from real driving image sequences and renders parametrizable vehicle meshes to enable multi-agent interaction training for end-to-end driving policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Multi-agent data-driven photorealistic simulator (custom)",
            "simulator_description": "Constructs virtual 3D worlds by projecting pre-collected real-world images into 3D using approximate depth, places parametrizable vehicle meshes for other agents, renders photorealistic ego-agent views with lighting and harmonization, and simulates kinematic vehicle dynamics for interactive driving tasks.",
            "scientific_domain": "autonomous driving / robotics (visual control, multi-agent interaction)",
            "fidelity_level": "High visual fidelity (photorealistic rendering from real-world image data) with medium-fidelity dynamics (continuous kinematic vehicle model, RK3 integration); perceptual realism prioritized over full physical fidelity.",
            "fidelity_characteristics": "High-fidelity visual observations synthesized from real images and approximate depth; vehicle appearance randomized (meshes, materials, colors); lighting approximated (ambient + directional from egocentric viewpoint) and image-space harmonization applied; vehicle dynamics use continuous kinematic model integrated with third-order Runge-Kutta; collision detection via polygon overlap with dilation; does not model high-fidelity tire dynamics, suspension, aerodynamics, or full-contact physics.",
            "model_or_agent_name": "End-to-end visual driving policy (PPO agent)",
            "model_description": "Reinforcement learning agent trained with Proximal Policy Optimization (PPO); CNN encoder for images followed by LSTM for temporal/motion information; outputs steering commands (action = steering angle).",
            "reasoning_task": "Interactive driving control tasks (car following and overtaking) requiring perception-driven decision-making in multi-agent scenarios (avoidance, passing, lane-stable maneuvers).",
            "training_performance": "Trained with PPO in simulation using ~30 min indoor garage + ~1 hr rural driving data as the environment; offline evaluations: car-following over 100 simulated episodes and overtaking over 1000 simulated episodes; typical in-simulator minimal clearance ~0.20 m (average) for overtaking; intervention rate lower in controlled (garage) data than rural.",
            "transfer_target": "Full-scale real-world autonomous vehicle (2019 Lexus RX 450H) for zero-shot deployment",
            "transfer_performance": "Zero-shot transfer demonstrated: 63 online trials (~2 hours autonomous). Reported online intervention counts: static front car scenario 0/33 interventions (0.00), dynamic front car 3/30 (0.10); overtake-from-left 0/32, from-right 3/31. Real-world minimal clearance and max deviation/yaw larger than offline, but successful direct transfer without domain randomization or fine-tuning.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors argue photorealistic, data-driven visual fidelity is critical for zero-shot transfer; they prioritize photorealism over full physics fidelity and demonstrate direct transfer without domain randomization, suggesting high visual realism + approximate kinematic dynamics sufficed for their interactive driving tasks.",
            "failure_cases": "Reported failure cases tied to viewpoint/yaw calibration drift (even ~1Â° drift causing ~9 cm lateral error), limited field-of-view (front car can be lost from view), and challenging initial conditions or higher speeds increasing interventions; limitations stem from perceptual/visual synthesis biases and simplified dynamics rather than detailed physical modeling.",
            "uuid": "e1334.0"
        },
        {
            "name_short": "CARLA",
            "name_full": "CARLA: An open urban driving simulator",
            "brief_description": "A widely-used open-source urban driving simulator based on game engines offering high-fidelity visual and traffic simulation for autonomous driving research.",
            "citation_title": "CARLA: An open urban driving simulator.",
            "mention_or_use": "mention",
            "simulator_name": "CARLA",
            "simulator_description": "Game-engine-based urban driving simulator providing photorealistic rendered scenes, dynamic traffic participants, sensor models, and controllable environments for training and evaluating driving policies.",
            "scientific_domain": "autonomous driving / robotics",
            "fidelity_level": "High visual fidelity (synthetic photorealistic rendering) with moderate physical fidelity (game-engine physics); synthetic appearance can differ from real-world data.",
            "fidelity_characteristics": "Provides rendered RGB, depth, segmentation, lidar-like sensors; realistic urban layouts and actor behaviors, but images are synthetic and frequently require domain adaptation or domain randomization for real-world transfer.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Mentioned as simulator for navigation and driving policy training (general reference).",
            "training_performance": null,
            "transfer_target": "Real-world (requires domain randomization/style transfer for effective sim-to-real)",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes that synthetically generated images from CARLA are often insufficient for zero-shot transfer and typically require domain randomization or style transfer to deploy in the real world.",
            "failure_cases": "Mentioned deficiency: synthetic visual domains often fail zero-shot transfer without adaptation techniques.",
            "uuid": "e1334.1"
        },
        {
            "name_short": "AirSim",
            "name_full": "AirSim: High-fidelity visual and physical simulation for autonomous vehicles",
            "brief_description": "A simulator toolkit leveraging game engines to provide high-fidelity visual and physics simulation for drones and vehicles, used for perception and control research.",
            "citation_title": "AirSim: High-fidelity visual and physical simulation for autonomous vehicles.",
            "mention_or_use": "mention",
            "simulator_name": "AirSim",
            "simulator_description": "Game-engine-based simulator providing realistic camera and sensor outputs, vehicle/drone dynamics models, and APIs for RL and perception algorithm development.",
            "scientific_domain": "autonomous vehicles / robotics",
            "fidelity_level": "High visual fidelity with moderate physics fidelity; synthetic rather than data-driven visuals.",
            "fidelity_characteristics": "Supports rich sensor models (camera, depth, IMU), simulated physics for vehicles/drones; synthetic rendering may not fully match real-world appearance leading to sim-to-real gaps.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Mentioned as example simulator for navigation tasks and RL training.",
            "training_performance": null,
            "transfer_target": "Real-world (sim-to-real often requires adaptation techniques)",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Referenced as representative of physics/visual simulators that still commonly require domain randomization for transfer.",
            "failure_cases": "Implied: synthetic visuals insufficient for zero-shot transfer without domain adaptation.",
            "uuid": "e1334.2"
        },
        {
            "name_short": "FlightGoggles",
            "name_full": "FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality",
            "brief_description": "A photorealistic simulator that uses photogrammetry and VR technology to simulate sensors for perception-driven robotics, focusing on high-fidelity visual realism.",
            "citation_title": "FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality.",
            "mention_or_use": "mention",
            "simulator_name": "FlightGoggles",
            "simulator_description": "Photorealistic sensor simulator using photogrammetric reconstructions and VR rendering pipelines to create realistic camera observations for drones and robots.",
            "scientific_domain": "robotics / perception",
            "fidelity_level": "High visual fidelity (photogrammetry-based) with focus on perceptual realism; physical dynamics modeling is present but not the central feature.",
            "fidelity_characteristics": "Uses real-world-scanned environments to render sensors; good photorealism but environment editing flexibility can be limited compared to purely synthetic engines.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Perception-driven control and navigation tasks (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Real-world (aimed to reduce sim-to-real gap via photogrammetry)",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Mentioned as a high-visual-fidelity simulator in related work; paper contrasts data-driven photorealism with synthetic engines.",
            "failure_cases": "",
            "uuid": "e1334.3"
        },
        {
            "name_short": "Gibson",
            "name_full": "Gibson Env: Real-World Perception for Embodied Agents",
            "brief_description": "A data-driven environment that reconstructs real-world spaces for embodied AI, providing realistic visual inputs for navigation and perception tasks.",
            "citation_title": "Gibson Env: Real-World Perception for Embodied Agents.",
            "mention_or_use": "mention",
            "simulator_name": "Gibson Env",
            "simulator_description": "Reconstructs real indoor spaces from scans to create photorealistic environments for embodied agent research, focusing on perceptual realism over editable synthetic generation.",
            "scientific_domain": "embodied AI / robotics / perception",
            "fidelity_level": "High visual fidelity (real-world reconstruction) with limited environment editing; physics fidelity varies and often simplified.",
            "fidelity_characteristics": "Uses reconstructed geometry and textures from real scans; limited ability to insert arbitrary new objects or massively edit environment dynamics.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Embodied navigation and perception (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Real-world embodied agents",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper groups Gibson with other data-driven simulators that favor photorealism to ease real-world transfer.",
            "failure_cases": "",
            "uuid": "e1334.4"
        },
        {
            "name_short": "Habitat",
            "name_full": "Habitat: A Platform for Embodied AI Research",
            "brief_description": "A platform providing photorealistic, reconstructed environments and fast simulation for training embodied agents on navigation and perception tasks.",
            "citation_title": "Habitat: A Platform for Embodied AI Research.",
            "mention_or_use": "mention",
            "simulator_name": "Habitat",
            "simulator_description": "Provides a fast simulator with many reconstructed indoor/outdoor environments to train and evaluate embodied agents with realistic sensor inputs.",
            "scientific_domain": "embodied AI / perception",
            "fidelity_level": "High visual fidelity (reconstructed scenes) with efficient simulation; editing flexibility limited compared to fully synthetic engines.",
            "fidelity_characteristics": "Real-scene reconstructions produce realistic visuals; physics and interaction models are typically simplified for speed.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Navigation and perception for embodied agents (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Real-world embodied tasks",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1334.5"
        },
        {
            "name_short": "RoboTHOR",
            "name_full": "RoboTHOR: An Open Simulation-to-Real Embodied AI Platform",
            "brief_description": "Open platform providing photorealistic simulated indoor environments and tasks designed for simulation-to-real embodied AI research.",
            "citation_title": "RoboTHOR: An Open Simulation-to-Real Embodied AI Platform.",
            "mention_or_use": "mention",
            "simulator_name": "RoboTHOR",
            "simulator_description": "Simulation platform with photorealistic indoor scenes and object interaction tasks intended for sim-to-real embodied agent research.",
            "scientific_domain": "embodied AI / robotics",
            "fidelity_level": "High visual fidelity for indoor scenes, with task designs focusing on sim-to-real benchmarking; physical interaction fidelity moderate.",
            "fidelity_characteristics": "Photorealistic rendering from curated scenes; physics for interactions included but simplified relative to real-contact dynamics.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Embodied AI tasks with sim-to-real focus (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Real-world robots",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1334.6"
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo: A physics engine for model-based control",
            "brief_description": "A high-performance physics engine commonly used for model-based control and reinforcement learning research, offering accurate rigid-body dynamics.",
            "citation_title": "MuJoCo: A physics engine for model-based control.",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "Physics engine providing accurate and efficient simulation of articulated rigid-body dynamics for control, locomotion, and robotics tasks.",
            "scientific_domain": "dynamics / control / robotics",
            "fidelity_level": "High-fidelity physics for articulated rigid bodies and contacts; visual rendering is not the central feature.",
            "fidelity_characteristics": "Accurate dynamics, contact models, and continuous-time integration; typically lacks photorealistic sensor rendering unless paired with separate rendering systems.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Continuous control and model-based RL benchmarks (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Physical robots / control tasks",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1334.7"
        },
        {
            "name_short": "PyBullet",
            "name_full": "Pybullet, a python module for physics simulation for games, robotics and machine learning",
            "brief_description": "A Python interface to the Bullet physics engine used for simulating rigid-body dynamics, contacts, and robotics scenarios for learning and control.",
            "citation_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning.",
            "mention_or_use": "mention",
            "simulator_name": "PyBullet",
            "simulator_description": "Python module wrapping Bullet physics for simulations of rigid-body dynamics, collision detection, and robotic mechanisms; commonly used in ML and robotics research.",
            "scientific_domain": "robotics / dynamics / control",
            "fidelity_level": "Medium-to-high physics fidelity for rigid-body dynamics; graphics are typically lower-fidelity unless coupled with other renderers.",
            "fidelity_characteristics": "Collision detection, contacts, joint dynamics, and support for sensors via plugins; not focused on photorealistic visual realism.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Learning control policies and simulating robot dynamics (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Physical robots",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1334.8"
        },
        {
            "name_short": "dm_control",
            "name_full": "dm control: Software and tasks for continuous control",
            "brief_description": "A suite of continuous control tasks and simulation software (by DeepMind) for RL research, built on physics simulation backends.",
            "citation_title": "dm control: Software and tasks for continuous control.",
            "mention_or_use": "mention",
            "simulator_name": "dm_control",
            "simulator_description": "Collection of continuous control benchmarks and environments built on a physics simulator, used to train and evaluate RL agents.",
            "scientific_domain": "control / reinforcement learning / dynamics",
            "fidelity_level": "Medium-to-high physics fidelity depending on the underlying physics backend; visuals are typically synthetic and task-focused.",
            "fidelity_characteristics": "Provides standardized RL tasks with consistent physics; visualization is secondary to accurate dynamics for control benchmarking.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Continuous control benchmarks for RL (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Control tasks / potentially real robots",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1334.9"
        },
        {
            "name_short": "AADS",
            "name_full": "AADS: Augmented autonomous driving simulation using data-driven algorithms",
            "brief_description": "A data-driven augmented driving simulation system that composes and augments real-world data to produce training scenes for autonomous driving.",
            "citation_title": "AADS: Augmented autonomous driving simulation using data-driven algorithms.",
            "mention_or_use": "mention",
            "simulator_name": "AADS",
            "simulator_description": "Combines real-world data with algorithmic augmentation to generate realistic driving scenes for training perception and planning systems.",
            "scientific_domain": "autonomous driving / perception",
            "fidelity_level": "High visual fidelity (data-driven composition) with limited full-environment editing; focuses on realism by leveraging real-world data.",
            "fidelity_characteristics": "Creates photorealistic training images by augmenting real data; may limit degrees of freedom for environment editing compared to fully synthetic engines.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Perception and driving policy training (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Real-world perception and driving systems",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper places AADS in the category of data-driven simulators that trade environment-editability for photorealism to improve transfer.",
            "failure_cases": "",
            "uuid": "e1334.10"
        },
        {
            "name_short": "GeoSim",
            "name_full": "GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition",
            "brief_description": "A simulator for photorealistic image synthesis using geometry-aware composition techniques to generate realistic driving scenes.",
            "citation_title": "GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition.",
            "mention_or_use": "mention",
            "simulator_name": "GeoSim",
            "simulator_description": "Generates photorealistic synthetic images by composing geometry-aware elements to better match real driving scenes for training perception models.",
            "scientific_domain": "autonomous driving / perception",
            "fidelity_level": "High visual fidelity in synthesized images via geometry-aware composition; still synthetic rather than pure data-driven.",
            "fidelity_characteristics": "Focus on photorealistic composition and geometry alignment to produce better training images for perception; may not simulate full dynamics.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "Perception training (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Real-world perception tasks",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1334.11"
        },
        {
            "name_short": "LiDARsim",
            "name_full": "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World",
            "brief_description": "A simulator that uses real-world data to generate realistic LiDAR sensor outputs for training and evaluation of LiDAR-based perception systems.",
            "citation_title": "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World.",
            "mention_or_use": "mention",
            "simulator_name": "LiDARsim",
            "simulator_description": "Produces realistic LiDAR point clouds by leveraging and augmenting real-world scans to train perception systems that use LiDAR.",
            "scientific_domain": "perception / LiDAR sensing",
            "fidelity_level": "High sensor fidelity for LiDAR data via real-world leveraging; focuses on accurate LiDAR returns rather than full environment physics.",
            "fidelity_characteristics": "Generates LiDAR point clouds grounded in real measurements to reduce sim-to-real gap for LiDAR perception; does not necessarily simulate full vehicle dynamics or photorealistic RGB.",
            "model_or_agent_name": "",
            "model_description": null,
            "reasoning_task": "LiDAR perception/model training (mentioned in related work).",
            "training_performance": null,
            "transfer_target": "Real-world LiDAR perception systems",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1334.12"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CARLA: An open urban driving simulator.",
            "rating": 2,
            "sanitized_title": "carla_an_open_urban_driving_simulator"
        },
        {
            "paper_title": "AirSim: High-fidelity visual and physical simulation for autonomous vehicles.",
            "rating": 2,
            "sanitized_title": "airsim_highfidelity_visual_and_physical_simulation_for_autonomous_vehicles"
        },
        {
            "paper_title": "FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality.",
            "rating": 2,
            "sanitized_title": "flightgoggles_photorealistic_sensor_simulation_for_perceptiondriven_robotics_using_photogrammetry_and_virtual_reality"
        },
        {
            "paper_title": "Gibson Env: Real-World Perception for Embodied Agents.",
            "rating": 1,
            "sanitized_title": "gibson_env_realworld_perception_for_embodied_agents"
        },
        {
            "paper_title": "Habitat: A Platform for Embodied AI Research.",
            "rating": 1,
            "sanitized_title": "habitat_a_platform_for_embodied_ai_research"
        },
        {
            "paper_title": "MuJoCo: A physics engine for model-based control.",
            "rating": 2,
            "sanitized_title": "mujoco_a_physics_engine_for_modelbased_control"
        },
        {
            "paper_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning.",
            "rating": 2,
            "sanitized_title": "pybullet_a_python_module_for_physics_simulation_for_games_robotics_and_machine_learning"
        },
        {
            "paper_title": "AADS: Augmented autonomous driving simulation using data-driven algorithms.",
            "rating": 2,
            "sanitized_title": "aads_augmented_autonomous_driving_simulation_using_datadriven_algorithms"
        },
        {
            "paper_title": "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World.",
            "rating": 2,
            "sanitized_title": "lidarsim_realistic_lidar_simulation_by_leveraging_the_real_world"
        },
        {
            "paper_title": "GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition.",
            "rating": 1,
            "sanitized_title": "geosim_photorealistic_image_simulation_with_geometryaware_composition"
        }
    ],
    "cost": 0.016746999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Interactive Driving Policies via Data-driven Simulation</p>
<p>Tsun-Hsuan Wang 
Alexander Amini 
Wilko Schwarting 
Igor Gilitschenski 
Sertac Karaman 
Daniela Rus 
Learning Interactive Driving Policies via Data-driven Simulation</p>
<p>Data-driven simulators promise high dataefficiency for driving policy learning. When used for modelling interactions, this data-efficiency becomes a bottleneck: Small underlying datasets often lack interesting and challenging edge cases for learning interactive driving. We address this challenge by proposing a simulation method that uses in-painted ado vehicles for learning robust driving policies. Thus, our approach can be used to learn policies that involve multi-agent interactions and allows for training via state-of-the-art policy learning methods. We evaluate the approach for learning standard interaction scenarios in driving. In extensive experiments, our work demonstrates that the resulting policies can be directly transferred to a full-scale autonomous vehicle without making use of any traditional sim-to-real transfer techniques such as domain randomization.</p>
<p>I. INTRODUCTION</p>
<p>Intelligent agents can achieve complex continuous control and decision making in the presence of rich multi-agent interactions as well as diverse lighting and environmental conditions. This ability requires learning representations from raw perception to high-level control actions. The interactive multi-agent case is challenging for autonomous navigation. End-to-end policy learning has demonstrated great promise for lane-stable single-agent navigation. However, to date, these networks are limited to simplistic road environments [1], [2], navigation with no interactions [3], [4], testing in solely passive settings [5], [6], or are deployed only in simulated environments disregarding real-world transferability [7], [8], [9].</p>
<p>End-to-end learning of multi-agent visual control policies will increase the abilities of autonomous agents to reason and make decisions about how to move in interactive environments. End-to-end imitation learning requires capturing expert training data from extensive edge cases, such as recovery from off-orientation positions or near collisions with other agents. This is prohibitively expensive and dangerous for interactive situations. Simulation presents a solution to efficiently training and testing autonomous agents before deploying them in the real-world. Sim-to-real transfer has achieved great results in constrained problem settings [10], [11], [12], but it remains challenging for interactive autonomous driving and other interactive robotics tasks. * The first two authors have contributed equally to this work. This work was supported by National Science Foundation and Toyota Research Institute. We gratefully acknowledge the support of NVIDIA with the donation of the Drive AGX Pegasus. 1 {tsunw,amini,wilkos,sertac,rus}@mit.edu Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology (MIT). 2 gilitschenski@cs.utoronto.edu Department of Computer Science, University of Toronto and Toyota Research Institute (TRI). Fig. 1: Data-driven multi-agent simulation. End-to-end training and testing of control policies with direct transferability to real-world robotic platforms.</p>
<p>In this paper, we present an end-to-end framework for photorealistic simulation and training of autonomous agents in the presence of both static and dynamic agent interactions. Our training environment is photorealistic and supports highfidelity rendering of multiple agents such that ego-agent learned control policies can be directly transferred onboard a full-scale autonomous vehicle in the real world, without requiring any degree of domain randomization, augmentation, or fine-tuning. Our simulator is fully data-driven: from a single real-world human driving trajectory, virtual agents can synthesize a continuum of new viewpoints from their simulated position. In addition to perceiving the synthesized environment, the simulation system supports the inclusion and rendering of other agents along with their motion dynamics. Several tasks of increased multi-agent interaction complexity are modeled in simulation and used to train policies that can later be deployed directly on physical autonomy platforms such as autonomous cars.</p>
<p>By simulating arbitrary agent interactions, we do not require massive amounts of expert training data and dense supervisory signals, which are two common limitations of existing imitation learning approaches. Furthermore, by formulating training as a reinforcement learning (RL) problem, we allow the ego agent to autonomously explore the virtual environment and learn to recover from out-of-distribution edge cases and near collisions. We extensively evaluate our method through closed-loop deployment on challenging realworld environments and agents not previously encountered during training. Beyond exhibiting direct transferability, our models demonstrate high performance and generalizability on complex tasks such as autonomous overtaking and avoidance of a partially observed dynamic agent.</p>
<p>In summary, the key contributions of this paper are: 1) A method for jointly synthesizing novel multi-agent viewpoints of an environment enabling interaction modelling and simulation. 2) Adaptation of state-of-the-art policy learning techniques to learn robust control policies for interactive driving tasks such as vehicle following and overtaking. 3) Real-world experiments with zero-shot policy transfer on a full-scale autonomous vehicle. This includes an extensive evaluation demonstrating successful overtaking behavior on a perceptually challenging test track not seen during training.</p>
<p>II. RELATED WORK</p>
<p>Model-Based Simulators. Most simulation engines for reinforcement learning and robotics focus on modelling the underlying physics [13], [14], [15]. Recently, simulators based on video-game engines (such as CARLA [16], Air-Sim [17], or FlightGoggles [18]) also offer a high-fidelity stream of visual data mainly focusing on navigation tasks. However, synthetically generated images are typically still insufficient to enable zero shot transfer of learned policies and require techniques such as domain-randomization [19], [20] or style transfer [10] to enable real-world deployment of learned policies. In contrast, the present work follows the philosophy of data-driven simulation to enable zero-shot policy transfer to real-world platforms.</p>
<p>Data-driven Simulation. A philosophically different approach for achieving the goal of visually and physically realistic simulators is using real-world data for building the simulator. This is inspired by elaborate data augmentation [21], weakly-supervised learning [22], and generative video manipulation [23] techniques. Typically this sacrifices some of the environment editing abilities in favor of maintaining photo-realism. Improving learning for autonomous driving [24] is one of the main applications of these ideas focusing on individual sensors such as camera [25] or LiDAR [26], [27]. Simulators such as Gibson [28], Habitat [29], or RoboTHOR [30] offer reconstructed real-world environments for embodied AI research. In a similar spirit, the main focus of the present work is enabling zero-shot transfer for learning multi-agent interactions.</p>
<p>Navigation Policy Learning. Traditional planning and control approaches for navigation [31], [32], [33], [34], [35] typically do not make use of high dimensional inputs and use game-theoretic approaches for interaction modelling [36], [37], [38], [39]. End-to-end navigation approaches can learn driving policies from image inputs [40], [41]. This, however, usually requires human supervision [42], [3], [43], [44], [45], [1], [4], [46], [47], [48], [49], does not consider interactions [12], [11], [50], or does not consider deployment on real-world platforms [51], [52], [53], [54], [55]. The present work addresses these limitations demonstrating interactive driving with policies purely trained in simulation.</p>
<p>III. METHODOLOGY</p>
<p>In the following section, we outline our simulation and training pipleine (Fig. 2) starting with the creation of a multi-agent simulator and photorealistic synthesis of novel perception viewpoints. We then discuss how such an environment can serve as a flexible backbone for optimizing RL policies by defining control tasks which encourage multiagent interaction.</p>
<p>A. Data-driven Simulation and Learning Environment</p>
<p>The high-level pipeline of the proposed multi-agent datadriven simulation consists of (1) updating states for all agents, (2) recreating the world by projecting real-world image data to 3D space based on depth information, (3) configuring and placing meshes for all agents in the scene, (4) rendering the agent's viewpoint, and (5) post-processing in image space to add lighting and to harmonize the image. The vehicle dynamics follow the continuous kinematic model
[áº,áº,Ï ,Î´ ,v] = v cos Ï , v sin Ï , v L tan Î´ , u Î´ , u a ,(1)
where x, y, Ï are vehicle position and heading. Î´ , v are the steering angle and speed, L is the inter-axle distance, and u Î´ , u a are the steering velocity and acceleration. We ensure accuracy of discrete-time dynamics equivalent by integration through a third-order Runge-Kutta scheme. We follow [12] to recreate the virtual world from real data. The pre-collected image sequences represent sparsely-sampled trajectories of perception observations from the real world, which allow photorealistic and semantically accurate local view synthesis around the viewpoint of each frame. Every agent's current state is associated with a pre-collected frame according to the closest distance from its pose to the pose of the datacollection vehicle. We compensate for the local transform between the data and the agent leveraging the camera extrinsics during rendering and using approximate depth information. We project the image from the data coordinate frame, i.e. the data viewpoint p s , to the agent's viewpoint p t ,
p t = KT v 2 ât T v 1 âv 2 T sâv 1 D s (p s )K â1 p s , T sâv 1 = T â1 v 2 ât , (2) where K is the camera intrinsic, D s (p s )
is the depth of the data at point p s , T sâv * is the transform from camera to vehicle body, and T v 1 âv 2 is the transform from the vehicle body of the data to that of agent. We explicitly model the transform using vehicle body T v 1 âv 2 since it simplifies placing new meshes in the scene and checking for collisions among meshes. Note that the yaw difference in T sâv * may induce a bias of how the mesh is placed towards the left or right, which is of great importance with other agents' present in the scene. Each agent is embodied by a mesh randomly sampled from a parametrizable vehicle mesh library, which  allows randomization over different car models, a set of physically-realistic diffuse color, specular color, specular highlight, metalness and roughness of car body material, as shown in Fig. 3B. The mesh is reconfigured at the beginning and remains the same throughout an entire episode. We place the mesh based on the relative transform between the corresponding agent and the egocentric viewpoint for rendering. For lighting, we cast ambient light based on the average color of the scene and directional light from the egocentric viewpoint infinitely far away. For postprocessing in image space, we run image harmonization [56] with the rendered images and the foreground mask obtained via the rendering process.</p>
<p>B. Multi-agent Objectives and Tasks</p>
<p>Tasks for autonomous driving. We showcase end-to-end policy learning with the proposed multi-agent data-driven simulation on two autonomous driving tasks, car following and overtaking, majorly focusing on the latter one as it is more challenging. Both tasks involve two agents in the scene, the ego car and the front car. The front car is randomly initialized at some distance along the forward direction of the ego car with random lateral shift and heading with respect to the road curvature. The objective of car following is to keep track of the front car while the front car may perform lane changes. The ego car and the front car are set to a random speed but the same for both so that the ego car never loses the front car in its viewpoint and has sufficient information to perform tracking. In overtaking, the speed of ego car is set to a larger value than that of the front car, allowing the ego car to catch up and overtake. The goal of the ego car is to pass the front car without collision while performing lane stable maneuver. The front car of both tasks use pure pursuit controller to trace out trajectories automatically generated based on road curvature, while the ego car commands steering. Environment for reinforcement learning. We formulate our end-to-end policy learning as a RL problem given its generalizability across a wide variety of tasks. Setting up a RL environment requires definitions of environment dynamics, terminal condition, and reward function. The simulator defines how the scene dynamically evolves after receiving an agent's action. Due to the nature of the simulator rendering, a default set of terminal conditions is exceeding a threshold maximum translation or rotation, as discussed in [12]. 
ï£¹ ï£» ï£® ï£° x t â x s y t â y s Î¸ t â Î¸ s ï£¹ ï£» ,
where (x t , y t , Î¸ t ) and (x s , y s , Î¸ s ) are the poses of the virtual agent and corresponding human reference respectively. With this notation, the two aforementioned terminal conditions can be written as 1 q lat &gt;Z lat and 1 q rot &gt;Z rot , where Z * is the threshold that triggers termination. We define a lane following reward as
R lane = 1 â q lat Z lat 2 .
For car following, we can simply adapt the lane reward by changing the center line to the trajectory traced out by the front car. In overtaking, additional to lane reward, we define a pass reward based on comparing the distances traced out by both cars,
R pass = 1 áº e (t) +áº e (t) â áº f (t) +áº f (t) â¥ Z pass ,
where subscripts * e and * f denote ego and front car respectively, and Z pass is hyperparameter. To provide more learning signal for collision avoidance, we dilate the polygon of the ego car and compute its overlap with other agents,
R collision = â |Dilate(P ego ) â© P other | |P ego | ,
where P denotes a vehicle's polygons. In both tasks, we add a comfort reward that is computed as the negative second derivative of steering R com f ort = âÎ´ to reduce jittering. Policy learning. While there is no limitation for RL algorithms to be applied in our simulator, we adopt proximal policy optimization (PPO) [57] for its ubiquity and simplicity. PPO is an on-policy algorithm that maximizes the objective, E s,aâ¼Ï k min( Ï kâ1 (a|s) Ï k (a|s) A Ï k (a|s), clip( Ï kâ1 (a|s) Ï k (a|s) , 1 â Îµ, 1 + Îµ)A Ï k (a|s)) ,</p>
<p>where Ï(a|s) is the policy's action distribution given observation s, A Ï k (a|s) is advantage function which estimates how good an action is, and Îµ is a hyperparameter. The intuition is to maximize task performance, measured by the advantage function, while making sure the new policy Ï k does not deviate too much from the old policy Ï kâ1 by bounding the ratio Ï kâ1 /Ï k to a small interval, Îµ. In our case, the policy takes observation s as images and outputs action a as steering angle. We adopt a convolutional neural network (CNN) to extract image features, followed by a long shortterm memory (LSTM) recurrent network to capture motion information of other agents in the scene.</p>
<p>IV. RESULTS</p>
<p>A. Data Collection and Experimental Setup</p>
<p>Real-world testbed. We deploy the trained policies onboard our full-scale autonomous vehicles (2019 Lexus RX 450H) which we equipped with a NVIDIA 2080Ti GPU and an AMD Ryzen 7 3800X 8-Core Processor. The primary perception sensor is a 30Hz BFS-PGE-23S3C-CS camera, with a 130 â¢ horizontal field-of-view. Image data is serialized with a resolution of 960 Ã 600. Other onboard sensors include inertial measurement units (IMUs) and wheel encoders, which provide curvature feedback and odometry estimates, as well as a global positioning system (GPS) for evaluation purposes. Data for simulation is collected at a speed of â¼ 30 kph while for autonomous evaluation, the vehicle speed is lowered to â¼ 15 kph for safety reasons. Implementation details for policy learning. At episode initialization, the front car is placed randomly 6 â 15m in front and 1 â 2m laterally shifted relative to the ego car. During simulation, the front car uses a pure pursuit controller from the reference path with gain 0.8 and lookahead distance as 5m. The overlap threshold for determining collision is 5% during training and 0% at evaluation. We dilate the ego car by 1m in length and 0.4m in width and adopt episode level data augmentation by perturbing image gamma, brightness, saturation, and contrast. Images are standardized before fed to the CNN. We follow default parameters for PPO [57], except for a larger buffer size 32000 and minibatch size 512 to increase training stability. Evaluation metric. We follow two types of evaluation protocols: online active test and offline active test. Online test runs end-to-end control policies on physical full-scale autonomous vehicle, while offline test runs policies in the simulator. These tests examine the transferability from our photorealistic simulator to real-world. Active (i.e. closedloop) testing allows policies to take action and interact with the environment as opposed to passive test that computes difference from control command pre-collected in demonstration. Active testing presents a significantly more challenging evaluation and is preferred here due to the inability of passive, open-loop testing to effectively evaluate a policy [58]. In complicated scenarios and interactions with multiple agents in a scene, it is ambiguous to define ground truth for maneuvers, e.g., the policy can perform overtaking at any time as long as it keeps sufficient clearance from the front car, resulting in an infinite numbers of trajectories qualified to be considered as ground truth.</p>
<p>We measure the performance of our end-to-end policies from two perspectives: safety and stability. For a safety metric, we compute the rate of intervention while autonomous. In offline test, intervention is determined by whether the overlap of agents is larger than zero. In online test, intervention is determined by whether the human test driver takes over control of the vehicle. An additional safety metric is minimal clearance, which estimates the shortest distance between polygons that represent vehicle throughout a trial. Stability metrics measure the ability of the policy to perform lane stable maneuver during overtaking. We compute the maximal deviation, the largest lateral shift from the lane center, and maximal yaw, the largest yaw difference from road curvature, throughout a trial. The ideal maneuver finds a perfect trade-off between these two metrics types.</p>
<p>B. Policy Learning and Offline Testing</p>
<p>In this section, we present policy learning in the proposed multi-agent data-driven simulator and extensive analysis from offline active test. Compared to existing imitation learning approaches, which require vast amounts of data to generalize to real-world driving [3], [4], [48], our system demands significantly less data due to its ability to synthesize a continum of different viewpoints and trajectories from a single driving trace. Specifically, we collect around 30 minutes data in an indoor garage and around 1 hour in rural roads under different weather and time of day as dataset used for our simulator. The garage data contains only straight lanes under fixed lighting. Data from outdoor rural roads contains a wide variety of road curvatures and lighting conditions with glares and shadows, presenting significant challenges for image-based controllers. In Fig. 4, we evaluate our car-following policy through 100 simulated episodes on rural road data, each randomly initialized at a random position and tracing out a segment in the test track, and compute average tracking error at every position of the track. While the tracking error increases generally due to random lane change of the front car, we can still observe greater deviation at positions with large road curvature. To evaluate offline overtaking policies, we run 1000 simulated episodes in both the garage and rural environments (Tab. I). The rate of intervention in the garage is lower since it is an easier environment overall due to consistent lighting. The minimal clearance throughout an episode is on average around 20cm. The maximal deviation and yaw are also both relatively low to allow for overtaking without exceeding maximal lateral shift Z lat , which deteriorates the local view synthesis.</p>
<p>In Fig. 5, we perform error analysis on different configurations, including road curvature, front car speed, and initial condition (lateral difference between two cars). We group each configuration according to 4 levels: easy, normal, hard, and challenging. We can see that higher car speed and challenging initial condition greatly increase intervention rate, while the effect of larger road curvature is inconclusive, potentially due to unbalanced number of samples. The results of maximal deviation is consistent with intervention. To further investigate the effect of road curvature, in the left of Fig. 7 we run 1000 episodes, with every position of an segment tagged by whether the corresponding episode involves intervention, allowing each position at the track have its own normalized measure of intervention rate. We   can see increased number of intervention at region with high road curvature. In the middle of Fig. 7, the average deviation from the center spreads out almost evenly across the test track. In the right of Fig. 7, we overlay the relative trajectories with respect to the front car and the polygons of both vehicles at the last step for episodes with intervention. Most interventions result from slight sideswipes, implying the end-to-end policy still perform reasonable maneuvers even in failure cases.</p>
<p>C. Real-world Testing and Evaluation</p>
<p>To further verify the effectiveness of the end-to-end policy learned from our simulator, we deploy the learned model onto our full-scale autonomous vehicle and carry out in total 63 trials (2 hours autonomously) for obstacle avoidance and overtaking. In Tab  for difference scenarios. Front car being dynamic poses a more challenging task in comparison to being static and results in more intervention. In comparison to offline test, minimal clearance, maximal deviation and yaw are larger since the agent was placed in a more challenging setting by initializing at either extreme side of the road and starting the ego car immediately behind it. We also find the intervention frequency and minimal clearance to be biased towards overtaking from the right side. Our hypothesis is that the yaw difference between camera and vehicle slightly drifts along time between the real car test and camera calibration for the simulator. Even 1 degree drift can cause â¼9cm lateral shift for objects 5m in the front of the camera.</p>
<p>We perform clearance analysis in Fig. 6. We show the histogram of steps with clearance less than 2m across all episodes. The peak is at 1m, which is good for safe maneuver. Observing the local maximum at very small clearance, we evaluate the normalized cumulative trials that recall different clearance thresholds. The small initial slope indicates that most low clearance steps are contributed by a very small proportion of trials. Besides, the largest slope occurs roughly close to 0.8m, indicating that most trials have clearance at such distance. In Fig. 8, we demonstrate qualitative results with two of them performing successful overtaking and one involving intervention. The first example (row) shows how the end-to-end learned policy can avoid the moving front car and recover to the lane. In the second row, we further showcase the capability of our policy to make a challenging turn, where the road curves toward the right while the ego car can only overtake from the left. In the last row, we conduct a case study involving intervention. At the first two timesteps t1 and t2, the policy did manage to avoid the front car, while at t3, it cut back too early since the front car is lost in the view and the ego car speed is too slow for the recurrent model to memorize there was a car some time ago in the left. A potential straightforward solution can be augmenting more cameras on the side of the autonomous vehicles.</p>
<p>V. CONCLUSION</p>
<p>In this paper, we present a novel method to learn an endto-end controller using multi-agent data-driven simulation for autonomous driving. We propose several multi-agent tasks with increasing levels of complexity and conduct extensive empirical analysis within simulation as well as the realworld, where our learned policy is deployed onboard a full-scale autonomous vehicle. By leveraging photorealistic simulation we drastically reduce the amount of data required by our agent to learn a transferable policy. Potential future directions include but are not limited to multi-modality simulation, more complicated tasks that involve situational awareness from agent interaction, and edge case generation by evaluation with adversary. We believe our work opens up a new venue for leveraging high fideltiy data-driven simulation to train and evaluate visual controllers in multiagent scenarios.</p>
<p>Fig. 2 :
2Learning and deployment pipeline. (A) Agent dynamics are simulated within a virtual 3D environment world, which is generated from real 2D-image data. Photorealistic observations are rendered from the point-of-view of each agent. (B) Tasks with increasing levels of multi-agent interaction are defined within the virtual simulation world. (C) A control policy for a given task is optimized within the simulator using RL and is evaluated both offline as well as online on a full-scale autonomous vehicle in the real-world.</p>
<p>Fig. 3 :
3Multi-agent rendering. (A) Example simulated scene with an oncoming agent. Different levels of rendering compare the effect of lighting and harmonization. (B) A variety of agents are available during simulation with different styles, body material, specularity, and color.</p>
<p>Another terminal condition occurs when there is collision among agents, determined by the overlap of polygons with shape as vehicle dimension exceeding certain threshold. Besides, given how new observation is synthesized based on a local transform with respect to the data in the simulator and all autonomous driving tasks involving lane following, it's natural to define reward function based on rotational, lateral, and longitudinal component of vehicle pose with respect to the data (center line). Î¸ s â sin Î¸ s 0 sin Î¸ s cos Î¸ s 0 0 0 1</p>
<p>Fig. 4 :
4Car following tests.</p>
<p>Fig. 5 :Fig. 6 :
56Error analysis for overtaking. Breakdown of how the rate of interventions (left) and maximal deviation (right) change as a function of the trial complexity. Clearance analysis in real-world tests. Histogram of clearance across steps (left), and normalized cumulative trials recall at clearance (right).</p>
<p>TABLE I :
IOffline overtaking policy evaluation.Intervention </p>
<p>E a s y N o rm a l 
H a rd 
C h a lle n g in g </p>
<p>0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>E a s y N o rm a l 
H a rd 
C h a lle n g in g </p>
<p>0.0 </p>
<p>0.2 </p>
<p>0.4 </p>
<p>Maximal Deviation </p>
<p>Road Curvature 
Front Car Speed 
Initial Condition </p>
<p>. II, we show overall performanceFig. 7: Overtaking performance analysis. Rural intervention rates (left), and deviation from center (middle). Visualization of trajectories from intervention episodes (right).Ego Car 
Front Car </p>
<p>Deviation From Center (m) Relative Trajecories 
with Intervention 
Intervention Rate </p>
<p>0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.0 </p>
<p>0.2 </p>
<p>0.4 </p>
<p>0.6 </p>
<p>Scenario 
Average 
Intervention </p>
<p>Minimum 
Clearance </p>
<p>Maximum 
Deviation </p>
<p>Maximum 
Yaw </p>
<p>Front 
Car </p>
<p>Static 
0 / 33 (0.00) 
1.092 
1.341 
0.783 
Dynamic 
3 / 30 (0.10) 
1.063 
1.453 
0.332 </p>
<p>Overtake 
From </p>
<p>Left 
0 / 32 (0.00) 
1.410 
1.961 
0.573 
Right 
3 / 31 (0.09) 
0.735 
0.809 
0.563 </p>
<p>TABLE II :
IIOnline (real-car) active test.Fig. 8: Qualitative real-world inspection. Rows correspond to individual trials from different locations on on the test track. GPS tracking data is visualized (left) along with timepoints tagged. For each timepoint, the corresponding image view is provided (right) along with a semantic description.t1 </p>
<p>t2 </p>
<p>t3 </p>
<p>t1 
t2 
t3 </p>
<p>Out-of-view </p>
<p>t1 
t2 
t3 </p>
<p>Challenging Turn </p>
<p>t1 
t2 
t3 </p>
<p>t1 
t2 
t3 </p>
<p>Recover </p>
<p>t2 </p>
<p>t3 </p>
<p>t1 </p>
<p>End-to-end driving via conditional imitation learning. F Codevilla, M Miiller, A LÃ³pez, V Koltun, A Dosovitskiy, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)F. Codevilla, M. Miiller, A. LÃ³pez, V. Koltun, and A. Dosovitskiy, "End-to-end driving via conditional imitation learning," in Proceedings of the International Conference on Robotics and Automation (ICRA), 2018.</p>
<p>Learning to drive in a day. A Kendall, J Hawke, D Janz, P Mazur, D Reda, J.-M Allen, V.-D Lam, A Bewley, A Shah, 1807.00412arXiv preprintA. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam, A. Bewley, and A. Shah, "Learning to drive in a day," arXiv preprint 1807.00412, 2018.</p>
<p>End to end learning for self-driving cars. M Bojarski, D Testa, D Dworakowski, B Firner, B Flepp, P Goyal, L D Jackel, M Monfort, U Muller, J Zhang, arXiv preprint:1604.07316M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., "End to end learning for self-driving cars," arXiv preprint:1604.07316, 2016.</p>
<p>Variational End-to-End Navigation and Localization. A Amini, G Rosman, S Karaman, D Rus, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)A. Amini, G. Rosman, S. Karaman, and D. Rus, "Variational End-to- End Navigation and Localization," in Proceedings of the International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>End-to-end learning of driving models from large-scale video datasets. H Xu, Y Gao, F Yu, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionH. Xu, Y. Gao, F. Yu, and T. Darrell, "End-to-end learning of driving models from large-scale video datasets," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2174-2182.</p>
<p>Calibrating uncertainty models for steering angle estimation. C Hubschneider, R Hutmacher, J M ZÃ¶llner, 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEEC. Hubschneider, R. Hutmacher, and J. M. ZÃ¶llner, "Calibrating uncertainty models for steering angle estimation," in 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE, 2019, pp. 1511-1518.</p>
<p>Deep imitative models for flexible inference, planning, and control. N Rhinehart, R Mcallister, S Levine, arXiv:1810.06544arXiv preprintN. Rhinehart, R. McAllister, and S. Levine, "Deep imitative mod- els for flexible inference, planning, and control," arXiv preprint arXiv:1810.06544, 2018.</p>
<p>Learning endto-end autonomous driving using guided auxiliary supervision. A Mehta, A Subramanian, A Subramanian, Proceedings of the 11th Indian Conference on Computer Vision, Graphics and Image Processing. the 11th Indian Conference on Computer Vision, Graphics and Image ProcessingA. Mehta, A. Subramanian, and A. Subramanian, "Learning end- to-end autonomous driving using guided auxiliary supervision," in Proceedings of the 11th Indian Conference on Computer Vision, Graphics and Image Processing, 2018, pp. 1-8.</p>
<p>Multimodal end-to-end autonomous driving. Y Xiao, F Codevilla, A Gurram, O Urfalioglu, A M LÃ³pez, IEEE Transactions on Intelligent Transportation Systems. Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. LÃ³pez, "Multimodal end-to-end autonomous driving," IEEE Transactions on Intelligent Transportation Systems, 2020.</p>
<p>Virtual to Real Reinforcement Learning for Autonomous Driving. X Pan, Y You, Z Wang, C Lu, Proceedings of the British Machine Vision Conference (BMVC). the British Machine Vision Conference (BMVC)X. Pan, Y. You, Z. Wang, and C. Lu, "Virtual to Real Reinforcement Learning for Autonomous Driving," in Proceedings of the British Machine Vision Conference (BMVC), 2017.</p>
<p>Learning to Drive from Simulation without Real World Labels. A Bewley, J Rigley, Y Liu, J Hawke, R Shen, V.-D Lam, A Kendall, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)A. Bewley, J. Rigley, Y. Liu, J. Hawke, R. Shen, V.-D. Lam, and A. Kendall, "Learning to Drive from Simulation without Real World Labels," in Proceedings of the International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Learning Robust Control Policies for Endto-End Autonomous Driving From Data-Driven Simulation. A Amini, I Gilitschenski, J Phillips, J Moseyko, R Banerjee, S Karaman, D Rus, IEEE Robotics and Automation Letters. 52A. Amini, I. Gilitschenski, J. Phillips, J. Moseyko, R. Banerjee, S. Karaman, and D. Rus, "Learning Robust Control Policies for End- to-End Autonomous Driving From Data-Driven Simulation," IEEE Robotics and Automation Letters, vol. 5, no. 2, 2020.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, E. Coumans and Y. Bai, "Pybullet, a python module for physics sim- ulation for games, robotics and machine learning," http://pybullet.org, 2016-2021.</p>
<p>Y Tassa, S Tunyasuvunakool, A Muldal, Y Doron, S Liu, S Bohez, J Merel, T Erez, T Lillicrap, N Heess, dm control: Software and tasks for continuous control. arXiv preprintY. Tassa, S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, and N. Heess, "dm control: Software and tasks for continuous control," arXiv preprint: 2006.12983, 2020.</p>
<p>MuJoCo: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, Proceedings of the International Conference on Intelligent Robots and Systems (IROS). the International Conference on Intelligent Robots and Systems (IROS)E. Todorov, T. Erez, and Y. Tassa, "MuJoCo: A physics engine for model-based control," in Proceedings of the International Conference on Intelligent Robots and Systems (IROS), 2012.</p>
<p>CARLA: An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, Proceedings of the Conference on Robot Learning (CoRL). the Conference on Robot Learning (CoRL)A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, "CARLA: An open urban driving simulator," in Proceedings of the Conference on Robot Learning (CoRL), 2017.</p>
<p>AirSim: High-fidelity visual and physical simulation for autonomous vehicles. S Shah, D Dey, C Lovett, A Kapoor, Field and Service Robotics (FSR). S. Shah, D. Dey, C. Lovett, and A. Kapoor, "AirSim: High-fidelity visual and physical simulation for autonomous vehicles," in Field and Service Robotics (FSR), 2017.</p>
<p>FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality. W Guerra, E Tal, V Murali, G Ryou, S Karaman, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). W. Guerra, E. Tal, V. Murali, G. Ryou, and S. Karaman, "FlightGog- gles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality," in 2019 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems (IROS), 2019.</p>
<p>Deep Drone Racing: From Simulation to Reality With Domain Randomization. A Loquercio, E Kaufmann, R Ranftl, A Dosovitskiy, V Koltun, D Scaramuzza, Transactions on Robotics. 361A. Loquercio, E. Kaufmann, R. Ranftl, A. Dosovitskiy, V. Koltun, and D. Scaramuzza, "Deep Drone Racing: From Simulation to Reality With Domain Randomization," Transactions on Robotics, vol. 36, no. 1, 2020.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, Intelligent Robots and Systems (IROS). J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017, pp. 23-30.</p>
<p>Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes. H Alhaija, S K Mustikovela, L Mescheder, A Geiger, C Rother, International Journal of Computer Vision. 1269H. Abu Alhaija, S. K. Mustikovela, L. Mescheder, A. Geiger, and C. Rother, "Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes," International Journal of Computer Vision, vol. 126, no. 9, 2018.</p>
<p>Learning to Segment via Cutand-Paste. T Remez, J Huang, M Brown, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)T. Remez, J. Huang, and M. Brown, "Learning to Segment via Cut- and-Paste," in Proceedings of the European Conference on Computer Vision (ECCV), 2018.</p>
<p>Playable Video Generation. W Menapace, S Lathuiliere, S Tulyakov, A Siarohin, E Ricci, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2021W. Menapace, S. Lathuiliere, S. Tulyakov, A. Siarohin, and E. Ricci, "Playable Video Generation," in Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p>
<p>. W Li, C W Pan, R Zhang, J P Ren, Y X Ma, J Fang, F L Yan, Q C Geng, X Y Huang, H J Gong, W W Xu, G P , W. Li, C. W. Pan, R. Zhang, J. P. Ren, Y. X. Ma, J. Fang, F. L. Yan, Q. C. Geng, X. Y. Huang, H. J. Gong, W. W. Xu, G. P.</p>
<p>AADS: Augmented autonomous driving simulation using data-driven algorithms. D Wang, R G Manocha, Yang, Science Robotics. 428Wang, D. Manocha, and R. G. Yang, "AADS: Augmented autonomous driving simulation using data-driven algorithms," Science Robotics, vol. 4, no. 28, 2019.</p>
<p>GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition. Y Chen, F Rong, S Duggal, S Wang, X Yan, S Manivasagam, S Xue, E Yumer, R Urtasun, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2021Y. Chen, F. Rong, S. Duggal, S. Wang, X. Yan, S. Manivasagam, S. Xue, E. Yumer, and R. Urtasun, "GeoSim: Photorealistic Image Simulation with Geometry-Aware Composition," in Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p>
<p>LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World. S Manivasagam, S Wang, K Wong, W Zeng, M Sazanovich, S Tan, B Yang, W.-C Ma, R Urtasun, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2020S. Manivasagam, S. Wang, K. Wong, W. Zeng, M. Sazanovich, S. Tan, B. Yang, W.-C. Ma, and R. Urtasun, "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World," in Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</p>
<p>V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction. T.-H Wang, S Manivasagam, M Liang, B Yang, W Zeng, R Urtasun, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and R. Urtasun, "V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction," in Proceedings of the European Confer- ence on Computer Vision (ECCV), 2020.</p>
<p>Gibson Env: Real-World Perception for Embodied Agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, "Gibson Env: Real-World Perception for Embodied Agents," in Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</p>
<p>Habitat: A Platform for Embodied AI Research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra, "Habitat: A Platform for Embodied AI Research," in Proceedings of the International Conference on Computer Vision (ICCV), 2019.</p>
<p>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, L Weihs, M Yatskar, A Farhadi, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2020M. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador, D. Schwenk, E. VanderBilt, M. Wallingford, L. Weihs, M. Yatskar, and A. Farhadi, "RoboTHOR: An Open Simulation-to- Real Embodied AI Platform," in Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</p>
<p>Autonomous racing using linear parameter varying-model predictive control (LPV-MPC). E AlcalÃ¡, V Puig, J Quevedo, U Rosolia, Control Engineering Practice. 95E. AlcalÃ¡, V. Puig, J. Quevedo, and U. Rosolia, "Autonomous racing using linear parameter varying-model predictive control (LPV-MPC)," Control Engineering Practice, vol. 95, 2020.</p>
<p>Efficient implementation of Randomized MPC for miniature race cars. J V Carrau, A Liniger, X Zhang, J Lygeros, European Control Conference (ECC). J. V. Carrau, A. Liniger, X. Zhang, and J. Lygeros, "Efficient imple- mentation of Randomized MPC for miniature race cars," in European Control Conference (ECC), 2016.</p>
<p>Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction: Theory and experiment. E Galceran, A G Cunningham, R M Eustice, E Olson, Autonomous Robots. 416E. Galceran, A. G. Cunningham, R. M. Eustice, and E. Olson, "Multi- policy decision-making for autonomous driving via changepoint-based behavior prediction: Theory and experiment," Autonomous Robots, vol. 41, no. 6, 2017.</p>
<p>Real-time control for autonomous racing based on viability theory. A Liniger, J Lygeros, IEEE Transactions on Control Systems Technology. 272A. Liniger and J. Lygeros, "Real-time control for autonomous racing based on viability theory," IEEE Transactions on Control Systems Technology, vol. 27, no. 2, 2019.</p>
<p>Planning and decisionmaking for autonomous vehicles. W Schwarting, J Alonso-Mora, D Rus, Robotics, and Autonomous Systems. Annual Review of ControlW. Schwarting, J. Alonso-Mora, and D. Rus, "Planning and decision- making for autonomous vehicles," Annual Review of Control, Robotics, and Autonomous Systems, 2018.</p>
<p>Stochastic dynamic games in belief space. W Schwarting, A Pierson, S Karaman, D Rus, arxiv preprint: 1909.06963W. Schwarting, A. Pierson, S. Karaman, and D. Rus, "Stochastic dynamic games in belief space," arxiv preprint: 1909.06963, 2019.</p>
<p>G Williams, B Goldfain, P Drews, J M Rehg, E A Theodorou, 1707.04540Autonomous racing with AutoRally vehicles and differential games. arXiv preprintG. Williams, B. Goldfain, P. Drews, J. M. Rehg, and E. A. Theodorou, "Autonomous racing with AutoRally vehicles and differential games," arXiv preprint: 1707.04540, 2017.</p>
<p>A noncooperative game approach to autonomous racing. A Liniger, J Lygeros, IEEE Transactions on Control Systems Technology. 283A. Liniger and J. Lygeros, "A noncooperative game approach to au- tonomous racing," IEEE Transactions on Control Systems Technology, vol. 28, no. 3, 2020.</p>
<p>Social behavior for autonomous vehicles. W Schwarting, A Pierson, J Alonso-Mora, S Karaman, D Rus, Proceedings of the National Academy of Sciences. the National Academy of Sciences116W. Schwarting, A. Pierson, J. Alonso-Mora, S. Karaman, and D. Rus, "Social behavior for autonomous vehicles," Proceedings of the Na- tional Academy of Sciences, vol. 116, no. 50, 2019.</p>
<p>Learning by cheating. D Chen, B Zhou, V Koltun, P KrÃ¤henbÃ¼hl, Conference on Robot Learning (CoRL). 2020D. Chen, B. Zhou, V. Koltun, and P. KrÃ¤henbÃ¼hl, "Learning by cheating," in Conference on Robot Learning (CoRL), 2020.</p>
<p>CIRL: Controllable Imitative Reinforcement Learning for Vision-based Self-driving. X Liang, T Wang, L Yang, E Xing, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)X. Liang, T. Wang, L. Yang, and E. Xing, "CIRL: Controllable Imitative Reinforcement Learning for Vision-based Self-driving," in Proceedings of the European Conference on Computer Vision (ECCV), 2018.</p>
<p>ALVINN: An autonomous land vehicle in a neural network. D A Pomerleau, Advances in Neural Information Processing Systems (NeurIPS). D. A. Pomerleau, "ALVINN: An autonomous land vehicle in a neural network," in Advances in Neural Information Processing Systems (NeurIPS), 1989.</p>
<p>From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots. M Pfeiffer, M Schaeuble, J Nieto, R Siegwart, C Cadena, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)M. Pfeiffer, M. Schaeuble, J. Nieto, R. Siegwart, and C. Cadena, "From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots," in Proceedings of the International Conference on Robotics and Automation (ICRA), 2017.</p>
<p>Learning steering bounds for parallel autonomous systems. A Amini, L Paull, T Balch, S Karaman, D Rus, 2018 IEEE International Conference on Robotics and Automation (ICRA). A. Amini, L. Paull, T. Balch, S. Karaman, and D. Rus, "Learning steering bounds for parallel autonomous systems," in 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018.</p>
<p>End to end vehicle lateral control using a single fisheye camera. M Toromanoff, E Wirbel, F Wilhelm, C Vejarano, X Perrotton, F Moutarde, Proceedings of the International Conference on Intelligent Robots and Systems (IROS). the International Conference on Intelligent Robots and Systems (IROS)M. Toromanoff, E. Wirbel, F. Wilhelm, C. Vejarano, X. Perrotton, and F. Moutarde, "End to end vehicle lateral control using a single fisheye camera," in Proceedings of the International Conference on Intelligent Robots and Systems (IROS), 2018.</p>
<p>ChauffeurNet: Learning to drive by imitating the best and synthesizing the worst. M Bansal, A Krizhevsky, A Ogale, Proceedings of Robotics: Science and Systems (RSS). Robotics: Science and Systems (RSS)M. Bansal, A. Krizhevsky, and A. Ogale, "ChauffeurNet: Learning to drive by imitating the best and synthesizing the worst," in Proceedings of Robotics: Science and Systems (RSS), 2019.</p>
<p>Neural circuit policies enabling auditable autonomy. M Lechner, R Hasani, A Amini, T A Henzinger, D Rus, R Grosu, Nature Machine Intelligence. 210M. Lechner, R. Hasani, A. Amini, T. A. Henzinger, D. Rus, and R. Grosu, "Neural circuit policies enabling auditable autonomy," Nature Machine Intelligence, vol. 2, no. 10, pp. 642-652, 2020.</p>
<p>Urban Driving with Conditional Imitation Learning. J Hawke, R Shen, C Gurau, S Sharma, D Reda, N Nikolov, P Mazur, S Micklethwaite, N Griffiths, A Shah, A Kndall, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)2020J. Hawke, R. Shen, C. Gurau, S. Sharma, D. Reda, N. Nikolov, P. Mazur, S. Micklethwaite, N. Griffiths, A. Shah, and A. Kndall, "Urban Driving with Conditional Imitation Learning," in Proceedings of the International Conference on Robotics and Automation (ICRA), 2020.</p>
<p>Autonomous Robot Navigation Based on Multi-Camera Perception. K Zhu, W Chen, W Zhang, R Song, Y Li, Proceedingss of the International Conference on Intelligent Robots and Systems (IROS). s of the International Conference on Intelligent Robots and Systems (IROS)2020K. Zhu, W. Chen, W. Zhang, R. Song, and Y. Li, "Autonomous Robot Navigation Based on Multi-Camera Perception," in Proceedingss of the International Conference on Intelligent Robots and Systems (IROS), 2020.</p>
<p>Model-based versus Model-free Deep Reinforcement Learning for Autonomous Racing Cars. A Brunnbauer, L Berducci, A BrandstÃ¤tter, M Lechner, R Hasani, D Rus, R Grosu, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)2021A. Brunnbauer, L. Berducci, A. BrandstÃ¤tter, M. Lechner, R. Hasani, D. Rus, and R. Grosu, "Model-based versus Model-free Deep Re- inforcement Learning for Autonomous Racing Cars," in Proceedings of the International Conference on Robotics and Automation (ICRA), 2021.</p>
<p>Conditional Affordance Learning for Driving in Urban Environments. A Sauer, N Savinov, A Geiger, Proceedings of the Conference on Robot Learning (CoRL). the Conference on Robot Learning (CoRL)A. Sauer, N. Savinov, and A. Geiger, "Conditional Affordance Learn- ing for Driving in Urban Environments," in Proceedings of the Conference on Robot Learning (CoRL), 2018.</p>
<p>Variational autoencoder for end-to-end control of autonomous driving with novelty detection and training de-biasing. A Amini, W Schwarting, G Rosman, B Araki, S Karaman, D Rus, 2018A. Amini, W. Schwarting, G. Rosman, B. Araki, S. Karaman, and D. Rus, "Variational autoencoder for end-to-end control of autonomous driving with novelty detection and training de-biasing," in 2018</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 568-575.</p>
<p>Autonomous Overtaking in Gran Turismo Sport Using Curriculum Reinforcement Learning. Y Song, H Lin, E Kaufmann, P Duerr, D Scaramuzza, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)2021Y. Song, H. Lin, E. Kaufmann, P. Duerr, and D. Scaramuzza, "Au- tonomous Overtaking in Gran Turismo Sport Using Curriculum Rein- forcement Learning," in Proceedings of the International Conference on Robotics and Automation (ICRA), 2021.</p>
<p>Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space. W Schwarting, T Seyde, I Gilitschenski, L Liebenwein, R Sander, S Karaman, D Rus, Proceedingss of the Conference on Robot Learning (CoRL). s of the Conference on Robot Learning (CoRL)2020W. Schwarting, T. Seyde, I. Gilitschenski, L. Liebenwein, R. Sander, S. Karaman, and D. Rus, "Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space," in Proceedingss of the Conference on Robot Learning (CoRL), 2020.</p>
<p>End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances. M Toromanoff, E Wirbel, F Moutarde, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2020M. Toromanoff, E. Wirbel, and F. Moutarde, "End-to-End Model- Free Reinforcement Learning for Urban Driving Using Implicit Af- fordances," in Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</p>
<p>Foreground-aware semantic representations for image harmonization. K Sofiiuk, P Popenova, A Konushin, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionK. Sofiiuk, P. Popenova, and A. Konushin, "Foreground-aware seman- tic representations for image harmonization," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 1620-1629.</p>
<p>Proximal Policy Optimization Algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv prepring:1707.06347J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal Policy Optimization Algorithms," arXiv prepring:1707.06347, 2017.</p>
<p>On offline evaluation of vision-based driving models. F Codevilla, A M Lopez, V Koltun, A Dosovitskiy, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)F. Codevilla, A. M. Lopez, V. Koltun, and A. Dosovitskiy, "On offline evaluation of vision-based driving models," in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 236- 251.</p>            </div>
        </div>

    </div>
</body>
</html>
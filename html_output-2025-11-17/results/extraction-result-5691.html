<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5691 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5691</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5691</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-afdf2b9c7cc9ebccfa0876b9b090e2f2850c194d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/afdf2b9c7cc9ebccfa0876b9b090e2f2850c194d" target="_blank">Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https://github.com/JosephJeesungSuh/subpop.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5691.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5691.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained decoder-only transformer family member (7B) used as a text-based simulator to predict distributions of multiple-choice survey responses conditional on demographic/ideological subpopulation descriptions; fine-tuned with LoRA on the SubPOP survey dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Llama-2 family model (decoder transformer). In this paper it is used in its pretrained form (not instruction/RLHF tuned) and fine-tuned via LoRA (rank=8, alpha=32, dropout 0.05) on survey-response distribution data (SubPOP).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public opinion research / political and social science survey simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict the full probability distribution over multiple-choice survey answers for a given survey question conditioned on a specified subpopulation (demographic, socioeconomic, or ideological).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Wasserstein distance (WD) between model-predicted and human response distributions (primary); training uses forward KL divergence as the objective (alternative WD training reported), and one-hot accuracy is reported for some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>OpinionQA WD: zero-shot 0.173 → fine-tuned 0.106; SubPOP-Eval WD: zero-shot 0.206 → fine-tuned 0.121. Human lower bound WD (OpinionQA) = 0.031; uniform upper bound = 0.178. Reported relative improvements across subpopulations: average relative improvement ≈ 46.7% (per-group average reported across models).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Fine-tuning on structured survey distributions (SubPOP) vs. prompting; model initialization (pretrained base vs instruction-tuned/chat); model size (comparison across sizes); prompt format and few-shot vs zero-shot; how the human distribution is represented during training (explicit probability modelling vs one-hot vs augmentation); amount of fine-tuning data (scaling); choice of training objective (forward KL vs WD); subpopulation heterogeneity and sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Direct comparisons in Table 1 showing large WD reductions after fine-tuning; ablations in Appendix C.1 (explicit distribution modeling outperforms one-hot and augmentation), C.2 (chat/instruction-tuned variants differ), Section 4.4 and Figure 5 (dataset-scaling experiments showing diminishing returns but continued gains), Section 3.1 and Appendix B (KL vs WD training objectives and Figure 9 showing similar WD convergence), and prompt-baseline experiments (zero-shot/few-shot prompt formats) showing degraded performance relative to SubPOP fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated: compute token-level probabilities for each option with vLLM and derive a predicted distribution; map ordinal choices to numeric values and compute 1-D Wasserstein distance between predicted and weighted (survey) human distributions averaged over questions and subpopulations; human lower bound estimated by bootstrapping respondent samples (R=1000) to obtain confidence intervals. Datasets used: OpinionQA (held-out ATP questions) and SubPOP-Eval (GSS subset).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fine-tuned models still do not reach human lower bound (residual WD gap). Performance varies across some small or highly heterogeneous groups (e.g., 'Other' race/ethnicity shows worse alignment). Instruction-tuned/chat variants behaved differently (sometimes worse on OOD evals). Sensitivity to prompt formatting and to the representation of response distributions if not trained with explicit probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to zero-shot QA/BIO/PORTRAY prompting and few-shot prompting (Table 1) and to Modular Pluralism, fine-tuning on SubPOP substantially reduces WD (paper reports 32–46% reduction on OpinionQA and 39–42% on SubPOP-Eval vs best baselines). Ablations: explicit probability modeling > augmentation > one-hot (Table 5). Data-scaling comparisons in Figure 5 show monotonic improvement as training data grows.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Fine-tune pretrained base LLMs (not instruction-tuned) on structured survey data with explicit probability targets; use forward KL training objective for practicality (paper uses KL for most experiments), represent answer options as ordinal for WD evaluation, allocate training samples uniformly across subpopulations to avoid overfitting to large groups, use LoRA for efficient fine-tuning, and increase high-quality survey data size for stronger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5691.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5691.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (13 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Llama-2 model (13B) used similarly as a simulator of subpopulation survey-response distributions and fine-tuned with the same SubPOP procedure; evaluated on same datasets and metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Llama-2 family model (13B parameters). In experiments it is fine-tuned via LoRA on SubPOP to predict response distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public opinion research / survey simulation (social science)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict full distributions over multiple-choice survey answers for prompts specifying a question and a target subpopulation.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Wasserstein distance (WD) between model and human distributions; KL used as training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>OpinionQA WD: zero-shot ~0.170 → fine-tuned 0.102; SubPOP-Eval WD: zero-shot ~0.196 → fine-tuned 0.113. These represent ~40%+ reductions in WD vs zero-shot baselines reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Same as for Llama-2-7B: fine-tuning on survey distributions, model initialization, dataset size, representation of response distributions, prompt design, and ordinality mapping for WD.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Table 1 and group/wave-level analyses (Tables 8–10) show consistent improvements after SubPOP fine-tuning across waves and subpopulations, and Appendix experiments (C.1, C.2) show response-modeling and instruction-tuning effects.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>WD on OpinionQA and SubPOP-Eval; bootstrapped human lower bounds; aggregated averages across questions/groups; inference via vLLM to extract option probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although improved, WD remains above human bootstrap lower bound; some groups and waves still have higher WD. Sensitivity to instruction-tuned checkpoints observed in other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to Llama-2-7B, Mistral-7B, and Llama-3-70B in Table 1; all fine-tuned models outperform prompting baselines, with smaller differences between fine-tuned sizes than between fine-tuned vs prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use explicit probability targets and balanced subpopulation sampling for fine-tuning; scale dataset size to further reduce WD; consider base pretrained checkpoints rather than instruction-tuned ones for this distributional prediction task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5691.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5691.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B (v0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter Mistral model used as a simulator to predict distributions of survey responses conditioned on subpopulation labels; fine-tuned on SubPOP and evaluated alongside Llama models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Mistral 7B decoder transformer (paper references Mistral-7B). In this work it is fine-tuned with LoRA on survey distribution targets from SubPOP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public opinion research / survey simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict probability distributions over multiple-choice survey options conditioned on a target subpopulation and question text.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Wasserstein distance (WD) between predicted and human distributions; forward KL used as training objective in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>OpinionQA WD: zero-shot 0.153 → fine-tuned 0.096; SubPOP-Eval WD: zero-shot 0.187 → fine-tuned 0.115 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Fine-tuning on SubPOP, dataset size, modeling choice for response distributions (explicit probability modeling best), prompt format sensitivity, and model initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Table 1 (comparison across models and baselines), Table 5 (explicit modeling ablation), Figure 5 (data-scaling), and per-group/wave analyses showing consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>WD on OpinionQA and SubPOP-Eval, averaged across questions and groups; bootstrapped human lower bound; vLLM inference to obtain option probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still above human lower bound; some demographic groups and some GSS evaluation items show higher WD; modular pluralism and certain prompting strategies can produce different trade-offs (e.g., better one-hot accuracy but worse distributional WD).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms zero-shot and few-shot prompting baselines after fine-tuning; comparable or slightly better WD than some Llama variants when fine-tuned (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Fine-tune Mistral-7B on explicit survey distributions using LoRA and forward KL; prefer explicit probability outputs rather than one-hot or heavy augmentation; increase dataset size for further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5691.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5691.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (70 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large (70B) Llama-3 model used as a text-based simulator for survey-response distributions; fine-tuned with LoRA on SubPOP and evaluated for generalization across topics and survey families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Llama-3 family large decoder model (70B). Used in pretrained form and fine-tuned (LoRA) on SubPOP to align generated response distributions with human survey distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public opinion research / survey simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate probability distributions over multiple-choice survey answers conditional on question and subpopulation descriptions, aiming to mirror human response distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Wasserstein distance (WD) between predicted and human distributions (primary); training via forward KL in most experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>OpinionQA WD: zero-shot 0.138 → fine-tuned 0.094; SubPOP-Eval WD: zero-shot 0.160 → fine-tuned 0.096 (Table 1). Fine-tuned Llama-3-70B is among the best-performing models by WD in the reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Same set: fine-tuning on SubPOP, model size and pretraining, representation of human distributions during training, prompt sensitivity, and amount/diversity of fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Table 1 shows Llama-3-70B achieves the lowest WD after SubPOP fine-tuning; Figure 5 (scaling) indicates gains continue with more data; per-wave/group breakdowns in Tables 8–10 demonstrate consistent improvements across topics and groups.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Wasserstein distance on OpinionQA and SubPOP-Eval with ordinality mapping; bootstrapped human lower bounds; vLLM inference for option probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite strong performance, still above empirical human lower bound; zero-shot prompting improvements are modest even for large models; sensitivity to prompt formatting and to whether the model is instruction-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared with smaller models (7B/13B) and prompting baselines; Llama-3-70B fine-tuned yields the best absolute WD numbers among evaluated models in Table 1, but relative gains from fine-tuning are consistent across sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Fine-tune large pretrained models on structured, distributional survey data with explicit probability targets and forward KL; increasing SubPOP-scale training data would further reduce WD toward the human lower bound.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5691.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5691.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 Chat (7B, instruction-tuned / RLHF variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned / chat-form Llama-2-7B variant evaluated as a simulator; shows different behavior from base pretrained checkpoints—worse zero-shot WD but after fine-tuning can match base fine-tuned performance on in-distribution data while sometimes lagging on OOD evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned (chat/RLHF) variant of Llama-2-7B. The paper fine-tunes this chat model on SubPOP to observe how instruction tuning affects distributional prediction behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public opinion research / survey simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same survey-response distribution prediction as other models: output a probability distribution over options for a question conditioned on a subpopulation.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Wasserstein distance (WD); forward KL for training.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Table 6: Chat zero-shot WD: 0.308 (OpinionQA) / 0.383 (SubPOP-Eval) — substantially worse than base zero-shot; after fine-tuning Chat LLM FT WD: 0.109 (OpinionQA) and 0.148 (SubPOP-Eval). Base Llama-2-7B fine-tuned: 0.106 / 0.121 respectively, showing chat FT nearly matches base FT on OpinionQA but worse on SubPOP-Eval.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Instruction-tuning changes output token probability concentration (often sharper/high-probability tokens), which can hurt distributional WD; fine-tuning can mitigate but OOD generalization may still be worse relative to base model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Appendix C.2 and Table 6 compare base vs chat variants; authors note instruction-tuned models tend to assign high probabilities to specific tokens, increasing WD when compared to distributed human responses (cited in main text and C.2).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Wasserstein distance on OpinionQA and SubPOP-Eval before and after SubPOP fine-tuning; same bootstrapped lower-bound procedure and vLLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Instruction-tuned/chat checkpoints have much worse zero-shot WD and may generalize worse on out-of-distribution survey families even after fine-tuning (observed higher WD on SubPOP-Eval relative to base fine-tuned model).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparison in Table 6 between Base Llama-2-7B, Chat zero-shot, and Chat fine-tuned demonstrates the negative effect of instruction tuning on zero-shot distributional alignment and partial recovery via fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer base pretrained checkpoints for distributional opinion prediction tasks or be cautious when using instruction-tuned/chat checkpoints; if using chat models, perform explicit fine-tuning on distributional survey data to recover aligned output probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Out of one, many: Using language models to simulate human samples <em>(Rating: 2)</em></li>
                <li>Whose opinions do language models reflect? <em>(Rating: 2)</em></li>
                <li>Towards measuring the representation of subjective global opinions in language models <em>(Rating: 2)</em></li>
                <li>Specializing large language models to simulate survey response distributions for global populations <em>(Rating: 2)</em></li>
                <li>Virtual personas for language models via an anthology of backstories <em>(Rating: 1)</em></li>
                <li>Group preference optimization: Few-shot alignment of large language models <em>(Rating: 1)</em></li>
                <li>Modular pluralism: Pluralistic alignment via multi-LLM collaboration <em>(Rating: 2)</em></li>
                <li>Language models trained on media diets can predict public opinion <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5691",
    "paper_id": "paper-afdf2b9c7cc9ebccfa0876b9b090e2f2850c194d",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama 2 (7 billion parameters)",
            "brief_description": "A pretrained decoder-only transformer family member (7B) used as a text-based simulator to predict distributions of multiple-choice survey responses conditional on demographic/ideological subpopulation descriptions; fine-tuned with LoRA on the SubPOP survey dataset.",
            "citation_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_description": "Pretrained Llama-2 family model (decoder transformer). In this paper it is used in its pretrained form (not instruction/RLHF tuned) and fine-tuned via LoRA (rank=8, alpha=32, dropout 0.05) on survey-response distribution data (SubPOP).",
            "model_size": "7B",
            "scientific_subdomain": "Public opinion research / political and social science survey simulation",
            "simulation_task": "Predict the full probability distribution over multiple-choice survey answers for a given survey question conditioned on a specified subpopulation (demographic, socioeconomic, or ideological).",
            "accuracy_metric": "Wasserstein distance (WD) between model-predicted and human response distributions (primary); training uses forward KL divergence as the objective (alternative WD training reported), and one-hot accuracy is reported for some baselines.",
            "reported_accuracy": "OpinionQA WD: zero-shot 0.173 → fine-tuned 0.106; SubPOP-Eval WD: zero-shot 0.206 → fine-tuned 0.121. Human lower bound WD (OpinionQA) = 0.031; uniform upper bound = 0.178. Reported relative improvements across subpopulations: average relative improvement ≈ 46.7% (per-group average reported across models).",
            "factors_affecting_accuracy": "Fine-tuning on structured survey distributions (SubPOP) vs. prompting; model initialization (pretrained base vs instruction-tuned/chat); model size (comparison across sizes); prompt format and few-shot vs zero-shot; how the human distribution is represented during training (explicit probability modelling vs one-hot vs augmentation); amount of fine-tuning data (scaling); choice of training objective (forward KL vs WD); subpopulation heterogeneity and sample sizes.",
            "evidence_for_factors": "Direct comparisons in Table 1 showing large WD reductions after fine-tuning; ablations in Appendix C.1 (explicit distribution modeling outperforms one-hot and augmentation), C.2 (chat/instruction-tuned variants differ), Section 4.4 and Figure 5 (dataset-scaling experiments showing diminishing returns but continued gains), Section 3.1 and Appendix B (KL vs WD training objectives and Figure 9 showing similar WD convergence), and prompt-baseline experiments (zero-shot/few-shot prompt formats) showing degraded performance relative to SubPOP fine-tuning.",
            "evaluation_method": "Automated: compute token-level probabilities for each option with vLLM and derive a predicted distribution; map ordinal choices to numeric values and compute 1-D Wasserstein distance between predicted and weighted (survey) human distributions averaged over questions and subpopulations; human lower bound estimated by bootstrapping respondent samples (R=1000) to obtain confidence intervals. Datasets used: OpinionQA (held-out ATP questions) and SubPOP-Eval (GSS subset).",
            "limitations_or_failure_cases": "Fine-tuned models still do not reach human lower bound (residual WD gap). Performance varies across some small or highly heterogeneous groups (e.g., 'Other' race/ethnicity shows worse alignment). Instruction-tuned/chat variants behaved differently (sometimes worse on OOD evals). Sensitivity to prompt formatting and to the representation of response distributions if not trained with explicit probabilities.",
            "comparisons": "Compared to zero-shot QA/BIO/PORTRAY prompting and few-shot prompting (Table 1) and to Modular Pluralism, fine-tuning on SubPOP substantially reduces WD (paper reports 32–46% reduction on OpinionQA and 39–42% on SubPOP-Eval vs best baselines). Ablations: explicit probability modeling &gt; augmentation &gt; one-hot (Table 5). Data-scaling comparisons in Figure 5 show monotonic improvement as training data grows.",
            "recommendations_or_best_practices": "Fine-tune pretrained base LLMs (not instruction-tuned) on structured survey data with explicit probability targets; use forward KL training objective for practicality (paper uses KL for most experiments), represent answer options as ordinal for WD evaluation, allocate training samples uniformly across subpopulations to avoid overfitting to large groups, use LoRA for efficient fine-tuning, and increase high-quality survey data size for stronger gains.",
            "uuid": "e5691.0",
            "source_info": {
                "paper_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-2-13B",
            "name_full": "Llama 2 (13 billion parameters)",
            "brief_description": "A larger Llama-2 model (13B) used similarly as a simulator of subpopulation survey-response distributions and fine-tuned with the same SubPOP procedure; evaluated on same datasets and metrics.",
            "citation_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B",
            "model_description": "Pretrained Llama-2 family model (13B parameters). In experiments it is fine-tuned via LoRA on SubPOP to predict response distributions.",
            "model_size": "13B",
            "scientific_subdomain": "Public opinion research / survey simulation (social science)",
            "simulation_task": "Predict full distributions over multiple-choice survey answers for prompts specifying a question and a target subpopulation.",
            "accuracy_metric": "Wasserstein distance (WD) between model and human distributions; KL used as training objective.",
            "reported_accuracy": "OpinionQA WD: zero-shot ~0.170 → fine-tuned 0.102; SubPOP-Eval WD: zero-shot ~0.196 → fine-tuned 0.113. These represent ~40%+ reductions in WD vs zero-shot baselines reported in Table 1.",
            "factors_affecting_accuracy": "Same as for Llama-2-7B: fine-tuning on survey distributions, model initialization, dataset size, representation of response distributions, prompt design, and ordinality mapping for WD.",
            "evidence_for_factors": "Table 1 and group/wave-level analyses (Tables 8–10) show consistent improvements after SubPOP fine-tuning across waves and subpopulations, and Appendix experiments (C.1, C.2) show response-modeling and instruction-tuning effects.",
            "evaluation_method": "WD on OpinionQA and SubPOP-Eval; bootstrapped human lower bounds; aggregated averages across questions/groups; inference via vLLM to extract option probabilities.",
            "limitations_or_failure_cases": "Although improved, WD remains above human bootstrap lower bound; some groups and waves still have higher WD. Sensitivity to instruction-tuned checkpoints observed in other experiments.",
            "comparisons": "Compared directly to Llama-2-7B, Mistral-7B, and Llama-3-70B in Table 1; all fine-tuned models outperform prompting baselines, with smaller differences between fine-tuned sizes than between fine-tuned vs prompting.",
            "recommendations_or_best_practices": "Use explicit probability targets and balanced subpopulation sampling for fine-tuning; scale dataset size to further reduce WD; consider base pretrained checkpoints rather than instruction-tuned ones for this distributional prediction task.",
            "uuid": "e5691.1",
            "source_info": {
                "paper_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B (v0.1)",
            "brief_description": "A 7B-parameter Mistral model used as a simulator to predict distributions of survey responses conditioned on subpopulation labels; fine-tuned on SubPOP and evaluated alongside Llama models.",
            "citation_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-v0.1",
            "model_description": "Pretrained Mistral 7B decoder transformer (paper references Mistral-7B). In this work it is fine-tuned with LoRA on survey distribution targets from SubPOP.",
            "model_size": "7B",
            "scientific_subdomain": "Public opinion research / survey simulation",
            "simulation_task": "Predict probability distributions over multiple-choice survey options conditioned on a target subpopulation and question text.",
            "accuracy_metric": "Wasserstein distance (WD) between predicted and human distributions; forward KL used as training objective in main experiments.",
            "reported_accuracy": "OpinionQA WD: zero-shot 0.153 → fine-tuned 0.096; SubPOP-Eval WD: zero-shot 0.187 → fine-tuned 0.115 (Table 1).",
            "factors_affecting_accuracy": "Fine-tuning on SubPOP, dataset size, modeling choice for response distributions (explicit probability modeling best), prompt format sensitivity, and model initialization.",
            "evidence_for_factors": "Table 1 (comparison across models and baselines), Table 5 (explicit modeling ablation), Figure 5 (data-scaling), and per-group/wave analyses showing consistent gains.",
            "evaluation_method": "WD on OpinionQA and SubPOP-Eval, averaged across questions and groups; bootstrapped human lower bound; vLLM inference to obtain option probabilities.",
            "limitations_or_failure_cases": "Still above human lower bound; some demographic groups and some GSS evaluation items show higher WD; modular pluralism and certain prompting strategies can produce different trade-offs (e.g., better one-hot accuracy but worse distributional WD).",
            "comparisons": "Outperforms zero-shot and few-shot prompting baselines after fine-tuning; comparable or slightly better WD than some Llama variants when fine-tuned (see Table 1).",
            "recommendations_or_best_practices": "Fine-tune Mistral-7B on explicit survey distributions using LoRA and forward KL; prefer explicit probability outputs rather than one-hot or heavy augmentation; increase dataset size for further gains.",
            "uuid": "e5691.2",
            "source_info": {
                "paper_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3-70B",
            "name_full": "Llama 3 (70 billion parameters)",
            "brief_description": "A large (70B) Llama-3 model used as a text-based simulator for survey-response distributions; fine-tuned with LoRA on SubPOP and evaluated for generalization across topics and survey families.",
            "citation_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B",
            "model_description": "Pretrained Llama-3 family large decoder model (70B). Used in pretrained form and fine-tuned (LoRA) on SubPOP to align generated response distributions with human survey distributions.",
            "model_size": "70B",
            "scientific_subdomain": "Public opinion research / survey simulation",
            "simulation_task": "Generate probability distributions over multiple-choice survey answers conditional on question and subpopulation descriptions, aiming to mirror human response distributions.",
            "accuracy_metric": "Wasserstein distance (WD) between predicted and human distributions (primary); training via forward KL in most experiments.",
            "reported_accuracy": "OpinionQA WD: zero-shot 0.138 → fine-tuned 0.094; SubPOP-Eval WD: zero-shot 0.160 → fine-tuned 0.096 (Table 1). Fine-tuned Llama-3-70B is among the best-performing models by WD in the reported results.",
            "factors_affecting_accuracy": "Same set: fine-tuning on SubPOP, model size and pretraining, representation of human distributions during training, prompt sensitivity, and amount/diversity of fine-tuning data.",
            "evidence_for_factors": "Table 1 shows Llama-3-70B achieves the lowest WD after SubPOP fine-tuning; Figure 5 (scaling) indicates gains continue with more data; per-wave/group breakdowns in Tables 8–10 demonstrate consistent improvements across topics and groups.",
            "evaluation_method": "Wasserstein distance on OpinionQA and SubPOP-Eval with ordinality mapping; bootstrapped human lower bounds; vLLM inference for option probabilities.",
            "limitations_or_failure_cases": "Despite strong performance, still above empirical human lower bound; zero-shot prompting improvements are modest even for large models; sensitivity to prompt formatting and to whether the model is instruction-tuned.",
            "comparisons": "Compared with smaller models (7B/13B) and prompting baselines; Llama-3-70B fine-tuned yields the best absolute WD numbers among evaluated models in Table 1, but relative gains from fine-tuning are consistent across sizes.",
            "recommendations_or_best_practices": "Fine-tune large pretrained models on structured, distributional survey data with explicit probability targets and forward KL; increasing SubPOP-scale training data would further reduce WD toward the human lower bound.",
            "uuid": "e5691.3",
            "source_info": {
                "paper_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-2-7B-chat",
            "name_full": "Llama 2 Chat (7B, instruction-tuned / RLHF variant)",
            "brief_description": "An instruction-tuned / chat-form Llama-2-7B variant evaluated as a simulator; shows different behavior from base pretrained checkpoints—worse zero-shot WD but after fine-tuning can match base fine-tuned performance on in-distribution data while sometimes lagging on OOD evaluation.",
            "citation_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat",
            "model_description": "An instruction-tuned (chat/RLHF) variant of Llama-2-7B. The paper fine-tunes this chat model on SubPOP to observe how instruction tuning affects distributional prediction behavior.",
            "model_size": "7B",
            "scientific_subdomain": "Public opinion research / survey simulation",
            "simulation_task": "Same survey-response distribution prediction as other models: output a probability distribution over options for a question conditioned on a subpopulation.",
            "accuracy_metric": "Wasserstein distance (WD); forward KL for training.",
            "reported_accuracy": "Table 6: Chat zero-shot WD: 0.308 (OpinionQA) / 0.383 (SubPOP-Eval) — substantially worse than base zero-shot; after fine-tuning Chat LLM FT WD: 0.109 (OpinionQA) and 0.148 (SubPOP-Eval). Base Llama-2-7B fine-tuned: 0.106 / 0.121 respectively, showing chat FT nearly matches base FT on OpinionQA but worse on SubPOP-Eval.",
            "factors_affecting_accuracy": "Instruction-tuning changes output token probability concentration (often sharper/high-probability tokens), which can hurt distributional WD; fine-tuning can mitigate but OOD generalization may still be worse relative to base model fine-tuning.",
            "evidence_for_factors": "Appendix C.2 and Table 6 compare base vs chat variants; authors note instruction-tuned models tend to assign high probabilities to specific tokens, increasing WD when compared to distributed human responses (cited in main text and C.2).",
            "evaluation_method": "Wasserstein distance on OpinionQA and SubPOP-Eval before and after SubPOP fine-tuning; same bootstrapped lower-bound procedure and vLLM inference.",
            "limitations_or_failure_cases": "Instruction-tuned/chat checkpoints have much worse zero-shot WD and may generalize worse on out-of-distribution survey families even after fine-tuning (observed higher WD on SubPOP-Eval relative to base fine-tuned model).",
            "comparisons": "Direct comparison in Table 6 between Base Llama-2-7B, Chat zero-shot, and Chat fine-tuned demonstrates the negative effect of instruction tuning on zero-shot distributional alignment and partial recovery via fine-tuning.",
            "recommendations_or_best_practices": "Prefer base pretrained checkpoints for distributional opinion prediction tasks or be cautious when using instruction-tuned/chat checkpoints; if using chat models, perform explicit fine-tuning on distributional survey data to recover aligned output probabilities.",
            "uuid": "e5691.4",
            "source_info": {
                "paper_title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Out of one, many: Using language models to simulate human samples",
            "rating": 2
        },
        {
            "paper_title": "Whose opinions do language models reflect?",
            "rating": 2
        },
        {
            "paper_title": "Towards measuring the representation of subjective global opinions in language models",
            "rating": 2
        },
        {
            "paper_title": "Specializing large language models to simulate survey response distributions for global populations",
            "rating": 2
        },
        {
            "paper_title": "Virtual personas for language models via an anthology of backstories",
            "rating": 1
        },
        {
            "paper_title": "Group preference optimization: Few-shot alignment of large language models",
            "rating": 1
        },
        {
            "paper_title": "Modular pluralism: Pluralistic alignment via multi-LLM collaboration",
            "rating": 2
        },
        {
            "paper_title": "Language models trained on media diets can predict public opinion",
            "rating": 1
        }
    ],
    "cost": 0.019601,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</h1>
<p>Joseph Suh ${ }^{1 <em>}$, Erfan Jahanparast ${ }^{1 </em>}$, Suhong Moon ${ }^{1 <em>}$, Minwoo Kang ${ }^{1 </em>}$, Serina Chang ${ }^{1,2}$<br>${ }^{1}$ University of California, Berkeley ${ }^{2}$ Microsoft Research<br>Correspondence: [josephsuh,serinac] @berkeley.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70 K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to $46 \%$ compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https: //github.com/JosephJeesungSuh/subpop.</p>
<h2>1 Introduction</h2>
<p>Surveys provide an essential tool for probing public opinions on societal issues, especially as opinions vary over time and across subpopulations. However, surveys are also costly, time-consuming, and require careful calibration to mitigate non-response and sampling biases (Choi and Pak, 2004; Bethlehem, 2010). Recent work suggests that large language models (LLMs) can assist public opinion studies by predicting survey responses across different subpopulations, explored in both social science</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of our method and SubPOP. We collect survey data from two survey families-ATP from Pew Research (Pew Research Center, 2018) (forming SubPOP-Train) and GSS from NORC (Davern et al., 2024) (forming SubPOP-Eval). LLMs are fine-tuned on SubPOP-Train and evaluated on both OpinionQA (Santurkar et al., 2023) and SubPOP-Eval to assess generalization of distributional opinion prediction across unseen survey topics, survey families, and subpopulations.
(Argyle et al., 2023; Bail, 2024; Ashokkumar et al., 2024; Manning et al., 2024) and NLP (Santurkar et al., 2023; Chu et al., 2023; Moon et al., 2024; Hämäläinen et al., 2023; Chiang and Lee, 2023). Such capabilities could substantially enhance the survey development process, not as a replacement for human participants but as a tool for researchers to conduct pilot testing, identify subpopulations to over-sample, and test analysis pipelines prior to conducting the full survey (Rothschild et al., 2024).</p>
<p>Prior work in steering language models, i.e. conditioning models to reflect the opinions of a specific subpopulation, has primarily investigated different prompt engineering techniques (Santurkar et al., 2023; Moon et al., 2024; Park et al., 2024a).</p>
<p>However, prompting alone has shown limited success in generating completions that accurately reflect the distributions of survey responses collected from human subjects. Off-the-shelf LLMs (Achiam et al., 2023; Dubey et al., 2024; Jiang et al., 2023) have shown to mirror the opinions of certain US subpopulations such as the wealthy and educated (Santurkar et al., 2023; Gallegos et al., 2024; Deshpande et al., 2023; Kim and Lee, 2023), while generating stereotypical or biased predictions of underrepresented groups (Cheng et al., 2023b,a; Wang et al., 2024). Furthermore, these models often fail to capture the variation of human opinions within a subpopulation (Kapania et al., 2024; Park et al., 2024b). While fine-tuning presents opportunities to address these limitations (Chu et al., 2023; He et al., 2024), existing methods fail to train models that accurately predict opinion distributions across diverse survey question topics and subpopulations.</p>
<p>The present work. Here, we propose directly finetuning LLMs on large-scale, high-quality survey data, consisting of questions about diverse topics and responses from each subpopulation, defined by demographic, socioeconomic, and ideological traits. By casting pairs of (subpopulation, survey question) as input prompts, we train the LLM to align its response distribution against that of human subjects in a supervised manner. We posit that survey data is particularly well-suited for fine-tuning LLMs since: (1) We can train the model with clear subpopulation-response pairs that explicitly link group identities and expressed opinions, which is rare in LLMs' pre-training corpora, (2) Large-scale opinion polls are carefully designed and calibrated (e.g. using post-stratification) to estimate representative human responses, in contrast with LLMs' pretraining data where certain populations are over- or underrepresented, (3) Our training objective explicitly aligns model predictions with response distributions from each subpopulation, enabling LLMs to capture variance within human subpopulations.</p>
<p>Training on public opinion survey data has remained under-explored due to the limited availability of structured survey datasets. To this end, we curate and release SubPOP (Subpopulation-level Public Opinion Prediction), a dataset of 70K subpopulation-response distribution pairs ( $6.5 \times$ larger compared to previous datasets). We show that fine-tuning LLMs on SubPOP significantly improves the distributional match between LLM generated and human responses, and improvements
are consistent across subpopulations of varying sizes. Additionally, the improvement generalizes to unseen subpopulations, survey waves (topics), and survey families, i.e. surveys administered by different institutions. Such broad generalization is particularly critical for real-world public opinions research, where practitioners are most in need of synthetic data for survey questions or subpopulations (or both) that they have not tested before.</p>
<p>Our contributions are summarized as follows:</p>
<ul>
<li>We show that training LLMs on response distributions from survey data significantly improves their ability to predict the opinions of subpopulations, reducing the Wasserstein distance between LLM and human distributions by 32-46\% compared to top-performing baselines. (Section 4.2)</li>
<li>We show that the performance of the fine-tuned LLMs strongly generalizes to out-of-distribution data, including unseen subpopulations, new survey waves, and different survey families. (Section 4.2 and Section 4.3)</li>
<li>We release SubPOP, a curated and pre-processed dataset of public opinion survey results that is $6.5 \times$ larger than existing datasets, enabling fine-tuning at scale.</li>
</ul>
<h2>2 Related Work</h2>
<p>Public opinion datasets. Several research institutions conduct large-scale public opinion polls and release data from those surveys. Important examples include Pew Research Center's American Trends Panel (ATP), which consists of multiple waves of cross-sectional surveys on different topics (Pew Research Center, 2018), and General Social Survey (GSS) from the NORC at the University of Chicago (Davern et al., 2024). Existing datasets have curated such data for evaluating LLM-based opinion predictions, including OpinionQA (Santurkar et al., 2023), a subset of ATP survey waves containing about 500 questions on contentious social topics. While OpinionQA is widely used in prior work (He et al., 2024; Zhao et al., 2023; Li et al., 2023, 2024), we find its total number of questions limited in scale for fine-tuning LLMs and instead use this dataset for evaluation. We further collect an extended set of survey data from ATP waves not included in OpinionQA, as well as from GSS to curate SubPOP. Other datasets, such as GlobalOpinionQA (Durmus et al., 2023)derived from the World Values Survey (WVS)</p>
<p>(World Values Survey, 2022) and the Pew Global Attitudes Survey (Pew Research Center, 2024)—and the PRISM dataset (Kirk et al., 2024) investigates how language models align with opinions from populations across the globe and different cultures.</p>
<p>Predicting human opinions with LLMs. Prior work has explored various prompt engineering approaches for steering LLM responses: earlier work use rule-based prompts that incorporate demographic profiles of individuals or populations, or few-shot examples of survey question-response (Hwang et al., 2023; Simmons, 2022; Santurkar et al., 2023; Dominguez-Olmedo et al., 2023). Recent work explore prompting LLMs with open-ended text, including interview transcripts (Park et al., 2024a), personal narratives (Moon et al., 2024), or LLM-refined prompts (Kim and Yang, 2024; Sun et al., 2024). Our fine-tuning approach is complementary to prompt engineering methods: while prompt engineering seeks to optimize what information is provided to the LLM (while the model is frozen), fine-tuning seeks to optimize how the model utilizes the provided information (while the prompt is frozen). In this work, we demonstrate that our fine-tuned models exhibit significant improvements in matching the response distributions of humans without requiring elaborate prompt engineering methods.</p>
<p>Other work (Chu et al., 2023; He et al., 2024; Feng et al., 2024) fine-tune language models on text corpora from specific communities (e.g., Reddit) to infer the most popular response or response distribution for a given survey question. While this approach benefits from large-scale and continuously updated text corpora, it struggles with disproportionate representation online and lacks comprehensive coverage of diverse subpopulations. A few works have explored directly fine-tuning on public opinion survey data, but in different problem settings from ours. Li et al. (2023) apply collaborative filtering to individual-level responses to learn embeddings for individuals, and Zhao et al. (2023) develop a meta-learning framework to predict the opinions of new groups given a small number of in-context examples for that group. In contrast, our approach does not require individual-level responses and can generalize to unseen groups and survey questions without any responses.</p>
<p>A recent work (Li et al., 2024) and a work concurrent to ours (Cao et al., 2025) also explored fine-tuning LLMs on the World Values Survey
(WVS) to align the LLM's opinion response with a culture or entire country populations. In comparison, our work focuses on US surveys, testing whether LLMs can align with finer-grained subpopulations within one country and whether LLMs fine-tuned on one US-representative survey can generalize to another. However, we note that our proposed method for fine-tuning language models applies to any survey dataset with distributional information about subpopulation responses.</p>
<p>Pluralistic alignment of LLMs. Recent literature on pluralistic and distributional alignment target a similar yet different problem in fine-tuning LLMs (Chakraborty et al., 2024; Melnyk et al., 2024; Poddar et al., 2024; Siththaranjan et al., 2023; Yao et al., 2024; Sorensen et al., 2024; Lake et al., 2024; Chen et al., 2024; Jiang et al., 2024). While this line of work shares a similar goal as ours in training models to reflect on opinions (and preferences) of diverse subpopulations, most work differ from ours in that they operate in the context of training against pair-wise preference orderings between alternative language model completions, extending the Bradley-Terry-Luce model (Rajkumar and Agarwal, 2014; Ouyang et al., 2022; Rafailov et al., 2024) or investigating alternative models to account for diverging preference orderings across populations. In contrast, our work trains the model to directly predict the opinion distributions of human subpopulations, where accurately matching distributions across a large variety of subpopulations is of paramount interest. Our work additionally focuses on the particular context of estimating human opinions about societal issues-the objective of public opinion research-which enables relatively straightforward supervised training on openly available, structured survey data as presented by SubPOP.</p>
<h2>3 Methods</h2>
<h3>3.1 Fine-tuning</h3>
<h2>LLMs on Human Response Distributions</h2>
<p>Our goal is to fine-tune an LLM to predict the distribution of responses for a multiple-choice question, conditioned on descriptions of a human subpopulation we want to simulate, typically a specific demographic, socioeconomic, or ideological subgroup. Consider the example in Figure 2: the question asks, "What do you think the chances are these days that a woman won't get a job or promotion while an equally or less qualified man gets one instead?" The</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Proposed supervised fine-tuning setup with a survey response dataset such as SubPOP. Survey data is 3-tuple of a survey question, target subpopulation information, and the observed human opinion distribution (<em>i.e.</em> how subjects in the group responded to the given question). The training objective, $\mathcal{L}(\theta)$, is a forward KL divergence loss on language model predicted distribution of question option likelihoods; our loss guides the model predictions to match the response distribution of the specified human subpopulation.</p>
<p>available responses are: <em>A. Very likely, B. Somewhat likely, C. Not very likely, D. Very unlikely, and E. Refused</em>. In this case, the LLM will output a probability for each of the tokens corresponding to the choices A through E, thereby generating a complete response distribution that we aim to align with the true distribution observed in survey data.</p>
<p>Formally, let $q \in Q$ be a question, $g \in G$ be a subpopulation, and $\mathcal{A}<em>q$ be the set of possible choices for question $q$. An LLM with parameters $\theta$ produces a conditional probability distribution $p</em>\theta(\mathcal{A}<em>q \mid q, g)$. We fine-tune this model so that its predicted distribution for each $(q, g)$ mirrors the human response distribution $p_H(\mathcal{A}_q \mid q, g)$ collected from real survey data. To accomplish this, we apply LoRA fine-tuning (Hu et al., 2021) and use the forward Kullback–Leibler (KL) divergence as our loss. Concretely, if $p_H(\mathcal{A}_q \mid q, g)$ represents the group-level empirical distribution of human opinions and $p</em>\theta(\mathcal{A}_q \mid q, g)$ represents the model's predicted distribution, our training objective is:</p>
<p>$$
\mathcal{L}(\theta) = \mathbb{E}<em _text_KL="\text{KL">{q, g} \left[ D</em>}} \left( p_H(\mathcal{A<em>q \mid q, g) | p</em>\theta(\mathcal{A}_q \mid q, g) \right) \right],
$$</p>
<p>where $D_{\text{KL}}$ denotes the KL divergence. In the example shown in Figure 2, the model is trained to reduce the KL divergence between the target (survey-based) distribution over ${A, B, C, D, E}$ and its predicted distribution for the subpopulation living in the Southern United States.</p>
<p>We choose forward KL (i.e., $\text{KL}(p_H \mid p_\theta)$) since it is sensitive to cases where $p_H$ assigns high probability but $p_\theta$ does not, naturally encouraging the model to <em>cover</em> the real distribution. This property aligns with standard maximum-likelihood training, where the model is penalized for underestimating any response that is frequent in the data. In other words, if many participants in group $g$ choose option "A" for question $q$, then the model probability on "A" should be correspondingly high.</p>
<p>Instead of explicitly modeling the group response distribution as $p_H(\mathcal{A}_q | q, g)$, one could do two alternatives. (1) One-hot encoding: this approach (Li et al., 2024) approximates the distribution by a one-hot vector, assigning a value of one to the most probable option and zero elsewhere. (2) Data augmentation by response frequency: this approach (Zhao et al., 2023) expands the dataset by replicating question-choice pairs in proportion to their observed frequency. We adopt the explicit distribution modeling in our main experiments because it directly encodes the distributional information without requiring discrete sampling or replicating data points. A detailed comparison of these approaches is provided in Section C.1.</p>
<h3>3.2 SubPOP: a Comprehensive Survey Dataset to Fine-tune and Evaluate LLMs</h3>
<p>OpinionQA (Santurkar et al., 2023) is a widely used dataset for fine-tuning and evaluating large language models (LLMs) on opinion prediction, containing roughly 500 questions drawn from 14 American Trends Panel (ATP) waves (Pew Research Center, 2018). Although valuable, it faces two important limitations: (1) Limited thematic diversity—for instance, wave 26 focuses on the topic of firearms. (2) Reliance on a single survey family (ATP), which risks overfitting to a particular style of questions and limits out-of-distribution evaluation on other sources (e.g., GSS).</p>
<p>To address these limitations, we introduce a new dataset, SubPOP, that broadens both the thematic and institutional scope of opinion prediction data. For training, SubPOP comprises 3,229 multiple-choice questions drawn from ATP waves 61–132,</p>
<p>excluding waves included in OpinionQA. In Table 4, we list the topics of the ATP waves in SubPOP vs. OpinionQA, both showing the increased thematic diversity of SubPOP (with over 20 new topics) and the remaining unseen topics in OpinionQA that allow us to test whether LLMs fine-tuned on SubPOP can generalize to unseen topics.</p>
<p>For evaluation, SubPOP also includes 133 multiple-choice questions from the General Social Survey (GSS) (Davern et al., 2024), serving as an out-of-distribution benchmark. This expanded collection not only broadens the range of topics beyond OpinionQA's initial 500 questions, but also enables evaluation on surveys created and administered by different institutions (Pew Research Center vs. NORC-Chicago). Dataset curation and refinement pipeline is available in Appendix A.</p>
<h3>3.3 Evaluation Metric</h3>
<p>We use Wasserstein distance (WD) to quantify how closely the model's predicted opinion distribution matches human survey data (Santurkar et al., 2023; Moon et al., 2024; Meister et al., 2024; Zhao et al., 2023). Formally, for a group $g$ representing some subpopulation and a question $q$ WD is defined as $\mathcal{W D}<em H="H">{\theta}(q, g)=\mathcal{W D}\left(p</em>}\left(\mathcal{A<em _theta="\theta">{q} \mid q, g\right), p</em> \mid q, g\right)\right)$ (see formula in Appendix B). Since WD is computed over ordinal values, we map the categorical answer options to numbers, such as mapping "Very likely" to 1 , "Likely" to 2 , and so on.}\left(\mathcal{A}_{q</p>
<p>Some prior work utilizes one-hot accuracy (Feng et al., 2024; Li et al., 2023) as an evaluation metric. However, one-hot accuracy only verifies whether the top-predicted choice matches the top human response, thereby discarding distributional information. In contrast, WD accounts for partial overlaps among the categories and reflects the 'cost' of shifting probability mass, providing a more nuanced assessment of distribution discrepancy. Consider the example question provided in Figure 2, where the human response distribution indicates that option B ("Somewhat likely") is the most probable. Now consider two cases in which the model incorrectly predicts the top choice. In the first case, the model assigns a high probability to option A ("Very likely"), while in the second case, it assigns a high probability to option D ("Very unlikely"). Although one-hot accuracy would treat both predictions equally as errors, WD differentiates between them by accounting for the ordinal relationship among the
options, penalizing the second prediction more heavily for its larger deviation from the true distribution.</p>
<h2>4 Experiments</h2>
<h3>4.1 Bounds of WD and Baselines</h3>
<p>In this section, we describe the lower/upper bounds and two baseline methods against which we compare our method.</p>
<p>Lower and upper bounds. We use a uniform distribution over all available choices to establish an upper bound of the WD between a predicted and the target response distribution. To compute a lower bound, we sample a group of human respondents from the original human respondents to calculate the WD between the two, and perform bootstrapping to obtain a robust estimate. This lower bound captures the intrinsic variance arising from the respondent sampling process in opinion surveys.</p>
<p>Baselines. We compare our approach with two baseline methods: prompting and Modular Pluralism (Feng et al., 2024). For prompting, we consider both zero-shot and few-shot methods. In zero-shot prompting, we steer the LLM using demographic prompt formats. Specifically, we employ three different formats following Santurkar et al. (2023): QA, BIO, and PORTRAY. For instance, to condition the LLM to a person living in the South of the US, the QA format uses a question-answer format as illustrated in Figure 2; the BIO format conditions the model with a first-person narrative such as "I currently reside in the South."; and the PORTRAY format uses a third-person narrative like "Answer the following question as if you currently reside in the South.".</p>
<p>Few-shot prompting augments the prompt with a few examples of question-response distribution pairs alongside the demographic label (Hwang et al., 2023). In particular, we select the top five few-shot examples from the SubPOP training set based on cosine similarity computed by the embedding model. In our experiments, we represent the response distribution in JSON format and require the model to output its prediction in the same JSON format, following the approach in Meister et al. (2024).</p>
<p>Modular pluralism (Feng et al., 2024) fine-tunes multiple LLMs on distinct datasets to capture the viewpoints of different communities (Feng et al., 2023). For a given question, each fine-tuned LLM generates an opinion that reflects the perspective</p>
<p>Table 1: Evaluation on OpinionQA and the SubPOP evaluation set (SubPOP-Eval) for 22 subpopulations following (Santurkar et al., 2023). We compute the WD by averaging over all questions and subpopulations. Lower and upper bounds of performance give guidance on how each method performs. For Modular Pluralism, we provide an error rate of one-hot prediction ( $\dagger$ ) (Section 3.3) which was used in the original paper.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>OpinionQA</th>
<th></th>
<th></th>
<th></th>
<th>SubPOP-Eval</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Llama-2-7B</td>
<td>Llama-2-13B</td>
<td>Mistral-7B</td>
<td>Llama-3-70B</td>
<td>Llama-2-7B</td>
<td>Llama-2-13B</td>
<td>Mistral-7B</td>
<td>Llama-3-70B</td>
</tr>
<tr>
<td>Upper bound (UniC)</td>
<td>0.178</td>
<td></td>
<td></td>
<td></td>
<td>0.208</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Lower bound (Human)</td>
<td>0.031</td>
<td></td>
<td></td>
<td></td>
<td>0.033</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Zero-shot prompt (Q4)</td>
<td>0.173</td>
<td>0.170</td>
<td>0.153</td>
<td>0.138</td>
<td>0.206</td>
<td>0.196</td>
<td>0.187</td>
<td>0.160</td>
</tr>
<tr>
<td>Zero-shot prompt (B10)</td>
<td>0.193</td>
<td>0.183</td>
<td>0.162</td>
<td>0.143</td>
<td>0.221</td>
<td>0.212</td>
<td>0.202</td>
<td>0.175</td>
</tr>
<tr>
<td>Zero-shot prompt (PORTRAY)</td>
<td>0.195</td>
<td>0.207</td>
<td>0.158</td>
<td>0.209</td>
<td>0.212</td>
<td>0.242</td>
<td>0.194</td>
<td>0.247</td>
</tr>
<tr>
<td>Few-shot prompt</td>
<td>0.186</td>
<td>0.175</td>
<td>0.174</td>
<td>0.166</td>
<td>0.217</td>
<td>0.194</td>
<td>0.175</td>
<td>0.182</td>
</tr>
<tr>
<td>Modular Pluralism</td>
<td>0.285 ( ${ }^{\dagger} 55.6 \%$ )</td>
<td></td>
<td></td>
<td></td>
<td>0.279 ( ${ }^{\dagger} 55.2 \%$ )</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Ours (SubPOP-FT)</td>
<td>0.106</td>
<td>0.102</td>
<td>0.096</td>
<td>0.094</td>
<td>0.121</td>
<td>0.113</td>
<td>0.115</td>
<td>0.096</td>
</tr>
</tbody>
</table>
<p>of the community it represents, and a separate black-box LLM aggregates these outputs to produce the final distributional response. Detailed implementation of the lower/upper bounds and the baselines is provided in Appendix D.</p>
<h3>4.2 Generalization to Unseen Topics and Survey Families</h3>
<p>In this section, we assess the ability of our finetuned LLMs to generalize to unseen data-both in terms of new topics and entirely different survey families. To evaluate these aspects, we use OpinionQA to measure generalization to unseen topics, and SubPOP-Eval to test generalization to a different survey family. We fine-tune four LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B, and Llama-3-70B) on SubPOP-Train. We opt for pretrained LLMs rather than instruction-following models, as previous work has shown that pretrained models perform better on this task (Moon et al., 2024). A detailed comparison between these model types is provided in Appendix C.2.</p>
<p>Summary of results. Table 1 reports the average WD metrics computed over all demographic groups and survey questions, comparing our fine-tuned models against various baseline approaches. Our experiments show that fine-tuning on SubPOP-Train significantly outperforms all other methods, yielding a $32-46 \%$ reduction in WD on OpinionQA and a $39-42 \%$ reduction on SubPOP-Eval compared to the best baselines. Notably, SubPOP-Train is based on ATP data, while SubPOP-Eval is derived from GSS surveys-two distinct survey families that can differ in respondent pools, calibration techniques, and other methodological factors, leading to non-trivial distribution shifts despite both being representative of the US population. Furthermore, our fine-grained analyses at the wave level (see Appendix E) confirm that these trends persist even at more detailed levels of evaluation.</p>
<p>Comparison to zero- and few-shot prompting. We first compare the performance of prompting methods with our approach. Zero-shot prompting results in only modest WD improvements over the upper bound, with the largest gain observed for Llama-3-70B and negligible improvements for Llama-27B. Even when using few-shot prompting-where five example question-response distribution pairs are provided-the performance gains remain minimal. This may be partly due to an under-optimized prompt format (e.g. requiring JSON output) and the inherent sensitivity of language models to prompt formatting (Sclar et al., 2023; Anagnostidis and Bulian, 2024). These findings underscore the need for methods, such as fine-tuning, that enable relatively reliable predictions of opinion distributions.</p>
<p>Comparison to Modular Pluralism. Modular Pluralism improves one-hot accuracy, reducing prediction error from $72.7 \%$ (zero-shot prompting) to $55.6 \%$ on OpinionQA, but underperforms in matching the full distribution of option choices, measured as WD. This discrepancy in performance highlights the limitations of methods that train LLMs to identify only the most probable response rather than modeling the entire distribution of responses. Opinions are inherently distributed: even within a particular subpopulation such as a single demographic subgroup, distribution of opinions cannot be captured as a single most likely response. Moreover, instruction-tuned models that serve as a black-box LLM tend to assign high probabilities on only specific tokens (Lin et al., 2022; Kadavath et al., 2022; Achiam et al., 2023), further pushing the generated distribution away from the human distribution.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Per-group evaluation performance of our model Llama-2-7B-SubPOP-FT (red lines) on OpinionQA. For comparison, the results from zero-shot QA prompting (black lines) and the lower bound (blue lines) are presented. We observe that the relative improvement, measuring how much of the gap between zero-shot prompting and the lower bound has been closed, remains consistent across subpopulations. Shaded blue regions represent the 95% confidence interval of the lower-bound estimation for each group. Per-group results for other models (Table 9) and the results on SubPOP evaluation set (Table 10) are available in Appendix E.</p>
<h3>4.3 Generalization across Target Subpopulations</h3>
<p>Here we report two key observations: (1) prediction performance improves consistently across most subpopulations represented in the fine-tuning data, and (2) the LLMs fine-tuned on SubPOP-Train generalize well to subpopulations that were not included during fine-tuning.</p>
<p><strong>Consistent performance improvements over subpopulations.</strong> Figure 3 shows the per-group WD on the OpinionQA evaluation for Llama-2-7B, comparing our fine-tuning approach with zero-shot prompting and the empirical WD lower bound. To evaluate the consistency of performance gains, we calculate the <em>relative improvement</em> for each subpopulation as how much of the gap between zero-shot prompting and the empirical lower bound is reduced after fine-tuning. This measure allows us to account for varying lower bounds across subpopulations: since some groups have fewer respondents, there is greater uncertainty in their reported distribution in the survey data and greater variance between the original sample and bootstrap samples.</p>
<p>All 22 subpopulations demonstrate a large relative improvement after fine-tuning, ranging from 38%–54%. The average relative improvement is 46.7% with a standard deviation of 4.4%. This consistency confirms that our fine-tuning approach delivers balanced performance gains without disproportionately favoring any particular demographic subgroup. We hypothesize that the consistent gains over groups largely stem from our dataset design, which allocates an equal number of training samples to each group. By ensuring uniformly distributed data points across subpopulations, the model captures sufficient subgroup-specific signals, ultimately leading to consistent performance improvements.</p>
<p><strong>Generalization to unseen subpopulations.</strong> We further investigate how models fine-tuned with our approach and SubPOP might show generalization to subpopulations that were not represented in the training data, a circumstance that may arise in real-world survey development. For the evaluation, we benchmark our methods against a zero-shot prompting baseline. Specifically, we evaluate our model, which is fine-tuned on 22 subpopulations provided in SubPOP-Train, on a set of subpopulations in OpinionQA that were not included in fine-tuning. This experiment not only checks generalization to unseen subpopulations, but also involves unseen survey questions, providing a robust assessment of the model capability for generalization to out-of-distribution data.</p>
<p>As shown in Table 2, our model achieves a strong reduction in WD even for unseen subpopulations, indicating that the model can be steered by demographic prompts beyond the seen subpopulations in training. Interestingly, although SubPOP-Train does not contain any data with opinion distributions of particular age groups (<em>e.g.</em> subjects of age 18-29 or those of age 65+), the average relative improvement is 44.7%, which is compatible with the average</p>
<p>Table 2: Per-group evaluation performance of Llama-2-7B-SubPOP-FT (Ours) on OpinionQA. We report the lower bound, WD for zero-shot prompting, WD for Llama-2-7B-SubPOPFT, and the relative improvement. Rows highlighted in blue represent subpopulations included during fine-tuning, while uncolored rows correspond to subpopulations that were unseen during fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Group</th>
<th style="text-align: left;">Lower <br> Bound</th>
<th style="text-align: left;">Zero <br> Shot</th>
<th style="text-align: left;">Ours</th>
<th style="text-align: center;">Relative <br> Improvement (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Age: 18-29</td>
<td style="text-align: left;">0.023</td>
<td style="text-align: left;">0.185</td>
<td style="text-align: left;">0.096</td>
<td style="text-align: center;">54.9</td>
</tr>
<tr>
<td style="text-align: left;">Age: 30-49</td>
<td style="text-align: left;">0.014</td>
<td style="text-align: left;">0.151</td>
<td style="text-align: left;">0.093</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: left;">Age: 50-64</td>
<td style="text-align: left;">0.014</td>
<td style="text-align: left;">0.154</td>
<td style="text-align: left;">0.101</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: left;">Age: 65+</td>
<td style="text-align: left;">0.013</td>
<td style="text-align: left;">0.195</td>
<td style="text-align: left;">0.113</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: left;">Less than high school</td>
<td style="text-align: left;">0.043</td>
<td style="text-align: left;">0.161</td>
<td style="text-align: left;">0.101</td>
<td style="text-align: center;">50.8</td>
</tr>
<tr>
<td style="text-align: left;">High school graduate</td>
<td style="text-align: left;">0.017</td>
<td style="text-align: left;">0.144</td>
<td style="text-align: left;">0.092</td>
<td style="text-align: center;">40.9</td>
</tr>
<tr>
<td style="text-align: left;">Some college, no degree</td>
<td style="text-align: left;">0.018</td>
<td style="text-align: left;">0.144</td>
<td style="text-align: left;">0.093</td>
<td style="text-align: center;">40.5</td>
</tr>
<tr>
<td style="text-align: left;">Associate's degree</td>
<td style="text-align: left;">0.026</td>
<td style="text-align: left;">0.159</td>
<td style="text-align: left;">0.098</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: left;">College grad</td>
<td style="text-align: left;">0.018</td>
<td style="text-align: left;">0.165</td>
<td style="text-align: left;">0.099</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: left;">Postgraduate</td>
<td style="text-align: left;">0.015</td>
<td style="text-align: left;">0.174</td>
<td style="text-align: left;">0.106</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr>
<td style="text-align: left;">Very conservative</td>
<td style="text-align: left;">0.026</td>
<td style="text-align: left;">0.208</td>
<td style="text-align: left;">0.107</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: left;">Conservative</td>
<td style="text-align: left;">0.021</td>
<td style="text-align: left;">0.169</td>
<td style="text-align: left;">0.103</td>
<td style="text-align: center;">44.6</td>
</tr>
<tr>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">0.016</td>
<td style="text-align: left;">0.151</td>
<td style="text-align: left;">0.094</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: left;">Liberal</td>
<td style="text-align: left;">0.022</td>
<td style="text-align: left;">0.192</td>
<td style="text-align: left;">0.100</td>
<td style="text-align: center;">54.1</td>
</tr>
<tr>
<td style="text-align: left;">Very liberal</td>
<td style="text-align: left;">0.025</td>
<td style="text-align: left;">0.202</td>
<td style="text-align: left;">0.111</td>
<td style="text-align: center;">51.4</td>
</tr>
<tr>
<td style="text-align: left;">Democrat</td>
<td style="text-align: left;">0.016</td>
<td style="text-align: left;">0.172</td>
<td style="text-align: left;">0.099</td>
<td style="text-align: center;">47.1</td>
</tr>
<tr>
<td style="text-align: left;">Republican</td>
<td style="text-align: left;">0.019</td>
<td style="text-align: left;">0.196</td>
<td style="text-align: left;">0.105</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: left;">Independent</td>
<td style="text-align: left;">0.016</td>
<td style="text-align: left;">0.155</td>
<td style="text-align: left;">0.093</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: left;">Something Else</td>
<td style="text-align: left;">0.026</td>
<td style="text-align: left;">0.162</td>
<td style="text-align: left;">0.092</td>
<td style="text-align: center;">51.0</td>
</tr>
</tbody>
</table>
<p>relative improvement for seen subpopulations. We provide results for other unseen groups in Table 7 of Appendix C. 3 (average relative improvement of $43.1 \%$ with a standard deviation of $6.7 \%$ ).</p>
<p>Steerability towards subpopulations. Given the large improvements in WD across subpopulations after fine-tuning, we want to test whether the LLM is truly adapting its predictions based on the subpopulation specified in its prompt (i.e. the LLM is being steered) or if the improvements can be explained by the LLMs' predictions getting closer to human responses in general, without any subpopulationspecific adaptation. If the LLM is being steered, we should expect that the LLM's predictions for a target subpopulation $g_{t}$ are closer to the human distribution for $g_{t}$ when $g_{t}$ is the subpopulation specified in the prompt, compared to when another group $g_{s}$ is specified in the prompt. We should also expect the gap in WD to be larger if the distance between the true human distributions for $g_{t}$ and $g_{s}$ are larger, such as differences between the youngest and oldest age groups compared to adjacent groups.</p>
<p>Formally, we define the intergroup disagreement between a target group $g_{t}$ and a source group $g_{s}$ as $\mathcal{W D}\left(p_{H}\left(\mathcal{A}<em t="t">{q} \mid q, g</em>}\right), p_{H}\left(\mathcal{A<em s="s">{q} \mid q, g</em>$ with the LLM-predicted distribution
when the source group $g_{s}$ is specified in the prompt, $\mathcal{W D}\left(p_{H}\left(\mathcal{A}}\right)\right)$ averaged over evaluation questions. In human responses (left of Figure 4), the disagreement shows the pattern of locality: increases as the disparity in education levels between two groups grows. We extend this notion to compare the human distribution from the target group $g_{t<em t="t">{q} \mid q, g</em>}\right), p_{\theta}\left(\mathcal{A<em s="s">{q} \mid q, g</em>\right)\right)$. If the model truly incorporates subpopulation information from the prompt, its intergroup disagreement pattern should mirror that of the human data.</p>
<p>Zero-shot prompting with the base model (right of Figure 4) does not exhibit the locality pattern seen in the human data, indicating that it cannot be steered by subpopulation labels. In contrast, the fine-tuned model (middle of Figure 4) reproduces a pattern resembling the human-human case, even though it was trained on only two education groups ("less than high school" and "college graduate/some postgrad") and the other four groups were unseen. This result demonstrates that our fine-tuned model not only learns to condition on subpopulation information but also generalizes to subpopulations unseen during fine-tuning. We provide the intergroup disagreement for other traits in Appendix C.3.</p>
<h3>4.4 Effect of Scaling the Dataset</h3>
<p>In this section, we examine performance scales with training dataset size. We randomly sample subsets containing $25 \%, 50 \%, 75 \%$, and $87.5 \%$ of the full SubPOP training set and evaluate three models-Llama-2-7B, Llama-2-13B, and Mistral-7B-on OpinionQA. As shown in Figure 5, we observe diminishing marginal returns, as is typical with fine-tuning; for example, after training on a random $25 \%$, the models reach $72 \%-78 \%$ of the total improvement they achieve after fine-tuning on all of SubPOP-train. However, the performance does not entirely plateau. Instead, it continues to improve as we further increase the training data from $25 \%$ to $100 \%$. We fit linear trend lines (dotted in Figure 5) to the results and observe that the slopes are similar for each model. This suggests that the rate of im-provement-reflected by the slope in the power-law relationship-is intrinsic to the data and task rather than to the specific model architecture. In other words, LLMs exhibit comparable data efficiency, with performance gains that are fundamentally tied to dataset size rather than model-specific factors.</p>
<p>Using these trend lines, we can estimate the amount of fine-tuning data required to reach a target performance. For instance, we estimate that fine-tuning Mistral-7B on a dataset 25 times larger than the current SubPOP training set would yield a WD value of 0.07 , which is much closer to the empirical lower bound of 0.031 reported in Table 1.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Intergroup disagreement pattern between groups of different education levels calculated with OpinionQA and Llama-2-7B as a base model. A target human group is compared to (left) a source human group, (middle) our fine-tuned model conditioned on a source group, (right) a base model conditioned on a source group. Bold-faced groups are included in the fine-tuning data SubPOP-Train, while the others aren't. In the human response (left), we observe a decreasing disagreement level as the education level becomes similar. This disagreement pattern exists in our fine-tuned model but not in the zero-shot prompting with a base model, indicating that our model can be steered to given subpopulation label even for unseen demographics while the base model cannot.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Evaluation results on OpinionQA after fine-tuning each LLM on increasingly large sampled subsets of SubPOP-Train. Both axes are presented in a log scale. The x-axis is the size of sampled dataset and the y-axis is WD against human responses measured on OpinionQA. Dashed lines represent a line of best fit. Performances at data percentage of 100% are identical to ours (SubPOP-FT) in Table 1.</p>
<p>This result underscores the critical importance of collecting more high-quality data, as increased dataset size can drive significant improvements in model performance.</p>
<h1>5 Conclusion</h1>
<p>In this work, we demonstrated that fine-tuning large language models on structured public opinion survey data markedly improves their ability to predict human response distributions. We curate SubPOP—a dataset 6.5× larger than previous collections to fine-tune and evaluate LLMs on survey response distribution prediction task. By fine-tuning on SubPOP, we showed that LLMs can capture the nuanced, group-specific variability in public opinions, while also generalizing to unseen subpopulations, survey waves and question topics, and different survey families. Fine-tuning achieves consistent improvements across subpopulations of varying sizes, and our experiments demonstrate that fine-tuned LLMs are indeed adapting their responses to the subpopulation specified in the prompt, even for subpopulations unseen during fine-tuning. Finally, our experiments also reveal that as the fine-tuning dataset grows, model performance continues to scale favorably, underscoring the value of our larger dataset.</p>
<p>Generalization is a critical capability for LLMs, if they are to be used to assist public opinion research, as researchers are most in need of accurate opinion predictions for questions or subpopulations whom they have not surveyed before. Our work, by greatly improving LLMs' ability to accurately predict opinions with fine-tuning and demonstrating strong generalization to out-of-distribution data, moves us closer towards the goal of leveraging LLMs for opinion prediction. However, many open questions remain: why is the model able to generalize well to unseen subpopulations and questions, and under what conditions might it fail to do so? How do we ensure that LLMs faithfully capture opinions along other dimensions that are not explored in this work, such as intersections of demographic identities or changing opinions over time? How should LLMs be integrated into survey designs, to serve as tools that can complement large-scale surveys with human participants? Answering these questions will require interdisciplinary collaborations with</p>
<p>domain experts and critical assessments of LLMs' and traditional survey methods' strengths and weaknesses, so that we can most effectively and responsibly combine them to better estimate public opinions and inform public policies.</p>
<h2>Acknowledgments</h2>
<p>We sincerely appreciate Prof. John Canny for constructive comments and guidance. We also thank Prof. Emma Pierson and Dr. Sehoon Kim for valuable discussions as well as Woosuk Kwon for his support with vLLM. J.S. and S.M. would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). S.M. is supported by BAIR-Google Commons and M.K. is supported by the Apple Ph.D. Fellowship in Integrated Systems. We gratefully acknowledge the generous support from Strong Compute for providing fully managed GPU for fine tuning through the Strong Compute Instant Super Computer system, including support from Adam Peaston.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Sotiris Anagnostidis and Jannis Bulian. 2024. How susceptible are llms to influence in prompts? arXiv preprint arXiv:2408.11865.</p>
<p>Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David Wingate. 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337-351.</p>
<p>Ashwini Ashokkumar, Luke Hewitt, Isaias Ghezae, and Robb Willer. 2024. Predicting results of social science experiments using large language models. accessed September, 19:2024.</p>
<p>Christopher A Bail. 2024. Can generative ai improve social science? Proceedings of the National Academy of Sciences, 121(21):e2314021121.</p>
<p>Jelke Bethlehem. 2010. Selection bias in web surveys. International statistical review, 78(2):161-188.</p>
<p>Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Röttger, and Daniel Hershcovich. 2025. Specializing large language models to simulate survey response distributions for global populations. arXiv preprint arXiv:2502.07068.</p>
<p>Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Dinesh Manocha, Furong Huang, Amrit Bedi, and Mengdi Wang. 2024. Maxmin-rlhf: Alignment with diverse human preferences. In Forty-first International Conference on Machine Learning.</p>
<p>Daiwei Chen, Yi Chen, Aniket Rege, and Ramya Korlakai Vinayak. 2024. Pal: Pluralistic alignment framework for learning from heterogeneous preferences. arXiv preprint arXiv:2406.08469.</p>
<p>Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023a. Marked personas: Using natural language prompts to measure stereotypes in language models. arXiv preprint arXiv:2305.18189.</p>
<p>Myra Cheng, Tiziano Piccardi, and Diyi Yang. 2023b. Compost: Characterizing and evaluating caricature in llm simulations. arXiv preprint arXiv:2310.11501.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937.</p>
<p>Bernard CK Choi and Anita WP Pak. 2004. A catalog of biases in questionnaires. Preventing chronic disease, 2(1):A13.</p>
<p>Eric Chu, Jacob Andreas, Stephen Ansolabehere, and Deb Roy. 2023. Language models trained on media diets can predict public opinion. arXiv preprint arXiv:2303.16779.</p>
<p>Michael Davern, Rene Bautista, Jeremy Freese, Pamela Herd, and Stephen L. Morgan. 2024. General social survey 1972-2024. Principal Investigator: Michael Davern; Co-Principal Investigators: Rene Bautista, Jeremy Freese, Pamela Herd, and Stephen L. Morgan. Sponsored by National Science Foundation. NORC ed. Chicago: NORC, 2024.</p>
<p>Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335.</p>
<p>Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. 2023. Questioning the survey responses of large language models. arXiv preprint arXiv:2306.07951.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.</p>
<p>Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. 2023. Towards measuring the representation of subjective global opinions in language models. arXiv preprint arXiv:2306.16388.</p>
<p>Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair nlp models. arXiv preprint arXiv:2305.08283.</p>
<p>Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, and Yulia Tsvetkov. 2024. Modular pluralism: Pluralistic alignment via multi-LLM collaboration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 4151-4171, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2024. Bias and fairness in large language models: A survey. Computational Linguistics, pages 1-79.</p>
<p>Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large language models in generating synthetic hci research data: a case study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-19.</p>
<p>Zihao He, Minh Duc Chu, Rebecca Dorn, Siyi Guo, and Kristina Lerman. 2024. Community-cross-instruct: Unsupervised instruction generation for aligning large language models to online communities. arXiv preprint arXiv:2406.12074.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>EunJeong Hwang, Bodhisattwa Prasad Majumder, and Niket Tandon. 2023. Aligning language models to user opinions. arXiv preprint arXiv:2305.14929.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Liwei Jiang, Taylor Sorensen, Sydney Levine, and Yejin Choi. 2024. Can language models reason about individualistic human values and preferences? arXiv preprint arXiv:2410.03868.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Shivani Kapania, William Agnew, Motahhare Eslami, Hoda Heidari, and Sarah Fox. 2024. 'simulacrum of stories': Examining large language models as qualitative research participants. arXiv preprint arXiv:2409.19430.</p>
<p>Jaehyung Kim and Yiming Yang. 2024. Few-shot personalization of llms with mis-aligned responses. arXiv preprint arXiv:2406.18678.</p>
<p>Junsol Kim and Byungkyu Lee. 2023. Ai-augmented surveys: Leveraging large language models and surveys for opinion prediction. arXiv preprint arXiv:2305.09620.</p>
<p>Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, et al. 2024. The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. arXiv preprint arXiv:2404.16019.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.</p>
<p>Thom Lake, Eunsol Choi, and Greg Durrett. 2024. From distributional to overton pluralism: Investigating large language model alignment. arXiv preprint arXiv:2406.17692.</p>
<p>Cheng Li, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. 2024. Culturellm: Incorporating cultural differences into large language models. arXiv preprint arXiv:2402.10946.</p>
<p>Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang, Aram Galstyan, Richard Zemel, and Rahul Gupta. 2023. On the steerability of large language models toward data-driven personas. arXiv preprint arXiv:2311.04978.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334.</p>
<p>I Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.</p>
<p>Benjamin S Manning, Kehang Zhu, and John J Horton. 2024. Automated social science: Language models as scientist and subjects. Technical report, National Bureau of Economic Research.</p>
<p>Nicole Meister, Carlos Guestrin, and Tatsunori Hashimoto. 2024. Benchmarking distributional alignment of large language models. arXiv preprint arXiv:2411.05403.</p>
<p>Igor Melnyk, Youssef Mroueh, Brian Belgodere, Mattia Rigotti, Apoorva Nitsure, Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, and Jerret Ross. 2024. Distributional preference alignment of llms via optimal transport. arXiv preprint arXiv:2406.05882.</p>
<p>Andrew Mercer, Arnold Lau, and Courtney Kennedy. 2018. For weighting online opt-in samples, what matters most?</p>
<p>Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, and David Chan. 2024. Virtual personas for language models via an anthology of backstories. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 19864-19897, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744.</p>
<p>Joon Sung Park, Carolyn Q Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, and Michael S Bernstein. 2024a. Generative agent simulations of 1,000 people. arXiv preprint arXiv:2411.10109.</p>
<p>Peter S Park, Philipp Schoenegger, and Chongyang Zhu. 2024b. Diminished diversity-of-thought in a standard large language model. Behavior Research Methods, pages $1-17$.</p>
<p>Pew Research Center. 2018. America trends panel waves. Retrieved February 06, 2025, from https://www.pewsocialtrends.org/dataset.</p>
<p>Pew Research Center. 2024. Pew research center. Accessed: February 10, 2025.</p>
<p>Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. 2024. Personalizing reinforcement learning from human feedback with variational preference learning. arXiv preprint arXiv:2408.10075.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.</p>
<p>Arun Rajkumar and Shivani Agarwal. 2014. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In International conference on machine learning, pages 118-126. PMLR.</p>
<p>David M. Rothschild, James Brand, Hope Schroeder, and Jenny Wang. 2024. Opportunities and risks of llms in survey research. Available on SSRN: http://dx.doi.org/10.2139/ssrn.5001645.</p>
<p>Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models reflect? In International Conference on Machine Learning, pages 29971-30004. PMLR.</p>
<p>Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324.</p>
<p>Gabriel Simmons. 2022. Moral mimicry: Large language models produce moral rationalizations tailored to political identity. arXiv preprint arXiv:2209.12106.</p>
<p>Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. 2023. Distributional preference learning: Understanding and accounting for hidden context in rlhf. arXiv preprint arXiv:2312.08358.</p>
<p>Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, et al. 2024. A roadmap to pluralistic alignment. arXiv preprint arXiv:2402.05070.</p>
<p>Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R Fung, Hou Pong Chan, Kevin Small, ChengXiang Zhai, and Heng Ji. 2024. Persona-db: Efficient large language model personalization for response prediction with collaborative data refinement. arXiv preprint arXiv:2402.11060.</p>
<p>Angelina Wang, Jamie Morgenstern, and John P Dickerson. 2024. Large language models cannot replace human participants because they cannot portray identity groups. arXiv preprint arXiv:2402.01908.</p>
<p>World Values Survey. 2022. World Values Survey. [Online; accessed 02/15/2025].</p>
<p>Binwei Yao, Zefan Cai, Yun-Shiuan Chuang, Shanglin Yang, Ming Jiang, Diyi Yang, and Junjie Hu. 2024. No preference left behind: Group distributional preference optimization. arXiv preprint arXiv:2412.20299.</p>
<p>Siyan Zhao, John Dang, and Aditya Grover. 2023. Group preference optimization: Few-shot alignment of large language models. arXiv preprint arXiv:2310.11523.</p>
<h2>A Dataset Details</h2>
<h2>A. 1 American Trends Panel Datasets</h2>
<p>Pew Research holds regular American Trends Panel (ATP) survey (called waves) (Pew Research Center, 2018) covering various topics (e.g. veterans, political priorities, gender and leadership) and releases result at an individual level. For each anonymized individual, the following information is released: unique identification number, demographic details, survey responses, and weight. Weights (Mercer et al., 2018) are the output of post-survey calibration process that helps adjusting survey results for response bias (e.g., non-response bias, sampling bias) correction and population representativeness. As of January 2025, survey data until wave 132 has been released. About 20 surveys are conducted in each year.</p>
<h3>A. 2 OpinionQA</h3>
<p>OpinionQA is a subset of ATP curated in (Santurkar et al., 2023). This dataset consists of contentious 500 questions sampled from 14 ATP waves which have high intergroup disagreement (i.e. large Wasserstein distances among subpopulations' responses to a question). It also comes with hand-crafted ordinality information which provides structure to option lists. For example, options 'Major reason', 'Minor reason', and 'Not a reason', are assigned an ordinality mapping to 1,2 , and 3 , respectively. This ordinality allows a calculation of 1-dimensional Wasserstein distance.</p>
<p>Subpopulations we employ are listed in Table 3. This set of groups is adopted for several small-scale analysis (Santurkar et al., 2023; Zhao et al., 2023; Kim and Yang, 2024). We note that our approach is not limited to a specific number of groups and data is available for small or fine-grained demographic subpopulations.</p>
<p>Table 3: A list of 22 subpopulations used throughout our fine-tuning and analysis. We provide the number of respondents in each subpopulation in American Trends Panel Wave 82 for reference.</p>
<table>
<thead>
<tr>
<th>Trait</th>
<th>Groups</th>
<th>Population \% in Wave 82</th>
</tr>
</thead>
<tbody>
<tr>
<td>Region</td>
<td>Northeast</td>
<td>17.2</td>
</tr>
<tr>
<td></td>
<td>South</td>
<td>37.8</td>
</tr>
<tr>
<td>Education</td>
<td>College grad+</td>
<td>24.2</td>
</tr>
<tr>
<td></td>
<td>Less than high school</td>
<td>5.2</td>
</tr>
<tr>
<td>Gender</td>
<td>Male</td>
<td>44.3</td>
</tr>
<tr>
<td></td>
<td>Female</td>
<td>54.6</td>
</tr>
<tr>
<td>Race / ethnicity</td>
<td>Black</td>
<td>9.6</td>
</tr>
<tr>
<td></td>
<td>White</td>
<td>66.1</td>
</tr>
<tr>
<td></td>
<td>Asian</td>
<td>4.8</td>
</tr>
<tr>
<td></td>
<td>Hispanic</td>
<td>15.2</td>
</tr>
<tr>
<td>Income</td>
<td>\$100,000 or more</td>
<td>21.8</td>
</tr>
<tr>
<td></td>
<td>Less than \$30,000</td>
<td>21.3</td>
</tr>
<tr>
<td>Political Party</td>
<td>Democrat</td>
<td>35.1</td>
</tr>
<tr>
<td></td>
<td>Republican</td>
<td>29.1</td>
</tr>
<tr>
<td>Political Ideology</td>
<td>Liberal</td>
<td>20.0</td>
</tr>
<tr>
<td></td>
<td>Conservative</td>
<td>22.6</td>
</tr>
<tr>
<td></td>
<td>Moderate</td>
<td>38.3</td>
</tr>
<tr>
<td>Religion</td>
<td>Protestant</td>
<td>40.8</td>
</tr>
<tr>
<td></td>
<td>Jewish</td>
<td>2.0</td>
</tr>
<tr>
<td></td>
<td>Hindu</td>
<td>0.9</td>
</tr>
<tr>
<td></td>
<td>Atheist</td>
<td>0.6</td>
</tr>
<tr>
<td></td>
<td>Muslims</td>
<td>0.7</td>
</tr>
</tbody>
</table>
<p>Table 4: American Trends Panel (ATP) wave topics for waves included in SubPOP-Train (top) and OpinionQA (bottom). Golden rows represent wave topics in SubPOP-Train that are not present in OpinionQA, and blue rows represent wave topics in OpinionQA that are not present in SubPOP-Train. For waves 68-79, survey questions related to COVID-19 (e.g., contact tracing, vaccines, and relocation) were included as part of a survey along with the main survey topic.</p>
<table>
<thead>
<tr>
<th>Wave</th>
<th># questions</th>
<th>Wave Topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>68</td>
<td>90</td>
<td>American News Pathways, George Floyd, Black Lives Matter</td>
</tr>
<tr>
<td>69</td>
<td>92</td>
<td>Politics, 2020 Census</td>
</tr>
<tr>
<td>70</td>
<td>56</td>
<td>Religion in public life, social media's role in politics and society</td>
</tr>
<tr>
<td>71</td>
<td>84</td>
<td>Voter attitudes</td>
</tr>
<tr>
<td>72</td>
<td>18</td>
<td>New media</td>
</tr>
<tr>
<td>73</td>
<td>82</td>
<td>American News Pathways, social media</td>
</tr>
<tr>
<td>74</td>
<td>51</td>
<td>Online harassment, race relations</td>
</tr>
<tr>
<td>75</td>
<td>18</td>
<td>2020 pre-election survey</td>
</tr>
<tr>
<td>76</td>
<td>44</td>
<td>American News Pathways</td>
</tr>
<tr>
<td>77</td>
<td>13</td>
<td>Culture of work</td>
</tr>
<tr>
<td>78</td>
<td>57</td>
<td>2020 post-election survey</td>
</tr>
<tr>
<td>79</td>
<td>93</td>
<td>American News Pathways</td>
</tr>
<tr>
<td>80</td>
<td>45</td>
<td>Political priorities</td>
</tr>
<tr>
<td>81</td>
<td>52</td>
<td>Economics, pandemic financial outlook</td>
</tr>
<tr>
<td>83</td>
<td>54</td>
<td>Coronavirus vaccines and restrictions</td>
</tr>
<tr>
<td>84</td>
<td>50</td>
<td>Religion in politics and tolerance</td>
</tr>
<tr>
<td>85</td>
<td>93</td>
<td>News coverage of the Biden administration's first 100 days</td>
</tr>
<tr>
<td>87</td>
<td>90</td>
<td>Current political news and topics</td>
</tr>
<tr>
<td>88</td>
<td>37</td>
<td>Tech companies and policy issues</td>
</tr>
<tr>
<td>90</td>
<td>79</td>
<td>Twitter news attitudes</td>
</tr>
<tr>
<td>91</td>
<td>64</td>
<td>Benchmark study</td>
</tr>
<tr>
<td>93</td>
<td>19</td>
<td>Social media update</td>
</tr>
<tr>
<td>95</td>
<td>78</td>
<td>Politics timely and topical</td>
</tr>
<tr>
<td>96</td>
<td>57</td>
<td>Post-coronavirus pandemic spirituality</td>
</tr>
<tr>
<td>98</td>
<td>76</td>
<td>Coronavirus impacts on communities, living arrangements and life decisions</td>
</tr>
<tr>
<td>99</td>
<td>20</td>
<td>Artificial intelligence (AI) and human enhancement</td>
</tr>
<tr>
<td>103</td>
<td>12</td>
<td>Economic well-being</td>
</tr>
<tr>
<td>104</td>
<td>92</td>
<td>Politics, Religion in Public Life</td>
</tr>
<tr>
<td>105</td>
<td>38</td>
<td>Global Attitudes US Survey 2022</td>
</tr>
<tr>
<td>106</td>
<td>62</td>
<td>Religion and the environment</td>
</tr>
<tr>
<td>107</td>
<td>92</td>
<td>Government and Parties</td>
</tr>
<tr>
<td>108</td>
<td>83</td>
<td>COVID and Climate, Energy and the Environment</td>
</tr>
<tr>
<td>109</td>
<td>51</td>
<td>New Digital Platforms and Gender Identity</td>
</tr>
<tr>
<td>110</td>
<td>90</td>
<td>Politics timely and topical</td>
</tr>
<tr>
<td>111</td>
<td>23</td>
<td>Online dating and E-commerce</td>
</tr>
<tr>
<td>112</td>
<td>31</td>
<td>Social media update</td>
</tr>
<tr>
<td>113</td>
<td>53</td>
<td>2022 National Survey of Latinos (NSL)</td>
</tr>
<tr>
<td>114</td>
<td>93</td>
<td>Covid, scientists, and religion</td>
</tr>
<tr>
<td>115</td>
<td>63</td>
<td>Parents survey</td>
</tr>
<tr>
<td>116</td>
<td>75</td>
<td>Politics timely and topical</td>
</tr>
<tr>
<td>117</td>
<td>16</td>
<td>Religion and politics</td>
</tr>
<tr>
<td>118</td>
<td>25</td>
<td>Podcasts, news, and racial identity</td>
</tr>
<tr>
<td>119</td>
<td>70</td>
<td>AI and human enhancement</td>
</tr>
<tr>
<td>120</td>
<td>61</td>
<td>Politics timely and topical</td>
</tr>
<tr>
<td>121</td>
<td>31</td>
<td>Culture of work</td>
</tr>
<tr>
<td>124</td>
<td>75</td>
<td>Global Attitudes US Survey 2023</td>
</tr>
<tr>
<td>125</td>
<td>69</td>
<td>Politics timely and topical</td>
</tr>
<tr>
<td>126</td>
<td>93</td>
<td>Racial attitudes, modern family</td>
</tr>
<tr>
<td>127</td>
<td>59</td>
<td>Americans and their data</td>
</tr>
<tr>
<td>128</td>
<td>89</td>
<td>Americans and planet Earth</td>
</tr>
<tr>
<td>129</td>
<td>107</td>
<td>Politics timely and topical</td>
</tr>
<tr>
<td>130</td>
<td>94</td>
<td>Politics representation</td>
</tr>
<tr>
<td>131</td>
<td>70</td>
<td>Gender and leadership</td>
</tr>
<tr>
<td>Wave</td>
<td># questions</td>
<td>Wave Topic</td>
</tr>
<tr>
<td>26</td>
<td>44</td>
<td>Guns</td>
</tr>
<tr>
<td>29</td>
<td>20</td>
<td>Views on gender</td>
</tr>
<tr>
<td>32</td>
<td>24</td>
<td>Community types, Sexual harassment</td>
</tr>
<tr>
<td>34</td>
<td>16</td>
<td>Biomedical and food issues</td>
</tr>
<tr>
<td>36</td>
<td>68</td>
<td>Gender and leadership</td>
</tr>
<tr>
<td>41</td>
<td>41</td>
<td>Views of America in 2050</td>
</tr>
<tr>
<td>42</td>
<td>26</td>
<td>Trust in science</td>
</tr>
<tr>
<td>43</td>
<td>51</td>
<td>Race in America</td>
</tr>
<tr>
<td>45</td>
<td>13</td>
<td>Misinformation</td>
</tr>
<tr>
<td>49</td>
<td>19</td>
<td>Privacy and surveillance</td>
</tr>
<tr>
<td>50</td>
<td>43</td>
<td>American families</td>
</tr>
<tr>
<td>54</td>
<td>50</td>
<td>Economic inequality</td>
</tr>
<tr>
<td>82</td>
<td>56</td>
<td>2021 Global Attitudes Project U.S. survey</td>
</tr>
<tr>
<td>92</td>
<td>23</td>
<td>Political Typology</td>
</tr>
</tbody>
</table>
<h2>A. 3 SubPOP-Train</h2>
<p>We gather additional data from the American Trends Panel, specifically collecting 53 waves from Wave 61 to 132. There are 62 waves from Wave 61 - 132, however, some waves have missing demographic or ideology information (for example, wave 63 does not contain political ideology information) or the data is not available hence removed during the curation process. To refine the dataset, we exclude questions that meet the following criteria: those with more than 10 response options, redacted response data, or dependencies on prior questions (e.g., assessing political strength). For the remaining questions, we use GPT-40 to refine their wording, ensuring they are well-suited for prompting the language models while making minimal modifications. In Figure 6 we provide a few-shot prompt for question refinement.</p>
<p>In Figure 7, we visualize the embeddings of the question texts (projected to 2-dimensions using tSNE) from OpinionQA compared to SubPOP-Train and SubPOP-Eval. The visualization shows how much larger our dataset is than OpinionQA ( $6.5 \times$ ), along with the expanded coverage of our dataset into semantic areas untouched by OpinionQA. The embeddings also reveal the distribution shift from ATP questions to GSS questions: while the ATP and GSS questions overlap in embedding space, the GSS question appear as small clusters, not evenly distributed over the ATP questions. In Table 4, we list each ATP wave in SubPOP-Train and OpinionQA, along with its number of questions and wave topic(s), as defined by ATP. ${ }^{1}$ The table indicates which topics are new in SubPOP-Train compared to OpinionQA, indicating the expanded coverage of our dataset, along with which topics remain unseen in OpinionQA, which we can use to test LLMs fine-tuned on SubPOP-Train for generalization.</p>
<h2>A. 4 SubPOP-Eval</h2>
<p>To further evaluate the out-of-distribution generalization ability of our fine-tuned models, we subsample 133 questions from the GSS 2022 dataset (Davern et al., 2024). We apply the same selection criteria as outlined in Appendix A.3, excluding questions that are redacted, conditioned on prior questions, inferable directly from the group</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Instruction: Refine the question with a minimal change to make the question sensible. Do not modify options, and do not modify a question if it makes sense. Always start your answer with "Refined question:".</p>
<p>Question: A cross // Do you have any of the following for spiritual purposes?
A. Yes, I have this for spiritual purposes
B. No, I do not have this for spiritual purposes</p>
<p>Refined question: Do you have a cross for spiritual purposes?</p>
<p>Question: As you may know, same-sex marriage is now legal in the U.S. Do you think this is [a good thing or a bad thing] for our society?
A. Very good thing
B. Somewhat good thing
C. Somewhat bad thing
D. Very bad thing</p>
<p>Refined question: As you may know, same-sex marriage is now legal in the U.S. Do you think this is a good thing or a bad thing for our society?,</p>
<p>Question: On a different subject...How much, if at all, do white people benefit from advantages in society that black people do not have
A. A great deal
B. A fair amount
C. Not too much
D. Not at all</p>
<p>Refined question: How much, if at all, do white people benefit from advantages in society that black people do not have?,</p>
<p>Question: Thinking about the past couple of weeks, would you say the news for Donald Trump has been...
A. Very good
B. Mostly good
C. Neither good nor bad
D. Mostly bad
E. Very bad</p>
<p>Refined question: Thinking about the past couple of weeks, would you say the news for Donald Trump has been...</p>
<p>Question: (Question to refine)
(Options)
Refined question:
Figure 6: Few-shot prompt for refining the question to suit a language model prompting. An instruction is designed to make a minimal change to the original question, and in-context examples are provided.
information, derived from a set of questions, or those with more than 10 response options.</p>
<h2>A. 5 Inspection of Identical Questions</h2>
<p>Distribution of cosine similarities between two text embeddings (an output of the embedding model OpenAI-text-embedding-3-large given a question text), one from a question in SubPOP-Train and another from OpinionQA is shown in Figure 8. We observed a fraction of pairs having high cosine similarity, and manually inspected question pairs with high relevance. We find that by setting a</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Embeddings of questions from OpinionQA, SubPOP-Train, and SubPOP-Eval.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Distribution of cosine similarities between a question in SubPOP-ATP and OpinionQA, having a long tail towards a high cosine similarity. We inspect the question pairs in the range of 0.8 to 1.0 (distribution shown in the magnified view) and use a similarity of 0.87 as a safe threshold to identify a semantically identical question pair.</p>
<p>The threshold cosine similarity of 0.87 was the most important. We can detect all semantically identical pairs. We took a conservative threshold of cosine similarity; this value was to maximize the recall at a cost of precision to ensure detection of overlapping questions.</p>
<h2>B Experiment Details</h2>
<p>We conduct our experiments using Nvidia A100 GPUs with 80GB VRAM. Hyperparameter tuning is performed over learning rates {5e-5, 1e-4, 2e-4} and batch sizes {64, 128, 256}. After evaluating possible combinations, we select a (learning rate, batch size) = (2e-4, 256) for Llama-2-7B, (learning rate, batch size) = (2e-4, 256) for Mistral-7B-v0.1, and (learning rate, batch size) = (1e-4, 256) for Llama-2-13B when utilizing the full training dataset. For Llama-3-70B, we have not done hyperparameter search but heuristically used (learning rate, batch size) = (2e-5, 256). For sub-sampled training data (Figure 5), we use the following configurations:</p>
<ul>
<li>(lr, bs) = (2e-4, 256) for 75% of the training data</li>
<li>(lr, bs) = (1e-4, 128) for 50% of the training data</li>
<li>(lr, bs) = (1e-4, 128) for 25% of the training data</li>
</ul>
<p>All training is performed using LoRA (Hu et al., 2021), with LoRA parameters initialized from a normal distribution with σ = 0.02. We set the LoRA rank to 8, alpha to 32, and apply a dropout rate of 0.05. LoRA weights are applied to the query and value matrices. The AdamW (Loshchilov, 2017) optimizer is used with a weight decay of 0.</p>
<p>We use offline batched inference of vLLM (version 0.7.2) (Kwon et al., 2023) for inference and measuring response probability distribution of all methods.</p>
<h3>Choice of the Training Objective</h3>
<p>In this section, we explore both forward KL-divergence and Wasserstein Distance (WD) as training objectives. The forward KL-divergence is defined as</p>
<p>$$D_{\text{KL}}(p_H|p_\theta) = \sum_{a \in \mathcal{A}<em>q} p_H(a) \log \frac{p_H(a)}{p</em>\theta(a)},$$</p>
<p>where $p_H(a) \equiv p_H(a \mid q, g)$ and $p_\theta(a) \equiv p_\theta(a \mid q, g)$. Similarly, WD is given by</p>
<p>$$\mathcal{WD}(p_H, p_\theta) = \min_{\gamma \in \Pi(p_H, p_\theta)} \sum_{a, a' \in \mathcal{A}_q} \gamma(a, a') d(a, a'),$$</p>
<p>with $\Pi(p_H, p_\theta)$ denoting the set of all couplings between $p_H$ and $p_\theta$, and $d(a, a')$ the L1 distance between choices. Since survey responses are inherently one-dimensional and ordinal, we can simplify the computation of WD using cumulative distribution functions (CDFs). In the 1-D case, WD is computed as</p>
<p>$$\begin{aligned} \mathcal{WD}(p_H, p_\theta) &amp;= \int_{-\infty}^{+\infty} |F_{p_H}(x) - F_{p_\theta}(x)| \, dx, \ &amp;= \sum_{i=1}^{n} |F_{p_H}(i) - F_{p_\theta}(i) \end{aligned}$$</p>
<p>where $F_{p_H}$ and $F_{p_\theta}$ are the CDFs corresponding to $p_H$ and $p_\theta$, respectively. We use this discrete formulation as the WD loss in our training.</p>
<p>While training with WD resulted in a higher KL-divergence on the validation set, the validation</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Train loss curve (left) and validation loss curve (right) for Llama-2-7B fine-tuned on 90% of OpinionQA, with the remaining 10% used for validation. Light and dark blue lines represent KL-divergence (KL) and Wasserstein distance (WD) when used KL as a training objective, while light and dark red lines represent KL and WD when used WD as a training objective. The two training objectives yield similar results in terms of WD, the primary measure of opinion distribution matching in our work.</p>
<p>WD converged to similar levels regardless of the objective (see Figure 9). We attribute this to KL-divergence penalizing low-probability assignments without significantly altering the overall distribution geometry. Given the KL divergence's broader applicability—without requiring ordinal information—we primarily used KL-divergence in our experiments.</p>
<h2><strong>C Additional Experiments</strong></h2>
<h3><strong>C.1 Effect of Response Distribution Modeling</strong></h3>
<p>In this section, we compare different methods for capturing the distribution of human responses. We consider three approaches:</p>
<ol>
<li><em>One-hot</em>: Predicting only the most probable response, which ignores the full distribution over all responses (Li et al., 2024).</li>
<li><em>Augment by N</em>: Augmenting the dataset by replicating each response by a factor of N according to its observed frequency (Zhao et al., 2023).</li>
<li><em>Explicit probability modeling</em>: Directly modeling the full response distribution using the actual probability values for each option.</li>
</ol>
<p>Table 5 summarizes the results of these approaches. Notably, explicit probability modeling substantially outperforms the one-hot method, demonstrating that simply predicting the single most frequent response fails to capture the opinion diversity present within each subpopulation.</p>
<p>Compared with augment by <em>N</em> (2nd and 3rd column in Table 5), explicit probability modeling also achieves better performance. Importantly, the performance gap exceeds the quantization error introduced by discretizing the response distribution. For instance, when discretizing with a factor of <em>N</em>, the quantization error is $$\frac{1}{2N}$$—approximately 0.01 or 0.005 in the cases shown in Table 5. Moreover, explicit modeling offers the practical benefit of reducing the data volume by a factor of <em>N</em> compared to the augmentation approach, thereby lowering the computational cost of fine-tuning LLMs.</p>
<p>These results underscore the importance of explicit distribution modeling. By aligning the model's predictive distribution directly with the survey distribution, we achieve higher accuracy with fewer data samples, avoiding the rounding errors and replication overheads that are inherent to data-augmentation approaches.</p>
<h3><strong>C.2 Post-trained Model</strong></h3>
<p>We fine-tune Llama-2-7B-chat to observe the effect of starting from checkpoints that have been instruction-tuned via Reinforcement Learning from Human Feedback (RLHF). Table 6 shows the evaluation performance of a baseline method (Zero-shot prompt (QA)), fine-tuned base model (Llama-2-7B) and fine-tuned chat model (Llama-2-7B-chat). We observe the significant performance improvement, while the baseline method performs worse than the models not instruction-tuned (Table 1). Especially, the performance for SubPOP-Eval of chat model is significantly worse than that of base model. We observe the high WD of the baseline method resulting from the model assigning high probability to a specific token (e.g., 'A'), being far apart from the human opinion distribution. After fine-tuning the model are able to generate a more distributed probability of answer tokens. This result coincides with the result reported in (Moon et al., 2024).</p>
<p>Table 5: Comparison of evaluation performance for three response distribution modeling approaches, with Llama-2-7B as a base model. The last column (Explicit) is identical to the ours presented in Table 1. A model fine-tuned to predict the most probable choice (one-hot) performs the worst, as the model has not learned distributional opinion at fine-tuning phase. A model trained on augmented data (Aug. (×50, ×100)), while performing much better than one-hot still underperforms the explicit distribution modeling.</p>
<table>
<thead>
<tr>
<th>Eval Dataset</th>
<th>One-hot</th>
<th>Aug. (×50)</th>
<th>Aug. (×100)</th>
<th>Explicit (Ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpinionQA</td>
<td>0.163</td>
<td>0.110</td>
<td>0.107</td>
<td>0.106</td>
</tr>
<tr>
<td>SubPOP-Eval</td>
<td>0.178</td>
<td>0.130</td>
<td>0.123</td>
<td>0.121</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance of the fine-tuned Llama-2-7B-chat model (Chat LLM FT). For comparison, we also present lower and upper bounds, the baseline method Zero-shot prompt (QA) and our fine-tuned Llama-2-7B (Base LLM FT).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>OpinionQA</th>
<th>SubPOP-Eval</th>
</tr>
</thead>
<tbody>
<tr>
<td>Upper bound (Unif.)</td>
<td>0.178</td>
<td>0.208</td>
</tr>
<tr>
<td>Lower bound (Human)</td>
<td>0.031</td>
<td>0.033</td>
</tr>
<tr>
<td>Base zero-shot prompt (QA)</td>
<td>0.173</td>
<td>0.206</td>
</tr>
<tr>
<td>Base LLM FT</td>
<td>0.106</td>
<td>0.121</td>
</tr>
<tr>
<td>Chat zero-shot prompt (QA)</td>
<td>0.308</td>
<td>0.383</td>
</tr>
<tr>
<td>Chat LLM FT</td>
<td>0.109</td>
<td>0.148</td>
</tr>
</tbody>
</table>
<h3>C. 3 Generalization to Unseen Subpopulations</h3>
<p>Here we present a complete list of evaluation performance on OpinionQA for unseen subpopulations (the groups not used to fine-tune our model) and perform an analysis that shows our fine-tuned models are able to steer towards the given subpopulation information.</p>
<p>As shown in Table 7, we observe a performance improvement across unseen subpopulations. To verify that the performance improvements arise from the fine-tuned model being able to steer towards given subpopulations, we measure intergroup disagreement pattern for the demographic and ideology traits, shown in Figure 10, 11, 12, and 13. We consistently observe across traits that the disagreement pattern of our model resembles that of the human group, while zero-shot prompting with the base model exhibits a pattern completely different from the human group result. This observation shows that our fine-tuned model learns to condition on subpopulation information and also generalizes to subpopulations unseen during fine-tuning.</p>
<h2>D Baseline Details</h2>
<ul>
<li>Zero-shot prompting: Three prompt styles-QA, BIO, and PORTRAY—are introduced in (Santurkar et al., 2023) to integrate group information into prompts. These prompts are then combined with survey questions to construct inputs for LLM. Then, the first-token log-probability from LLM is measured to calculate the model's response distribution over options. In our baseline (and also in fine-tuning experiments) we focus on the QA steering format. Examples of this prompting method are shown in Figure 14.</li>
<li>Few-shot prompting: We craft a conditioning prompt that contains not only group information but also the group's response distribution to $k$
train questions, following (Hwang et al., 2023). For a test question $q_{\text {test }} \in Q_{\text {test }}$, we first sort training questions $Q_{\text {train }}$ into $\left{q_{1}, q_{2}, \ldots\right}$ such that $\operatorname{sim}\left(\mathrm{E}\left(q_{1}\right), \mathrm{E}\left(q_{\text {test }}\right)\right)&gt;\operatorname{sim}\left(\mathrm{E}\left(q_{2}\right), \mathrm{E}\left(q_{\text {test }}\right)\right)$, and so on. $\mathrm{E}(q)$ denotes the embedding model (OpenAI-text-embedding-3-large) output of the input $q$ and sim is a cosine similarity between two embedding vectors. Then, response information of the first $k$ questions $\left{q_{i}, p\left(\mathcal{A}<em i="i">{q</em>, g\right)\right}}} \mid q_{i<em _test="{test" _text="\text">{i=1}^{k}$ are used as few shot prompts to have the language model verbalize (Meister et al., 2024) expected response distribution for the given $g$ and $q</em>$. An example of the prompt for $k=3$ case is shown in Figure 15, while we run the baseline experiment in a $k=5$ setting.}</li>
<li>Modular Pluralism: The intuition behind Modular Pluralism (Feng et al., 2024) is that a language model trained on a text corpus of a specific subpopulation will faithfully represent public opinion of that population. Given a survey question with a PORTRAY-style steering prompt, each of language model 'modules' (fine-tuned Mistral-7B-Instructv0.1) generates an option choice with explanation. A black-box LLM (GPT-3.5-turbo-Instruct) receives all generations and select a generation that best aligns with the given group. Finally, using the chosen generation as a context, a black-box LLM generates probability distribution over options. The example pipeline is shown in Figure 16. Instead of the sub-sampled OpinionQA dataset the authors of the method used, we use the exactly same evaluation set across all baseline methods and our approach for a fair comparison.</li>
<li>Upper bound: We estimate the distribution between human responses and uniform distribution as an upper bound of WD metrics.</li>
<li>Lower bound: We compute a lower bound by randomly sampling a group of respondents and calculating the Wasserstein distance (WD) between the distribution of the sampled group and that of the original respondents for each question. We then bootstrap with $R=1000$ to construct a $95 \%$ confidence interval (CI) for the WDs. Further details on this estimation process are provided below.</li>
</ul>
<p>Computing weighted answer distributions: For each group $g$ and question $q$, we have $n_{g q}$ responses from respondents who belong to group $g$ answering question $q: x_{1}, x_{2}, \cdots, x_{n_{g q}}$, where $x_{i} \in \mathcal{A}_{q}$, i.e., the answer set for question</p>
<p>Table 7: Evaluation performance of our fine-tuned Llama-2-7B model on OpinionQA for subpopulations not included in the fine-tuning dataset SubPOP-Train. For reference, we present a lower bound (human) and the zero-shot prompting (QA). Absolute difference refers to the WD difference between zero-shot prompting and ours, and the relative improvement is calculated in a same way as Figure 3.</p>
<table>
<thead>
<tr>
<th>Attribute</th>
<th>Group</th>
<th>Lower Bound (Human)</th>
<th>Zero-shot (QA)</th>
<th>Ours</th>
<th>Absolute Diff.</th>
<th>Relative Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Age</td>
<td>18-29</td>
<td>0.023</td>
<td>0.185</td>
<td>0.096</td>
<td>0.089</td>
<td>0.548</td>
</tr>
<tr>
<td>Age</td>
<td>30-49</td>
<td>0.014</td>
<td>0.151</td>
<td>0.093</td>
<td>0.058</td>
<td>0.424</td>
</tr>
<tr>
<td>Age</td>
<td>50-64</td>
<td>0.014</td>
<td>0.154</td>
<td>0.101</td>
<td>0.052</td>
<td>0.377</td>
</tr>
<tr>
<td>Age</td>
<td>65+</td>
<td>0.013</td>
<td>0.195</td>
<td>0.115</td>
<td>0.080</td>
<td>0.438</td>
</tr>
<tr>
<td>Region</td>
<td>Midwest</td>
<td>0.016</td>
<td>0.153</td>
<td>0.095</td>
<td>0.058</td>
<td>0.425</td>
</tr>
<tr>
<td>Region</td>
<td>West</td>
<td>0.017</td>
<td>0.162</td>
<td>0.095</td>
<td>0.068</td>
<td>0.465</td>
</tr>
<tr>
<td>Education</td>
<td>Associate's Degree</td>
<td>0.026</td>
<td>0.159</td>
<td>0.098</td>
<td>0.061</td>
<td>0.455</td>
</tr>
<tr>
<td>Education</td>
<td>High School Graduate</td>
<td>0.017</td>
<td>0.144</td>
<td>0.092</td>
<td>0.053</td>
<td>0.413</td>
</tr>
<tr>
<td>Education</td>
<td>Postgraduate</td>
<td>0.015</td>
<td>0.174</td>
<td>0.106</td>
<td>0.068</td>
<td>0.426</td>
</tr>
<tr>
<td>Education</td>
<td>Some College, No Degree</td>
<td>0.018</td>
<td>0.144</td>
<td>0.093</td>
<td>0.051</td>
<td>0.405</td>
</tr>
<tr>
<td>Income</td>
<td>\$50,000-\$55,000</td>
<td>0.016</td>
<td>0.153</td>
<td>0.098</td>
<td>0.054</td>
<td>0.396</td>
</tr>
<tr>
<td>Income</td>
<td>\$30,000-\$50,000</td>
<td>0.019</td>
<td>0.144</td>
<td>0.094</td>
<td>0.050</td>
<td>0.400</td>
</tr>
<tr>
<td>Political Ideology</td>
<td>Very Conservative</td>
<td>0.026</td>
<td>0.208</td>
<td>0.107</td>
<td>0.101</td>
<td>0.555</td>
</tr>
<tr>
<td>Political Ideology</td>
<td>Very Liberal</td>
<td>0.025</td>
<td>0.202</td>
<td>0.111</td>
<td>0.091</td>
<td>0.514</td>
</tr>
<tr>
<td>Political Party</td>
<td>Independent</td>
<td>0.016</td>
<td>0.155</td>
<td>0.093</td>
<td>0.062</td>
<td>0.445</td>
</tr>
<tr>
<td>Political Party</td>
<td>Something Else</td>
<td>0.026</td>
<td>0.162</td>
<td>0.092</td>
<td>0.069</td>
<td>0.510</td>
</tr>
<tr>
<td>Race</td>
<td>Other</td>
<td>0.050</td>
<td>0.180</td>
<td>0.144</td>
<td>0.036</td>
<td>0.275</td>
</tr>
<tr>
<td>Religion</td>
<td>Agnostic</td>
<td>0.028</td>
<td>0.189</td>
<td>0.115</td>
<td>0.074</td>
<td>0.459</td>
</tr>
<tr>
<td>Religion</td>
<td>Buddhist</td>
<td>0.063</td>
<td>0.207</td>
<td>0.149</td>
<td>0.059</td>
<td>0.405</td>
</tr>
<tr>
<td>Religion</td>
<td>Nothing in Particular</td>
<td>0.019</td>
<td>0.153</td>
<td>0.092</td>
<td>0.061</td>
<td>0.454</td>
</tr>
<tr>
<td>Religion</td>
<td>Orthodox</td>
<td>0.083</td>
<td>0.221</td>
<td>0.180</td>
<td>0.041</td>
<td>0.298</td>
</tr>
<tr>
<td>Religion</td>
<td>Other</td>
<td>0.051</td>
<td>0.184</td>
<td>0.123</td>
<td>0.061</td>
<td>0.457</td>
</tr>
<tr>
<td>Religion</td>
<td>Roman Catholic</td>
<td>0.018</td>
<td>0.145</td>
<td>0.098</td>
<td>0.047</td>
<td>0.371</td>
</tr>
</tbody>
</table>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Heatmap of intergroup disagreement between a target human group ( $y$-axis) and a source group ( $x$-axis, either a human group or a group simulated with the language model), for OpinionQA evaluation data and age trait using Llama-2-7B as a base model. All subpopulations are unseen during fine-tuning.
$q$ (e.g., ${1,2,3,4}$ ). Furthermore, each respondent (and thus, their response) is associated with a wave-specific weight $w_{1}, w_{2}, \cdots, w_{n_{g q}}$, provided by Pew Research. We compute the human answer distribution $\pi_{g q}^{(H)}$ as a weighted sum over responses, where the proportion of respondents providing answer $a \in \mathcal{A}_{q}$ is estimated as</p>
<p>$$
\pi_{g q}^{(H)}(a)=\frac{\sum_{i=1}^{n_{g q}} w_{i} \mathbb{1}\left[x_{i}=a\right]}{\sum_{i=1}^{n_{g q}} w_{i}}
$$</p>
<p>Bootstrapping at the respondent-level: We draw bootstrap samples per group at the respondent-level including questions from all survey waves. This allows us to capture correlations in answer distributions across questions and across waves.</p>
<p>Specifically, let $\mathcal{P}<em g="g">{g}$ represent the set of respondents in group $g$, where $\left|\mathcal{P}</em>}\right|=n_{g}$. We produce bootstrapped samples by repeatedly sampling $n_{g}$ respondents from $\mathcal{P<em 1="1">{g}$ with replacement. Let $p</em>$ represent their corresponding weights.}^{(r)}, p_{2}^{(r)}, \cdots, p_{n_{g}}^{(r)}$ represent the sampled respondents for the $r$-th bootstrap, and let $w_{1}^{(r)}, w_{2}^{(r)}, \cdots, w_{n_{g}}^{(r)</p>
<p>For each question $q$, let $\mathcal{P}<em g="g">{g q} \subseteq \mathcal{P}</em>}$ represent the set of respondents from group $g$ who answered question $q$; as before, $\left|\mathcal{P<em g="g" q="q">{g q}\right|=n</em>$, and 0 otherwise. Then, we compute the $r$-th answer}$. Let us define $q\left(p_{i}\right)$ as person $p_{i}$ 's response to question $q$ if $p_{i}$ answered question $q$, i.e., $p_{i} \in \mathcal{P}_{g q</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Heatmap of intergroup disagreement between a target human group ( $y$-axis) and a source group ( $x$-axis, either a human group or a group simulated with the language model), for OpinionQA evaluation data and political party (affiliation) trait using Llama-2-7B as a base model. Two subpopulations, Democrat and Republican, are seen during fine-tuning, while Independent and Something Else are unseen.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Heatmap of intergroup disagreement between a target human group ( $y$-axis) and a source group ( $x$-axis, either a human group or a group simulated with the language model), for OpinionQA evaluation data and race / ethnicity trait using Llama-2-7B as a base model. Four subpopulations except 'Other' are seen during fine-tuning. In this case, the model does not well predict the opinions of Other group. We suspect this occurs because Other is a group with highly diverse race or ethnicity backgrounds, making it inherently difficult to infer its opinion distribution from those of White, Hispanic, Black, and Asian subpopulations.
distribution to question $q$ as:</p>
<p>$$
\pi_{g q}^{(r)}(a)=\frac{\sum_{i=1}^{n_{g}} \mathbb{1}\left[p_{i}^{(r)} \in \mathcal{P}<em i="i">{g q}\right] w</em>}^{(r)} \mathbb{1}\left[q\left(p_{i}^{(r)}\right)=a\right]}{\sum_{i=1}^{n_{g}} \mathbb{1}\left[p_{i}^{(r)} \in \mathcal{P<em i="i">{g q}\right] w</em>
$$}^{(r)}</p>
<p>Human lower bound of WD. Our statistic of interest is the mean Wasserstein distance over all questions $Q$ across all waves per group. We approximate this as the WD between the observed human distribution $\pi_{g q}^{(H)}$ and the bootstrap sample $\pi_{g q}^{(r)}$ for question $q$ and group $g$. Over all $R=1000$ bootstraps, we have</p>
<p>$$
\mathcal{D}<em Q="Q" _in="\in" q="q">{g}^{(H)}=\left{\frac{1}{|Q|} \sum</em>
$$} W D\left(\pi_{g q}^{(H)}, \pi_{g q}^{(r)}\right)\right}_{r=1}^{R</p>
<p>To quantify agreement between human samples, we report the mean and $95 \%$ CI (i.e., from $2.5^{\text {th }}$ to $97.5^{\text {th }}$ percentiles) of $\mathcal{D}_{g q}^{(H)}$.</p>
<h2>E Wave, Group-level Opinion Matching</h2>
<p>Here we present a group-level and wave-level averaged Wasserstein distance. Wave-level result is in Table 8, and group-level results for OpinionQA and SubPOP-Eval are in Table 9, 10, respectively. We observe that the improvements in distribution matching between LLM response and human response are consistent across diverse subpopulations and waves.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Heatmap of intergroup disagreement between a target human group (y-axis) and a source group (z-axis, either a human group or a group simulated with the language model), for OpinionQA evaluation data and political ideology trait using Llama-2-7B as a base model. Three subpopulations, Conservative, Moderate, and Liberal are seen during fine-tuning, while Very conservative and Very liberal are not seen.</p>
<p>Table 8: Per-wave Wasserstein distance on OpinionQA for each base model, comparing baseline zero-shot prompting (QA) with our fine-tuned model Ours(SubPOP-FT). Highlighted rows represent waves whose topics are not covered by the training data (SubPOP-Train). We observe WD improvement consistently across survey waves and also for waves of topics not covered in the training data.</p>
<table>
<thead>
<tr>
<th>Wave</th>
<th>Llama-2-7B</th>
<th></th>
<th>Llama-2-13B</th>
<th></th>
<th>Mistral-7B-v0.1</th>
<th></th>
<th>Llama-3-70B</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zero-shot</td>
<td>Ours (SubPOP-FT)</td>
<td>Zero-shot</td>
<td>Ours (SubPOP-FT)</td>
<td>Zero-shot</td>
<td>Ours (SubPOP-FT)</td>
<td>Zero-shot</td>
<td>Ours (SubPOP-FT)</td>
</tr>
<tr>
<td>26</td>
<td>0.191</td>
<td>0.145</td>
<td>0.180</td>
<td>0.126</td>
<td>0.178</td>
<td>0.131</td>
<td>0.134</td>
<td>0.084</td>
</tr>
<tr>
<td>29</td>
<td>0.169</td>
<td>0.096</td>
<td>0.172</td>
<td>0.123</td>
<td>0.153</td>
<td>0.096</td>
<td>0.125</td>
<td>0.085</td>
</tr>
<tr>
<td>32</td>
<td>0.163</td>
<td>0.110</td>
<td>0.156</td>
<td>0.098</td>
<td>0.137</td>
<td>0.099</td>
<td>0.151</td>
<td>0.091</td>
</tr>
<tr>
<td>34</td>
<td>0.155</td>
<td>0.105</td>
<td>0.171</td>
<td>0.089</td>
<td>0.134</td>
<td>0.095</td>
<td>0.138</td>
<td>0.083</td>
</tr>
<tr>
<td>36</td>
<td>0.175</td>
<td>0.120</td>
<td>0.184</td>
<td>0.126</td>
<td>0.175</td>
<td>0.107</td>
<td>0.130</td>
<td>0.087</td>
</tr>
<tr>
<td>41</td>
<td>0.160</td>
<td>0.090</td>
<td>0.155</td>
<td>0.084</td>
<td>0.134</td>
<td>0.073</td>
<td>0.116</td>
<td>0.085</td>
</tr>
<tr>
<td>42</td>
<td>0.159</td>
<td>0.053</td>
<td>0.146</td>
<td>0.059</td>
<td>0.127</td>
<td>0.059</td>
<td>0.131</td>
<td>0.084</td>
</tr>
<tr>
<td>43</td>
<td>0.179</td>
<td>0.112</td>
<td>0.172</td>
<td>0.104</td>
<td>0.154</td>
<td>0.102</td>
<td>0.124</td>
<td>0.099</td>
</tr>
<tr>
<td>45</td>
<td>0.177</td>
<td>0.101</td>
<td>0.177</td>
<td>0.093</td>
<td>0.149</td>
<td>0.084</td>
<td>0.126</td>
<td>0.091</td>
</tr>
<tr>
<td>49</td>
<td>0.151</td>
<td>0.098</td>
<td>0.143</td>
<td>0.131</td>
<td>0.128</td>
<td>0.116</td>
<td>0.159</td>
<td>0.087</td>
</tr>
<tr>
<td>50</td>
<td>0.209</td>
<td>0.139</td>
<td>0.196</td>
<td>0.121</td>
<td>0.188</td>
<td>0.125</td>
<td>0.154</td>
<td>0.078</td>
</tr>
<tr>
<td>54</td>
<td>0.158</td>
<td>0.087</td>
<td>0.158</td>
<td>0.087</td>
<td>0.128</td>
<td>0.077</td>
<td>0.118</td>
<td>0.079</td>
</tr>
<tr>
<td>82</td>
<td>0.173</td>
<td>0.098</td>
<td>0.171</td>
<td>0.075</td>
<td>0.148</td>
<td>0.077</td>
<td>0.174</td>
<td>0.093</td>
</tr>
<tr>
<td>92</td>
<td>0.165</td>
<td>0.073</td>
<td>0.153</td>
<td>0.071</td>
<td>0.140</td>
<td>0.055</td>
<td>0.126</td>
<td>0.081</td>
</tr>
</tbody>
</table>
<p>Table 9: Per-group Wasserstein distance on OpinionQA for each base models, before and after fine-tuning on SubPOP-Train. Base refers to zero-shot prompting (QA). *Full group variable name is "College grad, some Postgrad".</p>
<table>
<thead>
<tr>
<th>Attribute</th>
<th>Group</th>
<th></th>
<th>Llama-2-7B</th>
<th></th>
<th>Llama-2-13B</th>
<th></th>
<th>Mistral-7B-v0.1</th>
<th></th>
<th>Llama-3-70B</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Human Baseline</td>
<td>Base</td>
<td>Fine-tuned</td>
<td>Base</td>
<td>Fine-tuned</td>
<td>Base</td>
<td>Fine-tuned</td>
<td>Base</td>
<td>Fine-tuned</td>
</tr>
<tr>
<td>Region</td>
<td>Northeast</td>
<td>0.023</td>
<td>0.165</td>
<td>0.094</td>
<td>0.155</td>
<td>0.088</td>
<td>0.155</td>
<td>0.083</td>
<td>0.134</td>
<td>0.084</td>
</tr>
<tr>
<td></td>
<td>South</td>
<td>0.017</td>
<td>0.149</td>
<td>0.092</td>
<td>0.143</td>
<td>0.085</td>
<td>0.133</td>
<td>0.081</td>
<td>0.113</td>
<td>0.078</td>
</tr>
<tr>
<td>Education</td>
<td>College grad*</td>
<td>0.018</td>
<td>0.165</td>
<td>0.099</td>
<td>0.157</td>
<td>0.096</td>
<td>0.136</td>
<td>0.089</td>
<td>0.125</td>
<td>0.085</td>
</tr>
<tr>
<td></td>
<td>Less than high school</td>
<td>0.043</td>
<td>0.161</td>
<td>0.101</td>
<td>0.150</td>
<td>0.096</td>
<td>0.134</td>
<td>0.094</td>
<td>0.151</td>
<td>0.091</td>
</tr>
<tr>
<td>Gender</td>
<td>Male</td>
<td>0.015</td>
<td>0.182</td>
<td>0.093</td>
<td>0.152</td>
<td>0.089</td>
<td>0.131</td>
<td>0.083</td>
<td>0.138</td>
<td>0.083</td>
</tr>
<tr>
<td></td>
<td>Female</td>
<td>0.013</td>
<td>0.162</td>
<td>0.100</td>
<td>0.158</td>
<td>0.092</td>
<td>0.146</td>
<td>0.088</td>
<td>0.130</td>
<td>0.087</td>
</tr>
<tr>
<td>Race / ethnicity</td>
<td>Black</td>
<td>0.031</td>
<td>0.151</td>
<td>0.102</td>
<td>0.144</td>
<td>0.095</td>
<td>0.132</td>
<td>0.091</td>
<td>0.116</td>
<td>0.085</td>
</tr>
<tr>
<td></td>
<td>White</td>
<td>0.012</td>
<td>0.176</td>
<td>0.097</td>
<td>0.178</td>
<td>0.093</td>
<td>0.145</td>
<td>0.085</td>
<td>0.131</td>
<td>0.084</td>
</tr>
<tr>
<td></td>
<td>Asian</td>
<td>0.051</td>
<td>0.165</td>
<td>0.111</td>
<td>0.167</td>
<td>0.104</td>
<td>0.143</td>
<td>0.102</td>
<td>0.124</td>
<td>0.099</td>
</tr>
<tr>
<td></td>
<td>Hispanic</td>
<td>0.044</td>
<td>0.162</td>
<td>0.102</td>
<td>0.163</td>
<td>0.098</td>
<td>0.134</td>
<td>0.092</td>
<td>0.126</td>
<td>0.091</td>
</tr>
<tr>
<td>Income</td>
<td>$100,000 or more</td>
<td>0.019</td>
<td>0.172</td>
<td>0.103</td>
<td>0.162</td>
<td>0.100</td>
<td>0.147</td>
<td>0.091</td>
<td>0.159</td>
<td>0.087</td>
</tr>
<tr>
<td></td>
<td>Less than $30,000</td>
<td>0.021</td>
<td>0.162</td>
<td>0.091</td>
<td>0.148</td>
<td>0.083</td>
<td>0.127</td>
<td>0.080</td>
<td>0.154</td>
<td>0.078</td>
</tr>
<tr>
<td>Political Party</td>
<td>Democrat</td>
<td>0.016</td>
<td>0.172</td>
<td>0.099</td>
<td>0.158</td>
<td>0.092</td>
<td>0.161</td>
<td>0.082</td>
<td>0.118</td>
<td>0.079</td>
</tr>
<tr>
<td></td>
<td>Republican</td>
<td>0.019</td>
<td>0.196</td>
<td>0.105</td>
<td>0.235</td>
<td>0.101</td>
<td>0.181</td>
<td>0.095</td>
<td>0.174</td>
<td>0.093</td>
</tr>
<tr>
<td>Political Ideology</td>
<td>Liberal</td>
<td>0.022</td>
<td>0.192</td>
<td>0.100</td>
<td>0.181</td>
<td>0.094</td>
<td>0.166</td>
<td>0.084</td>
<td>0.126</td>
<td>0.081</td>
</tr>
<tr>
<td></td>
<td>Conservative</td>
<td>0.021</td>
<td>0.169</td>
<td>0.103</td>
<td>0.153</td>
<td>0.099</td>
<td>0.144</td>
<td>0.094</td>
<td>0.141</td>
<td>0.092</td>
</tr>
<tr>
<td></td>
<td>Moderate</td>
<td>0.016</td>
<td>0.151</td>
<td>0.094</td>
<td>0.153</td>
<td>0.090</td>
<td>0.132</td>
<td>0.082</td>
<td>0.106</td>
<td>0.081</td>
</tr>
<tr>
<td>Religion</td>
<td>Protestant</td>
<td>0.016</td>
<td>0.015</td>
<td>0.166</td>
<td>0.096</td>
<td>0.158</td>
<td>0.092</td>
<td>0.146</td>
<td>0.086</td>
<td>0.143</td>
</tr>
<tr>
<td></td>
<td>Jewish</td>
<td>0.058</td>
<td>0.182</td>
<td>0.124</td>
<td>0.182</td>
<td>0.122</td>
<td>0.165</td>
<td>0.115</td>
<td>0.144</td>
<td>0.115</td>
</tr>
<tr>
<td></td>
<td>Hindu</td>
<td>0.079</td>
<td>0.211</td>
<td>0.160</td>
<td>0.232</td>
<td>0.163</td>
<td>0.211</td>
<td>0.161</td>
<td>0.181</td>
<td>0.157</td>
</tr>
<tr>
<td></td>
<td>Atheist</td>
<td>0.035</td>
<td>0.202</td>
<td>0.118</td>
<td>0.204</td>
<td>0.110</td>
<td>0.196</td>
<td>0.099</td>
<td>0.135</td>
<td>0.098</td>
</tr>
<tr>
<td></td>
<td>Muslim</td>
<td>0.089</td>
<td>0.202</td>
<td>0.159</td>
<td>0.209</td>
<td>0.156</td>
<td>0.204</td>
<td>0.146</td>
<td>0.171</td>
<td>0.144</td>
</tr>
</tbody>
</table>
<p>Table 10: Per-group Wasserstein distance on SubPOP-Eval for each base models, before and after fine-tuning on SubPOP-Train. Base refers to zero-shot prompting (QA). *Full group variable name is "College grad, some Postgrad".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attribute</th>
<th style="text-align: center;">Group</th>
<th style="text-align: center;">Llama-2-7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Llama-2-13B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mistral-7B-v0.1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Llama-3-70B</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human Baseline</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Fine-tuned</td>
</tr>
<tr>
<td style="text-align: center;">Region</td>
<td style="text-align: center;">Northeast</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.113</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.078</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">South</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.080</td>
</tr>
<tr>
<td style="text-align: center;">Education</td>
<td style="text-align: center;">College grad*</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">0.077</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Less than high school</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.108</td>
</tr>
<tr>
<td style="text-align: center;">Gender</td>
<td style="text-align: center;">Male</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.079</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Female</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.080</td>
</tr>
<tr>
<td style="text-align: center;">Race / ethnicity</td>
<td style="text-align: center;">Black</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.094</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">White</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.083</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Asian</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.096</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hispanic</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.115</td>
</tr>
<tr>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">\$100,000 or more</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Less than \$30,000</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.086</td>
</tr>
<tr>
<td style="text-align: center;">Political Party</td>
<td style="text-align: center;">Democrat</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.076</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Republican</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.123</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.093</td>
</tr>
<tr>
<td style="text-align: center;">Political Ideology</td>
<td style="text-align: center;">Liberal</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.096</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.076</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conservative</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.113</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.092</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Moderate</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: center;">Religion</td>
<td style="text-align: center;">Protestant</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jewish</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.119</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hindu</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.166</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Atheist</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.106</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Muslim</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.158</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ ATP wave topics and time periods are defined at https://www.pewresearch.org/ american-trends-panel-datasets/.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
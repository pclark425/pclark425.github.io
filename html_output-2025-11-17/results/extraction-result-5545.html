<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5545 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5545</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5545</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-259252104</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.13679v1.pdf" target="_blank">GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks</a></p>
                <p><strong>Paper Abstract:</strong> The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and providing guidance to resolve them.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5545.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5545.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-based models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale pre-trained Generative Pre-trained Transformer (GPT) family (e.g., ChatGPT, GPT-4, GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses GPT-family LLMs as candidate text-based simulators for multiple modeling & simulation tasks (graph-to-text, table-to-text, image-to-text, and error explanation), outlining workflows, practical steps, and limitations but reporting no quantitative simulation accuracy from this paper's own experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / GPT-4 / GPT-3 (collectively discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based, large-scale pre-trained language models (LLMs) exemplified by GPT-3, ChatGPT, GPT-4; authors also mention other large LLMs such as Google PaLM, Meta LLaMA, and Megatron-Turing. The paper treats them as general-purpose natural language generators that can be prompted or fine-tuned to produce textual explanations/ summaries from simulation artifacts (graphs, tables, images, error reports).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>paper mentions GPT-4 (claimed ~1T parameters), PaLM (540B), Megatron-Turing (530B); exact model sizes for deployed instances not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Modeling & Simulation broadly; examples and motivating domains include social simulation/public health (youth suicide prevention, ACEs), epidemiological models (COVID-19), environmental and Earth system science, molecular dynamics, and accessibility-focused visualization of simulation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Four high-level text-based simulation tasks presented: (1) graph-to-text: generate narrative explanations of conceptual/simulation models (structure and causal logic); (2) table-to-text: summarize and compare simulation outcomes across scenarios to support decision-making; (3) image-to-text: translate simulation visualizations into textual descriptions for accessibility; (4) error explanation: help explain or hypothesize causes for model verification/validation anomalies and suggest fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt design and engineering; need for fine-tuning vs. zero/ few-shot prompting; input linearization/decomposition (graph decomposition size); input length (larger inputs degrade output quality); representation format (use of RDF-like unambiguous flags); numerical-data handling limits in table-to-text and summarization algorithms; model hallucination and bias; audience specificity and verbosity preferences; model's inherent limits in understanding/executing simulation code or complex emergent behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Conceptual and literature-supported evidence cited in the paper: reports of hallucinations in LLM outputs (Haman & Školník 2023; Borji 2023); experiments referenced showing text quality degrades with larger graph inputs (Shrestha et al. 2022); discussion of need for fine-tuning and prompt engineering with citations (Ding et al. 2023; White et al. 2023); surveys and method papers on table-to-text and numerical summarization highlighting inadequacy of general summarizers for numerical simulation outputs (Suadaa et al. 2021; Sindhu & Seshadri 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No quantitative evaluation of LLM-as-simulator accuracy is reported in this paper; proposed evaluation approaches are conceptual and include human interpretability/usability (e.g., participant comprehension studies), statistical comparison of simulation outputs to expectations (for error detection), and established table-to-text or graph-to-text evaluation paradigms referenced in the literature (but not applied with numeric results here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Hallucinations (fabricated facts), biased outputs, redundancy and low-quality machine-like phrasing, deterioration of output when inputs are too large or not properly linearized, inability to reliably ‘understand’ simulation code or emergent behaviors, LLM-generated code that may look plausible but not run, and inadequacy of general summarization systems for numerical simulation traces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Conceptual comparisons only: pipeline/template-based table-to-text vs end-to-end neural approaches; small-input decomposition vs whole-graph linearization (smaller decomposed inputs yield better outputs per cited experience); use of visualizations (charts) may outperform textual summaries for single-critical-output decision tasks. No quantitative model-vs-model accuracy comparisons are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Decompose large graphs into small parts before prompting; use unambiguous edge representations (e.g., RDF-style flags) for graph-to-text; consider fine-tuning LLMs when sufficient paired input-output examples exist; apply filters (e.g., minimum-change thresholds) to avoid verbose reports; prefer visualization for simple single-metric choices and complement text with visual summaries when needed; elicit audience preferences and verbosity levels for accessibility summaries; provide contextual statistical summaries to the LLM when asking it to explain errors; avoid over-relying on LLMs for tasks requiring exact code execution or deep model internal reasoning; apply human-in-the-loop corrections and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases_explicit</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5545.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5545.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 prototype (Shrestha et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype using GPT-3 to generate text from causal maps (Shrestha et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references a 2022 prototype that used GPT-3 to generate explanatory text from causal maps/ conceptual model schemas, illustrating early feasibility but also technical challenges (e.g., input decomposition and output quality degradation for larger inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (prototype referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An earlier GPT-family large language model used in a prototype to convert causal maps/graphical conceptual models into textual explanations (graph-to-text). The referenced work is framed as a prototype rather than a fully validated system.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social simulation / public-health-oriented social models (example context: causal maps such as ACEs and youth suicide prevention), general conceptual model explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Graph-to-text: automatically generating narratives that explain the structure and causal logic of conceptual models/causal maps to improve stakeholder understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Input decomposition strategy (how the graph is split), size of each input chunk (larger chunks degrade quality), need for task-specific fine-tuning, disambiguation of edges in linearized text, post-generation correction for redundancy and typos.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Cited experimental observation (Shrestha et al. 2022) that generated text quality deteriorates with larger inputs; the paper uses this prior work to motivate decomposition strategies and further research.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced as prototype work; specific quantitative evaluation methods or metrics are not reported in this paper's discussion of the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality degradation on larger inputs; requirement for custom linearization algorithms; possible biases and inability to capture all model nuances; necessity for human modeler involvement to correct and contextualize outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>No numerical comparisons; discussion contrasts pipeline/template-driven generation vs fine-tuned LLM approaches and highlights trade-offs (simplicity vs repetitiveness; data-hungry end-to-end methods vs template methods).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Break conceptual models into small parts for input; consider introducing auxiliary nodes to capture cycles if needed; use RDF-like unambiguous representations for edges; fine-tune LLMs with paired RDF-to-sentence examples if possible; include human-in-the-loop editing to mitigate bias and correct errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5545.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5545.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for table-to-text simulation summaries (ACEs example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based table-to-text summarization of simulation outcomes (illustrated with the ACEs / youth suicide prevention model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper outlines a workflow where simulation outputs across scenarios are converted into tables (numerical traces) and then transformed into textual summaries by LLMs to help users identify preferred scenarios, but provides no quantitative accuracy measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLMs (paper references GPT-family capabilities and table-to-text research)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs or table-to-text neural systems used to convert numerical simulation tables into concise textual summaries; paper references both template/pipeline methods and modern neural end-to-end or few-shot table-to-text approaches as possible implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Simulation decision support in social/health domains (example: ACEs model for youth suicide prevention); applicable to any domain where simulation outputs are numerical tables (e.g., environmental, epidemiological, engineering simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Table-to-text: summarize and compare final values/trajectories of simulation constructs across baseline and intervention cases (numerical traces) to support scenario selection and decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Ability of LLMs or summarization systems to handle numerical data faithfully (many general summarizers ignore or mis-handle numbers); input table size and dimensionality; whether extractive vs abstractive summarization is used (abstractive may be more concise but risk altering numeric meaning); prompt/template design and inclusion of difference-from-baseline framing; filters for significance thresholds to avoid verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Literature and conceptual analysis: references to work on table-to-text (Gong et al. 2020; Guo et al. 2023) and numerical-data-focused methods (Suadaa et al. 2021); discussion that general-purpose summarizers ignore numbers or over-weight numeric sentences (Sindhu & Seshadri 2022). No numeric experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Proposed approaches (conceptual): human users reading summaries to identify preferred scenarios; application of existing table-to-text evaluation frameworks in the literature (not executed here); suggestion to apply filters and numeric thresholds as part of the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Potential loss or misreporting of numeric detail with abstractive summarizers; overly verbose reports for large tables; existing summarization algorithms not directly applicable to pure-numerical simulation traces; possible mismatches between user implicit preferences and what automated summarization highlights.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Conceptual contrast between visualization (charts) and textual summaries: charts often better for single-key metrics via preattentive features, whereas text may be useful when multiple outputs must be communicated or for accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Transform simulation outputs into consolidated tables and express differences relative to baseline; use numeric-aware table-to-text models (cite Suadaa et al. 2021); apply user-defined filters (e.g., ignore changes < X%) to reduce verbosity; consider extractive methods when exact numeric fidelity is critical; combine visual and textual modalities when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5545.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5545.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for visualization-to-text (accessibility)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM/image-capable LLMs for converting simulation visualizations into textual descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper describes the emerging capability of image-capable LLMs (e.g., GPT-4 with image inputs) to decode charts and produce textual summaries for accessibility, noting user-specific preferences and task-dependent usefulness but providing no accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Image-capable LLMs (e.g., GPT-4 image-enabled) referenced conceptually</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs with multimodal capabilities (text+image) or neural chart-decoding systems that can convert visualizations (plots, heatmaps) into textual descriptions aimed at accessible consumption by visually impaired users or compliance with accessibility regulations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Visualization and accessibility across scientific domains that use simulation visualizations (e.g., Earth system science, molecular dynamics, agent-based modeling platforms).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Image-to-text: extract main patterns, trends, and decision-relevant insights from simulation visualizations (charts, heatmaps) and produce concise descriptions compatible with screen readers.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Reader/task-specific preferences (verbosity and content focus), the design of prompts specifying desired level of detail, quality of underlying chart-decoding models, and the need to align descriptions with users' intended tasks (e.g., identify best intervention vs. describe parameter dependence).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Cited user studies and literature: visually impaired users prefer highly individualized descriptions (Lundgard & Satyanarayan 2021); verbosity and task affect usefulness (Zong et al. 2022); prior neural chart-decoding work exists (Choi et al. 2019). No quantitative accuracy numbers reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Paper suggests human-centered evaluation (usability with visually impaired users and task-based effectiveness) rather than a purely automated numeric metric; no concrete evaluation run in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Emerging capability with immature tooling; variability in user preferences can make a single generated summary suboptimal; overly verbose descriptions can be counterproductive; chart-decoding may miss task-relevant patterns if prompts are not explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Discussion contrasts brief bullet-point summaries versus more verbose structural descriptions; recommends starting with short statements for screen-reader users.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Elicit user preferences for content and verbosity; frame prompts by the user's task (e.g., identify best intervention or explain parameter dependence); start with short bullet-point statements; complement textual descriptions with downloadable underlying data tables for precise numeric inspection when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5545.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5545.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for explaining/fixing simulation errors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of LLMs to explain and suggest fixes for simulation verification/validation errors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper frames explaining and proposing fixes for model-level simulation errors as a frontier application of LLMs: LLMs can be prompted with model goals, causal pathways, and contextual statistical discrepancies to generate hypotheses about causes and remediation suggestions, but they are not yet reliable for fully automated fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLMs (e.g., GPT-family) discussed conceptually</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs used as hypothesis generators and natural-language explainers for anomalies between simulation outputs and expected behavior; authors emphasize combining LLM causal knowledge with contextual statistical summaries from the model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Model verification/validation across simulation domains (epidemiology example given: COVID-19 compartmental/agent models).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Explain why simulation outputs deviate from expectations and propose possible fixes or hypotheses (e.g., missing disease stage in agent rules), functioning as an automated tutor or hypothesis generator for modelers.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality and completeness of prompt (including model goal and causal pathways), contextual statistical information about discrepancies, LLM's general causal knowledge and transfer learning capabilities, and the inherent difficulty of mapping code structure to emergent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Conceptual argumentation and references to nascent work on LLM-driven debugging (Liventsev et al. 2023; Kang et al. 2023); the paper notes that LLMs do not truly understand model internals and may need prompts teaching behavior of related models for transfer reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Proposed: combine statistical anomaly detection with LLM prompt inputs and then have modelers/human experts judge the plausibility and usefulness of generated hypotheses. No quantitative evaluation presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs may produce plausible-sounding but incorrect explanations; do not reliably map code to behavior; likely need iterative human-in-the-loop verification; cannot be expected to produce runnable corrected code reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Contrasted with existing automated debugging literature that focuses on small functions; noted that whole-model verification/validation is more complex and less explored.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Provide explicit model goals and summarized causal pathways in prompts; include contextual statistical comparisons of outputs vs expectations; use LLMs primarily to generate candidate hypotheses for human review rather than as autonomous fixers; prefer combination of statistical detection tools plus LLM explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatically Explaining a Model: Using Deep Neural Networks to Generate Text From Causal Maps <em>(Rating: 2)</em></li>
                <li>Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching <em>(Rating: 2)</em></li>
                <li>Few-Shot Table-to-Text Generation with Prompt-based Adapter <em>(Rating: 2)</em></li>
                <li>Towards table-to-text generation with reasoning <em>(Rating: 2)</em></li>
                <li>Accessible visualization via natural language descriptions: A four-level model of semantic content <em>(Rating: 2)</em></li>
                <li>Automated Insights on Visualizations with Natural Language Generation <em>(Rating: 1)</em></li>
                <li>Explainable Automated Debugging via Large Language Model-driven Scientific Debugging <em>(Rating: 2)</em></li>
                <li>Using ChatGPT to conduct a literature review <em>(Rating: 1)</em></li>
                <li>Improving text-to-text pre-trained models for the graph-to-text task <em>(Rating: 1)</em></li>
                <li>Towards Deadlock Handling with Machine Learning in a Simulation-Based Learning Environment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5545",
    "paper_id": "paper-259252104",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "GPT-based models (general)",
            "name_full": "Large-scale pre-trained Generative Pre-trained Transformer (GPT) family (e.g., ChatGPT, GPT-4, GPT-3)",
            "brief_description": "The paper discusses GPT-family LLMs as candidate text-based simulators for multiple modeling & simulation tasks (graph-to-text, table-to-text, image-to-text, and error explanation), outlining workflows, practical steps, and limitations but reporting no quantitative simulation accuracy from this paper's own experiments.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / GPT-4 / GPT-3 (collectively discussed)",
            "model_description": "Transformer-based, large-scale pre-trained language models (LLMs) exemplified by GPT-3, ChatGPT, GPT-4; authors also mention other large LLMs such as Google PaLM, Meta LLaMA, and Megatron-Turing. The paper treats them as general-purpose natural language generators that can be prompted or fine-tuned to produce textual explanations/ summaries from simulation artifacts (graphs, tables, images, error reports).",
            "model_size": "paper mentions GPT-4 (claimed ~1T parameters), PaLM (540B), Megatron-Turing (530B); exact model sizes for deployed instances not specified",
            "scientific_subdomain": "Modeling & Simulation broadly; examples and motivating domains include social simulation/public health (youth suicide prevention, ACEs), epidemiological models (COVID-19), environmental and Earth system science, molecular dynamics, and accessibility-focused visualization of simulation outputs.",
            "simulation_task": "Four high-level text-based simulation tasks presented: (1) graph-to-text: generate narrative explanations of conceptual/simulation models (structure and causal logic); (2) table-to-text: summarize and compare simulation outcomes across scenarios to support decision-making; (3) image-to-text: translate simulation visualizations into textual descriptions for accessibility; (4) error explanation: help explain or hypothesize causes for model verification/validation anomalies and suggest fixes.",
            "accuracy_metric": null,
            "reported_accuracy": null,
            "factors_affecting_accuracy": "Prompt design and engineering; need for fine-tuning vs. zero/ few-shot prompting; input linearization/decomposition (graph decomposition size); input length (larger inputs degrade output quality); representation format (use of RDF-like unambiguous flags); numerical-data handling limits in table-to-text and summarization algorithms; model hallucination and bias; audience specificity and verbosity preferences; model's inherent limits in understanding/executing simulation code or complex emergent behaviors.",
            "evidence_for_factors": "Conceptual and literature-supported evidence cited in the paper: reports of hallucinations in LLM outputs (Haman & Školník 2023; Borji 2023); experiments referenced showing text quality degrades with larger graph inputs (Shrestha et al. 2022); discussion of need for fine-tuning and prompt engineering with citations (Ding et al. 2023; White et al. 2023); surveys and method papers on table-to-text and numerical summarization highlighting inadequacy of general summarizers for numerical simulation outputs (Suadaa et al. 2021; Sindhu & Seshadri 2022).",
            "evaluation_method": "No quantitative evaluation of LLM-as-simulator accuracy is reported in this paper; proposed evaluation approaches are conceptual and include human interpretability/usability (e.g., participant comprehension studies), statistical comparison of simulation outputs to expectations (for error detection), and established table-to-text or graph-to-text evaluation paradigms referenced in the literature (but not applied with numeric results here).",
            "limitations_or_failure_cases": "Hallucinations (fabricated facts), biased outputs, redundancy and low-quality machine-like phrasing, deterioration of output when inputs are too large or not properly linearized, inability to reliably ‘understand’ simulation code or emergent behaviors, LLM-generated code that may look plausible but not run, and inadequacy of general summarization systems for numerical simulation traces.",
            "comparisons": "Conceptual comparisons only: pipeline/template-based table-to-text vs end-to-end neural approaches; small-input decomposition vs whole-graph linearization (smaller decomposed inputs yield better outputs per cited experience); use of visualizations (charts) may outperform textual summaries for single-critical-output decision tasks. No quantitative model-vs-model accuracy comparisons are reported in this paper.",
            "recommendations_or_best_practices": "Decompose large graphs into small parts before prompting; use unambiguous edge representations (e.g., RDF-style flags) for graph-to-text; consider fine-tuning LLMs when sufficient paired input-output examples exist; apply filters (e.g., minimum-change thresholds) to avoid verbose reports; prefer visualization for simple single-metric choices and complement text with visual summaries when needed; elicit audience preferences and verbosity levels for accessibility summaries; provide contextual statistical summaries to the LLM when asking it to explain errors; avoid over-relying on LLMs for tasks requiring exact code execution or deep model internal reasoning; apply human-in-the-loop corrections and verification.",
            "limitations_or_failure_cases_explicit": null,
            "uuid": "e5545.0",
            "source_info": {
                "paper_title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT-3 prototype (Shrestha et al. 2022)",
            "name_full": "Prototype using GPT-3 to generate text from causal maps (Shrestha et al. 2022)",
            "brief_description": "The paper references a 2022 prototype that used GPT-3 to generate explanatory text from causal maps/ conceptual model schemas, illustrating early feasibility but also technical challenges (e.g., input decomposition and output quality degradation for larger inputs).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (prototype referenced)",
            "model_description": "An earlier GPT-family large language model used in a prototype to convert causal maps/graphical conceptual models into textual explanations (graph-to-text). The referenced work is framed as a prototype rather than a fully validated system.",
            "model_size": null,
            "scientific_subdomain": "Social simulation / public-health-oriented social models (example context: causal maps such as ACEs and youth suicide prevention), general conceptual model explanation.",
            "simulation_task": "Graph-to-text: automatically generating narratives that explain the structure and causal logic of conceptual models/causal maps to improve stakeholder understanding.",
            "accuracy_metric": null,
            "reported_accuracy": null,
            "factors_affecting_accuracy": "Input decomposition strategy (how the graph is split), size of each input chunk (larger chunks degrade quality), need for task-specific fine-tuning, disambiguation of edges in linearized text, post-generation correction for redundancy and typos.",
            "evidence_for_factors": "Cited experimental observation (Shrestha et al. 2022) that generated text quality deteriorates with larger inputs; the paper uses this prior work to motivate decomposition strategies and further research.",
            "evaluation_method": "Referenced as prototype work; specific quantitative evaluation methods or metrics are not reported in this paper's discussion of the prototype.",
            "limitations_or_failure_cases": "Quality degradation on larger inputs; requirement for custom linearization algorithms; possible biases and inability to capture all model nuances; necessity for human modeler involvement to correct and contextualize outputs.",
            "comparisons": "No numerical comparisons; discussion contrasts pipeline/template-driven generation vs fine-tuned LLM approaches and highlights trade-offs (simplicity vs repetitiveness; data-hungry end-to-end methods vs template methods).",
            "recommendations_or_best_practices": "Break conceptual models into small parts for input; consider introducing auxiliary nodes to capture cycles if needed; use RDF-like unambiguous representations for edges; fine-tune LLMs with paired RDF-to-sentence examples if possible; include human-in-the-loop editing to mitigate bias and correct errors.",
            "uuid": "e5545.1",
            "source_info": {
                "paper_title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLMs for table-to-text simulation summaries (ACEs example)",
            "name_full": "LLM-based table-to-text summarization of simulation outcomes (illustrated with the ACEs / youth suicide prevention model)",
            "brief_description": "The paper outlines a workflow where simulation outputs across scenarios are converted into tables (numerical traces) and then transformed into textual summaries by LLMs to help users identify preferred scenarios, but provides no quantitative accuracy measurements.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLMs (paper references GPT-family capabilities and table-to-text research)",
            "model_description": "LLMs or table-to-text neural systems used to convert numerical simulation tables into concise textual summaries; paper references both template/pipeline methods and modern neural end-to-end or few-shot table-to-text approaches as possible implementations.",
            "model_size": null,
            "scientific_subdomain": "Simulation decision support in social/health domains (example: ACEs model for youth suicide prevention); applicable to any domain where simulation outputs are numerical tables (e.g., environmental, epidemiological, engineering simulations).",
            "simulation_task": "Table-to-text: summarize and compare final values/trajectories of simulation constructs across baseline and intervention cases (numerical traces) to support scenario selection and decision-making.",
            "accuracy_metric": null,
            "reported_accuracy": null,
            "factors_affecting_accuracy": "Ability of LLMs or summarization systems to handle numerical data faithfully (many general summarizers ignore or mis-handle numbers); input table size and dimensionality; whether extractive vs abstractive summarization is used (abstractive may be more concise but risk altering numeric meaning); prompt/template design and inclusion of difference-from-baseline framing; filters for significance thresholds to avoid verbosity.",
            "evidence_for_factors": "Literature and conceptual analysis: references to work on table-to-text (Gong et al. 2020; Guo et al. 2023) and numerical-data-focused methods (Suadaa et al. 2021); discussion that general-purpose summarizers ignore numbers or over-weight numeric sentences (Sindhu & Seshadri 2022). No numeric experiments in this paper.",
            "evaluation_method": "Proposed approaches (conceptual): human users reading summaries to identify preferred scenarios; application of existing table-to-text evaluation frameworks in the literature (not executed here); suggestion to apply filters and numeric thresholds as part of the pipeline.",
            "limitations_or_failure_cases": "Potential loss or misreporting of numeric detail with abstractive summarizers; overly verbose reports for large tables; existing summarization algorithms not directly applicable to pure-numerical simulation traces; possible mismatches between user implicit preferences and what automated summarization highlights.",
            "comparisons": "Conceptual contrast between visualization (charts) and textual summaries: charts often better for single-key metrics via preattentive features, whereas text may be useful when multiple outputs must be communicated or for accessibility.",
            "recommendations_or_best_practices": "Transform simulation outputs into consolidated tables and express differences relative to baseline; use numeric-aware table-to-text models (cite Suadaa et al. 2021); apply user-defined filters (e.g., ignore changes &lt; X%) to reduce verbosity; consider extractive methods when exact numeric fidelity is critical; combine visual and textual modalities when appropriate.",
            "uuid": "e5545.2",
            "source_info": {
                "paper_title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLMs for visualization-to-text (accessibility)",
            "name_full": "LLM/image-capable LLMs for converting simulation visualizations into textual descriptions",
            "brief_description": "The paper describes the emerging capability of image-capable LLMs (e.g., GPT-4 with image inputs) to decode charts and produce textual summaries for accessibility, noting user-specific preferences and task-dependent usefulness but providing no accuracy metrics.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Image-capable LLMs (e.g., GPT-4 image-enabled) referenced conceptually",
            "model_description": "LLMs with multimodal capabilities (text+image) or neural chart-decoding systems that can convert visualizations (plots, heatmaps) into textual descriptions aimed at accessible consumption by visually impaired users or compliance with accessibility regulations.",
            "model_size": null,
            "scientific_subdomain": "Visualization and accessibility across scientific domains that use simulation visualizations (e.g., Earth system science, molecular dynamics, agent-based modeling platforms).",
            "simulation_task": "Image-to-text: extract main patterns, trends, and decision-relevant insights from simulation visualizations (charts, heatmaps) and produce concise descriptions compatible with screen readers.",
            "accuracy_metric": null,
            "reported_accuracy": null,
            "factors_affecting_accuracy": "Reader/task-specific preferences (verbosity and content focus), the design of prompts specifying desired level of detail, quality of underlying chart-decoding models, and the need to align descriptions with users' intended tasks (e.g., identify best intervention vs. describe parameter dependence).",
            "evidence_for_factors": "Cited user studies and literature: visually impaired users prefer highly individualized descriptions (Lundgard & Satyanarayan 2021); verbosity and task affect usefulness (Zong et al. 2022); prior neural chart-decoding work exists (Choi et al. 2019). No quantitative accuracy numbers reported in this paper.",
            "evaluation_method": "Paper suggests human-centered evaluation (usability with visually impaired users and task-based effectiveness) rather than a purely automated numeric metric; no concrete evaluation run in this paper.",
            "limitations_or_failure_cases": "Emerging capability with immature tooling; variability in user preferences can make a single generated summary suboptimal; overly verbose descriptions can be counterproductive; chart-decoding may miss task-relevant patterns if prompts are not explicit.",
            "comparisons": "Discussion contrasts brief bullet-point summaries versus more verbose structural descriptions; recommends starting with short statements for screen-reader users.",
            "recommendations_or_best_practices": "Elicit user preferences for content and verbosity; frame prompts by the user's task (e.g., identify best intervention or explain parameter dependence); start with short bullet-point statements; complement textual descriptions with downloadable underlying data tables for precise numeric inspection when needed.",
            "uuid": "e5545.3",
            "source_info": {
                "paper_title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLMs for explaining/fixing simulation errors",
            "name_full": "Use of LLMs to explain and suggest fixes for simulation verification/validation errors",
            "brief_description": "The paper frames explaining and proposing fixes for model-level simulation errors as a frontier application of LLMs: LLMs can be prompted with model goals, causal pathways, and contextual statistical discrepancies to generate hypotheses about causes and remediation suggestions, but they are not yet reliable for fully automated fixes.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLMs (e.g., GPT-family) discussed conceptually",
            "model_description": "LLMs used as hypothesis generators and natural-language explainers for anomalies between simulation outputs and expected behavior; authors emphasize combining LLM causal knowledge with contextual statistical summaries from the model.",
            "model_size": null,
            "scientific_subdomain": "Model verification/validation across simulation domains (epidemiology example given: COVID-19 compartmental/agent models).",
            "simulation_task": "Explain why simulation outputs deviate from expectations and propose possible fixes or hypotheses (e.g., missing disease stage in agent rules), functioning as an automated tutor or hypothesis generator for modelers.",
            "accuracy_metric": null,
            "reported_accuracy": null,
            "factors_affecting_accuracy": "Quality and completeness of prompt (including model goal and causal pathways), contextual statistical information about discrepancies, LLM's general causal knowledge and transfer learning capabilities, and the inherent difficulty of mapping code structure to emergent outputs.",
            "evidence_for_factors": "Conceptual argumentation and references to nascent work on LLM-driven debugging (Liventsev et al. 2023; Kang et al. 2023); the paper notes that LLMs do not truly understand model internals and may need prompts teaching behavior of related models for transfer reasoning.",
            "evaluation_method": "Proposed: combine statistical anomaly detection with LLM prompt inputs and then have modelers/human experts judge the plausibility and usefulness of generated hypotheses. No quantitative evaluation presented here.",
            "limitations_or_failure_cases": "LLMs may produce plausible-sounding but incorrect explanations; do not reliably map code to behavior; likely need iterative human-in-the-loop verification; cannot be expected to produce runnable corrected code reliably.",
            "comparisons": "Contrasted with existing automated debugging literature that focuses on small functions; noted that whole-model verification/validation is more complex and less explored.",
            "recommendations_or_best_practices": "Provide explicit model goals and summarized causal pathways in prompts; include contextual statistical comparisons of outputs vs expectations; use LLMs primarily to generate candidate hypotheses for human review rather than as autonomous fixers; prefer combination of statistical detection tools plus LLM explanation generation.",
            "uuid": "e5545.4",
            "source_info": {
                "paper_title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatically Explaining a Model: Using Deep Neural Networks to Generate Text From Causal Maps",
            "rating": 2,
            "sanitized_title": "automatically_explaining_a_model_using_deep_neural_networks_to_generate_text_from_causal_maps"
        },
        {
            "paper_title": "Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching",
            "rating": 2,
            "sanitized_title": "tablegpt_fewshot_tabletotext_generation_with_table_structure_reconstruction_and_content_matching"
        },
        {
            "paper_title": "Few-Shot Table-to-Text Generation with Prompt-based Adapter",
            "rating": 2,
            "sanitized_title": "fewshot_tabletotext_generation_with_promptbased_adapter"
        },
        {
            "paper_title": "Towards table-to-text generation with reasoning",
            "rating": 2,
            "sanitized_title": "towards_tabletotext_generation_with_reasoning"
        },
        {
            "paper_title": "Accessible visualization via natural language descriptions: A four-level model of semantic content",
            "rating": 2,
            "sanitized_title": "accessible_visualization_via_natural_language_descriptions_a_fourlevel_model_of_semantic_content"
        },
        {
            "paper_title": "Automated Insights on Visualizations with Natural Language Generation",
            "rating": 1,
            "sanitized_title": "automated_insights_on_visualizations_with_natural_language_generation"
        },
        {
            "paper_title": "Explainable Automated Debugging via Large Language Model-driven Scientific Debugging",
            "rating": 2,
            "sanitized_title": "explainable_automated_debugging_via_large_language_modeldriven_scientific_debugging"
        },
        {
            "paper_title": "Using ChatGPT to conduct a literature review",
            "rating": 1,
            "sanitized_title": "using_chatgpt_to_conduct_a_literature_review"
        },
        {
            "paper_title": "Improving text-to-text pre-trained models for the graph-to-text task",
            "rating": 1,
            "sanitized_title": "improving_texttotext_pretrained_models_for_the_graphtotext_task"
        },
        {
            "paper_title": "Towards Deadlock Handling with Machine Learning in a Simulation-Based Learning Environment",
            "rating": 1,
            "sanitized_title": "towards_deadlock_handling_with_machine_learning_in_a_simulationbased_learning_environment"
        }
    ],
    "cost": 0.014602750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GPT-BASED MODELS MEET SIMULATION: HOW TO EFFICIENTLY USE LARGE-SCALE PRE-TRAINED LANGUAGE MODELS ACROSS SIMULATION TASKS
21 Jun 2023</p>
<p>Philippe J Giabbanelli 
Department of Computer Science
Software Engineering Miami University
501 East High Street Oxford45056OHUSA</p>
<p>GPT-BASED MODELS MEET SIMULATION: HOW TO EFFICIENTLY USE LARGE-SCALE PRE-TRAINED LANGUAGE MODELS ACROSS SIMULATION TASKS
21 Jun 2023BF613E1DC76D6CE25CBB6527378CCD6CarXiv:2306.13679v1[cs.HC]
The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns.This paper is the first examination regarding the use of LLMs for scientific simulations.We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved.The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process.The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario.The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text.Finally, the last task evokes the possibility of explaining simulation errors and providing guidance to resolve them.</p>
<p>INTRODUCTION</p>
<p>Natural Language Generation (NLG) has been in the limelight recently, following the release of ChatGPT and its wide potential application areas from writing (academic) papers to assignments and software code.Much in the same way as 'Google' colloquially refers to using a search engine, ChatGPT has served as a proxy to discuss the opportunities and concerns raised by Large Language Models (LLMs) (van Dis et al. 2023;Zhou et al. 2023).These large-scale pre-trained models are based on transformer architectures (Tay et al. 2022;Wang et al. 2022) and include several versions of GPT (e.g., GPT4 with one trillion parameters) alongside Google's Pathways Language Model (PaLM, whose 540 billion parameters support the Bard chatbot), LLaMA from Meta (available at several sizes), or Megatron-Turing (530 billion parameters) created by Microsoft and NVIDIA (Chowdhery et al. 2022;Smith et al. 2022).Beyond the sensational headlines, there is a growing realization that these models are complex tools that require technical attention before being adequately deployed.As summarized by the editor-in-chief of Science, ChatGPT provides "endless entertainment" but ultimately, like other machines, it serves as a tool "for the people posing the hypotheses, designing the experiments, and making sense of the results" (Thorp 2023).For example, researchers illustrated that ChatGPT was not going to perform a literature review by itself, as two thirds of the scientific studies that it discussed did not exist (Haman and Školník 2023); this phenomenon known as hallucination is one of the many errors or 'unpredictable qualities' occurring in LLMs (Ganguli et al. 2022;Borji 2023).It is thus important to complement the nascent literature on high-level opportunities and concerns with an emphasis on practical tasks and how they may be facilitated with LLMs under the right human intervention, which may include fine-tuning (Ding et al. 2023), asking the right questions (i.e., prompt engineering as discussed in White et al. 2023), and identifying where to correct the generated text.</p>
<p>In this paper, we are interested in shifting from using LLMs such as GPT as assistants in high-level science tasks (e.g., summarizing papers) to becoming central actors in specific tasks for Modeling &amp; Simulation (M&amp;S).This shift finds several parallels with the advent of Machine Learning and its impact on M&amp;S (Giabbanelli 2019;Elbattah 2019).Neither NLG nor machine learning are brand new, as their concepts and early systems were operational decades ago (Gatt and Krahmer 2018;Dong et al. 2022).However, their rise is based on the ability of new tools to operate at an unprecedented scale while being easily accessible.Plethora of online courses can equip practitioners with machine learning skills and a model can be quickly trained thanks to library such as scikit-learn or drag-and-drop software.Tools such as GPT have been in existence for several years already, as we presented a prototype using GPT-3 for simulation in 2022 (Shrestha et al. 2022).But the recent availability of products such as ChatGPT now makes these tools accessible, as there is no need for programming via an API.We can thus expect that NLG will potentially permeate most stages of the M&amp;S process, just as machine learning has become commonplace through a multitude of innovative hybrid simulations (Müller et al. 2022;Ghasemi et al. 2022;Onggo et al. 2018).In this context, this paper contributes to preparing our research community for this shift by examining which M&amp;S tasks can benefit from NLG and how it would be achieved.We note that such inventories of candidate tasks for NLG are now abundant in healthcare and medical education (Sallam 2023), business and marketing (Rivas and Zhao 2023), or environmental science (Zhu et al. 2023), but such guidance had yet to be issued for M&amp;S.</p>
<p>This paper proceeds in the order of simulation tasks summarized in Figure 1.This summary only illustrates the key steps of this paper, as we acknowledge that M&amp;S involves several other steps such as the transition from a conceptual model to a mathematical specification and its implementation as a computational model.As a popular phrase goes, "if all you have is a hammer, everything looks like a nail"; each section thus begins by establishing the rationale for using NLG.Then, we detail the methods involved, while noting that their maturity decreases as we progress along simulation tasks.</p>
<p>EXPLAINING THE STRUCTURE OF A SIMULATION MODEL</p>
<p>Rationale for Using Natural Language Generation</p>
<p>Modelers work within interdisciplinary teams, where members have (potentially overlapping) roles such as model commissioners who set the purpose of the model or participants who inform its content (Calder et al. 2018).Empirical studies have repeatedly highlighted the importance of communication skills in such teams.As discussed by Ahrweiler, Frank, and Gilbert (2019), "the first and most important [demand] is that the clients want to understand the model", which means that the structure of the model and the logic of its decision should be clear, rather than treated as blackbox that only allows to view and discuss outputs.Clearly conveying a model's structure is challenging, since team members may not be expert in modeling techniques hence they delving into code is not a viable solution.Although modeling languages (e.g., UML, SysML) can be familiar tools for model development, their steep learning curve for participants also presents an obstacle (Padilla, Shuttleworth, and O'Brien 2019).Our experiments confirmed that even a graph that only shows concepts and whether they are connected can be a significant learning curve for participants (senior executives), who struggled to provide confident and timely answers for basic questions about the model (Giabbanelli and Vesuvala 2023).Natural Language Generation thus opens up the opportunity to explain a model in a format that is potentially accessible to all parties: textual narratives.</p>
<p>Explaining model as narratives has limitations.For example, modelers should still be involved to assist participants.As argued by Gilbert et al. (2018), "it is impossible to capture in a report all the nuances of the model simplifications, data weaknesses etc. in a way that [participants] can use reliably".In addition, a report automatically generated from a schema may not be able to cover all topics that ought to be communicated, such as the purpose of the model (Grimm et al. 2020).However, we argue that an automatically generated report can at least convey the structure of a model so that participants understand which variables are involved and how they interact.Providing these expectations can address the existing Figure 1: After creating a conceptual model, it is implemented and utilized to perform experiments whose outputs support decision-making activities.Outputs are also used to ensure the correctness of the model vis-à-vis its specification (i.e., verification) or the adequacy of the model with regards to its real-world counterpart (i.e., validation).In this paper, we map each of these steps to a task in NLG.disconnect between the transparent and often informal process to elicit information from participants, and the opacity of the resulting model (Figure 2).Since transparency and trust in a model often come together (Falconi and Palmer 2017), we posit that a careful use of NLG to turn models into reports may ultimately increase the engagement of participants with the modeling process and their support of the decisions suggested by the simulations.</p>
<p>Core Methodological Components</p>
<p>Although the relation between textual descriptions and conceptual models has been discussed elsewhere, it has primarily been from the viewpoint of extracting model elements from text (Shuttleworth and Padilla 2022).This involves Natural Language Processing, with a focus on identifying entities such as agents and their properties.In contrast, turning a model into text is a matter of Natural Language Generation and it involves vastly different steps, particularly when operating via LLMs.Although LLMs such as GPT-4 have made headways with the use of images as inputs (Section 4), modelers cannot generally expect to just 'drop' a schema of their conceptual model and have it turned into a report.The schema first needs to be converted into a textual form.</p>
<p>Since schema (e.g., UML, causal maps) often depict concepts and their relations, the corresponding NLG task is known as graph-to-text.Graphs can have cycles: for example, the conceptual model in Figure 1 has a cycle of parenting stress increasing the risk of suicide attempt in their children, thus causing more stress.In contrast, sentences must be linear.The conversion thus starts by turning the model's schema into linearized sentences, but it cannot simply be achieved by removing parts of the schema to break existing cycles (this is known as a 'loss of structural information').One strategy is to break the schema into parts that collectively can recreate the full schema, thus resulting in some nodes being duplicated among the decomposed parts.These parts should be kept small (Figure 2) as experiments show that the quality of the generated text deteriorates with larger inputs (Shrestha et al. 2022).Linearization is a problem in itself, as it cannot simply be achieved by removing parts of the schema to break existing cycles (this is known as a 'loss of structural information').One strategy is to break the schema into parts that collectively can recreate the full schema, thus resulting in some nodes being duplicated among the decomposed parts.Alternatively, a new schema could be defined and introduce additional nodes to encapsulate the meaning of cycles (Rodrigues Ribeiro 2022).So far, linearization has primarily been studied for knowledge graphs rather than simulation schema, hence modelers would need to write custom linearization algorithms.</p>
<p>Once the graph is linearized, it needs to be expressed as a textual input.It does not suffice to just write out the graph as words, as the edges would be ambiguous.For example, the list A, B,C, D could be interpreted as the edges (A, B) and (C, D), or (A, B), (B,C), (C, D).An unambiguous representation thus uses flags to separate the origin node of an edge from the target.The Resource Description Framework (RDF) is widely used for this purpose (Yang et al. 2020;Zhang et al. 2023).Depending on its sophistication, the LLM may need to be fine-tuned by being presented with numerous RDF inputs and the expected sentence to generate.Once the (fine-tuned) LLM creates sentences, they may still need to be corrected.This can involve correcting typos (e.g., in earlier versions such as GPT-2), avoiding the redundancy that readers quickly associated with machine-generated text (Liu et al. 2023), or mitigating various forms of biases.The latter has received abundant attention, as authors have discussed the presence (and sometimes the inevitability) of bias (Ferrara 2023) on sex, race, religion, or disability -all of which are protected classes in US law on discrimination.However, studies on bias have not yet been conducted in the case of text generated from model schema, hence additional work is needed to assess the problem and whether some application fields are more at risk (e.g., models of social systems vs. physical systems).</p>
<p>Accomplishing the above steps results in generating sentences, but it does not yet make a report.As an analogy, consider teaching: an instructor cannot deliver content material in random order, or teach a second year elective class in the same way as a graduate seminar.Generating a report also needs to account for the audience and orchestrate sentences into meaningful paragraphs with an appropriate flow.Satisfying either of these requirements is currently an open topic when applying NLG to explaining simulation models.Because modeling is conducted in an interdisciplinary setting, insufficiently accounting for "the languages of the different research traditions can lead to misunderstanding and resentment" (Smaldino 2020).We posit that forming paragraphs may be an easier problem (and hence a prime research target) because the readability of paragraphs can be automatically measured for a language in general (e.g., using the Flesch-Kincaid readability test), whereas the appropriateness of terms and style with regards to a scientific discipline does not have an algorithmic scoring method.</p>
<p>HANDLING DYNAMICITY: COMPARING OUTCOMES FROM PREDICTIVE SIMULATIONS</p>
<p>Rationale for Using Natural Language Generation</p>
<p>In the previous section, we discussed the benefits of explaining the structure of a model by converting its schema to text.At a high-level, explaining the simulation outcomes would yield similar benefits, such as greater transparency and engagement.To further estimate benefits and limitations, it is necessary to precisely define the task of 'explaining outcomes'.A simulation model may be presented with a number of cases, also known as scenarios or what-if questions.For instance, in a model for suicide prevention (Figure 3), such scenarios could include educating parents to avoid harsh discipline, improving coping mechanisms in children, or providing treatment for substance misuse.One of the cases may be marked as baseline, thus reflecting the current state of the world in the absence of hypothetical interventions.The goal is to inform model users about the difference in simulation outcomes across cases, such that they can choose the best candidate interventions.By summarizing simulation outcomes across cases to focus on key differences, NLG can reduce the cognitive efforts involved in decision-making.</p>
<p>There are potential limitations when attempting to automatically convey the main differences across cases.First, a textual format may not always be most efficient.For example, if a simulation has only one critical output, then users may prefer a bar chart visualization (Figure 4) that shows the output of interest (y-axis) across simulation cases (x-axis).Since a bar chart relies on preattentive visual properties such as line length (Wolfe and Utochkin 2019) to bring attention to data points that stand out (e.g., lowest, highest), users would quickly notice which case yields the minimal/maximal outcome and hence would be preferred.A well-constructed visualization would thus be more effective than text.Second, a simulation may have multiple objectives that cannot all be optimized.For example, interventions for obesity could be characterized by indicators related to physical health (e.g., type-2 diabetes, musculoskeletal disorders, hypertension) as well as mental health (e.g., self-esteem and body image).Different users may give more importance to some of these indicators, and these preferences may be implicit.The incorporation of implicit preferences in multi-objective optimization (Cruz-Reyes et al. 2017) is a complex problem and has yet to be studied in the context of summaries generated by NLG.This section thus focuses on cases that have multiple outputs of interest and assumes that their importance is either equal or can be explicitly quantified.</p>
<p>Core Methodological Components</p>
<p>The input is a set of simulation cases, each containing the value of the model's constructs from the initial to the final iteration.Consider without loss of generality that the output seeks to summarize the baseline scenario, the design of each intervention and how it leads to changes compared to the baseline (Figure 3); the remainder of this section would be similar if there was no 'baseline' case.As explained in section 2.2, a LLM does not directly go from the input to the desired output: modelers need to perform at least two additional steps.The first step is to gather the results into a single table that contains the characteristics of each intervention and the final value of each construct (Figure 3, bottom right); characteristics and final values can be expressed as a difference with respect to the baseline (if applicable).By transforming simulation outputs into one table, we frame the problem as a table-to-text task, which has been well studied.2022).The implications of the baseline or 'status quo' scenario can be organized as a table, showing how the 6 constructs of the ACEs model change over discrete iterations until a final step.An intervention also consists of a table, whereby the initial values of some constructs change (due to the intervention) hence the final values may also change.</p>
<p>The second step is to transform the table into textual input for a LLM.Early methods follow a 'pipeline paradigm' in which a table transformation module applies a template to turn a table into text (Gong et al. 2020).Although this approach has the advantage of being conceptually simple, it requires users to design a template.Furthemore, the output is limited (hence repetitive) due to a reliance on set templates and rules.Newer approaches use end-to-end methods based on neural networks (Yang et al. 2021), but they depend on a large training set and may not be readily applicable to the specific context of a simulation model.We refer the reader to Guo et al. (2023) for an overview of current options, and to Table 4 by Sharma, Gogineni, and Ramakrishnan (2022) for a summary of methods based on the application dataset.While many prior works are concerned with tables that contain words, this is not directly applicable to simulation traces since they consist of numerical outputs.We thus recommend methods specifically designed for tables with numerical content, such as Suadaa et al. (2021).</p>
<p>Accomplishing the above steps results in a complete transformation of a table into text.If the table is short (e.g., few simulation cases and/or constructs), then model users may be able to read the generated text and identify the best simulation case.However, additional steps would be necessary to selectively transform larger tables and avoid overly verbose reports (Figure 4).As shown in Figure 3, it is not necessary to generate text regarding the initial state of every construct for every case: we can simply state which constructs had a different value than in the baseline case.We also do not need to specify the impact on every construct at the end of the simulation: applying a user-defined filter (e.g., ignore changes of less than 10%, only include changes on three specific constructs) can trim the list.Beyond these simple means to keep the text short, we note that text summarization algorithms may offer additional solutions (Gupta and Gupta 2019;Raffel et al. 2020).These algorithms operate either by compiling the most important existing sentences (extractive summarization e.g., Textrank, BERT-ext, Longformer-Ext) or by generating sentences (abstractive summarization e.g., BART, T5), which tend to have higher readability and are more concise but may not exactly reflect the meaning of the original text (Alomari et al. 2022).However, new summarization algorithms would need to be developed since the existing ones are not readily applicable to simulation data, which consist entirely of numbers.Indeed, general purpose summarization algorithms either ignore numbers or treat sentences with numerical data as more important (Sindhu and Seshadri 2022); neither option would help to identify the main characteristics of simulation scenarios or the key changes that they produce.Figure 4: A simple bar chart (left) can quickly reveal the preferred scenario for a model user, as scanning the image to find the lowest/highest bar relies on preattentive visual features.Even when scenarios are not defined by intervention category (left) but rather than different levels of change in numerical parameters (center), a visualization can rely on other preattentive visual features such as hue to guide decision makers.In contrast, a complete text report can be overly verbose (right) and less effective for decision-making.There are thus situations in which visualizations can be used instead of, or in complement to, NLG.</p>
<p>EMERGING CAPABILITIES: SIMULATION VISUALIZATION AS TEXT</p>
<p>Rationale for Using Natural Language Generation</p>
<p>Since about 70% of all human sensory receptors are in the eyes, it is no surprise that scientific visualizations are commonplace to derive insight from simulation results.New packages are regularly developed to support visualizations of simulation outputs in diverse application areas such as the large simulation datasets used in Earth system science (Li et al. 2019;Wang et al. 2019) or molecular dynamics simulation (Hildebrand, Rose, and Tiemann 2019).However, such visualizations create obstacles for individuals with visual impairments, which applies to 253 million individuals worldwide (Ackland, Resnikoff, and Bourne 2017).An academic group developing a simulation software for its own purpose or research can decide to rely extensively on visualizations to interpret results.However, when a simulation software is developed for government agencies, laws on information technology can require that the simulation be accessible for people with disabilities (including visual impairments).This applies even if the software resides solely on the intranet and is intended to be used by a team that does not currently have members with visual impairments.Section 508 of the Rehabilitation Act in the United States enacts such requirements, which are echoed in the Barrier-Free Information Technology Ordinance from Germany, and various other disability discrimination acts.An approach to ensure compliance with regulations is to provide a data table for every simulation visualization (Figure 5), as screen readers can read tables one cell at a time.This technically supports accessibility, to the same extent as a wheelchair ramp circling around a building at a steep angle would technically provide access.Reading every simulation data point is not only cumbersome, but it also prevents users from identifying patterns, just as it would be challenging to make sense of a picture if it was read one pixel at a time.Turning visualizations into written reports focusing on the main patterns would thus broaden accessibility to simulations and ensure compliance with legal requirements.</p>
<p>Figure 5: An Agent-Based Model platform for a U.S. federal client was presented with an emphasis on accessibility requirements (Huddleston et al. 2022).Providing a table for each visualization contributes to meeting these requirements, as the HTML code can be used by screen readers.</p>
<p>Core Methodological Components</p>
<p>Neural networks have been used in browser extensions to automatically decode charts (Choi et al. 2019) and newer LLMs (e.g., GPT-4) now include the ability of transforming images to text.However, this is still only an emerging capability, hence we note that significant research efforts are still needed.Recent studies can guide modelers who seek to operate LLMs in the near future to generate textual summaries of their visualizations.First, evaluations with visually impaired individuals concluded that the preferred natural language descriptions for visualizations were very reader-specific (Lundgard and Satyanarayan 2021).Modelers thus need to be cognizant of their target audience, ideally by eliciting individual preferences.Second, the usefulness of the description depends on the task that the reader seeks to accomplish.For instance, the heatmap in Figure 4 can be examined to find the best intervention (top-right corner) or to know how the outcome depends on the two control parameters (strictly decreases as a function of both parameters).Third, users may expect different verbosity levels.As exemplified by Zong et al. (2022), "at higher verbosity the screen reader announces more structural, wayfinding content (e.g. the start and end of regions)."More verbose summaries are not necessarily more effective, and a few bullet point statements can be a better starting point (Brath and Hagerman 2021).Modelers may thus consider starting their NLG prompts by requiring a few short statements instead of a comprehensive summary.</p>
<p>THE NEXT FRONTIER? EXPLAINING AND FIXING SIMULATION ERRORS</p>
<p>Rationale for Using Natural Language Generation</p>
<p>Once a simulation model can perform experiments, we need to ensure the correctness of the implementation with regards to the specification (verification) and with respect to the expected approximation of a real-world phenomenon (validation).At first, this may resemble the task of detecting and explaining implementation errors, which is increasingly studied in the nascent literature on LLMs for debugging.However, the literature on automated debugging has mostly examined the behavior of small functions, for example by using prompts to state that a function 'obviously has a bug' because it returned Out put prompt instead of Out put expected for a given Input (Liventsev et al. 2023).While locating and fixing bugs within functions is undoubtedly beneficial, verification and validation are concerned with errors that may happen at the level of the whole model.Two sub-tasks are involved: explaining why there are errors by comparing simulation outputs with expectations, and providing guidance to address these errors.We posit that LLMs may be best employed to explain errors (e.g., 'without vaccines and social distancing your population saw a reduction in COVID-19 cases but we believe that either of these interventions would have been needed to yield such results') than to identify them, which may be achieved through established statistical techniques.We also note that the guidance offered would primarily consist of generating hypotheses (Kang et al. 2023) such as 'your agents may need an exposed stage before infection'.This would already be tremendously supportive for modelers in training, by automating part of the feedback that is otherwise provided by instructors.We caution against expectations that a LLM may directly write the code logic for a model, as current LLMs write text that looks like code but does not always run and they are best suited to automate mundane tasks by writing functions that have already been encountered in their training data (Merow et al. 2023).</p>
<p>Core Methodological Components</p>
<p>We view the use of LLMs to explain and address simulation errors as the next frontier, as the tools are further from practical use than in the previous sections.Two components can play an important role in allowing an LLM to identify errors.First, the LLM needs to relate the behavior of this specific model to its general knowledge base, which can leverage an LLM's ability as a causal learner.Modelers would thus need to write a prompt that states the goal of the model and summarizes its causal pathways.For example: "A model for COVID-19 assumes that most people have not been exposed to the virus.There is a chance of getting sick upon exposure.Infected people either recover or die".Second, the LLM would benefit from contextual information on the error (e.g., which outputs are lower/higher than expected and by how much), which can be provided by existing statistical packages that compare model outputs with expected outputs.The result can complete the prompt as follows: "After one year, without vaccines or social distancing, nobody is infected by COVID-19 anymore.This is obviously wrong.Why?" Note that keywords such as 'obvious' can trigger LLMs such as GPT to give particular considerations to some statements (Liventsev et al. 2023).Providing guidance on the existence of the error is an arduous task.A LLM does not understand the structure of a simulation model, and it may also struggle to also understand how the code is related to the model's output since that can be an emerging behavior.It is likely that a LLM would need to be taught (by prompts) about the behavior of related models and then rely on transfer learning to investigate abnormalities in the proposed model.</p>
<p>CONCLUSION</p>
<p>We focused on tasks that are enabled by the emerging technology of LLMs.There are cases in which tasks that used to be performed by other technology (e.g., question-answering systems based on information retrieval rather than machine learning) are now also using LLMs, in the manner of an oracle.For instance, Q&amp;A systems have previously served to check whether the concepts and relationships of a conceptual model built by a modeling team are supported by the literature (Sandhu, Giabbanelli, and Mago 2019).The same 'yes' or 'no' questions could be asked to a GPT-based model (e.g., 'given the following documents [...], can infection from COVID-19 follow exposure to COVID-19 particles?'),but this would be a relatively minor update of a technology rather than a breakthrough for M&amp;S.Recently, researchers have shifted from checking a proposed conceptual model to building it automatically.This was first done by retrieving a corpus and repeatedly running a Q&amp;A system in the same manner as a facilitator would develop a model by talking with a subject matter expert (Davis, Jetter, and Giabbanelli 2022), but researchers are now examining the feasibility of relying entirely on LLMs to build causal graphs (Long et al. 2023;Zhang et al. 2023).We believe that this automation is an exciting step for M&amp;S, particularly if the conceptual model built by LLMs can then be mapped onto simulation building blocks to automatically create a working prototype (Schroeder et al. 2022).The guidance provided in this paper can thus be updated as progress is realized by the NLG community, to ensure that it ultimately benefits modeling and simulation.</p>
<p>Figure 2 :
2
Figure 2: Modelers can work with the literature and/or participants to create a conceptual model.Each article or participant may provide a small model on a facet of the problem, but the overall model may be larger and harder too explain due to the sheer number of constructs and relationships or complex dynamics (e.g., loops).NLG can explain the model in textual form and it requires several steps: decomposing the model into smaller inputs, converting them to textual prompts, and correcting the output (if necessary).</p>
<p>Figure 3 :
3
Figure 3: The illustrative model of Adverse Childhood Experiences (ACEs) is a part of a larger model on youth suicide, detailed in Giabbanelli et al. (2022).The implications of the baseline or 'status quo' scenario can be organized as a table, showing how the 6 constructs of the ACEs model change over discrete iterations until a final step.An intervention also consists of a table, whereby the initial values of some constructs change (due to the intervention) hence the final values may also change.</p>
<p>ACKNOWLEDGEMENTSSeveral of the reflections in this paper are the product of fruitful discussions with numerous individuals.In particular, the author wishes to thank Mr. Anish Shrestha and Mr. Tyler Gandee, whose theses help to realize the potential but also the technical challenges in using GPT-based models.The author has also benefited from stimulating exchanges of ideas with various participants (including Dr. Ameeta Agrawal and Dr. Jose Padilla) at a research seminar co-organized at Miami University with Dr Vijay Mago.
World blindness and visual impairment: despite many successes, the problem is growing. P Ackland, S Resnikoff, R Bourne, Community Eye Health. 301002017</p>
<p>Co-designing social simulation models for policy advise: lessons learned from the INFSO-SKIN study. P Ahrweiler, D Frank, N Gilbert, 2019 Spring Simulation Conference (SpringSim). IEEE2019</p>
<p>Deep reinforcement and transfer learning for abstractive text summarization: A review. A Alomari, N Idris, A Q M Sabri, I Alsmadi, Computer Speech &amp; Language. 711012762022</p>
<p>A categorical archive of ChatGPT failures. A Borji, arXiv:2302.034942023arXiv preprint</p>
<p>Automated Insights on Visualizations with Natural Language Generation. R Brath, C Hagerman, 2021 25th International Conference Information Visualisation (IV). IEEE2021</p>
<p>Computational modelling for decision-making: where, why, what, who and how. M Calder, C Craig, D Culley, R De Cani, C A Donnelly, R Douglas, B Edmonds, J Gascoigne, N Gilbert, C Hargrove, Royal Society open science. 561720962018</p>
<p>Visualizing for the non-visual: Enabling the visually impaired to use visualization. J Choi, S Jung, D G Park, J Choo, N Elmqvist, Computer Graphics Forum. Wiley Online Library201938</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.023112022arXiv preprint</p>
<p>Incorporation of implicit decisionmaker preferences in multi-objective evolutionary optimization using a multi-criteria classification method. L Cruz-Reyes, E Fernandez, P Sanchez, C A C Coello, C Gomez, Applied Soft Computing. 502017</p>
<p>Automatically Generating Scenarios from a Text Corpus: A Case Study on Electric Vehicles. C W Davis, A J Jetter, P J Giabbanelli, Sustainability. 141379382022</p>
<p>Parameter-efficient fine-tuning of large-scale pre-trained language models. N Ding, Y Qin, G Yang, F Wei, Z Yang, Y Su, S Hu, Y Chen, C.-M Chan, W Chen, Nature Machine Intelligence. 2023</p>
<p>A survey of natural language generation. C Dong, Y Li, H Gong, M Chen, J Li, Y Shen, M Yang, ACM Computing Surveys. 5582022</p>
<p>How can Machine Learning Support the Practice of Modeling and Simulation?-A Review and Directions for Future Research. M Elbattah, 2019 IEEE/ACM 23rd International Symposium on Distributed Simulation and Real Time Applications. IEEE2019</p>
<p>An interdisciplinary framework for participatory modeling design and evaluation-What makes models effective participatory decision tools?. S M Falconi, R N Palmer, Water Resources Research. 5322017</p>
<p>Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models. E Ferrara, arXiv:2304.037382023arXiv preprint</p>
<p>Predictability and surprise in large generative models. D Ganguli, D Hernandez, L Lovitt, A Askell, Y Bai, A Chen, T Conerly, N Dassarma, D Drain, N Elhage, 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. A Gatt, E Krahmer, Journal of Artificial Intelligence Research. 612018</p>
<p>Demonstration of the Feasibility of Real Time Application of Machine Learning to Production Scheduling. A Ghasemi, K E Kabak, C Heavey, 2022 Winter Simulation Conference (WSC). IEEE2022</p>
<p>Solving challenges at the interface of simulation and big data using machine learning. P J Giabbanelli, 2019 Winter Simulation Conference (WSC). IEEE2019</p>
<p>Pathways to suicide or collections of vicious cycles? Understanding the complexity of suicide through causal mapping. P J Giabbanelli, K L Rice, M C Galgoczy, N Nataraj, M M Brown, C R Harper, M D Nguyen, R Foy, 20221260</p>
<p>P J Giabbanelli, C X Vesuvala, Human Factors in Leveraging Systems Science to Shape Public Policy for Obesity: A Usability Study. 202314196</p>
<p>Computational modelling of public policy: Reflections on practice. N Gilbert, P Ahrweiler, P Barbrook-Johnson, K P Narasimhan, H Wilkinson, Journal of Artificial Societies and Social Simulation. 2112018</p>
<p>Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching. H Gong, Y Sun, X Feng, B Qin, W Bi, X Liu, T Liu, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Three questions to ask before using model outputs for decision support. V Grimm, A S Johnston, H.-H Thulke, V Forbes, P Thorbek, Nature Communications. 11149592020</p>
<p>Few-Shot Table-to-Text Generation with Prompt-based Adapter. Z Guo, M Yan, J Qi, J Zhou, Z He, Z Lin, G Zheng, X Wang, arXiv:2302.124682023arXiv preprint</p>
<p>Abstractive summarization: An overview of the state of the art. S Gupta, S K Gupta, Expert Systems with Applications. 1212019</p>
<p>Using ChatGPT to conduct a literature review. M Haman, M Školník, Accountability in Research. 2023</p>
<p>Bringing molecular dynamics simulation data into view. P W Hildebrand, A S Rose, J K Tiemann, Trends in Biochemical Sciences. 44112019</p>
<p>Design and Deployment of a Simulation Platform: Case Study of an Agent-Based Model for Youth Suicide Prevention. J Huddleston, M C Galgoczy, K A Ghumrawi, P J Giabbanelli, K L Rice, N Nataraj, M M Brown, C R Harper, C S Florence, 2022 Winter Simulation Conference (WSC). IEEE2022</p>
<p>Explainable Automated Debugging via Large Language Model-driven Scientific Debugging. S Kang, B Chen, S Yoo, J.-G Lou, arXiv:2304.021952023arXiv preprint</p>
<p>Vapor: A visualization package tailored to analyze simulation data in earth system science. S Li, S Jaroszynski, S Pearse, L Orf, J Clyne, Atmosphere. 1094882019</p>
<p>ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. Y Liu, Z Zhang, W Zhang, S Yue, X Zhao, X Cheng, Y Zhang, H Hu, arXiv:2304.076662023arXiv preprint</p>
<p>Fully Autonomous Programming with Large Language Models. V Liventsev, A Grishina, A Härmä, L Moonen, arXiv:2304.104232023arXiv preprint</p>
<p>Can large language models build causal graphs?. S Long, T Schuster, A Piché, S Research, arXiv:2303.052792023arXiv preprint</p>
<p>Accessible visualization via natural language descriptions: A four-level model of semantic content. A Lundgard, A Satyanarayan, IEEE transactions on visualization and computer graphics. 2812021</p>
<p>AI chatbots can boost scientific coding. C Merow, J M Serra-Diaz, B J Enquist, A M Wilson, Nature Ecology &amp; Evolution. 2023</p>
<p>Towards Deadlock Handling with Machine Learning in a Simulation-Based Learning Environment. M Müller, T Reggelin, I Kutsenko, H Zadek, L S Reyes-Rubiano, 2022 Winter Simulation Conference (WSC). IEEE2022</p>
<p>Symbiotic simulation system: Hybrid systems model meets big data analytics. B S Onggo, N Mustafee, A Smart, A A Juan, O Molloy, 2018 Winter Simulation Conference (WSC). IEEE2018</p>
<p>Agent-based model characterization using natural language processing. J J Padilla, D Shuttleworth, K O'brien, 2019 Winter Simulation Conference (WSC). IEEE2019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Marketing with ChatGPT: Navigating the Ethical Terrain of GPT-Based Chatbot Technology. P Rivas, L Zhao, AI. 422023</p>
<p>ChatGPT utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns. L F Rodrigues Ribeiro, M. 2023Healthcare2022MDPI11887Technische Universität Darmstadt. SallamPh. D. thesisGraph-based Approaches to Text Generation</p>
<p>From social media to expert reports: The impact of source selection on automatically validating complex conceptual models of obesity. M Sandhu, P J Giabbanelli, V K Mago, Social Computing and Social Media. Design, Human Behavior and Analytics: 11th International Conference, SCSM 2019, Held as Part of the 21st HCI International Conference. Orlando, FL, USASpringer2019. July 26-31, 20192019Proceedings, Part I 21</p>
<p>Towards reusable building blocks to develop COVID-19 simulation models. S A Schroeder, C Vendome, P J Giabbanelli, A M Montfort, 2022 Winter Simulation Conference (WSC). IEEE2022</p>
<p>Innovations in neural data-to-text generation. M Sharma, A Gogineni, N Ramakrishnan, arXiv:2207.125712022arXiv preprint</p>
<p>Automatically Explaining a Model: Using Deep Neural Networks to Generate Text From Causal Maps. A Shrestha, K Mielke, T A Nguyen, P J Giabbanelli, 2022 Winter Simulation Conference (WSC). IEEE2022</p>
<p>From Narratives to Conceptual Models via Natural Language Processing. D Shuttleworth, J Padilla, 2022 Winter Simulation Conference (WSC). IEEE2022</p>
<p>Text Summarization: A Technical Overview and Research Perspectives. K Sindhu, K Seshadri, Handbook of Intelligent Computing and Optimization for Sustainable Development. 2022</p>
<p>How to translate a verbal theory into a formal model. P E Smaldino, Social Psychology. 512020</p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. S Smith, M Patwary, B Norick, P Legresley, S Rajbhandari, J Casper, Z Liu, S Prabhumoye, G Zerveas, V Korthikanti, arXiv:2201.119902022arXiv preprint</p>
<p>Towards table-to-text generation with reasoning. L H Suadaa, H Kamigaito, K Funakoshi, M Okumura, H Takamura, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Efficient transformers: A survey. Y Tay, M Dehghani, D Bahri, D Metzler, ACM Computing Surveys. 5562022</p>
<p>ChatGPT is fun, but not an author. H Thorp, Holden, 2023</p>
<p>ChatGPT: five priorities for research. E A Dis, J Bollen, W Zuidema, R Van Rooij, C L Bockting, Nature. 61479472023</p>
<p>Flood risk management in sponge cities: The role of integrated simulation and 3D visualization. C Wang, J Hou, D Miller, I Brown, Y Jiang, International Journal of Disaster Risk Reduction. 391011392019</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. H Wang, J Li, H Wu, E Hovy, Y Sun, Engineering, J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, arXiv:2302.113822022. 2023arXiv preprintPre-Trained Language Models and Their Applications</p>
<p>What is a preattentive feature?. J M Wolfe, I S Utochkin, Current opinion in psychology. 292019</p>
<p>Table to text generation with accurate content copying. Y Yang, J Cao, Y Wen, P Zhang, Scientific reports. 111227502021</p>
<p>Improving text-to-text pre-trained models for the graph-to-text task. Z Yang, A Einolghozati, H Inan, K Diedrick, A Fan, P Donmez, S Gupta, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)2020</p>
<p>Understanding Causality with Large Language Models: Feasibility and Opportunities. C Zhang, S Bauer, P Bennett, J Gao, W Gong, A Hilmkil, J Jennings, C Ma, T Minka, N Pawlowski, arXiv:2304.055242023arXiv preprint</p>
<p>Enhancing RDF Verbalization with Descriptive and Relational Knowledge. F Zhang, M Zhang, S Liu, Y Sun, N Duan, ACM Transactions on Asian and Low-Resource Language Information Processing. 2023</p>
<p>ChatGPT: Potential, prospects, and limitations. J Zhou, P Ke, X Qiu, M Huang, J Zhang, Frontiers of Information Technology &amp; Electronic Engineering. 2023</p>
<p>ChatGPT and environmental research. J.-J Zhu, J Jiang, M Yang, Z J Ren, Environmental Science &amp; Technology. 2023</p>
<p>Rich screen reader experiences for accessible data visualization. J Zong, C Lee, A Lundgard, J Jang, D Hajas, A Satyanarayan, Computer Graphics Forum. Wiley Online Library202241</p>            </div>
        </div>

    </div>
</body>
</html>
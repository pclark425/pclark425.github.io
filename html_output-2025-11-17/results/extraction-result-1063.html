<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1063 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1063</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1063</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-9468634b94fbded8b3362bdf230ab2becba0c0ef</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9468634b94fbded8b3362bdf230ab2becba0c0ef" target="_blank">Active learning of inverse models with intrinsically motivated goal exploration in robots</a></p>
                <p><strong>Paper Venue:</strong> Robotics Auton. Syst.</p>
                <p><strong>Paper TL;DR:</strong> TheSAGG-RIAC architecture is introduced as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots and shows that exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1063.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1063.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGG-RIAC (redundant arm)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) — redundant arm experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied robotic learning system that actively self-generates goals in a low-dimensional task space and uses competence-progress-driven selection to learn inverse kinematics in a high-dimensional redundant manipulator by combining goal babbling, local regression and local goal-directed optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-DOF redundant manipulator using SAGG-RIAC</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Physical robotic arm that incrementally learns forward and inverse kinematics via active goal exploration (goal babbling). High-level: competence-progress driven goal selection (SAGG-RIAC). Low-level: incremental local regression (ANN nearest neighbours) to build local Jacobians and Moore–Penrose pseudo-inverse for local inverse solutions, plus a goal-directed optimization/SSA-like local exploration when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (robotic agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Manipulator workspace / operational task space</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous Euclidean end-effector workspace (Y) for a 7-DOF arm mapped from high-dimensional joint/actuator space (S, n=7). Complexity arises from redundancy (n>m), high-dimensional actuator space, local non-linearities and possible singularities; the reachable subspace of the full workspace may be only a subset of the larger goal space; starting/context states (joint configurations) vary (setpoints) producing variation in the mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Dimensionality and redundancy: joint space dimensionality n (here n=7) vs task-space dimensionality m (example m=2); local non-linear Jacobian J(α) varying across α; presence of singularities and non-convex inverse solutions; task complexity operationalized as learning difficulty to reduce distance-to-goal in task space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (high-dimensional redundant actuator/control space; non-linear, potentially singular mappings)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation characterized by changing starting contexts s (setpoints), noise in actuation, and by existence of subregions of task space that are reachable/unreachable (unknown limits); measured implicitly via observed competence across regions and by the discovery of previously unreached points (conservation of reached points) and competence progress time-series.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium — starting contexts vary during learning; reachable vs unreachable subregions create effective variation across the task field</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Competence C(y_g,y_f,y_start) = - D(y_g,y_f) / D(y_start,y_g) (normalized negative distance); competence progress (absolute derivative over sliding window) used as interest measure; also sample complexity / speed of learning and generalization to novel goals reported qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: high actuator-space complexity (high-dimensional redundant control) makes direct exploration in actuator space inefficient; exploring a typically lower-dimensional task space (goal babbling) greatly reduces sample complexity. Variation in the environment (unlearnable subregions, non-stationarity, changing body geometry) undermines traditional active learning criteria that assume learnability everywhere; competence-progress-driven goal selection (which focuses on regions with increasing/decreasing competence) addresses this by directing exploration toward regions of appropriate learning complexity and rediscovering regions after changes.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active goal exploration (goal babbling) driven by competence progress (SAGG-RIAC), local incremental regression (ANN nearest neighbours), local Jacobian estimation and pseudo-inverse; periodic resets to a rest position (reduction of initiation set); region splitting and interest-based sampling; optional SSA-like local optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative results reported: goal-space exploration (SAGG-RIAC) achieves faster learning and better generalization to reach novel goals in the task space than actuator-space exploration and than random/standard active motor babbling; the architecture infers motor policy parameters for novel goals via regression on previously learned correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: significantly faster than direct actuator-space exploration and more sample-efficient than random goal selection or standard active motor-babbling; exact number of interactions not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Goal-space (task-space) active exploration is much faster than actuator-space exploration for inverse model learning in redundant manipulators; 2) competence-progress-driven goal selection creates developmental trajectories that focus exploration on tasks of increasing learning complexity; 3) the architecture discovers which parts of the task space are reachable and which are not; 4) local regression with pseudo-inverse and reset heuristics (reduced initiation set) make inverse learning tractable in a high-dimensional redundant arm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active learning of inverse models with intrinsically motivated goal exploration in robots', 'publication_date_yy_mm': '2013-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1063.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1063.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGG-RIAC (quadruped)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) — quadruped locomotion experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotic quadruped controlled with parameterized motor primitives (motor synergies) that learns omnidirectional locomotion by self-generating goals in a low-dimensional task space and selecting goals using competence progress to update mappings from motor primitive parameters to displacement/orientation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Quadruped robot with parameterized motor primitives (CPG-like motor synergies)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Physical quadruped robot controlled by parameterized motor synergies (motor primitives) with ~24 dimensional parameters (frequency/amplitude per joint). Learning maps motor synergy parameters θ to task outcomes y (position and orientation displacement after executing the primitive) using memory-based regression and SAGG-RIAC high-level goal selection based on competence progress.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (robotic agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Locomotion task space (displacement and orientation after primitive execution)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environment consists of the robot executing motor primitives from a fixed rest context and observing resulting displacement/orientation. Complexity stems from moderately high-dimensional action parameter space (e.g., 24-parameter synergy), non-linear body-environment dynamics, coupling between joints, and many possible motor parameter combinations producing redundant or overlapping effects. The task space itself (displacement/orientation) is lower-dimensional than the motor parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Action-parameter dimensionality (e.g., 24 parameters for motor synergies), nonlinearity and multi-modal mapping θ→y, redundancy (many θ map to similar y), and requirement to learn useful mappings for omnidirectional locomotion.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (moderately high action-parameter dimensionality with non-linear dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation characterized by the distribution of motor-primitive parameters tried and by the range of task outcomes; context is fixed (reset to rest position) so environmental/contextual variation is low, but internal variation arises from motor parameter diversity and possible changes in body/environment dynamics (noise/non-stationarity).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium (fixed context reduces context variation; wide motor-parameter variety introduces internal variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Competence (distance to self-generated goals / normalized distance), competence progress (interest), speed of learning (sample complexity), and ability to cover varied directions in task space (capability to reach many goals).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed qualitatively: using motor synergies reduces control dimensionality but leaves a high-dimensional parameter space; because the task space is lower-dimensional, active goal exploration using competence progress efficiently focuses exploration on useful motor-parameter subregions. Fixed context (reset) reduces variation making learning of θ→y mapping easier; when environment/body change, competence-decreases trigger re-exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active, competence-based goal self-generation (SAGG-RIAC), motor primitive parameter exploration (goal-directed optimization or stochastic optimization), reset to rest position between trials (fixed context), memory-based regression for mapping θ→y.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitatively, the method allows efficient learning of omnidirectional locomotion primitives and generalization to reach a distribution of displacements/orientations; reported as significantly more efficient than random or standard motor babbling approaches (exact numbers not provided in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: more sample efficient than undirected exploration; exact interaction counts not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Motor synergies reduce control dimensionality but leave a large parameter space—goal-space active exploration helps discover useful regions faster; 2) competence progress selection focuses exploration on subregions where learnability is improving; 3) resetting to a rest context simplifies the mapping and aids learning; 4) SAGG-RIAC is suitable when task space is lower-dimensional than control space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active learning of inverse models with intrinsically motivated goal exploration in robots', 'publication_date_yy_mm': '2013-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1063.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1063.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGG-RIAC (fishing rod)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) — fishing rod (flexible wire) experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotic arm learning to control a flexible fishing rod (underactuated, compliant, flexible dynamics) using active goal generation and competence-progress-driven exploration to learn inverse mappings from motor policy parameters to resulting float positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Robotic arm controlling a fishing rod with flexible wire using SAGG-RIAC</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Physical robotic arm interacting with a flexible fishing rod (complex, compliant tool) that learns which motor policy parameters produce desired float positions in the water by self-generating goals and optimizing local motor parameters, leveraging memory-based regression and competence progress for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (robotic agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Fishing task (float position placement on water surface)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Highly non-linear, underactuated, compliant dynamics due to flexible wire and coupled arm-tool interactions; task space is end-effector/float positions on water (2D or 3D), while control/policy parameter space can be high-dimensional; environment complexity arises from flexible dynamics, time dependencies, and non-analytic forward/inverse models.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity characterized by underactuated and compliant dynamics (flexible wire), temporal dynamics of the primitive (sequence of actions), high non-linearity and sensitivity to initial conditions; task difficulty measured as ability to place float within tolerance of desired position.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (flexible, underactuated dynamics; temporally extended motor policies)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation arises from physical variability of the flexible wire dynamics, sensitivity to initial conditions, and potentially changing environmental conditions (e.g., water disturbances). Measured implicitly by competence across sampled goals and by using subgoals and conservation of reached states to bootstrap exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (physical dynamics and temporal sensitivity create substantial variation and unpredictability across trials)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Competence (normalized distance-to-goal), competence progress as interest to choose future goals, and ability to discover reachable parts of task space (coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit discussion: flexible, underactuated tool dynamics increase task complexity and produce regions of the task space that may be initially unreachable; competence-progress-driven goal selection and addition of subgoals/conservation of reached points bootstrap discovery and create a growing region of competence around initially reachable areas — thus the architecture handles high complexity and high variation by focusing exploration where competence is improving and by subdividing tasks into subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active goal generation with competence-progress selection (SAGG-RIAC), use of subgoals and conservation of reached points to increase feedback density, local policy optimization (stochastic optimization variants) and memory-based regression to infer policy→goal mappings; rest resets used where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative: architecture permits discovery of which parts of the task space can be learned and generalizes motor parameter inferences to novel goals within discovered reachable regions; claimed to be significantly more efficient than random or standard active methods (no numerical values in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: improved sample efficiency via goal-space active exploration and subgoal heuristics; exact counts not reported in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) SAGG-RIAC can cope with complex flexible/tool dynamics by actively generating goals and leveraging competence progress; 2) adding subgoals and conserving reached states increases feedback and bootstrap discovery of reachable regions; 3) the approach helps discover and delimit reachable vs unreachable parts of the task space and drives developmental trajectories toward progressively more complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active learning of inverse models with intrinsically motivated goal exploration in robots', 'publication_date_yy_mm': '2013-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1063",
    "paper_id": "paper-9468634b94fbded8b3362bdf230ab2becba0c0ef",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "SAGG-RIAC (redundant arm)",
            "name_full": "Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) — redundant arm experiment",
            "brief_description": "An embodied robotic learning system that actively self-generates goals in a low-dimensional task space and uses competence-progress-driven selection to learn inverse kinematics in a high-dimensional redundant manipulator by combining goal babbling, local regression and local goal-directed optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-DOF redundant manipulator using SAGG-RIAC",
            "agent_description": "Physical robotic arm that incrementally learns forward and inverse kinematics via active goal exploration (goal babbling). High-level: competence-progress driven goal selection (SAGG-RIAC). Low-level: incremental local regression (ANN nearest neighbours) to build local Jacobians and Moore–Penrose pseudo-inverse for local inverse solutions, plus a goal-directed optimization/SSA-like local exploration when needed.",
            "agent_type": "physical robot (robotic agent)",
            "environment_name": "Manipulator workspace / operational task space",
            "environment_description": "Continuous Euclidean end-effector workspace (Y) for a 7-DOF arm mapped from high-dimensional joint/actuator space (S, n=7). Complexity arises from redundancy (n&gt;m), high-dimensional actuator space, local non-linearities and possible singularities; the reachable subspace of the full workspace may be only a subset of the larger goal space; starting/context states (joint configurations) vary (setpoints) producing variation in the mapping.",
            "complexity_measure": "Dimensionality and redundancy: joint space dimensionality n (here n=7) vs task-space dimensionality m (example m=2); local non-linear Jacobian J(α) varying across α; presence of singularities and non-convex inverse solutions; task complexity operationalized as learning difficulty to reduce distance-to-goal in task space.",
            "complexity_level": "high (high-dimensional redundant actuator/control space; non-linear, potentially singular mappings)",
            "variation_measure": "Variation characterized by changing starting contexts s (setpoints), noise in actuation, and by existence of subregions of task space that are reachable/unreachable (unknown limits); measured implicitly via observed competence across regions and by the discovery of previously unreached points (conservation of reached points) and competence progress time-series.",
            "variation_level": "medium — starting contexts vary during learning; reachable vs unreachable subregions create effective variation across the task field",
            "performance_metric": "Competence C(y_g,y_f,y_start) = - D(y_g,y_f) / D(y_start,y_g) (normalized negative distance); competence progress (absolute derivative over sliding window) used as interest measure; also sample complexity / speed of learning and generalization to novel goals reported qualitatively.",
            "performance_value": null,
            "complexity_variation_relationship": "Explicitly discussed: high actuator-space complexity (high-dimensional redundant control) makes direct exploration in actuator space inefficient; exploring a typically lower-dimensional task space (goal babbling) greatly reduces sample complexity. Variation in the environment (unlearnable subregions, non-stationarity, changing body geometry) undermines traditional active learning criteria that assume learnability everywhere; competence-progress-driven goal selection (which focuses on regions with increasing/decreasing competence) addresses this by directing exploration toward regions of appropriate learning complexity and rediscovering regions after changes.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active goal exploration (goal babbling) driven by competence progress (SAGG-RIAC), local incremental regression (ANN nearest neighbours), local Jacobian estimation and pseudo-inverse; periodic resets to a rest position (reduction of initiation set); region splitting and interest-based sampling; optional SSA-like local optimization.",
            "generalization_tested": true,
            "generalization_results": "Qualitative results reported: goal-space exploration (SAGG-RIAC) achieves faster learning and better generalization to reach novel goals in the task space than actuator-space exploration and than random/standard active motor babbling; the architecture infers motor policy parameters for novel goals via regression on previously learned correspondences.",
            "sample_efficiency": "Qualitative: significantly faster than direct actuator-space exploration and more sample-efficient than random goal selection or standard active motor-babbling; exact number of interactions not provided in the excerpt.",
            "key_findings": "1) Goal-space (task-space) active exploration is much faster than actuator-space exploration for inverse model learning in redundant manipulators; 2) competence-progress-driven goal selection creates developmental trajectories that focus exploration on tasks of increasing learning complexity; 3) the architecture discovers which parts of the task space are reachable and which are not; 4) local regression with pseudo-inverse and reset heuristics (reduced initiation set) make inverse learning tractable in a high-dimensional redundant arm.",
            "uuid": "e1063.0",
            "source_info": {
                "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
                "publication_date_yy_mm": "2013-01"
            }
        },
        {
            "name_short": "SAGG-RIAC (quadruped)",
            "name_full": "Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) — quadruped locomotion experiment",
            "brief_description": "A robotic quadruped controlled with parameterized motor primitives (motor synergies) that learns omnidirectional locomotion by self-generating goals in a low-dimensional task space and selecting goals using competence progress to update mappings from motor primitive parameters to displacement/orientation outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Quadruped robot with parameterized motor primitives (CPG-like motor synergies)",
            "agent_description": "Physical quadruped robot controlled by parameterized motor synergies (motor primitives) with ~24 dimensional parameters (frequency/amplitude per joint). Learning maps motor synergy parameters θ to task outcomes y (position and orientation displacement after executing the primitive) using memory-based regression and SAGG-RIAC high-level goal selection based on competence progress.",
            "agent_type": "physical robot (robotic agent)",
            "environment_name": "Locomotion task space (displacement and orientation after primitive execution)",
            "environment_description": "Environment consists of the robot executing motor primitives from a fixed rest context and observing resulting displacement/orientation. Complexity stems from moderately high-dimensional action parameter space (e.g., 24-parameter synergy), non-linear body-environment dynamics, coupling between joints, and many possible motor parameter combinations producing redundant or overlapping effects. The task space itself (displacement/orientation) is lower-dimensional than the motor parameter space.",
            "complexity_measure": "Action-parameter dimensionality (e.g., 24 parameters for motor synergies), nonlinearity and multi-modal mapping θ→y, redundancy (many θ map to similar y), and requirement to learn useful mappings for omnidirectional locomotion.",
            "complexity_level": "medium-high (moderately high action-parameter dimensionality with non-linear dynamics)",
            "variation_measure": "Variation characterized by the distribution of motor-primitive parameters tried and by the range of task outcomes; context is fixed (reset to rest position) so environmental/contextual variation is low, but internal variation arises from motor parameter diversity and possible changes in body/environment dynamics (noise/non-stationarity).",
            "variation_level": "low-to-medium (fixed context reduces context variation; wide motor-parameter variety introduces internal variation)",
            "performance_metric": "Competence (distance to self-generated goals / normalized distance), competence progress (interest), speed of learning (sample complexity), and ability to cover varied directions in task space (capability to reach many goals).",
            "performance_value": null,
            "complexity_variation_relationship": "Discussed qualitatively: using motor synergies reduces control dimensionality but leaves a high-dimensional parameter space; because the task space is lower-dimensional, active goal exploration using competence progress efficiently focuses exploration on useful motor-parameter subregions. Fixed context (reset) reduces variation making learning of θ→y mapping easier; when environment/body change, competence-decreases trigger re-exploration.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active, competence-based goal self-generation (SAGG-RIAC), motor primitive parameter exploration (goal-directed optimization or stochastic optimization), reset to rest position between trials (fixed context), memory-based regression for mapping θ→y.",
            "generalization_tested": true,
            "generalization_results": "Qualitatively, the method allows efficient learning of omnidirectional locomotion primitives and generalization to reach a distribution of displacements/orientations; reported as significantly more efficient than random or standard motor babbling approaches (exact numbers not provided in excerpt).",
            "sample_efficiency": "Qualitative: more sample efficient than undirected exploration; exact interaction counts not provided in the excerpt.",
            "key_findings": "1) Motor synergies reduce control dimensionality but leave a large parameter space—goal-space active exploration helps discover useful regions faster; 2) competence progress selection focuses exploration on subregions where learnability is improving; 3) resetting to a rest context simplifies the mapping and aids learning; 4) SAGG-RIAC is suitable when task space is lower-dimensional than control space.",
            "uuid": "e1063.1",
            "source_info": {
                "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
                "publication_date_yy_mm": "2013-01"
            }
        },
        {
            "name_short": "SAGG-RIAC (fishing rod)",
            "name_full": "Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) — fishing rod (flexible wire) experiment",
            "brief_description": "A robotic arm learning to control a flexible fishing rod (underactuated, compliant, flexible dynamics) using active goal generation and competence-progress-driven exploration to learn inverse mappings from motor policy parameters to resulting float positions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Robotic arm controlling a fishing rod with flexible wire using SAGG-RIAC",
            "agent_description": "Physical robotic arm interacting with a flexible fishing rod (complex, compliant tool) that learns which motor policy parameters produce desired float positions in the water by self-generating goals and optimizing local motor parameters, leveraging memory-based regression and competence progress for exploration.",
            "agent_type": "physical robot (robotic agent)",
            "environment_name": "Fishing task (float position placement on water surface)",
            "environment_description": "Highly non-linear, underactuated, compliant dynamics due to flexible wire and coupled arm-tool interactions; task space is end-effector/float positions on water (2D or 3D), while control/policy parameter space can be high-dimensional; environment complexity arises from flexible dynamics, time dependencies, and non-analytic forward/inverse models.",
            "complexity_measure": "Complexity characterized by underactuated and compliant dynamics (flexible wire), temporal dynamics of the primitive (sequence of actions), high non-linearity and sensitivity to initial conditions; task difficulty measured as ability to place float within tolerance of desired position.",
            "complexity_level": "high (flexible, underactuated dynamics; temporally extended motor policies)",
            "variation_measure": "Variation arises from physical variability of the flexible wire dynamics, sensitivity to initial conditions, and potentially changing environmental conditions (e.g., water disturbances). Measured implicitly by competence across sampled goals and by using subgoals and conservation of reached states to bootstrap exploration.",
            "variation_level": "medium-to-high (physical dynamics and temporal sensitivity create substantial variation and unpredictability across trials)",
            "performance_metric": "Competence (normalized distance-to-goal), competence progress as interest to choose future goals, and ability to discover reachable parts of task space (coverage).",
            "performance_value": null,
            "complexity_variation_relationship": "Explicit discussion: flexible, underactuated tool dynamics increase task complexity and produce regions of the task space that may be initially unreachable; competence-progress-driven goal selection and addition of subgoals/conservation of reached points bootstrap discovery and create a growing region of competence around initially reachable areas — thus the architecture handles high complexity and high variation by focusing exploration where competence is improving and by subdividing tasks into subgoals.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active goal generation with competence-progress selection (SAGG-RIAC), use of subgoals and conservation of reached points to increase feedback density, local policy optimization (stochastic optimization variants) and memory-based regression to infer policy→goal mappings; rest resets used where applicable.",
            "generalization_tested": true,
            "generalization_results": "Qualitative: architecture permits discovery of which parts of the task space can be learned and generalizes motor parameter inferences to novel goals within discovered reachable regions; claimed to be significantly more efficient than random or standard active methods (no numerical values in excerpt).",
            "sample_efficiency": "Qualitative: improved sample efficiency via goal-space active exploration and subgoal heuristics; exact counts not reported in the provided text.",
            "key_findings": "1) SAGG-RIAC can cope with complex flexible/tool dynamics by actively generating goals and leveraging competence progress; 2) adding subgoals and conserving reached states increases feedback and bootstrap discovery of reachable regions; 3) the approach helps discover and delimit reachable vs unreachable parts of the task space and drives developmental trajectories toward progressively more complex tasks.",
            "uuid": "e1063.2",
            "source_info": {
                "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
                "publication_date_yy_mm": "2013-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01279,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots *</h1>
<p>Adrien Baranes and Pierre-Yves Oudeyer<br>INRIA and Ensta-ParisTech, France</p>
<p>November 27, 2024</p>
<h2>Highlights:</h2>
<p>1) SAGG-RIAC is an architecture for active learning of inverse models in highdimensional redundant spaces
2) This allows a robot to learn efficiently distributions of parameterized motor policies that solve a corresponding distribution of parameterized tasks
3) Active sampling of parameterized tasks, called active goal exploration, can be significantly faster than direct active sampling of parameterized policies
4) Active developmental exploration, based on competence progress, autonomously drives the system to progressively explore tasks of increasing learning complexity.</p>
<h4>Abstract</h4>
<p>We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.</p>
<p>Keywords: Active Learning, Competence Based Intrinsic Motivation, Curiosity-Driven Task Space Exploration, Inverse Models, Goal Babbling, Autonomous Motor Learning, Developmental Robotics, Motor Development.</p>
<h1>1 Motor Learning and Exploration of Forward and Inverse Models</h1>
<p>To operate robustly and adaptively in the real world, robots need to know how to predict the consequences of their actions (called here forward models, mapping typically $X=\left(S, \pi_{\theta}\right)$, where $S$ is the state of a robot and $\pi_{\theta}: S \rightarrow A$ is a parameterized action policy, to the space of effect, or task space, $Y$ ). Reversely, they need to be able to compute the action policies that can generate given effects (called here inverse models, $(S, Y) \rightarrow \pi_{\theta}$ ). These models can be quite varied, for example mapping joint angles to hand position in the visual field, oscillation of the legs to body translation, movement of the hand in the visual field to movement of the end point of a tool, or properties of a hand tap an object to the sound it produces. Some of these models can be analytically elaborated by an engineer and provided to a robot (e.g. forward and inverse kinematics of a rigid body robot). But in many cases, this is impossible either because the physical properties of the body itself cannot be easily modeled (e.g. compliant bodies with soft materials), or because it is impossible to anticipate all possible objects the robot might interact with, and thus the properties of objects. More generally, it is impossible to model a priori all the possible effects a robot can produce on its environment, especially when robots are targeted to interact with in everyday human environments, such as in assistive robotics. As a consequence, learning these models through experience becomes necessary. Yet, this poses highly difficult technical challenges, due in particular to the combination of the following facts: 1) these models are often high-dimensional, continuous and highly non-stationary spatially, and sometimes temporally; 2) learning examples have to be collected autonomously and incrementally by robots; 3) learning, as we will detail below, can happen either through self-experimentation or observation, and both of these takes significant</p>
<p>physical time in the real world. Thus, the number of training examples that can be collected in a life-time is strongly limited with regards to the size and complexity of the spaces. Advanced statistical learning techniques dedicated to incremental high-dimensional regression have been elaborated recently, such as $[107,72]$. Yet, these regression mechanisms are efficient only if the quality and quantity of data is high enough, which is not the case when using unconstrained exploration such as random exploration. Fundamental complementary mechanisms for guiding and constraining autonomous exploration and data collection for learning are needed.</p>
<p>In this article, we present a particular approach to address constrained exploration and learning of inverse models in robots, based on an active learning process inspired by mechanisms of intrinsically motivated learning and exploration in humans. As we will explain, the approach studies the combination of two principles for learning efficiently inverse models in high-dimensional redundant continuous spaces:</p>
<ul>
<li>Active goal/task exploration in a parameterized task space: The architecture makes the robot sample actively novel parameterized tasks in the task space, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. This allows to leverage the redundancies of the sensorimotor mapping, leading the system to explore densely only subregions of the space of action policies that are enough to achieve all possible effects. Thus, it does not need to learn a complete forward model and contrasts with approaches that directly sample action policy parameters and observe their effects in the task space. The system also leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters.</li>
<li>Interestingness as empirically evaluated competence progress: The measure of interestingness for a given goal/task is based on competence progress empirically evaluated, i.e. how previous attempts of low-level optimization directed at similar goals allowed to improve the capability of the robot to reach these goals.</li>
</ul>
<p>In the rest of the section, we review various related approaches to constraining exploration for motor learning.</p>
<h1>1.1 Constraining the Exploration</h1>
<p>A common way to carry out exploration is to use a set of constraints on guiding mechanisms and maximally reduce the size and/or dimensionality of explored spaces. Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process $[1,63,18,24,59,26,60]$. Typically, a robot teacher manually interacts with the robot by showing it a few</p>
<p>behaviors corresponding to a desired movement or goal that it will then have to reproduce. This strategy prevents the robot from performing any autonomous exploration of its space and requires an attentive demonstrator. Some other techniques allow more freedom to the human teacher and the robot by allowing the robot to explore. This is typically what happens in the reinforcement learning (RL) framework where no demonstration is originally required and only a goal has to be fixed (as a reward) by the engineer who conceives the system [114, 84, 117]. Nevertheless, when the robot evolves in high-dimensional and large spaces, the exploration still has to be constrained. For instance, studies presented in [80] combine RL with the framework of learning by demonstration. In their experiments, an engineer has to first define a specific goal in a task space as a handcrafted reward function, then, a human demonstrator provides a few examples of successful motor policies to reach that goal, which is then used to initialize an optimization procedure. The Shifting Setpoint Algorithm (SSA) introduced by Schaal and Atkeson [92] proposes another way to constrain the exploration process. Once a goal fixed in an handcrafted manner, a progressive exploration process is proposed: the system explores the world gradually from the start position and toward the goal by creating a local model around the current position and shifting in direction of the goal once this model is reliable enough, and so on. These kinds of techniques therefore restrain the exploration to narrow tubes of data targeted at learning specific tasks/goals that have to be defined by a human, either the programmer or a non-engineer demonstrator.</p>
<p>These methods are efficient and useful in many cases. Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.e. having a robot learn to generate in a controlled manner many effects rather than only a single goal), it is not conceivable that a human being interacts with a robot at each instant or that an engineer designs and tunes a specific reward function for each novel task to be learned. For this reason, it is necessary to introduce mechanisms driving the learning and exploration of robots in an autonomous manner.</p>
<h1>1.2 Driving Autonomous Exploration</h1>
<p>Active learning algorithms can be considered as organized and constrained selfexploration processes $[41,30,89,105,60]$. In the regression setting, they are used to learn a regression mapping between an input space $X$ and an output space $Y$ while minimizing the sample complexity, i.e. with a minimal number of examples necessary to reach a given performance level. These methods, typically beginning by random and sparse exploration, build meta-models of performances of the motor learning mechanisms and concurrently drive the exploration in various sub-spaces for which a notion of interest is defined, often consisting in variants of expected informational gain. A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-</p>
<p>ment [51], or maximal uncertainty of the model [119] among others. There have been active-extensions to most of the existent learning methods, e.g. logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57]. Only very recently have these approaches been applied to robotic problems, and even more recently if we consider examples with real robots. Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.</p>
<p>Another approach to exploration came from an initially different problem, that of understanding how robots could achieve cumulative and open-ended learning autonomously. This raised the question of the task-independent mechanisms that may allow a robot to get interested in practicing skills and learn new tasks that were not specified at design time. Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].</p>
<p>As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries. Yet, in spite of these similarities between work in active learning and intrinsic motivation, these two strands of approaches often differ in their underlying assumptions and constraints, leading to sometimes very different active learning algorithms. In many active learning models, one often assumes that it is possible to learn a model of the complete world within lifetime, and/or that the world is learnable everywhere, and/or where noise is homogeneous everywhere. Given those assumptions, heuristics based on the exploration of parts of the space where the learned model has maximal uncertainties or where its prediction are maximally wrong are often very efficient. Yet, these assumptions typically do not hold in real world robots in an unconstrained environment: the sensorimotor spaces, including the body dynamics and its interactions with the external world, are simply much too large to be learned entirely in a life time; there are typically subspaces which are unlearnable due to inadequate learning biases or unobservable variables; noise can be strongly homogeneous. Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101]. This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any</p>
<p>external reward $[96,12,109,110,65,68,94,97,48,7,76,64,40]$. Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77]. For instance, the architecture called RobustIntelligent Adaptive Curiosity (RIAC) [8], which is a refinement of the IAC architecture which was elaborated for open-ended learning of affordances and skills in real robots [77], defines the interestingness of a sensorimotor subspace by the velocity of the decrease of the errors made by the robot when predicting the consequences of its actions, given a context, within this subspace. As shown in $[77,8]$, it biases the system to explore subspaces of progressively increasing complexity.</p>
<p>Nevertheless, RIAC and similar "knowledge based" approaches (see [74]) have some limitations: first, while they can deal with the spatial or temporal non-stationarity of the model to be learned, they face the curse-of-dimensionality and can only be efficient when considering a moderate number of control dimensions (e.g. up to $9 / 10$ ). Indeed, as many other active learning methods, RIAC needs a certain level of sampling density in order to extract and compare the interest of different areas of the space. Also, because performing these measure costs time, this approach becomes more and more inefficient as the dimensionality of the control space grows [19]. Second, they focus on the active choice of motor commands and measures of their consequences, which allows learning forward models that can be re-used as a side effect for achieving goals/tasks through online inversion: this approach is sub-optimal in many cases since it explores in the high-dimensional space of motor commands and consider the achievement of tasks only indirectly.</p>
<p>A more efficient approach consists in directly actively exploring task spaces, which are also often much lower-dimensional, by actively self-generating goals within those task spaces, and then learn associated local coupled forward/inverse models that are useful to achieve those goals. Yet, as we will see, the process is not as straightforward as learning the forward model, since because of the space redundancy it is not possible to learn directly the inverse model (and this is the reason why learning the forward model and then only inversing it has often been achieved). In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in $[20,8,55,108]$.</p>
<h1>1.3 Driving the Exploration at a Higher Level</h1>
<p>In a framework where a system should be able to learn to perform a maximal amount of different tasks (here this means achieving many goals/tasks in a parameterized task space) before focusing on different ways to perform the same tasks (here this means finding several alternative actions to achieve the same goal), knowledge-based exploration techniques like RIAC cannot be efficient in robots with redundant forward models. Indeed, they typically direct a robotic</p>
<p>system to spend copious amounts of time exploring variations of action policies that produce the same effect, at the disadvantage of exploring other actions that might produce different outcomes, useful to achieve more tasks. An example of this is learning 10 ways to push a ball forward instead of learning to push a ball in 10 different directions. One way to address this issue is to take inspiration infant's motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123]. Once a goal/task is chosen, the system would then try to reach it with a lower-level goal-reaching architecture typically based on coupled inverse and forward models, which might include a lower-level goal-directed active exploration mechanism.</p>
<p>Two other developmental constraints, playing an important role in infant motor development, and presented in the experimentations of this paper, can also play an important role when considering such a task-level exploration process. First, we use motor synergies which have been shown as simplifying motor learning by reducing the number of dimensions for control (nevertheless, even with motor synergies, the dimensionality of the control space can easily go over several dozens, and exploration still needs to be organized). These motor synergies are often encoded using Central Pattern Generators (CPG) $[49,36,34,58,15]$ or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning. Second, we will use a heuristic inspired by observations of infants who sometimes prepare their reaching movements by starting from a same rest position [17], by resetting the robot to such a rest position, which allows reducing the set of starting states used to perform a task.</p>
<p>In this paper, we propose an approach which allows us to transpose some of the basic ideas of IAC and RIAC architectures, combined with ideas from the SSA algorithm, into a multi-level active learning architecture called SelfAdaptive Goal Generation RIAC algorithm (SAGG-RIAC) (an outline and initial evaluation of this architecture was presented in [9]). Unlike RIAC which was made for active learning of forward models mapping action policy parametes to effects in a task space, we show that this new algorithm allows for efficient learning of inverse models mapping parameters of tasks to parameters of action policies that allow to achieve these tasks in redundant robots. This is achieved through active sampling of novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. This takes advantage of both the typical redundancy of the mapping and of the fact that very often the dimensionality of the task space considered is much smaller than the dimensionality of motor primitives/action parameter space. Such an architecture also leverages both techniques for optimizing action policy parameters for a single predefined tasks (e.g. [79, 112]), as well as regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters (e.g. [20, 8, 55, 108]). While</p>
<p>approaches such as $[79,112]$ or $[20,55,108]$ do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.</p>
<p>SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127]. In a competence based active exploration mechanism, according to the definition [74], the robot is pushed to perform an active exploration in the goal/operational space as opposed to motor babbling in the actuator space.</p>
<p>Several strands of previous research have began exploration various aspects of this family of mechanisms. First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102]. While the high expressivity of these formalisms allows in principle to tackle a wide diversity of problems, they were not designed nor experimented for the particular family of problems of learning high-dimensional continuous models in robotics. While SAGG-RIAC also actively and adaptively self-generates goals, this is achieved with a formalism based on applied mathematics and dedicated to the problem of learning inverse models in continuous redundant spaces.</p>
<p>Measures of interestingness based on a measure of competence to perform a skill were studied in [6], as well as in [94] where a selector chooses to perform different skills depending on the temporal difference error to reach each skill. The study proposed in [111] is based on the competence progress, which they use to select goals in a pre-specified set of skills considered in a discrete world. As we will show, SAGG-RIAC also uses competence progress, but targets learning in high-dimensional continuous robot spaces.</p>
<p>A mechanism for passive exploration in the task space for learning inverse models in high-dimensional continuous robotics spaces was presented in [85, 86], where a robot has to learn its arm inverse kinematics while trying to reach in a preset order goals put on a pre-specified grid informing the robot about the limits of its reachable space. In SAGG-RIAC exploration is actively driven in the task space, allowing the learning process to minimize its sample complexity, and as we will show, to reach a high-level of performances in generalization and to discover automatically its own limits of reachability.</p>
<p>In the following sections we introduce the global architecture and formalization of the Self-Adaptive Goal-Generation SAGG-RIAC architecture. Then, we study experimentally its capabilities to allow a robot efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals, and in the context of three experimental setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible</p>
<p>wire. More precisely, we focus on the following aspects and contributions of the architecture:</p>
<ul>
<li>SAGG-RIAC creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity of learnability;</li>
<li>Drives the learning of a high variety of parameterized tasks (i.e. capability to reach various regions of the goal/task space) instead of numerous ways to perform the same task;</li>
<li>Allows learning fields of tasks in high-dimensional high-volume control spaces as long as the task space is low-dimensional (it can be high-volume);</li>
<li>Allows learning in task-spaces where only small and initially unknown subparts are reachable;</li>
<li>Drives the learning of inverse models of highly-redundant robots with different body schemas;</li>
<li>Guides the self-discovery of the limits of what the robot can achieve in its task space;</li>
<li>Allows improving significantly the quality of learned inverse models in terms of speed of learning and generalization performance to reach goals in the task space, compared to different methods proposed in the literature;</li>
</ul>
<h1>2 Competence Based Intrinsic Motivation: The Self-Adaptive Goal Generation RIAC Architecture</h1>
<h3>2.1 Global Architecture</h3>
<p>Let us consider the definition of competence based models outlined in [74], and extract from it two different levels for active learning defined at different time scales (Fig. 1):</p>
<ol>
<li>The higher level of active learning (higher time scale) takes care of the active self-generation and self-selection of goals/tasks in a parameterized task space, depending on a measure of interest based on the level of competences to reach previously generated goals (e.g. competence progress);</li>
<li>The lower level of active learning (lower time scale) considers the goaldirected active choice and active exploration of lower-level actions to be taken to reach the goals selected at the higher level, and depending on local measures of interest related to the evolution of the quality of learned inverse and/or forward models;</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Global Architecture of the SAGG-RIAC architecture. The structure is comprised of two parts defining two levels of active learning: a higher level which considers the active self-generation and self-selection of goals, and a lower level, which considers the goal-directed active choice and active exploration of low-level actions, in order to reach the goals selected at the higher level.</p>
<h1>2.2 Model Formalization</h1>
<p>Let us consider a robotic system described in both a state/context space $S$, and a task space $Y$ which is a field of parameterized tasks/goals that can be viewed as defining a field of parameterized reinforcement learning problems. For a given context $s \in S$, a sequence of actions $a=\left{a_{1}, a_{2}, \ldots, a_{n}\right} \in A$, potentially generated by a parameterized motor synergy $\pi_{\theta}: S \rightarrow A$ (alternatively called an "option" and including a self-termination mechanism), allows a transition toward the new states $y \in Y$ such that $(s, a) \rightarrow y$, also written $\left(s, \pi_{\theta}\right) \rightarrow$ $y$. For instance, in the first experiment introduced in the following sections where we use a robotic manipulator, $S$ represents its actuator/joint space, $Y$ the operational space corresponding to the cartesian position of its end-effector, and $A$ relates to velocity commands in the joints. Also, in the second experiment involving a quadruped where we use motor synergies, the context $s$ is always reset to a same state and has thus no influence on the learning, $A$ relates to the 24 dimensional parameters of a motor synergy which considers the frequency and amplitude of sinusoids controlling the position of each joints over time, and $Y$ relates to the position and orientation of the robot after the execution of the synergy during a fixed amount of time.</p>
<p>SAGG-RIAC drives the exploration and learning of how to reach goals given starting contexts/states. Starting states are formalized as configurations $s \in S$ and goals as a desired $y_{g} \in Y$. All states are considered to be potential starting</p>
<p>states; therefore, once a goal has been generated, the low-level goal directed exploration and learning mechanism always tries to reach it by starting from the current state of the system as formalized and explained below.</p>
<p>When the initiation position $s_{\text {start }}$, the goal $y_{g}$ and constraints $\rho$ (e.g. linked with the spent energy) are chosen, it generates a motor policy $\pi_{\theta(\text { Data })}\left(s_{\text {start }}, y_{g}, \rho\right)$ parameterized by $s_{\text {start }}, y_{g}$ and $\rho$ as well as parameters $\theta$ of internal forward and inverse models already learned with previously acquired data Data. Also, it is important to notice that $\pi_{\theta(\text { Data })}\left(s_{\text {start }}, y_{g}, \rho\right)$ can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in $[20,8,55,108]$.</p>
<p>We can make an analogy of this formalization with the Semi-Markov Option framework introduced by Sutton [116]. In the case of SAGG-RIAC, when considering an option $\langle I, \pi, \beta\rangle$, we can first define the initiation set $I: S \rightarrow[0 ; 1]$, where $I$ is true everywhere, because, as presented before, every state can here be considered as a starting state. Also, goals are related to the terminal condition $\beta$ and $\beta=1$ when the goal is reached, and the policy $\pi$ encodes the skill learned through the process induced by the lower-level of active learning and shall be indexed by the goal $y_{g}$, i.e. $\pi_{y_{g}}$. More formally, as induced by the use of semi-markov options, we define policies and termination conditions as dependent on all events between the initiation of the option, and the current instant. This means that the policy $\pi$, and $\beta$ are depending on the history $h_{t \tau}=\left{s_{t}, a_{t}, s_{t+1}, a_{t+1} \ldots, s_{\tau}\right}$ where $t$ is the initiation time of the option, and $\tau$, the time of the latest event. Denoting the set of all histories by $\Omega$, the policy and termination condition become defined by $\pi: \Omega \times A \rightarrow[0 ; 1]$ and $\beta: \Omega \rightarrow[0 ; 1]$.</p>
<p>Moreover, because we have to consider cases where goals are not reachable (either because of physical impossibility or because the robot is not capable of doing it at that point of its development), we need to define a timeout $t_{\max }$ which can stop a goal reaching attempt once a maximal number of actions has been executed. $h_{t \tau}$ is thus needed to stop $\pi$, (i.e. the low-level active learning process), if $\tau&gt;t_{\max }$.</p>
<p>Eventually, using the framework of options, we can define the process of goal self-generation, as the self-generation and self-selection of parameterized options, and a goal reaching attempt corresponding to the learning of a particular option. Therefore, the global SAGG-RIAC process can be described as exploring and learning fields of options.</p>
<h1>2.3 Lower Time Scale: Active Goal Directed Exploration and Learning</h1>
<p>In SAGG-RIAC, once a goal has been actively chosen at the high-level, the goal directed exploration and learning mechanism at the lower can be carried out in numerous ways: the architecture makes only little assumptions about them, and thus is compatible with many methods such as those described below (this is the reason why SAGG-RIAC is an architecture defining a family of algorithms).</p>
<p>Its main idea is to guide the system toward the goal by executing low-level actions which allow a progressive exploration of the world toward this specific goal and that updates at the same time the local corresponding forward and inverse models, leveraging previously learnt correspondences with regression. The main assumptions about the methods that can be used for this lower level are:</p>
<ul>
<li>Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in $[20,8,55,108] ;$</li>
<li>Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods $[78,112]$ or stochastic optimization [45];
A optional feature, which is a variant of the second assumption above, is:</li>
<li>Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal. A learning feedback mechanism has to be added such that the exploration is active, and the selection of new actions depends on local measures about the quality of the learned model.</li>
</ul>
<p>In the following experiments that will be introduced, we will use two different methods: one mechanism where optimization is inspired by the SSA algorithm [92], coupled with memory-based local forward and inverse regression models using local Moore-Penrose pseudo-inverses, and a more generic optimization algorithm mixing stochastic optimization with memory-based regression models using pseudo-inverse. Other kinds of techniques could be used. For the optimization part, algorithms such as natural actor-critic architectures in model based reinforcement learning [78], algorithms of convex optimization [33], algorithms of stochastic optimization like CMA (e.g. [45]), or path-integral methods (e.g. $[113,112])$.</p>
<p>For the regression part, we are here using a memory-based approach, which if combined with efficient data storage and access structures [4, 70], scales well from a computational point of view. Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in $[20,8,55,108]$.</p>
<h1>2.4 Higher Time Scale: Goal Self-Generation and Self-Selection</h1>
<p>The Goal Self-Generation and Self-Selection process relies on a feedback defined using the concept of competence, and more precisely on the competence</p>
<p>improvement in given regions (or subspaces) of the task space where goals are chosen. The measure of competence can be computed at different instants of the learning process. First, it can be estimated once a reaching attempt in direction of a goal has been declared as terminated. Second, for robotic setups which are compatible with this option, competence can be computed during low-level reaching attempts. In the following sections, we detail these two different cases:</p>
<h1>2.4.1 Measure of Competence for a Terminated Reaching Attempt</h1>
<p>A reaching attempt for a goal is considered terminated according to two conditions:</p>
<ul>
<li>A timeout related to a maximum number of iterations allowed by the low-level of active learning has been exceeded.</li>
<li>The goal has effectively been reached.</li>
</ul>
<p>We introduce a measure of competence for a given goal reaching attempt as dependent on two metrics: the similarity between the point in the task space $y_{f}$ attained when the reaching attempt has terminated, and the actual goal $y_{g}$; and the respect of constraints $\rho$. These conditions are represented by a cost, or competence, function $C$ defined in $[-\infty ; 0]$, such that higher $C\left(y_{g}, y_{f}, \rho\right)$ will be, the more a reaching attempt will be considered as efficient. From this definition, we set a measure of competence $\Gamma_{y_{g}}$ directly linked with the value of $C\left(y_{g}, y_{f}, \rho\right)$ :</p>
<p>$$
\Gamma_{y_{g}}= \begin{cases}C\left(y_{g}, y_{f}, \rho\right) &amp; \text { if } C\left(y_{g}, y_{f}, \rho\right) \leq \varepsilon_{s i m}&lt;0 \ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>where $\varepsilon_{\text {sim }}$ is a tolerance factor such that $C\left(y_{g}, y_{f}, \rho\right)&gt;\varepsilon_{\text {sim }}$ corresponds to a goal reached. We note that a high value of $\Gamma_{y_{g}}$ (i.e. close to 0 ) represents a system that is competent to reach the goal $y_{g}$ while respecting constraints $\rho$. A typical instantiation of $C$, without constraints $\rho$, is defined as $C\left(y_{g}, y_{f}, \emptyset\right)=$ $-\left|y_{g}-y_{f}\right|^{2}$, and is the direct transposition of prediction error in RIAC $[77,8]$ to the task space in SAGG-RIAC. Yet, this competence measure might take some other forms in the SAGG-RIAC architecture, such as the variants explored in the experiments below.</p>
<h3>2.4.2 Measure of Competence During a Reaching Attempt or During Goal-Directed Optimization</h3>
<p>When the system exploits its previously learnt models to reach a goal $y_{g}$, using a computed $\pi_{\theta}$ through adequate local regression, or when it is using the low-level goal-directed optimization to optimize the best current $\pi_{\theta}$ to reach a self-generated goal $y_{g}$, it does not only collect data allowing to measure its competence to reach $y_{g}$, but since the computed $\pi_{\theta}$ might lead to a different effect $y_{e} \neq y_{g}$, it also allows to collect new data for improving the inverse model and the measure of competence to reach other goals in the locality of $y_{e}$. This allows to use all experiments of the robot to update the model of competences over the space of paremeterized goals.</p>
<h1>2.4.3 Definition of Local Competence Progress</h1>
<p>The active goal self-generation and self-selection relies on a feedback linked with the notion of competence introduced above, and more precisely on the monitoring of the progress of local competences. We first need to define this notion of local competence. Let us consider a subspace called a region $R \subset$ $Y$. Then, let us consider different measures of competence $\Gamma_{y_{i}}$ computed for different attempted goals $y_{i} \in R$, in a time window consisting of the $\zeta$ last attempted goals. For the region $R$, we can compute a measure of competence $\Gamma$ that we call a local measure such that:</p>
<p>$$
\Gamma=\left(\frac{\sum_{y_{j} \in R}\left(\Gamma_{y_{j}}\right)}{|R|}\right)
$$</p>
<p>with $|R|$, cardinal of $R$.
Let us now consider different regions $R_{i}$ of $Y$ such that $R_{i} \subset Y, \bigcup_{i} R_{i}=$ $Y$ (initially, there is only one region which is then progressively and recursively split; see below and see Fig. 2). Each $R_{i}$ contains attempted goals $\left{y_{i_{1}, t_{1}}, y_{i_{2}, t_{2}}, \ldots, y_{i_{k}, t_{k}}\right}<em i="i">{R</em>\right}}}$ and corresponding competences obtained $\left{\Gamma_{y_{i_{1}}, t_{1}}, \Gamma_{y_{i_{2}}, t_{2}}, \ldots, \Gamma_{y_{i_{k}}, t_{k}<em i="i">{R</em>$ are not the absolute time, but integer indexes of relative order in the given region).}}$, indexed by their relative time order of experimentation $t_{1}&lt;t_{2}&lt;\ldots&lt;t_{k} \mid t_{n+1}=$ $t_{n}+1$ inside this precise subspace $R_{i}$ ( $t_{i</p>
<p>An estimation of interest is computed for each region $R_{i}$. The interest interest $<em i="i">{i}$ of a region $R</em>$ (equation 2):}$ is described as the absolute value of the derivative of local competences inside $R_{i}$, hence the amplitude of local competence progress, over a sliding time window of the $\zeta$ more recent goals attempted inside $R_{i</p>
<p>$$
\text { interest }<em j="\left|R_{i">{i}=\frac{\left|\left(\sum</em>
$$}\right|-\zeta}^{\left|R_{i}\right|-\frac{\zeta}{2}} \Gamma_{y_{j}}\right)-\left(\sum_{j=\left|R_{i}\right|-\frac{\zeta}{2}}^{\left|R_{i}\right|} \Gamma_{y_{j}}\right)\right|}{\zeta</p>
<p>By using a derivative, the interest considers the variation of competences, and by using an absolute value, it considers cases of increasing and decreasing competences. In SAGG-RIAC, we will use the term competence progress with its general meaning to denote this increase and decrease of competences.</p>
<p>An increasing competence signifies that the expected competence gain in $R_{i}$ is important. Therefore, potentially, selecting new goals in regions of high competence progress could bring both a high information gain for the learned model, and also drive the reaching of not previously achieved goals.</p>
<p>Depending on the starting position and potential evolution of the environment or of the body (e.g. breaking of a body part), a decrease of competences inside already well-reached regions can arise. In this case, the system should be able to focus again in these regions in order to at least verify the possibility to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Task space and example of regions and subregions split during the learning process according to the competence level. Each region displays its competence level over time, measure which is used for the computation of the interest according to equation 2 .
re-establish a high level of competence inside. This explains the usefulness to consider the absolute value of the competence progress as shown in equation 2.</p>
<p>Using a sliding window in order to compute the value of interest prevents the system from keeping each measure of competence in its memory, and thus limits the storage resource needed by the core of the SAGG-RIAC architecture.</p>
<h1>2.4.4 Goal Self-Generation Using the Measure of Interest</h1>
<p>Using the previous description of interest, the goal self-generation and selfselection mechanism carries out two different processes:</p>
<ol>
<li>Splitting of the space $Y$ where goals are chosen, into subspaces, according to heuristics that allows to maximally discriminate areas according to their levels of interest.</li>
<li>Selecting the next goal to perform.</li>
</ol>
<p>Such a mechanism has been described in the RIAC algorithm introduced in [8], but was previously applied to the actuator space $S$ rather than to the goal/task space $Y$ as is done in SAGG-RIAC. Here, we use the same kind of methods such as a recursive split of the space, each split being triggered once a predefined maximum number of goals $g_{\max }$ has been attempted inside. Each split is performed such that it maximizes the difference of the interest measure described above in the two resulting subspaces. This allows the easy separation of areas of differing interest and therefore of differing reaching difficulty. More precisely, here the split of a region $R_{n}$ into $R_{n+1}$ and $R_{n+2}$ is done by selecting among $m$ randomly generated splits, a split dimension $j \in|Y|$ and then a position $v_{j}$ such that:</p>
<ul>
<li>All the $y_{i}$ of $R_{n+1}$ have a $j^{\text {th }}$ component smaller than $v_{j}$;</li>
<li>
<p>All the $y_{i}$ of $R_{n+2}$ have a $j^{\text {th }}$ component higher than $v_{j}$;</p>
</li>
<li>
<p>The quantity $\operatorname{Qual}\left(j, v_{j}\right)=\operatorname{card}\left(R_{n+1}\right) \cdot \operatorname{card}\left(R_{n+2}\right) \cdot\left|\operatorname{interest}<em n_1="n+1">{R</em>}}-\operatorname{interest<em n_2="n+2">{R</em>\right|$ is maximal;}</p>
</li>
</ul>
<p>Finally, as soon as at least two regions exist after an initial random exploration of the whole space, goals are chosen according to the following heuristics, selected according to probabilistic distributions:</p>
<ol>
<li>$\operatorname{mode}(1)$ : in $p_{1} \%$ percent (typically $p_{1}=70 \%$ ) of goal selections, a random goal is chosen along a uniform distribution inside a region which is selected with a probability proportional to its interest value:</li>
</ol>
<p>$$
P_{n}=\frac{\operatorname{interest}<em i="i">{n}-\min \left(\text { interest }</em>}\right)}{\sum_{i=1}^{\left|R_{n}\right|} \text { interest <em i="i">{i}-\min \left(\text { interest }</em>
$$}\right)</p>
<p>Where $P_{n}$ is the selection probability of the region $R_{n}$, and interest $<em i="i">{i}$ corresponds to the current interest of the region $R</em>$.
2. $\operatorname{mode}(2)$ : in $p_{2} \%$ (typically $p_{2}=20 \%$ of cases), a random goal is chosen inside the whole space $Y$.
3. $\operatorname{mode}(3)$ : in $p_{3} \%$ (typically $p_{3}=10 \%$ ), a region is first selected according to the interest value (like in mode(1)) and then a new goal is generated close to the already experimented one which received the lowest competence estimation.</p>
<h1>2.4.5 Reduction of the Initiation Set</h1>
<p>In order to improve the quality of the learned inverse model, we add a heuristic inspired by two observations on infant motor exploration and learning. The first one, proposed by Berthier et al. [17] is that infant's reaching attempts are often preceded by movements that either elevate their hand or move their hand back to their side. And the second one, noticed in [85], is that infants do not try to reach for objects forever but sometimes relax their muscles and rest. Practically, these two characteristics allow them to reduce the number of initiation positions that they use to reach an object, which simplifies the reaching problem by letting them learn a reduced number of reaching movements.</p>
<p>Such mechanism can be transposed in robotics to motor learning of arm reaching tasks as well as other kind of skills such as locomotion or fishing as shown in experiments below. In such a framework, it directly allows a highlyredundant robotic system to reduce the space of initiation states used to learn to reach goals, and also typically prevent it from experimenting with too complex actuator configurations. We add such a process in SAGG-RIAC, by specifying a rest position $\left(s_{\text {rest }}, y_{\text {rest }}\right)$ reachable without any need of planning from the system, that is set for each $r$ subsequent reaching attempts (we call $r$ the reset value, with $r&gt;0$ ).</p>
<h3>2.5 New Challenges of Unknown Limits of the Task Space</h3>
<p>In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval</p>
<p>(for instance, the range of angles that can be taken by a motor, or the phases and amplitudes of CPGs which can be easily identified). In these cases, the challenge is to select which areas would potentially give the most information to the system, to improve its knowledge, inside this fixed range of possibilities. As argued earlier, a limit of these approaches is that they become less and less efficient as the dimensionality of the control space increases. Competence based approaches allow to address this issue when a low-dimensional task space can be identified. Nevertheless, in that case, a new problem arises when considering unbounded learning: the space where goals are reachable can be extremely large and it is generally very difficult to predict its limits and undesirable to ask the engineer to identify them. Therefore, when carried out in large spaces where the reachable area is only a small part of it, the algorithm could necessitate numerous random goal self-generations to be able to estimate interests of different subregions. In order to reduce this number, and help the system to converge easily toward regions where competence can be improved, we emphasize two different mechanisms that can be used in SAGG-RIAC, during a reaching attempt:</p>
<ol>
<li>Conservation of every point reached inside the task space even if they do not correspond to the attempted goal (see section 2.4.2): when the robot performs a reaching attempt toward a goal $y$, and, instead of reaching it, terminates at another state $y^{\prime}$, we consider $y^{\prime}$ as a goal reached with a value of competence depending on constraints $\rho$. In cases where no constraints are studied, we can consider the $y^{\prime}$ as another goal reached with the highest level of competence.</li>
<li>Addition of subgoals: in robotic setups where the process of goal reaching can be subdivided and described using subgoals which could be fixed on the pathway toward the goal, we artificially add states $y_{1}, y_{2}, \ldots, y_{n}$ that have to be reached before $y$ while also respecting the constraints $\rho$, and estimate a competence measure for each one.</li>
</ol>
<p>The consideration of these two heuristics has important advantages: first, they can significantly increase the number of estimations of competence, and thus the quantity of feedback returned to the goal self-generation mechanism. This reduces the number of goals that have to be self-generated to bootstrap the system, and thus the number of low-level iteration required to extract first interesting subspaces. Also, by creating areas of different competence values around already reached states, they influence the discovery of reachable areas. Finally, they result in an interesting emergent phenomena: they create a growing area of increasing competence around the first discovered reachable areas. Indeed, by obtaining values of competences inside reachable areas, the algorithm is able to split the space first in these regions, and compute values of interest. These values of interest will typically be high in already reached areas and influence the goal self-generation process to create new goals in its proximity. Once the level of competence becomes important and stabilized in efficiently reached ar-</p>
<p>eas, the interest becomes null, then, new areas of interest close to these ones will be discovered, and so on.</p>
<h1>2.6 PseudoCode</h1>
<p>Pseudo-code 1 and algorithm 2 present the flow of operations in the SAGGRIAC architecture. Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like $P I^{2}-C M A$ [112], $C M A$ [45], or those presented in [79]. The function Inefficient can also be built in numerous manners and will not be described in details in the pseudo-code (examples will be described then for each experimentation). Its function is to judge if the current model has been efficient enough to reach or come closer to the decided goal, or if the model has to be improved in order to reach it.</p>
<p>In the following sections, we will present two different kinds of experiments. The first one is a reaching experiment where a robotic arm has to learn its inverse kinematics to reach self-generated end-effector positions. It uses an evolving context $s \in S$, also called setpoint in SSA, representing its current joint configuration. Therefore, it can be described by the relationship $(s, a) \rightarrow y$ where $s, a$ and $y$ can evolve. It is thus possible to use a goal-directed optimization algorithm very similar to SSA in this experiment, like the one in algorithm 3.</p>
<p>In the two other experiments, in contrast, we control the robots using parameterized motor synergies and consider a fixed context (a rest position) $s \in S$ where the robot is reset before each action: we will first consider a quadruped learning omnidirectional locomotion, and then an arm controlling a flexible fishing rod learning to put the float in precise self-generated positions on top of the water. Thus, these systems can be described by the relationship $\left(s, \pi_{\theta}\right) \rightarrow y$, where $s$ will here be fixed and $\theta$ will be the parameters of the motor synergy used to control the robots. Thus, a variation of setpoint being prevented here, a variant of SSA will be proposed for such experiments (similar to a more traditional optimization algorithm), where the context will not evolve and always be reset, like in algorithm 4.</p>
<h2>3 Experimental Setup 1: Learning Inverse Kinematics with a Redundant Arm</h2>
<p>In this section, we propose an experiment carried out with a robotic arm which has to explore and learn its forward and inverse kinematics. Also, before discussing the details of our active exploration approach in this first experimentation case, we firstly define the representations of the models and control paradigms involved in this experiment. Here, we focus on robotic systems whose actuators are settable by positions and velocities, and restrict our analysis to discrete time models.</p>
<p>Allowing robots to be self-adaptive to environmental conditions and changes in their own geometry is an important challenge of machine learning. These</p>
<p>changes in the robot geometry directly have an impact on its Inverse Kinematics IK, relating workspace coordinates (where tasks are usually specified), to actuators coordinates (like joint position, velocity, or torque used to command the robot). Learning inverse kinematics is useful in numerous machine learning cases, such as when no accurate kinematic model of a robot is available or when an online calibration is needed due to sensor or motor imprecision. Moreover, in developmental robotics studies, the a priori knowledge of a precise model of the body is often avoided, because of its implausibility from the biological point of view. In the following experiment, we assume that the inverse kinematics of our system is totally unknown, and we are interested in studying how SAGG-RIAC can efficiently guide the discovery and learning of its inverse kinematics.</p>
<h1>3.1 Control Paradigms for Learning Inverse Kinematics</h1>
<p>Let us mathematically formulate forward and inverse kinematics relations. We define the intrinsic coordinates (joint/actuator positions) of a manipulator as the $n$-dimensional vector $S=\alpha \in \mathbb{R}^{n}$, and the position and orientation of the manipulator's end-effector as the $m$-dimensional vector $y \in \mathbb{R}^{m}$. Relative to this formalization, actions of the robot corresponds to speed commands parameterized by a vector $\theta=\dot{\alpha} \in \mathbb{R}^{n}$ which controls the instantaneous speed of each of the $n$ joints of the arm. The forward kinematic function of this system is generally written as $y=f(\alpha)$, and inverse kinematics relationship is defined as $\alpha=f^{-1}(y)$.</p>
<p>When a redundant manipulator is considered $(n&gt;m)$, or when $m=n$, solutions to the inverse relationship are generally non-unique [106]. The problem posed to inverse learning algorithms is thus to determine particular solutions to $\alpha=f^{-1}(y)$, when multiple solutions exists. A typical approach used for solving this problem considers local methods, which learn relationships linking small changes $\Delta \alpha$ and $\Delta y$ :</p>
<p>$$
\dot{y}=J(\alpha) \dot{\alpha}
$$</p>
<p>where $J(\alpha)$ is the Jacobian matrix.
Then, using the Jacobian matrix and inverting it to get a single solution $\dot{\alpha}$ corresponding to a desired $\dot{y}$ raises the problem of the non-convexity property of this last equation. A solution to this non-convex problem has then been proposed by Bullock in [23] who converted it into a convex problem, by only considering the learning task within the spatial vicinity $\overline{\bar{\alpha}}$ of a particular $\alpha$ :</p>
<p>$$
\dot{y}=J(\alpha) \overline{\bar{\alpha}}
$$</p>
<h3>3.2 Representation of Forward and Inverse Models to be Learnt</h3>
<p>We use here non-parametric models which typically determine local models in the vicinity of a current datapoint. By computing a model using parameterized</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Values used to compute the competence $\Gamma_{y_{g}}$, considering a manipulator of 7 degrees-of-freedom, in a 2 dimensions operational/task space. Here, the arm is set in a position called rest position which is not straight and slightly bent. $\left(\alpha_{\text {rest }}, y_{\text {rest }}\right)$.
functions on datapoints restrained to a locality, they have been proposed as useful for real time queries, and incremental learning. Learning inverse kinematics typically deals with these kind of constraints, and these local methods have thus been proposed as an efficient approach to IK learning [125, 107]. In the following study, we use an incremental version of the Approximate Nearest Neighbors algorithm (ANN) [70], based on a tree split using the k-means process, to determine the vicinity of the current $\alpha$. Also, in the environments that we use to introduce our contribution, we do not need highly robust, and computationally very complex regression methods. Using the pseudo-inverse of Moore-Penrose [2] to compute the pseudo-inverse $J^{+}(\alpha)$ of the Jacobian $J(\alpha)$ in a vicinity $\widehat{\hat{\alpha}}$ is thus sufficient. Possible problems happening due to singularities $[106,28,91]$ being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).</p>
<p>Also, in the following equation, we use this method to deduce the change $\Delta \alpha$ corresponding to a $\Delta x$, for a given joint position $\alpha$ :</p>
<p>$$
\dot{\alpha}=J^{+}(\alpha) \dot{y}
$$</p>
<h1>3.3 Robotic Setup</h1>
<p>In the following experiments, we consider a $n$-dimensional manipulator controlled in position and speed (as many of today's robots), updated at discrete time values. The vector $\alpha \in \mathbb{R}^{n}$ which represents joint angles corresponds to the context/state space $S$ and the vector $y \in \mathbb{R}^{m}$ which is the position of the manipulator's end-effector in $m$ dimensions in the Euclidian space $\mathbb{R}^{m}$ corresponds to the task space $Y$ (see Fig. 3 where $n=7$ and $m=2$ ). We evaluate how</p>
<p>the SAGG-RIAC architecture can be used by a robot to learn how to reach all reachable points in the environment $Y$ with this arm's end-effector. Learning the inverse kinematics is here an online process that arises each time a micro-action $\theta=\Delta \alpha \in A$ is executed by the manipulator: by doing each micro-action, the robot stores measures $(\alpha, \Delta \alpha, \Delta x)$ in its memory and creates a database Data which contains elements $\left(\alpha_{i}, \Delta \alpha_{i}, \Delta y_{i}\right)$ representing the discovered change $\Delta y_{i}$ corresponding to a given $\Delta \alpha_{i}$ in the configuration $\alpha_{i}$ (this learning entity can be called a schema according to the terminology of Drescher [38]). These measures are then reused online to compute the Jacobian $J(\alpha)=\Delta y / \Delta \alpha$ locally to move the end-effector in a desired direction $\Delta y_{\text {desired }}$ fixed toward the selfgenerated goal. Therefore, we consider a learning problem of $2 n$ dimensions, the relationship that the system has to learn being $(\alpha, \Delta \alpha) \rightarrow \Delta y$. Also, in this experiment, where we suppose $Y$ Euclidian, and do not consider obstacles, the direction to a goal can be defined as following a straight line between the current end-effector's position and the goal.</p>
<h1>3.4 Evaluation of Competence</h1>
<p>In this experiment, in order to clearly illustrate the main contribution of our algorithm, we do not consider constraints $\rho$ and only focus on the reaching of goal positions $y_{g}$. It is nevertheless important to notice that a constraint $\rho$ has a direct influence on the low-level of active learning of SAGG-RIAC, and thus an indirect influence on the higher level. As using a constraint can require a more complex exploration process guided at the low-level, a more important number of iterations at this level can be required to reach a goal, which could have an influence on the global evolution of the performances of the learning process used by the higher-level of SAGG-RIAC.</p>
<p>We define here the competence function $C$ with the Euclidian distance $D\left(y_{g}, y_{f}\right)$, between the goal position and the final reached position $y_{f}$, which is normalized by the starting distance $D\left(y_{\text {start }}, y_{g}\right)$, where $y_{\text {start }}$ is the endeffector's starting position. This allows, for instance, to give a same competence level when considering a goal at 1 cm from the origin position, which the robot approaches at 0.5 cm and a goal at 1 mm , which the robot approaches at 0.5 mm .</p>
<p>$$
C\left(y_{g}, y_{f}, y_{\text {start }}\right)=-\frac{D\left(y_{g}, y_{f}\right)}{D\left(y_{\text {start }}, y_{g}\right)}
$$</p>
<p>where $C\left(y_{g}, y_{f}, y_{\text {start }}\right)=0$ if $D\left(y_{\text {start }}, y_{g}\right)&lt;\varepsilon_{C}$ (the goal is too close from the start position) and $C\left(y_{g}, y_{f}, y_{\text {start }}\right)=-1$ if $D\left(y_{g}, y_{f}\right)&gt;D\left(y_{\text {start }}, y_{g}\right)$ (the end-effector moved away from the goal).</p>
<h3>3.5 Addition of subgoals</h3>
<p>Computing local competence progress in subspaces/regions typically requires the reaching of numerous goals. Because reaching a goal can necessitate several micro-actions, and thus time, obtaining competence measures can be long. Also,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Baranes, A., Oudeyer, P-Y. (2012) Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and Autonomous Systems, 61(1), pp. 49-73. http://dx.doi.org/10.1016/j.robot.2012.05.008&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3101 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3101</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3101</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268876017</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.02255v1.pdf" target="_blank">LM2: A Simple Society of Language Models Solves Complex Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning – a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) – the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement. The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions. These models are trained to coordinate using policy learning. Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain reasoning problems, outperforming the best baselines by 8.1% on MATH, 7.71% on JEEBench, and 9.7% on MedQA problems (code available at https://github.com/ LCS2-IIITD/Language_Model_Multiplex).</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3101.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3101.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model Multiplex (LM^2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular multi-LLM coordination framework that decomposes complex reasoning into concept identification, iterative subquestioning, solving, and fine-grained verification; decomposer and verifier are finetuned LLaMA-2 models coordinated with a frozen solver via policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM^2 (Language Model Multiplex)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system composed of three cooperating LMs: a frozen solver (OpenAI text-davinci-003 / GPT-3.5) that answers subquestions, a decomposer (LLaMA-2 13B with LoRA finetuning) that generates concepts and next subquestions conditioned on prior solver answers and verifier feedback, and a verifier (LLaMA-2 13B with LoRA finetuning) that classifies/ explains errors into nine fine-grained categories. The decomposer is first supervised-finetuned on GPT-4-annotated decomposition data and then trained with Proximal Policy Optimization using verifier feedback as reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>solver: GPT-3.5 (proprietary, unspecified exact param count); decomposer/verifier: LLaMA-2 13B (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['explicit decomposition (step-by-step subquestions)', 'concept conditioning (provide concepts required to solve problem)', 'iterative solver-decomposer-verifier loop (feedback-driven subquestion generation)', 'structured answer template (controlled, stepwise context)', 'policy-optimized decomposer (PPO using verifier feedback)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Decomposer first generates a list of concepts required for the problem and then iteratively generates the next sub-question conditioned on the original question, previous subquestions and their answers, and verifier feedback. The solver (frozen GPT-3.5) answers each subquestion. The verifier (finetuned LLaMA-2 13B) classifies answers into nine categories of mistakes and supplies explanations; these classifications are converted into scalar rewards used in PPO to fine-tune the decomposer policy so it learns to produce subquestions that the solver can answer correctly. A structured answering template constrains outputs and reduces redundant generation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse methods — LM^2 explicitly combines multiple distinct reasoning styles (decomposition, verification, iterative corrective feedback, policy-optimized subquestioning, and concept conditioning) rather than relying on a single monolithic chain-of-thought style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, JEEBench (Physics/Chemistry/Math subsets), MedQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MATH: hard competition-style mathematical problems requiring multi-step reasoning; JEEBench: Indian JEE Advanced style science (Physics/Chemistry/Math) problems; MedQA: professional medical exam open-domain questions in English.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>LM^2 (with GPT-3.5 solver) reports absolute improvements over best baselines by: +8.1 percentage points on MATH (average), +7.71 percentage points on JEEBench, and +9.7 percentage points on MedQA. Example direct numbers: MedQA — Chain-of-Thought (CoT) 50.3% vs LM^2 57.1% (absolute). (All reported accuracies use GPT-3.5 as the solver for fair comparison unless otherwise noted.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>LM^2 was compared against CoT, Least-to-most (L2M), Progressive Hint Prompting (PHP), Demonstrate-Search-Predict (DSP), and DaSLaM. LM^2 outperforms all baselines on in-domain (MATH) and out-of-domain (MedQA, JEEBench) datasets. Ablations contrasted variants removing the verifier (LM^2\V), removing concepts (LM^2\C), omitting policy-RL (LM^2\RL), and variants using only specific reward categories (LM^2-Type, LM^2-Position). Key comparisons show large drops when verifier or concepts are removed (see key findings). The paper also compared token usage against DaSLaM and CoT, showing LM^2 reduces modified-response token generation relative to DaSLaM due to iterative, non-redundant subquestioning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Combining explicit concept generation, verifier-guided feedback, and policy-optimized decomposer yields large gains across in-domain and out-of-domain reasoning tasks. 2) The verifier and concept conditioning are especially important for out-of-domain generalization (MedQA, JEEBench chemistry/physics). 3) Fine-tuning the decomposer with verifier feedback (PPO) yields further gains (~9.6% average drop when omitted). 4) The decomposer (finetuned LLaMA-2) generated higher-quality concepts than GPT-4 for downstream solving (using GPT-4 for concepts caused an average -9.13% drop). 5) Structured answer templates alone help (improve GPT-4 by ~7.8% on JEEBench), but the full LM^2 coordination yields larger improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>LM^2 is not universally better than all large single-pass models; e.g., GPT-4 remains stronger on many problems (the paper notes LM^2 outperforms GPT-4 in 3/12 JEEBench cases but not overall). Ablations show performance can sharply drop if verifier or concepts are removed (e.g., removing verifier causes up to -13.0% drop in Chemistry subset). LM^2 also incurs additional training complexity and token overhead relative to single-pass CoT methods, and total token usage (including base CoT) is higher even though modified-response token generation is reduced compared to DaSLaM.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results_note</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3101.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaSLaM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaSLaM (Distilled Small LM for Decomposition & Solver Coordination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior two-model approach that distills decomposition ability into a smaller finetuned decomposer LM that coordinates with a larger solver LM; used as a competitive baseline here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Small language models fine-tuned to coordinate larger language models improve complex reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaSLaM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-model architecture: a finetuned smaller decomposer LM produces subquestions which are answered by a (larger) solver LM; decomposer is distilled from larger models to capture decomposition behavior. In this paper DaSLaM is used as a baseline with a frozen solver (GPT-3.5) and a finetuned decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies in original; in comparisons here solver is GPT-3.5 and decomposer is a smaller finetuned LM (not further specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['decomposition (generate subquestions upfront)', 'chain-of-thought style reasoning used in solver']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DaSLaM distills decomposition into a separate decomposer LM that outputs a set of subquestions (generated all at once) which the solver answers; there is less iterative feedback from solver/verifier to the decomposer compared to LM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar-style decomposition — uses explicit decomposition but generates subquestions in a memoryless/batched manner (all at once), i.e., less diverse/iterative feedback mechanisms than LM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, JEEBench, MedQA (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same benchmarks as LM^2 — multi-step math, JEE advanced style science problems, and medical QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>DaSLaM is the best-performing prior baseline cited; LM^2 reports outperforming DaSLaM by large margins on MATH (average +8.1% over best baseline) and on out-of-domain tasks where DaSLaM degrades relative to CoT (e.g., MedQA), LM^2 achieves +6.8 percentage points over CoT while DaSLaM degraded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared directly to LM^2, DaSLaM underperforms on several axes: LM^2's iterative, verifier-informed subquestion generation reduces redundancy and corrects solver errors that in DaSLaM would be integrated into the reasoning context. Token usage comparison shows LM^2 reduces modified-response generation tokens vs DaSLaM because DaSLaM generates subquestions all-at-once leading to redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DaSLaM's approach of pre-generating all subquestions is less effective at correcting solver errors and generalizing out-of-domain relative to LM^2's iterative verifier-guided decomposition. Distilling decomposition remains valuable, but tighter solver–decomposer coordination (via verifier and PPO) improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DaSLaM can perform competitively on some in-domain math categories but suffers on out-of-domain tasks like JEEBench Chemistry and MedQA when compared to LM^2; the paper reports DaSLaM sometimes degrading performance relative to CoT on MedQA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3101.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that elicits step-by-step intermediate reasoning (a 'chain of thought') from LLMs to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (applied to GPT-3.5/GPT-4 baseline runs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompting method that encourages the model to generate intermediate reasoning steps (scratchpad) before the final answer. Used here as a core baseline for all evaluated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to solver LLMs used in experiments (GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (step-by-step internal reasoning in one pass)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Single-pass prompting where the model is prompted (zero/few-shot) to produce an explicit chain of intermediate reasoning steps followed by an answer. No modular decomposer/verifier in the basic CoT setup.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single-style — CoT is a single monolithic reasoning style (explicit internal chain) without external verifier or iterative decomposition in its basic form.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, JEEBench, MedQA (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Standard multi-step reasoning benchmarks used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Examples reported: MedQA CoT accuracy = 50.3% (GPT-3.5 solver); other per-category numbers shown in tables. LM^2 reports improvements over CoT across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT is used as baseline; LM^2 and other decomposition-based methods (L2M, DaSLaM) are compared against CoT. LM^2 and decomposer-based methods show higher robustness on multi-step tasks compared to single-pass CoT, particularly with verifier and concept conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>While CoT improves reasoning relative to direct answer prompting, modular decomposition plus verification (as in LM^2) further improves performance and out-of-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT remains competitive and simpler; in some categories single-pass very large models (GPT-4 with CoT) still outperform modular pipelines unless the pipeline uses a strong verifier and fine-tuned decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3101.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most prompting (L2M)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that decomposes a complex problem into an increasing sequence of easier subproblems and solves them sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Least-to-Most (L2M) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompting method which asks the model to decompose the task into simpler subproblems (least to most difficult) and then solve them sequentially, passing answers forward as context. Used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to the same solver LLMs used in experiments (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['decomposition (least-to-most sequential solving)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt-based decomposition where subproblems are derived up-front (or via prompting) and then solved in sequence; less explicit verifier feedback and fewer cross-model coordination mechanisms than LM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar-style decomposition — L2M decomposes but does not incorporate a trainable decomposer/verifier loop; decomposition is produced via prompting rather than learned/iterative policy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, JEEBench, MedQA (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring multi-step reasoning; L2M used as a decomposition baseline in comparison experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported in tables as a baseline; specific per-dataset numbers are provided in the paper's tables (but aggregated comparisons emphasize LM^2 gains over L2M).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>LM^2 outperforms L2M; main advantage of LM^2 is iterative decomposer that conditions on solver/verifier feedback rather than one-shot prompting decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompted decomposition (L2M) improves over vanilla CoT, but learned decomposers with verifier-guided feedback (LM^2) provide better robustness and out-of-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>L2M can still be effective in zero-shot settings and is simpler to apply without finetuning; LM^2 requires finetuning and more coordination overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3101.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive-Hint Prompting (PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that provides progressively stronger hints (few-shot examples) to guide the model toward a correct solution; used as a baseline in 1-shot here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progressive-hint prompting improves reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Progressive-Hint Prompting (PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Originally used with multi-shot prompting to give progressively more detailed hints until the model solves the task; in this paper PHP was applied in a 1-shot setup for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to solver (GPT-3.5) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['hint-based progressive prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompting strategy that starts with minimal hints and progressively provides more guidance if the model fails, aiming to lead the model through intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single-style/hint-based prompting — similar in style to prompting-based decomposition; lacks learned verifier/iterative policy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, JEEBench, MedQA (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring multi-step reasoning; used as a prompting-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported as baseline in tables; generally underperforms LM^2 in the experiments reported (1-shot PHP is weaker than LM^2's coordinated pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>LM^2 (with finetuned decomposer and verifier plus PPO) outperforms PHP even when PHP uses multiple shots in original formulation; in this paper PHP was constrained to 1-shot for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Progressive hint prompting helps but benefits are smaller than full modular coordination with verifier and learned decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PHP's original multi-shot setup can be stronger than the constrained 1-shot used here; the paper restricts PHP to 1-shot making it a conservative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3101.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstrate-Search-Predict (DSP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A programmatic retrieval-augmented framework that coordinates a generative LM and a retrieval model to handle knowledge-intensive generation; used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Demonstrate-Search-Predict (DSP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that composes retrieval and generation: first demonstrates (few-shot), then retrieves supporting knowledge, then predicts the answer; used as a baseline for knowledge-intensive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to solver (GPT-3.5) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented generation', 'programmatic coordination between retrieval and generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DSP integrates retrieval to furnish supporting facts before generation; it coordinates generator and retriever via programs rather than learned decomposer/verifier loops.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse but orthogonal — DSP's diversity comes from retrieval + generation, not from iterative verifier-guided decomposition used in LM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, JEEBench, MedQA (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as a competing method for knowledge-intensive components of tasks; generally less tailored for multi-step arithmetic reasoning compared to decomposition/verification pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported in paper tables as a baseline; DSP underperforms LM^2 on evaluated multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>LM^2's explicit decomposition + fine-grained verifier outperforms DSP on multi-step reasoning tasks where procedural correctness and stepwise verification matter more than external retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented methods like DSP help for knowledge-intensive tasks but do not replace the benefits of iterative decomposition plus verification for complex procedural reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DSP can still be effective on knowledge-heavy questions; paper focuses on multi-step procedural reasoning where DSP is weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3101.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-davinci-003 (GPT-3.5 family, proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as the frozen solver LM in LM^2 experiments; generates answers to subquestions and initial answers for baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A proprietary autoregressive transformer-based language model from OpenAI used as the solver in all main experiments; kept frozen (no finetuning) to preserve world knowledge and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (proprietary; often referred to as 'GPT-3.5' family)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting (when used as baseline)', 'answering subquestions in the LM^2 pipeline']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>In LM^2 GPT-3.5 answers each decomposer-generated subquestion and the final question using the growing context of verified subquestion-answer pairs; as a baseline it is evaluated with CoT and structured prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>acts as the single solver in LM^2; diversity of reasoning arises from the pipeline (decomposer+verifier) rather than changes to GPT-3.5 itself.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, JEEBench, MedQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Serves as the answering engine across benchmarks either in isolation (CoT, L2M etc.) or as the solver inside LM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Used as the solver for which LM^2 reports the +8.1% (MATH), +7.71% (JEEBench), +9.7% (MedQA) absolute gains over best baselines. Example: MedQA CoT 50.3% (GPT-3.5) -> LM^2 57.1% (GPT-3.5 solver in pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>GPT-3.5's performance improves substantially when placed inside LM^2's decomposition+verifier pipeline versus when used with vanilla CoT or prompting-based decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A frozen, non-finetuned solver (GPT-3.5) benefits significantly from external modular guidance (decomposer + verifier), showing that solver capability can be amplified by modular coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The pipeline inherits solver limitations (e.g., numerical precision, length generalization); LM^2 does not remove fundamental solver weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3101.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 13B (base model used with LoRA finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-weight foundation model (13B parameters) used as the base for the decomposer and verifier; finetuned with LoRA and PPO for task-specific decomposition and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 13B (finetuned with LoRA adapters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open-weight transformer foundation model of ~13B parameters used as the base for both the decomposer and verifier. Both components are finetuned using parameter-efficient LoRA adapters (and further PPO for the decomposer). Training used 8-bit quantization on A100 hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['learned decomposition (supervised + PPO)', 'learned fine-grained verification (multi-class classification + explanation generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Decomposer LLaMA-2 generates concepts and next subquestions; verifier LLaMA-2 classifies solver answers into nine mistake categories and generates explanations. Decomposer uses supervised finetuning on GPT-4-curated decomposition data followed by PPO where verifier feedback is the reward signal.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse within the pipeline — the two finetuned LLaMA-2 instances implement complementary reasoning functions (concept extraction and multi-class verification) enabling diverse procedural control over solver reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH (training data from MATH train split for decomposer), evaluation on MATH, JEEBench, MedQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used both for training (decomposer/verifier datasets curated on MATH via GPT-4) and inference as components of LM^2 across multi-step reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>LLaMA-2 13B as decomposer/verifier enables LM^2's performance gains reported above (LM^2 improvements: +8.1% MATH, +7.71% JEEBench, +9.7% MedQA). The paper reports that finetuned LLaMA-2 13B generates better concepts than GPT-4 for downstream solving (GPT-4-concept replacement caused -9.13% average).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Finetuned LLaMA-2 components outperform prompting-only concept generation (GPT-4) and are crucial to LM^2's generalization; ablation removing finetuned decomposer/verifier or skipping PPO significantly degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parameter-efficient finetuning (LoRA) of LLaMA-2 13B can produce decomposer/verifier modules that, when coordinated with a frozen solver, meaningfully improve multi-step reasoning and out-of-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Trained decomposer/verifier are specialized (trained on MATH) and require extra compute/training effort; without verifier or concept conditioning their removal causes sharp drops in accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3101.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3101.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in the paper as the annotator for creating supervised decomposer and verifier datasets and as a comparative oracle for concept generation; not used as the main solver in LM^2 experiments (except some comparative structured-prompting runs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large, proprietary multimodal/advanced transformer model from OpenAI; used in the paper to generate decomposition datasets (concepts, subquestions, subanswers) and verifier training data; also used in comparative experiments (structured template tests).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting and decomposition via prompting (used to generate training data and for comparison)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-4 was prompted to produce gold decompositions, concepts, and mistake explanations to create supervised datasets for finetuning the decomposer and verifier. The paper also compares the quality of concept generation between the finetuned decomposer and GPT-4 prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>GPT-4 is used principally as an annotator and comparator; the diversity of reasoning methods discussed in the paper centers on LM^2's architecture rather than GPT-4 itself.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Data curation tasks (decomposition/verification labels) and comparative runs on JEEBench with structured template (GPT4-SP experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GPT-4 used to create training datasets by producing concepts, subquestions, and error explanations; also used in structured-prompting experiments (GPT4-SP) to measure template effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>When GPT-4 is used as a concept generator (replacing LM^2 decomposer's concepts), experiments show average -9.13% drop in downstream performance relative to the finetuned decomposer. Structured prompting with GPT-4 (GPT4-SP) yields average gains (e.g., ~7.8% on JEEBench) but the full LM^2 pipeline still yields different trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Finetuned LLaMA-2 decomposer produced better concepts for the solver than GPT-4 prompted concepts according to downstream performance; GPT-4 remains a strong baseline for structured prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-quality, task-specific concept generation by a finetuned smaller model can outperform direct GPT-4 prompt outputs for the downstream solver, supporting the value of task-aligned finetuning and feedback-driven decomposer training.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>GPT-4 remains a very strong solver in many scenarios; LM^2's improvements are particularly notable when using a mid-tier solver (GPT-3.5) plus learned decomposer/verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LM2: A Simple Society of Language Models Solves Complex Reasoning', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Small language models fine-tuned to coordinate larger language models improve complex reasoning <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Decomposed prompting: A modular approach for solving complex tasks <em>(Rating: 2)</em></li>
                <li>Progressive-hint prompting improves reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3101",
    "paper_id": "paper-268876017",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "LM2",
            "name_full": "Language Model Multiplex (LM^2)",
            "brief_description": "A modular multi-LLM coordination framework that decomposes complex reasoning into concept identification, iterative subquestioning, solving, and fine-grained verification; decomposer and verifier are finetuned LLaMA-2 models coordinated with a frozen solver via policy optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LM^2 (Language Model Multiplex)",
            "model_description": "A system composed of three cooperating LMs: a frozen solver (OpenAI text-davinci-003 / GPT-3.5) that answers subquestions, a decomposer (LLaMA-2 13B with LoRA finetuning) that generates concepts and next subquestions conditioned on prior solver answers and verifier feedback, and a verifier (LLaMA-2 13B with LoRA finetuning) that classifies/ explains errors into nine fine-grained categories. The decomposer is first supervised-finetuned on GPT-4-annotated decomposition data and then trained with Proximal Policy Optimization using verifier feedback as reward signals.",
            "model_size": "solver: GPT-3.5 (proprietary, unspecified exact param count); decomposer/verifier: LLaMA-2 13B (13B)",
            "reasoning_methods": [
                "explicit decomposition (step-by-step subquestions)",
                "concept conditioning (provide concepts required to solve problem)",
                "iterative solver-decomposer-verifier loop (feedback-driven subquestion generation)",
                "structured answer template (controlled, stepwise context)",
                "policy-optimized decomposer (PPO using verifier feedback)"
            ],
            "reasoning_methods_description": "Decomposer first generates a list of concepts required for the problem and then iteratively generates the next sub-question conditioned on the original question, previous subquestions and their answers, and verifier feedback. The solver (frozen GPT-3.5) answers each subquestion. The verifier (finetuned LLaMA-2 13B) classifies answers into nine categories of mistakes and supplies explanations; these classifications are converted into scalar rewards used in PPO to fine-tune the decomposer policy so it learns to produce subquestions that the solver can answer correctly. A structured answering template constrains outputs and reduces redundant generation.",
            "diversity_of_methods": "diverse methods — LM^2 explicitly combines multiple distinct reasoning styles (decomposition, verification, iterative corrective feedback, policy-optimized subquestioning, and concept conditioning) rather than relying on a single monolithic chain-of-thought style.",
            "reasoning_task_name": "MATH, JEEBench (Physics/Chemistry/Math subsets), MedQA",
            "reasoning_task_description": "MATH: hard competition-style mathematical problems requiring multi-step reasoning; JEEBench: Indian JEE Advanced style science (Physics/Chemistry/Math) problems; MedQA: professional medical exam open-domain questions in English.",
            "performance_by_method": "LM^2 (with GPT-3.5 solver) reports absolute improvements over best baselines by: +8.1 percentage points on MATH (average), +7.71 percentage points on JEEBench, and +9.7 percentage points on MedQA. Example direct numbers: MedQA — Chain-of-Thought (CoT) 50.3% vs LM^2 57.1% (absolute). (All reported accuracies use GPT-3.5 as the solver for fair comparison unless otherwise noted.)",
            "comparison_of_methods": "LM^2 was compared against CoT, Least-to-most (L2M), Progressive Hint Prompting (PHP), Demonstrate-Search-Predict (DSP), and DaSLaM. LM^2 outperforms all baselines on in-domain (MATH) and out-of-domain (MedQA, JEEBench) datasets. Ablations contrasted variants removing the verifier (LM^2\\V), removing concepts (LM^2\\C), omitting policy-RL (LM^2\\RL), and variants using only specific reward categories (LM^2-Type, LM^2-Position). Key comparisons show large drops when verifier or concepts are removed (see key findings). The paper also compared token usage against DaSLaM and CoT, showing LM^2 reduces modified-response token generation relative to DaSLaM due to iterative, non-redundant subquestioning.",
            "key_findings": "1) Combining explicit concept generation, verifier-guided feedback, and policy-optimized decomposer yields large gains across in-domain and out-of-domain reasoning tasks. 2) The verifier and concept conditioning are especially important for out-of-domain generalization (MedQA, JEEBench chemistry/physics). 3) Fine-tuning the decomposer with verifier feedback (PPO) yields further gains (~9.6% average drop when omitted). 4) The decomposer (finetuned LLaMA-2) generated higher-quality concepts than GPT-4 for downstream solving (using GPT-4 for concepts caused an average -9.13% drop). 5) Structured answer templates alone help (improve GPT-4 by ~7.8% on JEEBench), but the full LM^2 coordination yields larger improvements.",
            "counter_examples_or_negative_results": "LM^2 is not universally better than all large single-pass models; e.g., GPT-4 remains stronger on many problems (the paper notes LM^2 outperforms GPT-4 in 3/12 JEEBench cases but not overall). Ablations show performance can sharply drop if verifier or concepts are removed (e.g., removing verifier causes up to -13.0% drop in Chemistry subset). LM^2 also incurs additional training complexity and token overhead relative to single-pass CoT methods, and total token usage (including base CoT) is higher even though modified-response token generation is reduced compared to DaSLaM.",
            "counter_examples_or_negative_results_note": null,
            "uuid": "e3101.0",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DaSLaM",
            "name_full": "DaSLaM (Distilled Small LM for Decomposition & Solver Coordination)",
            "brief_description": "Prior two-model approach that distills decomposition ability into a smaller finetuned decomposer LM that coordinates with a larger solver LM; used as a competitive baseline here.",
            "citation_title": "Small language models fine-tuned to coordinate larger language models improve complex reasoning",
            "mention_or_use": "use",
            "model_name": "DaSLaM",
            "model_description": "Two-model architecture: a finetuned smaller decomposer LM produces subquestions which are answered by a (larger) solver LM; decomposer is distilled from larger models to capture decomposition behavior. In this paper DaSLaM is used as a baseline with a frozen solver (GPT-3.5) and a finetuned decomposer.",
            "model_size": "varies in original; in comparisons here solver is GPT-3.5 and decomposer is a smaller finetuned LM (not further specified in this paper).",
            "reasoning_methods": [
                "decomposition (generate subquestions upfront)",
                "chain-of-thought style reasoning used in solver"
            ],
            "reasoning_methods_description": "DaSLaM distills decomposition into a separate decomposer LM that outputs a set of subquestions (generated all at once) which the solver answers; there is less iterative feedback from solver/verifier to the decomposer compared to LM^2.",
            "diversity_of_methods": "similar-style decomposition — uses explicit decomposition but generates subquestions in a memoryless/batched manner (all at once), i.e., less diverse/iterative feedback mechanisms than LM^2.",
            "reasoning_task_name": "MATH, JEEBench, MedQA (used as baseline)",
            "reasoning_task_description": "Same benchmarks as LM^2 — multi-step math, JEE advanced style science problems, and medical QA.",
            "performance_by_method": "DaSLaM is the best-performing prior baseline cited; LM^2 reports outperforming DaSLaM by large margins on MATH (average +8.1% over best baseline) and on out-of-domain tasks where DaSLaM degrades relative to CoT (e.g., MedQA), LM^2 achieves +6.8 percentage points over CoT while DaSLaM degraded.",
            "comparison_of_methods": "Compared directly to LM^2, DaSLaM underperforms on several axes: LM^2's iterative, verifier-informed subquestion generation reduces redundancy and corrects solver errors that in DaSLaM would be integrated into the reasoning context. Token usage comparison shows LM^2 reduces modified-response generation tokens vs DaSLaM because DaSLaM generates subquestions all-at-once leading to redundancy.",
            "key_findings": "DaSLaM's approach of pre-generating all subquestions is less effective at correcting solver errors and generalizing out-of-domain relative to LM^2's iterative verifier-guided decomposition. Distilling decomposition remains valuable, but tighter solver–decomposer coordination (via verifier and PPO) improves results.",
            "counter_examples_or_negative_results": "DaSLaM can perform competitively on some in-domain math categories but suffers on out-of-domain tasks like JEEBench Chemistry and MedQA when compared to LM^2; the paper reports DaSLaM sometimes degrading performance relative to CoT on MedQA.",
            "uuid": "e3101.1",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "Prompting technique that elicits step-by-step intermediate reasoning (a 'chain of thought') from LLMs to improve multi-step problem solving.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting (applied to GPT-3.5/GPT-4 baseline runs)",
            "model_description": "A prompting method that encourages the model to generate intermediate reasoning steps (scratchpad) before the final answer. Used here as a core baseline for all evaluated datasets.",
            "model_size": "applied to solver LLMs used in experiments (GPT-3.5, GPT-4)",
            "reasoning_methods": [
                "chain-of-thought (step-by-step internal reasoning in one pass)"
            ],
            "reasoning_methods_description": "Single-pass prompting where the model is prompted (zero/few-shot) to produce an explicit chain of intermediate reasoning steps followed by an answer. No modular decomposer/verifier in the basic CoT setup.",
            "diversity_of_methods": "single-style — CoT is a single monolithic reasoning style (explicit internal chain) without external verifier or iterative decomposition in its basic form.",
            "reasoning_task_name": "MATH, JEEBench, MedQA (used as baseline)",
            "reasoning_task_description": "Standard multi-step reasoning benchmarks used for comparison.",
            "performance_by_method": "Examples reported: MedQA CoT accuracy = 50.3% (GPT-3.5 solver); other per-category numbers shown in tables. LM^2 reports improvements over CoT across tasks.",
            "comparison_of_methods": "CoT is used as baseline; LM^2 and other decomposition-based methods (L2M, DaSLaM) are compared against CoT. LM^2 and decomposer-based methods show higher robustness on multi-step tasks compared to single-pass CoT, particularly with verifier and concept conditioning.",
            "key_findings": "While CoT improves reasoning relative to direct answer prompting, modular decomposition plus verification (as in LM^2) further improves performance and out-of-domain generalization.",
            "counter_examples_or_negative_results": "CoT remains competitive and simpler; in some categories single-pass very large models (GPT-4 with CoT) still outperform modular pipelines unless the pipeline uses a strong verifier and fine-tuned decomposer.",
            "uuid": "e3101.2",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "L2M",
            "name_full": "Least-to-Most prompting (L2M)",
            "brief_description": "A prompting technique that decomposes a complex problem into an increasing sequence of easier subproblems and solves them sequentially.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Least-to-Most (L2M) prompting",
            "model_description": "A prompting method which asks the model to decompose the task into simpler subproblems (least to most difficult) and then solve them sequentially, passing answers forward as context. Used here as a baseline.",
            "model_size": "applied to the same solver LLMs used in experiments (GPT-3.5)",
            "reasoning_methods": [
                "decomposition (least-to-most sequential solving)"
            ],
            "reasoning_methods_description": "Prompt-based decomposition where subproblems are derived up-front (or via prompting) and then solved in sequence; less explicit verifier feedback and fewer cross-model coordination mechanisms than LM^2.",
            "diversity_of_methods": "similar-style decomposition — L2M decomposes but does not incorporate a trainable decomposer/verifier loop; decomposition is produced via prompting rather than learned/iterative policy.",
            "reasoning_task_name": "MATH, JEEBench, MedQA (used as baseline)",
            "reasoning_task_description": "Benchmarks requiring multi-step reasoning; L2M used as a decomposition baseline in comparison experiments.",
            "performance_by_method": "Reported in tables as a baseline; specific per-dataset numbers are provided in the paper's tables (but aggregated comparisons emphasize LM^2 gains over L2M).",
            "comparison_of_methods": "LM^2 outperforms L2M; main advantage of LM^2 is iterative decomposer that conditions on solver/verifier feedback rather than one-shot prompting decomposition.",
            "key_findings": "Prompted decomposition (L2M) improves over vanilla CoT, but learned decomposers with verifier-guided feedback (LM^2) provide better robustness and out-of-domain generalization.",
            "counter_examples_or_negative_results": "L2M can still be effective in zero-shot settings and is simpler to apply without finetuning; LM^2 requires finetuning and more coordination overhead.",
            "uuid": "e3101.3",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PHP",
            "name_full": "Progressive-Hint Prompting (PHP)",
            "brief_description": "A prompting technique that provides progressively stronger hints (few-shot examples) to guide the model toward a correct solution; used as a baseline in 1-shot here.",
            "citation_title": "Progressive-hint prompting improves reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Progressive-Hint Prompting (PHP)",
            "model_description": "Originally used with multi-shot prompting to give progressively more detailed hints until the model solves the task; in this paper PHP was applied in a 1-shot setup for fair comparison.",
            "model_size": "applied to solver (GPT-3.5) in experiments",
            "reasoning_methods": [
                "hint-based progressive prompting"
            ],
            "reasoning_methods_description": "Prompting strategy that starts with minimal hints and progressively provides more guidance if the model fails, aiming to lead the model through intermediate reasoning steps.",
            "diversity_of_methods": "single-style/hint-based prompting — similar in style to prompting-based decomposition; lacks learned verifier/iterative policy.",
            "reasoning_task_name": "MATH, JEEBench, MedQA (used as baseline)",
            "reasoning_task_description": "Benchmarks requiring multi-step reasoning; used as a prompting-based baseline.",
            "performance_by_method": "Reported as baseline in tables; generally underperforms LM^2 in the experiments reported (1-shot PHP is weaker than LM^2's coordinated pipeline).",
            "comparison_of_methods": "LM^2 (with finetuned decomposer and verifier plus PPO) outperforms PHP even when PHP uses multiple shots in original formulation; in this paper PHP was constrained to 1-shot for fairness.",
            "key_findings": "Progressive hint prompting helps but benefits are smaller than full modular coordination with verifier and learned decomposition.",
            "counter_examples_or_negative_results": "PHP's original multi-shot setup can be stronger than the constrained 1-shot used here; the paper restricts PHP to 1-shot making it a conservative baseline.",
            "uuid": "e3101.4",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DSP",
            "name_full": "Demonstrate-Search-Predict (DSP)",
            "brief_description": "A programmatic retrieval-augmented framework that coordinates a generative LM and a retrieval model to handle knowledge-intensive generation; used here as a baseline.",
            "citation_title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
            "mention_or_use": "use",
            "model_name": "Demonstrate-Search-Predict (DSP)",
            "model_description": "Framework that composes retrieval and generation: first demonstrates (few-shot), then retrieves supporting knowledge, then predicts the answer; used as a baseline for knowledge-intensive reasoning.",
            "model_size": "applied to solver (GPT-3.5) in experiments",
            "reasoning_methods": [
                "retrieval-augmented generation",
                "programmatic coordination between retrieval and generation"
            ],
            "reasoning_methods_description": "DSP integrates retrieval to furnish supporting facts before generation; it coordinates generator and retriever via programs rather than learned decomposer/verifier loops.",
            "diversity_of_methods": "diverse but orthogonal — DSP's diversity comes from retrieval + generation, not from iterative verifier-guided decomposition used in LM^2.",
            "reasoning_task_name": "MATH, JEEBench, MedQA (used as baseline)",
            "reasoning_task_description": "Used as a competing method for knowledge-intensive components of tasks; generally less tailored for multi-step arithmetic reasoning compared to decomposition/verification pipelines.",
            "performance_by_method": "Reported in paper tables as a baseline; DSP underperforms LM^2 on evaluated multi-step reasoning tasks.",
            "comparison_of_methods": "LM^2's explicit decomposition + fine-grained verifier outperforms DSP on multi-step reasoning tasks where procedural correctness and stepwise verification matter more than external retrieval.",
            "key_findings": "Retrieval-augmented methods like DSP help for knowledge-intensive tasks but do not replace the benefits of iterative decomposition plus verification for complex procedural reasoning.",
            "counter_examples_or_negative_results": "DSP can still be effective on knowledge-heavy questions; paper focuses on multi-step procedural reasoning where DSP is weaker.",
            "uuid": "e3101.5",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003)",
            "name_full": "OpenAI text-davinci-003 (GPT-3.5 family, proprietary)",
            "brief_description": "Used as the frozen solver LM in LM^2 experiments; generates answers to subquestions and initial answers for baseline comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5)",
            "model_description": "A proprietary autoregressive transformer-based language model from OpenAI used as the solver in all main experiments; kept frozen (no finetuning) to preserve world knowledge and generalization.",
            "model_size": "not specified (proprietary; often referred to as 'GPT-3.5' family)",
            "reasoning_methods": [
                "chain-of-thought prompting (when used as baseline)",
                "answering subquestions in the LM^2 pipeline"
            ],
            "reasoning_methods_description": "In LM^2 GPT-3.5 answers each decomposer-generated subquestion and the final question using the growing context of verified subquestion-answer pairs; as a baseline it is evaluated with CoT and structured prompting.",
            "diversity_of_methods": "acts as the single solver in LM^2; diversity of reasoning arises from the pipeline (decomposer+verifier) rather than changes to GPT-3.5 itself.",
            "reasoning_task_name": "MATH, JEEBench, MedQA",
            "reasoning_task_description": "Serves as the answering engine across benchmarks either in isolation (CoT, L2M etc.) or as the solver inside LM^2.",
            "performance_by_method": "Used as the solver for which LM^2 reports the +8.1% (MATH), +7.71% (JEEBench), +9.7% (MedQA) absolute gains over best baselines. Example: MedQA CoT 50.3% (GPT-3.5) -&gt; LM^2 57.1% (GPT-3.5 solver in pipeline).",
            "comparison_of_methods": "GPT-3.5's performance improves substantially when placed inside LM^2's decomposition+verifier pipeline versus when used with vanilla CoT or prompting-based decomposition.",
            "key_findings": "A frozen, non-finetuned solver (GPT-3.5) benefits significantly from external modular guidance (decomposer + verifier), showing that solver capability can be amplified by modular coordination.",
            "counter_examples_or_negative_results": "The pipeline inherits solver limitations (e.g., numerical precision, length generalization); LM^2 does not remove fundamental solver weaknesses.",
            "uuid": "e3101.6",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaMA-2 13B",
            "name_full": "LLaMA-2 13B (base model used with LoRA finetuning)",
            "brief_description": "Open-weight foundation model (13B parameters) used as the base for the decomposer and verifier; finetuned with LoRA and PPO for task-specific decomposition and verification.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 13B (finetuned with LoRA adapters)",
            "model_description": "An open-weight transformer foundation model of ~13B parameters used as the base for both the decomposer and verifier. Both components are finetuned using parameter-efficient LoRA adapters (and further PPO for the decomposer). Training used 8-bit quantization on A100 hardware.",
            "model_size": "13B",
            "reasoning_methods": [
                "learned decomposition (supervised + PPO)",
                "learned fine-grained verification (multi-class classification + explanation generation)"
            ],
            "reasoning_methods_description": "Decomposer LLaMA-2 generates concepts and next subquestions; verifier LLaMA-2 classifies solver answers into nine mistake categories and generates explanations. Decomposer uses supervised finetuning on GPT-4-curated decomposition data followed by PPO where verifier feedback is the reward signal.",
            "diversity_of_methods": "diverse within the pipeline — the two finetuned LLaMA-2 instances implement complementary reasoning functions (concept extraction and multi-class verification) enabling diverse procedural control over solver reasoning.",
            "reasoning_task_name": "MATH (training data from MATH train split for decomposer), evaluation on MATH, JEEBench, MedQA",
            "reasoning_task_description": "Used both for training (decomposer/verifier datasets curated on MATH via GPT-4) and inference as components of LM^2 across multi-step reasoning benchmarks.",
            "performance_by_method": "LLaMA-2 13B as decomposer/verifier enables LM^2's performance gains reported above (LM^2 improvements: +8.1% MATH, +7.71% JEEBench, +9.7% MedQA). The paper reports that finetuned LLaMA-2 13B generates better concepts than GPT-4 for downstream solving (GPT-4-concept replacement caused -9.13% average).",
            "comparison_of_methods": "Finetuned LLaMA-2 components outperform prompting-only concept generation (GPT-4) and are crucial to LM^2's generalization; ablation removing finetuned decomposer/verifier or skipping PPO significantly degrades performance.",
            "key_findings": "Parameter-efficient finetuning (LoRA) of LLaMA-2 13B can produce decomposer/verifier modules that, when coordinated with a frozen solver, meaningfully improve multi-step reasoning and out-of-domain generalization.",
            "counter_examples_or_negative_results": "Trained decomposer/verifier are specialized (trained on MATH) and require extra compute/training effort; without verifier or concept conditioning their removal causes sharp drops in accuracy.",
            "uuid": "e3101.7",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "Referenced in the paper as the annotator for creating supervised decomposer and verifier datasets and as a comparative oracle for concept generation; not used as the main solver in LM^2 experiments (except some comparative structured-prompting runs).",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large, proprietary multimodal/advanced transformer model from OpenAI; used in the paper to generate decomposition datasets (concepts, subquestions, subanswers) and verifier training data; also used in comparative experiments (structured template tests).",
            "model_size": "not specified in paper (proprietary)",
            "reasoning_methods": [
                "chain-of-thought prompting and decomposition via prompting (used to generate training data and for comparison)"
            ],
            "reasoning_methods_description": "GPT-4 was prompted to produce gold decompositions, concepts, and mistake explanations to create supervised datasets for finetuning the decomposer and verifier. The paper also compares the quality of concept generation between the finetuned decomposer and GPT-4 prompting.",
            "diversity_of_methods": "GPT-4 is used principally as an annotator and comparator; the diversity of reasoning methods discussed in the paper centers on LM^2's architecture rather than GPT-4 itself.",
            "reasoning_task_name": "Data curation tasks (decomposition/verification labels) and comparative runs on JEEBench with structured template (GPT4-SP experiments).",
            "reasoning_task_description": "GPT-4 used to create training datasets by producing concepts, subquestions, and error explanations; also used in structured-prompting experiments (GPT4-SP) to measure template effects.",
            "performance_by_method": "When GPT-4 is used as a concept generator (replacing LM^2 decomposer's concepts), experiments show average -9.13% drop in downstream performance relative to the finetuned decomposer. Structured prompting with GPT-4 (GPT4-SP) yields average gains (e.g., ~7.8% on JEEBench) but the full LM^2 pipeline still yields different trade-offs.",
            "comparison_of_methods": "Finetuned LLaMA-2 decomposer produced better concepts for the solver than GPT-4 prompted concepts according to downstream performance; GPT-4 remains a strong baseline for structured prompting.",
            "key_findings": "High-quality, task-specific concept generation by a finetuned smaller model can outperform direct GPT-4 prompt outputs for the downstream solver, supporting the value of task-aligned finetuning and feedback-driven decomposer training.",
            "counter_examples_or_negative_results": "GPT-4 remains a very strong solver in many scenarios; LM^2's improvements are particularly notable when using a mid-tier solver (GPT-3.5) plus learned decomposer/verifier.",
            "uuid": "e3101.8",
            "source_info": {
                "paper_title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Small language models fine-tuned to coordinate larger language models improve complex reasoning",
            "rating": 2,
            "sanitized_title": "small_language_models_finetuned_to_coordinate_larger_language_models_improve_complex_reasoning"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Decomposed prompting: A modular approach for solving complex tasks",
            "rating": 2,
            "sanitized_title": "decomposed_prompting_a_modular_approach_for_solving_complex_tasks"
        },
        {
            "paper_title": "Progressive-hint prompting improves reasoning in large language models",
            "rating": 1,
            "sanitized_title": "progressivehint_prompting_improves_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
            "rating": 1,
            "sanitized_title": "demonstratesearchpredict_composing_retrieval_and_language_models_for_knowledgeintensive_nlp"
        }
    ],
    "cost": 0.01894925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LM 2 : A Simple Society of Language Models Solves Complex Reasoning
2 Apr 2024</p>
<p>Gurusha Juneja gurushajuneja@gmail.com 
Subhabrata Dutta 
Tanmoy Chakraborty </p>
<p>Microsoft Research
India</p>
<p>IIT Delhi
India</p>
<p>IIT Delhi
India</p>
<p>LM 2 : A Simple Society of Language Models Solves Complex Reasoning
2 Apr 2024DDFFB1B8C76DE0D41B9F02295D72E685arXiv:2404.02255v1[cs.CL]
Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning.Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -a decomposer generates the subproblems, and a solver solves each of these subproblems.However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning.In this paper, we propose LM 2 to address these challenges.LM 2 modularizes the decomposition, solution, and verification into three different language models.The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement.The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions.These models are trained to coordinate using policy learning.Exhaustive experimentation suggests the superiority of LM 2 over existing methods on in-and out-domain reasoning problems, outperforming the best baselines by 8.1% on MATH, 7.71% on JEEBench, and 9.7% on MedQA problems (code available at https://github.com/LCS2-IIITD/Language_Model_Multiplex).</p>
<p>Introduction</p>
<p>Recent trends in solving complex reasoning tasks using Large Language Models (LLMs) typically follow two different dominant approaches: (i) well-curated prompting techniques (Zheng et al., 2023;Yao et al., 2024) on LLMs of exorbitant * Equal contribution size like GPT-4 (OpenAI, 2023), or (ii) finetuning a relatively smaller LLM using domain-focused data (Shao et al., 2024;Toshniwal et al., 2024;Dutta et al., 2024).Methods from the former category heavily rely on the proprietary LLM being used and are prone to fail absolutely when employed with less powerful models.The latter category, though cost-effective compared to humongous LLMs, often loses in generalizability due to a narrow training domain.</p>
<p>The chronicle of decomposed reasoning.A number of recent literature has pointed out that LLMs tend to perform better on complex reasoning tasks when the problem is decomposed into step-by-step subproblems (Zhou et al., 2023;Khattab et al., 2022;Juneja et al., 2023).Earlier techniques demonstrated the superiority by providing the model with examples containing the original problem decomposed into multiple sub-problems along with their answers (Zhou et al., 2023).However, Juneja et al. (2023) illustrated that decoupling the decomposer from the solver by finetuning a separate decomposer language model (LM) to coordinate with a larger solver LM is beneficial to simply prompting a single monolithic LM to decompose and solve.Echoing their findings, Wu et al. (2024) also found that distilling decomposition abilities from a larger LM to a smaller LM is much more generalizable compared to decomposing the solver abilities directly.</p>
<p>Our contributions.However, a major bottleneck in existing methods of decomposer finetuning is the lack of tightness between the decomposersolver interactions.Typically, the decomposition is done in a memoryless manner, with or without the solver's initial response; no strategy is employed to track whether the solver can follow the decomposed chain of reasoning.Towards this very end, we propose a novel multi-LLM coordination framework, Language Model Multiplex (LM 2 ).LM 2 is built upon three separate LMs, each dedicated to : How many distinct, non-equilateral triangles with a perimeter of 60 units have integer side lengths , , and such that , , is an arithmetic sequence?Let be the common difference, so and We can assume that is positive In particular, can't be 0, because the triangle is not equilateral Then the perimeter of the triangle is , so Hence, the sides of the triangle are , 20, and These sides must satisfy the triangle inequality, which gives us Solving for , we find , or Therefore, the possible values of are 1, 2, , 9, which gives us possible triangles 1.We know that for a triangle with side lengths a, b, and c, the sum of the lengths of any two sides must be greater than the length of the third side.The inference procedure of LM 2 on a question from the MATH dataset.A question (in blue) is provided to the Solver LM that produces an incorrect answer (in red).The question is then provided to the Decomposer LM that generates the concepts and step-by-step subquestions (in lilac).Each subquestion is answered by the Solver LM, and the sub-answer is verified by a Verifier LM.If the Verifier LM approves the sub-answer, that subqustion-subanswer pair is added to the context of reasoning steps; otherwise, a new subquestion is generated.The question, concepts, subquestions, and subanswers are provided in context to the Decomposer LM to generate the next subquestion.Finally, the question, concepts, subquestions, and subanswers are provided to the Solver LM to generate the final answer (in green).three different components of complex multistep reasoning -a solver LM is responsible for answering questions; a verifier LM provides feedback on the correctness of the output from the solver, and a decomposer LM identifies the basic concepts required to solve the problem and generates stepby-step subproblems by decomposing the original question (see Figure 1 for a working example).Unlike prior approaches, the decomposer in LM 2 generates each subproblem depending on the solver's answers to prior subproblems, along with the verifier's feedback on those answers.Furthermore, the decomposer generates the conceptual requirements to solve the problem, which further streamlines the solver LM.Irrespective of the complexity of the underlying reasoning, the world knowledge required to answer any question is typically better preserved in larger, proprietary LMs.Considering this, we use  as the solver without finetuning.For both the decomposer and verifier, we implement parameter-efficient finetuning (Hu et al., 2022) of LLaMA-2 (13 billion parameters) separately.First, these models are finetuned separately towards the tasks of decomposition and verification using datasets annotated by GPT-4.The decomposer is then taught to coordinate with the solver and the verifier models in a policy learning setup.LM 2 achieves promising performance across a diverse set of reasoning tasks.On the MATH dataset of mathematical reasoning, LM 2 outperforms the best decomposer-tuning baseline by a staggering margin 8.1% of absolute accuracy on average.Although LM 2 uses the training split of the MATH dataset for tuning the decomposer and the solver, it seamlessly generalizes to out-of-distribution tasks in MedQA and JEEBench, outperforming the best competitive baseline with 9.7 % and 7.71% difference on absolute accuracy respectively.</p>
<p>Beyond the discourse of overall numbers, we perform in-depth ablation analyses to identify the roles of each component of the model.We observe that (i) the verifier LM and concept generated by the decomposer LM play a crucial role in generalizing out-of-distribution reasoning tasks like MedQA, JEEBench Chemistry, etc.; (ii) finetuning the decomposer is crucial for better concept identification -finetuned LLaMA-2 7B generates more effective conceptual requirements compared to even GPT-4; (iii) even while not using all the modular components of LM 2 , the prompt template of structured reasoning boosts the performance of GPT-4.</p>
<p>Related Work</p>
<p>The efficacy of explicitly generating intermediate reasoning steps over direct generation of the required answer was first demonstrated by Nye et al. (2021).Chain-of-thought prompting (Wei et al., 2022) generalized the scratchpad learning of Nye et al. (2021) into an in-context learning regime using LLMs.Chain-of-thought and its successors (Chen et al., 2022;Yao et al., 2024) typically let the decomposition of a composite, multi-step reasoning problem remain implicit in the LLM.Zhou et al. (2023) demonstrated that instead, an explicit call to the LLM to generate multiple smaller problems that are steps to answer the original query achieves more robust reasoning.Their proposed method, Least-to-Most prompting, uses these simpler subproblems and their answers as the context to solve the original problem.Similarly, Khot et al. (2023) proposed a promptingbased problem decomposition approach where the LLM is asked to decompose a complex task using few-shot examples.However, this still burdens a single language model in handling both decomposition and solution.Juneja et al. (2023) circumvented this challenge by distilling the decomposition abilities into a relatively smaller language model.Their proposed method, DaSLaM, utilizes two separate language models that coordinate with each other to solve complex reasoning problems.Their findings suggest that finetuning the decomposer is more generalizable than finetuning the solver model.This has been further supported by Wu et al. (2024) recently.Tarasov and Shridhar (2024) explored the distillation of decomposition abilities via offline reinforcement learning.Khattab et al. (2022) proposed a programmatic retrieval augmentation framework, namely Demonstrate-Search-Predict (DSP), for knowledge-intensive generation tasks.DSP relies on the coordination between a generative LM and a retrieval model through sophisticated programs.Recent attempts have been made to incorporate dense verifiers (typically, a finetuned, bidirectional language model acting as a classifier) aiding a generative model towards robust, verifiable problem solving and text generation (Cobbe et al., 2021;Sun et al., 2023).Different techniques for verification of LM-generated outputs have been pro-posed subsequently, such as self-verification (Weng et al., 2023), majority voting (Li et al., 2023), etc.</p>
<p>Methodology</p>
<p>Our proposed method, LM 2 , is built upon the coordination of multiple LMs to perform reasoning in a modular fashion.However, such coordination is not implicit in the pertaining stage of a model; instead, we seek to inculcate this ability via finetuning (parts of) the LM multiplex.To this end, LM 2 is built upon three functional components: a (preferably larger) solver model, a decomposer model, and a verifier model.</p>
<p>For fine-grained control over the function of the different components of LM 2 , we make use of a structured, step-by-step input-output framework (see Figure 1).The role of each of the modules in LM 2 is described as follows.</p>
<p>Decomposer</p>
<p>The decomposer LM guides the solver LM to solve a multi-step reasoning question in two ways.First, it provides the solver model with a set of concepts required to solve the problem.Second, it tells the solver LM what is the next sub-question required to solve given the previous sub-questions and their answers.More specifically, the decomposer LM is a function that can be defined as D(q, {s i , sa i }, c) : Q × S × SA → {S, C}, where q represents the initial question to be solved, {s i , sa i } denotes the set of previous sub-questions (s i ) and their corresponding answers (sa i ), and (c) signifies whether the function needs to predict the concept or the next sub-question.Q is the space of all the questions, S is the space of all sub-questions, SA is the space of all sub-answers, and C is the space of all concepts.</p>
<p>Supervised finetuning.The decomposer training is performed in two stages similar to (Juneja et al., 2023).The first stage is supervised finetuning, where the language model is finetuned on a dataset prepared using GPT-4.To create the dataset, we provided GPT-4 with a question and its gold reasoning.It was then asked to first generate all the concepts required to solve the question, followed by sub-questions and sub-answers.Only the questions that were answered correctly were included in the dataset.Each sample in the dataset can be expressed as a tuple {Q, c, {s i , sa i } n i=1 , s n+1 }, where s n+1 is the next sub-question given the previous sub-questions and answers.The decomposer was then finetuned on the standard language modelling objective.</p>
<p>Policy optimization.With the supervised finetuning step, the decomposer LM is conditioned to respond to reasoning problems with concepts and decomposed subquestions.However, it is still not able to take the feedback from the solver and the verifier models into account.To this end, we utilize Proximal Policy Optimization (Schulman et al., 2017) with the decomposer as the policy and the solver and the verifier model as a black-box environment.Precisely, we compute different types of rewards utilizing the feedback from the verifier model that takes the solver model's response into account at each step and provides the decomposer with necessary refinement signals.</p>
<p>Verifier</p>
<p>Given the complexity of multistep reasoning, we need the verifier to be able to provide nuanced feedback to the decomposer on the possible mistakes made by the solver; a binary correct/incorrect message as employed by prior works with verifiers (Li et al., 2023;Weng et al., 2023) will limit the decomposer model's scope of vision.For fine-grained control, the verifier is finetuned on a supervised dataset containing a question, an answer with an error made in the correct answer, a classification for the type of error, and an explanation for the classification.The verifier classifies the given input into nine classes as follows: 1 Conceptual mistakes, 2 Computational mistakes, 3 Procedural mistakes, 4 Misunderstood question, 5 Mistake in the first step, 6 Mistake in first half, 7 Mistake in second half, 8 Mistake in last step, and 9 No mistake.The dataset was produced using GPT-4, asking it to generate an explanation for the classification given the correct solution, wrong solution and the classification.The verifier is finetuned to generate the explanation and the classification (see Section 3.3 for examples of each type of error message and explanation).</p>
<p>Training with Decomposer Feedback</p>
<p>The training dataset curated for the decomposer LM consists of only the correct answers; hence, the decomposer is blind to the possible errors that the language model can make.In order to make the decomposer generate meaningful questions, we further finetune the decomposer while working in synergy with the solver language model using Policy gradient methods.</p>
<p>Environment.The environment consists of a black-box solver model Θ.The model Θ generates an answer to the current question given the concepts and previous questions and their answers.</p>
<p>Policy, action and state space.The decomposer language model ϕ comprises the policy network.A state s in the state space S is defined by the concatenation of the initial state s 0 and all the actions taken from the initial state to the current state.The initial state s 0 is defined as the initial question Q.The action space is defined as the token space of the language model ϕ.Hence, a state s n can be represented as (s 0 , {a i } n i=1 ), where a i is the action taken at the i th time step.</p>
<p>Reward function.The reward is based on the feedback given by the verifier at each sub-question produced by the decomposer.The reward structure is intuitively designed to impose penalties for errors occurring in earlier sub-questions relative to those occurring in later ones.This is because fixing an early mistake can significantly increase the chances of the question being correct.Further, the policy is penalised more for conceptual and procedural mistakes as compared to computational mistakes.We construct the reward function for the k th subquestion as follows:
R = γ k 9 i=1 r i (1)
where γ &lt; 1 is the discount factor responsible for imposing more penalties on the earlier generations.r i are the rewards for individual feedback given by the verifier as defined below (for each type of reward, we provide an example question asked by the decomposer, an erroneous answer to that question by the solver, type of error identified and the explanation generated by the verifier in red textboxes).</p>
<p>Conceptual correctness reward is defined as,
r 1 = −0.15I[V (s k , sa k ) = 1] (2)
where I is the indicator function, V is the verifier that takes in input the k th sub-question (s k ) and its answer produced by the solver (sa k ) and outputs the category of mistake.This reward accounts for any mistake made by the solver in understanding or while applying a concept incorrectly.</p>
<p>Q: What is a, b, c in terms of common difference d?</p>
<p>A :Since a, b, and c form an arithmetic sequence, we can express c in terms of a as c = a + d, where d is the common difference.</p>
<p>Verifier: Conceptual Mistakes: The student misunderstood the concept of finding the possible values of the common difference in an arithmetic sequence to form a triangle with a given perimeter.</p>
<p>Computational correctness reward is defined as,
r 2 = −0.05I<a href="3">V (s k , sa k ) = 2</a>
This reward accounts for cases where the model has made any mistakes in calculation or formula application.These kinds of mistakes are weighted less compared to all other types because an external tool can be easily used for these mechanical tasks.</p>
<p>Q : What is the value of a + c?</p>
<p>A : Since we are given that a, b, and c form an arithmetic sequence, we can express b as the average of a and c.This</p>
<p>gives us b = a+c 2 .The perimeter of the triangle is given by
P = a + b + c = a + a+c 2 + c = 3a+3c 2
. We are given that the perimeter is 60 units, so we have 60 = 3a+3c Procedural correctness reward is defined as,
r 3 = −0.15I<a href="4">V (s k , sa k ) = 3</a>
This encompasses any mistake made by the model while following a step-by-step procedure whose information is already given in-context while solving the problem.Question understanding reward is defined as,
r 4 = −0.2I<a href="5">V (s k , sa k ) = 4</a>
This reward accounts for the cases when the model has made a mistake in understanding the subquestion.Either it has not answered the quantity asked in the sub-question.This gives a signal that the sub-question was either incoherent with the previous questions or was too complex for the model to answer.This kind of mistake is important to address and, hence, is given a higher weight.Reward based on place of mistake.As discussed above, later mistakes are penalised less than the earlier ones.Hence, if a mistake is made in the first step, it is given a reward of −0.2.If the model makes a mistake in the first half of the sub-answer, it is given a reward of −0.12.For a mistake in the last half of the sub-answer, it is given a reward of −0.08.If the mistake is made in the last step, it is given a reward of −0.05.</p>
<p>No-mistake rseward is the case when the model has not made any mistake in answering the subquestion and is given a positive reward of +1.</p>
<p>Inference</p>
<p>During the inference, the decomposer, solver, and verifier models work together to answer a given question (see working example in Figure 1).During the inference, the decomposer first produces a list of concepts required to solve the question.Then, given the question and concepts as context, the decomposer produces a sub-question.The subquestion is answered by the solver.Now, given the sub-question and sub-answer, the verifier provides feedback in the form of a multi-class classification into the above-described classes of mistakes.If the feedback provided by the verifier consists of either a conceptual mistake, procedural mistake, mistake in understanding or mistake in the first step, we again generate the sub-question.</p>
<p>Experiments</p>
<p>For all the experiments, LM 2 uses the OpenAI text-davinci-003 model (hereafter mentioned as GPT-3.5) as the solver and LLaMA-2 13B (Touvron et al., 2023) as the base models for the decomposer and the verifier.</p>
<p>Training data curation</p>
<p>For the first stage of finetuning of the decomposer LM, we curated a dataset of 15, 396 question, concept, sub-question, sub-answer tuples.The questions were taken from the train split of the MATH dataset (Hendrycks et al., 2021).The questions were taken from the MATH dataset.For verifier LM finetuning, a dataset of 3, 674 question-answerclassification tuples was generated.Details of the prompts used for each of these steps are provided in Appendix A.</p>
<p>Training details</p>
<p>We finetune LLaMA2-13B for both the decomposer and verifier.We train for 8 epochs with a batch size of 128, learning rate 2e-5, warmup steps of 100, a Lora r value of 4, LoRA Alpha of 16 and dropout of 0.05.The models were trained in 8-bit quantization on an 80G A100 GPU.</p>
<p>For the second stage of fine-tuning, we finetuned the last 3 layers of LoRA adapters, using a batch size of 16, gradient accumulation steps=4, init kl coef=0.01,target=4.For inference, we used a temperature of 0 in all experiments for consistency of results with a max output length of 2000.</p>
<p>Evaluation</p>
<p>We evaluate our method on hard reasoning datasets that require multi-step reasoning.These datasets include MATH (Hendrycks et al., 2021) (test split), JEEBench (Arora et al., 2023), and MedQA (Jin et al., 2020) (English questions).The MATH dataset contains math questions from challenging math competitions, since it was also used for training, this shows our performance on in-domain questions.Next, we evaluate on the out-of-distribution datasets like JEEBench which contains PCM questions extracted from the JEE Advanced exam and MedQA which contains open-domain questions from professional medical board exams.We only evaluate questions in the English language.</p>
<p>Baseline Details</p>
<p>We compare LM 2 with five existing methods: Chain-of-thought prompting (CoT) (Wei et al., 2022), Least-to-most prompting (L2M) (Zhou et al., 2023), Progressive Hint Prompting (PHP) (Zheng et al., 2023), Demonstrate-Search-Predict (DSP) (Khattab et al., 2022), and DaSLaM (Juneja et al., 2023).The original setting of PHP requires an 8-shot prompting; however, since all other methods including LM 2 predict in the zero-shot setting, we use PHP in 1-shot for a fairer comparison.</p>
<p>Ablation Study</p>
<p>In our investigation, we perform five types of ablation studies aimed at comprehensively understanding the significance of each component within the LM 2 pipeline.We start with investigating the relevance of the verifier by conducting an experiment where we remove it entirely (LM 2 \V).Here, we accept each question generated by the decomposer during the inference process without any verification.Then, we explore the role of concepts within the pipeline.Here, we alter the approach by instructing the decomposer to directly generate sub-questions, without providing the concepts to the Solver LM during the answer generation phase (LM 2 \C).Following this, we investigate the incremental gains achieved through the second stage of finetuning via policy learning.To accomplish this, we analyze the performance of the decomposer checkpoint after the initial stage of fine-tuning, referred to as (LM 2 \RL).</p>
<p>To assess the impact of different types of rewards provided, we partition the rewards into two distinct categories: i) based on the type of mistake, which encompasses conceptual, computational, procedural, and question understanding correctness, and ii) based on the position of mistake.Subsequently, we come up with two ablation variants, finetuned using each category of rewards: LM 2 -Type and LM 2 -Position.</p>
<p>Results</p>
<p>We summarize the performance of LM 2 along with the baseline methods on the MATH and MedQA datasets in Table 1 and on the JEEBench dataset in Table 2. Across all the datasets, LM 2 improves upon existing methods (using GPT-3.5 solver) by a huge margin.It demonstrates an average 8% improvement on the MATH dataset and an average 2.5% improvement on the JEEBench dataset as compared In 3/12 cases, LM 2 outperforms GPT-4.(Middle third) we observe a large drop in performance with each ablation variant, pointing towards an efficient integration of these modules into LM 2 (see Section 4.5 for the description of each variant).(Bottom third) Performance of the structured answer generation employed in LM 2 , without decomposer and verifier, using GPT-3.5 and GPT-4 as solvers.</p>
<p>to the best-performing baseline DaSLaM.</p>
<p>Can it improve on out-of-domain tasks?In both DaSLaM and LM 2 , the solver model is kept frozen with the hope of retaining generalizability.However, the decomposer model in both methods (and the verifier in LM 2 ) are finetuned using mathematical reasoning problems.This raises the question of the generalizability of these finetuned components over problems other than mathematical reasoning.One of the most significant challenges with DaSLaM is that it is not able to perform well on out-of-domain tasks like JEEBench Chemistry.We find that our method can surpass this limitation as can be seen in Tables 1 (MedQA) and 2 (JEEBench Chemistry).While DaSLaM degrades the performance over CoT on MedQA, LM 2 achieves an absolute accuracy gain of 6.8 percentage points.</p>
<p>How important is the verifier?Next, we seek to investigate the relative importance of each component in our pipeline.We observe that the accuracy decreases substantially upon removing the verifier model (LM 2 \V in the middle third of Table 2).We can see that there is a drop of 13.0% in Chemistry versus 10.08% in Physics and 3.4% in Math subsets.The relative drop in accuracy with the ablation of the verifier is sharper with multi-answer, numeric, and integer answer questions.This makes sense given the computational reasoning requirement is higher in these problems and the verifier plays a crucial role in guiding the decomposer and the solver along the correct reasoning path.How important are the concepts?As can be seen from Table 2, removing concepts decreases the accuracy of Physics subset by 11.6%, Maths subset by 6.03%, and Chemistry subset by 17.5%.This shows that concepts also play a very important role in improving the performance on out-of-domain datasets like Physics and Chemistry.Typically, LM 2 \C fares worse than the rest of the ablation variants, demonstrating that the concepts are the most important component in LM 2 .</p>
<p>GPT-4 as concept generator?We also check how our decomposer compares to GPT-4 while generating concepts.To compare this, we prompt GPT-4 to generate concepts given the question.We observe that there is an average decrease of 9.13% when generating concepts using GPT-4 when compared to the Decomposer model, indicating the higher quality of concepts generated as a result of feedback-based fine-tuning.</p>
<p>What is the effect of feedback-based finetuning?The effect of feedback-based fine-tuning is evident when comparing the performance of the decomposer without the second stage of fine-tuning alongside the verifier to that of LM 2 .On average, we observe a notable decrease of 9.6% in performance when the second stage of fine-tuning is omit-ted.This finding highlights the significance of finetuning as a crucial step in optimizing model performance.However, the importance of concepts and the verifier appears to outweigh that of fine-tuning.This suggests that while fine-tuning contributes to improved model performance, the incorporation of concepts and a verifier into the model architecture yields more substantial enhancements.</p>
<p>How does the structured answering template contribute?Recall that in LM 2 , we introduce a novel, structured answering template for controllable coordination between the three models.It is imperative to investigate the role of such a template alone behind the performance boost.We make use of the template with two different solver models, GPT-3.5 and GPT-4.As we can see in the bottom third of Table 2 (coined as modelname-SP), both models improve upon their base performance with our structured template.However, the stronger GPT-4 model is able to utilize the template much more efficiently, with an average gain of 7.8% across the JEEBench problems.Typically, improvement on Physics problems is higher than the Math problems, indicating that language models are not very good at retrieving physics concepts and solving the problem when using chainof-thought prompting.It should noted that while the structured answering template alone is a powerful boost, it is much weaker alone without the complete coordination in LM 2 .</p>
<p>Does guided reasoning help limit token usage?An important challenge with iteratively interacting with an LLM is the increased token usage that will translate to expenses in either computational or monetary terms.In Figure 2, we plot the average token usage (per problem) incurred by the solver model (GPT-3.5)while using LM 2 and DaSLaM against that of base chain-of-thought generation.Note that we only show the token usage corresponding to the modified responses while using LM 2 and DaSLaM.Both these methods originally use base CoT to generate the initial response and therefore, their total token usage will always be higher than that of CoT.However, the added structure and guided reasoning significantly reduce the token usage in the modified response.LM 2 prevails in this aspect too.A major reason behind this is the step-by-step synergy between the decomposer, the solver, and the verifier in LM 2 .Since the decomposer generates the subquestion depending upon the response from the solver to the previous subquestion, the chances of redundant generation decrease, as opposed to DaSLaM where the subquestions are generated all at once.Example analysis.To further understand the nuances of LM 2 , we perform an analysis of the generated output on an example from the MATH dataset (see Figure 3).We compare between LM 2 , DaSLaM and GPT-4 with CoT.As we can see, GPT-4 makes an incorrect interpretation of the question itself.It assumes that the total journey after delay takes 10 hours, leading to an incorrect choice of option.The subquestions produced by DaSLaM do not adhere to the order of reasoning required to solve the problem and generate redundant questions.It starts with asking What is the total distance to be covered?However, in the second question, it asks for the speed of the train which is already given in the question itself.The 3rd subquestion generated by DaSLaM is actually the original question, and the solver makes a numerical mistake by simplifying the fraction 3d 4 75 to d 300 instead of d 100 .Without a verifier, this erroneous response is integrated into the reasoning context of the solver.In the next questions, the same problem is asked to be solved and the solver continues to make incorrect responses.With LM 2 , we observe a much more well-defined, crisp line of questioning by the decomposer model; the solver is able to reach the correct answer option without regenerating the same information or drawing incorrect subanswers.</p>
<p>Conclusion</p>
<p>In this paper, we present LM 2 , a cooperative cohort of generative language models working together to solve complex reasoning problems.LM 2 utilizes a frozen solver model that is guided to solve reasoning problems by incrementally answering questions framed by a decomposer model and checked by the verifier model that is trained to coordinate with each other.We find that LM 2 proves its supremacy over existing methods over a variety of reasoning tasks, both in-domain and out-domain.We find that despite being trained using mathematical reasoning examples, our proposed structured response scheme along with the fine-grained verification strategy plays a crucial role in generalizing LM 2 to heavily out-of-distribution tasks like medical question answering and chemistry.</p>
<p>Limitations.Despite promising results, LM 2 bears some inherent limitations.Compared to purely prompting-based methods, it requires a certain computational overhead for the two-staged training.With proprietary LLM-based solvers, LM 2 incurs extra token usage over single-pass solutions like chain-of-thought.Implicit limitations of the solver model, like lack of length generalization, arbitrary digit manipulation, etc. are expected to be inherited in LM 2 as well.A possible future work can be towards incorporating deterministic solvers and tools into the multiplex.</p>
<p>is a, b, c in terms of common difference d? SA:Since a, b, and c form an arithmetic sequence, we can express c in terms of a as c = a + d, where d is the common difference</p>
<ol>
<li>Since we are given that a, b, and c form an arithmetic sequence, we can express b as the average of a and c.This gives us .... 6.Since a, b, and c form an arithmetic sequence, we can express c in terms of a as c = a + d, where d is the common difference.... 10.This gives us the side lengths of the triangle as a = 10, b = 15, and c = 30.Hence final answer x,y,z be sides of triangle, then x+y&gt;z, x+z&gt;y, z+y&gt;x 2. If x,y,z are in arithmetic progression then y-x=z-y SQ:What is the value of a + c? SA:Since we are given that a, b, and c form an arithmetic sequence, we can express b as the average of a and c.This gives us .The perimeter of the triangle is given by P = .We are given that the perimeter is 60 units, so we have .Concepts: Solving for a + c, we get a + c = 40 SQ: What is triangle inequality in terms of a,b,c?SA: Triangle inequality sayd</li>
</ol>
<p>Figure</p>
<p>Figure1: The inference procedure of LM 2 on a question from the MATH dataset.A question (in blue) is provided to the Solver LM that produces an incorrect answer (in red).The question is then provided to the Decomposer LM that generates the concepts and step-by-step subquestions (in lilac).Each subquestion is answered by the Solver LM, and the sub-answer is verified by a Verifier LM.If the Verifier LM approves the sub-answer, that subqustion-subanswer pair is added to the context of reasoning steps; otherwise, a new subquestion is generated.The question, concepts, subquestions, and subanswers are provided in context to the Decomposer LM to generate the next subquestion.Finally, the question, concepts, subquestions, and subanswers are provided to the Solver LM to generate the final answer (in green).</p>
<p>2.</p>
<p>Solving for a + c, we get a + c = 20 Verifier : Calculation Mistakes: The model has made a calculation mistake while calculating the value of a + c. a + c should be 40.</p>
<p>Q:</p>
<p>What are the possible values of a, d?A : Substituting this into the equation a + c = 20, we get a + (a + d) = 20, which simplifies to 2a + d = 20 − a.Since the side lengths must be integers, a and d must be integers.We can then try different values of a and d that satisfy the equation 2a + d = 20 − a.After trying different values, we find that the only solution that gives integer side lengths is a = 10 and d = 20.Verifier : Procedural Mistakes: The student made a mistake in the procedure of solving the problem by not accurately finding the possible values of the common difference.</p>
<p>Q:</p>
<p>How many distinct values of a, b, c are possible?A : This gives us the side lengths of the triangle as a = 10, b = 15, and c = 30.Verifier : Mistakes Understanding Question: The model has made a mistake by not giving the number of distinct values.</p>
<p>Figure 2 :
2
Figure 2: Comparison of token generation cost.We depict the average number of tokens generated by the solver model using different methods to solve the given question averaged over 50 questions from the JEEBench dataset.</p>
<p>Figure 3 :
3
Figure 3: Comparison of GPT-4, DaSLaM and LM 2 on an example from MATH dataset.</p>
<p>Table 1 :
1
Performance comparison of LM 2 with the baselines on MATH and MedQA datasets using GPT-3.5 as the solver LM.
DatasetMethod CoT L2M PHP DSP DaSLaM LM 2PnC16.4 16.0 10.2 16.221.430.0NT14.4 11.09.8 20.326.141.0ALG27.6 22.4 24.0 15.333.434.0I-ALG 16.4 16.8 10.0 17.024.827.8Calc.14.0 14.58 14.28 18.818.234.0P-ALG 32.3 28.0 26.5 28.044.047.0Geom.14.2 12.5 14.0 5.221.432.0MedQA 50.3 49.8 47.5 52.350.157.1</p>
<p>Table 2 :
2
Performance of LM 2 on JEEBench Dataset along with baselines and ablation variants.(Top third) we highlight best and second best methods in boldface and underline.LM 2 generally outperforms all existing prompting techniques with GPT-3.5 on different topics and different types of questions (other than Physics MCQ questions).
MethodPhy.Math.Phy.Math.Phy.Dataset Math.Phy.Math.Chem.Chem.Chem.Chem.MCQMCQMulti.Multi.Num.Num.Int.Int.Int.Num.Multi.MCQCoT33.33 21.96.2512.03.031.6912.510.817.311.611.640.0PHP22.22 17.076.257.593.031.690*4.011.79.712.237.5L2M22.22 21.96.2512.53.033.3810.010.813.09.710.020.0DaSLaM55.529.518.716.06.0610.115.711.714.29.211.614.6GPT455.534.127.521.515.111.822.724.317.925.548.360.0LM 251.85 30.1826.816.4 15.15 13.116.213.526.023.226.653.3LM 2 \V37.03 24.5214.611.712.211.411.411.717.316.213.330.0LM 2 \C29.62 20.7514.69.49.0910.89.08.117.311.613.316.6GPT4-C29.62 28.314.611.5 15.15 11.49.011.421.723.233.3330.0LM 2 \RL33.33 21.918.712.712.210.110.08.117.312.413.327.3LM 2 -Type46.128.020.314.013.411.415.013.524.023.223.645.4LM 2 -Position 38.4 24.5216.012.912.211.415.010.824.020.620.333.0GPT35-SP33.329.27.512.69.08.412.58.017.69.212.241.6GPT4-SP61.136.530.026.530.014.2 43.75 32.017.636.549.166.6
A Training Data CreationThe data was generated using GPT-4.A temperature of 0.7 is used to ensure diversity in the generated data.We only stored the sub-question, subanswer dataset if the number of sub-questions generated was more than three, this was done to ensure high data quality so that the model is able to decompose longer and more difficult questions effectively.First, we generate all the concepts, then the subquestions given the question and the gold chain of thought.Finally, we generate the sub-answer given the question, a gold chain of thought and the sub-question to be answered.for the verifier, we first ask the LLM to answer the given question using standard COT prompting.Then based on the correctness of the answer, we take the solution chain of thought produced by the LLM and the gold answer and ask the LLM to classify the produced solution based on the mistake made.If the answer is correct, we store it separately and include it to make up to 10% of the dataset with the label as 'No Mistake'.Prompts for the data curation are given below.A.1 Verifier Data CreationA.1.1 PromptYou are a teacher, and you are grading a student's answer to a question.The student's answer is as follows: {COT_LLM} The correct answer is as follows: {COT_gold} Please provide feedback to the students on the mistakes they have made.You need to fill out a rubric and classify the mistakes into the following categories:1. Conceptual Mistakes: The student has misunderstood the concept or has applied the wrong concept.2. Computational Mistakes: The student has made a mistake in the calculations.3. Procedural Mistakes: The student has made a mistake in the procedure of solving the problem.4. Mistake in understanding the question: The student has made a mistake in understanding the question.5. Mistake in the first step: The student has made a mistake in the first step of the solution.6. Mistake in the first half: The student has made a mistake in the first half of the solution.7. Mistake in the second half: The student has made a mistake in the second half of the solution.8. Mistake in the last step: The student has made a mistake in the last step of the solution.9.No mistake: The student has not made any mistake.Please first provide feedback then fill the rubric and then finally tell your feedback to the student in between <feedback> and </feedback> tags as shown below: For example, if you want to tell the student that they have made a mistake in the first step and a conceptual mistake, then you need to write the following: <feedback> 1,4 </feedback> Do not write anything else in between <feedback> and </feedback> tags except the numbers.Now, please provide feedback to the student on the mistakes they have made.A.2 Decomposer Data CreationA.2.1 Concepts data creationI have a question's solution, tell me all the specific concepts, theorems and formulas (separated by a comma,) used in it.An example is given below.Question: How many primes are in the row of Pascal's Triangle that starts with a 1 followed by a 6? Answer: If the row contains a 1, then a 6, then the binomial coefficients must be 6 0 and 6 1 .All we need to check now are 6 2 and 6 3 , since 6 0 = 6 6 , 6 1 = 6 5 , and 6 2 = 6 4 .6 2 = 6! 4!×2! = 15, andNone of those is prime, so there are 0 prime numbers in the given row.Concepts:Coefficients in Pascal's Triangle, Binomial Coefficients Formula, Prime Numbers Question: question Answer: answer Concepts:A.2.2 Sub-question data creationI have a question, it's a solution and a sub-question.Your task is to break the question into sub-questions based on the steps in the answer.Keep the following tips in mind: 1.Make sure not to break the question into trivial sub-questions, the sub-questions should be informative.2. The sub-questions should not require multiple steps to answer, something like 2-3 steps to solve is ideal.3.One way to break the question could be to identify what all quantities are required in the question by observing it's answer and then try to frame sub-questions based on the unknown entities.4. Make sure to put each question in the question tag like $ question(What is the acceleration of the car as a function of time?)$One example is given below.Question: How many primes are in the row of Pascal's Triangle that starts with a 1 followed by a 6? Answer: If the row contains a 1, then a 6, then the binomial coefficients must be 6 0 and 6 1 .All we need to check now are 6 2 and 6 3 , since 6 0 = 6 6 , 6 1 = 6 5 , and 6 2 = 6 4 .6 2 = 6! 4!×2! = 15, and 6 3 = 6! 3!×3!= 20.None of those is prime, so there are 0 prime numbers in the given row.Sub-questions: $ question(How can the first two numbers be represented in form of binomial coefficients?)$,$ question(What are the values of all the coefficients in the row?)$, $ question(How many of the above numbers are prime?)$Question: question Answer: answer Sub-questions:A.3 Sub-answer data generation I have a question, it's solution and a sub-question.I want you to answer the subquestion along with an explanation.Make sure to put the sub-answer in the answer tag like $sub-answer(The acceleration of the car at time t = 2 seconds is speed / time = 2m/s/2s = 1m/s 2 )$ Think step by step.Question: question Answer: answer Sub-question: sub-question-array[i] Sub-Answer:
Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, Mausam , 2023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Frugal lms trained to invoke symbolic solvers achieve parameter-efficient arithmetic reasoning. Subhabrata Dutta, Ishan Pandey, Joykirat Singh, Sunny Manchanda, Soumen Chakrabarti, Tanmoy Chakraborty, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, 2020</p>
<p>Small language models fine-tuned to coordinate larger language models improve complex reasoning. Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty, 10.18653/v1/2023.emnlp-main.225Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Making large language models better reasoners with stepaware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 2023</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, CoRR, abs/1707.063472017</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Li, Daya Wu, Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Towards verifiable text generation with evolving memory and self-reflection. Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, Dawei Yin, arXiv:2312.090752023arXiv preprint</p>
<p>Distilling llms' decomposition abilities into compact language models. Denis Tarasov, Kumar Shridhar, 2024</p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, arXiv:2402.101762024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao. 2023</p>
<p>Divide-or-conquer? which part should you distill your llm?. Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, Vinod Vg, Navdeep Vydiswaran, Yizhe Jaitly, Zhang, 2024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4536 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4536</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4536</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-7ae64df494f0f34aff7d2827e3deace4463e0952</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ae64df494f0f34aff7d2827e3deace4463e0952" target="_blank">Automated Extraction of Molecular Interactions and Pathway Knowledge using Large Language Model, Galactica: Opportunities and Challenges</a></p>
                <p><strong>Paper Venue:</strong> Workshop on Biomedical Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This study presents an initial investigation of the efficacy of utilizing a large language model, Galactica in life science research by assessing its performance on tasks involving protein interactions, pathways, and gene regulatory relation recognition.</p>
                <p><strong>Paper Abstract:</strong> Understanding protein interactions and pathway knowledge is essential for comprehending living systems and investigating the mechanisms underlying various biological functions and complex diseases. While numerous databases curate such biological data obtained from literature and other sources, they are not comprehensive and require considerable effort to maintain. One mitigation strategies can be utilizing large language models to automatically extract biological information and explore their potential in life science research. This study presents an initial investigation of the efficacy of utilizing a large language model, Galactica in life science research by assessing its performance on tasks involving protein interactions, pathways, and gene regulatory relation recognition. The paper details the results obtained from the model evaluation, highlights the findings, and discusses the opportunities and challenges.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4536.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4536.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (scientific large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose scientific LLM trained on large amounts of scientific literature; in this paper it was applied to extract biological qualitative relations (protein-protein interactions, gene membership in pathways, and gene regulatory relation types) from database entries and text snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Prompted few-shot extraction (generative / binary / multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The authors used Galactica with prompt engineering and few-shot prompting (0/1/2/3-shot ablation) to perform three task formats: (1) generative questions asking the model to list proteins/genes associated with an entity (STRING Task 1, KEGG Task 1); (2) binary yes/no relation questions for pairs (STRING Task 2, KEGG Task 2) using positive and constructed negative pairs; and (3) multiple-choice classification of gene regulatory relations using INDRA text snippets as context (INDRA Task). Prompt templates are listed in Appendix C and an ablation over number-of-shots determined the preferred shot setting per task. The pipeline processed batches on 8× V100 GPUs using Galactica's tensor-parallel option; INDRA evidence text was used to provide contextual grounding for relation classification. The authors also tested consistency between generative and binary answers and discussed plans to iterate prompts (chain-of-query/CoT) to build context for items lacking supporting text.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>molecular biology / bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Qualitative mechanistic/causal relations and membership patterns: protein–protein interactions (binding relationships), gene membership in biological pathways, and gene regulatory relation types (activation, inhibition, phosphorylation, dephosphorylation, ubiquitination, deubiquitination).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) From INDRA text: 'Upon binding with Shh, Ptc1 inactivation allows Smo to initiate signaling ... through the Gli family of transcription factors.' → classified as Activation. 2) KEGG pathway membership example: genes predicted to be involved in 'Adherens junction' (e.g., CDH1, CTNNA3, CTNNB1). 3) STRING generative example: model lists proteins related to IKZF4 (IKZF1, IKZF2, IKZF3, IKZF5).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to ground-truth annotations from structured databases (STRING for PPIs, KEGG for pathway membership, INDRA for regulatory statements) using held-out samples; constructed negative samples from unconnected pairs (or genes in other pathways); ablation studies on prompt shot count; consistency checks between generative and binary task outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>STRING Task 1 (generative): precision ≈ 0.166 (1K) and 0.161 (10K) (Table 1, Table 8). STRING Task 2 (binary): micro F-scores ≈ 0.552 (1K), 0.558 (10K), 0.562 (100K) (Table 2); precision on Task1 pairs: 0.691 and model consistency 0.726 for certain subsets (Table 3). KEGG Task 1 (generative): precision ≈ 0.256 (Table 4). KEGG Task 2 (binary): micro F-score ≈ 0.562 across 35,174 pairs and precision/consistency numbers up to 0.883 for Task1 pairs (Tables 5,6). INDRA Task (multiple-choice): micro F-scores vary by number of choices (e.g., 2-class F1 ≈ 0.704; performance declines with more classes but results reported for 2–6 class setups in Table 7 and ablation Tables 9–12). Ablation showed best shot-count depended on task (e.g., 1-shot optimal for several tasks, 2-shot or 3-shot for others).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Primary baseline was database ground-truth (STRING/KEGG/INDRA) and constructed negative samples; no other LLMs or human-expert extractions were experimentally compared in this paper (authors state plans to evaluate GPT series, LLaMA, Alpaca, and domain-specific models like BioGPT/BioMedLM in future work).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Galactica can recover certain genes/proteins and relation types but performance is far from perfect: (1) model benefits strongly from contextual textual evidence (INDRA text) and performs better on context-rich multiple-choice tasks than on bare generative list or yes/no tasks; (2) prompting and number of shots materially affect performance (task-specific optimal shots found); (3) the model often generates name patterns based on string similarity (prefix-driven errors) causing many false positives in generative list tasks; (4) consistency between generative and binary formats is imperfect; (5) targeted collections (well-defined pathway mentions) produced better results than heterogeneous protein name extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Hallucination / name-pattern generation (predicting similarly prefixed names rather than true interactors), sensitivity to prompt wording and shot count, skewed yes/no outputs towards positive (possibly due to noisy negative samples), limited evaluation to a single model size (6.7B), reliance on databases where negative pairs may not be truly negative, and overall suboptimal accuracy that limits immediate use for high-confidence knowledge base curation without human verification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4536.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4536.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of transformer-based pre-trained language models noted in the paper for capability in complex language tasks and potential for extracting knowledge from scientific text; cited in related work as promising for few-shot/zero-shot information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described in the introduction/related work as capable of few-shot or zero-shot extraction of knowledge from text; the paper cites prior studies that used LLMs for clinical information extraction and reasoning and motivates applying LLMs to extract biological pathway and interaction knowledge. The paper does not provide a unified method for using generic LLMs beyond referencing prompting/few-shot approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing applied to biology / biomedical text</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Generally described as information/knowledge extraction (entity relations, patterns), but no specific laws/principles are presented for general LLMs in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated directly in this paper (cited prior work evaluates LLMs elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited literature claims LLMs can match traditional supervised models on some tasks; no direct comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper presents the view (from related work) that LLMs are flexible and can perform multiple tasks with few-shot prompting, motivating their application to biological knowledge extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No additional experimental detail here; paper emphasizes known general issues (prompt sensitivity, domain adaptation) and the need for further comparison to domain-specific models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4536.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4536.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other LLMs (planned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-series, LLaMA, Alpaca, BioGPT, BioMedLM (mentioned for future comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Specific LLMs the authors plan to evaluate in future work (general-purpose and domain-specific biomedical models) to compare against Galactica's performance on biological extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT series / LLaMA / Alpaca / BioGPT / BioMedLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>No methods executed in this paper — authors state intention to evaluate these models and compare to Galactica and to smaller domain-specific models, likely using similar prompting/formats (generative, yes/no, multiple-choice) and the same evaluation datasets (STRING, KEGG, INDRA).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>molecular biology / biomedical NLP (future planned evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Intended to extract similar qualitative relations (PPIs, pathway membership, regulatory relation types) for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Planned: comparative evaluation against Galactica on the same tasks/datasets; exact protocol not specified yet.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>These models are proposed as comparison baselines in future work (authors mention comparing to domain-specific models like BioGPT/BioMedLM and to smaller-sized models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>None yet; listed as planned future work to establish comparative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No experimental results yet; authors note limitations of current single-model evaluation and plan broader benchmarking.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Large language models are few-shot clinical information extractors <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4536",
    "paper_id": "paper-7ae64df494f0f34aff7d2827e3deace4463e0952",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "Galactica",
            "name_full": "Galactica (scientific large language model)",
            "brief_description": "A general-purpose scientific LLM trained on large amounts of scientific literature; in this paper it was applied to extract biological qualitative relations (protein-protein interactions, gene membership in pathways, and gene regulatory relation types) from database entries and text snippets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Galactica",
            "model_size": "6.7B",
            "method_name": "Prompted few-shot extraction (generative / binary / multiple-choice)",
            "method_description": "The authors used Galactica with prompt engineering and few-shot prompting (0/1/2/3-shot ablation) to perform three task formats: (1) generative questions asking the model to list proteins/genes associated with an entity (STRING Task 1, KEGG Task 1); (2) binary yes/no relation questions for pairs (STRING Task 2, KEGG Task 2) using positive and constructed negative pairs; and (3) multiple-choice classification of gene regulatory relations using INDRA text snippets as context (INDRA Task). Prompt templates are listed in Appendix C and an ablation over number-of-shots determined the preferred shot setting per task. The pipeline processed batches on 8× V100 GPUs using Galactica's tensor-parallel option; INDRA evidence text was used to provide contextual grounding for relation classification. The authors also tested consistency between generative and binary answers and discussed plans to iterate prompts (chain-of-query/CoT) to build context for items lacking supporting text.",
            "number_of_papers": null,
            "domain_or_field": "molecular biology / bioinformatics",
            "type_of_laws_extracted": "Qualitative mechanistic/causal relations and membership patterns: protein–protein interactions (binding relationships), gene membership in biological pathways, and gene regulatory relation types (activation, inhibition, phosphorylation, dephosphorylation, ubiquitination, deubiquitination).",
            "example_laws_extracted": "1) From INDRA text: 'Upon binding with Shh, Ptc1 inactivation allows Smo to initiate signaling ... through the Gli family of transcription factors.' → classified as Activation. 2) KEGG pathway membership example: genes predicted to be involved in 'Adherens junction' (e.g., CDH1, CTNNA3, CTNNB1). 3) STRING generative example: model lists proteins related to IKZF4 (IKZF1, IKZF2, IKZF3, IKZF5).",
            "evaluation_method": "Comparison to ground-truth annotations from structured databases (STRING for PPIs, KEGG for pathway membership, INDRA for regulatory statements) using held-out samples; constructed negative samples from unconnected pairs (or genes in other pathways); ablation studies on prompt shot count; consistency checks between generative and binary task outputs.",
            "performance_metrics": "STRING Task 1 (generative): precision ≈ 0.166 (1K) and 0.161 (10K) (Table 1, Table 8). STRING Task 2 (binary): micro F-scores ≈ 0.552 (1K), 0.558 (10K), 0.562 (100K) (Table 2); precision on Task1 pairs: 0.691 and model consistency 0.726 for certain subsets (Table 3). KEGG Task 1 (generative): precision ≈ 0.256 (Table 4). KEGG Task 2 (binary): micro F-score ≈ 0.562 across 35,174 pairs and precision/consistency numbers up to 0.883 for Task1 pairs (Tables 5,6). INDRA Task (multiple-choice): micro F-scores vary by number of choices (e.g., 2-class F1 ≈ 0.704; performance declines with more classes but results reported for 2–6 class setups in Table 7 and ablation Tables 9–12). Ablation showed best shot-count depended on task (e.g., 1-shot optimal for several tasks, 2-shot or 3-shot for others).",
            "comparison_baseline": "Primary baseline was database ground-truth (STRING/KEGG/INDRA) and constructed negative samples; no other LLMs or human-expert extractions were experimentally compared in this paper (authors state plans to evaluate GPT series, LLaMA, Alpaca, and domain-specific models like BioGPT/BioMedLM in future work).",
            "key_findings": "Galactica can recover certain genes/proteins and relation types but performance is far from perfect: (1) model benefits strongly from contextual textual evidence (INDRA text) and performs better on context-rich multiple-choice tasks than on bare generative list or yes/no tasks; (2) prompting and number of shots materially affect performance (task-specific optimal shots found); (3) the model often generates name patterns based on string similarity (prefix-driven errors) causing many false positives in generative list tasks; (4) consistency between generative and binary formats is imperfect; (5) targeted collections (well-defined pathway mentions) produced better results than heterogeneous protein name extraction.",
            "challenges_limitations": "Hallucination / name-pattern generation (predicting similarly prefixed names rather than true interactors), sensitivity to prompt wording and shot count, skewed yes/no outputs towards positive (possibly due to noisy negative samples), limited evaluation to a single model size (6.7B), reliance on databases where negative pairs may not be truly negative, and overall suboptimal accuracy that limits immediate use for high-confidence knowledge base curation without human verification.",
            "uuid": "e4536.0"
        },
        {
            "name_short": "LLMs (general)",
            "name_full": "Large language models (general reference)",
            "brief_description": "General class of transformer-based pre-trained language models noted in the paper for capability in complex language tasks and potential for extracting knowledge from scientific text; cited in related work as promising for few-shot/zero-shot information extraction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs)",
            "model_size": null,
            "method_name": null,
            "method_description": "Described in the introduction/related work as capable of few-shot or zero-shot extraction of knowledge from text; the paper cites prior studies that used LLMs for clinical information extraction and reasoning and motivates applying LLMs to extract biological pathway and interaction knowledge. The paper does not provide a unified method for using generic LLMs beyond referencing prompting/few-shot approaches.",
            "number_of_papers": null,
            "domain_or_field": "natural language processing applied to biology / biomedical text",
            "type_of_laws_extracted": "Generally described as information/knowledge extraction (entity relations, patterns), but no specific laws/principles are presented for general LLMs in this paper.",
            "example_laws_extracted": "",
            "evaluation_method": "Not evaluated directly in this paper (cited prior work evaluates LLMs elsewhere).",
            "performance_metrics": null,
            "comparison_baseline": "Cited literature claims LLMs can match traditional supervised models on some tasks; no direct comparisons in this paper.",
            "key_findings": "The paper presents the view (from related work) that LLMs are flexible and can perform multiple tasks with few-shot prompting, motivating their application to biological knowledge extraction.",
            "challenges_limitations": "No additional experimental detail here; paper emphasizes known general issues (prompt sensitivity, domain adaptation) and the need for further comparison to domain-specific models.",
            "uuid": "e4536.1"
        },
        {
            "name_short": "Other LLMs (planned)",
            "name_full": "GPT-series, LLaMA, Alpaca, BioGPT, BioMedLM (mentioned for future comparison)",
            "brief_description": "Specific LLMs the authors plan to evaluate in future work (general-purpose and domain-specific biomedical models) to compare against Galactica's performance on biological extraction tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT series / LLaMA / Alpaca / BioGPT / BioMedLM",
            "model_size": null,
            "method_name": null,
            "method_description": "No methods executed in this paper — authors state intention to evaluate these models and compare to Galactica and to smaller domain-specific models, likely using similar prompting/formats (generative, yes/no, multiple-choice) and the same evaluation datasets (STRING, KEGG, INDRA).",
            "number_of_papers": null,
            "domain_or_field": "molecular biology / biomedical NLP (future planned evaluations)",
            "type_of_laws_extracted": "Intended to extract similar qualitative relations (PPIs, pathway membership, regulatory relation types) for comparison.",
            "example_laws_extracted": "",
            "evaluation_method": "Planned: comparative evaluation against Galactica on the same tasks/datasets; exact protocol not specified yet.",
            "performance_metrics": null,
            "comparison_baseline": "These models are proposed as comparison baselines in future work (authors mention comparing to domain-specific models like BioGPT/BioMedLM and to smaller-sized models).",
            "key_findings": "None yet; listed as planned future work to establish comparative performance.",
            "challenges_limitations": "No experimental results yet; authors note limitations of current single-model evaluation and plan broader benchmarking.",
            "uuid": "e4536.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2
        },
        {
            "paper_title": "Large language models are few-shot clinical information extractors",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.0122225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Extraction of Molecular Interactions and Pathway Knowledge using Large Language Model, Galactica: Opportunities and Challenges</h1>
<p>Gilchan Park*, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Patrick Johnstone, Shinjae Yoo, Francis J. Alexander<br>Computational Science Initiative, Brookhaven National Laboratory, Upton, NY<br>{gpark,byoon,xluo,vlopezmar,pjohnston,sjyoo,falexander}@bnl.gov</p>
<h4>Abstract</h4>
<p>Understanding protein interactions and pathway knowledge is essential for comprehending living systems and investigating the mechanisms underlying various biological functions and complex diseases. While numerous databases curate such biological data obtained from literature and other sources, they are not comprehensive and require considerable effort to maintain. One mitigation strategies can be utilizing large language models to automatically extract biological information and explore their potential in life science research. This study presents an initial investigation of the efficacy of utilizing a large language model, Galactica in life science research by assessing its performance on tasks involving protein interactions, pathways, and gene regulatory relation recognition. The paper details the results obtained from the model evaluation, highlights the findings, and discusses the opportunities and challenges. The code and data are available at: https://github.com/ boxorange/BioIE-LLM</p>
<h2>1 Introduction</h2>
<p>A significant portion of contemporary molecular biology research is dedicated to studying and comprehending the roles and interactions of the countless proteins that form the fundamental building blocks of life. The prediction of protein structures and functions is essential in addressing crucial challenges in life science, including developing therapeutic solutions for various diseases. By speeding up drug discovery and development, such advancements could significantly enhance healthcare. The majority of proteins have undefined functions, and only a fraction of them have been unequivocally identified through arduous and intensive laboratory research. These established protein functions</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>are used as a benchmark to predict functions computationally by analyzing DNA and amino acid sequence homology across the continuously growing repository of protein sequences obtained from genome sequencing. To deeply understand protein functions, protein interaction information can be crucial, and many databases such as STRING, KEGG, IntAct, BioGrid, DIP, and HPRD have been established to gather and maintain pathway analysis and regulatory results obtained by lab experiments and from the scientific literature. Regrettably, extracting information from the existing literature demands significant manual labor and is a time-consuming process. One viable solution to remedy this is to leverage efficient machine learning models that can accurately recognize such information in scientific texts.</p>
<p>In recent years, large language models (LLMs) have gained significant attention in the natural language processing (NLP) field owing to their capability to execute complex language tasks, their flexibility, and their potential to generate responses similar to humans (Brown et al., 2020; Zhao et al., 2023). Their application in various domains and tasks, including knowledge extraction from texts, has yielded promising outcomes (Agrawal et al., 2022). Our study aims to investigate the potential of LLMs in extracting pathway knowledge, protein interaction, and gene regulatory information. In this study, we have assessed the capability of Galactica (Taylor et al., 2022), a general-purpose scientific LLM, to accomplish these biological tasks. Although Galactica did not yield optimal outcomes in our biology-related tasks, it exhibited the capacity to identify specific genes/proteins, pathways, and their interactions. Our preliminary findings regarding this evaluation are presented in this paper.</p>
<h2>2 Related Work</h2>
<p>The field of biology encompasses challenging tasks such as identifying protein-protein interactions</p>
<p>(PPIs) and gene regulatory relations. Additionally, pathway analysis is a crucial area of study as it documents the interactions between proteins and reflects important molecular biological processes, such as metabolic, signaling, protein interaction, and gene regulation processes. Research such as that for expression-based disease diagnosis (Lee et al., 2008; Gatza et al., 2010) and the identification of disease markers (Khunlertgit and Yoon, 2016) suggests that tasks based on pathway activity can be more stable than tasks based solely on genes. The scientific literature of biological sciences serves as an important repository of essential knowledge that has yet to be effectively discovered. To address this, NLP models based on deep neural networks have been widely adopted for analysis of structural properties of proteins (Vig et al., 2020), PPIs (Peng and Lu, 2017; Park et al., 2022), and pathway analysis (Casaní-Galdón et al., 2020).</p>
<p>Several studies showed that LLMs performed comparably to traditional neural network models that necessitate labeled training data and finetuning processes, resulting in significant time and effort savings while providing a universal model capable of managing multiple tasks simultaneously (Kojima et al., 2022; Yuan and Liu, 2022). The Galactica LLM (Taylor et al., 2022) has been trained on a massive amount of scientific literature and has successfully tackled biological understanding task such as sequence validation perplexity, functional keyword prediction, protein function description. Hence, we aim to further examine the potential of Galactica in the domain of biological scientific knowledge.</p>
<h2>3 Experiments</h2>
<p>We investigated the potential of Galactica for addressing biological tasks related to PPIs, pathway knowledge, and gene regulatory relations. To accomplish this, STRING, KEGG, INDRA databases were adopted, and the details pertaining to the utilized data for these tasks can be found in Appendix A. In the context of a LLM, the proper selection of the number of examples or shots is essential to ensure efficient engineering. For this purpose, an ablation study was conducted to identify the optimal number of shots for each task. The shot number associated with the highest performance in test samples was selected for implementation, as detailed in Appendix B. Additionally, prompt construction is another critical factor that merits</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\mathbf{1 K}$</th>
<th style="text-align: center;">$\mathbf{1 0 K}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Galactica $(6.7 \mathrm{~B})$</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.161</td>
</tr>
</tbody>
</table>
<p>Table 1: STRING Task 1 - Precision for the generated binding proteins for $1 \mathrm{~K} / 10 \mathrm{~K}$ protein samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\mathbf{1 K}$</th>
<th style="text-align: center;">$\mathbf{1 0 K}$</th>
<th style="text-align: center;">$\mathbf{1 0 0 K}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Galactica $(6.7 \mathrm{~B})$</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.562</td>
</tr>
</tbody>
</table>
<p>Table 2: STRING Task 2 - Micro F-scores for randomly selected $1 \mathrm{~K} / 10 \mathrm{~K} / 100 \mathrm{~K}$ positive and negative protein pairs (I.e., $1 \mathrm{~K}=500 \mathrm{pos}+500 \mathrm{neg}$ ).
attention, and the prompts tested for each task are listed in Appendix C. The experimental setup is detailed in Appendix D.</p>
<h3>3.1 Recognizing Protein-Protein Interactions</h3>
<p>We evaluated the Galactica on protein binding information recognition on a human protein network from the STRING DB. Specifically, we employed the model to produce a list of proteins that bind to a designated protein, as part of the generative question task (STRING Task 1: generative question).</p>
<div class="codehilite"><pre><span></span><code>&lt;Predicted answer by model&gt;
</code></pre></div>

<p>Question: Which proteins are related to TBC1D9?
Answer: TBC1D8, TBC1D14, TBC1D7, TBC1D5,
TBC1D6, TBC1D
$&lt;$ Actual answer&gt;
Answer: TBC1D8, TBC1D14, TBC1D7, TBC1D5,
PLK5, MYO16</p>
<p>To assess performance, we randomly selected 1,000 and 10,000 samples from the network for testing. The generated binding proteins were matched with the proteins in the network with an approximately 0.16 precision as described in Table 1. The results of the prediction analysis indicated that the model exhibited a tendency to generate words primarily from the initial letters of a given protein. Consequently, the accuracy of the predictions was considerably high for proteins with similar names, such as IKZF4 and RFC5, while a significant mismatch between predicted and actual binding proteins was observed in cases where dissimilar protein names were involved, such as DNAJC10 and TRIP11. The details of those examples are provided in Appendix E.</p>
<p>Subsequently, we tested the model's recognition of protein binding relationships in a binary setting, which was formulated as a yes/no question to determine if two proteins bind to each other (STRING
Task 2: yes/no question).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Confusion matrices for STRING Task 2 (The observed disparity between the total number of samples and the sum of values in the confusion matrix can be attributed to the omission of responses other than 'yes' or 'no'.)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Task1 pairs ${ }^{\dagger}$</th>
<th style="text-align: center;">Consistency ${ }^{\ddagger}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Galactica (6.7B)</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.726</td>
</tr>
</tbody>
</table>
<p>Table 3: STRING Task 2 - Precision for the protein pairs used in STRING Task 1. ${ }^{\dagger}$ All positive protein pairs. ${ }^{\ddagger}$ Model prediction consistency between Task1 and Task2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"><Predicted answer by model></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question: Are CHEK2 and BRCA2 related to each</td>
</tr>
<tr>
<td style="text-align: left;">other?</td>
</tr>
<tr>
<td style="text-align: left;">Answer: yes</td>
</tr>
<tr>
<td style="text-align: left;"><Actual answer></td>
</tr>
<tr>
<td style="text-align: left;">Answer: yes</td>
</tr>
</tbody>
</table>
<p>To create negative protein binding pairs, we utilized unconnected pairs from the human protein network. The experiment was conducted on $1 \mathrm{~K} / 10 \mathrm{~K} / 100 \mathrm{~K}$ protein pairs, and the model exhibited F-scores slightly exceeding 0.5 in its predictions as described in Table 2. In order to evaluate the consistency of the model's predictions across Task1 and Task2, we conducted an assessment of STRING Task 2 using the identical protein pairs employed in STRING Task 1. These pairs exclusively consisted of positive instances. More specifically, our examination focused on whether the model successfully generated protein A associated with protein B and correctly classified their relationship as 'yes'. To this end, we tested STRING Task 2 using the 1 K protein pairs used in STRING Task 1, and the model performed STRING Task 2 on all positive protein pairs and those generated correctly by the models in STRING Task 1. The evaluation results and the confusion matrices are described in Table 3 and Figure 1. The model exhibited variability in its predictive performance between STRING Task 1 and STRING Task 2 (0.73).</p>
<p>Although this precision (0.73) is slightly higher than the precision of all positive protein pairs used in STRING Task 1, this level of precision aligns closely with the precision observed for 50K random positive pairs ( $0.74=36,991$ true positives / 50,000 true positive + false positives). This implies that while the model successfully made some accurate predictions for proteins in generative questions, it encountered challenges when identifying protein relations in the binary format prompts.</p>
<h3>3.2 KEGG Pathway Recognition</h3>
<p>Galactica was assessed for its ability to recognize genes related to human pathways associated with low-dose radiation exposure in KEGG. The task involved generating a list of genes belonging to the top 20 human pathways linked to low-dose radiation exposure (KEGG Task 1: generative question).</p>
<div class="codehilite"><pre><span></span><code>&lt;Predicted answer by model&gt;
</code></pre></div>

<p>Question: Which genes are involved in "Adherens junction"?
Answer: CDH1, CTNNA3, CTNNB1, CTNNA1, CTNNA2, CTNNA8, CTNNA15
<Actual answer>
Answer: CDH1, CTNNA3, CTNNB1, CTNNA1, CTNNA2, TGF1a, MEKK7</p>
<p>The model accurately predicted the genes that belong to the pathways with about precision 0.26 as presented in Table 4, which outperformed the previous STRING Task 1 generative test. Our speculation is that the model's superior performance in recognizing low-dose radiation-related pathways compared to proteins might be attributed to the fact that pathway names related to low-dose radiation exposure are typically mentioned in specific sections or categories, whereas protein names are</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Confusion matrices for KEGG Task 2.</p>
<p>| Pathways |
| :--: | :--: |
| Galactica (6.7B) 0.256 |</p>
<p>Table 4: KEGG Task 1 - Precision for the generated genes that belong to the top 20 pathways relevant to low-dose radiation exposure.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">35,174 gene and pathway pairs</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Galactica (6.7B)</td>
<td style="text-align: center;">0.562</td>
</tr>
</tbody>
</table>
<p>Table 5: KEGG Task 2 - Micro F-scores for all positive and negative pairs $(35,174=17,587$ pos $+17,587 \mathrm{neg})$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task1 pairs ${ }^{1}$</th>
<th style="text-align: center;">Consistency ${ }^{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Galactica (6.7B)</td>
<td style="text-align: center;">0.883</td>
</tr>
</tbody>
</table>
<p>Table 6: KEGG Task 2 - Precision for the gene-pathway pairs used in KEGG Task 1. ${ }^{1}$ All positive gene-pathway pairs. ${ }^{2}$ Model prediction consistency between Task1 and Task2.
more commonly found in a wider range of topic papers. This suggests that searching for information in a well-defined collection of data may yield more precise results than searching for information derived from ambiguous inputs in heterogeneous sources. The outcome of the prediction analysis showed that the genes produced for a particular pathway exhibited comparable patterns, a finding which had also been observed in the prior STRING Task 1 experiment. Examples of this can be found in Appendix F.</p>
<p>We performed yes/no questions for pathways and genes relation recognition (KEGG Task 2: yes/no question). Similar to the STRING Task 2, we used member genes in other pathways as negative samples for a given pathway if they do not appear in the pathway. The model was evaluated on all positive relations (+ randomly chosen negative relations) and the relations used in KEGG Task 1
to measure the model consistency between Task1 and Task2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"><Predicted answer by model></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question: Are "DP beta 1" and "Type I diabetes</td>
</tr>
<tr>
<td style="text-align: left;">mellitus" related to each other?</td>
</tr>
<tr>
<td style="text-align: left;">Answer: yes</td>
</tr>
<tr>
<td style="text-align: left;"><Actual answer></td>
</tr>
<tr>
<td style="text-align: left;">Answer: yes</td>
</tr>
</tbody>
</table>
<p>The results and the confusion matrices are displayed in Table 5, Table 6, and Figure 2 respectively. The model achieved an F-score of approximately 0.56 when making predictions for both positive and negative gene-pathway pairs. The precision of 0.92 for the model consistency signifies the model's reliable predictive performance. Moreover, the higher score achieved for the generated positive relationships $(0.92)$ in contrast to the scores for all positive relationships in the pathways $(0.78=$ $13,628 / 17,552)$ and Task 1 pairs $(0.88)$ suggests that the model possesses a greater level of comprehension concerning specific pathways compared to others.</p>
<p>In the STRING Task 2 and KEGG Task 2, the model's responses to yes/no questions utilizing positive and negative samples skewed more towards positive, as illustrated by the leftmost confusion matrix in Figures 1 and 2. A plausible explanation for this outcome is the likelihood of erroneous negative relationships in the negative samples. For instance, among the negative samples is the relationship between the gene "HD1" and the pathway "Adherens junction" despite the fact that they are genuinely connected.</p>
<h3>3.3 Evaluating Gene Regulatory Relations</h3>
<p>Finally, we examined Galactica's ability to recognize human gene regulatory relations using data from the INDRA DB. Unlike the previous datasets,</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\mathbf{2}$</th>
<th style="text-align: center;">$\mathbf{3}$</th>
<th style="text-align: center;">$\mathbf{4}$</th>
<th style="text-align: center;">$\mathbf{5}$</th>
<th style="text-align: center;">$\mathbf{6}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Galactica 6.7B</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.597</td>
</tr>
</tbody>
</table>
<p>Table 7: INDRA Task - Micro F-scores with 1K samples for each class. See Appendix G for class names.</p>
<p>INDRA statements not only provide relation entities but also text snippets from research papers. We used these text snippets as contextual information about regulatory relations to generate questions for the model. The task involved asking the model to select the correct relationship between two genes from multiple relation classes in a given text (INDRA Task: multiple-choice question). This task serves as an evaluation of the model's reading comprehension skills specifically related to gene regulatory relation texts.</p>
<div class="codehilite"><pre><span></span><code><span class="o">&lt;</span><span class="n">Predicted</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">model</span><span class="o">&gt;</span>
<span class="n">Upon</span><span class="w"> </span><span class="n">binding</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">Shh</span><span class="p">,</span><span class="w"> </span><span class="n">Ptc1</span><span class="w"> </span><span class="n">inactivation</span><span class="w"> </span><span class="n">al</span><span class="o">-</span>
<span class="n">lows</span><span class="w"> </span><span class="n">Smo</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">initiate</span><span class="w"> </span><span class="n">signaling</span><span class="w"> </span><span class="n">XREF_BIBR</span><span class="p">,</span>
<span class="n">XREF_BIBR</span><span class="p">,</span><span class="w"> </span><span class="n">XREF_BIBR</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Gli</span><span class="w"> </span><span class="n">fam</span><span class="o">-</span>
<span class="n">ily</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">transcription</span><span class="w"> </span><span class="n">factors</span><span class="o">.</span>
<span class="n">Question</span><span class="p">:</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Activation&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;Inhi-</span>
<span class="n">bition</span><span class="s2">&quot;, &quot;</span><span class="n">Phosphorylation</span><span class="s2">&quot;, &quot;</span><span class="n">Dephosphorylation</span><span class="s2">&quot;,</span>
<span class="s2">&quot;Ubiquitination&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;Deubiquitination&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">is</span>
<span class="n">the</span><span class="w"> </span><span class="n">relation</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">Ptc1</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Smo</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span>
<span class="n">above</span><span class="err">?</span>
<span class="n">Answer</span><span class="p">:</span><span class="w"> </span><span class="n">Activation</span>
<span class="o">&lt;</span><span class="n">Actual</span><span class="w"> </span><span class="n">answer</span><span class="o">&gt;</span>
<span class="n">Answer</span><span class="p">:</span><span class="w"> </span><span class="n">Activation</span>
</code></pre></div>

<p>To construct multiple-choice questions, we identified the six most frequently occurring classes in the dataset and utilized two to six of them for choices. The model was assessed using 1 K samples for each class, and the findings are detailed in Table 7 and Figure 3. With the escalation of problem complexity due to the increased number of choices, the model initially encountered difficulties in identifying correct answers. Nonetheless, it exhibited improved performance in six-choice problems compared to cases involving four or five choices. When examining the results of yes/no questions in STRING Task 2 and KEGG Task 2 ( $\approx$ 0.56 ) using the two-class F-score ( 0.70 ) in INDRA Task, it becomes evident that the model possesses a more consistent ability to recognize entity relations within contexts as compared to extracting information through straightforward questioning. This observation suggests that incorporating contextual information in questions could potentially enhance the model's predictive capabilities.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Confusion matrix of 6 choice questions for INDRA Task.</p>
<h2>4 Discussion and Conclusion</h2>
<p>This study evaluated the performance of Galactica on various biological tasks using different types of database resources, including PPIs, pathway knowledge, and regulatory relations. Based on our findings, it was observed that the model faced greater difficulty in answering questions that contained limited information, in contrast to questions that provided contextual details and were more specific. While the Galactica did not produce ideal results in our biology-related tasks, we observed that the model demonstrated the ability to recognize certain genes and proteins and their interactions. Despite this, our approach presents a potential avenue for using the model, and we anticipate that our findings will assist domain scientists and researchers in employing the model for their applications and obtaining insights into the model's behavior based on experimental outcomes.</p>
<h2>5 Work in progress</h2>
<p>Our study suggests that the model can show better performances when contextual text is provided. We will consider forging a chain of queries to generate context for the datasets not having supporting text (See Appendix H). We found that the model's performance was largely affected by prompts. This needs to be further investigated. We plan to evaluate other LLMs such as GPT series, LLaMA, and Alpaca by comparing with smaller sized domainspecifically trained models such as BioGPT and BioMedLM in biological tasks.</p>
<h2>Limitations</h2>
<p>There are a few limitations for our work. First, to assess the model's ability to recognize negative protein-protein and gene-pathway pairs, we used unconnected pairs in the datasets. However, the negative pairs have not been proved, which might contain authentic or potential positive connections. We continuously search for truly negative gene/protein pairs established by experiments and research. Second, this work only reports the assessment of the Galactica standard model (6.7B). The evaluation of the other models (mini: 125 M , base: 1.3 B , large: 30 B , huge: 120 B ) remains as future works. In-depth studies on the model inference will be also followed in our future works such as clustering of PPIs and genes belonging to pathways predicted by the model.</p>
<h2>Acknowledgements</h2>
<p>This work is supported by the U.S. Department of Energy, Office of Science, RadBio program under Award KP1601011/KP1601017/FWP CC121.</p>
<h2>References</h2>
<p>Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022. Large language models are few-shot clinical information extractors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1998-2022.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Salvador Casaní-Galdón, Cecile Pereira, and Ana Conesa. 2020. Padhoc: a computational pipeline for pathway reconstruction on the fly. Bioinformatics, 36(Supplement_2):i795-i803.</p>
<p>Michael L Gatza, Joseph E Lucas, William T Barry, Jong Wook Kim, Quanli Wang, Matthew D Crawford, Michael B Datto, Michael Kelley, Bernard MatheyPrevot, Anil Potti, et al. 2010. A pathway-based classification of human breast cancer. Proceedings of the National Academy of Sciences, 107(15):69946999 .</p>
<p>Benjamin M Gyori, John A Bachman, Kartik Subramanian, Jeremy L Muhlich, Lucian Galescu, and Peter K Sorger. 2017. From word models to executable models of signaling networks using automated assembly. Molecular systems biology, 13(11):954.</p>
<p>Minoru Kanehisa and Susumu Goto. 2000. Kegg: kyoto encyclopedia of genes and genomes. Nucleic acids research, 28(1):27-30.</p>
<p>Minoru Kanehisa, Yoko Sato, Miho Furumichi, Kanae Morishima, and Mao Tanabe. 2019. New approach for understanding genome variations in kegg. $N u$ cleic acids research, 47(D1):D590-D595.</p>
<p>Navadon Khunlertgit and Byung-Jun Yoon. 2016. Incorporating topological information for predicting robust cancer subnetwork markers in human proteinprotein interaction network. In BMC bioinformatics, volume 17, pages 143-152. Springer.</p>
<p>Hyunwoong Ko. 2021. Parallelformers: An efficient model parallelization toolkit for deployment. https: //github.com/tunib-ai/parallelformers.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Eunjung Lee, Han-Yu Chuang, Jong-Won Kim, Trey Ideker, and Doheon Lee. 2008. Inferring pathway activity toward precise disease classification. PLoS computational biology, 4(11):e1000217.</p>
<p>Xihaier Luo, Sean McCorkle, Gilchan Park, Vanessa López-Marrero, Shinjae Yoo, Edward R Dougherty, Xiaoning Qian, Francis J Alexander, and Byung-Jun Yoon. 2022. Comprehensive analysis of gene expression profiles to radiation exposure reveals molecular signatures of low-dose radiation response. In 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 2366-2374. IEEE.</p>
<p>Gilchan Park, Sean McCorkle, Carlos Soto, Ian Blaby, and Shinjae Yoo. 2022. Extracting protein-protein interactions (ppis) from biomedical literature using attention-based relational context information. In 2022 IEEE International Conference on Big Data (Big Data), pages 2052-2061. IEEE.</p>
<p>Yifan Peng and Zhiyong Lu. 2017. Deep learning for extracting protein-protein interactions from biomedical literature. BioNLP 2017, page 29.</p>
<p>Damian Szklarczyk, Annika L Gable, Katerina C Nastou, David Lyon, Rebecca Kirsch, Sampo Pyysalo, Nadezhda T Doncheva, Marc Legeay, Tao Fang, Peer Bork, et al. 2021. The string database in 2021: customizable protein-protein networks, and functional characterization of user-uploaded gene/measurement sets. Nucleic acids research, 49(D1):D605-D612.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Jesse Vig, Ali Madani, Lav R Varshney, Caiming Xiong, Nazneen Rajani, et al. 2020. Bertology meets biology: Interpreting attention in protein language models. In International Conference on Learning Representations.</p>
<p>Boshi Wang, Xiang Deng, and Huan Sun. 2022. Iteratively prompt pre-trained language models for chain of thought. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2714-2730.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Weizhe Yuan and Pengfei Liu. 2022. restructured pretraining. arXiv preprint arXiv:2206.11147.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<h2>A Data Description</h2>
<p>STRING DB (Szklarczyk et al., 2021): The present study employed the human (Homo sapiens) protein network for performing a protein-protein interaction (PPI) recognition task. The network was constructed based on the STRING (Search Tool for the Retrieval of Interacting Genes/Proteins) database, which is a comprehensive biological repository and online resource for both predicted and confirmed protein interactions. The database integrates data from a range of sources, including experimental studies, computational prediction methods, and publicly available text collections. The human network encompasses 19,566 proteins and 5,968,680 protein bindings.</p>
<p>KEGG DB (Kanehisa and Goto, 2000): The Kyoto Encyclopedia of Genes and Genomes (KEGG) is a set of databases encompassing a wide range of biological information, including genomic data, disease information, chemical compounds, and biological pathways. It houses a staggering collection of over 28,000 complete genomes, encompassing a diverse range of organisms. Furthermore, it hosts an expansive repertoire of more than 500 pathways, meticulously curated and annotated to illuminate the intricate web of molecular interactions that govern various biological processes. Moreover, the database includes approximately 5 million reference genes, providing researchers with invaluable resources
for gene-centric investigations (Kanehisa et al., 2019). The KEGG pathways contain molecular interactions and reactions, which are designed to link genes in the genome to gene products (mostly proteins) in biological pathways. The focus of our investigation pertains to the pathways within the human body that are affected by exposure to low-dose ionizing radiation, which remains a significant threat to human health and is not yet fully comprehended. To explore this topic, we utilized the KEGG human pathways which have been identified as being activated in response to low-dose radiation exposure in a recent study (Luo et al., 2022).</p>
<p>INDRA DB (Gyori et al., 2017): The Integrated Network and Dynamical Reasoning Assembler (INDRA) is a tool that facilitates the integration of information regarding causal mechanisms into a unified format suitable for the construction of a variety of predictive and explanatory models. In the field of molecular biology, sources of mechanistic information include pathway databases, textual descriptions of mechanisms generated by human curators, and information extracted from the scientific literature through text mining. The INDRA platform streamlines this information by removing duplicates, standardizing the data, and organizing it into a set of Statements accompanied by associated evidence. By collating information from multiple sources in this manner, INDRA enables researchers to build robust models for exploring the complex molecular mechanisms underlying biological systems. The present study utilized a set of human gene regulatory relation statements that represent mechanistic interactions between biological agents. The dataset comprises a total of 4,258,718 distinct statements.</p>
<h2>B Ablation study on the number of shots</h2>
<ol>
<li>STRING Task 1: We randomly drew 1K and 10K samples out of the STRING DB human protein network for testing, and the generated binding proteins corresponded to the proteins in the human network the most with 1-shot prompting as seen in Table 8</li>
<li>STRING Task 2: We evaluated 1K samples (500 true cases +500 false cases) randomly drawn from the STRING DB human protein network with different number of prompt</li>
</ol>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Confusion matrices for KEGG Pathway Recognition Task 2 ablation study.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;">1-shot</th>
<th style="text-align: center;">2-shot</th>
<th style="text-align: center;">3-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 6}$</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.135</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10 K</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">$\mathbf{0 . 1 4 4}$</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.137</td>
</tr>
</tbody>
</table>
<p>Table 8: Precision of different shots with $1 \mathrm{~K} / 10 \mathrm{~K}$ samples for STRING Task 1 using a human protein network from STRING DB.
shots. Here, $N$-shot indicates the combination of $N$ number of true samples and $N$ number of false samples (e.g., 1-shot: 1 true +1 false (total 2 samples)). The result showed that 3 -shot prompt performed the best in Table 9.
3. KEGG Pathway Recognition Task 1: We assessed human pathways associated with lowdose radiation exposure in KEGG DB with different number of shots, and 1-shot prompting showed the best performance as described in Table 10.
4. KEGG Pathway Recognition Task 2: We evaluated 1 K samples ( 500 true cases +500 false cases) randomly drawn from human pathways associated with low-dose radiation exposure in KEGG DB with different number of prompt shots. Here, $N$-shot indicates the combination of $N$ number of true samples and $N$ number of false samples (e.g., 1-shot: 1 true +1 false (total 2 samples)). The result showed that 1-shot prompt performed the best in Table 11.
5. Evaluating Gene Regulatory Relations Task: We tested different shots with 400 samples for 4 classes ( 100 Activation + 100 Inhibition + 100 Phosphorylation + 100 Dephosphorylation) from INDRA DB, and 2-shot prompting showed the best performance on the multiple choice task in Table 12.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;">1-shot</th>
<th style="text-align: center;">2-shot</th>
<th style="text-align: center;">3-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Micro F1</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 2}$</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">$0.590^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Micro F-scores of different shots with 1K samples for STRING Task 2 using a human protein network from STRING DB. $\dagger$ Due to the high false positive rate, 1-shot prompting was adopted.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;">1-shot</th>
<th style="text-align: center;">2-shot</th>
<th style="text-align: center;">3-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">$\mathbf{0 . 2 5 9}$</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.209</td>
</tr>
</tbody>
</table>
<p>Table 10: Precision of different shots for KEGG Pathway Recognition Task 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;">1-shot</th>
<th style="text-align: center;">2-shot</th>
<th style="text-align: center;">3-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Micro F1</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 4}$</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.501</td>
</tr>
</tbody>
</table>
<p>Table 11: Micro F-scores of different shots with 1K samples for KEGG Pathway Recognition Task 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;">1-shot</th>
<th style="text-align: center;">2-shot</th>
<th style="text-align: center;">3-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Micro F1</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 0}$</td>
<td style="text-align: center;">0.560</td>
</tr>
</tbody>
</table>
<p>Table 12: Micro F-scores of different shots with 400 samples ( 100 Activation + 100 Inhibition + 100 Phosphorylation + 100 Dephosphorylation) choice for Evaluating Gene Regulatory Relations Task using INDRA DB.</p>
<h2>C Tested Prompts</h2>
<h2>STRING Task 1:</h2>
<ol>
<li>"Which proteins are bound to $x$ ?"</li>
<li>"What proteins are bound to $x$ ?"</li>
<li>"What proteins are bound to $x$ ?"</li>
<li>"What proteins does $x$ bind to?"</li>
<li>"To what proteins does $x$ bind?"</li>
<li>"Which proteins are related to $x$ ?"</li>
</ol>
<h2>STRING Task 2:</h2>
<ol>
<li>"Do the two proteins " $x$ " and " $y$ " bind each other?"</li>
<li>"Do the two proteins $x$ and $y$ bind each other? True or False"</li>
<li>"Do the two proteins $x$ and $y$ bind to each other? True or False"</li>
<li>"Do $x$ and $y$ bind each other? True or False"</li>
<li>"Does $x$ bind to $y$ ? True or False"</li>
<li>"Do $x$ and $y$ bind to each other? True or False"</li>
<li>"Are $x$ and $y$ related to each other? True or False"</li>
<li>"Are $x$ and $y$ related to each other?"</li>
<li>"Are $x$ and $y$ related to each other? yes or no"</li>
<li>" $x$ and $y$ are related to each other. Is this statement True or False?"</li>
<li>" $x$ and $y$ are related to each other."</li>
<li>"Given the options: "Related", "Unrelated", which one is the relation type between $x$ and y?"</li>
</ol>
<p>KEGG Pathway Recognition Task 1:</p>
<ol>
<li>"Which genes are involved in "x"?"</li>
<li>"Which genes are involved in $x$ ?"</li>
<li>"Which genes are related to $x$ ?"</li>
<li>"Which proteins are related to $x$ ?"</li>
<li>"Which genes or proteins are related to $x$ ?"</li>
<li>"Which genes/proteins are related to $x$ ?"</li>
</ol>
<p>KEGG Pathway Recognition Task 2:</p>
<ol>
<li>"Are $x$ and $y$ related to each other?"</li>
<li>"Are " $x$ " and " $y$ " related to each other?"</li>
<li>"Is $x$ related to $y$ ?"</li>
<li>"Is $x$ related to the pathway $y$ ?"</li>
<li>"Is $x$ involved in $y$ ?"</li>
<li>"Is " $x$ " involved in " $y$ "?"</li>
<li>"Is $x$ involved in the human pathway $y$ ?"</li>
<li>"Is $x$ involved in the KEGG pathway $y$ ?"</li>
<li>"Does " $y$ " have " $x$ "?</li>
</ol>
<p>Evaluating Gene Regulatory Relations Task:</p>
<ol>
<li>"Which of the following is the relation type between $x$ and $y$ in the text above?"</li>
<li>"Which of the following is the relation type between " $x$ " and " $y$ " in the text above?"</li>
<li>"Which of the following is the relation between " $x$ " and " $y$ " in the text above?"</li>
<li>"Given the options: z , which one is the relation type between " $x$ " and " $y$ " in the text above?"</li>
<li>"Given the options: z , which one is the relation type between $x$ and $y$ in the text above?"</li>
</ol>
<h2>D Experimental Setup</h2>
<p>We used the Galactica standard model with 6.7 billion parameters. The experiments were conducted on $8 \times$ NVIDIA V100 GPUs, and we exploited Galactica's option for model tensor parallelizm based on Parallelformers (Ko, 2021) when the machine has enough memories, which significantly increases task processing time (about twice faster). The model processed a batch sized input for a task, which is the number of prompts to infer (I.e., the number of input texts for model generation at once). The batch sizes for the tasks are as follows.</p>
<ul>
<li>STRING Task 1 (generative question): 16, 32</li>
<li>STRING Task 2 (yes/no question): 32, 64</li>
<li>KEGG Task 1 (generative question): 16, 32</li>
<li>KEGG Task 2 (yes/no question): 32, 64</li>
<li>INDRA Task (multiple choice question): 4, 8</li>
</ul>
<h2>E STRING Task 1 examples</h2>
<p>Question: Which proteins are related to IKZF4? Answer (pred): IKZF1, IKZF2, IKZF3, IKZF5
Answer (true): IKZF1, IKZF2, IKZF3, IKZF5</p>
<p>Question: Which proteins are related to RFC5? Answer (pred): RFC1, RFC2, RFC3, RFC4, RFC5 Answer (true): RFC1, RFC2, RFC3, RFC4, ERCC1</p>
<p>Question: Which proteins are related to USP32? Answer (pred): USP54, USP41, USP42, USP34, USP38, USP50, USP52, USP32, USP55, USP56, ... Answer (true): USP54, USP41, USP42, USP34, USP38, USP50, CACNA1H, ACTC1, DHX32, MAGI3, ...</p>
<p>Question: Which proteins are related to DNAJC10?
Answer (pred): DNAJC10K, DNAJC10O, DNAJC10D, DNAJC10Q, DNAJC10E ... Answer (true): ARL5B, MATN3, KPNA4, SRSF10, MTHFD2, ...</p>
<p>Question: Which proteins are related to TRIP11? Answer (pred): TRIP32, TRIP31, TRIP20, TRIP23, TRIP25, ... Answer (true): RIC1, GOLGA8J, ARL5B, TMTC4, KIF5A, ...</p>
<h2>F KEGG Pathway Recognition Task 1 examples</h2>
<p>Question: Which genes are involved in "Cellular senescence"?
Answer (pred): CDKN2A, CDKN2B, CDKN1A, CDKN1D, CDKN1D, CDKN1B, CDKN1C Answer (true): CDKN2A, CDKN2B, CDKN1A, p107, major histocompatibility complex, mitogenactivated protein kinase 14, P44MAPK</p>
<p>Question: Which genes are involved in "Phagosome"?
Answer (pred): RAB5A, RAB7A, RAB47, RAB41, RAB48, RAB39B, RAB50 ... Answer (true): RAB5A, RAB7A, ATP6G, CD51, HEL-S-62p, phospholipase A2 receptor 1, ATP6EL2, ...</p>
<p>Question: Which genes are involved in "Proteoglycans in cancer"?
Answer (pred): CD63, CD284, CD282, CD44, CD166, CD276, CD278, CD81, CD55, ... Answer (true): CD63, CD284, CD282, CD44, SJS1, G17P1, GAB1, PLCE1, HPSE1, ...</p>
<p>Question: Which genes are involved in "Autoimmune thyroid disease"?
Answer (pred): TSHR, TSH
Answer (true): TSHR, hTSHR-I</p>
<h2>G INDRA DB class names</h2>
<p>Table 13 displays the name of classes in INDRA DB statements used in the Evaluating Gene Regu-
latory Relations Task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># Choices</th>
<th style="text-align: left;">Classes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2 class</td>
<td style="text-align: left;">Activation, Inhibition</td>
</tr>
<tr>
<td style="text-align: left;">3 class</td>
<td style="text-align: left;">Activation, Inhibition, Phosphorylation</td>
</tr>
<tr>
<td style="text-align: left;">4 class</td>
<td style="text-align: left;">Activation, Inhibition, Phosphorylation,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Dephosphorylation,</td>
</tr>
<tr>
<td style="text-align: left;">5 class</td>
<td style="text-align: left;">Activation, Inhibition, Phosphorylation,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Dephosphorylation, Ubiquitination,</td>
</tr>
<tr>
<td style="text-align: left;">6 class</td>
<td style="text-align: left;">Activation, Inhibition, Phosphorylation,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Dephosphorylation, Ubiquitination, Deu-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">biquitination</td>
</tr>
</tbody>
</table>
<p>Table 13: The class names used in the multiple choice question for Evaluating Gene Regulatory Relations Task using INDRA DB.</p>
<h2>H A chain of query example</h2>
<p>To provide a model with context information about a query, we plan to apply an iterative prompting for a chain of thought (CoT) development (Wei et al., 2022; Wang et al., 2022). An example is illustrated below.</p>
<p>Question: what is "Natural killer cell mediated cytotoxicity"?
-&gt; The answer of this query can be a context of the following query.</p>
<p>Question: Which genes are involved in "Natural killer cell mediated cytotoxicity"?
Answer: VAV3, NFAT5, HCST, CHP1, SH2D1B, RAET1E</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
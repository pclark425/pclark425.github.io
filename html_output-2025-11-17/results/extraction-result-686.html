<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-686 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-686</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-686</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-dbeeca8466e0c177ec67c60d529899232415ca87</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dbeeca8466e0c177ec67c60d529899232415ca87" target="_blank">On Faithfulness and Factuality in Abstractive Summarization</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found that neural abstractive summarization models are highly prone to hallucinate content that is unfaithful to the input document and textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.</p>
                <p><strong>Paper Abstract:</strong> It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e686.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e686.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>source-target divergence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Divergence between source documents and target (gold) summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dataset-level mismatch where gold summaries often include information not present in the source article (introductory sentences or background), causing a mismatch between the natural-language dataset description/assumption and the model training/evaluation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>XSum extreme-document summarization evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Training and evaluation pipeline for abstractive summarization models on the XSum dataset (BBC articles paired with single-sentence journalist-written summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset documentation / reference summaries description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model training & evaluation code (seq2seq training on (document, gold-summary) pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset-documentation mismatch / implicit divergence between source and target</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Gold summaries are journalistic introductory sentences that frequently introduce background facts not present in the article; the dataset documentation and common evaluation uses implicitly assume the reference is faithful to the source, but in practice many references diverge. Models trained on this data therefore learn to generate content not directly supported by the source (extrinsic hallucinations), because the training pairs expose them to target-side information absent from inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data / data preprocessing / dataset design</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical human annotation study comparing source documents and summaries (annotators labeled hallucinated spans and whether they were intrinsic/extrinsic); descriptive analysis of dataset properties</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>human annotation counts and percentages (Table 2 & text): measured percent of summaries containing intrinsic/extrinsic hallucinations; statistics reported over 500 sampled test examples and 3 annotators per item (e.g., >70% of single-sentence summaries contained hallucinations; gold summaries had 73.1% extrinsic hallucinations in the sample).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: results in models learning to produce extrinsic content; gold references themselves exhibit high extrinsic rates (73.1%), thus complicating training targets and evaluation. The paper reports that intrinsic+extrinsic hallucinations happen frequently (>70% single-sentence summaries) and that the prevalence of extrinsic hallucinations in generated summaries is high (PTGEN 63.3%, TConvS2S 71.5%, TRANS2S 68.1%, BERTS2S 64.1%), distorting both training signals and evaluation interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High — dataset gold summaries: extrinsic hallucination annotated in 73.1% of sampled golds; model outputs: extrinsic hallucination present in ~63–71% of summaries depending on model; overall hallucination (>I ∪ E) in 73–79% of system summaries (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>dataset artifact / implicit assumptions in natural-language description of the dataset (journalists' introductory summaries include external/background knowledge) and omission of this divergence in dataset specification and evaluation practices</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly treat source-target divergence during training and evaluation: (i) use human annotations of faithfulness for training/dev, (ii) incorporate reference-less faithfulness measures (textual entailment) for model selection and decoding, and (iii) be explicit in dataset documentation about expected target divergence or filter/label such examples.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: using entailment-based selection or fine-tuning to predict faithfulness improved selection of faithful summaries (ENTAIL selection increased faithfulness from 26.9% to 31.5% for the best model; further fine-tuning ENTAIL→FAITH gave 31.7%), but the underlying dataset divergence remains and many hallucinations persist. No full elimination reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / natural language processing (abstractive summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Faithfulness and Factuality in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e686.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e686.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE_misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misalignment of ROUGE/BERTScore with faithfulness and factuality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automatic metrics (ROUGE, BERTScore) are commonly described/used as proxies for summary quality (informativeness/overlap), but empirically they correlate weakly with human judgments of faithfulness and factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automatic summarization evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation framework that uses ROUGE and BERTScore to assess model summarization quality and to guide development/selection.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric specification / evaluation protocol description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation metric implementation (ROUGE, BERTScore computations in experiment scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>metric-behavior mismatch / insufficient metric specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>ROUGE and BERTScore are described and used as proxies for informativeness and fluency, but they do not capture semantic faithfulness; they can report high scores for summaries that are fluent and topical yet contain hallucinated or incorrect facts. The intended natural-language claim that these metrics reflect useful summarization quality is therefore misleading for faithfulness-sensitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / model selection</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>correlation analysis (Spearman) between automatic metrics and human annotations of faithfulness and factuality; human annotation study comparing metric scores and labeled faithfulness</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Spearman correlation coefficients: ROUGE-1 with faithfulness = 0.197, ROUGE-2 = 0.162, ROUGE-L = 0.162, BERTScore = 0.190 (Table 4/9); these weak correlations demonstrate poor alignment. Additional qualitative examples show fluent but hallucinated summaries receiving high ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Misleading model evaluation and model selection: reliance on ROUGE/BERTScore can favour models that are fluent/topical but unfaithful; the paper excluded GPT-TUNED after poor performance on automatic measures and yet found cases where pretrained models are more factual. Quantitatively, ROUGE-based selection does not align with human faithfulness judgments, potentially causing researchers to overestimate real-world factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>ROUGE/BERTScore are standard and widely used; in this study they consistently showed weak correlation across models and human-labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>metric design focused on lexical overlap / embedding similarity rather than semantic entailment; implicit assumption in natural-language descriptions that lexical/embedding overlap proxies for factual consistency</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Complement ROUGE/BERTScore with semantic-inference-based, reference-less measures such as textual entailment scores or QA-based consistency checks; use human faithfulness annotations to fine-tune evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Textual entailment correlates substantially better (entailment |r_s| with faithful = 0.431 vs ROUGE-1 0.197). Using entailment for model selection increased faithfulness (see ENTAIL experiments), demonstrating practical benefit over ROUGE alone.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Faithfulness and Factuality in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e686.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e686.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>entailment_selection_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-less textual entailment measure limitations and gaming risk</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an entailment classifier as a reference-less faithfulness evaluator or model-selection objective is effective but has limitations: it can be gamed and its assumptions differ from intended evaluation descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Entailment-based evaluation and model selection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A BERT-Large NLI classifier (fine-tuned on MultiNLI, further fine-tuned on human 'faithful' annotations) used to score whether a summary is entailed by its document and to select among candidate summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric description / model-selection objective</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation & selection code (entailment classifier scoring over document-summary pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>assumption mismatch / reference-less metric vulnerability (gaming) / domain mismatch for classifier training</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although textual entailment is described as a strong proxy for faithfulness, the paper notes the entailment classifier is trained on sentence-sentence NLI (MultiNLI) and is not perfectly matched to document-summary pairs; additionally, because the measure is reference-less it can be gamed (e.g., selecting the first sentence of the document will often be judged as entailed). This is a gap between the natural-language claim 'entailment measures faithfulness' and the practical behavior of the implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metric design and model-selection procedures</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical experiments: (i) computing entailment-based rankings and selecting summaries (ENTAIL row) and (ii) fine-tuning entailment classifier on 'faithful' annotations (ENTAIL→FAITH) then measuring faithfulness and ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparison of faithfulness and ROUGE across selection strategies: BERTS2S baseline faithful = 26.9%, ENTAIL selection faithful = 31.5% (≈+4.6% absolute), ENTAIL→FAITH faithful = 31.7%. ROUGE tradeoff: BERTS2S R1 = 38.42, ENTAIL R1 = 35.93, ENTAIL→FAITH R1 = 37.31 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Positive for faithfulness selection (≈5% absolute increase in faithful summaries), but introduces trade-offs in ROUGE and may be manipulable; shows that using an imperfect entailment model can change which summaries are chosen and therefore affects reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Entailment-based evaluation was effective in this experimental setting; vulnerability to gaming is a general limitation of reference-less metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Domain mismatch in classifier training (sentence-sentence vs document-summary), and inherent property of reference-less metrics not constraining selection to reference similarity; implicit assumptions in natural-language claim about metric robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Combine entailment-based measures with reference-based metrics (e.g., ROUGE) to avoid trivial gaming; fine-tune entailment classifier on task-specific human-labeled faithful annotations; use cross-validation and larger faithful datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Fine-tuning the entailment classifier on faithfulness annotations improved selection slightly (ENTAIL→FAITH faithful 31.7% vs ENTAIL 31.5%) and improved ROUGE compared to ENTAIL alone, indicating domain-specific fine-tuning helps but larger labeled data is needed for larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLP evaluation / model selection</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Faithfulness and Factuality in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e686.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e686.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA_eval_compounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compounding errors in question-answering-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A QA-based pipeline intended to measure summary faithfulness can produce unreliable signals because question generation and answering components are trained on distributions different from model summaries, causing compounded errors and low correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QA-based summary evaluation pipeline (QG + QA + MRC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A round-trip system that generates question-answer pairs from a summary (using a question generator and answer extraction) and verifies answers by applying an MRC model to the source document.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>evaluation protocol / question-generation specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>pipeline code combining question generation, answer extraction, and reading-comprehension models</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>training-distribution mismatch / pipeline compounding errors</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Question generation models are trained on human-written references but are used to generate questions from automatically generated summaries; when summaries contain hallucinations these QG models may produce hallucinated questions, and the downstream MRC can sometimes answer them from the source by chance, creating false positives. Thus the natural-language claim that QA round-trip assesses fidelity does not hold reliably in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation pipeline (question generation and answer extraction components)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Spearman correlation analysis between QA-based scores and human faithfulness/factuality judgments; qualitative inspection of generated Q/A pairs and failure cases (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Correlation: QA metric correlation with faithful = 0.044 and with factual = 0.027 (very weak); generated question counts and answerability rates were computed (e.g., 731–820 Q/A pairs across systems) and qualitative examples showed hallucinated questions being answered correctly by source articles by chance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>QA-based evaluation performed poorly as a proxy for faithfulness (very weak correlations), risking both false reassurance and misleading comparisons between systems; the paper therefore cautions against straightforward use of QA-based metrics without improved QG.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across the evaluated models in this study (PTGEN, TConvS2S, TRANS2S, BERTS2S, Gold) and in 500 sampled pairs; correlation consistently weak.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mismatch between QG training distribution (human references) and usage on model-generated summaries (which include hallucinations), compounded by imperfect downstream QA models and the assumption that matching QA answers implies faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Improve question generation models (pretraining on broader distributions, QG specialized for model outputs), filter or validate generated questions, and combine QA signals with other semantic-inference measures; develop QA methods robust to hallucinated inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively in this paper; authors hypothesize that better QG (e.g., Narayan et al., 2020) and improved factual-consistency measures could alleviate issues.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLP evaluation / QA-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Faithfulness and Factuality in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e686.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e686.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenIE_unsuitability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenIE-style fact extraction unsuitability for extreme summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenIE-based automatic measures, which extract relation triples and compare source vs summary facts, were tested and found not well suited for evaluating extreme single-sentence summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenIE-based factuality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An evaluation approach that extracts OpenIE facts from source and generated summaries and checks for alignment as a proxy for faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric description / OpenIE evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>OpenIE extraction & fact-alignment scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>method-dataset mismatch / insufficient coverage of evaluation method</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although OpenIE metrics are described in related work as a way to measure factual consistency, when applied to XSum extreme summarization (single-sentence, highly abstractive) the OpenIE extraction performed poorly across models and failed to distinguish model performance — i.e., the implementation/assumption doesn't match the nature of the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation method applicability</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical application of OpenIE metrics to model outputs and observing uniformly poor performance and lack of significant differences across models (reported in Related Work / experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative statement and experimental observation: models perform poorly on OpenIE-based measures without significant differences; specific numeric OpenIE scores not reported, but authors state unsuitability based on experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>OpenIE measures would be unreliable for this task; relying on them could obscure real differences between models and give false conclusions about factual consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in experiments on XSum in this work; likely common for other highly abstractive single-sentence summarization datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>OpenIE tools and triple-extraction assumptions (structured fact extraction) do not capture the compressed, abstract, and stylistically varied content of extreme single-sentence summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use alternative semantic-inference methods (textual entailment) or tailor OpenIE extraction / post-processing specifically for short abstractive summaries; develop OpenIE tailored to the dataset style.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantified in this paper; authors report that alternative approaches (NLI/entailment) correlated better with human faithfulness judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLP evaluation / information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Faithfulness and Factuality in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e686.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e686.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pretraining_vs_hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining improves faithfulness but does not eliminate hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained encoder-decoder models (e.g., BERTS2S) show improved faithfulness and factuality relative to randomly-initialized models, but a majority of hallucinations remain erroneous — demonstrating a partial mismatch between claims about pretraining benefits and the residual errors in implementation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pretrained encoder-decoder summarization systems (BERTS2S)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sequence-to-sequence Transformer initialized with pretrained BERT checkpoints for both encoder and decoder and fine-tuned on XSum.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>model description / expected benefits from pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model initialization & fine-tuning code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>overgeneralized expectation vs empirical outcome (partial improvement only)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language claims and community expectations that large-scale pretraining will substantially fix factuality are only partially true: BERTS2S achieves higher ROUGE and higher human-judged faithfulness/factuality than baselines, but over 90% of its hallucinations remain erroneous. Thus the practical implementation still yields many faithfulness failures.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture & training outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>human annotation of hallucination and factuality across models and correlating with pretraining status; statistical comparison (ANOVA/Tukey HSD).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantified in Table 2 and Section 5.3: BERTS2S faithful = 26.9%, faithful+factual = 34.7% (best among models); nevertheless, >90% of BERTS2S hallucinations were judged erroneous in the factuality assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Pretraining yields measurable but limited improvements in faithfulness/factuality; relying solely on pretraining and automatic metrics may overstate real-world factual reliability of generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed for pretrained vs non-pretrained models in this evaluation (BERTS2S vs PTGEN/TRANS2S/TConvS2S); improvements consistent but residual hallucination common.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Pretraining provides richer priors and world knowledge but does not guarantee document-grounded inference; dataset divergence and decoding/training objectives still permit hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Combine pretraining with faithfulness-focused objectives (entailment rewards, faithfulness fine-tuning), better decoding and training that penalizes hallucination, and improved evaluation signals for model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: pretraining increased faithful+factual summaries to 34.7% (BERTS2S) compared to 27.3% for PTGEN (≈7.4% absolute improvement). Additional entailment-based selection and fine-tuning produced further modest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / NLP model development</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Faithfulness and Factuality in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating the Factual Consistency of Abstractive Text Summarization <em>(Rating: 2)</em></li>
                <li>Ranking generated summaries by correctness: An interesting but challenging application for natural language inference <em>(Rating: 2)</em></li>
                <li>Asking and answering questions to evaluate the factual consistency of summaries <em>(Rating: 2)</em></li>
                <li>Neural text summarization: A critical evaluation <em>(Rating: 1)</em></li>
                <li>Guiding extractive summarization with question-answering rewards <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-686",
    "paper_id": "paper-dbeeca8466e0c177ec67c60d529899232415ca87",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "source-target divergence",
            "name_full": "Divergence between source documents and target (gold) summaries",
            "brief_description": "Dataset-level mismatch where gold summaries often include information not present in the source article (introductory sentences or background), causing a mismatch between the natural-language dataset description/assumption and the model training/evaluation behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "XSum extreme-document summarization evaluation pipeline",
            "system_description": "Training and evaluation pipeline for abstractive summarization models on the XSum dataset (BBC articles paired with single-sentence journalist-written summaries).",
            "nl_description_type": "dataset documentation / reference summaries description",
            "code_implementation_type": "model training & evaluation code (seq2seq training on (document, gold-summary) pairs)",
            "gap_type": "dataset-documentation mismatch / implicit divergence between source and target",
            "gap_description": "Gold summaries are journalistic introductory sentences that frequently introduce background facts not present in the article; the dataset documentation and common evaluation uses implicitly assume the reference is faithful to the source, but in practice many references diverge. Models trained on this data therefore learn to generate content not directly supported by the source (extrinsic hallucinations), because the training pairs expose them to target-side information absent from inputs.",
            "gap_location": "training data / data preprocessing / dataset design",
            "detection_method": "empirical human annotation study comparing source documents and summaries (annotators labeled hallucinated spans and whether they were intrinsic/extrinsic); descriptive analysis of dataset properties",
            "measurement_method": "human annotation counts and percentages (Table 2 & text): measured percent of summaries containing intrinsic/extrinsic hallucinations; statistics reported over 500 sampled test examples and 3 annotators per item (e.g., &gt;70% of single-sentence summaries contained hallucinations; gold summaries had 73.1% extrinsic hallucinations in the sample).",
            "impact_on_results": "Substantial: results in models learning to produce extrinsic content; gold references themselves exhibit high extrinsic rates (73.1%), thus complicating training targets and evaluation. The paper reports that intrinsic+extrinsic hallucinations happen frequently (&gt;70% single-sentence summaries) and that the prevalence of extrinsic hallucinations in generated summaries is high (PTGEN 63.3%, TConvS2S 71.5%, TRANS2S 68.1%, BERTS2S 64.1%), distorting both training signals and evaluation interpretations.",
            "frequency_or_prevalence": "High — dataset gold summaries: extrinsic hallucination annotated in 73.1% of sampled golds; model outputs: extrinsic hallucination present in ~63–71% of summaries depending on model; overall hallucination (&gt;I ∪ E) in 73–79% of system summaries (Table 2).",
            "root_cause": "dataset artifact / implicit assumptions in natural-language description of the dataset (journalists' introductory summaries include external/background knowledge) and omission of this divergence in dataset specification and evaluation practices",
            "mitigation_approach": "Explicitly treat source-target divergence during training and evaluation: (i) use human annotations of faithfulness for training/dev, (ii) incorporate reference-less faithfulness measures (textual entailment) for model selection and decoding, and (iii) be explicit in dataset documentation about expected target divergence or filter/label such examples.",
            "mitigation_effectiveness": "Partially effective: using entailment-based selection or fine-tuning to predict faithfulness improved selection of faithful summaries (ENTAIL selection increased faithfulness from 26.9% to 31.5% for the best model; further fine-tuning ENTAIL→FAITH gave 31.7%), but the underlying dataset divergence remains and many hallucinations persist. No full elimination reported.",
            "domain_or_field": "machine learning / natural language processing (abstractive summarization)",
            "reproducibility_impact": true,
            "uuid": "e686.0",
            "source_info": {
                "paper_title": "On Faithfulness and Factuality in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "ROUGE_misalignment",
            "name_full": "Misalignment of ROUGE/BERTScore with faithfulness and factuality",
            "brief_description": "Standard automatic metrics (ROUGE, BERTScore) are commonly described/used as proxies for summary quality (informativeness/overlap), but empirically they correlate weakly with human judgments of faithfulness and factuality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Automatic summarization evaluation pipeline",
            "system_description": "Evaluation framework that uses ROUGE and BERTScore to assess model summarization quality and to guide development/selection.",
            "nl_description_type": "metric specification / evaluation protocol description",
            "code_implementation_type": "evaluation metric implementation (ROUGE, BERTScore computations in experiment scripts)",
            "gap_type": "metric-behavior mismatch / insufficient metric specification",
            "gap_description": "ROUGE and BERTScore are described and used as proxies for informativeness and fluency, but they do not capture semantic faithfulness; they can report high scores for summaries that are fluent and topical yet contain hallucinated or incorrect facts. The intended natural-language claim that these metrics reflect useful summarization quality is therefore misleading for faithfulness-sensitive tasks.",
            "gap_location": "evaluation metrics / model selection",
            "detection_method": "correlation analysis (Spearman) between automatic metrics and human annotations of faithfulness and factuality; human annotation study comparing metric scores and labeled faithfulness",
            "measurement_method": "Spearman correlation coefficients: ROUGE-1 with faithfulness = 0.197, ROUGE-2 = 0.162, ROUGE-L = 0.162, BERTScore = 0.190 (Table 4/9); these weak correlations demonstrate poor alignment. Additional qualitative examples show fluent but hallucinated summaries receiving high ROUGE.",
            "impact_on_results": "Misleading model evaluation and model selection: reliance on ROUGE/BERTScore can favour models that are fluent/topical but unfaithful; the paper excluded GPT-TUNED after poor performance on automatic measures and yet found cases where pretrained models are more factual. Quantitatively, ROUGE-based selection does not align with human faithfulness judgments, potentially causing researchers to overestimate real-world factuality.",
            "frequency_or_prevalence": "ROUGE/BERTScore are standard and widely used; in this study they consistently showed weak correlation across models and human-labeled data.",
            "root_cause": "metric design focused on lexical overlap / embedding similarity rather than semantic entailment; implicit assumption in natural-language descriptions that lexical/embedding overlap proxies for factual consistency",
            "mitigation_approach": "Complement ROUGE/BERTScore with semantic-inference-based, reference-less measures such as textual entailment scores or QA-based consistency checks; use human faithfulness annotations to fine-tune evaluators.",
            "mitigation_effectiveness": "Textual entailment correlates substantially better (entailment |r_s| with faithful = 0.431 vs ROUGE-1 0.197). Using entailment for model selection increased faithfulness (see ENTAIL experiments), demonstrating practical benefit over ROUGE alone.",
            "domain_or_field": "machine learning / NLP evaluation",
            "reproducibility_impact": true,
            "uuid": "e686.1",
            "source_info": {
                "paper_title": "On Faithfulness and Factuality in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "entailment_selection_gap",
            "name_full": "Reference-less textual entailment measure limitations and gaming risk",
            "brief_description": "Using an entailment classifier as a reference-less faithfulness evaluator or model-selection objective is effective but has limitations: it can be gamed and its assumptions differ from intended evaluation descriptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Entailment-based evaluation and model selection",
            "system_description": "A BERT-Large NLI classifier (fine-tuned on MultiNLI, further fine-tuned on human 'faithful' annotations) used to score whether a summary is entailed by its document and to select among candidate summaries.",
            "nl_description_type": "metric description / model-selection objective",
            "code_implementation_type": "evaluation & selection code (entailment classifier scoring over document-summary pairs)",
            "gap_type": "assumption mismatch / reference-less metric vulnerability (gaming) / domain mismatch for classifier training",
            "gap_description": "Although textual entailment is described as a strong proxy for faithfulness, the paper notes the entailment classifier is trained on sentence-sentence NLI (MultiNLI) and is not perfectly matched to document-summary pairs; additionally, because the measure is reference-less it can be gamed (e.g., selecting the first sentence of the document will often be judged as entailed). This is a gap between the natural-language claim 'entailment measures faithfulness' and the practical behavior of the implementation.",
            "gap_location": "evaluation metric design and model-selection procedures",
            "detection_method": "empirical experiments: (i) computing entailment-based rankings and selecting summaries (ENTAIL row) and (ii) fine-tuning entailment classifier on 'faithful' annotations (ENTAIL→FAITH) then measuring faithfulness and ROUGE.",
            "measurement_method": "Comparison of faithfulness and ROUGE across selection strategies: BERTS2S baseline faithful = 26.9%, ENTAIL selection faithful = 31.5% (≈+4.6% absolute), ENTAIL→FAITH faithful = 31.7%. ROUGE tradeoff: BERTS2S R1 = 38.42, ENTAIL R1 = 35.93, ENTAIL→FAITH R1 = 37.31 (Table 5).",
            "impact_on_results": "Positive for faithfulness selection (≈5% absolute increase in faithful summaries), but introduces trade-offs in ROUGE and may be manipulable; shows that using an imperfect entailment model can change which summaries are chosen and therefore affects reported results.",
            "frequency_or_prevalence": "Entailment-based evaluation was effective in this experimental setting; vulnerability to gaming is a general limitation of reference-less metrics.",
            "root_cause": "Domain mismatch in classifier training (sentence-sentence vs document-summary), and inherent property of reference-less metrics not constraining selection to reference similarity; implicit assumptions in natural-language claim about metric robustness.",
            "mitigation_approach": "Combine entailment-based measures with reference-based metrics (e.g., ROUGE) to avoid trivial gaming; fine-tune entailment classifier on task-specific human-labeled faithful annotations; use cross-validation and larger faithful datasets.",
            "mitigation_effectiveness": "Fine-tuning the entailment classifier on faithfulness annotations improved selection slightly (ENTAIL→FAITH faithful 31.7% vs ENTAIL 31.5%) and improved ROUGE compared to ENTAIL alone, indicating domain-specific fine-tuning helps but larger labeled data is needed for larger gains.",
            "domain_or_field": "NLP evaluation / model selection",
            "reproducibility_impact": true,
            "uuid": "e686.2",
            "source_info": {
                "paper_title": "On Faithfulness and Factuality in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "QA_eval_compounding",
            "name_full": "Compounding errors in question-answering-based evaluation",
            "brief_description": "A QA-based pipeline intended to measure summary faithfulness can produce unreliable signals because question generation and answering components are trained on distributions different from model summaries, causing compounded errors and low correlation with human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "QA-based summary evaluation pipeline (QG + QA + MRC)",
            "system_description": "A round-trip system that generates question-answer pairs from a summary (using a question generator and answer extraction) and verifies answers by applying an MRC model to the source document.",
            "nl_description_type": "evaluation protocol / question-generation specification",
            "code_implementation_type": "pipeline code combining question generation, answer extraction, and reading-comprehension models",
            "gap_type": "training-distribution mismatch / pipeline compounding errors",
            "gap_description": "Question generation models are trained on human-written references but are used to generate questions from automatically generated summaries; when summaries contain hallucinations these QG models may produce hallucinated questions, and the downstream MRC can sometimes answer them from the source by chance, creating false positives. Thus the natural-language claim that QA round-trip assesses fidelity does not hold reliably in practice.",
            "gap_location": "evaluation pipeline (question generation and answer extraction components)",
            "detection_method": "Spearman correlation analysis between QA-based scores and human faithfulness/factuality judgments; qualitative inspection of generated Q/A pairs and failure cases (Figure 3).",
            "measurement_method": "Correlation: QA metric correlation with faithful = 0.044 and with factual = 0.027 (very weak); generated question counts and answerability rates were computed (e.g., 731–820 Q/A pairs across systems) and qualitative examples showed hallucinated questions being answered correctly by source articles by chance.",
            "impact_on_results": "QA-based evaluation performed poorly as a proxy for faithfulness (very weak correlations), risking both false reassurance and misleading comparisons between systems; the paper therefore cautions against straightforward use of QA-based metrics without improved QG.",
            "frequency_or_prevalence": "Observed across the evaluated models in this study (PTGEN, TConvS2S, TRANS2S, BERTS2S, Gold) and in 500 sampled pairs; correlation consistently weak.",
            "root_cause": "Mismatch between QG training distribution (human references) and usage on model-generated summaries (which include hallucinations), compounded by imperfect downstream QA models and the assumption that matching QA answers implies faithfulness.",
            "mitigation_approach": "Improve question generation models (pretraining on broader distributions, QG specialized for model outputs), filter or validate generated questions, and combine QA signals with other semantic-inference measures; develop QA methods robust to hallucinated inputs.",
            "mitigation_effectiveness": "Not evaluated quantitatively in this paper; authors hypothesize that better QG (e.g., Narayan et al., 2020) and improved factual-consistency measures could alleviate issues.",
            "domain_or_field": "NLP evaluation / QA-based evaluation",
            "reproducibility_impact": true,
            "uuid": "e686.3",
            "source_info": {
                "paper_title": "On Faithfulness and Factuality in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "OpenIE_unsuitability",
            "name_full": "OpenIE-style fact extraction unsuitability for extreme summarization evaluation",
            "brief_description": "OpenIE-based automatic measures, which extract relation triples and compare source vs summary facts, were tested and found not well suited for evaluating extreme single-sentence summarization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "OpenIE-based factuality evaluation",
            "system_description": "An evaluation approach that extracts OpenIE facts from source and generated summaries and checks for alignment as a proxy for faithfulness.",
            "nl_description_type": "metric description / OpenIE evaluation protocol",
            "code_implementation_type": "OpenIE extraction & fact-alignment scripts",
            "gap_type": "method-dataset mismatch / insufficient coverage of evaluation method",
            "gap_description": "Although OpenIE metrics are described in related work as a way to measure factual consistency, when applied to XSum extreme summarization (single-sentence, highly abstractive) the OpenIE extraction performed poorly across models and failed to distinguish model performance — i.e., the implementation/assumption doesn't match the nature of the dataset.",
            "gap_location": "evaluation method applicability",
            "detection_method": "empirical application of OpenIE metrics to model outputs and observing uniformly poor performance and lack of significant differences across models (reported in Related Work / experiments).",
            "measurement_method": "Qualitative statement and experimental observation: models perform poorly on OpenIE-based measures without significant differences; specific numeric OpenIE scores not reported, but authors state unsuitability based on experiments.",
            "impact_on_results": "OpenIE measures would be unreliable for this task; relying on them could obscure real differences between models and give false conclusions about factual consistency.",
            "frequency_or_prevalence": "Observed in experiments on XSum in this work; likely common for other highly abstractive single-sentence summarization datasets.",
            "root_cause": "OpenIE tools and triple-extraction assumptions (structured fact extraction) do not capture the compressed, abstract, and stylistically varied content of extreme single-sentence summaries.",
            "mitigation_approach": "Use alternative semantic-inference methods (textual entailment) or tailor OpenIE extraction / post-processing specifically for short abstractive summaries; develop OpenIE tailored to the dataset style.",
            "mitigation_effectiveness": "Not quantified in this paper; authors report that alternative approaches (NLI/entailment) correlated better with human faithfulness judgments.",
            "domain_or_field": "NLP evaluation / information extraction",
            "reproducibility_impact": true,
            "uuid": "e686.4",
            "source_info": {
                "paper_title": "On Faithfulness and Factuality in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "pretraining_vs_hallucination",
            "name_full": "Pretraining improves faithfulness but does not eliminate hallucinations",
            "brief_description": "Pretrained encoder-decoder models (e.g., BERTS2S) show improved faithfulness and factuality relative to randomly-initialized models, but a majority of hallucinations remain erroneous — demonstrating a partial mismatch between claims about pretraining benefits and the residual errors in implementation outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Pretrained encoder-decoder summarization systems (BERTS2S)",
            "system_description": "Sequence-to-sequence Transformer initialized with pretrained BERT checkpoints for both encoder and decoder and fine-tuned on XSum.",
            "nl_description_type": "model description / expected benefits from pretraining",
            "code_implementation_type": "model initialization & fine-tuning code",
            "gap_type": "overgeneralized expectation vs empirical outcome (partial improvement only)",
            "gap_description": "Natural-language claims and community expectations that large-scale pretraining will substantially fix factuality are only partially true: BERTS2S achieves higher ROUGE and higher human-judged faithfulness/factuality than baselines, but over 90% of its hallucinations remain erroneous. Thus the practical implementation still yields many faithfulness failures.",
            "gap_location": "model architecture & training outcomes",
            "detection_method": "human annotation of hallucination and factuality across models and correlating with pretraining status; statistical comparison (ANOVA/Tukey HSD).",
            "measurement_method": "Quantified in Table 2 and Section 5.3: BERTS2S faithful = 26.9%, faithful+factual = 34.7% (best among models); nevertheless, &gt;90% of BERTS2S hallucinations were judged erroneous in the factuality assessment.",
            "impact_on_results": "Pretraining yields measurable but limited improvements in faithfulness/factuality; relying solely on pretraining and automatic metrics may overstate real-world factual reliability of generated summaries.",
            "frequency_or_prevalence": "Observed for pretrained vs non-pretrained models in this evaluation (BERTS2S vs PTGEN/TRANS2S/TConvS2S); improvements consistent but residual hallucination common.",
            "root_cause": "Pretraining provides richer priors and world knowledge but does not guarantee document-grounded inference; dataset divergence and decoding/training objectives still permit hallucination.",
            "mitigation_approach": "Combine pretraining with faithfulness-focused objectives (entailment rewards, faithfulness fine-tuning), better decoding and training that penalizes hallucination, and improved evaluation signals for model selection.",
            "mitigation_effectiveness": "Partial: pretraining increased faithful+factual summaries to 34.7% (BERTS2S) compared to 27.3% for PTGEN (≈7.4% absolute improvement). Additional entailment-based selection and fine-tuning produced further modest gains.",
            "domain_or_field": "deep learning / NLP model development",
            "reproducibility_impact": true,
            "uuid": "e686.5",
            "source_info": {
                "paper_title": "On Faithfulness and Factuality in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
            "rating": 2
        },
        {
            "paper_title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "rating": 2
        },
        {
            "paper_title": "Neural text summarization: A critical evaluation",
            "rating": 1
        },
        {
            "paper_title": "Guiding extractive summarization with question-answering rewards",
            "rating": 1
        }
    ],
    "cost": 0.01695025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On Faithfulness and Factuality in Abstractive Summarization</h1>
<p>Joshua Maynez<em> ${ }^{</em>}$ Shashi Narayan* Bernd Bohnet Ryan McDonald<br>Google Research<br>{joshuahm, shashinarayan, bohnetbd, ryanmcd}@google.com</p>
<h4>Abstract</h4>
<p>It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Current state of the art conditional text generation models accomplish a high level of fluency and coherence, mostly thanks to advances in sequence-to-sequence architectures with attention and copy (Sutskever et al., 2014; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al., 2017; Dai et al., 2019) and more recently pretrained language modeling for natural language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a).</p>
<p>Document summarization - the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of "Conservative MP Zac Smith winning the primary for 2016 London mayoral election", but summaries often forge entities (e.g., "Nigel Goldsmith" or "Zac Goldwin") or information (e.g., "UKIP leader Nigel Goldsmith", "Nigel Goldsmith winning the mayoral election", "Sadiq Khan being the former London mayor" or "Zac Goldwin being the Labour's candidate") that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and fluent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003).</p>
<p>We conducted a large-scale human evaluation of hallucinated content in systems that use Recurrent Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme Summarization task (XSum, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive summarizers hallucinate content?; (ii) Do models hal-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">GOLD</th>
<th style="text-align: center;">Zac Goldsmith will contest the 2016 London mayoral election for the Conservatives, it has been announced.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Document:</td>
<td style="text-align: center;">The Richmond Park and North Kingston MP said he was "honoured" after winning 70\% of the 9,227 votes cast using an online primary system. <br> He beat London Assembly Member Andrew Boff, MEP Syed Kamall and London's deputy mayor for crime and policing Stephen Greenhalgh.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mr Goldsmith's main rival is likely to be Labour's Sadiq Khan. (2 sentences with 59 words are abbreviated here.) Mr Goldsmith, who was the favourite for the Tory nomination, balloted his constituents earlier this year to seek permission to stand.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">At the very point of his entry into the race for London mayor, Zac Goldsmith's decision revealed two big characteristics. (5 sentences with 108 words are abbreviated here.)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mr Goldsmith - who first entered Parliament in 2010 - told the BBC's Daily Politics that he hoped his environmental record would appeal to Green and Lib Dem voters and he also hoped to "reach out" to UKIP supporters frustrated with politics as usual and the UK's relationship with the EU.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zac Goldsmith Born in 1975, educated at Eton and the Cambridge Centre for Sixth-form Studies (5 sentences with 76 words are abbreviated here.)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mr Goldsmith, who has confirmed he would stand down from Parliament if he became mayor, triggering a by-election, said he wanted to build on current mayor Boris Johnson's achievements. (3 sentences with 117 words are abbreviated here.) Both Mr Khan and Mr Goldsmith oppose a new runway at Heathrow airport, a fact described by the British Chambers of Commerce as "depressing". (1 sentences with 31 words is abbreviated here.)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Current mayor Boris Johnson will step down next year after two terms in office. He is also currently the MP for Uxbridge and South Ruislip, having been returned to Parliament in May.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Some Conservatives have called for an inquiry into the mayoral election process after only 9,227 people voted - compared with a 87,884 turnout for the Labour contest. (4 sentences with 121 words are abbreviated here.)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UKIP leader Nigel Goldsmith has been elected as the new mayor of London to elect a new Conservative MP.</td>
</tr>
<tr>
<td style="text-align: center;">TCONVS2S</td>
<td style="text-align: center;">Former London mayoral candidate Zac Goldsmith has been chosen to stand in the London mayoral election.</td>
</tr>
<tr>
<td style="text-align: center;">Trans2S</td>
<td style="text-align: center;">Former London mayor Sadiq Khan has been chosen as the candidate to be the next mayor of London.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Tuned</td>
<td style="text-align: center;">Conservative MP Zac Goldwin's bid to become Labour's candidate in the 2016 London mayoral election.</td>
</tr>
<tr>
<td style="text-align: center;">BertS2S</td>
<td style="text-align: center;">Zac Goldsmith has been chosen to contest the London mayoral election.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">45.7, 6.1, 28.6]</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$[50.0,26.7,37.5]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$[35.3,12.5,23.5]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$[42.4,25.8,36.4]$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 1: Hallucinations in extreme document summarization: the abbreviated article, its gold summary and the abstractive model generated summaries (PtGen, See et al. 2017; TConvS2S, Narayan et al. 2018a; and, GPTTuned, TransS2S and BertS2S, Rothe et al. 2020) for a news article from the extreme summarization dataset (Narayan et al., 2018a). The dataset and the abstractive models are described in Section 3 and 4. We also present the [ROUGE-1, ROUGE-2, ROUGE-L] $\mathrm{F}_{1}$ scores relative to the reference gold summary. Words in red correspond to hallucinated information whilst words in blue correspond to faithful information.
lucinate by manipulating the information present in the input document (intrinsic hallucinations) or by adding information not directly inferable from the input document (extrinsic hallucinations)?; (iii) How much hallucinated content is factual, even when unfaithful?; and (iv) Are there automatic means of measuring these hallucinations?</p>
<p>Our main conclusions are as follows: First, intrinsic and extrinsic hallucinations happen frequently - in more than $70 \%$ of single-sentence summaries. Second, the majority of hallucinations are extrinsic, which potentially could be valid abstractions that use background knowledge. However, our study found that over $90 \%$ of extrinsic hallucinations were erroneous. Thus, hallucinations happen in most summaries and the majority of these are neither faithful nor factual. Third, models initialized with pretrained parameters perform best both on automatic metrics and human judgments of faithfulness/factuality. Furthermore, they have the highest percentage of extrinsic hallucinations that are factual. This suggests that while some studies
argue that large-scale pretrained models are merely better at learning data-specific regularities (Niven and Kao, 2019), at least on in-domain summarization the gains in automatic metrics are realized in observable differences by humans. Fourth, ROUGE (Lin and Hovy, 2003) and BERTScore (Zhang et al., 2020) correlates less with faithfulness/factuality than metrics derived from automatic semantic inference systems, specifically the degree to which a summary is entailed by the source document. This presents an opportunity for improved automatic evaluation measures as well as model training and decoding objectives. We show preliminary experiments in this direction.</p>
<h2>2 Hallucinations in Summarization</h2>
<p>Open-ended generation - the task of generating text that forms a natural continuation from the input text - requires the model to hallucinate text; hence the focus has been to ensure that the model learns to generate text that is more human-like (i.e., less repetitive or dull with more content-related words)</p>
<p>(Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In contrast, tasks such as document summarization (Nenkova and McKeown, 2011; See et al., 2017; Paulus et al., 2018) and data-to-text generation (Lebret et al., 2016; Wiseman et al., 2017) which are not open-ended, require models to be factual and/or faithful to the source text.</p>
<p>Despite recent improvements in conditional text generation, most summarization systems are trained to maximize the log-likelihood of the reference summary at the word-level, which does not necessarily reward models for being faithful. Moreover, models are usually agnostic to the noises or artifacts of the training data, such as reference divergence, making them vulnerable to hallucinations (Kryscinski et al., 2019a; Wiseman et al., 2017; Dhingra et al., 2019). Thus, models can generate texts that are not consistent with the input, yet would likely have reasonable model log-likelihood.</p>
<h3>2.1 Intrinsic and Extrinsic Hallucinations</h3>
<p>Given a document $D$ and its abstractive summary $S$, we try to identify all hallucinations in $S$ with respect to the content of $D$, regardless of the quality of the summary. In this work, we define a summary as being hallucinated if it has a span(s) $w_{i} \ldots w_{i+j}$, $j \geq i$, that is not supported by the input document. To distinguish hallucinations further in the context of a document and a summary, we categorize hallucinations by the information source as intrinsic and extrinsic hallucinations. Note, paraphrases or any information that can be inferred from the document are not categorized as hallucinations.</p>
<p>Intrinsic hallucinations are consequences of synthesizing content using the information present in the input document. For example, in Figure 1, "Former London mayoral candidate" in the TConVS2S abstract and "Former London mayor" in the TRANS2S abstract are hallucinations of intrinsic nature; both use terms or concepts from the document but misrepresent information from the document, making them unfaithful to the document. The article does not confirm if "Zac Goldsmith" was a "Former London mayoral candidate" or if "Sadiq Khan" was a "Former London mayor". One may suspect that a model with poor input document representation will fail to do document level inference, often required for abstraction, and will be vulnerable to such errors.</p>
<p>Extrinsic hallucinations are model generations that ignore the source material altogether. For example, in Figure 1, "Nigel" in the PtGEN abstract and "2016" in both Gold and GPT-TunED are
extrinsic hallucinations; these terms are not introduced in the document. A model with a poorlyinformed decoder and that is agnostic to the divergence issue between the source and target texts (Wiseman et al., 2017; Dhingra et al., 2019), will function more as an open-ended language model and will be prone to extrinsic hallucinations.</p>
<h3>2.2 Factual Hallucinations in Summarization</h3>
<p>A summary $S$ of a document $D$ contains a factual hallucination if it contains information not found in $D$ that is factually correct. Factual hallucinations may be composed of intrinsic hallucinations or extrinsic hallucinations.</p>
<p>By definition, abstractive summaries are written to preserve the salient information in the input document, but they are expressed in the words of the summary author as opposed to the input document author (Nenkova and McKeown, 2011). As such, it is natural to construct summaries that integrate with the author's background knowledge (van Dijk and Kintsch, 1978; Brown and Day, 1983). Such knowledge integration can also be desirable in real world applications. For instance, an engaging sports report will reflect an understanding of the game to provide color and context. Another example is audience-targeted summarization where a good summary will reflect understanding of both the article domain and the desired audience. Nonetheless, there is no consensus in the research community if the summary should be faithful (without any hallucinations) to the input document or if there is tolerance for factual hallucinations.</p>
<p>Recent deep learning approaches to abstractive summarization naturally learn to integrate knowledge from the training data while generating an abstractive summary for a document (See et al., 2017; Gehrmann et al., 2018). More advanced pretrained text generators (Radford et al., 2018, 2019; Dong et al., 2019; Song et al., 2019; Khandelwal et al., 2019; Rothe et al., 2020) are even better at capturing world knowledge as they are informed by a vast amount of background text. This can be observed in the example shown in Figure 1 as the input document does not mention that the discussed "London mayoral election" is from "2016"; but the abstract generated by the pretrained text generator GPT-TUNED correctly predicts this information similar to the human-authored abstract. ${ }^{2}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this paper we stand in favour of the assertion that abstractive systems may integrate with the background knowledge to generate rich and meaningful summaries. More concretely, "hallucinations in summarization are acceptable if they lead to better summaries that are factual with respect to the document and the associated background knowledge." This assumption also allows us to assess the capability of recent neural models to integrate with the background knowledge to generate factual abstracts (see Section 5.3).</p>
<h2>3 Extreme Document Summarization</h2>
<p>We focus on the recently introduced extreme summarization dataset (XSum, Narayan et al., 2018a) ${ }^{3}$ which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their singlesentence summaries, provided by the journalists writing the articles. The dataset is split into three subsets: training ( $90 \%, 204,045$ ), validation ( $5 \%$, 11,332), and test ( $5 \%, 11,334$ ) sets. All models in $\S 4$ trained to generate abstractive summaries are trained and evaluated using this standard split, provided by the authors.</p>
<p>We choose to focus our study on extreme summarization for the following reasons: First, this task aims to create a single-sentence summary of a news article; these shorter summaries are relatively easier to annotate and analyze than longer summaries such as story highlights from the CNN/Dailymail dataset (Hermann et al., 2015) or abstracts from the NY Times (Sandhaus, 2008) or the WikiSum (Liu et al., 2018) dataset. Secondly, the gold summary in the extreme summarization dataset is an introductory sentence prefacing each article. By virtue of this property, the extreme summarization task is not amenable to extractive strategies and requires an abstractive modeling approach. Hence, it provides us a better benchmark to assess abstractive models' abilities to produce abstractions which are faithful and factual. Finally, since we conclude that hallucination is a problem on this dataset, then we can safely conclude it is a problem for summarization datasets with longer summaries, as modeling longer-distance dependencies and discourse structures make the task harder.</p>
<h2>4 Abstractive Summaries</h2>
<p>We evaluate summaries from RNN, CNN and Transformer-based state-of-the-art abstractive summarization methods and the reference human writ-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ten summaries. See the Appendix for hyperparameter and decoding details for all models.</p>
<p>Human Written Reference Summaries. The single-sentence summaries contained in the extreme summarization dataset (XSUM) are also evaluated as part of this study. These summaries were written by journalists as introductions to the news articles they precede. These summaries, therefore, often have true additional information not found in the document. Such divergence issue between source and target is not uncommon in conditional text generation (Kryscinski et al., 2019a; Wiseman et al., 2017; Dhingra et al., 2019).</p>
<p>RNN-based Seq2Seq. We use the PointerGenerator model (PTGEN) introduced by See et al. (2017), an RNN-based attention-based sequence to sequence model which not only generates from the target vocabulary but can copy words from the source text.</p>
<p>Topic-aware Convolutional Seq2Seq. The Topic-aware Convolutional Sequence to Sequence model (TConvS2S) introduced by Narayan et al. (2018a) is an abstractive system which is conditioned on the article's topics and based entirely on Convolutional Neural Networks (Gehring et al., 2017). TConvS2S is better suited for extreme summarization, as convolution layers capture long-range dependencies between words in the document more effectively than RNNs. Simultaneously, the convolutional encoder associates each word with a topic vector, capturing whether it is representative of the document's content.</p>
<p>Transformer-based Abstractive Methods. We experiment with three Transformer-based model variants, all of which have 12 layers, a hidden size of 768 , filter size of 3072 , and 12 attention heads. GPT-TunED: Radford et al. (2019) proposed Transformer-based Generative Pre-Trained (GPT) language models that can generate high quality text in open-ended generation setups. The proposed decoder-only architecture for language modeling can be easily adapted to abstractive summarization where the model first sees the document and, given a prompt, such as TL;DR;, generates its summary. Our GPT-TUNED is warm-started with a publicly available GPT checkpoint (Radford et al., 2019), but fine-tuned with supervised training on the extreme summarization dataset.
TranS2S and BertS2S: TranS2S and BERTS2S are sequence to sequence models</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Human Eval Test Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">R2</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;">BERTScore</td>
</tr>
<tr>
<td style="text-align: left;">PTGEN</td>
<td style="text-align: center;">30.01</td>
<td style="text-align: center;">9.38</td>
<td style="text-align: center;">23.76</td>
<td style="text-align: center;">74.30</td>
</tr>
<tr>
<td style="text-align: left;">TCONVS2S</td>
<td style="text-align: center;">30.89</td>
<td style="text-align: center;">11.47</td>
<td style="text-align: center;">25.80</td>
<td style="text-align: center;">75.23</td>
</tr>
<tr>
<td style="text-align: left;">TRANS2S</td>
<td style="text-align: center;">32.28</td>
<td style="text-align: center;">11.66</td>
<td style="text-align: center;">24.65</td>
<td style="text-align: center;">75.69</td>
</tr>
<tr>
<td style="text-align: left;">GPT-TUNED</td>
<td style="text-align: center;">21.82</td>
<td style="text-align: center;">4.72</td>
<td style="text-align: center;">16.28</td>
<td style="text-align: center;">70.35</td>
</tr>
<tr>
<td style="text-align: left;">BERTS2S</td>
<td style="text-align: center;">$\mathbf{3 8 . 4 2}$</td>
<td style="text-align: center;">$\mathbf{1 6 . 9 6}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 2 7}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 8 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: ROUGE and BERTScore $\mathrm{F}_{1}$ scores for nonpretrained (the top block) and pretrained (the bottom block) models reported on the XSum dataset. These results are on the sampled human evaluation (500 items) dataset. The best results are boldfaced.
where both encoder and decoder are composed of Transformer layers (Vaswani et al., 2017; Rothe et al., 2020). All weights in Trans2S are randomly initialized, but in BERTS2S, both encoder and decoder are initialized with the BERT-Base checkpoints (Devlin et al., 2019), with parameter sharing between the encoder and decoder, following Rothe et al. (2020). The only variable that is initialized randomly is the encoderdecoder attention in BERTS2S. Both models are then trained on the extreme summarization dataset.</p>
<h2>5 Experiments and Results</h2>
<p>The main focus of this work is not to propose a solution to hallucination related issues, but to achieve a better understanding of hallucinations in abstractive summarization through their human assessment. We randomly sampled 500 articles from the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of human judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020).</p>
<h3>5.1 Automatic Evaluation of Summaries</h3>
<p>ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model's ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate sum-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Summary</th>
<th style="text-align: center;">Intrinsic</th>
<th style="text-align: center;">Extrinsic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Conservative MP: Fac: Goldwör's bid to become Labour's candidate in the 2018 London</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mayoral election.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: Human assessment of a system generated summary for the article in Figure 1. The annotation user interface is shown as it was shown to raters.
mary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextual embeddings. Results are presented in Table 1.</p>
<p>For both cases, the pretrained encoder-decoder architecture BERTS2S performed far superior to any other randomly initialized models, such as PTGEN, TCONVS2S and TRANS2S, and the decoderonly architecture GPT-TUNED. The differences between PTGEN, TCONVS2S and TRANS2S are not significant; all other differences are significant. ${ }^{4}$</p>
<p>ROUGE and BERTScore are indicators of informativeness of summaries but they are not sufficient metrics to assess the overall quality of summaries. This becomes evident from our human assessments in the following sections where we employ human annotators to evaluate summaries generated with PTGEN, TCONVS2S, TRANS2S and BERTS2S, and the human authored summaries. We excluded GPT-TUNED abstracts from our study after their poor performance on the automatic measures.</p>
<h3>5.2 Assessment of Hallucinations</h3>
<p>In this assessment, human annotators were presented an article and a single-sentence summary for the article. They were stringently told to only assess the hallucinations in the summary and to not confuse their assessment with the quality of the summary. For summaries containing hallucinations, annotators were tasked with (i) identifying those text spans that were unfaithful to the article and (ii) for each text span, annotating whether the hallucination was intrinsic or extrinsic. We elicited judgments from three different annotators for each of 2500 (500x5) document-summary pairs. Figure 2 shows an example assessment of a summary of an article from Figure 1. Results from the full assessment are shown in Table 2, which shows the percentage of documents per system that were annotated as faithful or hallucinated (faithful $=100$ - hallucinated). The Appendix provides interannotator agreement of hallucinations as well as hallucinated span characteristics.</p>
<p>Extrinsic Hallucination due to Divergence between Source and Target. Our results con-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Hallucinated</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Faith.</th>
<th style="text-align: center;">+Fact.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">I $\cup$ E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PTGEN</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">TCONVS2S</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">26.9</td>
</tr>
<tr>
<td style="text-align: center;">TRANS2S</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">25.3</td>
</tr>
<tr>
<td style="text-align: center;">BERTS2S</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Intrinsic vs. Extrinsic Hallucinations. The numbers in "Hallucinated" columns show the percentage of summaries where at least one word was annotated by all three annotators as an intrinsic (I) or extrinsic (E) hallucination. When a summary is not marked with any hallucination, it is "faithful" (100 - I $\cup \mathrm{E}$ ), column "Faith.". The final " + Fact." column shows the total percentage of faithful and/or factual summaries, which includes all faithful summaries plus the percentage of non-faithful summaries annotated by all three annotators as factual. Higher numbers for faithful/factual and lower numbers for hallucinations are boldfaced.
firmed that the BBC gold summaries often have extrinsic hallucinations due to the dataset artifact that gold summaries are introductory sentences prefacing each article. It was not surprising that most models also had significant extrinsic hallucinations.</p>
<p>Intrinsic Hallucination is Also Common in Abstractive Summaries. Gold summaries can also display intrinsic hallucinations. For example, a news article could describe an event related to "Barack Obama" and "the office of the President of the United States" without inferring that "Obama is the President of the United States." A journalist with the knowledge of the event in the article could write a summary stating "President Obama."</p>
<p>However, the percentage of system summaries with intrinsic hallucination was much higher than in gold summaries ( $7.4 \%$ vs others). This phenomenon particularly revealed the models' tendency to misrepresent information in the document due to the lack of document-level understanding and inference. The copy mechanism in PTGEN is good at copying from the source (showing the least percentage of extrinsic hallucination of $63.3 \%$ ), but the mechanism lacks the inference capability and is prone to generate a summary that is not supported by the document ( $19.9 \%$ intrinsic hallucination). TRANS2S showed similar performance to PTGEN and ranked second worst. The BertS2S showed the least number of intrinsic hallucination ( $16.9 \%$ ) among all four abstractive systems.</p>
<p>Pretraining Improves Faithfulness. Hallucinations do not result from the artifacts in the training data only, but also due to model shortcomings. The PTGEN model with the copy mechanism (Gu et al., 2016; See et al., 2017) had the lowest extrinsic
hallucination (63.3\%), but BERTS2S reported the highest number of faithful summaries. It appears that BERT52S is overall most conservative among all four abstractive systems while getting closer to reference summaries in terms of ROUGE. The pretraining prepares BertS2S to be more aware of the domain of the document and less prone to language model vulnerabilities. Consequently, BertS2S is more confident in predicting tokens from the document than TranS2S, hence, improving faithfulness.</p>
<h3>5.3 Assessment of Factual Hallucinations.</h3>
<p>Hallucinations are not necessarily erroneous. In our second human assessment, we measured to what extent this is the case. Our annotators were presented a single-sentence summary with hallucinations and were asked to assess whether it is true or false. To better explain the context of the summary, annotators were made available the source document as well as the external resources such as Wikipedia or Google Search. The source document can be particularly important for generic summaries to better understand context. External resources assisted the evaluators to validate grounded facts in public knowledge bases.</p>
<p>Annotators were expected to validate the summary by looking for supporting evidence for the information found on the summary. If information in the summary contradicts the document, then the summary is not factual. If supporting evidence is found for all the information, then the summary is factual. The document is not useful when the summary has information that is neither supported nor contradicted in the article. For example, the summary in Figure 2 mentions "Conservative MP Zac Goldwin" which can not be verified from the article in Figure 1. Here, annotators could use Wikipedia or Google Search to check that there had not been a Conservative MP named Zac Goldwin who tried to change their party and become a Labour's candidate in the 2016 London mayoral election.</p>
<p>We dropped the human authored gold summaries from this evaluation; they were presumably factual. We also dropped the abstracts that were faithful to their input documents from the previous study. Finally, there were 1869 document-summary pairs where the summaries were marked with at least one intrinsic or extrinsic hallucination. We elicited judgments from three different annotators for each of them. Results from this assessment are also presented in Table 2 (see the column labelled " + Fact.") along with the hallucination assessment.</p>
<p>Pretraining Helps Generating Factual Summaries. In total, $34.7 \%$ of the BERTS2S abstracts were faithful ( $26.9 \%$ ) and/or factual $(+7.8 \%)$. This is $7.4 \%$ absolute better than the next-best model (PtGEN). The number of unfaithful yet factual summaries for BERTS2S, 7.8\%, was also the highest. In fact, for extrinsic hallucinations, even though PtGEN hallucinates less than BERTS2S ( $63.3 \%$ vs. $64.1 \%$ ), $6.6 \%$ of BERTS2S hallucinations were factual, compared to $2.2 \%$ of PtGen. ${ }^{5}$ Thus, if we consider factual hallucinations to be valid, this means that even for extrinsic cases, BERTS2S hallucinates the least.</p>
<p>The superior performance of BERTS2S is most likely due to its exposure to vast amount of text through pretraining, allowing it to integrate background knowledge with generation. Even so, over $90 \%$ of BERTS2S hallucinations are erroneous.</p>
<p>Finally, we carried out pairwise comparisons between all models (using a one-way ANOVA with post-hoc Tukey HSD tests; $p&lt;0.01$ ). For intrinsic hallucinations (the second column in Table 2), Gold is significantly different from all other systems. For extrinsic hallucinations (the third column in Table 2), there were significant differences between PtGen and TConvS2S, PtGen and Gold, and, BertS2S and Gold. For factuality, the differences between PtGen, TConvS2S, and TranS2S were insignificant.</p>
<h3>5.4 Automatic Measures for Hallucinations</h3>
<p>Summaries are a proxy for their source documents under the assumption that they highlight the most important content. With this assumption, we further studied the extent to which the hallucinated content can be measured by semantic inference related measures, such as textual entailment and question answering.</p>
<p>Textual Entailment. We trained an entailment classifier by finetuning a BERT-Large pretrained model (Devlin et al., 2019) on the Multi-NLI dataset (Williams et al., 2018). We calculated the entailment probability score between the document and its abstractive summaries. Note that this entailment classifier is not optimal for the BBC article-summary pairs; the Multi-NLI dataset contains sentence-sentence pairs.</p>
<p>Ideally a summary should entail the document or perhaps be neutral to the document, but never contradict the document. As can be seen in Table 3, the BERTS2S abstracts showed the least number of</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Textual entailment and question answering (QA) based measures for summary evaluation. For entailment, we show the percentage of times a summary entails (entail.) the document, is neutral (neut.) to the document and contradicts (cont.) the document. For QA, we report the percentage of questions that were correctly answered by a system. The highest numbers for entail., neut. and QA, and the lowest number for cont. are boldfaced.
contradictions compared to other system-generated abstracts and was at par with the Gold summaries. Similar to the performance on extrinsic hallucination in Table 2, the TConvS2S abstracts also displayed the highest number of contradictions. Interestingly, the Gold summaries are more neutral to their documents, whereas the BERTS2S summaries are more entailed by their documents. This is probably due to the nature of the data and that journalists tend to add color and have a high number of extrinsic (but valid) hallucinations.</p>
<p>Question Answering. QA frameworks have been used to assess or promote summary informativeness (Narayan et al., 2018b; Arumae and Liu, 2019). We adapted the QA framework to assess hallucination in model generated summaries; a faithful model will generate a summary that only has information that is supported by its document. Under this assumption, any question answerable by the summary should also be answerable by the source.</p>
<p>Given an abstractive summary, we used the round-trip consistency method of Alberti et al. (2019), which combines question generation and answer extraction models to generate synthetic question-answer pairs. For the 500 documentsummary pairs, we generated 731, 708, 720, 725 and 820 question-answer pairs for PtGEN, TConvS2S, TranS2S, BERTS2S and Gold, respectively. Finally, we used a machine reading comprehension model to answer these questions using the document as context. As in Alberti et al. (2019), we trained all models: question generation, answer extraction and reading comprehension models; using a BERT-Base pretrained model (Devlin et al., 2019) finetuned on the Natural Questions dataset (Kwiatkowski et al., 2019).</p>
<p>Similar to textual entailment results, the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">PTGEN</th>
<th style="text-align: left;">Leeds United fought back from 2-0 down <br> to beat Huddersfield town in the first round <br> of the EFL cup. (Q: What team did Leeds <br> United beat in the first round of the EFL cup?, <br> A: Huddersfield town)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TCONVS2S</td>
<td style="text-align: left;">A coal mine in South Yorkshire has collapsed <br> as a result of the loss of a coal mine. (Q: <br> What type of mine has collapsed?, A: Coal)</td>
</tr>
<tr>
<td style="text-align: left;">TRANS2S</td>
<td style="text-align: left;">Star Wars actor James Davis said he was <br> "locked in a caravan" and had his caravan <br> stolen during a break-in. (Q: Who said he <br> was locked in a caravan?, A: Davis)</td>
</tr>
</tbody>
</table>
<p>Figure 3: Sample of question-answer pairs generated from hallucinated summaries that are correctly answered by their source articles. Highlighted spans in the summaries are marked as extrinsic or intrinsic hallucinations by our annotators.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Faithful</th>
<th style="text-align: center;">Factual</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.125</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.095</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.113</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.116</td>
</tr>
<tr>
<td style="text-align: left;">QA</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.027</td>
</tr>
<tr>
<td style="text-align: left;">Entailment</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 4}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Spearman's correlation coefficient $\left(\left|r_{s}\right|\right)$ of different metrics with faithful and factual annotations.</p>
<p>BERT52S abstracts were the most faithful to their source documents in terms of question answering. The Gold abstracts were the least accurate due to a high number of extrinsic hallucination in them.</p>
<p>Spearman's Correlation. We estimate Spearman's correlation coefficients of different metrics with the faithful and factual human scores (see Table 4). We found that the textual entailment scores are best correlated with both faithful (moderate, $0.40 \leq\left|r_{s}\right| \leq 0.59$ ) and factual (weak, $0.20 \leq\left|r_{s}\right| \leq 0.39$ ) human scores. Comparatively, ROUGE-based metrics and BERTScore have very weak correlation, our findings are consistent with the recent studies (Goodrich et al., 2019; Kryscinski et al., 2019a; Wang et al., 2020). Surprisingly, the question answering scores showed a very weak correlation $\left(0.0 \leq\left|r_{s}\right| \leq 0.19\right)$ with faithful and factual human scores. We hypothesize that this is due to a compounding of errors where (i) the question generator is used to generate questions from the systems' generated abstracts, instead of human-written text on which they were trained, (ii) the question generator is susceptible to generate questions with hallucinated content when fed in with hallucinated summaries, and (iii) our assumption that a summary is faithful if the answers from the source and the summary match, is rather poor for extreme summarization. We demonstrate these issues in Figure 3. Irrespective of questions with hallucinated content, our reading comprehension</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">R1</th>
<th style="text-align: center;">R2</th>
<th style="text-align: center;">RL</th>
<th style="text-align: center;">Faith.</th>
<th style="text-align: center;">*Fact.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERTS2S</td>
<td style="text-align: center;">$\mathbf{3 8 . 4 2}$</td>
<td style="text-align: center;">$\mathbf{1 6 . 9 6}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 2 7}$</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: left;">ENTAIL</td>
<td style="text-align: center;">35.93</td>
<td style="text-align: center;">14.02</td>
<td style="text-align: center;">28.87</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$ FAITH</td>
<td style="text-align: center;">37.31</td>
<td style="text-align: center;">15.21</td>
<td style="text-align: center;">30.12</td>
<td style="text-align: center;">$\mathbf{3 1 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 5: ROUGE and faithfulness/factuality scores for BERT52S plus systems that use textual entailment as a criteria or fine-tuned on faithful annotations.
model can fortuitously answer them correctly from their source articles. Better ways of generating questions (Narayan et al., 2020) and measuring factual consistency may alleviate some of these issues (Wang et al., 2020).</p>
<h3>5.5 Model Selection with Entailment</h3>
<p>Our study suggests that entailment could be used as an automatic measure for faithfulness. However, we should point out that this measure is referencia less. Thus, it can easily be gamed, i.e., the first sentence of any source document is always entailed by the whole document. Because of this, entailmentbased measures for evaluation need to be coupled with reference-based measures like ROUGE.</p>
<p>However, one major advantage of the measure being reference-less is that we can use it as a model selection objective or during decoding. We tested the former. Specifically, we used the probability that a summary is entailed by a document as a selection criteria to select a summary between four candidates generated by systems evaluated: PTGEN, TCONVS2S, TRANS2S, and BERTS2S. Results are shown in the ENTAIL row of Table 5. We can see that indeed this is a strong metric to optimize towards if we want faithful summaries - almost 5\% absolute better. There is a trade-off in terms of ROUGE, but this model must select amongst 4 systems, 3 of which have significantly lower ROUGE than the best model.</p>
<p>A further experiment is to train a model explicitly to predict faithfulness. In order to do this, we further fine-tuned the entailment model using the 'faithful' annotations generated during our evaluation. For all summary-document pairs marked as 'faithful', we set the associated class to 'entailment', otherwise we set it to 'neutral'. This allowed for us to also fine-tune the last classification layers taking advantage of the correlation between 'entailment' and 'faithfulness'. Results using 5-fold cross validation are shown in the ENTAIL $\rightarrow$ FAITH row of Table 5. We see here that indeed this does improve the ability to select faithful summaries from a set of candidates, though slightly. We would expect to see larger gains with more training data. However, this model is significantly better than ENTAIL</p>
<p>on ROUGE-based metrics and seems like a good balance between ROUGE and better faithfulness.</p>
<h2>6 Related Work</h2>
<p>Following the Document Understanding Conference (DUC; Dang, 2005), a majority of work has focused on evaluating the content and the linguistic quality of summaries (Nenkova, 2005). Most popular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for fluency. ROUGE, however, can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjective human assessment of summaries. More objective measures have been proposed to improve agreement among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires summaries to be annotated by experts for salient information. Narayan et al. (2018a,b) used a questionanswering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with its pertinent content.</p>
<p>There has not been much work on evaluating faithfulness and truthfulness of abstractive summaries. The automatic evaluation such as ROUGE and the human evaluation of saliency and linguistic quality of summaries are not sufficient due to the complex nature of the task. Recently, Chen and Bansal (2018) asked human annotators to assess the summary relevance measuring both the saliency and the presence of contradictory/unrelated information. Dhingra et al. (2019) proposed a new automatic metric, PARENT, for data-to-text generation (Lebret et al., 2016; Wiseman et al., 2017) which aligns $n$-grams from the reference and generated texts to the source table to measure the accuracy of $n$-grams that are entailed from the source table. Goodrich et al. (2019) proposed a modelbased automatic metric to assess the faithfulness of Wikipedia summaries; they trained an end-to-end model to extract a complete set of OpenIE-style (Banko et al., 2007) facts from both the source text and the generated summary. The summary is faithful if it is precise in generating facts from the source text. In our experiments with OpenIEbased measures, we found that they are not suited
for evaluating extreme summarization models; all models perform poorly on these metrics without any significant differences. Like ours, few recent works (some in parallel) have explored natural language inference and question answering models to detect factual consistency in generated text (Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b; Wang et al., 2020). In line with our findings, Falke et al. (2019) observed that the BERT-based NLI models substantially improved summaries reranking in terms of their correctness. Kryscinski et al. (2019b) proposed an NLI-based fact checking model that is trained on a dataset tailored for detecting factual inconsistencies in generated text. Wang et al. (2020) proposed a question answering and generation based automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. Future work will likely investigate better ways of generating questions and measuring factual consistency to address poor correlation with faithfulness and factuality annotations.</p>
<p>Finally, others have used reinforcement learning to improve informativeness and reduce contradictory information in abstractive summaries, e.g., Pasunuru and Bansal (2018) used a textual entailment-based reward and Arumae and Liu (2019), a question-answering based reward. However, these approaches don't evaluate if these rewards improve faithfulness of summaries.</p>
<h2>7 Conclusion</h2>
<p>We conducted a large-scale study of hallucinations in abstractive document summarization. We found that (i) tackling hallucination is a critical challenge for abstractive summarization, perhaps the most critical, (ii) NLU-driven pretraining in neural text generators is key to generate informative, coherent, faithful and factual abstracts, but it is still far from solving the problem; and (iii) measures such as ROUGE or BERTScore will not be sufficient when studying the problem; semantic inference-based automatic measures are better representations of true summarization quality.</p>
<h2>Acknowledgments</h2>
<p>We thank Ratish Puduppully, Yova Kementchedjhieva, Ankur Parikh, Peter Liu, Slav Petrov, the reviewers and the action editor for invaluable feedback. The hard work of Muqthar Mohammad, Mohd Majeed and Ashwin Kakarla made our human annotation possible.</p>
<h2>References</h2>
<p>Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic QA corpora generation with roundtrip consistency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 61686173, Florence, Italy.</p>
<p>Kristjan Arumae and Fei Liu. 2019. Guiding extractive summarization with question-answering rewards. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2566-2577, Minneapolis, Minnesota.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, San Diego, CA, USA.</p>
<p>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, pages 2670-2676, Hyderabad, India.</p>
<p>Ann L. Brown and Jeanne D. Day. 1983. Macrorules for summarizing texts: The development of expertise. Journal of Verbal Learning and Verbal Behaviour, 22(1):1-14.</p>
<p>Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 675-686, Melbourne, Australia.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy.</p>
<p>Hoa Trang Dang. 2005. Overview of DUC 2005. In Proceedings of the Document Understanding Conference, pages 1-12.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186, Minneapolis, Minnesota.</p>
<p>Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4884-4895, Florence, Italy.</p>
<p>Teun A. van Dijk and Walter Kintsch. 1978. Cognitive psychology and discourse: Recalling and summarizing stories. In Wolfgang U. Dressler, editor, Current Trends in Textlinguistics, pages 61-80.</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems 32, pages 13063-13075. Curran Associates, Inc.</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy.</p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 1243—1252, Sydney, NSW, Australia.</p>
<p>Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098-4109, Brussels, Belgium.</p>
<p>Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 166-175, New York, NY, USA.</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1631-1640, Berlin, Germany.</p>
<p>Hardy, Shashi Narayan, and Andreas Vlachos. 2019. HighRES: Highlight-based reference-less evaluation of summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381-3392, Florence, Italy.</p>
<p>Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28, pages 1693-1701. Curran Associates, Inc.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proceedings of the 8th International Conference on Learning Representations, Virtual Conference, Formerly Addis Ababa Ethiopia.</p>
<p>Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, and Lukasz Kaiser. 2019. Sample efficient text summarization using a single pre-trained transformer. CoRR, abs/1905.08836.</p>
<p>Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019a. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 540-551, Hong Kong, China.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2019b. Evaluating the factual consistency of abstractive text summarization. CoRR, abs/1910.12840.</p>
<p>Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. CoRR, abs/1808.06226.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.
J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159-174.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas.</p>
<p>Chin Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 150-157.</p>
<p>Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. In Proceedings of the 6th International Conference on Learning Representations, Vancouver Canada.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Inderjeet Mani. 2001. Automatic summarization, volume 3. John Benjamins Publishing.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018a. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018b. Ranking sentences for extractive summarization with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1747-1759, New Orleans, Louisiana.</p>
<p>Shashi Narayan, Gonçalo Simoes, Ji Ma, Hannah Craighead, and Ryan T. McDonald. 2020. QURIOUS: Question generation pretraining for text generation. CoRR, abs/2004.11026.</p>
<p>Ani Nenkova. 2005. Automatic Text Summarization of Newswire: Lessons Learned from the Document Understanding Conference. In Proceedings of the 20th National Conference on Artificial Intelligence Volume 3, pages 1436-1441.</p>
<p>Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information Retrieval, 5(2-3):103-233.</p>
<p>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The Pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 145-152, Boston, Massachusetts, USA.</p>
<p>Timothy Niven and Hung-Yu Kao. 2019. Probing neural network comprehension of natural language arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy.</p>
<p>Ramakanth Pasunuru and Mohit Bansal. 2018. Multireward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 646-653, New Orleans, Louisiana.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In Proceedings of the 6th International Conference on Learning Representations, Vancouver, BC, Canada.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Technical report.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Technical report.</p>
<p>Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for sequence generation tasks. To appear in Transactions of the Association for Computational Linguistics, abs/1907.12461.</p>
<p>Evan Sandhaus. 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia, $6(12)$.</p>
<p>Natalie Schluter. 2017. The limits of automatic summarisation according to rouge. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 4145, Valencia, Spain.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1073-1083, Vancouver, Canada.</p>
<p>Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher D. Manning. 2019. Do massively pretrained language models make better storytellers? In Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 843-861, Hong Kong, China.</p>
<p>Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2019. MASS: Masked sequence to sequence pre-training for language generation. In Proceedings of the 36th International Conference on Machine Learning, Long Beach, California.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27, pages 3104-3112. Curran Associates, Inc.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30, pages 5998-6008. Curran Associates, Inc.</p>
<p>Alex Wang, Kyunghyun Cho, and Michael Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Virtual Conference, Formerly Seattle, USA.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In Proceedings of the 8th International Conference on Learning Representations, Virtual Conference, Formerly Addis Ababa Ethiopia.</p>
<p>Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. 2019. Dialogue natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731-3741, Florence, Italy.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1112-1122, New Orleans, Louisiana.</p>
<p>Sam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. CoRR, abs/1906.08237.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In Proceedings of the 8th International Conference on Learning Representations, Virtual Conference, Formerly Addis Ababa Ethiopia.</p>
<h2>A Model Hyperparameters and Predictions</h2>
<p>PtGen and TConvS2S model predictions are provided by Narayan et al. (2018a) and Transformer model predictions from GPT-Tuned, Trans2S and BertS2S, by Rothe et al. (2020). Both PtGen and TConvS2S use a Stanford tokenized vocabulary size of 50k. Trans2S and BertS2S use a vocabulary size of around $\sim 30 \mathrm{k}$ WordPieces (Wu et al., 2016) to match BERT pretrained vocabulary and, GPT-TUNED, a vocabulary size of around $\sim 50 \mathrm{k}$ SentencePieces (Kudo and Richardson, 2018) to match the GPT-2 pretrained vocabulary. All models use the same uncased vocabulary on both source and target sides. Both PtGen and TConvS2S summaries were generated using beam search with beam size 10, the Transformer models use beam size of 4. See Narayan et al. (2018a) and Rothe et al. (2020) for more details on these models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Fleiss' Kappa</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Hall.</td>
<td style="text-align: center;">Fact.</td>
<td style="text-align: center;">Rept.</td>
<td style="text-align: center;">Inco.</td>
</tr>
<tr>
<td style="text-align: left;">PtGen</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: left;">TConvS2S</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">Trans2S</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">BertS2S</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">Gold</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.98</td>
</tr>
</tbody>
</table>
<p>Table 6: Fleiss's Kappa scores measuring word-level agreements among annotators for different annotation tasks: hallucination (Hall.), factuality (Fact.), repetition (Rept.) and incoherence (Inco.) assessments.</p>
<h2>B Inter annotator agreement</h2>
<p>We estimated Fleiss's $\operatorname{Kappa}(k)$ to assess the agreement among our raters when categorizing a word in the summary as one of faithful, intrinsically hallucinated and extrinsically hallucinated. The results are shown in Table 6. All models showed substantial agreement $(0.61 \leq k \leq 0.80$; Landis and Koch, 1977) among their annotations.</p>
<p>Table 6 also shows Fleiss's Kappa ( $k$ ) to assess the agreement among our raters for factuality. All models showed almost perfect agreement ( 0.81 $\leq k \leq 1.0$; Landis and Koch, 1977) among their annotations.</p>
<h2>C Highlighted Span Characteristics</h2>
<p>Results in Table 7 shed some light on the characteristics of hallucinated spans observed in different abstracts. Gold abstracts showed the least number of intrinsically hallucinated spans ( 0.55 per document), whereas, PtGEN abstracts showed the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Intrinsic <br> total (avg.)</th>
<th style="text-align: center;">Extrinsic <br> total (avg.)</th>
<th style="text-align: center;">avg. <br> length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PtGEN</td>
<td style="text-align: center;">$625(1.35)$</td>
<td style="text-align: center;">$\mathbf{1 4 2 4 ( 2 . 8 5 )}$</td>
<td style="text-align: center;">8.48</td>
</tr>
<tr>
<td style="text-align: left;">TConvS2S</td>
<td style="text-align: center;">$518(1.04)$</td>
<td style="text-align: center;">$1556(3.11)$</td>
<td style="text-align: center;">8.44</td>
</tr>
<tr>
<td style="text-align: left;">Trans2S</td>
<td style="text-align: center;">$589(1.18)$</td>
<td style="text-align: center;">$1556(3.11)$</td>
<td style="text-align: center;">7.39</td>
</tr>
<tr>
<td style="text-align: left;">BertS2S</td>
<td style="text-align: center;">$530(1.06)$</td>
<td style="text-align: center;">$1520(3.04)$</td>
<td style="text-align: center;">$\mathbf{6 . 1 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Gold</td>
<td style="text-align: center;">$\mathbf{2 7 6 ( 0 . 5 5 )}$</td>
<td style="text-align: center;">$1807(3.61)$</td>
<td style="text-align: center;">7.11</td>
</tr>
</tbody>
</table>
<p>Table 7: Total number of spans and the average number of spans per document, annotated as intrinsic or extrinsic hallucinations for all 500 document-summary pairs by three annotators. We also show the average span length for each system.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Repetition</th>
<th style="text-align: center;">Incoherence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PtGen</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">20.3</td>
</tr>
<tr>
<td style="text-align: left;">TConvS2S</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">17.7</td>
</tr>
<tr>
<td style="text-align: left;">Trans2S</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: left;">BertS2S</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">9.5</td>
</tr>
<tr>
<td style="text-align: left;">Gold</td>
<td style="text-align: center;">$\mathbf{0 . 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Repetition and Incoherence Evaluation. The numbers show the the percentage of 500 summaries where at least one word in a summary was annotated by all three annotators with the "Repetition" or "Incoherence" related issue. The lowest numbers are boldfaced.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Faithful</th>
<th style="text-align: center;">Factual</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.125</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.095</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.113</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.116</td>
</tr>
<tr>
<td style="text-align: left;">Repetition</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.075</td>
</tr>
<tr>
<td style="text-align: left;">Incoherence</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: left;">QA</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.027</td>
</tr>
<tr>
<td style="text-align: left;">Entailment</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 4}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Spearman's correlation coefficient $\left(\left|r_{s}\right|\right)$ of different metrics with faithful and factual annotations.
least number of extrinsically hallucinated spans ( 2.85 per document). Interestingly, the average span length for PtGEN summaries was 8.48 words, much higher than 6.12 words for BERTS2S summaries. Our result demonstrates that (i) the effect of hallucination in BERTS2S is more local than what we observe in PtGEN and (ii) despite a lower number of extrinsically hallucinated spans or documents in PtGEN compared to that in BERTS2S ( 2.85 vs 3.04 spans per document, $63.3 \%$ vs $64.1 \%$ documents), the total number of words that were annotated as extrinsic hallucination is much higher in PtGEN than in BERTS2S ( 12075 vs 9302 words).</p>
<h2>D Assessment of Linguistic Irregularities.</h2>
<p>Following standard practice in summarization, all 2500 document-summary pairs were annotated for repetition and incoherence related linguistic irregularities. Annotators were presented only a singlesentence summary and were asked to identify all</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Faithful</th>
<th style="text-align: center;">Hallucinated</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">total</td>
<td style="text-align: center;">factual</td>
<td style="text-align: center;">$\begin{gathered} \text { E } \ \text { total } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { E } \ \text { factual } \end{gathered}$</td>
<td style="text-align: center;">total</td>
<td style="text-align: center;">$\begin{gathered} \text { E } \ \text { factual } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { Factual } \ \hline \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">PTGEN</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">TConvS2S</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">26.9</td>
</tr>
<tr>
<td style="text-align: center;">TRANS2S</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">25.3</td>
</tr>
<tr>
<td style="text-align: center;">BERTS2S</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 10: Intrinsic vs Extrinsic Hallucinations and their factuality. The numbers in "Hallucinated" columns show the percentage of summaries out of 500 where at least one word was annotated by all three annotators as an intrinsic (I) or extrinsic (E) hallucination. When a summary is not marked with any hallucination, it is "faithful" (1- IUE). The "factual" columns within the "Hallucinated" column show for each type (I, E and IUE), the percentage of summaries out of 500 annotated by all three annotators as factual. The final "Factual" column shows the total percentage of factual summaries (Faithful $+\mathrm{I} \cup \mathrm{E}_{\text {factual }}$ ). The highest numbers for faithful and factual, and the lowest numbers for hallucinations are boldfaced.
spans of text in the summary that were either repeated or made the summary incoherent. We again elicited judgments from three different annotators for each document-summary pair. Results are shown in Table 8.</p>
<p>Overall, all neural text generation systems are getting better in generating repetition-free and coherent single-sentence summaries of news articles. Transformer-based models, TRANS2S and BERTS2S in particular, perform superior to RNNbased PTGEN and CNN-based TCONVS2S models. Nonetheless, Table 9 shows that these metrics fail to correlate with faithful, hallucinated and factual assessments of summaries. Fleiss's Kappa ( $k$ ) values for repetition and incoherence assessments showed almost a perfect agreement $(0.81 \leq k \leq$ 1.0; Landis and Koch, 1977) among our raters (see Table 6).</p>
<h1>E Full Hallucination Results</h1>
<p>Table 10 has the full results from our human study of hallucinations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ See Appendix for full results.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
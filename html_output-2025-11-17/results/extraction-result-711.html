<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-711 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-711</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-711</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-7f7391217b2fdc843aeaa13a68cbc440aa35d190</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7f7391217b2fdc843aeaa13a68cbc440aa35d190" target="_blank">A Step Toward Quantifying Independently Reproducible Machine Learning Research</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Man manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results.</p>
                <p><strong>Paper Abstract:</strong> What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e711.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e711.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing or Incomplete Hyper-parameter Specification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers that do not report final hyper-parameter values (or only partially report them), preventing independent implementers from matching experimental setups and harming reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (manual implementation of published algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers attempted to implement paper algorithms from the methods descriptions alone (no author code) and recorded paper features and reproduction outcomes for 255 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>independently written implementation (reproducer code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors omitted full hyper-parameter choices (or only gave partial ranges/selection procedures), forcing reproducers to search or guess values; these unspecified 'minor' hyper-parameters often had large impacts on measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters (training & evaluation configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility study: manual implementation attempts flagged failures; paper features annotated and presence/absence of hyper-parameter specifications recorded; statistical association with reproduction success computed (chi-squared).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Categorical coding of 'Hyper-parameters Specified' (No / Yes / Partial) across 255 papers; statistical significance tested by chi-squared (Table 1 p = 8.45e-6). Frequency counts from Table 17 used to show prevalence (e.g., Hyperparams=Yes: 192/255 total; reproduced & hyperparams=yes: 138/162 ≈ 85.2%; non-reproduced & hyperparams=no: 34/93 ≈ 36.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: papers lacking hyper-parameter specification were much less likely to be independently reproduced; presence of hyper-parameter detail correlated with higher reproduction rate (138/162 reproduced papers specified hyperparameters vs 21/162 reproduced papers that did not). Paper-level outcome: 93/255 failed reproduction overall, and missing hyperparameters was a common contributor.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Hyper-parameters fully specified in ~192/255 (≈75.3%) of papers; among reproduced papers 138/162 (≈85.2%) had hyperparameters specified; among non-reproduced 54/93 (≈58.1%) had them specified. Missing hyperparameters (No) occurred in 55/255 (~21.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implementation details omitted from natural language description; assumed implicit defaults; space/page limits or belief that hyperparameters are unimportant to report.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly report final hyper-parameter values per dataset and selection methods (e.g., exact grid/search seeds); include hyperparameter tables in paper or appendix; release configuration files with code or in README.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Strong evidence of effectiveness: papers that specified hyper-parameters had much higher reproduction rates (138/162 reproduced papers had hyperparameters specified), and chi-squared significance indicates specification strongly associated with success. Quantified effect: presence of hyperparameter specification correlates with ~+?% absolute higher reproduction rate (85.2% vs 58.1% when comparing reproduced vs non-reproduced groups' hyperparam presence).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e711.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e711.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>insufficient_pseudocode</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Insufficient or Ambiguous Pseudo-code (Step-Code vs Code-Like)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper pseudo-code that is terse, split across sections ('Step-Code'), or otherwise incomplete leads to misinterpretation and extra cross-referencing, reducing reproducibility; conversely, 'code-like' detailed pseudocode increases reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (manual implementation of published algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers attempted to implement algorithms using only paper text; pseudo-code style was annotated and correlated with reproduction outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithm specification / pseudo-code in methods</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reproducer-written implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous algorithm steps (pseudo-code quality)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Four pseudo-code categories were tracked: No pseudo-code, Step-Code (high-level step list requiring cross-references), Yes (high-level but mostly complete), and Code-Like (detailed code-style description). 'Step-Code' often required jumping between sections, omitted low-level details, and associated with lower readability and reproducibility; Code-Like pseudo-code correlated with higher success.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>algorithm description / implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual annotation of pseudo-code type for each paper and cross-tabulation with reproduction outcomes; chi-squared test (Table 13 p = 2.308e-4) and contingency analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counts by pseudo-code category from Table 13: totals (No:79, Step-Code:57, Yes:102, Code-Like:17); reproduction splits: e.g., of Code-Like 14/17 reproduced (~82%), Step-Code had larger share of non-reproduced (34 non-reproduced vs 23 reproduced). Statistical significance measured by chi-squared.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Meaningful: Code-Like pseudo-code strongly associated with higher reproduction rates; Step-Code biased toward lower readability and lower reproduction. Example numbers: Code-Like reproduced 14/17 (≈82%); Step-Code reproduced 23/57 (≈40%), non-reproduced 34/57 (≈60%).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Most common category was 'Yes' (high-level mostly complete) 102/255 (~40%); Step-Code occurred in 57/255 (~22%); Code-Like rare (17/255 ~6.7%); 'No' pseudo-code present in 79/255 (~31%).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors provide high-level algorithm narratives or step-lists without low-level operational detail; implicit assumptions about implementer knowledge; space constraints lead to terse step-code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide detailed, code-like pseudocode or algorithm listings; include runnable snippets or configuration files; have appendices or supplementary material with full algorithmic details.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: 'Code-Like' pseudo-code associated with markedly higher reproduction rate (≈82% success); statistical tests indicate pseudo-code quality is a significant predictor of success (p=2.308e-4).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e711.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e711.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>unclear_notation_and_missing_steps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unclear Notation, Language, and Omitted Algorithm Steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ambiguous notation, unclear prose, or entirely missing algorithmic steps in the paper's natural language description that prevent reliable independent implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (manual implementation of published algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers read and re-read papers to implement methods; readability and subjective reasons for failure were recorded and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / mathematical notation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reproducer-written implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / incomplete specification / missing algorithm step</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Reproducers reported ambiguous notation or prose and cases where algorithmic steps were not described at all (omitted). These led to incorrect interpretations or incomplete implementations and were a common subjective reason for failure to reproduce (enumerated in Section 5).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>algorithm description / mathematical derivations / implementation steps</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Subjective reporting from reproducers during manual implementation attempts; correlation with 'Readability' scores and reproduction outcomes; statistical tests showed Readability strongly predicts reproducibility (p = 9.68e-25).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Readability scored as Low/Ok/Good/Excellent based on number of reads required to implement; contingency tables (Table 11) and statistical tests quantify the association: 'Excellent' papers always reproduced, 'Low' heavily biased towards failure.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>High: unreadable/ambiguous papers had much lower reproduction: e.g., 'Low' Readability count among non-reproduced = 61 vs reproduced = 14 (Table 11). Overall, unreadability was the single strongest empirical predictor of reproduction outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Readability distribution shows many papers scored 'Low' or 'Ok'; of 255 papers, 'Low' Readability had 75 total (61 non-reproduced + 14 reproduced).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous natural language and notation; insufficiently detailed exposition; possibly exacerbated by page limits and stylistic choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Improve exposition, include examples, add implementation-level detail, increase page limits or put detailed algorithmic steps into appendices/supplementary materials, provide runnable code/configs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Implied effective: 'Excellent' readability correlated with 100% reproduction in this dataset; authors replying and detailed pseudocode also improved readability and success.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e711.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e711.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>missing_gradients</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing Derivative/Gradient Details for Losses and Updates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers specifying loss functions but not the resulting gradients or update rules, forcing reproducers to re-derive nontrivial math and often causing mismatched results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (manual implementation of published algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers implemented algorithms, including deriving gradients needed for training when not provided by the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>mathematical specification of loss and optimization</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reproducer-written training code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing/derivation details / omitted gradients</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many papers defined loss functions or equations but omitted the analytic gradients (or specific discretization/implementation choices), so reproducers had to re-derive them; for complex expressions this re-derivation was non-trivial and led to mismatches in behavior and failure to reproduce.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / mathematical derivation / optimization updates</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual implementation attempts where reproducers discovered that gradients were necessary but not provided; cited subjectively in Section 5 as one of primary reasons for failed reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative enumeration in Section 5 (subjective recall). No formal count provided, but mentioned as a recurring cause; overall reproducibility statistics (93 failures) include many affected by missing gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Could cause complete failure to reproduce training outcomes or significantly different learned models; qualitative reports indicate this was a recurring and impactful issue, but no single quantitative delta provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as 'many papers' in subjective recall (Section 5) but not precisely counted; characterized as a common contributor to non-reproduction in the authors' experience.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors assume readers can or will re-derive gradients; space or style choices omit derivations; implicit mathematical steps left out.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide analytic gradients or update rules in the paper or appendix; include automatic-differentiation-friendly code snippets; include symbolic derivations in supplementary material.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not directly quantified in the paper, but given the association of detailed algorithmic explanation and pseudocode with higher reproducibility, providing gradients is expected to materially improve reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e711.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e711.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>compute_and_cluster_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compute Resource & Cluster Configuration Mismatch / Missing Compute Details</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers that require cluster/distributed compute or lack explicit compute/configuration details create barriers: cluster-specific configuration, interconnect, and scheduling details are often omitted and were associated with failed reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (manual implementation on available compute tiers)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproducers categorized papers by compute needed (Desktop, GPU, Server, Cluster) and attempted reproduction with available resources.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / compute specification in methods</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reproducer-written implementation and experiments</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing compute configuration details / resource mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often omitted exact compute specs or details on distributed setup; works requiring clusters were never reproduced by the authors despite access to cluster resources, suggesting omitted cluster-specific implementation/operational details (interconnects, scheduling, distributed code) impede reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>compute environment / distributed training / runtime configuration</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual annotation of 'Compute Needed' per paper and cross-tabulation with reproduction outcomes (Table 18); observed that Cluster-needed papers had 0 reproduced cases in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Contingency counts from Table 18: e.g., Cluster: non-reproduced count = 10, reproduced count = 0; statistical significance tested (Compute Needed p = 8.75e-5). Also 'Exact Compute Specified' categorical feature tested (not significant at p=0.257).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Severe for cluster-requiring works: none of the cluster-classified papers were reproduced in this study; GPU-needed works had higher reproduction rates. Overall, compute-resource issues contributed to non-reproducibility for higher-tier compute papers.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Cluster-needed papers were rare (10 in the non-reproduced group in Table 18) but had 0 successful reproductions; GPU-needed papers were reproduced more often (24 reproduced vs 5 non-reproduced in Table 18).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Omitted operational details of distributed setups; implicit assumptions about shared cluster infrastructures; lack of portable tooling for distributed runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Document exact compute environment (hardware, interconnect, software stack), provide distributed run scripts, containerized reproducible environments, or provide reduced-size experiments runnable on a single GPU/desktop.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in this paper, but authors hypothesize that reproducible tooling (e.g., repo2docker) and better documentation could improve reproduction; observed empirical pattern (GPU easier to reproduce) suggests tooling helps.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / distributed training</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e711.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e711.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>author_nonresponse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Author Non-Responsiveness to Questions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Failure of paper authors to reply to implementer queries is strongly associated with failed independent reproduction; author replies markedly improve reproduction success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (manual communication with authors during reproduction attempts)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors of 50 sampled papers were contacted for clarification during reproduction attempts; replies were recorded and correlated with success.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>author correspondence / clarifications to natural language description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reproducer-written implementation seeking clarifications</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing clarifications / unresolved ambiguities due to non-response</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When reproducers could not resolve ambiguities or missing details from the paper, contacting authors often resolved issues; non-response left unresolved gaps that prevented reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>any stage where natural language left ambiguities (hyperparameters, algorithm details, implementation choices)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical: authors contacted for 50 papers; reply/no-reply recorded; reproduction outcomes compared for replied vs non-replied groups (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Simple contact experiment: 50 papers contacted, reply rate 52% (26 replied). Reproduction success when author replied: 22/26 (≈84.6%); when no reply: 1/24 (≈4.2%). Chi-squared test highly significant (p = 6.016e-8).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Huge: author replies were the most individually predictive attribute for reproduction success; lack of reply strongly associated with failure (only 1 success among 24 unanswered cases).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>In the subsample of 50 contacted papers, 52% replied. The study did not attempt contacting all 255 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous/omitted details in paper and lack of accessible clarifications; authors may be unavailable or uninterested in answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage authors to respond to queries; embrace 'living' documents (arXiv updates, errata, interactive articles); provide contactable structured resources (issues on a code repo) and detailed documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Very high in this dataset: contacting authors and receiving replies was associated with ~80 percentage-point increase in reproduction success in the 50-paper subsample (84.6% vs 4.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e711.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e711.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code_availability_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch Between Code Release Presence and Independent Reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Availability of authors' code was not statistically associated with higher independent reproducibility in this study, indicating a misalignment between 'code released' and being independently reproducible from the paper alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (paper-only reproduction policy)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Study excluded papers where authors' code had been viewed prior to reproduction to ensure independence, but the presence/absence of author code (as a paper feature) was annotated and tested for correlation with reproduction success.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper statements about code availability / README</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>author-released code (presence/absence recorded) vs independent reimplementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>mismatch between documentation (paper claiming or providing code) and ability to reproduce without using that code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Whether authors released code had no significant relationship with independent reproduction success (Table 1 p = 0.213), suggesting that code release alone does not ensure that paper natural language descriptions are sufficient for independent reproduction, and in some cases may be associated with less detail in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>overall experimental reproducibility pipeline (documentation vs runnable artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Annotation of 'Code Available' as a paper feature and chi-squared testing against reproduction outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Contingency counts (Table 6): Author code available: reproduced 92 vs not reproduced 44; not available: reproduced 70 vs not reproduced 49. Chi-squared p = 0.184 (non-significant).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Null/complex: presence of code alone did not predict independent reproduction outcomes in this dataset; implies code release is insufficient to guarantee reproducibility when independent implementers do not consult it.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Author code was available for a substantial subset (136/255 inferred from Table 6 totals), but availability did not map cleanly to improved independent reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors may rely on code release instead of writing detailed natural language method descriptions; or code might be incomplete, undocumented, or require degrees of contextual knowledge to run, so paper's natural language remains insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Release well-documented, runnable code plus configuration and environment specs; do not treat code release as substitute for detailed paper-level descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not directly quantified here; paper notes that code release alone showed no correlation, so combined practices (code + detailed paper specs + hyperparameters + contactability) likely needed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e711.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e711.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>excess_equations_overformalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Excessive Equations / Overformalization Leading to Poor Readability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High density of equations per page correlated negatively with reproducibility, likely because excessive formalism harms readability and implicitness of implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Step Toward Quantifying Independently Reproducible Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent reproduction pipeline (manual reading and implementation from paper)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Papers were manually counted for number of equations normalized by pages; relationship to readability and reproduction success was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>mathematical exposition in methods / derivations</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reproducer-written implementation derived from math</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description due to overformalization / too many compact equations without prose or implementation guidance</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>A larger number of equations per page was negatively correlated with independent reproduction; the most readable and reproducible papers had fewer equations per page. Possible mechanisms: more equations reduce clarity, or indicate more complex algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>mathematical exposition -> mapping to implementation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual counting of equations normalized by pages; Mann-Whitney tests and Kruskal-Wallis ANOVA used to evaluate association with Readability and reproduction; significance reported (Number Equations p = 0.004).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Normalized equations-per-page statistics and grouping by Readability; example rates: 'Excellent' readability had 2.25 equations/page vs other groups ~3.6-3.9; statistical tests found readability hypothesis significant (p=0.001) while difficulty hypothesis not significant.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate: high equation density associated with lower readability and consequently lower reproduction success. Authors suggest careful and judicious use of equations improves reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Varies; 'Excellent' papers had fewer equations per page on average (2.25 eq/pg) compared to others (3.6-3.9 eq/pg).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Authors over-rely on dense formalism without implementation-oriented exposition; lack of bridging text and examples to map math to code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Complement formal derivations with explanatory prose, worked examples, pseudocode/code-like listings and highlight implementation choices; reduce unnecessary equation density or move derivations to appendices with clear guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Implied effective: papers with fewer equations per page tended to be more reproducible; statistical significance supports this association.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / theoretical exposition</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Toward Quantifying Independently Reproducible Machine Learning Research', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>State of the Art: Reproducibility in Artificial Intelligence <em>(Rating: 2)</em></li>
                <li>Replicability is not reproducibility: nor is it good science <em>(Rating: 2)</em></li>
                <li>Reproducible Research Environments with repo2docker <em>(Rating: 2)</em></li>
                <li>Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets <em>(Rating: 1)</em></li>
                <li>Winner's Curse? On Pace, Progress, and Empirical Rigor <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-711",
    "paper_id": "paper-7f7391217b2fdc843aeaa13a68cbc440aa35d190",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "missing_hyperparameters",
            "name_full": "Missing or Incomplete Hyper-parameter Specification",
            "brief_description": "Papers that do not report final hyper-parameter values (or only partially report them), preventing independent implementers from matching experimental setups and harming reproducibility.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (manual implementation of published algorithms)",
            "system_description": "Reproducers attempted to implement paper algorithms from the methods descriptions alone (no author code) and recorded paper features and reproduction outcomes for 255 papers.",
            "nl_description_type": "research paper methods section / experimental protocol",
            "code_implementation_type": "independently written implementation (reproducer code)",
            "gap_type": "hyperparameter mismatch / incomplete specification",
            "gap_description": "Authors omitted full hyper-parameter choices (or only gave partial ranges/selection procedures), forcing reproducers to search or guess values; these unspecified 'minor' hyper-parameters often had large impacts on measured performance.",
            "gap_location": "hyperparameters (training & evaluation configuration)",
            "detection_method": "reproducibility study: manual implementation attempts flagged failures; paper features annotated and presence/absence of hyper-parameter specifications recorded; statistical association with reproduction success computed (chi-squared).",
            "measurement_method": "Categorical coding of 'Hyper-parameters Specified' (No / Yes / Partial) across 255 papers; statistical significance tested by chi-squared (Table 1 p = 8.45e-6). Frequency counts from Table 17 used to show prevalence (e.g., Hyperparams=Yes: 192/255 total; reproduced & hyperparams=yes: 138/162 ≈ 85.2%; non-reproduced & hyperparams=no: 34/93 ≈ 36.6%).",
            "impact_on_results": "Substantial: papers lacking hyper-parameter specification were much less likely to be independently reproduced; presence of hyper-parameter detail correlated with higher reproduction rate (138/162 reproduced papers specified hyperparameters vs 21/162 reproduced papers that did not). Paper-level outcome: 93/255 failed reproduction overall, and missing hyperparameters was a common contributor.",
            "frequency_or_prevalence": "Hyper-parameters fully specified in ~192/255 (≈75.3%) of papers; among reproduced papers 138/162 (≈85.2%) had hyperparameters specified; among non-reproduced 54/93 (≈58.1%) had them specified. Missing hyperparameters (No) occurred in 55/255 (~21.6%).",
            "root_cause": "Implementation details omitted from natural language description; assumed implicit defaults; space/page limits or belief that hyperparameters are unimportant to report.",
            "mitigation_approach": "Explicitly report final hyper-parameter values per dataset and selection methods (e.g., exact grid/search seeds); include hyperparameter tables in paper or appendix; release configuration files with code or in README.",
            "mitigation_effectiveness": "Strong evidence of effectiveness: papers that specified hyper-parameters had much higher reproduction rates (138/162 reproduced papers had hyperparameters specified), and chi-squared significance indicates specification strongly associated with success. Quantified effect: presence of hyperparameter specification correlates with ~+?% absolute higher reproduction rate (85.2% vs 58.1% when comparing reproduced vs non-reproduced groups' hyperparam presence).",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e711.0",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "insufficient_pseudocode",
            "name_full": "Insufficient or Ambiguous Pseudo-code (Step-Code vs Code-Like)",
            "brief_description": "Paper pseudo-code that is terse, split across sections ('Step-Code'), or otherwise incomplete leads to misinterpretation and extra cross-referencing, reducing reproducibility; conversely, 'code-like' detailed pseudocode increases reproducibility.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (manual implementation of published algorithms)",
            "system_description": "Reproducers attempted to implement algorithms using only paper text; pseudo-code style was annotated and correlated with reproduction outcomes.",
            "nl_description_type": "algorithm specification / pseudo-code in methods",
            "code_implementation_type": "reproducer-written implementation",
            "gap_type": "incomplete specification / ambiguous algorithm steps (pseudo-code quality)",
            "gap_description": "Four pseudo-code categories were tracked: No pseudo-code, Step-Code (high-level step list requiring cross-references), Yes (high-level but mostly complete), and Code-Like (detailed code-style description). 'Step-Code' often required jumping between sections, omitted low-level details, and associated with lower readability and reproducibility; Code-Like pseudo-code correlated with higher success.",
            "gap_location": "algorithm description / implementation details",
            "detection_method": "Manual annotation of pseudo-code type for each paper and cross-tabulation with reproduction outcomes; chi-squared test (Table 13 p = 2.308e-4) and contingency analysis.",
            "measurement_method": "Counts by pseudo-code category from Table 13: totals (No:79, Step-Code:57, Yes:102, Code-Like:17); reproduction splits: e.g., of Code-Like 14/17 reproduced (~82%), Step-Code had larger share of non-reproduced (34 non-reproduced vs 23 reproduced). Statistical significance measured by chi-squared.",
            "impact_on_results": "Meaningful: Code-Like pseudo-code strongly associated with higher reproduction rates; Step-Code biased toward lower readability and lower reproduction. Example numbers: Code-Like reproduced 14/17 (≈82%); Step-Code reproduced 23/57 (≈40%), non-reproduced 34/57 (≈60%).",
            "frequency_or_prevalence": "Most common category was 'Yes' (high-level mostly complete) 102/255 (~40%); Step-Code occurred in 57/255 (~22%); Code-Like rare (17/255 ~6.7%); 'No' pseudo-code present in 79/255 (~31%).",
            "root_cause": "Authors provide high-level algorithm narratives or step-lists without low-level operational detail; implicit assumptions about implementer knowledge; space constraints lead to terse step-code.",
            "mitigation_approach": "Provide detailed, code-like pseudocode or algorithm listings; include runnable snippets or configuration files; have appendices or supplementary material with full algorithmic details.",
            "mitigation_effectiveness": "Effective: 'Code-Like' pseudo-code associated with markedly higher reproduction rate (≈82% success); statistical tests indicate pseudo-code quality is a significant predictor of success (p=2.308e-4).",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e711.1",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "unclear_notation_and_missing_steps",
            "name_full": "Unclear Notation, Language, and Omitted Algorithm Steps",
            "brief_description": "Ambiguous notation, unclear prose, or entirely missing algorithmic steps in the paper's natural language description that prevent reliable independent implementation.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (manual implementation of published algorithms)",
            "system_description": "Reproducers read and re-read papers to implement methods; readability and subjective reasons for failure were recorded and analyzed.",
            "nl_description_type": "research paper methods section / mathematical notation",
            "code_implementation_type": "reproducer-written implementation",
            "gap_type": "ambiguous description / incomplete specification / missing algorithm step",
            "gap_description": "Reproducers reported ambiguous notation or prose and cases where algorithmic steps were not described at all (omitted). These led to incorrect interpretations or incomplete implementations and were a common subjective reason for failure to reproduce (enumerated in Section 5).",
            "gap_location": "algorithm description / mathematical derivations / implementation steps",
            "detection_method": "Subjective reporting from reproducers during manual implementation attempts; correlation with 'Readability' scores and reproduction outcomes; statistical tests showed Readability strongly predicts reproducibility (p = 9.68e-25).",
            "measurement_method": "Readability scored as Low/Ok/Good/Excellent based on number of reads required to implement; contingency tables (Table 11) and statistical tests quantify the association: 'Excellent' papers always reproduced, 'Low' heavily biased towards failure.",
            "impact_on_results": "High: unreadable/ambiguous papers had much lower reproduction: e.g., 'Low' Readability count among non-reproduced = 61 vs reproduced = 14 (Table 11). Overall, unreadability was the single strongest empirical predictor of reproduction outcome.",
            "frequency_or_prevalence": "Readability distribution shows many papers scored 'Low' or 'Ok'; of 255 papers, 'Low' Readability had 75 total (61 non-reproduced + 14 reproduced).",
            "root_cause": "Ambiguous natural language and notation; insufficiently detailed exposition; possibly exacerbated by page limits and stylistic choices.",
            "mitigation_approach": "Improve exposition, include examples, add implementation-level detail, increase page limits or put detailed algorithmic steps into appendices/supplementary materials, provide runnable code/configs.",
            "mitigation_effectiveness": "Implied effective: 'Excellent' readability correlated with 100% reproduction in this dataset; authors replying and detailed pseudocode also improved readability and success.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e711.2",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "missing_gradients",
            "name_full": "Missing Derivative/Gradient Details for Losses and Updates",
            "brief_description": "Papers specifying loss functions but not the resulting gradients or update rules, forcing reproducers to re-derive nontrivial math and often causing mismatched results.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (manual implementation of published algorithms)",
            "system_description": "Reproducers implemented algorithms, including deriving gradients needed for training when not provided by the paper.",
            "nl_description_type": "mathematical specification of loss and optimization",
            "code_implementation_type": "reproducer-written training code",
            "gap_type": "missing preprocessing/derivation details / omitted gradients",
            "gap_description": "Many papers defined loss functions or equations but omitted the analytic gradients (or specific discretization/implementation choices), so reproducers had to re-derive them; for complex expressions this re-derivation was non-trivial and led to mismatches in behavior and failure to reproduce.",
            "gap_location": "training procedure / mathematical derivation / optimization updates",
            "detection_method": "Manual implementation attempts where reproducers discovered that gradients were necessary but not provided; cited subjectively in Section 5 as one of primary reasons for failed reproduction.",
            "measurement_method": "Qualitative enumeration in Section 5 (subjective recall). No formal count provided, but mentioned as a recurring cause; overall reproducibility statistics (93 failures) include many affected by missing gradients.",
            "impact_on_results": "Could cause complete failure to reproduce training outcomes or significantly different learned models; qualitative reports indicate this was a recurring and impactful issue, but no single quantitative delta provided in paper.",
            "frequency_or_prevalence": "Reported as 'many papers' in subjective recall (Section 5) but not precisely counted; characterized as a common contributor to non-reproduction in the authors' experience.",
            "root_cause": "Authors assume readers can or will re-derive gradients; space or style choices omit derivations; implicit mathematical steps left out.",
            "mitigation_approach": "Provide analytic gradients or update rules in the paper or appendix; include automatic-differentiation-friendly code snippets; include symbolic derivations in supplementary material.",
            "mitigation_effectiveness": "Not directly quantified in the paper, but given the association of detailed algorithmic explanation and pseudocode with higher reproducibility, providing gradients is expected to materially improve reproducibility.",
            "domain_or_field": "machine learning / optimization",
            "reproducibility_impact": true,
            "uuid": "e711.3",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "compute_and_cluster_mismatch",
            "name_full": "Compute Resource & Cluster Configuration Mismatch / Missing Compute Details",
            "brief_description": "Papers that require cluster/distributed compute or lack explicit compute/configuration details create barriers: cluster-specific configuration, interconnect, and scheduling details are often omitted and were associated with failed reproductions.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (manual implementation on available compute tiers)",
            "system_description": "Reproducers categorized papers by compute needed (Desktop, GPU, Server, Cluster) and attempted reproduction with available resources.",
            "nl_description_type": "experimental protocol / compute specification in methods",
            "code_implementation_type": "reproducer-written implementation and experiments",
            "gap_type": "missing compute configuration details / resource mismatch",
            "gap_description": "Papers often omitted exact compute specs or details on distributed setup; works requiring clusters were never reproduced by the authors despite access to cluster resources, suggesting omitted cluster-specific implementation/operational details (interconnects, scheduling, distributed code) impede reproduction.",
            "gap_location": "compute environment / distributed training / runtime configuration",
            "detection_method": "Manual annotation of 'Compute Needed' per paper and cross-tabulation with reproduction outcomes (Table 18); observed that Cluster-needed papers had 0 reproduced cases in the dataset.",
            "measurement_method": "Contingency counts from Table 18: e.g., Cluster: non-reproduced count = 10, reproduced count = 0; statistical significance tested (Compute Needed p = 8.75e-5). Also 'Exact Compute Specified' categorical feature tested (not significant at p=0.257).",
            "impact_on_results": "Severe for cluster-requiring works: none of the cluster-classified papers were reproduced in this study; GPU-needed works had higher reproduction rates. Overall, compute-resource issues contributed to non-reproducibility for higher-tier compute papers.",
            "frequency_or_prevalence": "Cluster-needed papers were rare (10 in the non-reproduced group in Table 18) but had 0 successful reproductions; GPU-needed papers were reproduced more often (24 reproduced vs 5 non-reproduced in Table 18).",
            "root_cause": "Omitted operational details of distributed setups; implicit assumptions about shared cluster infrastructures; lack of portable tooling for distributed runs.",
            "mitigation_approach": "Document exact compute environment (hardware, interconnect, software stack), provide distributed run scripts, containerized reproducible environments, or provide reduced-size experiments runnable on a single GPU/desktop.",
            "mitigation_effectiveness": "Not quantitatively evaluated in this paper, but authors hypothesize that reproducible tooling (e.g., repo2docker) and better documentation could improve reproduction; observed empirical pattern (GPU easier to reproduce) suggests tooling helps.",
            "domain_or_field": "machine learning / distributed training",
            "reproducibility_impact": true,
            "uuid": "e711.4",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "author_nonresponse",
            "name_full": "Author Non-Responsiveness to Questions",
            "brief_description": "Failure of paper authors to reply to implementer queries is strongly associated with failed independent reproduction; author replies markedly improve reproduction success.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (manual communication with authors during reproduction attempts)",
            "system_description": "Authors of 50 sampled papers were contacted for clarification during reproduction attempts; replies were recorded and correlated with success.",
            "nl_description_type": "author correspondence / clarifications to natural language description",
            "code_implementation_type": "reproducer-written implementation seeking clarifications",
            "gap_type": "missing clarifications / unresolved ambiguities due to non-response",
            "gap_description": "When reproducers could not resolve ambiguities or missing details from the paper, contacting authors often resolved issues; non-response left unresolved gaps that prevented reproduction.",
            "gap_location": "any stage where natural language left ambiguities (hyperparameters, algorithm details, implementation choices)",
            "detection_method": "Empirical: authors contacted for 50 papers; reply/no-reply recorded; reproduction outcomes compared for replied vs non-replied groups (Table 7).",
            "measurement_method": "Simple contact experiment: 50 papers contacted, reply rate 52% (26 replied). Reproduction success when author replied: 22/26 (≈84.6%); when no reply: 1/24 (≈4.2%). Chi-squared test highly significant (p = 6.016e-8).",
            "impact_on_results": "Huge: author replies were the most individually predictive attribute for reproduction success; lack of reply strongly associated with failure (only 1 success among 24 unanswered cases).",
            "frequency_or_prevalence": "In the subsample of 50 contacted papers, 52% replied. The study did not attempt contacting all 255 papers.",
            "root_cause": "Ambiguous/omitted details in paper and lack of accessible clarifications; authors may be unavailable or uninterested in answering.",
            "mitigation_approach": "Encourage authors to respond to queries; embrace 'living' documents (arXiv updates, errata, interactive articles); provide contactable structured resources (issues on a code repo) and detailed documentation.",
            "mitigation_effectiveness": "Very high in this dataset: contacting authors and receiving replies was associated with ~80 percentage-point increase in reproduction success in the 50-paper subsample (84.6% vs 4.2%).",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e711.5",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "code_availability_mismatch",
            "name_full": "Mismatch Between Code Release Presence and Independent Reproducibility",
            "brief_description": "Availability of authors' code was not statistically associated with higher independent reproducibility in this study, indicating a misalignment between 'code released' and being independently reproducible from the paper alone.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (paper-only reproduction policy)",
            "system_description": "Study excluded papers where authors' code had been viewed prior to reproduction to ensure independence, but the presence/absence of author code (as a paper feature) was annotated and tested for correlation with reproduction success.",
            "nl_description_type": "paper statements about code availability / README",
            "code_implementation_type": "author-released code (presence/absence recorded) vs independent reimplementation",
            "gap_type": "mismatch between documentation (paper claiming or providing code) and ability to reproduce without using that code",
            "gap_description": "Whether authors released code had no significant relationship with independent reproduction success (Table 1 p = 0.213), suggesting that code release alone does not ensure that paper natural language descriptions are sufficient for independent reproduction, and in some cases may be associated with less detail in the paper.",
            "gap_location": "overall experimental reproducibility pipeline (documentation vs runnable artifacts)",
            "detection_method": "Annotation of 'Code Available' as a paper feature and chi-squared testing against reproduction outcome.",
            "measurement_method": "Contingency counts (Table 6): Author code available: reproduced 92 vs not reproduced 44; not available: reproduced 70 vs not reproduced 49. Chi-squared p = 0.184 (non-significant).",
            "impact_on_results": "Null/complex: presence of code alone did not predict independent reproduction outcomes in this dataset; implies code release is insufficient to guarantee reproducibility when independent implementers do not consult it.",
            "frequency_or_prevalence": "Author code was available for a substantial subset (136/255 inferred from Table 6 totals), but availability did not map cleanly to improved independent reproduction.",
            "root_cause": "Authors may rely on code release instead of writing detailed natural language method descriptions; or code might be incomplete, undocumented, or require degrees of contextual knowledge to run, so paper's natural language remains insufficient.",
            "mitigation_approach": "Release well-documented, runnable code plus configuration and environment specs; do not treat code release as substitute for detailed paper-level descriptions.",
            "mitigation_effectiveness": "Not directly quantified here; paper notes that code release alone showed no correlation, so combined practices (code + detailed paper specs + hyperparameters + contactability) likely needed.",
            "domain_or_field": "machine learning",
            "reproducibility_impact": true,
            "uuid": "e711.6",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "excess_equations_overformalization",
            "name_full": "Excessive Equations / Overformalization Leading to Poor Readability",
            "brief_description": "High density of equations per page correlated negatively with reproducibility, likely because excessive formalism harms readability and implicitness of implementation details.",
            "citation_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
            "mention_or_use": "use",
            "system_name": "Independent reproduction pipeline (manual reading and implementation from paper)",
            "system_description": "Papers were manually counted for number of equations normalized by pages; relationship to readability and reproduction success was evaluated.",
            "nl_description_type": "mathematical exposition in methods / derivations",
            "code_implementation_type": "reproducer-written implementation derived from math",
            "gap_type": "ambiguous description due to overformalization / too many compact equations without prose or implementation guidance",
            "gap_description": "A larger number of equations per page was negatively correlated with independent reproduction; the most readable and reproducible papers had fewer equations per page. Possible mechanisms: more equations reduce clarity, or indicate more complex algorithms.",
            "gap_location": "mathematical exposition -&gt; mapping to implementation",
            "detection_method": "Manual counting of equations normalized by pages; Mann-Whitney tests and Kruskal-Wallis ANOVA used to evaluate association with Readability and reproduction; significance reported (Number Equations p = 0.004).",
            "measurement_method": "Normalized equations-per-page statistics and grouping by Readability; example rates: 'Excellent' readability had 2.25 equations/page vs other groups ~3.6-3.9; statistical tests found readability hypothesis significant (p=0.001) while difficulty hypothesis not significant.",
            "impact_on_results": "Moderate: high equation density associated with lower readability and consequently lower reproduction success. Authors suggest careful and judicious use of equations improves reproducibility.",
            "frequency_or_prevalence": "Varies; 'Excellent' papers had fewer equations per page on average (2.25 eq/pg) compared to others (3.6-3.9 eq/pg).",
            "root_cause": "Authors over-rely on dense formalism without implementation-oriented exposition; lack of bridging text and examples to map math to code.",
            "mitigation_approach": "Complement formal derivations with explanatory prose, worked examples, pseudocode/code-like listings and highlight implementation choices; reduce unnecessary equation density or move derivations to appendices with clear guidance.",
            "mitigation_effectiveness": "Implied effective: papers with fewer equations per page tended to be more reproducible; statistical significance supports this association.",
            "domain_or_field": "machine learning / theoretical exposition",
            "reproducibility_impact": true,
            "uuid": "e711.7",
            "source_info": {
                "paper_title": "A Step Toward Quantifying Independently Reproducible Machine Learning Research",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "State of the Art: Reproducibility in Artificial Intelligence",
            "rating": 2
        },
        {
            "paper_title": "Replicability is not reproducibility: nor is it good science",
            "rating": 2
        },
        {
            "paper_title": "Reproducible Research Environments with repo2docker",
            "rating": 2
        },
        {
            "paper_title": "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets",
            "rating": 1
        },
        {
            "paper_title": "Winner's Curse? On Pace, Progress, and Empirical Rigor",
            "rating": 1
        }
    ],
    "cost": 0.019141,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Step Toward Quantifying Independently Reproducible Machine Learning Research</h1>
<p>Edward Raff<br>Booz Allen Hamilton<br>raff_edward@bah.com<br>University of Maryland, Baltimore County<br>raff.edward@umbc.edu</p>
<h4>Abstract</h4>
<p>What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.</p>
<h2>1 Introduction</h2>
<p>As the fields of Artificial Intelligence (AI) and Machine Learning (ML) have grown in recent years, so too have calls that we are currently in an AI/ML reproducibility crisis [1]. Conferences, such as NeurIPS, have added reproducibility as a factor in the reviewing cycle or implemented policies to encourage code sharing. Many are pursing work centered around code and data availability as one of the more direct methods of enhancing reproducibility. For example, Dror et al. [2] developed a proposal to standardize the description and release of datasets. Others have proposed taxonomies and ontologies over reproducibility based on the availability of algorithm description, code, and data [3, 4]. Others have focused on building frameworks for sharing code and automation of hyper parameter selection in order to enable easier reconstruction of results [5].
While the ability to replicate the results of papers through open sourced code and data is valuable and should be lauded, it has been argued that releasing code is insufficient [6]. The inability to reproduce results without code availability may suggest problems with the paper. This may be due to the following: insufficient explanation of the approach, failure to describe important minute details, or a discrepancy between the code and description. We will call the act of reproducing the results of a paper without use of code from the paper's authors, independent reproducibility. We argue that for a paper to be scientifically sound and complete, it should be independently reproducible.
The question we wish to answer in this work is what makes a paper independently reproducible? Many have argued fiercely for different aspects of writing and publishing as critical factors of reproducability. Quantifiable study of these efforts is needed to advance the conversation. Otherwise, we as a community will not have scientific understanding that our work is addressing aspects of reproducibility. Gundersen and Kjensmo [7] defined several paper-properties of interest in regard to reproducibility. However, they defined a paper as reproducible purely as a function of the features without knowing if the selected features (e.g., method is described, data is available) actually impact a paper's reproducibility.
As a first step toward answering this question, we performed a study of 255 papers that we have attempted to implement independently. We developed the first empirical quantification about indepen-</p>
<p>dent reproducibility by recording features from each paper and reproduction outcome. We will review the entire procedure and features obtained in section 2. In section 3 we will discuss which features were determined to be statistically significant, and we will discuss the implication of these results. We will discuss the deficiencies of our study in section 4, with subjective analysis in section 5, and then conclude in section 6 .</p>
<h1>2 Procedure and Features</h1>
<p>For clarity, we will refer to ourselves, the author of this paper, as the reproducers, distinct from the authors of the papers we attempt to independently reproduce. To perform our analysis, we obtained features from 255 papers. Inclusion criteria included papers that proposed at least one new algorithm/method that is the subject of reproduction, and papers where the first implementation and reproduction attempts occurred between January 1st 2012 through December 31st 2017. We chose varied paper topics based on our historical interest. No papers were included from 2018 to present, as some papers take more time to reproduce than others, which could negatively skew results for papers from the past year. If the available source code for a paper under consideration was seen before having successfully reproduced the paper, we excluded the paper from this analysis because at that point we are not a fully independent party. In line with this, any paper was excluded if the paper's authors had any significant relationship with the reproducers (e.g., academic advisor, coworker, close friends, etc.) because intimate knowledge of communication style, work preferences, or the ability to have more regular communication could bias results. A paper was considered to be reproduced if the code for results were written by the reproducers, allowing the use of reasonable and standard libraries (e.g., BLAS, PyTorch, etc.), and the code reproduced the majority of claims from the paper.
Specifically, we regarded a paper as reproducible if the majority ( $75 \%+$ ) of the claims in the paper could be confirmed with code we independently wrote. If a claimed improvement was measured in orders-of-magnitude, being within the same order-of-magnitude was considered sufficient (e.g., a paper claims 700x faster, but reproducers observe 300x). This same order-of-magnitude criterion comes from an observation that such claims are highly dependent upon constant factor efficiency improvements that may be had/missing from both the prior methods, and the proposed method being replicated. Presence or absence of these improvements can cause, apparently, "dramatic" impacts without fundamentally changing the nature of the contribution we are attempting to reproduce. When compared to other algorithms, we consider a paper reproduced if the considerable majority $(90 \%+)$ of the new algorithm's rankings correspond to those found in the paper (e.g., the claim is that the proposed method was most accurate on $95 \%$ of tasks compared to 4 other models, we want to see our reproduction be most accurate on at least $95 \% \cdot 90 \%=81 \%$ of the same tasks, compared to the same models). As a last resort, we considered getting within $10 \%$ of the numbers reported in the paper (or better), or in the case of non-quantitative results (e.g., GAN sample quality), we subjectively compare our results with the paper to make a decision. We include this flexibility in specification to allow for small differences that can occur. While not common, we did encounter more than one instance where our independent reproduction achieved better results than the original paper.
After this selection process, we are left with 255 papers, of which 162 (63.5\%) were successfully replicated and 93 were not. We note that this is significantly better than the $26 \%$ reproducibility determined by [7], who defined reproducibility as a function of the features they believed would determine reproduction. Below we will describe each of the features used. We attempt to catalog both features that are believed relevant to a paper's reproduction and features that should not be relevant, which will help us quantify if these expectations hold. We will use statistical tests to determine which of these features have a significant relationship with reproduction. An anonymized version of the data can be found at https://github.com/EdwardRaff/ Quantifying-Independently-Reproducible-ML.</p>
<h3>2.1 Quantified Features</h3>
<p>We have manually recorded 26 attributes from each paper, which took approximately 20 minutes per paper to complete ${ }^{1}$. A policy for each feature was developed to minimize as much subjectivity as possible. Below we will review each feature, and how they were recorded, in order from least to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>most subjective. Each feature was obtained based on the body of the main paper only, excluding any appendices (unless specified otherwise).
Features to consider were selected based on two factors: 1) would one reasonably believe the feature should be correlated with the ability to reproduce a paper (positive or negative) and 2) was the feature reasonably available with little additional work? This was done to capture as much useful information as possible while also avoiding limiting our study to items where a priori one might believe that a feature's relevance (or lack thereof) to be "obvious."</p>
<p>Unambiguous Features: Some features are not ambiguous in nature. A few are simple and innate properties that require no explanation. This included the Number of Authors, the existence of an appendix (or supplementary material), the number of pages (including references, excluding any appendix), the number of references, the year the paper was published, the year first attempted to implement, the venue type (Book, Journal, Conference, Workshop, Tech-Report), as well as the specific publication venue (e.g., NeurIPS, ICML). Many papers follow a progression from TechReport to Workshop to Conference to Journal as the paper becomes more complete. For any paper that participated in parts of this progression, we use the version from the most "complete" venue under the assumption that it would be the most reproducible version of the paper allowing us to avoid issues with double-counting papers.
We also include whether or not the Author Replied to questions about their paper. If any author replied to any email, it was counted as a "Yes". If no author ever replied, we marked it as "No." In all cases, every paper author was sent an email before marking it as "No." If a current email could not be found, we marked that the authors were not contacted.</p>
<p>Mild Subjectivity: We spend more time expounding on the next set of features, which had minor degrees of subjectivity. We state below the developed procedure we used to make their quantification practical and reproducible.</p>
<ul>
<li>Number of Tables: The total number of tables in the paper, regardless of the content of those tables. While tables usually contain results, they often contain a wide variety of content, and we make no distinction between them due to their frequency and variety.</li>
<li>Number of Graphs/Plots: The total number of plots/graphs contained in the paper which includes scatter plots, bar-charts, contour-plots, or any other kind of 2D-3D numeric data visualization.</li>
<li>Number of Equations: Due to differing writing styles, we do not use equation number provided by the paper, nor do we count everything that might be typed between LaTeX "\$" brackets. We manually reviewed every line of every paper to arrive as a consistent counting process ${ }^{2}$. Inline mathematics were only counted if the the math involved 1) two or more variables interacting (e.g., $x \cdot y$ ) or 2) two or more "operations" (e.g, $P(x \mid y)$ or $O\left(x^{2}\right)$ ). If only one "operation" occurred (e.g, $P(x)$ or $x^{2}$ ), it was not considered. Inline equations were counted only once per line of text, regardless of how many equations occurred in a line of text. Whole-line equations were always counted, regardless of the simplicity of the equation. If multiple whole lines were used because of equation length (e.g., a " + " ), it was counted as one equation. If multiple whole lines were used due to showing a mathematical step or derivation, each step counted as an additional equation. Partial deference was given to equation numbers. If every line of an equation received its own number, they were counted accordingly. If a derivation over $n$ whole lines received only one equation number, the equation was counted $\lceil n / 3\rceil$ times.</li>
<li>Number of Proofs: A proof was only counted if it was done in a formal manner, beginning with the statement of a corollary or theorem, and included at least an overview of how to achieve the proof. A proof was counted if it occurred in the appendix or supplementary material. Derivations of update rules or other equations did not count as a proof unless the paper stated them as a proof. This was done as a practical matter in reducing ambiguity and the process of collecting the information.</li>
<li>Exact Compute Specified: If a paper indicated any of the specific compute resources used (e.g., CPU GHz speed or model number, GPU model, number of computers used), we considered it to have satisfied this requirement.</li>
<li>Hyper-parameters Specified: If a paper specified the final hyper-parameter values selected for each dataset or the method of selecting hyper-parameters (e.g., cross validation factor) and the value range (e.g., $\lambda \in[1,1000]$ ), we consider it to have satisfied this requirement. Simply stating that a grid-search (or similar procedure) was used was not sufficient. If a paper introduced</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>multiple hyper-parameters but only specified how a sub-set of the parameters where chosen, we marked it as "Partial".</p>
<ul>
<li>Compute Needed: We defined the compute level needed to reproduce a paper's results as needing either a Desktop (i.e., $\leq \$ 2000$ ), a consumer GPU (e.g., an Nvida Geforce type card), a Server (used 20 cores or more, or 64 GB of RAM or more), or a Cluster. If the compute resources needed were not explicitly stated, this was subjectively based on the computational complexity of the approach and amount of experiments believed necessary to reach reproduction. We stress that this compute level was selected based on today's common compute resources, not those available at the time of the paper's publication.</li>
<li>Data Available: If any of the datasets used in the paper are publicly available, we note it as having satisfied this requirement.</li>
<li>Pseudo Code: We allow for four different options for this feature: 1) no pseudo code is given in the paper, 2) "Step-Code" is given, where the paper outlines the algorithm/method as a sequence of steps, but the steps are terse and high-level or refer to other parts of the paper for details, 3) "Yes", the paper has some pseudo code which outlines the algorithm at a high level but with sufficient detail that it feels mostly complete, and 4) "Code-Like", the paper summarizes the approach in great detail that is reminiscent of reading code (or is in fact code ).</li>
</ul>
<p>Subjective: We have a final set of features which we recognize are of a significantly subjective nature. For all of these features, we are aware there may be significant issues, and in practice, any alternative protocol would impose its own different set of issues. We have made the choices in an attempt to minimize as many issues as possible and make the survey possible. Below is the protocol we followed to reduce ambiguity and make our procedure as reproducible as possible for future studies, which will help the reader fully understand our interpetation of the results.</p>
<ul>
<li>Number of Conceptualization Figures: Many papers include graphics or content for which the purpose is not to convey a result, but to try to convey the idea / method proposed itself. These are usually included to make it easier to understand the algorithm, and so we identify them as a separate item to count.</li>
<li>Uses Exemplar Toy Problem: As a binary "Yes"/"No" option, did the paper include an exemplar toy problem? These problems are not meaningful toward any application of the algorithm, but they are devised to show specific behaviors or create demonstrations that are easier to reproduce / help conceptualize the algorithm being presented. These are often 2D or 3D problems, or they are synthetically generated from some specified set of distributions.</li>
<li>Number of Other Figures: This was a catch-all class for any figure that was not a Graph/Plot, Table, or Conceptualization Figure as defined above. For most papers, this included samples of the output produced by an algorithm or example input images for Computer Vision applications.</li>
<li>Rigor vs Empirical: There have been a number of calls for more scientific rigor within the ML community[8], with many arguing that an overly empirical focus may in fact slow down progress [9]. We are not aware of any agreed upon taxonomy of what makes a paper "rigorous". Based on the interpretation that rigor equates to having grounded understanding of why and how our methods work, beyond simply showing that they do so empirically, we develop the following protocol: a paper is classified as "Theory" (read, rigorous), if it has formal proofs, provides mathematical reasoning or explanation to modeling decisions, or provides mathematical reasoning or explanation to why prior methods fail on some dataset. By default, we classify all other papers as "Empirical." However, if a "Theory" paper also includes discussion of practical implementation or deployment concerns, complete discussion of hyper-parameter setting such that there is no ambiguity, ablation studies of decisions made, or experiments on production datasets, we consider the paper "Balanced" as having both theory and empirical components.</li>
<li>Paper Readability: We give each paper a readability score of "Low", "Ok", "Good", or "Excellent." To minimize subjectivity in these scores, we tie each to the amount of times we had to read the paper in order to reach a point where we felt we had the proposed algorithm implemented in its entirety, and the failure to replicate would be a matter of finding and removing bugs. The score of "Excellent" means that we needed to read the paper only once to produce an implementation, "Good" papers needed two or three readings, "Ok" papers needed four or five, with "Low" being six or more reads through the paper ${ }^{3}$.</li>
<li>Algorithm Difficulty: We categorize the difficulty of implementing an algorithm as either "Low", "Medium", or "High." We grounded this to lines of code for any paper successfully implemented</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>or which made its implementation available online. For ones never successfully implemented and without code, we estimated this based on our intuition and experience on where the implementation would have landed based on reading the paper. "Low" difficulties could be completed in 500 lines of code or less, "Medium" difficulty between 500 and 1,500 lines, and "High" was $&gt;1,500$ lines. In these numbers we assume using common libraries (e.g., auto-differentiation, BLAS, etc.).</p>
<ul>
<li>Primary Topic: For each paper we tried to specify a single primary topic of the paper. Many papers cover different aspects of multiple problems, making this a challenge. We adjusted topics into higher-level categories so that each topic had at least three members, so that we could do meaningful statistics. Topics can be found in the appendix.</li>
<li>Looks Intimidating: The most subjective, does the paper "look intimidating" at first glance?</li>
</ul>
<h1>3 Results</h1>
<p>Our features are either numeric or categorical. For each numeric feature (except the number of pages and number of authors), we normalized the value by the number of pages in the paper. Longer papers naturally have more space to include more equations, figures, etc., and this was done to make all papers more directly comparable. For numeric features we used the non-parametric Mann-Whitney U [10] test to determine significance. A Shapiro-Wilk test of normality [11] confirmed that none of our features would have been appropriate for use with a Student's t-test and so the non-parametric testing is preferred. For all categorical features, we used a Chi-Squared test [12] with continuity correction [13]. In our analysis we will also examine relationships between some of our categorical features and other numeric features for suspected relationships. We will continue to use non-parametric tests for robustness/conservative estimates of significance, relying on the Kruskal-Walls [14] for ANOVA testing and the Dunn test [15] for post-hoc analysis. JSAP was used to compute all statistical tests [16]. In Table 1 we show the results for deciding which of our 26 features were correlated with a paper's reproducibility. Tables and graphs of all the features are too numerous to fit in the main paper, and will be found in the appendix.
We begin by noting that the year a paper was published or the year that we first tried to implement the paper were not correlated with successful reproduction. The concerns of a reproducibility crisis would generally imply that the issue is a recent one. However, the year a paper was published is not correlated with successful reproduction, with the oldest paper being from 1984. This would suggest that independent reproducibility has not changed over time. Depending on one's perspective, we could argue that there is not a reproducibility crisis, or that one has been ongoing for several decades. It is important the reader qualify this statistical result with the fact that the year of paper publication in our study is not evenly distributed over time, with the majority of papers occurring in between 2000 through 2017.
To our study's benefit, the year first attempted for reproduction was not significant. If our success was correlated with time (as one might expect in advance- with skill increasing with experience), we would worry about this skewing our results. This appears to not be an issue, removing a potential problem from our results.</p>
<p>Table 1: Significance test of which paper properties impact reproducibility. Results significant at $\alpha \leq 0.05$ marked with" ${ }^{\prime \prime}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Year Published</td>
<td style="text-align: left;">0.964</td>
</tr>
<tr>
<td style="text-align: left;">Year First Attempted</td>
<td style="text-align: left;">0.674</td>
</tr>
<tr>
<td style="text-align: left;">Venue Type</td>
<td style="text-align: left;">0.631</td>
</tr>
<tr>
<td style="text-align: left;">Rigor vs Empirical</td>
<td style="text-align: left;">$1.55 \times 10^{-9}$</td>
</tr>
<tr>
<td style="text-align: left;">Has Appendix</td>
<td style="text-align: left;">0.330</td>
</tr>
<tr>
<td style="text-align: left;">Looks Intimidating</td>
<td style="text-align: left;">0.829</td>
</tr>
<tr>
<td style="text-align: left;">Readability</td>
<td style="text-align: left;">$9.68 \times 10^{-25}$</td>
</tr>
<tr>
<td style="text-align: left;">Algorithm Difficulty</td>
<td style="text-align: left;">$2.94 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Pseudo Code</td>
<td style="text-align: left;">$2.31 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">Primary Topic</td>
<td style="text-align: left;">$7.039 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">Exemplar Problem</td>
<td style="text-align: left;">0.720</td>
</tr>
<tr>
<td style="text-align: left;">Compute Specified</td>
<td style="text-align: left;">0.257</td>
</tr>
<tr>
<td style="text-align: left;">Hyperparameters Specified</td>
<td style="text-align: left;">$8.45 \times 10^{-6}$</td>
</tr>
<tr>
<td style="text-align: left;">Compute Needed</td>
<td style="text-align: left;">$8.75 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Authors Reply</td>
<td style="text-align: left;">$6.01 \times 10^{-8}$</td>
</tr>
<tr>
<td style="text-align: left;">Code Available</td>
<td style="text-align: left;">0.213</td>
</tr>
<tr>
<td style="text-align: left;">Pages</td>
<td style="text-align: left;">0.364</td>
</tr>
<tr>
<td style="text-align: left;">Publication Venue</td>
<td style="text-align: left;">0.342</td>
</tr>
<tr>
<td style="text-align: left;">Number of References</td>
<td style="text-align: left;">0.740</td>
</tr>
<tr>
<td style="text-align: left;">Number Equations</td>
<td style="text-align: left;">0.004</td>
</tr>
<tr>
<td style="text-align: left;">Number Proofs</td>
<td style="text-align: left;">0.130</td>
</tr>
<tr>
<td style="text-align: left;">Number Tables</td>
<td style="text-align: left;">0.010</td>
</tr>
<tr>
<td style="text-align: left;">Number Graphs/Plots</td>
<td style="text-align: left;">0.139</td>
</tr>
<tr>
<td style="text-align: left;">Number Other Figures</td>
<td style="text-align: left;">0.217</td>
</tr>
<tr>
<td style="text-align: left;">Conceptualization Figures</td>
<td style="text-align: left;">0.365</td>
</tr>
<tr>
<td style="text-align: left;">Number of Authors</td>
<td style="text-align: left;">0.497</td>
</tr>
</tbody>
</table>
<h1>3.1 Significant Relationships</h1>
<p>There were ten variables that are significantly correlated with a paper's reproducibility. Of them, Number of Tables, Equations, Compute Needed, Pseudo-Code, and Hyper-parameters Specified are the least subjective variables which were significant.</p>
<p>Readability had the strongest empirical relationship, which on its face is not surprising. Note that by our definition, Readability corresponds to how many reads through the paper were necessary to get to a mostly complete implementation. As expected, the fewer attempts to read through a paper, the more likely it was to be reproduced. For "Excellent" papers, we were always able to reproduce results. Exact counts can be found in Table 11. Based on these results we argue the importance of clear and effective communication of implementation details, which may often be neglected. This neglect may come from forced page limits or a preference towards other, competing factors (e.g., preferring figures/results that better show the method's value, at the cost of method details). We suspect that a factor in this is page limits which we test by proxy via paper page length. A Krusal-Wallis test confirms the significance of paper length in pages $(p=0.035)$. A Dunn post-hoc test shows that "Low" Readability papers are the statistically significant source of this relationship, which are 3.17-5.67 pages shorter than the other Readability types. As a field that has historically focused on open-access and online availability, and with the decreasing relevance of paper conference and journal distributions, our study suggests that raising page limits on papers, and adding technical algorithmic details as an explicit review factor, could aid in increasing the reproducibility of papers.
Two factors we expected to Table 2: Relationship between use of Pseudo Code and Readability be related to a paper's Readability (as we have defined it) are the Algorithm's Difficulty and the presence of PseudoCode, both of which are significant factors and have a statistically significant relationship with Readability. If we look at Table 12, we see that Pseudo-Code has a complicated relationship with Reproduction. Highly Detailed "Code-Like" descriptions are more reproducible, but having "No" pseudo-code is also positively related with reproduction. Based on these results papers which can effectively describe their algorithms without pseudo-code are communicating the information in another way, but papers with "Step-Code" do an inadequate job at this task. Examining the relationship between Pseudo-Code and Readability in Table 2 supports this, where we see that using Step-Code is biased toward lower readability. This also makes sense in abstract, as step-code often requires one to repeatedly reference different parts of a paper. The relationship between an Algorithm's difficulty is more direct and intuitive, Table 10 showing that reproducibility decreases with difficulty.</p>
<p>It is also interesting to note how Rigor vs Empirical is correlated with reproducibility. One may have expected papers that focus on proving their methods correct would be the most reproducible. In Table 8 we can see that papers that are "Empirical" or "Balanced" both have higher than expected reproduction rates, while "Theory" oriented papers have lower than expected. These results would seem to suggest that empiricism is intrinsically valuable for reproduction on the micro scale of individual papers. This does not contradict any of the concerns about long-term behaviors and results that are side effects of overly-empirical issues discussed by Sculley et al. [9], such as new methods being inappropriately considered due to ineffectively tuned baselines and lack of ablation studies. We take this result as a further indication that rigor cannot just be math or learning bounds for their own sake, but that the practical relevance and execution of any theorems must be at the forefront in all papers ${ }^{4}$.
Unfortunately, the primary topic of a paper was found to be a significant factor for independent reproducibility. We were not able to reproduce any Bayesian or Fairness based papers. We had a higher-than expected success in implementing papers about Deep Learning and Search/Retrieval.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We, the reproducers, are not experts in all of the primary topic areas listed, and so we advise against extrapolation from this particular result. This leads to interesting questions regarding reproduction from inside/outside an expert peer group and when one qualifies as an expert in a general topic area. We hope to explore these questions further in future work.</p>
<p>Both Number of Tables and Hyper-parameters were positively correlated with reproducibility. The more tables included in a paper, or the more parameters specified, the more likely the paper was to be reproducible. This is not a surprising result for the Hyper-parameters case, and supports the emphasis the community has placed on this factor [5]. It is somewhat peculiar that Tables are significant, but Graphs/Plots are not as both convey primarily numeric information to the reader. We suspect that the ability for the reader to quickly understand the exact value/result from a table is the differentiating factor as it gives a target to meet and measure against. While a plot/graph may describe overall behavior, it may not readily avail itself to quickly extracting a hard number and using it as a goal.</p>
<p>The Number of Equations per page was negatively correlated with reproduction. Two theories as to why were developed based on our experience implementing the papers: 1) having a larger number of equations makes the paper more difficult to read, hence more difficult to reproduce or 2) papers with more equations correspond to more complex and difficult algorithms, naturally being more difficult to reproduce. A Kruskal-Wallis ANOVA reveals that the readability hypothesis is significant $(p=0.001)$ but not the difficulty hypothesis $(p=0.239)$. Following with a Dunn post-hoc test shows that papers which have "Excellent" readability have fewer equations per page ( $2.25 \mathrm{eq} / \mathrm{pg}$ ) than the others, as the source of the significant ( $p \leq 0.002$ ) relationship. There are no significant differences between papers of "Low," "Ok," and "Good" readability (3.91, 3.60, 3.78 equations per page respectively), leading us to postulate that the most readable and reproducible papers make careful and judicious use of equations.
Our last paper-intrinsic property is Compute Needed, which could be a "Desktop", "GPU", "Server", or "Cluster". In the time that these papers were implemented, we have had access to all four compute levels to varying degrees. Looking at Table 18, we see the use of a Cluster or GPU are the ones that depart from expectations. Despite having access to cluster resources, we have never successfully reproduced a paper that needed such resources. At the same time, we have a higher reproduction rate for works that require a GPU. Our suspicion is that frameworks such as PyTorch and Tensorflow, which make use of GPUs relatively easy, have been converging toward an effective paradigm for using that kind of resource. These libraries make it easier to reproduce current papers and historical ones that lacked such advanced tools, which then inflates reproduction rate. While frameworks like Spark exist for distributed computation, they may not be sufficiently developed for Machine Learning use cases to ease replication. Another alternative hypothesis for Cluster reproduction failure is that the details of how a cluster is organized, with interconnects, job scheduling, and more sophisticated code, are increasing the reproduction barrier and lack necessary details. We do not have sufficient information to confirm or reject these hypotheses, but we encourage others to consider them as avenues for study.</p>
<p>This leaves us with the last significant result, which is not a property of the paper itself: whether the paper authors reply to questions about their paper. We reached out to the authors of 50 different papers and had a reply rate of $52 \%$. Table 7 which shows that replying was the most individually predictive attribute studied. In the 24 cases where the author did not respond to questions, we succeeded in replication only once. For the 26 cases where they did reply, we succeeded 22 times. While this result demonstrates the importance of corresponding with readers, it gives credence to the idea of a non-stationary and "living" paper where updates may be made over time to address questions and concerns. Such is possible today with arxiv.org and distill.pub, and provides quantifiable evidence that their ability to update articles is a meaningful and powerful tool toward reproducibility (if leveraged). Other confounding hypotheses exist as well, such as receiving a reply increasing the motivation of the reproducers, or the nature of a discussion that is not constrained to a paper's limitations may also impact reproduction rates.</p>
<h1>3.2 Interesting Non-Significant/Negative Results</h1>
<p>While we have already discussed some non-significant results as they relate directly to significant ones above, we also want to highlight interesting non-significant results. In particular, we expected a priori that the use of Conceptualization Figures and Exemplar Problems would be significant pre-</p>
<p>dictors, as we have found them useful in our personal experiences both to understand the algorithm, and as an initial test-bed to confirm an algorithm was working to a minimal degree. Yet neither are significant. We also find that neither have a relationship with a paper's Readability ( $p \geq 0.476$ ). These results give us pause regarding our assumptions about what makes a "good" reproducible paper, and reinforce the importance of quantifying these important questions.
A positive indicator is that Venue (e.g., NeurIPS vs PKDD) had no significant impact, nor did Venue type (e.g., Workshop vs Journal). This result would seem to imply that the same issues and successes are occurring across most academic levels, though selection bias may play a role in this result.
The non-significance of including an appendix is of note given our results that the papers which are hardest to reproduce ("Low" Readability) are shorter on average. There is no significant difference between a paper's readability and the presence of an appendix ( $p=0.650$ ), which implies that appendices are not sufficient means of circumventing page limits at conference/workshop venues.
We found it interesting that whether or not the papers' authors released their code has no significant relationship with the paper's independent reproducibility. Before analysis, we could see hypotheticals that would cause correlations in either direction. Authors who release code might include less details in the paper under the assumption that readers will find them in the code itself. Conversely, one might imagine that authors who release code care more about reproduction and would include more of the necessary details. With more conferences encouraging code availability as a reviewer criteria, we would not necessarily expect any change in independent reproducibility from this change in isolation (impacts on cultural changes induced being a question beyond our scope).</p>
<h1>4 Study Deficiencies</h1>
<p>While we have taken the first step toward studying and quantifying factors of reproducibility, we must also acknowledge deficiencies in our study. Most apparent are a number of potential biases. The papers under consideration have a selection bias based on interest and filtering from consideration any paper where we had previously looked at released source code. More importantly, all papers were attempted by just this paper's author. So while we have a large sample size of papers, we have a low sample size of implementers. It is entirely possible that those with a different background in education, training, career, and interests, would find different papers easy or difficult to reproduce.
Because we are the sole reproducer, all the results must be taken with consideration conditioned on our background, and the origin of this work. A majority of attempted reproductions where in pursuit of contribution to a machine learning library that we are the author of, JSAT [17]. As such, we focused initially on a number of more common and widely used algorithm. These methods had already been independently reproduced by others many times, and alternative materials (e.g., lecture notes) were available to provide guidance without consulting code written by others. Further papers where spurred by our personal interest in what we considered useful for such a library, and our own personal interests (historical interests including nearest neighbor algorithms, linear models, and kernel methods). Such well known works do not make the majority of reproduction attempts, but they make up a sizable sub-population of the methods we attempted for JSAT, and so may skew results.</p>
<p>Our study is also limited by our own historical records. The use of paper cataloging software to take notes and record information made this study possible, but it also limits our study to the recorded notes and what can be re-derived from the paper itself (e.g., number of pages).
Towards improving upon the number of implementers and recording information, we hope to encourage extensions to projects such as the ICLR Reproducibility Challenge. A communal effort to standardize on an initial set of paper features, keep track of time and resources spent on reproduction, and information about the reproducers (years of experience, education, and background) may allow for a richer and more thorough macro study of reproducibility in the future. A design constraint we would like to include in such a system is differential privacy so that it is not known which individual papers are having reproduction difficulties. We have intentionally avoided identifying papers to avoid any perceived "naming and shaming", as our or other attempts in isolation should not be seen as conclusive statements on any individual paper's lack of reproduction.
In our experience attempting to reproduce these papers, we also note a failure in the framing of the problem: that a paper is reproducible or not. Depending on the paper, differing levels of resources</p>
<p>and even teams may be necessary for reproduction. As a point of reference, the longest effort toward reproduction we studied took 4.5 years of (non-continuous) effort to finally reproduce the results. In this light, it may be better to model reproduction as a kind of survival analysis conditioned on properties of the implementer(s). A paper "survives" as the implementers attempt reproduction and "dies" once successfully reproduced (or "lives" forever if never reproduced). Viewed in this light, we may ask: what environmental factors (e.g., libraries like PyTorch, Scikit-Learn, compute resources) impact survival rates and times, and should the necessity of code release be a function of survival time? A real life example of this is playing out now, as people attempt to reproduce OpenAI's recent GPT-2 results ${ }^{5}$, where information and data was intentionally withheld due to security concerns.
An important factor not included in our analysis are the authors of a paper, which has a direct impact on writing style, topic, and other factors. Subjectively we note that there are authors whose work we regularly fail to reproduce and ones we regularly succeed in reproducing, even when both make code available. Study of how the backgrounds and styles of both authors and implementers interact and impact reproduction seems to be a valuable line of inquiry, but it is beyond our current scope and requires additional thought and consideration.
We also note that the most significant factors in reproducibility are the most subjective factors. While we endeavored to reduce the impact of subjectivity as much as possible with our stated protocols, this indicates that more work is warranted in developing more objective measures that are related to these subjective factors, or using communal effort to reach a distributional determination on these subjective factors, for future studies.</p>
<h1>5 A Subjective Recall of Non-Reproduction</h1>
<p>We did not record the believed reason for failure to reproduce, although this would have been valuable information. We hope that this will be noted by others in the future, but for now we recount a subjective summary of the primary reasons we felt a paper could not be reproduced. We note that part of our belief in the below list stems from our efforts to email papers' authors when attempting to independently reproduce their works, in which we are often seeking information that would elucidate the below issues:</p>
<ol>
<li>Unclear notation or language. A component of the algorithm is explained, but not in a way easily understood by the reproducers, or was ambiguously specified.</li>
<li>Missing algorithm step or details, a step was completely left out of description.</li>
<li>Many papers would specify loss functions or other equations for which the gradient needed to be taken, but not detail the resulting gradients. Depending on the functions and math involved re-deriving was non-trivial, and our results did not match.</li>
<li>Missing hyper-parameters, or similar nuance details. The reproducers believe we have an implementation accurate to what was described, but some "minor" detail was not specified and makes a big difference in results.</li>
</ol>
<p>We avoided in this paper any attempt to imply or cast doubt on the veracity of any individual paper. In our experiences through this work, we have rarely had suspicion that the results of a paper were false or the result of serious flawed implementation, and thus could never be reproduced.</p>
<h2>6 Conclusions</h2>
<p>In this work we have conducted the first empirical study of what impacts a paper's reproducibility. We suspect this will lead to considerable debate about the meaning of results, and we hope to spur further quantifiable studies. Based on our results, we find that paper reproduction rates have not changed (in a statistically significant way) over the past 35 years. Papers of a more empirical nature tend to be more reproducible, as are ones that include factors relevant to implementation details - though simply including Pseudo-Code is not sufficient. Our study indicates papers with fewer equations and more tables tend to be more reproducible, and that there is a potential latent issue in reproduction when cluster computing becomes a requirement.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Acknowledgements</h1>
<p>I would like to thank Jared Sylvester, Arash Rahnama, Charles Nicholas, Cynthia Matuszek, Frank Ferraro, Ian Soboroff, and Ashley Klein, who all provided valuable discussion and feedback on this work through its formation to completion.</p>
<h2>References</h2>
<p>[1] M. Hutson, "Artificial intelligence faces reproducibility crisis," Science, vol. 359, no. 6377, pp. 725-726, 2018. [Online]. Available: https://science.sciencemag.org/content/359/6377/725
[2] R. Dror, G. Baumer, M. Bogomolov, and R. Reichart, "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets," Transactions of the Association for Computational Linguistics, vol. 5, pp. 471-486, 12 2017. [Online]. Available: https://www.aclweb.org/anthology/Q17-1033
[3] R. Tatman, J. Vanderplas, and S. Dane, "A Practical Taxonomy of Reproducibility for Machine Learning Research," in Reproducibility in ML Workshop, ICML'18, 2018.
[4] G. C. Publio, D. Esteves, and H. Zafar, "ML-Schema : Exposing the Semantics of Machine Learning with Schemas and Ontologies," in Reproducibility in ML Workshop, ICML'18, 2018.
[5] J. Forde, T. Head, C. Holdgraf, Y. Panda, F. Perez, G. Nalvarte, B. Ragan-kelley, and E. Sundell, "Reproducible Research Environments with repo2docker," in Reproducibility in ML Workshop, ICML'18, 2018.
[6] C. Drummond, "Replicability is not reproducibility: nor is it good science," in Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML, Montreal, Canada,2009, ser. Evaluation Methods for Machine Learning Workshop, the 26th ICML, June 14-18, 2009, Montreal, Canada, 2009.
[7] O. E. Gundersen and S. Kjensmo, "State of the Art: Reproducibility in Artificial Intelligence," Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI-18), pp. 1644-1651, 2018.
[8] A. Rahimi and B. Recht, "NIPS 2017 Test-of-time award presentation," 2017.
[9] D. Sculley, J. Snoek, A. Rahimi, and A. Wiltschko, "Winner's Curse? On Pace, Progress, and Empirical Rigor," in ICLR Workshop track, 2018. [Online]. Available: https://openreview.net/ pdf?id=rJWF0Fywf
[10] H. B. Mann and D. R. Whitney, "On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other," The Annals of Mathematical Statistics, vol. 18, no. 1, pp. 50-60, 3 1947. [Online]. Available: http://projecteuclid.org/euclid.aoms/1177730491
[11] S. S. Shapiro and M. B. Wilk, "An analysis of variance test for normality (complete samples)," Biometrika, vol. 52, no. 3-4, pp. 591-611, 12 1965. [Online]. Available: https://academic.oup. com/biomet/article-lookup/doi/10.1093/biomet/52.3-4.591
[12] K. Pearson, "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling," The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 50, no. 302, pp. 157-175, 7 1900. [Online]. Available: https://www. tandfonline.com/doi/full/10.1080/14786440009463897
[13] F. Yates, "Contingency Tables Involving Small Numbers and the $\chi 2$ Test," Supplement to the Journal of the Royal Statistical Society, vol. 1, no. 2, pp. 217-235, 1934. [Online]. Available: http://www.jstor.org/stable/2983604
[14] W. H. Kruskal and W. A. Wallis, "Use of Ranks in One-Criterion Variance Analysis," Journal of the American Statistical Association, vol. 47, no. 260, pp. 583-621, 12 1952. [Online]. Available: https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441</p>
<p>[15] O. J. Dunn, "Multiple Comparisons Among Means," Journal of the American Statistical Association, vol. 56, no. 293, pp. 52-64, 1961. [Online]. Available: http://www.jstor.org/ stable/2282330
[16] JASP Team, "JASP (Version 0.9)[Computer software]," 2018. [Online]. Available: https:// jasp-stats.org/
[17] E. Raff, "JSAT: Java Statistical Analysis Tool, a Library for Machine Learning," Journal of Machine Learning Research, vol. 18, no. 23, pp. 1-5, 2017. [Online]. Available: http://jmlr. org/papers/v18/16-131.html</p>
<h1>A Some Preemptive Responses to Questions</h1>
<p>In preparation of this manuscript, I have sought advice and feedback from a number of colleagues. These discussions have resulted in a few common questions of related themes. As such, many do not necessarily belong in the main content of an academic paper. Here I will list and preemptively answer the common ones in hope of aiding the reader in better understanding this work, the context around it, and the potential biases that may exist in the results as a function of my personal background.</p>
<h2>A. 1 Why Where you Recording Information About Papers?</h2>
<p>Before I started working on JSAT, or had even learned about machine learning, I had a side project implementing an arbitrary precision math library ${ }^{6}$. I worked on this library for four years, and implemented a number of algorithms for computing different decomposition, mathematical constants, common functions (e.g., Fibonacci numbers), complex numbers, all at arbitrary precision. As part of this I began to read and implement a number of papers for these techniques. As time went on, and I occasionally found and discovered bugs in my previous implementations, I grew frustrated in the bug fixing process. Fixing bugs for these more involved methods required me to re-understand and find previous papers, which I was not good at. As a result, when I started JSAT, I began keeping notes to myself from the onset. I again intended it to be a multi-year and long term project, and wanted to avoid repetition of previous failures.</p>
<h2>A. 2 Why Where you Implementing so many Papers?</h2>
<p>Early in my computer science career, I had forgone most all advanced math courses - taking the bare minimum to get my degree, with the exception of a numerical analysis course that I thought would be relevant to my arbitrary precision math library mentioned above. I had instead focused on taking many more CS courses until I happened to take a machine learning class and became enamoured with the concept and the field. This left me with a situation where I wanted to get into a domain that was math heavy, but my skills had long languished. While I personally felt I have always understood an algorithm or technique best once I can implement it, I came to rely on implementing an algorithm as a crutch to my lesser mathematical skills. This has continued far longer than I would like in many ways and so have continued to attempt to implement papers I want to understand as the fastest way for me to come to a functional understanding of a paper.</p>
<h2>B Statistical Test assumptions</h2>
<p>In Table 3, we perform a normality test, which confirms that all of our numeric features deviate significantly from a normal distribution, making a standard Student's t-test inappropriate for hypothesis testing.</p>
<p>The Mann-Whitney test assumes that the variance of the two distributions under test are equal. We can see from Table 4 that this again holds for all of our numeric features, with the exception of the Year of the publication and the total number of pages. If we instead preformed a Welch test, which does not have the equality of variance assumption, we still arrive at the conclusion that Year ( $p=$ .554) and number of Pages ( $p=0.134$ ) do not have any significant relationship with reproducibility. The pages variable is also impacted by a few outliers (the most extreme of which has over 400 pages), which is the cause of the apparent discrepancy in variance.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Test of Normality (Shapiro-Wilk) of numeric features, showing that a standard t-test would not be appropriate</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reproduced</th>
<th style="text-align: center;">W</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Number of References</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">$2.583 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">$1.824 \times 10^{-18}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Num References</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">$1.084 \times 10^{-11}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Number of Equations</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">$1.366 \times 10^{-8}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">$5.417 \times 10^{-13}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Number of Proofs</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">$1.757 \times 10^{-13}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">$1.454 \times 10^{-17}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Total Tables and Figures</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">$3.833 \times 10^{-6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">$3.608 \times 10^{-16}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Number of Tables</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">$2.990 \times 10^{-12}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">$6.970 \times 10^{-10}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Number of Graphs/Plots</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">$1.539 \times 10^{-8}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">$7.344 \times 10^{-18}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Number of Other Figures</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">$6.193 \times 10^{-14}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">$8.797 \times 10^{-24}$</td>
</tr>
<tr>
<td style="text-align: center;">Normalized Conceptualization Figures</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">$4.755 \times 10^{-15}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">$4.003 \times 10^{-19}$</td>
</tr>
<tr>
<td style="text-align: center;">Pages</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">$3.090 \times 10^{-10}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">$7.302 \times 10^{-17}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Test of Equality of Variances (Levene's) for numeric features.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">F</th>
<th style="text-align: right;">df</th>
<th style="text-align: right;">p</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Year</td>
<td style="text-align: right;">5.811</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.017</td>
</tr>
<tr>
<td style="text-align: left;">Year Attempted</td>
<td style="text-align: right;">0.443</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.506</td>
</tr>
<tr>
<td style="text-align: left;">Pages</td>
<td style="text-align: right;">5.299</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.022</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Num References</td>
<td style="text-align: right;">2.179</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.141</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Number of Equations</td>
<td style="text-align: right;">0.691</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.406</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Number of Proofs</td>
<td style="text-align: right;">3.343</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.069</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Number of Tables</td>
<td style="text-align: right;">0.260</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.610</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Number of Graphs/Plots</td>
<td style="text-align: right;">0.192</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.662</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Number of Other Figures</td>
<td style="text-align: right;">0.154</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.695</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Conceptualization Figures</td>
<td style="text-align: right;">0.095</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.758</td>
</tr>
<tr>
<td style="text-align: left;">Number of Authors</td>
<td style="text-align: right;">0.079</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.779</td>
</tr>
<tr>
<td style="text-align: left;">Normalized Total Tables and Figures</td>
<td style="text-align: right;">0.452</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.502</td>
</tr>
</tbody>
</table>
<h1>C Plots of Numeric Features</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Histograms of the unnormalized numeric variables considered.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Histograms of the page normalized numeric variables considered.</p>
<h1>D Contingency Tables for Nominal Features</h1>
<p>Table 5: $\chi^{2}$ test for Venue Type $(p=0.502)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tech Report</td>
<td style="text-align: center;">Workshop</td>
<td style="text-align: center;">Conference</td>
<td style="text-align: center;">Journal</td>
<td style="text-align: center;">Book</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">58.00</td>
<td style="text-align: center;">31.00</td>
<td style="text-align: center;">2.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">2.55</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">53.98</td>
<td style="text-align: center;">32.09</td>
<td style="text-align: center;">3.65</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">90.00</td>
<td style="text-align: center;">57.00</td>
<td style="text-align: center;">8.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">4.45</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">94.02</td>
<td style="text-align: center;">55.91</td>
<td style="text-align: center;">6.35</td>
</tr>
</tbody>
</table>
<p>Table 6: $\chi^{2}$ test for Author's Code being made available $(p=0.184)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Author Code Available</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">44.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">43.40</td>
<td style="text-align: center;">49.60</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">92.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">75.60</td>
<td style="text-align: center;">86.40</td>
</tr>
</tbody>
</table>
<p>Table 7: $\chi^{2}$ text for whether an Author Replied to email questions $\left(p=6.016 \times 10^{-8}\right)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Authors Reply</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">4.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">12.96</td>
<td style="text-align: center;">14.04</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">22.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">11.04</td>
<td style="text-align: center;">11.96</td>
</tr>
</tbody>
</table>
<p>Table 8: $\chi^{2}$ tests for Rigor vs Empirical ( $p=1.545 \times 10^{-9}$ ) counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Rigor vs Empirical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Empirical</td>
<td style="text-align: center;">Theory</td>
<td style="text-align: center;">Balance</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">14.00</td>
<td style="text-align: center;">53.00</td>
<td style="text-align: center;">26.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">29.18</td>
<td style="text-align: center;">30.64</td>
<td style="text-align: center;">33.19</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">66.00</td>
<td style="text-align: center;">31.00</td>
<td style="text-align: center;">65.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">50.82</td>
<td style="text-align: center;">53.36</td>
<td style="text-align: center;">57.81</td>
</tr>
</tbody>
</table>
<p>Table 9: $\chi^{2}$ tests for a paper having an Appendix $p=0.330$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Has Appendix</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">41.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">56.16</td>
<td style="text-align: center;">36.84</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">102.00</td>
<td style="text-align: center;">60.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">97.84</td>
<td style="text-align: center;">64.16</td>
</tr>
</tbody>
</table>
<p>Table 10: $\chi^{2}$ tests for when a paper "Looks Intimidating" $(p=0.829)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Looks Intimidating</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">44.00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">50.33</td>
<td style="text-align: center;">42.67</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">89.00</td>
<td style="text-align: center;">73.00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">87.67</td>
<td style="text-align: center;">74.33</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: $\chi^{2}$ test $\left(p=9.681 \times 10^{-25}\right)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Paper Readability</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Ok</td>
<td style="text-align: center;">Good</td>
<td style="text-align: center;">Excellent</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">61.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">27.35</td>
<td style="text-align: center;">20.79</td>
<td style="text-align: center;">28.45</td>
<td style="text-align: center;">16.41</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">14.00</td>
<td style="text-align: center;">33.00</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">45.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">47.65</td>
<td style="text-align: center;">36.21</td>
<td style="text-align: center;">49.55</td>
<td style="text-align: center;">28.59</td>
</tr>
</tbody>
</table>
<p>Table 12: $\chi^{2}$ tests for an Algorithm's Difficulty $\left(p=2.939 \times 10^{-5}\right)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Algorithm Difficulty</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">High</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">38.00</td>
<td style="text-align: center;">34.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">37.56</td>
<td style="text-align: center;">32.09</td>
<td style="text-align: center;">23.34</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">82.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">30.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">65.44</td>
<td style="text-align: center;">55.91</td>
<td style="text-align: center;">40.66</td>
</tr>
</tbody>
</table>
<p>Table 13: $\chi^{2}$ tests for whether a paper has Pseudo-Code $\left(p=2.308 \times 10^{-4}\right)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pseudo Code</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Step-Code</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Code-Like</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">35.00</td>
<td style="text-align: center;">3.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">28.81</td>
<td style="text-align: center;">20.79</td>
<td style="text-align: center;">37.20</td>
<td style="text-align: center;">6.20</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">58.00</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">67.00</td>
<td style="text-align: center;">14.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">50.19</td>
<td style="text-align: center;">36.21</td>
<td style="text-align: center;">64.80</td>
<td style="text-align: center;">10.80</td>
</tr>
</tbody>
</table>
<p>Table 14: $\chi^{2}$ tests for Data being Available $(p=.558)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Data Available</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">75.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">14.85</td>
<td style="text-align: center;">77.15</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">138.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">26.15</td>
<td style="text-align: center;">135.85</td>
</tr>
</tbody>
</table>
<p>Table 15: $\chi^{2}$ tests for use of an Exemplar Toy Problem $(p=0.720)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Uses Exemplar Toy Problem</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">28.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">66.74</td>
<td style="text-align: center;">26.26</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">118.00</td>
<td style="text-align: center;">44.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">116.26</td>
<td style="text-align: center;">45.74</td>
</tr>
</tbody>
</table>
<p>Table 16: $\chi^{2}$ tests for Exact Compute Used being specified $(p=0.257)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Exact Compute Used</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">76.00</td>
<td style="text-align: center;">17.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">71.85</td>
<td style="text-align: center;">21.15</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">121.00</td>
<td style="text-align: center;">41.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">125.15</td>
<td style="text-align: center;">36.85</td>
</tr>
</tbody>
</table>
<p>Table 17: $\chi^{2}$ tests for Hyperparamters being Specified $\left(p=8.450 \times 10^{-6}\right)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hyperparameters Specified</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Partial</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">5.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">20.06</td>
<td style="text-align: center;">70.02</td>
<td style="text-align: center;">2.92</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">138.00</td>
<td style="text-align: center;">3.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">34.94</td>
<td style="text-align: center;">121.98</td>
<td style="text-align: center;">5.08</td>
</tr>
</tbody>
</table>
<p>Table 18: $\chi^{2}$ tests for the level of Compute Needed $\left(p=2.788 \times 10^{-5}\right)$ counts and expectations for a paper's Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Compute Needed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reproduced</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Desktop</td>
<td style="text-align: center;">GPU</td>
<td style="text-align: center;">Server</td>
<td style="text-align: center;">Cluster</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">78.00</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">10.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">77.68</td>
<td style="text-align: center;">10.58</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">3.65</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">135.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Expected count</td>
<td style="text-align: center;">135.32</td>
<td style="text-align: center;">18.42</td>
<td style="text-align: center;">1.91</td>
<td style="text-align: center;">6.35</td>
</tr>
</tbody>
</table>
<p>Table 19: $\chi^{2}$ test for Primary Topic ( $p=7.039 \times 10^{-4}$ ) counts and expectations for a paper’s Readability towards ability to reproduce its results.</p>
<table>
<thead>
<tr>
<th>Primary Topic</th>
<th></th>
<th>Reproduced</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Bayesian</td>
<td>Count</td>
<td>6.00</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>2.19</td>
<td>3.81</td>
</tr>
<tr>
<td>Class Imbalance</td>
<td>Count</td>
<td>0.00</td>
<td>2.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>0.73</td>
<td>1.27</td>
</tr>
<tr>
<td>Classification</td>
<td>Count</td>
<td>2.00</td>
<td>8.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>3.65</td>
<td>6.35</td>
</tr>
<tr>
<td>Clustering</td>
<td>Count</td>
<td>10.00</td>
<td>14.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>8.75</td>
<td>15.25</td>
</tr>
<tr>
<td>Concept Drift</td>
<td>Count</td>
<td>0.00</td>
<td>4.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>Decision Trees</td>
<td>Count</td>
<td>2.00</td>
<td>2.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>Deep Learning</td>
<td>Count</td>
<td>1.00</td>
<td>27.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>10.21</td>
<td>17.79</td>
</tr>
<tr>
<td>Dimension Reduction</td>
<td>Count</td>
<td>4.00</td>
<td>4.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>2.92</td>
<td>5.08</td>
</tr>
<tr>
<td>Embedding</td>
<td>Count</td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>0.73</td>
<td>1.27</td>
</tr>
<tr>
<td>Ensembling</td>
<td>Count</td>
<td>7.00</td>
<td>13.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>7.29</td>
<td>12.71</td>
</tr>
<tr>
<td>Fairness</td>
<td>Count</td>
<td>4.00</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>Feature Engineering</td>
<td>Count</td>
<td>3.00</td>
<td>5.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>2.92</td>
<td>5.08</td>
</tr>
<tr>
<td>Feature Importanace</td>
<td>Count</td>
<td>0.00</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>0.36</td>
<td>0.64</td>
</tr>
<tr>
<td>Graph Classification</td>
<td>Count</td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>0.73</td>
<td>1.27</td>
</tr>
<tr>
<td>Kernel/SVMs</td>
<td>Count</td>
<td>16.00</td>
<td>20.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>13.13</td>
<td>22.87</td>
</tr>
<tr>
<td>Linear Models</td>
<td>Count</td>
<td>8.00</td>
<td>6.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>5.11</td>
<td>8.89</td>
</tr>
<tr>
<td>Meta</td>
<td>Count</td>
<td>0.00</td>
<td>4.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>NLP</td>
<td>Count</td>
<td>1.00</td>
<td>3.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>Non-Linear Other</td>
<td>Count</td>
<td>1.00</td>
<td>3.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>Online Classification</td>
<td>Count</td>
<td>3.00</td>
<td>12.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>5.47</td>
<td>9.53</td>
</tr>
<tr>
<td>Optimization</td>
<td>Count</td>
<td>6.00</td>
<td>8.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>5.11</td>
<td>8.89</td>
</tr>
<tr>
<td>Other</td>
<td>Count</td>
<td>2.00</td>
<td>2.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>Outlier Detection</td>
<td>Count</td>
<td>3.00</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.46</td>
<td>2.54</td>
</tr>
<tr>
<td>Parallel Learning</td>
<td>Count</td>
<td>3.00</td>
<td>2.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>1.82</td>
<td>3.18</td>
</tr>
<tr>
<td>Search/Retrieval</td>
<td>Count</td>
<td>5.00</td>
<td>17.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>8.02</td>
<td>13.98</td>
</tr>
<tr>
<td>Topic Modeling</td>
<td>Count</td>
<td>4.00</td>
<td>2.00</td>
</tr>
<tr>
<td></td>
<td>Expected count</td>
<td>2.19</td>
<td>3.81</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ e.g., see the GMP project as an example of a far more robust and similar project https://gmplib.org/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
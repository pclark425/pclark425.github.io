<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-991 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-991</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-991</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-b2c70c4d23c98dd4e77234fe0720595d3d565a12</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b2c70c4d23c98dd4e77234fe0720595d3d565a12" target="_blank">Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> NeurIPS Datasets and Benchmarks</p>
                <p><strong>Paper TL;DR:</strong> This work designs a suite of benchmarking RL environments and evaluates various representation learning algorithms from the literature and finds that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.</p>
                <p><strong>Paper Abstract:</strong> Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e991.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e991.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModularModel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Transition Model (Modular networks used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured world-model that factorizes observations into per-object latent variables and uses per-object transition modules (directed MLPs) to learn interactions and directed causal rules between objects; evaluated for causal induction from pixels in interactive RL environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Modular Transition Model (MTM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encoder produces a factorized representation (one vector per object). The transition model is modular: a separate MLP (module) predicts each object's next latent state while conditioning on other objects' latents and the action; learned directed interactions (rules) between object-factors allow modeling asymmetric causal effects (A -> B but not B -> A). Trained either with likelihood-based reconstruction losses (BCE for images + MSE in latent transitions) in a staged pretrain/transition/finetune schedule, or end-to-end with contrastive training (no decoder). The modular transition can represent higher-order interactions beyond pairwise by having modules incorporate other object latents as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physics (Weighted-block pushing) and Chemistry (vanilla chemistry) environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive, episodic virtual lab-style environments provided in the paper: (1) Physics: weighted-block pushing where heavier objects can push lighter ones (actions are interventions), with variants where object weight is observed via color intensity or unobserved (requiring interaction). (2) Chemistry: fixed-position objects whose color/state transitions follow an underlying DAG; environments allow perfect interventions (set variable state). Both are interactive and support active interventions and parametric manipulation of causal graph properties (size, sparsity, chain length).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Architectural factorization: per-object latent factors separate object-specific information (causal variables) from irrelevant visual features (distractors like shape). Directed per-object transition modules learn causal interactions on these factors; combined with training protocols (contrastive training or reconstruction pretraining) this reduces reliance on irrelevant cues.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant / nuisance visual features (e.g., object shape used as a distractor), measurement/representation entanglement from raw pixels, and distributional shifts in observed correlates (e.g., colors not predictive across episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit: by factorizing representation (separate latents per object) and learning transitions that depend on causal factors, the model reduces influence of distractor features; when trained contrastively, the objective (matching predicted and actual future latents against negatives) further encourages latents to capture predictive (causal) information rather than distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In the paper's experiments on the Observed Physics environment (3 objects), Modular+Contrastive achieved H@1 = 98.73±1.04 (1-step), 94.7±4.2 (5-step), 90.6±6.97 (10-step) and high MRR and downstream RL success rates (modular models achieved highest downstream planning success vs monolithic baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Monolithic models (AE/VAE) trained with NLL performed much worse in long-horizon prediction and downstream planning; e.g., for Observed Physics (3 objects) AE with NLL had 10-step H@1 = 40.46±3.48 (contrast: Modular+Contrastive 90.6±6.97).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>object-specific distractors present per object (experiments with 3 and 5 objects; shapes treated as distractors), i.e., 3-5 distractor-bearing objects</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Modularity (factorized object latents + directed per-object transition modules) is an effective inductive bias for causal induction from pixels in interactive environments; it helps separate causal variables from visual distractors (e.g., shapes), scales better to larger graphs and longer cause-effect chains than monolithic models, and — when combined with contrastive training — achieves substantially better long-horizon predictive ranking metrics and downstream planning success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e991.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e991.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C-SWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Structured World Model (C-SWM / GNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph neural network style structured world-model that factorizes state into object slots and models pairwise interactions via a relational network; trained with a contrastive objective to match predicted next-slot embeddings to true next-slot embeddings against negative samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Contrastive learning of structured world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>C-SWM / GNN (pairwise relational GNN with contrastive objective)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Object-wise encoder produces slot embeddings; a graph neural network (message-passing) computes pairwise interactions (node-to-edge, edge-to-node propagations) to produce next-slot predictions. Training can be contrastive: predicted slot embeddings are pulled close to true next-step embeddings and pushed away from negative (shuffled) embeddings, implementing negative sampling. In the paper, C-SWM style GNNs model pairwise symmetric/undirected interactions and are permutation invariant.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physics and Chemistry environments (same as above)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive virtual labs with ability to perform perfect interventions; environments allow specifying underlying causal graph shape (chain, collider, full), tuning stochasticity, number of objects and colors; interventions correspond to RL actions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant visual features (shapes) are present as distractors in experiments but GNNs do not include an explicit distractor-removal mechanism in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>C-SWM/GNN with contrastive training performed well on pairwise/collider graphs and small graphs: e.g., on Observed Physics (3 objects) GNN+Contrastive had 10-step H@1 = 72.06±19.38 and competitive MRR; it outperforms monolithic models on many ranking metrics and downstream tasks in some small-graph settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>GNNs trained with NLL (likelihood) often performed worse: e.g., Observed Physics (3 objects) GNN with NLL 10-step H@1 = 6.4±3.51 (very poor), showing sensitivity to training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>3-5 object-associated distractors in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GNNs (C-SWM) capture pairwise interactions and do well on collider/short-chain graphs, but they struggle with long cause-effect chains and larger graphs compared to modular directed models; contrastive training substantially improves GNN predictions relative to NLL training, but GNNs do not explicitly remove distractor features and can underperform on tasks requiring modeling long directed causal chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e991.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e991.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ContrastiveTraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive training (negative-sample latent matching)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training objective that learns latent dynamics by matching predicted next-step embeddings to true next-step embeddings and contrasting them with negative (shuffled) embeddings, used here to train world models without a decoder and to improve long-horizon predictive robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Contrastive objective with negative sampling</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The model predicts next-step latent embedding(s) and the loss pulls predicted embeddings toward the encoder-produced true next-step embeddings (MSE) while pushing them away from negative samples (randomly shuffled batch examples) using a hinge-like margin term: minimize H + max(0, γ − ~H) where H is MSE(predicted, true) and ~H is MSE(predicted, negative). This decoder-free setup focuses learning on predictive latent features and avoids reconstructing pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physics and Chemistry environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive virtual lab environments supporting perfect interventions and parametric control of causal graph properties; used to evaluate whether contrastive training improves causal induction and robustness to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Implicit robust representation learning via contrastive objective: by forcing latent predictions to be informative for future states and to discriminate true futures from negatives, the objective biases latents to encode predictive (causal) features rather than spurious/distractor features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant visual cues (shapes), non-predictive correlations in pixels, and noise in observations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit downweighting through objective: features not predictive of next-step latents fail to reduce contrastive loss and are thus de-emphasized during training.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Across experiments, contrastively trained structured models (Modular, GNN) generally outperformed likelihood-trained versions on ranking metrics and often on downstream RL: e.g., Modular+Contrastive (Observed Physics, 10-step, 3 objects) H@1 = 90.6±6.97 vs Modular+NLL Finetuned H@1 = 48.7±16.19.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Likelihood (NLL) trained models (with decoders) frequently had worse long-horizon ranking metrics and downstream planning; e.g., AE with NLL 10-step H@1 = 40.46±3.48 in Observed Physics (3 objects) vs AE+Contrastive 10-step H@1 = 34.36±8.42 (contrastive improved most for structured models).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Experiments include 3 and 5 objects (distractors per object); results reported separately showing contrastive benefit across these scales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrastive training concentrates learning on predictive latent structure and, when combined with structural inductive biases (modularity or GNN), yields improved robustness to spurious visual correlates and better long-horizon predictive and planning performance; contrastive objectives outperform plain likelihood objectives for structured models in many setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e991.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e991.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InterventionEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intervention-based evaluation & zero-shot transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation protocols that measure a model's causal induction ability by predicting outcomes of interventions, performing zero-shot transfer to altered causal graphs, and measuring downstream RL performance (planning), used as surrogate metrics for causal discovery from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Predicting Intervention Outcomes & Zero-Shot Transfer evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Predicting Intervention Outcomes: evaluate model predictions on held-out episodes that include randomly sampled interventions (actions) and compare predicted next states (or rewards/planning success) against actual environment outcomes; Zero-Shot Transfer: test generalization on new environments whose causal graphs differ from training but share structure (e.g., new color intensities mapping to weights) to detect whether learned representation encodes causal mechanisms rather than spurious correlates. Downstream RL (greedy planning using learned transition + reward predictor) measures practical consequence of causal induction.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Physics and Chemistry environments (interactive virtual labs with interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments support 'perfect' interventions (set variable state), allow specification of ground-truth causal graph (DAG) structure (chain, collider, full), and permit generation of test episodes with novel interventions or modified graphs for transfer tests; interactive and intervention-capable.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Used to reveal spurious correlations arising from visual distractors, unobserved variables, and distributional shifts (e.g., new colors not seen during training).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Interventional mismatch detection: if model predictions under interventions deviate from true interventional outcomes or fail to transfer under zero-shot changes (e.g., new color -> weight mappings), this indicates reliance on spurious correlations; ranking metrics (H@1, MRR), reconstruction loss, and downstream RL success are used as diagnostic signals.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation by intervention: performing interventions that break observational correlations (e.g., change color-weight mapping) to show that models which learned causal mechanisms still predict correctly while spurious models fail.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Random sampling of interventions in test episodes (not actively optimized experiment design in this paper); evaluation includes held-out random interventions and transfer cases where environment statistics change.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Models with modular structure and contrastive training show superior intervention prediction and zero-shot transfer: e.g., modular models maintain higher ranking metrics and downstream planning success in transfer tasks compared to monolithic baselines (reported across many tables/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Models that rely on spurious visual correlates (e.g., monolithic AE/VAE without factorization) frequently fail transfer tests and downstream RL planning even when ranking/reconstruction metrics appear acceptable; some models had good ranking metrics but poor downstream RL (revealing spurious-leveraging representations).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Evaluation sets include settings where non-causal visual features (shapes) are distractors across 3- and 5-object experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Intervention-based evaluation and zero-shot transfer are necessary diagnostics for detecting spurious correlations: models that score well on reconstruction or short-horizon ranking metrics can still fail interventions and planning; structured + contrastive models better predict interventional outcomes and generalize to changes that would break spurious correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e991.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e991.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhysicsEnv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted-block pushing physics environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal interactive physics virtual lab where blocks of unique weights interact (heavier pushes lighter); designed to test causal induction because weight (causal variable) can be observed via color intensity or unobserved, and other visual features (shape) act as distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Weighted-block pushing environment (physics)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Environment of M objects where each object has position, shape, color and weight; heavier objects can push lighter ones (acyclic tournament causal graph defined by weight ordering). Settings: Fully observed (weight encoded by color intensity), Unobserved (weights need to be inferred via interaction), and FixedUnobserved (fixed shape-weight mapping to ease identity). Actions are interventions (move a particular block); transitions are deterministic/stochastic according to setup and interventions cut parent edges for target variable.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Weighted-block pushing (physics)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive, episodic, intervention-capable virtual lab; allows varying observability (observed/unobserved), number of objects (3 or 5 in experiments), and acts as a controlled testbed for distractors (shapes) and spurious correlates (color-shape associations).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant visual features (object shape), non-causal correlations between shape and weight in some variants, unobserved causal variables (weight when color not informative).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Agents must perform interventions (move objects) to reveal unobserved weights in Unobserved settings; evaluation includes fixed and randomized interventions to probe causal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>3 or 5 object shapes acting as distractors in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Designed to expose spurious correlates: when weight is unobserved, models must perform or reason about interventions to discover causal relations. Shape serves as distractor; modular factorization and contrastive training improve robustness to these distractors compared to monolithic models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e991.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e991.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemistryEnv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Chemistry environment (DAG-based color-state interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An environment simulating chemical-like state changes on fixed-position objects where state transitions follow a user-specified DAG; interventions change an object's color/state and probabilistically affect descendants via CPTs, enabling controlled studies of arbitrary causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Chemistry environment (DAG reaction simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fixed-position objects each have discrete colors/states; an underlying causal DAG defines which objects influence which others. Interventions set an object's color unconditionally (cutting parental influence). Transitions are stochastic according to conditional probability tables (CPTs). The environment allows tuning: graph type (chain/collider/full/random DAG), stochasticity, number of objects and colors, enabling systematic exploration of sparsity and cause-effect chain length.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Chemistry environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive, episodic virtual lab supporting perfect interventions and parametrically variable DAGs; designed to test models on chain, collider and full graphs and to vary stochasticity and number of colors/objects.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Confounding due to unobserved variables can be simulated by leaving some variables unobserved; irrelevant visual features may exist but objects are fixed in position to ease identity.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Interventions are provided as actions; evaluation uses predicted outcomes of interventions, zero-shot transfer to different graphs, and downstream planning to measure causal learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Experiments vary number of objects; distractor count not primary but can be introduced via extra visual features; typical experiments used small graphs (3-5 nodes) and larger graphs for scaling tests.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chemistry environment enables testing of causal discovery across varied DAG structures and stochasticities; modular models outperform others on chain and full graphs (longer chains), showing architectural inductive bias advantages for discovering directed causal structure from observations and interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Contrastive learning of structured world models <em>(Rating: 2)</em></li>
                <li>Learning neural causal models from unknown interventions <em>(Rating: 2)</em></li>
                <li>Amortized learning of neural causal representations <em>(Rating: 2)</em></li>
                <li>Causally correct partial models for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Causal induction from visual observations for goal directed tasks <em>(Rating: 1)</em></li>
                <li>Toward causal representation learning <em>(Rating: 1)</em></li>
                <li>Causal confusion in imitation learning <em>(Rating: 1)</em></li>
                <li>Peters et al. - Causal inference by using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-991",
    "paper_id": "paper-b2c70c4d23c98dd4e77234fe0720595d3d565a12",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "ModularModel",
            "name_full": "Modular Transition Model (Modular networks used in this paper)",
            "brief_description": "A structured world-model that factorizes observations into per-object latent variables and uses per-object transition modules (directed MLPs) to learn interactions and directed causal rules between objects; evaluated for causal induction from pixels in interactive RL environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Modular Transition Model (MTM)",
            "method_description": "Encoder produces a factorized representation (one vector per object). The transition model is modular: a separate MLP (module) predicts each object's next latent state while conditioning on other objects' latents and the action; learned directed interactions (rules) between object-factors allow modeling asymmetric causal effects (A -&gt; B but not B -&gt; A). Trained either with likelihood-based reconstruction losses (BCE for images + MSE in latent transitions) in a staged pretrain/transition/finetune schedule, or end-to-end with contrastive training (no decoder). The modular transition can represent higher-order interactions beyond pairwise by having modules incorporate other object latents as inputs.",
            "environment_name": "Physics (Weighted-block pushing) and Chemistry (vanilla chemistry) environments",
            "environment_description": "Interactive, episodic virtual lab-style environments provided in the paper: (1) Physics: weighted-block pushing where heavier objects can push lighter ones (actions are interventions), with variants where object weight is observed via color intensity or unobserved (requiring interaction). (2) Chemistry: fixed-position objects whose color/state transitions follow an underlying DAG; environments allow perfect interventions (set variable state). Both are interactive and support active interventions and parametric manipulation of causal graph properties (size, sparsity, chain length).",
            "handles_distractors": true,
            "distractor_handling_technique": "Architectural factorization: per-object latent factors separate object-specific information (causal variables) from irrelevant visual features (distractors like shape). Directed per-object transition modules learn causal interactions on these factors; combined with training protocols (contrastive training or reconstruction pretraining) this reduces reliance on irrelevant cues.",
            "spurious_signal_types": "Irrelevant / nuisance visual features (e.g., object shape used as a distractor), measurement/representation entanglement from raw pixels, and distributional shifts in observed correlates (e.g., colors not predictive across episodes).",
            "detection_method": null,
            "downweighting_method": "Implicit: by factorizing representation (separate latents per object) and learning transitions that depend on causal factors, the model reduces influence of distractor features; when trained contrastively, the objective (matching predicted and actual future latents against negatives) further encourages latents to capture predictive (causal) information rather than distractors.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "In the paper's experiments on the Observed Physics environment (3 objects), Modular+Contrastive achieved H@1 = 98.73±1.04 (1-step), 94.7±4.2 (5-step), 90.6±6.97 (10-step) and high MRR and downstream RL success rates (modular models achieved highest downstream planning success vs monolithic baselines).",
            "performance_without_robustness": "Monolithic models (AE/VAE) trained with NLL performed much worse in long-horizon prediction and downstream planning; e.g., for Observed Physics (3 objects) AE with NLL had 10-step H@1 = 40.46±3.48 (contrast: Modular+Contrastive 90.6±6.97).",
            "has_ablation_study": true,
            "number_of_distractors": "object-specific distractors present per object (experiments with 3 and 5 objects; shapes treated as distractors), i.e., 3-5 distractor-bearing objects",
            "key_findings": "Modularity (factorized object latents + directed per-object transition modules) is an effective inductive bias for causal induction from pixels in interactive environments; it helps separate causal variables from visual distractors (e.g., shapes), scales better to larger graphs and longer cause-effect chains than monolithic models, and — when combined with contrastive training — achieves substantially better long-horizon predictive ranking metrics and downstream planning success.",
            "uuid": "e991.0",
            "source_info": {
                "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "C-SWM",
            "name_full": "Contrastive Structured World Model (C-SWM / GNN)",
            "brief_description": "A graph neural network style structured world-model that factorizes state into object slots and models pairwise interactions via a relational network; trained with a contrastive objective to match predicted next-slot embeddings to true next-slot embeddings against negative samples.",
            "citation_title": "Contrastive learning of structured world models",
            "mention_or_use": "use",
            "method_name": "C-SWM / GNN (pairwise relational GNN with contrastive objective)",
            "method_description": "Object-wise encoder produces slot embeddings; a graph neural network (message-passing) computes pairwise interactions (node-to-edge, edge-to-node propagations) to produce next-slot predictions. Training can be contrastive: predicted slot embeddings are pulled close to true next-step embeddings and pushed away from negative (shuffled) embeddings, implementing negative sampling. In the paper, C-SWM style GNNs model pairwise symmetric/undirected interactions and are permutation invariant.",
            "environment_name": "Physics and Chemistry environments (same as above)",
            "environment_description": "Interactive virtual labs with ability to perform perfect interventions; environments allow specifying underlying causal graph shape (chain, collider, full), tuning stochasticity, number of objects and colors; interventions correspond to RL actions.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Irrelevant visual features (shapes) are present as distractors in experiments but GNNs do not include an explicit distractor-removal mechanism in this paper.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "C-SWM/GNN with contrastive training performed well on pairwise/collider graphs and small graphs: e.g., on Observed Physics (3 objects) GNN+Contrastive had 10-step H@1 = 72.06±19.38 and competitive MRR; it outperforms monolithic models on many ranking metrics and downstream tasks in some small-graph settings.",
            "performance_without_robustness": "GNNs trained with NLL (likelihood) often performed worse: e.g., Observed Physics (3 objects) GNN with NLL 10-step H@1 = 6.4±3.51 (very poor), showing sensitivity to training objective.",
            "has_ablation_study": true,
            "number_of_distractors": "3-5 object-associated distractors in experiments",
            "key_findings": "GNNs (C-SWM) capture pairwise interactions and do well on collider/short-chain graphs, but they struggle with long cause-effect chains and larger graphs compared to modular directed models; contrastive training substantially improves GNN predictions relative to NLL training, but GNNs do not explicitly remove distractor features and can underperform on tasks requiring modeling long directed causal chains.",
            "uuid": "e991.1",
            "source_info": {
                "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "ContrastiveTraining",
            "name_full": "Contrastive training (negative-sample latent matching)",
            "brief_description": "A training objective that learns latent dynamics by matching predicted next-step embeddings to true next-step embeddings and contrasting them with negative (shuffled) embeddings, used here to train world models without a decoder and to improve long-horizon predictive robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Contrastive objective with negative sampling",
            "method_description": "The model predicts next-step latent embedding(s) and the loss pulls predicted embeddings toward the encoder-produced true next-step embeddings (MSE) while pushing them away from negative samples (randomly shuffled batch examples) using a hinge-like margin term: minimize H + max(0, γ − ~H) where H is MSE(predicted, true) and ~H is MSE(predicted, negative). This decoder-free setup focuses learning on predictive latent features and avoids reconstructing pixels.",
            "environment_name": "Physics and Chemistry environments",
            "environment_description": "Interactive virtual lab environments supporting perfect interventions and parametric control of causal graph properties; used to evaluate whether contrastive training improves causal induction and robustness to distractors.",
            "handles_distractors": true,
            "distractor_handling_technique": "Implicit robust representation learning via contrastive objective: by forcing latent predictions to be informative for future states and to discriminate true futures from negatives, the objective biases latents to encode predictive (causal) features rather than spurious/distractor features.",
            "spurious_signal_types": "Irrelevant visual cues (shapes), non-predictive correlations in pixels, and noise in observations.",
            "detection_method": null,
            "downweighting_method": "Implicit downweighting through objective: features not predictive of next-step latents fail to reduce contrastive loss and are thus de-emphasized during training.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Across experiments, contrastively trained structured models (Modular, GNN) generally outperformed likelihood-trained versions on ranking metrics and often on downstream RL: e.g., Modular+Contrastive (Observed Physics, 10-step, 3 objects) H@1 = 90.6±6.97 vs Modular+NLL Finetuned H@1 = 48.7±16.19.",
            "performance_without_robustness": "Likelihood (NLL) trained models (with decoders) frequently had worse long-horizon ranking metrics and downstream planning; e.g., AE with NLL 10-step H@1 = 40.46±3.48 in Observed Physics (3 objects) vs AE+Contrastive 10-step H@1 = 34.36±8.42 (contrastive improved most for structured models).",
            "has_ablation_study": true,
            "number_of_distractors": "Experiments include 3 and 5 objects (distractors per object); results reported separately showing contrastive benefit across these scales.",
            "key_findings": "Contrastive training concentrates learning on predictive latent structure and, when combined with structural inductive biases (modularity or GNN), yields improved robustness to spurious visual correlates and better long-horizon predictive and planning performance; contrastive objectives outperform plain likelihood objectives for structured models in many setups.",
            "uuid": "e991.2",
            "source_info": {
                "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "InterventionEval",
            "name_full": "Intervention-based evaluation & zero-shot transfer",
            "brief_description": "Evaluation protocols that measure a model's causal induction ability by predicting outcomes of interventions, performing zero-shot transfer to altered causal graphs, and measuring downstream RL performance (planning), used as surrogate metrics for causal discovery from pixels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Predicting Intervention Outcomes & Zero-Shot Transfer evaluation",
            "method_description": "Predicting Intervention Outcomes: evaluate model predictions on held-out episodes that include randomly sampled interventions (actions) and compare predicted next states (or rewards/planning success) against actual environment outcomes; Zero-Shot Transfer: test generalization on new environments whose causal graphs differ from training but share structure (e.g., new color intensities mapping to weights) to detect whether learned representation encodes causal mechanisms rather than spurious correlates. Downstream RL (greedy planning using learned transition + reward predictor) measures practical consequence of causal induction.",
            "environment_name": "Physics and Chemistry environments (interactive virtual labs with interventions)",
            "environment_description": "Environments support 'perfect' interventions (set variable state), allow specification of ground-truth causal graph (DAG) structure (chain, collider, full), and permit generation of test episodes with novel interventions or modified graphs for transfer tests; interactive and intervention-capable.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Used to reveal spurious correlations arising from visual distractors, unobserved variables, and distributional shifts (e.g., new colors not seen during training).",
            "detection_method": "Interventional mismatch detection: if model predictions under interventions deviate from true interventional outcomes or fail to transfer under zero-shot changes (e.g., new color -&gt; weight mappings), this indicates reliance on spurious correlations; ranking metrics (H@1, MRR), reconstruction loss, and downstream RL success are used as diagnostic signals.",
            "downweighting_method": null,
            "refutation_method": "Refutation by intervention: performing interventions that break observational correlations (e.g., change color-weight mapping) to show that models which learned causal mechanisms still predict correctly while spurious models fail.",
            "uses_active_learning": false,
            "inquiry_strategy": "Random sampling of interventions in test episodes (not actively optimized experiment design in this paper); evaluation includes held-out random interventions and transfer cases where environment statistics change.",
            "performance_with_robustness": "Models with modular structure and contrastive training show superior intervention prediction and zero-shot transfer: e.g., modular models maintain higher ranking metrics and downstream planning success in transfer tasks compared to monolithic baselines (reported across many tables/figures).",
            "performance_without_robustness": "Models that rely on spurious visual correlates (e.g., monolithic AE/VAE without factorization) frequently fail transfer tests and downstream RL planning even when ranking/reconstruction metrics appear acceptable; some models had good ranking metrics but poor downstream RL (revealing spurious-leveraging representations).",
            "has_ablation_study": true,
            "number_of_distractors": "Evaluation sets include settings where non-causal visual features (shapes) are distractors across 3- and 5-object experiments.",
            "key_findings": "Intervention-based evaluation and zero-shot transfer are necessary diagnostics for detecting spurious correlations: models that score well on reconstruction or short-horizon ranking metrics can still fail interventions and planning; structured + contrastive models better predict interventional outcomes and generalize to changes that would break spurious correlates.",
            "uuid": "e991.3",
            "source_info": {
                "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "PhysicsEnv",
            "name_full": "Weighted-block pushing physics environment",
            "brief_description": "A minimal interactive physics virtual lab where blocks of unique weights interact (heavier pushes lighter); designed to test causal induction because weight (causal variable) can be observed via color intensity or unobserved, and other visual features (shape) act as distractors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Weighted-block pushing environment (physics)",
            "method_description": "Environment of M objects where each object has position, shape, color and weight; heavier objects can push lighter ones (acyclic tournament causal graph defined by weight ordering). Settings: Fully observed (weight encoded by color intensity), Unobserved (weights need to be inferred via interaction), and FixedUnobserved (fixed shape-weight mapping to ease identity). Actions are interventions (move a particular block); transitions are deterministic/stochastic according to setup and interventions cut parent edges for target variable.",
            "environment_name": "Weighted-block pushing (physics)",
            "environment_description": "Interactive, episodic, intervention-capable virtual lab; allows varying observability (observed/unobserved), number of objects (3 or 5 in experiments), and acts as a controlled testbed for distractors (shapes) and spurious correlates (color-shape associations).",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Irrelevant visual features (object shape), non-causal correlations between shape and weight in some variants, unobserved causal variables (weight when color not informative).",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Agents must perform interventions (move objects) to reveal unobserved weights in Unobserved settings; evaluation includes fixed and randomized interventions to probe causal structure.",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": "3 or 5 object shapes acting as distractors in experiments",
            "key_findings": "Designed to expose spurious correlates: when weight is unobserved, models must perform or reason about interventions to discover causal relations. Shape serves as distractor; modular factorization and contrastive training improve robustness to these distractors compared to monolithic models.",
            "uuid": "e991.4",
            "source_info": {
                "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "ChemistryEnv",
            "name_full": "Vanilla Chemistry environment (DAG-based color-state interactions)",
            "brief_description": "An environment simulating chemical-like state changes on fixed-position objects where state transitions follow a user-specified DAG; interventions change an object's color/state and probabilistically affect descendants via CPTs, enabling controlled studies of arbitrary causal graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Chemistry environment (DAG reaction simulator)",
            "method_description": "Fixed-position objects each have discrete colors/states; an underlying causal DAG defines which objects influence which others. Interventions set an object's color unconditionally (cutting parental influence). Transitions are stochastic according to conditional probability tables (CPTs). The environment allows tuning: graph type (chain/collider/full/random DAG), stochasticity, number of objects and colors, enabling systematic exploration of sparsity and cause-effect chain length.",
            "environment_name": "Chemistry environment",
            "environment_description": "Interactive, episodic virtual lab supporting perfect interventions and parametrically variable DAGs; designed to test models on chain, collider and full graphs and to vary stochasticity and number of colors/objects.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Confounding due to unobserved variables can be simulated by leaving some variables unobserved; irrelevant visual features may exist but objects are fixed in position to ease identity.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Interventions are provided as actions; evaluation uses predicted outcomes of interventions, zero-shot transfer to different graphs, and downstream planning to measure causal learning.",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": "Experiments vary number of objects; distractor count not primary but can be introduced via extra visual features; typical experiments used small graphs (3-5 nodes) and larger graphs for scaling tests.",
            "key_findings": "Chemistry environment enables testing of causal discovery across varied DAG structures and stochasticities; modular models outperform others on chain and full graphs (longer chains), showing architectural inductive bias advantages for discovering directed causal structure from observations and interventions.",
            "uuid": "e991.5",
            "source_info": {
                "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Contrastive learning of structured world models",
            "rating": 2
        },
        {
            "paper_title": "Learning neural causal models from unknown interventions",
            "rating": 2
        },
        {
            "paper_title": "Amortized learning of neural causal representations",
            "rating": 2
        },
        {
            "paper_title": "Causally correct partial models for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Causal induction from visual observations for goal directed tasks",
            "rating": 1
        },
        {
            "paper_title": "Toward causal representation learning",
            "rating": 1
        },
        {
            "paper_title": "Causal confusion in imitation learning",
            "rating": 1
        },
        {
            "paper_title": "Peters et al. - Causal inference by using invariant prediction: identification and confidence intervals",
            "rating": 2
        },
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 2
        }
    ],
    "cost": 0.01841625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning</h1>
<p>Nan Rosemary Ke ${ }^{<em>, 1,2}$ Aniket Didolkar ${ }^{</em>, 3}$ Sarthak Mittal ${ }^{3}$ Anirudh Goyal ${ }^{3}$<br>Guillaume Lajoie ${ }^{3}$ Stefan Bauer ${ }^{6}$ Danilo Rezende ${ }^{2}$<br>Yoshua Bengio ${ }^{3, \dagger}$ Michael Mozer ${ }^{5}$ Christopher Pal ${ }^{1,4}$</p>
<h4>Abstract</h4>
<p>Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.</p>
<h2>1 Introduction</h2>
<p>Deep learning methods have made immense progress on many reinforcement learning (RL) tasks in recent years. However, the performance of these methods still pales in comparison to human abilities in many cases. Contemporary deep reinforcement learning models have a ways to go to achieve robust generalization [Nichol et al., 2018], efficient planning over flexible timescales [Silver and Ciosek, 2012], and long-term credit assignment [Osband et al., 2019]. Model-based methods in RL (MBRL) can potentially mitigate this issue [Schrittwieser et al., 2019]. These methods observe sequences of state-action pairs, and from these observations are able to learn a self-supervised model of the environment. With a well-trained world model, these algorithms can then simulate the environment and look ahead to future events to establish better value estimates, without requiring expensive interactions with the environment [Sutton, 1991]. Model-based methods can thus be far more sample-efficient than their model-free counterparts when multiple objectives are to be achieved in the same environment. However, for model-based approaches to be successful, the learned models must capture relevant mechanisms that guide the world, i.e., they must discover the right causal variables and structure. Indeed, models sensitive to causality have been shown to be robust and easily transferable [Bengio et al., 2019, Ke et al., 2019]. As a result, there has been a recent surge of interest</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a)-(d): Different aspects contributing to the complexity of causal graphs. (i), (ii): Difference between observational and interventional data. In RL setting, actions are interventions in the environment. The hammer denotes an intervention. Intervention on a variable not only affects its direct children, but also all reachable variables. Variables impacted by the intervention have a darker shade.</p>
<p>In learning causal models for deep reinforcement learning [de Haan et al., 2019, Dasgupta et al., 2019, Nair et al., 2019, Goyal et al., 2019, Goyal and Bengio, 2020, Rezende et al., 2020, Wang et al., 2021, Schölkopf et al., 2021]. Yet, many challenges remain, and a systematic framework to modulate environment causality structure and evaluate models' capacity to capture it is currently lacking, which motivates this paper.</p>
<p>What limits the use of causal modeling approaches in many AI tasks and realistic RL settings is that most of the current causal learning literature presumes abstract domain representations in which the cause and effect variables are explicit and given [Pearl, 2009]. Methods are needed to automate the inference and identification of such causal variables (i.e., <em>causal induction</em>) from low-level state representations (like images). Although one solution is manual labeling, it is often impractical and in some cases impossible to manually label all the causal variables. In some domains, the causal structure may not be known. Further, critical causal variables may change from one task to another, or from one environment to another. And in unknown environments, one ideally aims for an RL agent that could induce the causal structure of the environment from observations and interventions.</p>
<p>In this work, we seek to evaluate various model-based approaches parameterized to exploit structure of environments purposefully designed to modulate causal relations. We find that modular network architectures appear particularly well suited for causal learning. Our conjecture is that causality can provide a useful source of inductive bias to improve the learning of world models.</p>
<h3>Shortcomings of current RL development environments, and a path forward.</h3>
<p>Most existing RL environments are not a good fit for investigating causal induction in MBRL, as they have a single fixed causal graph, lack proper evaluation and have entangled aspects of causal learning. For instance, many tasks have complicated causal structures as well as unobserved confounders. These issues make it difficult to measure progress for causal learning. As we look towards the next great challenges for RL and AI, there is a need to better understand the implications of varying different aspects of the underlying causal graph for various learning procedures.</p>
<p>Hence, to systematically study various aspects of causal induction (i.e., learning the right causal graph from pixel data), we propose a new suite of environments as a platform for investigating inductive biases, causal representations, and learning algorithms. The goal is to disentangle distinct aspects of causal learning by allowing the user to choose and modulate various properties of the ground truth causal graph, such as the structure and size of the graph, the sparsity of the graph and whether variables are observed or not (see Figure 1 (a)-(d)). We also provide evaluation criteria for measuring causal induction in MBRL that we argue help measure progress and facilitate further research in these directions. We believe that the availability of standard experiments and a platform that can easily be extended to test different aspects of causal modeling will play a significant role in speeding up progress in MBRL.</p>
<h3>Insights and causally sufficient inductive biases.</h3>
<p>Using our platform, we investigate the impact of explicit structure and modularity for causal induction in MBRL. We evaluated two typical of monolithic models (autoencoders and variational autoencoders) and two typical models with explicit structure: graph neural networks (GNNs) and modular models (shown in Figure 5). Graph neural networks (GNNs) have a factorized representation of variables and can model undirected relationships between variables. Modular models also have a factorized representation of variables, along with directed edges between variables which can model directed relationship such as A causing B, but not the other way around. We investigated the performance of such structured approaches on learning from causal graphs with varying complexity, such as the size of the graph, the sparsity of the graph and the length of cause-effect chains (Figure 1 (a) - (d)).</p>
<p>The proposed environment gives novel insights in a number of settings. Especially, we found that even our naive implementation of modular networks can scale significantly better compared to other</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the key features of the suite. Environments have objects that interact according to the underlying causal graph which can be based on a subset of objects' properties. An efficient model should be able to infer the high level causal variables from raw pixel data and learn the underlying causal graph through interactions between these high level causal variables.
models (including graph neural networks). This suggests that explicit structure and modularity such as factorized representations and directed edges between variables help with causal induction in MBRL. We also found that graph neural networks, such as the ones from Kipf et al. [2019] are good at modeling pairwise interactions and significantly outperform monolithic models under this setting. However, they have difficulty modeling complex causal graphs with long cause-effect chains, such as the chain graph (demonstration of chain graphs are found in Figure 1 (i)). Another finding is that evaluation metrics such as likelihood and ranking loss do not always correspond to the performance of these models in downstream RL tasks.</p>
<h1>2 Environments for causal induction in model-based RL</h1>
<p>Causal models are frequently described using graphs in which the edges represent causal relationships. In these structural causal models, the existence of a directed edge from $A$ to $B$ indicates that intervening on $A$ directly impacts $B$, and the absence of an edge indicates no direct interventional impact (see Appendix B for formal definitions).</p>
<p>In parallel, world models in MBRL describe the underlying data generating process of the environment by modeling the next state given the current state-action pair, where the actions are interventions in the environment. Hence, learning world models in MBRL can be seen as a causal induction problem. Below, we first outline how a collection of simple causal structures can capture real-world MBRL cases, and we propose a set of elemental environments to express them for training. Second, we describe precise ways to evaluate models in these environments.</p>
<h3>2.1 Mini-environments: explicit cases for causal modulation in RL</h3>
<p>The ease with which an agent learns a task greatly depends on the structure of the environment's underlying causal graph. For example, it might be easier to learn causal relationships in a collider graph ( see Figure 1(a)) where all interactions are pairwise, meaning that an intervention on one variable $X_{i}$ impacts no more than one other variable $X_{j}$, hence the cause-effect chain has a length of at most 1. However, causal graphs such as full graphs (see Figure 1 (a)) can have more complex causal interactions, where intervening on one variable impacts can impact up to $n-1$ variables for graphs of size $n$ (see Figure 1). Therefore, one important aspect of understanding a model's performance on causal induction in MBRL is to analyze how well the model performs on causal graphs of varying complexity.
Impotant factors that contribute to the complexity of discovering the causal graph are the structure, size, sparsity of edges and length of cause-effect chains of the causal graph (Figure 1). Presence of unobserved variables also adds to the complexity. The size of the graph increases complexity because the number of possible graphs grows super-exponentially with the size of the graph [Eaton and Murphy, 2007, Peters et al., 2016, Ke et al., 2019]. The sparsity of graphs also impacts the difficulty of learning, as observed in [Ke et al., 2019]. Given graphs of the same size, denser graphs are often more challenging to learn. Futhermore, the length of the cause-effect chains can also impact learning. We have observed in our experiments, that graphs with shorter cause-effect lengths such as colliders (Figure 1 (a)) can be easier to model as compared to chain graphs with longer cause-effect chains. Finally, unobserved variables which commonly exist in the real-world can greatly impact learning, especially if they are confounding causes (shared causes of observed variables).
Taking these factors into account, we designed two suites of (toy) environments: the physics environment and the chemistry environment, which we discuss in more detail in the fol-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Demonstration of the weighted-block pushing environment (left: observed, right: unobserved) along with the feasible generalizations that the setup provides.
lowing section. They are designed with a focus on the underlying causal graph and thus have a minimalist design that is easy to visualize.</p>
<h1>2.1.1 Physics environment: Weighted-block pushing</h1>
<p>The physics environment simulates very simple physics in the world. It consists of blocks of different, unique weights. The rule for interaction between blocks is that heavier objects can push lighter ones. Interventions ammount to move a particular block, and the consequence depends on whether the block next to it (if present) is heavier or lighter. For an accurate world model, inferring the weights becomes essential. Additionally, one can allow the weight of the objects to be either observed through the intensity of the color, or unobserved, leading to two environment settings described below. The underlying causal graph is an acyclic tournament, shown in Figure 3. For more details about the setup, please refer to Appendix F.</p>
<p>Fully observed setting. In the fully observed setting, all objects are given a particular color and the weight of each block is represented by the intensity of the color. Once the agent learns this underlying causal structure, it does not have to perform interventions on new objects in order to infer they will interact with the others.</p>
<p>Unobserved setting. In this setting, the weight of each object is not directly observable by its color. The agent thus needs to interact with the object in order to understand the order of weights associated with the blocks. In this case, the weight of objects needs to be inferred through interventions. We consider two sub-divisions of this setting - FixedUnobserved where there is a fixed assignment between the shapes of the objects and their weights and Unobserved where there is no fixed assignment between the shape and the weight, hence making it a more challenging environment. We refer the reader to Appendix F. 2 for details.</p>
<h3>2.1.2 Chemistry environment</h3>
<p>The chemistry environment enables more complexity in the causal structure of the world by allowing arbitrary causal graphs. This is depicted by simple chemical reactions, where the state of an element can cause changes to another variable's state. The environment consists of a number of objects whose positions are kept fixed and thus, uniquely identifiable.
The interactions between different objects take place according to the underlying causal graph which can either be a randomly generated DAG, or specified by the
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Demonstration of the vanilla chemistry environment (left: ground truth causal graph and a sample from it - same sample shown to demonstrate the affect of interventions, right: the affect of interventions and how far they affect based on underlying causal graph)
user. An interaction consists of changing the color (state) of a variable. At this point, the color of all variables affected by this variable (according to the causal graph) can change. Interventions change a block's color unconditionally, thus cutting the graph edge linking it with its parents in the graph. All transitions are probabilistic and defined by conditional probability tables (CPTs). A visualization of the environment can be found in Figure 4.</p>
<p>This environment allows for a complete and thorough testing of causal models as there are various degrees of complexities which can be easily tuned such as: (1) Complexity of the graph: We can test</p>
<p>any model on many different graphs thus ensuring that a models performance is not only limited to a few select graphs. (2) Stochasticity: By tuning the skewness of the probability distribution of each object we can test how good is a given model in modelling data uncertainty. In addition to this we can also tune the number of object or the number of colors to test whether the model generalizes to larger graphs and more colors. A causally correct model should be able to infer the causal relationships between observed objects, as well as their respective color distribution and its dependence on a causal parent's distribution.</p>
<h1>2.2 Evaluating causal models</h1>
<p>In much of the existing literature, evaluation of learned causal models is based on the structural difference between the learned graph and the ground-truth graph [Peters et al., 2016, Zheng et al., 2018]. However, this may not be applicable for most deep RL algorithms, as they do not necessarily learn an explicit causal structure [Dasgupta et al., 2019, Ke et al., 2020]. Even if a structure is learned, it may not be unique as several variable permutations can be equivalent, introducing an additional evaluation burden.</p>
<p>Another possibility is to exhaustively evaluate models on all possible intervention predictions and all environment states, a process that quickly becomes intractable even for small environments.</p>
<p>We therefore propose a few evaluation methods that can be used as a surrogate metrics to measure the model's performance on recovering the correct causal structure.</p>
<p>Predicting Intervention Outcomes. While it may not be feasible to predict all intervention outcomes in an RL environment, we propose that evaluating predictions on a subset of interventions provides an informative evaluation. Here, the test data is collected from the same environment used in training, ensuring a single underlying causal graph. Test data is generated from new episodes that are unseen during training. All interventions (actions) in the test episodes are randomly sampled and we evaluate the model's performance on this test set.</p>
<p>Zero Shot Transfer. Here, we test the model's ability to generalize to unseen test environments, where the environment does not have exactly the same causal graph as training, but training and test causal graphs share some similarity.</p>
<p>For example, in the observed Physics environment, a model that has learned the underlying causal relationship between color intensity and weight would be able to generalize to new variables with a novel color intensity.</p>
<p>Downstream RL Tasks. Downstream RL tasks that require a good understanding of the underlying causal graph of the environment are also good metrics for measuring the model's performance. For example, in the physics environment, we can provide the model with a target configuration in the form of some specific arrangement of blocks on a grid and the model needs to perform actions in the environment to reach the target configuration. Models that capture causal relationships between objects should achieve the target configuration more easily (as it is can predict intervention outcomes). For more details about this setup, please refer to Appendix D.</p>
<p>Metrics. We also evaluate the learned models on ranking metrics in the latent space as well as reconstruction-based metrics in the observation space [Kipf et al., 2019]. In particular we measure and report Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) and Reconstruction loss for evaluation in standard as well as transfer testing settings. We report these metrics for 1, 5 and 10 steps of prediction in the latent space (refer Appendix C).</p>
<h2>3 Models</h2>
<p>A large variety of neural network models have been proposed as world models in MBRL. These models can roughly be divided into two categories: monolithic models and models that have structure and modularity. Monolithic models typically have no explicit structure (other than layers). Some typical monolithic models are Autoencoders and Variational Autoencoders [Kingma and Welling, 2013, Rezende et al., 2014]. Conversely, structured models have explicit architecture built into (or learned by) the model. Examples of such models are ones based on graph neural networks [Battaglia et al., 2016, Van Steenkiste et al., 2018, Kipf et al., 2019, Veerapaneni et al., 2020] and modular models [Ke et al., 2020, Goyal et al., 2019, Mittal et al., 2020, Goyal et al., 2020]. We picked some</p>
<p>commonly used models from these categories and evaluated their performance to understand their ability for causal induction in MBRL.</p>
<p>To disentangle the architectural biases and effects of different training methodologies, we trained all the models on both likelihood based and contrastive losses, respectively. All models share three common components: encoder, decoder and transition model. We follow a similar training procedure as in <em>Ha and Schmidhuber [2018], Kipf et al. [2019]</em>. Details of the architectures as well as the training protocols and losses can be found in Appendix E.</p>
<h3>3.1 Monolithic Models</h3>
<p>We evaluate causal induction on two commonly used monolithic models: multilayered autoencoders and variational autoencoders. We follow a similar setup as in <em>Ha and Schmidhuber [2018]</em>. These models do not have strong inductive biases other than the number of layers used.</p>
<h3>3.2 Modular and Structured Models</h3>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: All models have 3 components: encoder, decoder and transition model. The transition models can either be monolithic, modular model or graph neural networks (GNNs). Monothlic models don’t have explicit structure. GNNs have factorized representation of variables. Modular models have factorized representation of both variables and directed edges to potentially model causal relationships, e.g. $A$ causing $B$.</p>
<p>Several forms of structure can be included in neural networks, including modularity, factorized variables, and directed rules.</p>
<p>Taking the three factors into account, we consider two types of structured models in our paper, graph neural networks (GNN) and so called modular networks. Graph neural networks (GNN) [Gilmer et al., 2017, Tacchetti et al., 2018, Battaglia et al., 2018, Kipf et al., 2019] is a widely adopted relational model that have a factorized representation of variables and models pairwise interactions between objects while being permutation invariant. In particular, we consider the C-SWM model [Kipf et al., 2019], which is a state-of-art GNN used for modeling object interactions. Similar to most GNNs, the C-SWM model learns factorized representations of different objects but for modelling dynamics it considers all possible pairwise interactions, and hence the transition model is monolithic (i.e., not a modular transition model).</p>
<p>Modular networks on the other hand are composed of an initial encoder that factorizes inputs (images), and then a modular transition model (MTM) - $M$. This internal model is tasked to create separate factored representations for each objects in the environment, while taking into account all other objects’ representations. This model also learns interactions between objects. The rules learned here are directed rules.</p>
<h2>4 Experiments</h2>
<p>Our experiments seak to answer the following questions: (a) Does explicit structure and modularity help for causal induction in MBRL? If so, then what type of structures provide good inductive bias for causal induction in MBRL? (b) How do different objective functions (likelihood or contrastive) impact learning? (c) How do different models scale to complex causal graphs? (d) Do prediction metrics (likelihood and ranking metrics) correspond to better downstream RL performance? (e) What are good evaluation criteria for causal induction in MBRL?</p>
<p>We report the performance of our models on both the Physics and the Chemistry environments, and refer the readers to Appendix E for implementation details.. All models are trained using the procedure described in Appendix E.2 and are evaluated based on ranking and likelihood metrics on 1, 5 and 10 step predictions. For the Chemistry environment, we evaluate the models on causal graphs with varying complexity, namely - chain, collider and full graphs. These graphs vary in the sparsity of edges and the length of cause-effect chains. For the Physics environment, we evaluate the model in the fully observed setting as well as the unobserved setting.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Success Rate (higher is better) for different models and training losses for 1, 5 and 10 step prediction for the Fixed Unobserved Physics environment setting with 5 objects. Here, (a) Random stands for a random policy, (b) greedy is the policy with best greedy actions, (c) NLL are models trained in 2 stages: pretraining the encoder/ decoder, following by only training the transition model, (d) NLL with finetune are models in 3 stages: pretraining the encoder/ decoder, following by only training the transition model and then finetuning the encoder, decoder and transition models together. (e) Contrastive are models trained using a contrastive loss. The GNN and Modular models trained on constrastive loss significantly outperform the monolithic models (autoencoders and VAE). The margin significantly increases as the number of steps to reach the goal increase, suggesting that models with explicit structure and modularity have a much better understanding of the world.</p>
<h3>4.1 Explicit structure and causal induction</h3>
<p>We found that for both the Physics and the Chemistry environments, models with explicit structure outperform monolithic models on both prediction metrics and downstream RL performances. In particular, models with explicit structure (GNNs and modular models) scale better to graphs of larger size and longer cause-effect chains.</p>
<p>The Physics environment has a complex underlying causal graph (full graph: refer Figure 1 (a)). We found that GNNs performed well in this environment with 3 variables. They achieved good prediction metrics (Figure 7) and high RL performance (Figure 13) even at longer timescales. However, their performance drops significantly on environments with 5 objects both in terms of prediction metrics (Figure 8) and RL performance (Figure 14). We also see in Figures 8 and 14 that modular models scale much better compared to all other models, suggesting that they hold an advantage for larger causal graphs. Further, modular models and GNNs when evaluated on zero shot settings outperform monolithic models by a significant margin (Figures 19 and 20 and Tables 15 and 16).</p>
<p>For the chemistry environment, we find that modular models outperform all other models for almost all causal graphs in terms of both prediction metrics (Figure 23) and RL performance (Figure 25). This is especially true on more complex causal graphs, such as chain and full graphs which have long cause-effect chains. This suggests that modular models scales better to more complex causal graphs.</p>
<p>Overall, these results suggest that structure, and in particular modularity, help causal induction in MBRL when scaling up to larger and more complex causal graphs. The performance comparisons on modular networks and C-SWM [Kipf et al., 2019] suggest that both factorized representation of variables and directed edges between variables can help for causal induction in MBRL.</p>
<h3>4.2 Complexity of the Underlying Causal Graph</h3>
<p>There are several ways to vary complexity in a causal graph: size of the graph, sparsity of edges and length of cause-effect chain (Figure 1). Increasing the size of the graph significantly impacts all models' performances. We evaluate models on the Physics environments with 3 objects (Figure 7) and 5 objects (Figure 8) and find that increasing the number of objects from 3 to 5 has a significant impact on performance. Modular models achieve over 90 on ranking metrics over 10-step prediction for 3 objects while for 5 objects, they achieve only 50 (almost half the performance on 3 objects). A similar pattern is found in almost all models. Another factor impacting complexity of the graph is the length of cause-effect chain. We see that collider graphs are the easiest to learn, with modular models and autoencoders significantly outpeforming all other models (Figure 23). This is because the collider graph has short pair-wise interactions, i.e, intervention on any node in a collider graph can impact at most one other node. Chain and full graphs are significantly more challenging because of longer cause-effect chains. For a chain or a full graph of n nodes, an intervention on the kth node can impact all the subsequent (n-k) nodes. Modeling interventions on chain and full graphs require</p>
<p>modeling more than pairwise relationships, hence, making it much more challenging. We find that modular models slightly outperform all other models on these graphs.</p>
<h1>4.3 Prediction Metrics and RL Performance</h1>
<p>As discussed in Section 2.2, there are multiple evaluation metrics based on either prediction metrics or RL performance. The performance of the model on one metric may not necessarily transfer to another. We would like to analyze if this is the case for the models trained under various environments. We first note that while the ranking metrics were relatively good for most models on physics environments, most of them only did slightly better than a random policy on downstream RL, especially on larger graphs (Figures Figure 7 - 12 and Table 3 - 8 for ranking metrics; Figure 13 - 18 and Table 9 - 14 for downstream RL). Figures 21, 22 and 27 show scatter plots for each pair of losses, with one loss on each axis. While there is some correlation between ranking metric and RL performance (Modular and GNN; Figure 21), we did not find this trend to be consistent across models and environment settings. We feel that these results give further evidence of need to evaluate on RL performance.</p>
<h3>4.4 Training objectives and learning</h3>
<p>Likelihood loss and contrastive loss [Oord et al., 2018, Kipf et al., 2019] are two frequently used objectives for training world models in MBRL. We trained the models under each of these objective functions to understand how they impact learning. In almost all cases, models with explicit structure (modular models and GNNs) trained on contrastive loss perform better in terms of ranking loss compared to those trained on likelihood loss (refer to Figure 7 - 12). We don't see a very clear trend between training objective and downstream RL performance but we do see a few cases where contrastively trained models performed much better than others (refer to Figures 6, 13, 17 and 18 and Tables 9, 13 and 14).
For other key insights and experimental conclusions on different environments, we refer the readers to Appendix F. 6 for the physics environment and Appendix G. 3 for the chemistry environment.</p>
<h2>5 Related work</h2>
<p>Video Prediction and Visual Question Answering. There exist a number of video prediction [Yi et al., 2019, Baradel et al., 2019] and visual question answering [Johnson et al., 2017] datasets that also make use of a blocks world for visual representation. Though these datasets can appear visually similar to ours at first glance, they lack two essential ingredients for systematically evaluating models for causal induction in MBRL. The first is that they do not allow active interventions and hence make it challenging for evaluating model-based reinforcement learning algorithms. Another key point is that these environments do not allow one to systematically perturb different aspects of causal graphs, hence, preventing to systematically study the performances of models for causal induction.
RL Environments. There exist several benchmarks for multi-task learning for robotics (Meta-World [Yu et al., 2019] and RLBench [James et al., 2020]) and for video gaming domain (Arcade Learning Environment, CoinRun [Cobbe et al., 2018], Sonic Benchmark [Machado et al., 2018], MazeBase [Nichol et al., 2018] and BabyAI [Chevalier-Boisvert et al., 2018]). However, as mentioned earlier, these benchmarks do not allow one to systematically control different aspects of causal models (such as the structure, the sparsity of edges and the size of the graph), hence making it difficult to systematically study causal induction in MBRL. The Alchemy [Wang et al., 2021] environment, which was released earlier this year, moves a step towards causal induction for meta-RL. Though the environment allows for some level of control of the underlying causal structures of the environment, it still does so in a limited way.
Block World. The AI community has been using the "blocks world" for decades as a testbed for various AI problems, including learning theory [Winston, 1970], natural language [Winograd, 1972], and planning [Fahlman, 1974]. Block world allows to easily vary different aspects of the underlying causal structure, and also allow interventions to be performed on many high level variables of the environment giving rise to a large space of tasks which have well-defined relations between them.</p>
<h1>6 Discussions and conclusions</h1>
<p>In our work, we focus on studying various model-based approaches for causal induction in modelbased RL. We highlighted the limitations of existing benchmarks and introduced a novel suite of environments that can help measure progress and facilitate research in this direction. We evaluated various models under many different settings and discuss the essential problems and challenges in combining both fields i.e ingredients, that we believe are common in the real world, such as modular factorization of the objects and interactions of objects governed by some unknown rules. Using a proposed evaluation framework, we demonstrate that structural inductive biases are beneficial to learning causal relationships and yield significantly improved performances in learning world models. We hope that our work helps to facilitate future work for understanding causal relationships in model-based reinforcement learning.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Compute Canada, the Canada Research Chairs, CIFAR. We would also like to thank the developers of Pytorch for developments of great frameworks. We would like to thank Dmitriy Serdyuk for useful feedback and discussions.</p>
<h2>References</h2>
<p>Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics. arXiv preprint arXiv:1909.12000, 2019.</p>
<p>Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pages 4502-4510, 2016.</p>
<p>Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.</p>
<p>Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. arXiv preprint arXiv:1901.10912, 2019.</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. In International Conference on Learning Representations, 2018.</p>
<p>Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. arXiv preprint arXiv:1812.02341, 2018.</p>
<p>Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal reasoning from meta-reinforcement learning. arXiv preprint arXiv:1901.08162, 2019.</p>
<p>Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In Advances in Neural Information Processing Systems, pages 11698-11709, 2019.</p>
<p>Daniel Eaton and Kevin Murphy. Exact bayesian structure learning from uncertain interventions. In Artificial Intelligence and Statistics, pages 107-114, 2007.</p>
<p>Scott Elliott Fahlman. A planning system for robot construction tasks. Artificial intelligence, 5(1): $1-49,1974$.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1263-1272. JMLR. org, 2017.</p>
<p>Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition. arXiv preprint arXiv:2011.15091, 2020.</p>
<p>Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Schölkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893, 2019.</p>
<p>Anirudh Goyal, Alex Lamb, Phanideep Gampa, Philippe Beaudoin, Sergey Levine, Charles Blundell, Yoshua Bengio, and Michael Mozer. Object files and schemata: Factorizing declarative and procedural knowledge in dynamical systems. arXiv preprint arXiv:2006.16225, 2020.</p>
<p>David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark \&amp; learning environment. IEEE Robotics and Automation Letters, 5(2):30193026, 2020.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2901-2910, 2017.</p>
<p>Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprint arXiv:1910.01075, 2019.</p>
<p>Nan Rosemary Ke, Jane Wang, Jovana Mitrovic, Martin Szummer, Danilo J Rezende, et al. Amortized learning of neural causal representations. arXiv preprint arXiv:2008.09301, 2020.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models. arXiv preprint arXiv:1911.12247, 2019.</p>
<p>Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.</p>
<p>Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie, Michael Mozer, and Yoshua Bengio. Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules. arXiv preprint arXiv:2006.16981, 2020.</p>
<p>Suraj Nair, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Causal induction from visual observations for goal directed tasks. arXiv preprint arXiv:1910.01751, 2019.</p>
<p>Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. Gotta learn fast: A new benchmark for generalization in rl. arXiv preprint arXiv:1804.03720, 2018.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019.</p>
<p>Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947-1012, 2016.</p>
<p>Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. MIT press, 2017.</p>
<p>Danilo J Rezende, Ivo Danihelka, George Papamakarios, Nan Rosemary Ke, Ray Jiang, Theophane Weber, Karol Gregor, Hamza Merzic, Fabio Viola, Jane Wang, et al. Causally correct partial models for reinforcement learning. arXiv preprint arXiv:2002.02836, 2020.</p>
<p>Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of The 31st International Conference on Machine Learning, pages 1278-1286, 2014.</p>
<p>Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612-634, 2021.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.</p>
<p>David Silver and Kamil Ciosek. Compositional planning using optimal option models. arXiv preprint arXiv:1206.6473, 2012.</p>
<p>Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160-163, 1991.</p>
<p>Andrea Tacchetti, H Francis Song, Pedro AM Mediano, Vinicius Zambaldi, Neil C Rabinowitz, Thore Graepel, Matthew Botvinick, and Peter W Battaglia. Relational forward models for multi-agent learning. arXiv preprint arXiv:1809.11044, 2018.</p>
<p>Sjoerd Van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353, 2018.</p>
<p>Rishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement learning. In Conference on Robot Learning, pages 1439-1456. PMLR, 2020.</p>
<p>Jane X Wang, Michael King, Nicolas Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie Deck, Peter Choy, Mary Cassin, Malcolm Reynolds, Francis Song, et al. Alchemy: A structured task distribution for meta-reinforcement learning. arXiv preprint arXiv:2102.02926, 2021.</p>
<p>Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P Burgess, and Alexander Lerchner. Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration. arXiv preprint arXiv:1905.09275, 2019.</p>
<p>Terry Winograd. Understanding natural language. Cognitive psychology, 3(1):1-191, 1972.
Patrick H Winston. Learning structural descriptions from examples. 1970.
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019.</p>
<p>Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. arXiv preprint arXiv:1910.10897, 2019.</p>
<p>Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. DAGs with NO TEARS: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems, pages 9472-9483, 2018.</p>
<h1>Part I</h1>
<h2>Appendix</h2>
<h2>A Dataset Documentation</h2>
<p>We open-source our environment, data, code and instructions on how to run them ${ }^{1}$. We do not provide the data as a downloadable file, it can be generated using the instructions in the repository. We provide instructions to reproduce the results of our benchmark experiments. The data is provided under the MIT license. We bear all responsibility in-case the dataset leads to any violation of rights. The metadata for the dataset can also be found in the given github repository.
Intended Use. The intended use of this dataset is for causal learning research in model-based RL. We hope that this dataset can help to speed up discovery of novel methods that can learn causal relations in RL environments.
Reading the Data. The data is generated and stored in HDF5 format. ${ }^{2}$, it can be accessed using the h5py python package ${ }^{3}$. We provide the code for reading the data.</p>
<h2>B A short review to Structured Causal Models</h2>
<p>Causal modeling. A Structural Causal Model (SCM) [Peters et al., 2017] over a finite number $M$ of random variables $X_{i}$ is a function that maps from the jointly-independent noise $N_{i}$ and parents (direct causes) $X_{p a(i, C)}$ of $X_{i}$ to $X_{i}$. The matrix $C \in{0,1}^{M \times M}$ represents the adjacency matrix (structure) of the graph, such that $c_{i j}=1$ if node $i$ has node $j$ as a parent (equivalently, $X_{j} \in X_{p a(i, C)}$; i.e. $X_{j}$ is a direct cause of $X_{i}$ ).</p>
<p>$$
X_{i}:=f_{i}\left(X_{p a(i, C)}, N_{i}\right), \quad \forall i \in{0, \ldots, M-1}
$$</p>
<p>Causal structure discovery is the recovery of ground-truth $C$ from observational and/or interventional studies.</p>
<p>Interventions. An intervention on a variable $X_{i}$ changes the function $f_{i}$ that maps from the causal parents of $X_{i}$ and the independent noise $\left(\left(X_{p a(i, C)}, N_{i}\right)\right)$ to $X_{i}$. There are several common types of interventions available [Eaton and Murphy, 2007]: No intervention: only observational data is obtained from the ground truth model. Perfect: the value of a single or several variables is fixed and then ancestral sampling is performed on the other variables. Imperfect: the conditional distribution of the variable on which the intervention is performed is changed. All our experiments are performed with perfect interventions (aka. setting the state of a variable to a particular value, for example location or color), as they are the most common type of interventions in RL.</p>
<h2>C Ranking based Evaluation</h2>
<p>Apart from standard reconstruction loss, we also provide ranking results based on the evaluation metrics followed by Kipf et al. [2019]. Given observations at two different time steps, these metrics capture how close is the predicted transition in the embedding space to the embedding of the true observation obtained through the true environment transitions. Here the notion of closeness is defined as ranking from a large buffer of states under euclidean norm.</p>
<h2>C. 1 Hits at Rank 1 (H@1)</h2>
<p>This score is 1 for a particular example if the predicted state representation is nearest to the encoded true observation and 0 otherwise. Thus, it measures whether the rank of the predicted representation is equal to 1 or not, where ranking is done over all reference state representations by distance to the true state representation. We report the average of this score over the test set.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C. 2 Mean Reciprocal Rank (MRR)</h1>
<p>This is defined as the average inverse rank, i.e, $\operatorname{MRR}=\frac{1}{N} \sum_{n=1}^{N} \frac{1}{\operatorname{rank}<em n="n">{n}}$ where $\operatorname{rank}</em>$ sample of the test set where ranking is done over all reference state representations.}$ is the rank of the $n^{t h</p>
<h2>D Reward Prediction Evaluation</h2>
<p>Below, we provide the methodology of training the reward predictor and doing evaluation based on it as well as further implementation details relevant to our particular set of environments.</p>
<h2>D. 1 Methodology</h2>
<p>For downstream RL evaluation, we consider learning a reward predictor and then performing planning based on taking greedy actions in the direction of immediate highest reward (inspired from Watters et al. [2019]). For our tasks, the reward is a function of the next state and the target state but not the action. For example, in physics environment the reward is the average distance between the objects in their current configuration and a target configuration. Similarly, for chemistry environment it is the number of color matches between the current state and the target state.</p>
<p>More concretely, we learn a reward predictor function (parameterized by a single layered MLP) that takes as input the current state as well as the target state of the world and tries to predict the reward for the current state. This reward predictor is learned in a supervised way and all the other weights (encoder, decoder, transition models) are kept fixed during this training. Thus, it is only possible to learn a good reward predictor if the encoder model captures the important aspects of the objects from the raw image.</p>
<p>Given the current encoded state of the world, we consider all possible actions and transitions according to them in the latent space (using the learned transition model). After the transition, we use the learned reward predictor to predict the reward for the (new state, target state) pair. This gives us the immediate reward obtained from each action. Having obtained those rewards, our policy is to just greedily take the action that gives us the best immediate reward. Note that in our reward setting (dense and/or partial rewards) this is typically a good policy as can be seen in the oracle (greedy) performance (where we take actions according to the true reward).
For training, we consider the supervised $L_{1}$ loss optimized using the Adam Optimizer -</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _theta="\theta">{\text {Reward Predictor }}(\theta) &amp; =\left|f</em>\right)\right|}\left(s_{t}, s_{\text {target }}\right)-r\left(x_{t}, x_{\text {target }<em t="t">{1} \
s</em>\right) \
s_{\text {target }} &amp; =\operatorname{Encoder}\left(x_{\text {target }}\right)
\end{aligned}
$$} &amp; =\operatorname{Encoder}\left(x_{t</p>
<p>where $r(\cdot, \cdot)$ is the true reward function.
For evaluation, we consider the true final reward as well as the success rate obtained under policy $\pi$ where $\pi$ is implicitly defined using the learned reward function $f_{\theta}$ as follows -</p>
<p>$$
\pi\left(s_{t}, s_{\text {target }}\right)=\underset{a \in \mathcal{A}}{\arg \max } f_{\theta}\left(\operatorname{Transition}\left(s_{t}, a\right), s_{\text {target }}\right)
$$</p>
<p>We leave the formulation of training a value function estimator using a TD-learning objective as an important future work.</p>
<h2>D. 2 Implementation Details</h2>
<p>For all the environments, when training a reward predictor we consider a starting state of the environment and the state of the environment obtained after doing 10 random actions. Given the starting state and the target state, we use the dense reward obtained in the configuration to act as the supervision signal for training of the reward predictor model.
For physics environment, we consider the reward to be the average distance of objects from their target configurations. Whereas, for the chemistry environment we consider the number of partial matches between the two states as the reward function.</p>
<p>For evaluation on downstream RL tasks, for $k^{t h}$ step prediction, we consider targets that are generated from $k$ random actions in the environment. We also report baseline performances of a random policy as well as an optimal policy. For the physics environment, we set the optimal policy to be the one step greedy policy based on the true reward while for the chemistry environment, we consider the same actions that led to the target configuration to be the optimal policy. Note that since the chemistry environment is stochastic, the same actions may not lead to the same state. Hence any loss in performance even after performing optimal actions is due to the data uncertainty that arises due to the stochasticity.</p>
<h1>E Model setups and training procedure</h1>
<h2>E. 1 Model Based Experiments</h2>
<p>For our model based experiments, we consider four models that encode different inductive biases -</p>
<ul>
<li>Autoencoders (AE) - Monolithic model that compresses everything into a single entity.</li>
<li>Variational Autoencoders (VAE) - Similar to Autoencoders but with regularization to stay close to a prior distribution in latent space.</li>
<li>Modular Model (Modular) - Has a separate representation for each object and can be used to capture interactions between multiple sets of objects.</li>
<li>Graph Neural Networks (GNN) - Also has an object-wise representation but can capture only pairwise interactions between objects.</li>
</ul>
<p>Each model has an encoder-decoder model as well as a transition model. The encoder-decoder model is aimed at inferring the high level causal variables from raw pixel data whereas the transition model is tasked with controlling how the encoded state transitions based on the actions taken. We build all our models on the architectural backbone provided by Kipf et al. [2019].
The encoder model is a convolutional neural network followed by a 3-layered MLP (Table 1). It outputs a single representation in case of monolithic models and an object-wise representation (i.e. separate for each object) in case of modular networks and graph neural networks.
The decoder model (if used - refer Appendix E.2) takes either a single representation (in case of monolithic models) or object-wise representations (in case of modular networks / GNNs) and outputs an image as close as possible to the input image. The structure of the decoder is detailed in Table 2.
We follow the medium encoder-decoder structure followed by Kipf et al. [2019]. For embedding dimension, we use a fixed embedding dimension of 32 per object where the number of objects are specified by the environment description. For example, if we have 3 objects in the environment, then the embedding dimension of Autoencoder based models is 96 while it is 32 per object for Modular/GNN models.
Mathematically, given an observation $x_{t}$, the encoder maps the observation to its latent representation $s_{t}$ which is either monolithic or modular. Further, the decoder (if used) maps the latent representation back to the input space.</p>
<p>$$
\begin{aligned}
&amp; s_{t}=\operatorname{Encoder}\left(x_{t}\right) \
&amp; \hat{x}<em t="t">{t}=\operatorname{Decoder}\left(s</em>\right)
\end{aligned}
$$</p>
<p>Each architecture also has a transition model to model how a particular action affects the state of the world. Based on the current state of the world and an action taken, the transition model predicts the next state of the world. For monolithic models (AE and VAE), the transition model is a 3-layered MLP. For GNN, it is a graph neural network with only one node-to-edge and one edge-to-node information propagation, that is, it encodes only pairwise interactions. For modular models, it is a separate MLP for each object, that allows it to encode higher order interactions between multiple objects.
Mathematically, the transition (prediction of next state) from a given state $s_{t}$ based on an action $a_{t}$ can be shown as -</p>
<p>$$
\hat{s}<em t="t">{t+1}=\operatorname{Transition}\left(s</em>\right)
$$}, a_{t</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">channels</th>
<th style="text-align: center;">activation</th>
<th style="text-align: center;">stride</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Conv2D $9 \times 9$</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">Leaky Relu</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">BatchNorm2D</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Conv2D $5 \times 5$</td>
<td style="text-align: center;">$M$ (number of objects)</td>
<td style="text-align: center;">Sigmoid</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Table 1: Architecture of the encoder used for the world models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">channels</th>
<th style="text-align: center;">activation</th>
<th style="text-align: center;">stride</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">Relu</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">Relu</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$M \times 10 \times 10$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ConvTranspose2D $5 \times 5$</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">Relu</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">BatchNorm2D</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ConvTranspose2D $9 \times 9$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 2: Architecture of the decoder used for the world models.</p>
<h1>E. 2 Training Details</h1>
<p>We consider two methods of training for all our baseline models -</p>
<ul>
<li>Negative Log Likelihood (NLL)</li>
<li>Contrastive Loss (Decoder Free)</li>
</ul>
<p>For the models trained using NLL, we perform training in 3 stages. First, we do pretraining where only the encoder and decoder are trained to reconstruct the given image. Second, we learn the transition where the encoder and decoder are fixed and the transition function is trained to optimally predict the next state given the current state and action. Finally, we do finetuning where we train both the encoder-decoder model as well as the transition model on combined objectives of reconstructing the current images, reconstructing the images in next step as well as doing correct transitions in the latent space.</p>
<p>For the reconstructions, we use the binary cross entropy loss (BCE loss) while for the transitions, we use the mean squared error loss (MSE loss).</p>
<p>Mathematically, given the current observation $x_{t}$, the action taken $a_{t}$ and the next observation obtained $x_{t+1}$, we first encode both the observations into the latent space as -</p>
<p>$$
\begin{aligned}
s_{t} &amp; =\operatorname{Encoder}\left(x_{t}\right) \
s_{t+1} &amp; =\operatorname{Encoder}\left(x_{t+1}\right)
\end{aligned}
$$</p>
<p>We then perform a transition from the current step using the transition model as well as use the decoder to perform reconstructions based on the current encoded state as well as the predicted state -</p>
<p>$$
\begin{aligned}
\hat{s}<em t="t">{t+1} &amp; =\operatorname{Transition}\left(s</em>\right) \
\hat{x}}, a_{t<em t="t">{t} &amp; =\operatorname{Decoder}\left(s</em>\right) \
\hat{x}<em t_1="t+1">{t+1} &amp; =\operatorname{Decoder}\left(\hat{s}</em>\right)
\end{aligned}
$$</p>
<p>Given these variables, the pretraining, transition training and the finetuning can be characterized as -</p>
<p>$$
\begin{aligned}
&amp; \text { Pretraining : } \underset{\text { Encoder,Decoder }}{\arg \min } \operatorname{BCE}\left(x_{t}, \hat{x}<em t_1="t+1">{t}\right) \
&amp; \text { Transition : } \underset{\text { Transition }}{\arg \min } \operatorname{MSE}\left(s</em>}, \hat{s<em t="t">{t+1}\right) \
&amp; \text { Finetuning : } \underset{\text { Encoder, Decoder, Transition }}{\arg \min } \operatorname{BCE}\left(x</em>}, \hat{x<em t_1="t+1">{t}\right)+\operatorname{MSE}\left(s</em>}, \hat{s<em t_1="t+1">{t+1}\right)+\operatorname{BCE}\left(x</em>\right)
\end{aligned}
$$}, \hat{x}_{t+1</p>
<p>For models trained with contrastive loss, we follow the same setup as in Kipf et al. [2019]. In this setup we don't use a decoder and instead learn everything in encoded state end-to-end. Mathematically,</p>
<p>this can be described as the following -</p>
<p>$$
\begin{aligned}
\text { Contrastive Training : } \underset{\text { Encoder, Transition }}{\arg \min } &amp; H+\max (0, \gamma-\tilde{H}) \
H &amp; =\operatorname{MSE}\left(\tilde{s}<em t_1="t+1">{t+1}, s</em>\right) \
\tilde{H} &amp; =\operatorname{MSE}\left(\tilde{s}<em t_1="t+1">{t+1}, s</em>\right) \
\tilde{s}_{t+1}: &amp; \text { Negative state obtained from random shuffling of batch }
\end{aligned}
$$</p>
<p>We train each stage for 100 epochs using Adam optimizer [Kingma and Ba, 2014] with a learning rate of $5 \mathrm{e}-4$ and batch size 512 .</p>
<h1>F Physics Environment</h1>
<h2>F. 1 Detailed setups</h2>
<p>We provide an environment which consists of objects of different shapes and potentially different colors. Each object has a unique weight associated with it and only heavier objects can push lighter ones. This induces an acyclic tournament causal graph with sparse two-way interactions between the objects, which form the nodes of the graph.
More precisely, the physics environment with $M$ objects (eg. 3) and colormap $C$ (eg. blues) can be considered as the set $\left{o_{i}=\left{s_{i}, w_{i}, c_{i}, p_{i}\right} \mid i=1\right.$ to $\left.M\right}$ where $o_{i}$ denotes the $i^{\text {th }}$ object which is characterized by its position $p_{i}$, its shape $s_{i}$, its color $c_{i}$ and its weight $w_{i}$. An edge exists from $o_{i}$ to $o_{j}$ if and only if $w_{i}&gt;w_{j}$. We consider the weight of each object to be unique, thereby getting rid of cycles. The specifics of the environment are determined by how the shape, color and weight of an object are related. For our experimentation, we consider two different settings which are outlined below. However, we emphasize that the physics environment is not limited to just these specifications and can be easily extended to form more complicated relationships between the three properties.</p>
<h2>F. 2 Identity of Objects</h2>
<p>Since we are proposing RL environments, we need to make sure that the mapping from the action space to the object space is well defined and observable / learnable. Here, we briefly discuss that it is the case in the settings of the physics environment proposed in this paper. We also discuss that in the Unobserved environment this mapping can be very hard to learn and for this reason, we proposed another variant known as FixedUnobserved environment.
Our mapping from action space to object space is such that given an initialization of the environment, the first action dimension always corresponds to the heaviest object. Similarly, the second to the second heaviest and so on.
Now, in the Observed environment case, the heaviest object is also the darkest object in the scene so it is relatively easy for a model to infer the action to object mapping once it has learned the fact that intensity of color represents the weight of the object.
On the other hand, in the Unobserved case, the colors of the objects are sampled without replacement from a larger set of colors. For example, consider a 3 object environment with the set of colors to be red &lt; green &lt; orange &lt; yellow where the ordering defines the ordering of the weight. Then if in one initialization has the colors (red, green, yellow) then here the first action dimension corresponds to the color red. However, another initialization of the same environment can be (green orange, yellow) and then the first action dimension would correspond to the green object. Thus, for a model to learn the action to object mapping, it has to learn this global ranking of colors. We found that this was typically hard for the models to do.
To alleviate the above complexity, we consider another setting FixedUnobserved where we keep the shapes of the objects fixed and unique. Here, there is an additional constraint that apart from the colors following a global ordering of weights, the unique shapes also follow a global ordering of weights and hence, this creates an easily learnable mapping.</p>
<h1>F. 3 All variables are observed</h1>
<p>In this setting, we consider all the objects to be of the same color but different shades, eg. different shades of the color blue. The weight of each object is a monotonic function of its color intensity, meaning that darker objects are heavier.
Mathematically, given a colormap $C$ (single color; continuous in intensity of the color), $c_{i} \in[0,1]$ denotes the intensity of the color $C$ for object $i$ ( 1 being darkest; 0 lightest). Moreover, the weight of that object is given by $w_{i}=g\left(c_{i}\right)$, where g is a strictly monotonic function. Thus, darker objects are given heavier weights and thus can push lighter objects.
This setting easily allows for zero shot generalization since a model that has been trained on a subset of shades of a particular color can generalize to do well across different shades of the same color. Moreover, the shape of an object here is a distractor since the dynamics of the objects are only controlled by their colors.</p>
<h2>F. 4 Some variables are unobserved</h2>
<p>In this setting, all objects are of distinct discrete colors drawn from a discrete colormap $c$. Each color is associated with a unique weight and here, too, heavier objects can push lighter ones but not vice versa.</p>
<p>Mathematically, given a colormap $C$ (multiple discrete options), $c_{i} \in C$ denotes the color for object $i$ such that $c_{i} \neq c_{j} \forall i \neq j$. Moreover, the weight of that object is given by $w_{i}=g\left(h_{i}\right)$, where g is an injective function and $g: C \rightarrow \mathbb{R}$.
This setting does not allow for zero shot generalization in the colors since whenever a new color is introduced, the agent will have to perform interventions on it to infer its place in the graph. However, similar to the observed case, the shapes of the objects act as distractors since the dynamics is only controlled by the colors.</p>
<h2>F. 5 Unobserved Variables but Fixed Shapes</h2>
<p>In this setting, all objects are of distinct discrete colors and shapes where the set of shapes is kept constant across different episodes. Here, the weight of an object can be reflected either from its shape or its color. For example, the lightest object in the episode will always be of a fixed unique shape and it will always have the lightest color (where lightest color is defined according to the order on the color in the colormap - eg. red &lt; blue &lt; green)
This setting does not allow for zero shot generalization in either the colors or the shapes since whenever a new color or shape is introduced, the agent will have to perform interventions on it to infer its place in the graph.</p>
<h2>F. 6 Experimental Results</h2>
<p>We perform experiments on a wide range of settings for the underlying causal graph for the physics environment. We categorize our findings below -</p>
<ul>
<li>Graph Neural Networks (GNNs) generally don't perform well compared to Modular models and Autoencoders (AEs) on a wide variety of metrics (ranking metrics, reconstruction loss, downstream RL task) in the setting of likelihood based loss (refer to Figure 7 - Figure 18 and Table 3 - 14)</li>
<li>Models trained with contrastive loss are generally better at predictions made over longer time scales in terms of ranking metrics (refer to Figure 7 - 12 and Table 3 - 8)</li>
<li>Models trained with contrastive loss are also generally better at downstream RL tasks as compared to those trained with likelihood based loss. In particular there are some settings where the former were able to do almost perfect planning while the latter weren't able to do good planning in any setting (refer to Figures 15, 17 and 18 and Tables 9, 13 and 14)</li>
<li>Modular models and Graph Neural Networks scale better than the monolithic counterparts when the number of objects in the causal graph increases. Further, while the ranking metrics</li>
</ul>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) (higher is better) and Reconstruction Error (lower is better) for different models and training losses for 1, 5 and 10 step prediction for the Observed Physics environment setting with 3 objects.
still remain good, we see that the planning metrics suffer by a large margin (refer to Figure 7 - 18 and Table 3 - 14)</p>
<ul>
<li>While Autoencoder models perform decently based on ranking metrics, they generally don't perform as well on downstream RL tasks when compared to Graph Neural Networks and Modular models (refer to Figure 13 - 18 and Table 9 - 14)</li>
<li>While ranking metrics on the unobserved environment are still decent (refer to Figures 9 and 10 and Tables 5 and 6), we see that in terms of downstream RL planning, none of the models do much better than a random policy (refer to Figures 15 and 16 and Figures 15 and 16)</li>
<li>We see a case where models that have very good ranking metrics over long time horizons (AE with NLL Finetune; Figure 11 and Figure 17) perform much worse on downstream RL tasks than GNNs and Modular models which had lower ranking metrics (Table 13 and Figure 17).</li>
</ul>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) (higher is better) and Reconstruction Error (lower is better) for different models and training losses for 1, 5 and 10 step prediction for the Observed Physics environment setting with 5 objects.</p>
<table>
<thead>
<tr>
<th></th>
<th>Model</th>
<th>1 Step</th>
<th></th>
<th></th>
<th>5 Steps</th>
<th></th>
<th></th>
<th>10 Steps</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
</tr>
<tr>
<td>NLL</td>
<td>AE</td>
<td>97.23±0.15</td>
<td>98.23±0.28</td>
<td>0.04±0.01</td>
<td>72.78±2.5</td>
<td>77.74±2.14</td>
<td>0.1±0.01</td>
<td>40.46±3.48</td>
<td>47.4±3.37</td>
<td>0.22±0.01</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>64.86±4.43</td>
<td>73.39±4.08</td>
<td>0.11±0.01</td>
<td>17.73±6.15</td>
<td>25.44±7.73</td>
<td>0.33±0.05</td>
<td>6.4±3.51</td>
<td>11.05±5.17</td>
<td>0.44±0.06</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>97.13±0.55</td>
<td>98.22±0.42</td>
<td>0.04±0.01</td>
<td>70.7±9.01</td>
<td>76.46±7.95</td>
<td>0.13±0.02</td>
<td>36.66±9.88</td>
<td>44.25±10.14</td>
<td>0.26±0.03</td>
</tr>
<tr>
<td></td>
<td>VAE</td>
<td>49.52±1.51</td>
<td>58.98±1.79</td>
<td>0.25±0.02</td>
<td>1.7±0.13</td>
<td>3.4±0.36</td>
<td>1.0±0.11</td>
<td>0.16±0.03</td>
<td>0.56±0.06</td>
<td>1.18±0.14</td>
</tr>
<tr>
<td>NLL Finetuned</td>
<td>AE</td>
<td>98.08±0.2</td>
<td>98.81±0.15</td>
<td>0.03±0.01</td>
<td>80.95±2.2</td>
<td>84.54±1.96</td>
<td>0.07±0.01</td>
<td>51.98±4.12</td>
<td>57.96±3.84</td>
<td>0.16±0.01</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>74.64±11.03</td>
<td>78.88±10.19</td>
<td>0.04±0.01</td>
<td>32.43±16.24</td>
<td>39.39±17.45</td>
<td>0.14±0.05</td>
<td>8.23±7.15</td>
<td>12.03±9.29</td>
<td>0.28±0.07</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>98.16±0.49</td>
<td>99.0±0.33</td>
<td>0.03±0.01</td>
<td>81.49±10.07</td>
<td>86.17±8.66</td>
<td>0.07±0.02</td>
<td>48.7±16.19</td>
<td>56.48±16.41</td>
<td>0.17±0.04</td>
</tr>
<tr>
<td></td>
<td>VAE</td>
<td>77.61±16.75</td>
<td>83.27±13.68</td>
<td>0.04±0.01</td>
<td>18.96±13.9</td>
<td>25.5±17.07</td>
<td>0.29±0.08</td>
<td>1.3±1.08</td>
<td>2.87±1.96</td>
<td>0.51±0.07</td>
</tr>
<tr>
<td>Contrastive</td>
<td>AE</td>
<td>82.11±2.22</td>
<td>88.5±1.61</td>
<td>-</td>
<td>50.0±6.43</td>
<td>65.2±5.04</td>
<td>-</td>
<td>34.36±8.42</td>
<td>51.22±8.17</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>93.86±9.58</td>
<td>95.99±6.42</td>
<td>-</td>
<td>78.28±12.39</td>
<td>82.29±16.45</td>
<td>-</td>
<td>72.06±19.38</td>
<td>75.46±15.65</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>98.73±1.04</td>
<td>99.31±4.58</td>
<td>-</td>
<td>94.7±4.2</td>
<td>97.02±2.38</td>
<td>-</td>
<td>90.6±6.97</td>
<td>94.45±4.08</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 3: Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) (higher is better) and Reconstruction Error (lower is better) for different models and training losses for 1, 5 and 10 step prediction for the Observed Physics environment setting with 3 objects.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) (higher is better) and Reconstruction Error (lower is better) for different models and training losses for 1, 5 and 10 step prediction for the Unobserved Physics environment setting with 3 objects.</p>
<table>
<thead>
<tr>
<th></th>
<th>Model</th>
<th>1 Step</th>
<th></th>
<th></th>
<th>5 Steps</th>
<th></th>
<th></th>
<th>10 Steps</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
</tr>
<tr>
<td>NLL</td>
<td>AE</td>
<td>97.77 $\pm 1.45$</td>
<td>98.38 $\pm 1.05$</td>
<td>0.08 $\pm 0.01$</td>
<td>63.88 $\pm 0.77$</td>
<td>69.55 $\pm 0.0$</td>
<td>0.25 $\pm 0.01$</td>
<td>27.18 $\pm 7.09$</td>
<td>33.6 $\pm 7.71$</td>
<td>0.45 $\pm 0.03$</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>95.13 $\pm 3.02$</td>
<td>96.95 $\pm 2.24$</td>
<td>0.19 $\pm 0.02$</td>
<td>41.49 $\pm 3.95$</td>
<td>50.63 $\pm 3.93$</td>
<td>0.47 $\pm 0.05$</td>
<td>19.28 $\pm 3.57$</td>
<td>26.59 $\pm 3.06$</td>
<td>0.63 $\pm 0.07$</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>99.57 $\pm 0.16$</td>
<td>99.73 $\pm 0.12$</td>
<td>0.09 $\pm 0.0$</td>
<td>79.14 $\pm 4.89$</td>
<td>84.06 $\pm 4.09$</td>
<td>0.28 $\pm 0.01$</td>
<td>35.68 $\pm 4.99$</td>
<td>43.82 $\pm 7.93$</td>
<td>0.48 $\pm 0.02$</td>
</tr>
<tr>
<td></td>
<td>VAE</td>
<td>79.35 $\pm 0.48$</td>
<td>84.38 $\pm 0.4$</td>
<td>0.34 $\pm 0.01$</td>
<td>6.18 $\pm 1.76$</td>
<td>10.68 $\pm 2.25$</td>
<td>1.62 $\pm 0.1$</td>
<td>0.28 $\pm 0.09$</td>
<td>0.97 $\pm 0.22$</td>
<td>2.21 $\pm 0.13$</td>
</tr>
<tr>
<td>NLL</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Finetuned</td>
<td>AE</td>
<td>98.29 $\pm 0.77$</td>
<td>98.78 $\pm 0.53$</td>
<td>0.07 $\pm 0.01$</td>
<td>69.58 $\pm 7.23$</td>
<td>74.59 $\pm 6.45$</td>
<td>0.2 $\pm 0.02$</td>
<td>31.75 $\pm 6.64$</td>
<td>38.22 $\pm 6.97$</td>
<td>0.39 $\pm 0.02$</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>97.71 $\pm 2.81$</td>
<td>98.43 $\pm 2.13$</td>
<td>0.07 $\pm 0.0$</td>
<td>68.36 $\pm 18.69$</td>
<td>73.78 $\pm 17.13$</td>
<td>0.2 $\pm 0.05$</td>
<td>26.52 $\pm 13.33$</td>
<td>32.94 $\pm 14.63$</td>
<td>0.46 $\pm 0.13$</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>99.65 $\pm 0.2$</td>
<td>99.77 $\pm 0.14$</td>
<td>0.06 $\pm 0.0$</td>
<td>77.21 $\pm 6.81$</td>
<td>82.08 $\pm 5.83$</td>
<td>0.21 $\pm 0.04$</td>
<td>23.15 $\pm 6.27$</td>
<td>29.24 $\pm 7.12$</td>
<td>0.53 $\pm 0.12$</td>
</tr>
<tr>
<td></td>
<td>VAE</td>
<td>68.44 $\pm 2.1$</td>
<td>74.52 $\pm 1.6$</td>
<td>0.09 $\pm 0.0$</td>
<td>8.42 $\pm 1.32$</td>
<td>12.42 $\pm 1.8$</td>
<td>0.75 $\pm 0.03$</td>
<td>0.58 $\pm 0.14$</td>
<td>1.34 $\pm 0.28$</td>
<td>1.07 $\pm 0.05$</td>
</tr>
<tr>
<td>Contrastive</td>
<td>AE</td>
<td>96.12 $\pm 1.73$</td>
<td>97.71 $\pm 1.12$</td>
<td>-</td>
<td>67.36 $\pm 20.12$</td>
<td>76.98 $\pm 17.6$</td>
<td>-</td>
<td>44.65 $\pm 32.39$</td>
<td>55.38 $\pm 29.98$</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>99.28 $\pm 0.53$</td>
<td>99.6 $\pm 0.31$</td>
<td>-</td>
<td>78.85 $\pm 7.5$</td>
<td>84.81 $\pm 6.21$</td>
<td>-</td>
<td>50.1 $\pm 9.94$</td>
<td>60.25 $\pm 10.11$</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>99.71 $\pm 0.13$</td>
<td>99.84 $\pm 0.09$</td>
<td>-</td>
<td>84.3 $\pm 2.84$</td>
<td>89.35 $\pm 2.26$</td>
<td>-</td>
<td>52.36 $\pm 4.03$</td>
<td>63.28 $\pm 4.28$</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 4: Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) (higher is better) and Reconstruction Error (lower is better) for different models and training losses for 1, 5 and 10 step prediction for the Observed Physics environment setting with 5 objects.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) (higher is better) and Reconstruction Error (lower is better) for different models and training losses for 1, 5 and 10 step prediction for the Unobserved Physics environment setting with 5 objects.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>1 Step</th>
<th></th>
<th></th>
<th>5 Steps</th>
<th></th>
<th></th>
<th>10 Steps</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Model</td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
<td>H@1</td>
<td>MRR</td>
<td>Rec.</td>
</tr>
<tr>
<td>NLL</td>
<td>AE</td>
<td>$65.69 \pm 1.93$</td>
<td>$73.4 \pm 1.66$</td>
<td>$0.12 \pm 0.01$</td>
<td>$17.98 \pm 0.95$</td>
<td>$25.84 \pm 1.15$</td>
<td>$0.3 \pm 0.01$</td>
<td>$6.56 \pm 0.6$</td>
<td>$11.64 \pm 0.98$</td>
<td>$0.39 \pm 0.02$</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>$62.27 \pm 3.7$</td>
<td>$70.16 \pm 1.5$</td>
<td>$0.15 \pm 0.01$</td>
<td>$19.32 \pm 1.64$</td>
<td>$26.2 \pm 2.14$</td>
<td>$0.34 \pm 0.02$</td>
<td>$8.87 \pm 1.35$</td>
<td>$14.09 \pm 2.01$</td>
<td>$0.42 \pm 0.02$</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>$75.23 \pm 3.68$</td>
<td>$82.73 \pm 1.01$</td>
<td>$0.12 \pm 0.01$</td>
<td>$24.93 \pm 2.64$</td>
<td>$33.96 \pm 3.08$</td>
<td>$0.31 \pm 0.01$</td>
<td>$10.39 \pm 1.47$</td>
<td>$16.71 \pm 2.31$</td>
<td>$0.39 \pm 0.01$</td>
</tr>
<tr>
<td></td>
<td>VAE</td>
<td>$52.83 \pm 1.98$</td>
<td>$61.68 \pm 1.85$</td>
<td>$0.28 \pm 0.01$</td>
<td>$1.96 \pm 0.16$</td>
<td>$3.92 \pm 0.26$</td>
<td>$0.88 \pm 0.05$</td>
<td>$0.19 \pm 0.04$</td>
<td>$0.62 \pm 0.03$</td>
<td>$1.0 \pm 0.07$</td>
</tr>
<tr>
<td>NLL</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Finetuned</td>
<td>AE</td>
<td>$95.35 \pm 1.13$</td>
<td>$97.02 \pm 0.75$</td>
<td>$0.06 \pm 0.01$</td>
<td>$40.92 \pm 7.81$</td>
<td>$49.77 \pm 7.94$</td>
<td>$0.21 \pm 0.02$</td>
<td>$9.41 \pm 4.36$</td>
<td>$13.92 \pm 5.64$</td>
<td>$0.35 \pm 0.03$</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>$74.19 \pm 5.88$</td>
<td>$80.08 \pm 3.04$</td>
<td>$0.07 \pm 0.01$</td>
<td>$20.13 \pm 8.28$</td>
<td>$26.32 \pm 9.43$</td>
<td>$0.16 \pm 0.01$</td>
<td>$2.3 \pm 2.97$</td>
<td>$3.94 \pm 4.06$</td>
<td>$0.25 \pm 0.02$</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>$94.92 \pm 1.84$</td>
<td>$96.79 \pm 1.24$</td>
<td>$0.07 \pm 0.01$</td>
<td>$27.62 \pm 6.53$</td>
<td>$34.7 \pm 7.51$</td>
<td>$0.21 \pm 0.02$</td>
<td>$2.52 \pm 1.21$</td>
<td>$4.16 \pm 1.74$</td>
<td>$0.32 \pm 0.03$</td>
</tr>
<tr>
<td></td>
<td>VAE</td>
<td>$49.65 \pm 4.14$</td>
<td>$59.58 \pm 3.92$</td>
<td>$0.07 \pm 0.01$</td>
<td>$7.82 \pm 1.04$</td>
<td>$12.3 \pm 1.48$</td>
<td>$0.25 \pm 0.03$</td>
<td>$0.83 \pm 0.16$</td>
<td>$2.05 \pm 0.29$</td>
<td>$0.36 \pm 0.04$</td>
</tr>
<tr>
<td>Contrastive</td>
<td>AE</td>
<td>$89.77 \pm 3.3$</td>
<td>$94.0 \pm 2.11$</td>
<td>-</td>
<td>$37.57 \pm 9.15$</td>
<td>$53.53 \pm 8.72$</td>
<td>-</td>
<td>$13.87 \pm 7.64$</td>
<td>$26.54 \pm 10.18$</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GNN</td>
<td>$89.58 \pm 3.13$</td>
<td>$93.4 \pm 3.42$</td>
<td>-</td>
<td>$40.33 \pm 10.2$</td>
<td>$50.14 \pm 10.18$</td>
<td>-</td>
<td>$17.74 \pm 0.99$</td>
<td>$25.67 \pm 4.26$</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Modular</td>
<td>$96.55 \pm 3.09$</td>
<td>$97.96 \pm 1.06$</td>
<td>-</td>
<td>$62.15 \pm 12.91$</td>
<td>$71.49 \pm 13.62$</td>
<td>-</td>
<td>$31.02 \pm 10.96$</td>
<td>$42.39 \pm 12.6$</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 5: Hits at Rank 1 (H@1), Mean Reciprocal Rank (MRR) (higher is better) and Reconstruction Error (lower is better) for different models and training losses for 1, 5 and 10 step prediction for the Unobserved Physics environment setting with 3 objects.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/dido1998/CausalMBRL
${ }^{2}$ https://www.hdfgroup.org/solutions/hdf5/
${ }^{3}$ https://pypi.org/project/h5py/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-252 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-252</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-252</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-273501954</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.15580v1.pdf" target="_blank">Language Models are Symbolic Learners in Arithmetic</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are thought to struggle with arithmetic learning due to the inherent differences between language modeling and numerical computation, but concrete evidence has been lacking. This work responds to this claim through a two-side experiment. We first investigate whether LLMs leverage partial products during arithmetic learning. We find that although LLMs can identify some partial products after learning, they fail to leverage them for arithmetic tasks, conversely. We then explore how LLMs approach arithmetic symbolically by breaking tasks into subgroups, hypothesizing that difficulties arise from subgroup complexity and selection. Our results show that when subgroup complexity is fixed, LLMs treat a collection of different arithmetic operations similarly. By analyzing position-level accuracy across different training sizes, we further observe that it follows a U-shaped pattern: LLMs quickly learn the easiest patterns at the first and last positions, while progressively learning the more difficult patterns in the middle positions. This suggests that LLMs select subgroup following an easy-to-hard paradigm during learning. Our work confirms that LLMs are pure symbolic learners in arithmetic tasks and underscores the importance of understanding them deeply through subgroup-level quantification.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e252.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e252.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PartialProductsExp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Partial-products identification and usage experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning experiments testing whether LLMs generate or use partial products for multiplication by probing four calculation algorithms (standard, lattice, repetitive addition, Egyptian) and testing bidirectionality (Task→PartialProducts and PartialProducts→Task).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B; Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B; 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>autoregressive (decoder-only) transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multiplication (diagnostic partial-products for standard multiplication, lattice method, repetitive addition, Egyptian multiplication)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>two-digit × two-digit multiplication (partial-products derived from 2-digit operands)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning (LoRA) on multiplication tasks and on diagnostic partial-product sets; diagnostic probing sets for four multiplication algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>After fine-tuning on the multiplication task, models' ability to identify partial products increased for standard, lattice, and Egyptian paths (average reported gains across models: standard ≈ +17.45%, lattice ≈ +18.35%, Egyptian ≈ +10.45%); repetitive-addition partial product identification remained very low (~5%). Conversely, fine-tuning on partial-product diagnostic sets did NOT improve overall multiplication task accuracy and often decreased it (PartialProducts→Task accuracy change ranged from small decreases up to ≈ −20% for Gemma and very large decreases for certain cases in Llama; Table 1 reports e.g. Gemma Task→Partial P.: standard +4.1%, lattice +6.8%, repetitive −29.0%, egyptian +3.6%; Llama Task→Partial P.: standard +40.6%, lattice +40.8%, repetitive −59.0%, egyptian +29.6%; PartialP→Task shows negative deltas).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>LLMs learn token-level symbolic subgroups that allow identification of many partial products as a byproduct of pattern learning, but they do not appear to use partial products as an internal computational mechanism to perform multiplication (no bidirectional transfer). Improved partial-product recognition is symptomatic of symbolic subgroup learning rather than reflective of an algorithmic multiplication process.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Phenomenon observed in both a 2B and an 8B model with similar qualitative patterns; no claim of monotonic improvement with size beyond those two models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Models fail to exploit repetitive-addition style partial products (very low identification), and training only on partial products can harm overall multiplication performance; inability to use identified partial products to deduce full-product mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Before vs after fine-tuning on tasks (Task→PartialProducts) and fine-tuning on diagnostic partial-product sets then testing on task (PartialProducts→Task).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>LLMs can learn to recognize many partial products but do not leverage them to compute multiplication; partial-product recognition is a symbolic byproduct, not the model's computational strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models are Symbolic Learners in Arithmetic', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e252.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e252.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LabelEntropyPerturbation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Label-space entropy and rule-perturbation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled perturbations of arithmetic rules (translations, scalings, modular reductions) to test whether LLM arithmetic performance depends on label-space entropy H(L) rather than arithmetic semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B; Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B; 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>autoregressive (decoder-only) transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition and multiplication with perturbations: translation (a+b+Δc), scaling (a×b×λ), modular arithmetic ((a+b) mod M, (a×b) mod M)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>2-digit and multi-digit addition/multiplication (experiments reported across 2-digit baselines and larger digit counts; perturbations applied preserving or reducing label-space entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on perturbed functions (same training regime as baseline), measuring test accuracy differences for different perturbation magnitudes and modular reductions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When label-space entropy H(L) was held approximately constant (translation/scaling), accuracy changed only marginally (e.g., scaling ×2/×4/×8 produced small ± percentage changes). When label-space entropy was reduced via modular operations, accuracy improved substantially (examples from Table 4: for addition, (a+b) mod 100: Gemma +10.1%, Llama +3.7%; mod50: Gemma +13.1%, Llama +6.7%; mod10: Gemma +26.1%, Llama +13.7%; for multiplication, (a×b) mod100: Gemma +7.1%, Llama +3.8%; mod50: Gemma +12.1%, Llama +5.3%; mod10: Gemma +18.9%, Llama +10.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Label-space entropy H(L) is predictive of learnability: lower entropy (fewer possible output tokens) makes arithmetic tasks easier for LLMs. Models treat perturbed arithmetic rules similarly when H(L) is unchanged, indicating reliance on token-level symbolic mapping rather than arithmetic semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Consistent effects across the two tested models (2B and 8B); reductions in entropy produce improved accuracy for both.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High label-space entropy tasks remain hard even if mathematically simple; models do not generalize from one arithmetic rule to another unless the label-space/statistical mapping is favorable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baseline (standard a+b or a×b) vs perturbed rules (translation, scaling, modular reductions).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>LLMs' arithmetic performance is governed primarily by label-space entropy (output variability); when H(L) is fixed models treat different arithmetic rule variants similarly, and reducing H(L) (e.g., modulo) improves accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models are Symbolic Learners in Arithmetic', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e252.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e252.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SubgroupSelectionUcurve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subgroup selection dynamics and U-shaped position-level accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Position-wise analysis across learning shows a U-shaped accuracy curve: high accuracy at initial and final output digits and low accuracy for middle digits, attributed to subgroup quality Q(s) and an easy-to-hard subgroup selection process.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B; Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B; 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>autoregressive (decoder-only) transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multiplication (3-digit to 5-digit multiplication; output 6–10 digits depending on operands)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>3-digit to 5-digit multiplication problems (outputs spanning 6–10 digits)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on multiplication with varying dataset sizes (6.48K, 12.96K, 32.4K, 64.8K); analyzing position-level accuracy over training epochs/checkpoints</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Position-level accuracy forms a U-shaped curve: beginning (most significant) and end (least significant) output digits exceed ~95% accuracy, while middle digits drop to ~10% accuracy in higher-digit multiplication (e.g., middle positions such as 4th/5th in 4-digit multiplication and 5th/6th in 5-digit multiplication show ~10% accuracy). Increasing training size allows progressive learning of middle positions but the U-shape persists during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>LLMs select subgroups in an easy-to-hard order: high-quality subgroups (low domain ambiguity, high Q(s)) corresponding to edge digits are learned first, while middle-digit subgroups with larger domain cardinality and lower Q(s) are learned later. This indicates symbolic subgroup selection rather than algorithmic carry-based computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>With more training data the model progressively learns harder (lower-Q) subgroups—the middle digits improve over training—but the qualitative U-shaped learning pattern is consistent across dataset sizes and both models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Systematic failure on middle output digits (low accuracy), indicating concentrated difficulty where more operand digits affect the target digit (carry/aggregation complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Different dataset sizes and digit-length tasks compared; positional accuracy tracked across training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>LLMs learn arithmetic by selecting and internalizing high-quality symbolic subgroups first (edges), progressing to low-quality subgroups (middle digits); this yields a U-shaped position-wise accuracy curve reflecting symbolic, easy-to-hard learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models are Symbolic Learners in Arithmetic', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e252.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e252.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FormatPerturbationRobustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input-format perturbation robustness experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tests of robustness to input-format changes (natural language phrasing, random strings, disturbed digits) while preserving the same output label-space to see whether superficial input changes alter arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B; Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B; 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>autoregressive (decoder-only) transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition and multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>basic addition and multiplication tasks (same label space as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>format perturbations applied to inputs: Natural Language (NL), Random String (RS), Disturbed Digits (DD); fine-tuning and evaluation with fixed output label-space</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Performance remained largely unchanged when output label-space was fixed: NL and RS formats caused only marginal accuracy changes; DD formats caused minor fluctuations but no significant degradation across both addition and multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Robustness to input-format perturbations (so long as the output label-space is consistent) supports the interpretation that LLMs rely on token-level symbolic mappings rather than shallow surface cues — they map input token patterns to learned output tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Observed across both tested model sizes with similar qualitative outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When the output label-space changes (i.e., entropy changes) performance changes; superficial input perturbations alone do not break the learned symbolic mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baseline digit-token format vs NL, RS, DD perturbed input formats.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>LLMs are robust to superficial input-format variations for arithmetic tasks if the label-space/statistical mapping is preserved, consistent with symbolic token-level pattern learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models are Symbolic Learners in Arithmetic', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Interpreting and improving large language models in arithmetic calculation <em>(Rating: 2)</em></li>
                <li>GPT can solve mathematical problems without a calculator <em>(Rating: 1)</em></li>
                <li>From explicit cot to implicit cot: Learning to internalize cot step by step <em>(Rating: 1)</em></li>
                <li>Reverse that number! decoding order matters in arithmetic learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-252",
    "paper_id": "paper-273501954",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "PartialProductsExp",
            "name_full": "Partial-products identification and usage experiments",
            "brief_description": "Fine-tuning experiments testing whether LLMs generate or use partial products for multiplication by probing four calculation algorithms (standard, lattice, repetitive addition, Egyptian) and testing bidirectionality (Task→PartialProducts and PartialProducts→Task).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B; Llama-3.1-8B",
            "model_size": "2B; 8B",
            "model_architecture": "autoregressive (decoder-only) transformer",
            "arithmetic_operation_type": "multiplication (diagnostic partial-products for standard multiplication, lattice method, repetitive addition, Egyptian multiplication)",
            "number_range_or_complexity": "two-digit × two-digit multiplication (partial-products derived from 2-digit operands)",
            "method_or_intervention": "fine-tuning (LoRA) on multiplication tasks and on diagnostic partial-product sets; diagnostic probing sets for four multiplication algorithms",
            "performance_result": "After fine-tuning on the multiplication task, models' ability to identify partial products increased for standard, lattice, and Egyptian paths (average reported gains across models: standard ≈ +17.45%, lattice ≈ +18.35%, Egyptian ≈ +10.45%); repetitive-addition partial product identification remained very low (~5%). Conversely, fine-tuning on partial-product diagnostic sets did NOT improve overall multiplication task accuracy and often decreased it (PartialProducts→Task accuracy change ranged from small decreases up to ≈ −20% for Gemma and very large decreases for certain cases in Llama; Table 1 reports e.g. Gemma Task→Partial P.: standard +4.1%, lattice +6.8%, repetitive −29.0%, egyptian +3.6%; Llama Task→Partial P.: standard +40.6%, lattice +40.8%, repetitive −59.0%, egyptian +29.6%; PartialP→Task shows negative deltas).",
            "mechanistic_insight": "LLMs learn token-level symbolic subgroups that allow identification of many partial products as a byproduct of pattern learning, but they do not appear to use partial products as an internal computational mechanism to perform multiplication (no bidirectional transfer). Improved partial-product recognition is symptomatic of symbolic subgroup learning rather than reflective of an algorithmic multiplication process.",
            "performance_scaling": "Phenomenon observed in both a 2B and an 8B model with similar qualitative patterns; no claim of monotonic improvement with size beyond those two models.",
            "failure_modes": "Models fail to exploit repetitive-addition style partial products (very low identification), and training only on partial products can harm overall multiplication performance; inability to use identified partial products to deduce full-product mapping.",
            "comparison_baseline": "Before vs after fine-tuning on tasks (Task→PartialProducts) and fine-tuning on diagnostic partial-product sets then testing on task (PartialProducts→Task).",
            "key_finding": "LLMs can learn to recognize many partial products but do not leverage them to compute multiplication; partial-product recognition is a symbolic byproduct, not the model's computational strategy.",
            "uuid": "e252.0",
            "source_info": {
                "paper_title": "Language Models are Symbolic Learners in Arithmetic",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LabelEntropyPerturbation",
            "name_full": "Label-space entropy and rule-perturbation experiments",
            "brief_description": "Controlled perturbations of arithmetic rules (translations, scalings, modular reductions) to test whether LLM arithmetic performance depends on label-space entropy H(L) rather than arithmetic semantics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B; Llama-3.1-8B",
            "model_size": "2B; 8B",
            "model_architecture": "autoregressive (decoder-only) transformer",
            "arithmetic_operation_type": "addition and multiplication with perturbations: translation (a+b+Δc), scaling (a×b×λ), modular arithmetic ((a+b) mod M, (a×b) mod M)",
            "number_range_or_complexity": "2-digit and multi-digit addition/multiplication (experiments reported across 2-digit baselines and larger digit counts; perturbations applied preserving or reducing label-space entropy)",
            "method_or_intervention": "fine-tuning on perturbed functions (same training regime as baseline), measuring test accuracy differences for different perturbation magnitudes and modular reductions",
            "performance_result": "When label-space entropy H(L) was held approximately constant (translation/scaling), accuracy changed only marginally (e.g., scaling ×2/×4/×8 produced small ± percentage changes). When label-space entropy was reduced via modular operations, accuracy improved substantially (examples from Table 4: for addition, (a+b) mod 100: Gemma +10.1%, Llama +3.7%; mod50: Gemma +13.1%, Llama +6.7%; mod10: Gemma +26.1%, Llama +13.7%; for multiplication, (a×b) mod100: Gemma +7.1%, Llama +3.8%; mod50: Gemma +12.1%, Llama +5.3%; mod10: Gemma +18.9%, Llama +10.7%).",
            "mechanistic_insight": "Label-space entropy H(L) is predictive of learnability: lower entropy (fewer possible output tokens) makes arithmetic tasks easier for LLMs. Models treat perturbed arithmetic rules similarly when H(L) is unchanged, indicating reliance on token-level symbolic mapping rather than arithmetic semantics.",
            "performance_scaling": "Consistent effects across the two tested models (2B and 8B); reductions in entropy produce improved accuracy for both.",
            "failure_modes": "High label-space entropy tasks remain hard even if mathematically simple; models do not generalize from one arithmetic rule to another unless the label-space/statistical mapping is favorable.",
            "comparison_baseline": "Baseline (standard a+b or a×b) vs perturbed rules (translation, scaling, modular reductions).",
            "key_finding": "LLMs' arithmetic performance is governed primarily by label-space entropy (output variability); when H(L) is fixed models treat different arithmetic rule variants similarly, and reducing H(L) (e.g., modulo) improves accuracy.",
            "uuid": "e252.1",
            "source_info": {
                "paper_title": "Language Models are Symbolic Learners in Arithmetic",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SubgroupSelectionUcurve",
            "name_full": "Subgroup selection dynamics and U-shaped position-level accuracy",
            "brief_description": "Position-wise analysis across learning shows a U-shaped accuracy curve: high accuracy at initial and final output digits and low accuracy for middle digits, attributed to subgroup quality Q(s) and an easy-to-hard subgroup selection process.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B; Llama-3.1-8B",
            "model_size": "2B; 8B",
            "model_architecture": "autoregressive (decoder-only) transformer",
            "arithmetic_operation_type": "multiplication (3-digit to 5-digit multiplication; output 6–10 digits depending on operands)",
            "number_range_or_complexity": "3-digit to 5-digit multiplication problems (outputs spanning 6–10 digits)",
            "method_or_intervention": "fine-tuning on multiplication with varying dataset sizes (6.48K, 12.96K, 32.4K, 64.8K); analyzing position-level accuracy over training epochs/checkpoints",
            "performance_result": "Position-level accuracy forms a U-shaped curve: beginning (most significant) and end (least significant) output digits exceed ~95% accuracy, while middle digits drop to ~10% accuracy in higher-digit multiplication (e.g., middle positions such as 4th/5th in 4-digit multiplication and 5th/6th in 5-digit multiplication show ~10% accuracy). Increasing training size allows progressive learning of middle positions but the U-shape persists during learning.",
            "mechanistic_insight": "LLMs select subgroups in an easy-to-hard order: high-quality subgroups (low domain ambiguity, high Q(s)) corresponding to edge digits are learned first, while middle-digit subgroups with larger domain cardinality and lower Q(s) are learned later. This indicates symbolic subgroup selection rather than algorithmic carry-based computation.",
            "performance_scaling": "With more training data the model progressively learns harder (lower-Q) subgroups—the middle digits improve over training—but the qualitative U-shaped learning pattern is consistent across dataset sizes and both models.",
            "failure_modes": "Systematic failure on middle output digits (low accuracy), indicating concentrated difficulty where more operand digits affect the target digit (carry/aggregation complexity).",
            "comparison_baseline": "Different dataset sizes and digit-length tasks compared; positional accuracy tracked across training.",
            "key_finding": "LLMs learn arithmetic by selecting and internalizing high-quality symbolic subgroups first (edges), progressing to low-quality subgroups (middle digits); this yields a U-shaped position-wise accuracy curve reflecting symbolic, easy-to-hard learning dynamics.",
            "uuid": "e252.2",
            "source_info": {
                "paper_title": "Language Models are Symbolic Learners in Arithmetic",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FormatPerturbationRobustness",
            "name_full": "Input-format perturbation robustness experiments",
            "brief_description": "Tests of robustness to input-format changes (natural language phrasing, random strings, disturbed digits) while preserving the same output label-space to see whether superficial input changes alter arithmetic performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B; Llama-3.1-8B",
            "model_size": "2B; 8B",
            "model_architecture": "autoregressive (decoder-only) transformer",
            "arithmetic_operation_type": "addition and multiplication",
            "number_range_or_complexity": "basic addition and multiplication tasks (same label space as baselines)",
            "method_or_intervention": "format perturbations applied to inputs: Natural Language (NL), Random String (RS), Disturbed Digits (DD); fine-tuning and evaluation with fixed output label-space",
            "performance_result": "Performance remained largely unchanged when output label-space was fixed: NL and RS formats caused only marginal accuracy changes; DD formats caused minor fluctuations but no significant degradation across both addition and multiplication.",
            "mechanistic_insight": "Robustness to input-format perturbations (so long as the output label-space is consistent) supports the interpretation that LLMs rely on token-level symbolic mappings rather than shallow surface cues — they map input token patterns to learned output tokens.",
            "performance_scaling": "Observed across both tested model sizes with similar qualitative outcomes.",
            "failure_modes": "When the output label-space changes (i.e., entropy changes) performance changes; superficial input perturbations alone do not break the learned symbolic mappings.",
            "comparison_baseline": "Baseline digit-token format vs NL, RS, DD perturbed input formats.",
            "key_finding": "LLMs are robust to superficial input-format variations for arithmetic tasks if the label-space/statistical mapping is preserved, consistent with symbolic token-level pattern learning.",
            "uuid": "e252.3",
            "source_info": {
                "paper_title": "Language Models are Symbolic Learners in Arithmetic",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Interpreting and improving large language models in arithmetic calculation",
            "rating": 2,
            "sanitized_title": "interpreting_and_improving_large_language_models_in_arithmetic_calculation"
        },
        {
            "paper_title": "GPT can solve mathematical problems without a calculator",
            "rating": 1,
            "sanitized_title": "gpt_can_solve_mathematical_problems_without_a_calculator"
        },
        {
            "paper_title": "From explicit cot to implicit cot: Learning to internalize cot step by step",
            "rating": 1,
            "sanitized_title": "from_explicit_cot_to_implicit_cot_learning_to_internalize_cot_step_by_step"
        },
        {
            "paper_title": "Reverse that number! decoding order matters in arithmetic learning",
            "rating": 1,
            "sanitized_title": "reverse_that_number_decoding_order_matters_in_arithmetic_learning"
        }
    ],
    "cost": 0.013083,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models are Symbolic Learners in Arithmetic
21 Oct 2024</p>
<p>Chunyuan Deng chunyuan.deng@rice.edu 
Rice University</p>
<p>Zhiqi Li 
Georgia Tech</p>
<p>Roy Xie 
Duke University</p>
<p>Ruidi Chang 
Rice University</p>
<p>Hanjie Chen hanjie@rice.edu 
Rice University</p>
<p>Language Models are Symbolic Learners in Arithmetic
21 Oct 20242255C5EBE0F994CE860E1A3FFB6EC8D6arXiv:2410.15580v1[cs.LG]
Large Language Models (LLMs) are thought to struggle with arithmetic learning due to the inherent differences between language modeling and numerical computation, but concrete evidence has been lacking.This work responds to this claim through a two-side experiment.We first investigate whether LLMs leverage partial products during arithmetic learning.We find that although LLMs can identify some partial products after learning, they fail to leverage them for arithmetic tasks, conversely.We then explore how LLMs approach arithmetic symbolically by breaking tasks into subgroups, hypothesizing that difficulties arise from subgroup complexity and selection.Our results show that when subgroup complexity is fixed, LLMs treat a collection of different arithmetic operations similarly.By analyzing positionlevel accuracy across different training sizes, we further observe that it follows a U-shaped pattern: LLMs quickly learn the easiest patterns at the first and last positions, while progressively learning the more difficult patterns in the middle positions.This suggests that LLMs select subgroup following an easy-tohard paradigm during learning.Our work confirms that LLMs are pure symbolic learners in arithmetic tasks and underscores the importance of understanding them deeply through subgroup-level quantification.</p>
<p>Introduction</p>
<p>Modern math benchmarks like MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) have been rapidly saturated due to the advancements of frontier language models like GPT-4o (OpenAI et al., 2024) and Claude (Anthropic, 2024).This trend is driving the assessment of these models toward more challenging tasks, such as Olympic Math.However, it has been observed that even the most advanced language models struggle with basic arithmetic, such as 5-digit multiplication (Yang et al., 2023).This notable gap raises questions about the underlying mechanisms behind arithmetic in language models.And some hypothesize (Boguraev et al., 2024) that this difficulty stems from the fact that mathematical calculation differs fundamentally from autoregressive language modeling.</p>
<p>Previous research on this topic has primarily focused on causal abstraction to identify model components such as key attention layers (Stolfo et al., 2023) or attention heads (Zhang et al., 2024) responsible for arithmetic learning.While these studies provide valuable insights into the components involved in mathematical operations, they fall short of explaining why frontier models continue to struggle with certain arithmetic tasks.For instance, causal attribution reveals that the 14th attention head in the 19th layer is responsible for the operation 37 + 14 = 51.However, it cannot explain why the model handles 37 + 14 successfully but fails on 37 × 14.This observation suggests that there is still room to explore arithmetic learning from alternative perspectives.</p>
<p>To achieve this, we approach this problem from two sides (shown in Figure 1).First, we examine whether LLMs leverage partial products in arithmetic learning tasks.Then we explore whether, and more importantly, how LLMs handle arithmetic in a purely symbolic manner.Specifically, we decompose the task into subgroup level, hypothesizing the task can be understood through two aspects: subgroup complexity and subgroup selection.</p>
<p>For partial products, we considered four distinct methods for multiplication calculation: standard multiplication, repetitive addition, the lattice method, and Egyptian multiplication (detailed in §5.1), each generating distinct paths for partial product computation.We first fine-tune LLMs on multiplication tasks.After fine-tuning, models improved in identifying nearly all partial products, but explicit training on partial products did not Figure 1: Fundamental structure of the paper.We begin by investigating partial products and proceed to a detailed examination at the subgroup level to understand the mechanism in a symbolic manner.</p>
<p>Autoregressive Language Modeling</p>
<p>enhance their ability to perform multiplications, instead.These findings suggest that large language models may not leverage partial products to perform arithmetic calculations.The increased ability to identify partial products seems to be a byproduct of subgroup-level symbol learning rather than a mechanism the models inherently use for computation.</p>
<p>We then delve into a more fine-grained level by examining subgroups to understand their basic complexity.Subgroup complexity refers to the intrinsic complexity of the subgroup itself.We propose that this is related to the domain space cardinality |D|, label space entropy H(L), and subgroup quality Q(s).Here, |D| represents the maximum size of training data, H(L) represents the variability in the label space, and Q(s) refers to how deterministically one subgroup can map from the domain to the label space.While |D|, which strongly correlated with training size, is selfevident to influence learning, we will focus on label space entropy H(L) first then discuss Q(s) in the subgroup selection section.We observe influence of H(L) by introducing perturbations to addition and multiplication by applying translation (∆c) and scaling (×λ), while maintaining the total entropy the same across all output digits.Our findings show that LLMs have nearly the same accuracy across different perturbation magnitudes.Furthermore, when we intentionally reduce the entropy of the label space via modular operations, we observe an increase in accuracy as entropy decreases.These experiments confirm that label space entropy is an effective measure for quantifying task complexity and validates the hypothesis that LLMs operate as symbol-level pattern finders.</p>
<p>Subgroup selection refers to the process by which LLMs identify the correct subgroup during training, specifically by selecting the appropriate mapping between a token subset in the input domain and a corresponding token subset in the output label space.To investigate this, we curated four distinct training sets to analyze position-level accuracy throughout the learning process.Consistent patterns emerged: position-level accuracy exhibited a U-shaped curve, achieving near-perfect accuracy (close to 100%) at both the beginning and end of digits with high Q(s), but dropping significantly (to below 10%) across the intermediate digits with low Q(s).These results suggest that LLMs apply a selection mechanism driven by high-to-low subgroup quality, providing further evidence of their symbolic learning behavior.</p>
<p>Our work confirms that LLMs do not truly perform calculations in arithmetic learning.Instead, they approach it in a purely symbolic manner.We then provide a systematic framework to examine this mechanism from subgroup complexity and subgroup selection.Our research emphasizes the pivotal role of label space entropy in the convergence stage and the impact of subgroup quality on learning dynamics, highlighting the importance of deeply understanding arithmetic through subgrouplevel quantification symbolically.</p>
<p>Related Work</p>
<p>Mathematical Learning in LLMs Mathematical reasoning has been a longstanding area of research in natural language processing (NLP) (Kushman et al., 2014;Huang et al., 2016;Wang et al., 2017;Thawani et al., 2021;Sundaram et al., 2022;Guo et al., 2024).With recent advances in LLM, numerous studies have sought to improve their mathematical reasoning abilities through various techniques, including data annealing (Dubey et al., 2024), continued pretraining (Lewkowycz et al., 2022), fine-tuning (Yue et al., 2023;Liu et al., 2023), prompting (Wei et al., 2023;Wang et al., 2023), and inference-time computation (Zhou et al., 2023a;Wu et al., 2024a).However, LLMs still face challenges with basic calculations and remain vulnerable to adversarial examples or perturbations, where minor changes in problems can result in incorrect answers (Zhou et al., 2023b;Xie et al., 2024).Most research on LLMs' mathematical reasoning focuses on math word problems, where natural language is involved in the questions (Hendrycks et al., 2021;Cobbe et al., 2021;Arora et al., 2023;Zhao et al., 2024a,b).</p>
<p>Arithmetic Learning in Transformer Several previous efforts have aimed to improve arithmetic learning in LLMs. Lee et al. (2023) trained a 10.6M NanoGPT (Karpathy, 2022) model to learn arithmetic by carefully curating the data format, explicitly expanding each step using a method termed Scratchpad, which achieved remarkable performance compared to GPT-2 XL (Radford et al., 2019).Yang et al. (2023) fine-tuned MathGLM with a sufficient training dataset, demonstrating its capability to solve 5-digit multiplication.Deng et al. (2023Deng et al. ( , 2024) ) further advanced this field by internalizing the CoT process, hiding detailed steps in a scheduled manner, enabling GPT-2 small to solve 9-digit multiplication after multiple training runs.</p>
<p>Research on understanding arithmetic primarily stems from the interpretability community.The core idea is to identify causal correlations between model components and outputs.Stolfo et al. (2023) identify key attention layers responsible for arithmetic learning using causal mediation analysis (CMA), a weight perturbation method that observes changes in output.Similarly, Hanna et al. (2023) and Wu et al. (2024b) explore causal abstraction concepts at different model scales, specifically 0.1B and 7B parameters, respectively.More recently, Zhang et al. (2024) employed an attention attribution to isolate a small subset of attention heads and fine-tune them for improved performance at a lower cost.While these studies have made progress in understanding how LMs perform arithmetic at a component level, there remains a gap in understanding the learning mechanisms from a purely symbolic perspective.Our research aims to contribute to this missing gap in a systematic manner.</p>
<p>Preliminaries</p>
<p>In this section, we present the preliminaries of basic autoregressive language modeling, along with algebraic structure and arithmetic learning.</p>
<p>Autoregressive Language Modeling An autoregressive (AR) language model predicts the next token in a sequence based on the previously observed tokens.Formally, given a sequence of tokens x = {x 1 , x 2 , . . ., x T }, the probability of the sequence is decomposed using the chain rule of probability as:
P (x) = T t=1 P (x t |x 1 , x 2 , . . . , x t−1 ),(1)
where P (x t |x 1 , x 2 , . . ., x t−1 ) represents the conditional probability of token x t given all previous tokens.In autoregressive modeling, the next token is sampled from the conditional distribution P (x t |x 1 , x 2 , . . ., x t−1 ) until the end of the sequence is reached.</p>
<p>Algebraic Structure In our setting, we employ the algebraic structure known as a ring, which provides a formal framework for the arithmetic operations of addition and multiplication.A ring (R, +, •) is defined by:</p>
<p>• A set R (domain) of elements.Specifically, the domain R in our task is the set of integers Z.</p>
<p>• Two binary operations, addition and multiplication
f (a, b) = c : R×R → R. Specifically, we use A 1 A 2 + B 1 B 2 = C 1 C 2 to represent 2-digit addition. We use A 1 A 2 × B 1 B 2 = C 1 C 2 C 3 C 4 to represent 2-digit multiplica- tion.
In our case, for all a, b ∈ R, there exists a unique element a + b ∈ R, representing the sum of a and b.Similarly, there exists a unique element a • b ∈ R, representing the product of a and b.</p>
<p>Arithmetic Learning Let M denote a pretrained autoregressive language model with weights w.We define an arithmetic task T as a function learning problem where the goal is to predict numerical outputs based on arithmetic expressions.The training dataset for this task is given by:
D train = {(a (k) , b (k) , c (k) } N where c (k) = f (a (k) , b (k)
) and f (•) represents a binary operator, N denotes the number of data points.In this context, a (k) and b (k) are input operands, and c (k) is the corresponding output, which is computed using the operator f (e.g., addition, multiplication, etc.)</p>
<p>Experiment Setup</p>
<p>In this section, we will detail our experiment setup.Unless otherwise specified, the same setup will be used in the following section.</p>
<p>Domain We select addition and multiplication as the fundamental operations for our experiments following previous work (Lee et al., 2023;Deng et al., 2023Deng et al., , 2024)).</p>
<p>Model To investigate arithmetic learning at scale, we selected two open-source LLMs, Gemma-2-2B (Team et al., 2024) and Llama-3.1-8B(Dubey et al., 2024).Both models are top performers in their respective categories and excel at languagerelated tasks.We did not choose GPT-4o (OpenAI et al., 2024) or other proprietary LLMs due to concerns that they may internally integrate function calling (e.g., invoking APIs or executing Python programs), which could affect the experimental setup.Training details is included at Appendix A.1.</p>
<p>Conventional Data Format</p>
<p>We directly train the model to predict the output (e.g., 130) given the input operands and the operator (e.g., 13 × 10).e add one space between each digit to ensure tokens are split into individual digits We do not consider chain-of-thought (CoT) (Wei et al., 2023) or other prompting strategies to enforce the model to focus on arithmetic learning.We include ablations with respect to data format in Appendix A.2.</p>
<p>Are Large Language Models Implicit</p>
<p>Calculators?</p>
<p>In this section, we explore whether LLMs utilize partial products to enhance their arithmetic calculation capabilities, particularly in the context of multiplication.It is important to note that while multiplication is well-defined mathematically, the process of multiplication calculation is not limited to traditional methods defined in textbook.Thus, examining only one calculation method presents a flawed experimental design that is vulnerable to exploitation.We selected four calculation methods that are representative to cover the major approaches to multiplication.</p>
<p>Historical and Modern Multiplication</p>
<p>In M3: Lattice Method In the lattice method (or grid method), we place the numbers along the top and side of a grid, perform single-digit multiplications, and then sum along the diagonals: Since 34 = 2 + 32, we select the results for 12 × 16 and 12 × 8, and summing these gives the final product.</p>
<p>Examining Partial Product in Arithmetic Learning</p>
<p>To investigate whether LLMs generate partial products during arithmetic learning, we employ a set of diagnostic tasks as an approach to trace.We finetune Gemma-2-2B and Llama-3.1-8B on two-digit multiplication, observing changes in accuracy on diagnostic sets before and after fine-tuning (Task → Partial Products).Subsequently, we fine-tune the LLMs on these diagnostic sets and examine how their accuracy on the multiplication task changes (Partial Products → Task).</p>
<p>Method Diagnostic Sets</p>
<p>Standard Multiplication We probe language models' partial product in four different directions.As shown in Table 2, for a task formatting like
Pstd = {A1 × B1B2, A2 × B1B2, B1 × A1A2, B2 × A1A2} Repetitive Addition Pra = { B1B2 i=1 A1A2, A1A2 i=1 B1B2} Lattice Method Plattice = {A10 × B10, A10 × B2, A2 × B10, A2 × B2} Egyptian Multiplication Pegyptian = {2 k × A1A2 | k ∈ 0, 1, . . . , ⌊log 2 (B1B2)⌋}A 1 A 2 × B 1 B 2 = C 1 C 2 C 3 C 4 ,
we would generate diagnostic test for each algorithm (detailed derivation in Appendix A.3).</p>
<p>Accuracy on Identifying Partial Products According to the results in Figure 2, we found that standard multiplication, the lattice method, and the Egyptian method significantly improved in identifying partial products after fine-tuning, with gains of +17.45%, +18.35%, and +10.45%, respectively.However, for repetitive addition tasks, LLMs failed to identify partial products, achieving an accuracy of only about 5% after fine-tuning.A Deeper Look into Calculations Do the results showing increased accuracy across three paths really imply that partial products are used in arithmetic learning?We have two arguments against this interpretation.First, if LLMs genuinely use partial products to learn arithmetic, it is likely that they only use one calculation path at a time.Thus, the simultaneous improvement across three paths (standard, lattice, and Egyptian) is unusual.Second, if LLMs employ a specific path to compute partial products, this process should be demonstrated as bidirectional.Specifically, LLMs fine-tuned on a task should be able to identify partial products (inductive), and conversely, mastering partial products should enhance task learning (deductive).However, we currently have results for only one direction, lacking evidence for the other.Therefore, we extend our experiments to another direction.</p>
<p>Accuracy on Identifying Tasks</p>
<p>We fine-tune two LLMs on diagnostic sets and present the results of identifying tasks before and after fine-tuning in Table 1.Our findings reveal that, fine-tuning specifically on partial products does not enhance task learning.Instead, it results in a performance drop across all four calculation paths for both models.This indicates that pre-learning partial products does not aid in arithmetic learning.The improved ability to recognize partial products appears to stem from the symbol learning process (note that the standard partial product
A 1 × B 1 B 2 is a sub-portion of A 1 A 2 × B 1 B 2
, similar to lattice and Egyptian methods) rather than being an intrinsic computational method used by the models.</p>
<p>6 Are Language Models Symbolic Observers?</p>
<p>An intuitive alternative for explaining increasing performance from inductive tasks (Task → Partial Products) is to treat LLMs as subgroup-level symbol observers, which aligns with the intrinsic properties of language modeling.Notably, the standard multiplication, lattice method, and Egyptian methods share a similar structure, where their partial product sets form token-level subgroups within the tasks.This observation naturally leads us to explore this idea further.</p>
<p>Define Subgroup in Token Level</p>
<p>We first define subgroup in this section.Arithmetic learning involves a training set k) ) and f (•) represents a binary operator, N is the number of dataset.In n-digit arithmetic learning, a (k) and b (k) can be regarded as different values taken by random variable sequences, {A i } n i=1 and {B i } n i=1 , respectively.The random variables A i and B i all follow a discrete uniform distribution P (X = x) = 1 10 , x = 0, 1, ..., 9. c (k) can be regarded as different values taken by random variable sequence {C i } m i=1 , where the random variables C i , i = 1, ..., m has a joint distribution given by:
D train = {(a (k) , b (k) ), c (k) } N k=1 where c (k) = f (a (k) , b(I {f (a,b)=c} P ({A i } n i=1 = a)P ({B i } n i=1 = b)
where I {f (a,b)=c} is indicator function equals 1 if the condition f (a, b) = c holds true, and 0 otherwise.</p>
<p>Definition 1 (Subgroup): For n-digit arithmetic, a subgroup s is defined as any element of the set S n :
s ∈ S n = {((A, B), C) | A ⊆ {A i } n i=1 , B ⊆ {B i } n i=1 , C ⊆ {C i } m i=1 }
where A, B and C can be any subportion of random variable sequences {A i } n i=1 , {B i } n i=1 and {C i } n i=1 , respectively.Specifically, we use s i ∈ S n to denote subgroups in i-th prediction for C i .</p>
<p>Definition 2 (Subgroup Space): For any subgroup s = ((A, B), C) ∈ S n , we have:
• Domain Space: D s = {({a} |A| k=1 , {b} |B| k=1 ) | P (A = {a} |A| k=1 ) &gt; 0, P (B = {b} |B| k=1 ) &gt; 0}.
The size of domain space or cardinality is annotated as |D s |.</p>
<p>• Label Space: L s = {{c} The entropy of label space is given by:
H(L s ) = − y∈Ls P (C = y) log 2 P (C = y)</p>
<p>Difficulty in Arithmetic Learning: A Well-educated Hypothesis</p>
<p>We propose the following hypothesis: For an ndigit arithmetic task requiring m predictions, the overall difficulty ζ is related to:
ζ ∝ ĥm(2)
where ĥ = m i=1 h(s i )</p>
<p>1 m represents the geometric average difficulty of an individual prediction, with s i ∈ S n denoting subgroup selection for the i-th prediction, and h(•) representing the subgroup complexity evaluation function.</p>
<p>Subgroup Complexity (h(•)): This represents the most basic difficulty in arithmetic learning.It captures the inherent difficulty in the structure of the arithmetic learning tasks.We believe that the complexity on subgroup is strongly correlated with the property on that subgroup space:</p>
<p>• Domain Space Cardinality |D|: The size of the domain space |D| determines how many data points are available for learning a pattern.</p>
<p>If the label space is fixed, a larger domain space generally leads to improved learning outcomes.</p>
<p>• Label Space Entropy H(L): Label space entropy H(L) is also a critical factor in learning, as a low-entropy label space often leads to higher predictability.</p>
<p>• Subgroup Quality Q: For any subgroup s = ((A, B), C) ∈ S n ,
Q(s) = max g∈Ωs a ′ ={a} |A| k=1 b ′ ={b} |B| k=1 c ′ ={c} |C| k=1 P f s (a ′ , b ′ , c ′ )P p s (g, a ′ , b ′ , c ′ )(3)
where  Subgroup Selection (s i ): s i ∈ Sn represents the subgroup selection for the i-th prediction.When LLMs predict the token in the i-th position, they must select subgroups that include C i to align with the underlying pattern.This reflects the learning dynamics of language models in arithmetic tasks, abstractly linked to their decision-making and selection processes.As discussed in §6.4,LLMs seem to initially select the subgroup s i with high quality Q high (s i ), progressing to lower quality Q low (s i ) (easy-to-hard) during learning.
Ω s : D s → Θ(L s )</p>
<p>Subgroup Complexity: Label Space Matters in the Final Stage</p>
<p>In this section, we discuss subgroup complexity in arithmetic learning.The domain space cardinality |D| represents the number of training data available, which is an obvious factor influencing learning.Subgroup quality Q(s) will be detailed in §6.4.Thus, we primarily focus on label space entropy H(L) in this section.</p>
<p>Rule Perturbation</p>
<p>We first deliberately perturb the rules to observe whether these changes affect task difficulty for LLMs.We For multiplication, the perturbation is defined as f (a, b) = a × b × λ, where λ = 2, 4, 8 following similar reasons above.Additionally, we incorporate modular addition and multiplication as further perturbations.Table 3 showcases the basic change for label space entropy after applying perturbations.</p>
<p>We then fine-tune Gemma-2-2B and Llama-3.1-8B using different perturbation rules to observe how well these large language models can be influenced from such perturbations in learning.</p>
<p>Results</p>
<p>The results in Table 4 demonstrate that across two different rule perturbation methods and three distinct setups, both Gemma-2-2B and Llama-3.1-8Byield consistent outcomes.While calculating 13 × 27 = 2808 may seem counterintuitive, LLMs handle this arithmetic same as 13×27 = 351 when the label space entropy H(L) are fixed.
Gemma-2-2B Llama-3.1-8B f (a, b) = a + b − − f (a, b) = a + b + 1 −0.1% −0.1% f (a, b) = a + b + 15 −0.9% +0.1% f (a, b) = a + b + 115 −1.4% +0.7% f (a, b) = (a + b) mod 100 +10.1% +3.7% f (a, b) = (a + b) mod 50 +13.1% +6.7% f (a, b) = (a + b) mod 10 +26.1% +13.7% f (a, b) = a × b − − f (a, b) = a × b × 2 −1.1% −2.7% f (a, b) = a × b × 4 −1.7% +0.7% f (a, b) = a × b × 8 +0.2% −3.7% f (a, b) = (a × b) mod 100 +7.1% +3.8% f (a, b) = (a × b) mod 50 +12.1% +5.3% f (a, b) = (a × b) mod 10 +18.9% +10.7%
Table 4: Test Accuracy difference ∆ on perturbed addition and multiplication.</p>
<p>Regarding modular addition and multiplication with different modulus numbers, we find that decreasing the size of the entropy leads to performance improvements in both cases.These results highlight that an arithmetic task with low variability in the label space is more learnable.Together, these two observations reinforce the notion that LLMs are not performing traditional calculations but are instead functioning as sophisticated symbolic observers within the token space.</p>
<p>Subgroup Selection: Revealing Learning Dynamic in Arithmetic Learning</p>
<p>In the previous section, we established that subgroup space provides a basis for quantifying complexity in arithmetic tasks.However, the results mainly highlight insights at the end of learning  (test accuracy after 12 epochs), leaving the learning dynamics less explored.Here, we investigate these dynamics by analyzing digit-level accuracy in model outputs, observing how LLMs select subgroups S n based on their performance across different positions.</p>
<p>Settings We maintain the same basic experimental settings as in the previous section to ensure the discussion remains within the same scope.We will train Gemma-2-2B and Llama-3.1-8B on four different dataset sizes (6.48K, 12.96K, 32.4K, and 64.8K).Our experiments cover multiplication tasks ranging from 3-digit to 5-digit numbers, with output digits from 6 to 10.</p>
<p>Position-level Accuracy are U-curve Figure 3 reveals a phenomenon overlooked in previous studies.Contrary to the common assumption that position-level accuracy decreases from right to left due to carryover effects and least-to-most significant digit calculations (Lee et al., 2023;Zhang-Li et al., 2024), our results show a U-shaped accuracy curve in both Gemma-2-2B and Llama-3.1-8Bmodels.Accuracy peaks at the beginning and end positions, exceeding 95%, with lower accuracy (~10%) in the middle positions, especially in higher-digit multiplication (e.g., 4th/5th for 4-digit, 5th/6th for 5-digit).These results provide valuable insights, suggesting that the difficulty in learning multiplication is concentrated in the middle positions rather than at the beginning, which conceptually corresponds to the final steps of calculation.</p>
<p>Subgroup Selection via Quality We think that the U-curve occurs because the subgroup for middle digits has a lower subgroup quality Q(s) compared to the beginning and end digits.Given a subgroup s = ((A, B), C), where C mid represents the middle digits, more digits from the operands are required to determine the value of C mid compared to representing digits at the beginning or end positions.This leads to a larger domain size |D| when summing all candidates.Consequently, determining the value of C becomes less certain, resulting in a lower Q(s).Specifically, when C represents the last digit, its value can be fully determined by the least significant digits of the operands, which is not the case when C represents a middle digit.For instance, in the case of 3-digit multiplication, it is relatively easy to identify the subgroup s = {({A 1 }, {B 1 }), {C 1 }} for learning the first digits or the subgroup s = {({A 3 }, {B 3 }), {C 6 }} for learning the last digits.This observation explains why the digits at the beginning or the end are easier to learn.It also reveals that LLMs fit arithmetic learning patterns following an easy-to-hard mechanism (from high Q(s) to low Q(s)), which demonstrates gradient descent in the "fastest" symbolic direction.</p>
<p>Conclusions</p>
<p>In this work, we investigate whether LLMs solve arithmetic tasks using partial products or operate as symbolic pattern matchers.Our findings confirm that LLMs do not rely on partial products but instead approach arithmetic purely symbolically.By breaking tasks into subgroups, we demonstrate that the difficulty in arithmetic learning can be attributed to subgroup complexity and selection.Our results emphasize the crucial role of label space entropy in understanding the convergence stage and the quality of subgroups for learning dynamics.Overall, at least in our setting, LLMs function as purely symbolic learners in arithmetic tasks.We encourage further research to explore more complex tasks from this perspective.</p>
<p>In terms of limitations, one area our work does not currently address is the application of our framework to different chain-of-thought (CoT) methods (Wei et al., 2023;Deng et al., 2024).While CoT has proven to be highly effective in arithmetic learning, particularly by decomposing overall difficulty into smaller, more manageable operations-thereby reducing subgroup complexity from exponential to linear-this aspect has not been explored in our study.Additionally, we have not applied our framework in a totally natural languageaware setting like GSM8K or MATH.Exploring how LLMs leverage their symbolic capabilities in such a context could provide deeper insights into their reasoning abilities, particularly in tasks that require structured, multi-step problem solving.These unexplored areas present significant opportunities for future research.</p>
<p>Ethics Statement</p>
<p>Our research primarily focuses on the symbolic learning capabilities of large language models in arithmetic tasks.As such, it does not involve the collection or use of any human data.No personal or sensitive information is handled or analyzed in this study.We acknowledge the potential biases inherent in the datasets used for model training and the limitations of relying on symbolic learning without fully understanding the underlying numerical or logical processes.The societal impact of increased reliance on LLMs for arithmetic tasks, including overconfidence in symbolic learning without full comprehension, warrants careful consideration.We advocate for transparent model evaluations and awareness of the limitations in deploying such models for critical decision-making.is to investigate whether LLMs function as purely symbolic learners in arithmetic tasks.We consider the following three types of format perturbation in Table 5: 1. Natural Language (NL): Arithmetic expressions are converted into natural language statements.For example, the equation "3 + 5" becomes "What is 3 times 5?" 2. Random String (RS): Arithmetic expressions are first converted into natural language and then replaced with meaningless strings.Using the previous example, "3 + 5" might be transformed into "flad kf 3 lfd 5?"</p>
<ol>
<li>Disturbed Digits (DD): Arithmetic expressions are initially converted into natural language and subsequently replaced with random digits.For instance, "3 + 5" could become "65.1 44 3 4 5?" This approach creates a counterfactual context for arithmetic tasks, increasing the difficulty for the models.</li>
</ol>
<p>By implementing these perturbations, we aim to assess the robustness of LLMs in handling arithmetic operations under varied and challenging input formats.</p>
<p>Results</p>
<p>We examined the impact of three types of input format perturbations (Natural Language (NL), Random String (RS), and Disturbed Digits (DD)) on the arithmetic reasoning tasks for Gemma-2.2B and Llama-3.1-8Bmodels.Table 6 shows that across both addition and multiplication tasks, the performance of the models remains largely unaffected by the perturbations when the label space is fixed.Specifically, there is only a marginal change in accuracy under the NL and RS formats, while the DD format causes minor fluctuations but does not significantly degrade performance.This demonstrates that LLMs can effectively handle various input perturbations as long as the output space remains consistent, suggesting their robustness in symbolic reasoning tasks despite superficial input variations.</p>
<p>A.3 Mathematical Explanation of Diagnostic Sets for Multiplication Algorithms</p>
<p>For the multiplication task involving two twodigit numbers formatted as Table 5: Label space statistics with different format perturbations.H(L) represents the entropy of the space, and |L| is the size of the space.{C j } n i=1 represents all possible output digits.mathematical explanation for the formulation of these diagnostic sets for each multiplication algorithm.
A 1 A 2 × B 1 B 2 = C 1 C 2 C 3 C 4 ,</p>
<p>A.3.1 Standard Multiplication</p>
<p>In the standard multiplication algorithm, we multiply each digit of one number by each digit of the other number and sum the appropriately weighted results.</p>
<p>Formulation: Let the two-digit numbers be expressed as:
a = A 1 A 2 = 10A 1 + A 2 , b = B 1 B 2 = 10B 1 + B 2 .
The product is:
ab = (10A 1 + A 2 )(10B 1 + B 2 ).
Expanding, we get four partial products:
ab = 100A 1 B 1 + 10A 1 B 2 + 10A 2 B 1 + A 2 B 2 .
Diagnostic Set:
P std = {A 1 × b, A 2 × b, B 1 × a, B 2 × a} .</p>
<p>Explanation:</p>
<p>• A 1 × b: Multiplying the tens digit of a by the entire number b:
A 1 ×b = A 1 ×(10B 1 +B 2 ) = 10A 1 B 1 +A 1 B 2 .
• A 2 × b: Multiplying the units digit of a by b:
A 2 ×b = A 2 ×(10B 1 +B 2 ) = 10A 2 B 1 +A 2 B 2 .
• B 1 × a: Multiplying the tens digit of b by a:
B 1 ×a = B 1 ×(10A 1 +A 2 ) = 10A 1 B 1 +A 2 B 1 .
• B 2 × a: Multiplying the units digit of b by a:
B 2 ×a = B 2 ×(10A 1 +A 2 ) = 10A 1 B 2 +A 2 B 2 .
Including these partial products in P std captures all intermediary computations in the standard algorithm, facilitating a comprehensive diagnostic analysis.</p>
<p>A.3.2 Repetitive Addition</p>
<p>Repetitive addition interprets multiplication as adding one number to itself repeatedly.</p>
<p>Diagnostic Set: Both summations lead to the same product ab, and including them in P ra allows for analyzing both repetitive addition paths in the algorithm.
P ra =    b i=1 a,</p>
<p>Summing the results: 300 + 40 + 60 + 8 = 408 M4: Egyptian Multiplication Egyptian multiplication computes the product by doubling the multiplicand and adding the results corresponding to the powers of two that sum to the multiplier.For Summing the selected results: 24 + 384 = 408</p>
<p>Figure 2 :
2
Figure2: Partial products identification accuracy before and after fine-tuning on tasks.Scores are reported on average of Gemma-2-2B and Llama-3.1-8B.</p>
<p>|C| k=1 |
k=1
P (C = {c} |C| k=1 ) &gt; 0}.The size of label space is annotated as |L s |.</p>
<p>represents a function space mapping from D s to Θ(L s ), and Θ(L s ) denotes the space of random variables taking values in L s .Here, P p s (g, a ′ , b ′ , c ′ ) = P (g(a ′ , b ′ ) = c ′ ) represents the probability that the function g maps (a ′ , b ′ ) to c ′ , and P f s (a ′ , b ′ , c ′ ) represents the probability that A = a ′ , B = b ′ and C = c ′ , hold simultaneously in arithmetic tasks.Thus, Q(s) measures the maximum possible probability of predicting the value of C that is consistent with the values in the dataset from the values of (A, B). f (a, b) = (a + b) mod 100 A1A2 + B1B2 = C1C2 3f (a, b) = a × b × 2 A1A2 × B1B2 = C1C2C3C4C5 0.6873 3.2173 3.3215 3.2964 2.2227 2621 11.1172 f (a, b) = a × b × 4 A1A2 × B1B2 = C1C2C3C4C5 1.6030 3.3020 3.3204 3.2234 2.2227 2621 11.1172 f (a, b) = a × b × 8 A1A2 × B1B2 = C1C2C3C4C5 2.5811 3.3202 3.3151 3.2235 2.2227 2621 11.1172 f (a, b) = (a × b) mod 100 A1A2 × B1B2 = C1C2 3</p>
<p>consider addition f (a, b) = a + b and multiplication f (a, b) = a × b as our baselines.For addition, the perturbation is defined as f (a, b) = a + b + ∆c, where ∆c = 1, 15, 115 corresponds to perturbations at different position with different magnitudes.</p>
<p>Figure 3 :
3
Figure 3: Position-level Accuracy from Gemma-2-2B and Llama-3.1-8B.</p>
<p>we generate diagnostic test sets P for each algorithm to analyze and understand the partial computations involved.Below, we provide af (a, b) = a + b What is A1A2 add B1B2? Answer: C1C2C3 0f (a, b) = a + b fafr if A1A2 hfk B1B2?Ffhjar: C1C2C3 3f (a, b) = a + b 3.123 34 A1A2 461 B1B2? 952414: C1C2C3 0b) = a × b A1A2 × B1B2 = C1C2C3C4C5 2.5811 3.3202 3.3151 3.2235 2.2227 2621 11.1172 f (a, b) = a × b What is A1A2 multiply B1B2?Answer: C1C2C3C4 2.8979 3.3215 3.3160 3.0340 − 2621 11.1172 f (a, b) = a × b fafr if A1A2 hfk B1B2?Ffhjar: C1C2C3C4 0.6873 3.2173 3.3215 3.2964 2.2227 2621 11.1172 f (a, b) = a × b 3.123 34 A1A2 461 B1B2? 952414: C1C2C3C4 1.6030 3.3020 3.3204 3.2234 2.2227 2621 11.1172</p>
<p>Format</p>
<p>Adding a to itself b times: b i=1 a = a + a + • • • + a (b times) = ab.• a j=1 b: Adding b to itself a times: a j=1 b = b + b + • • • + b (a times) = ab.</p>
<p>terms of multiplication, four different calculation methods are most representative from history to now: Standard Multiplication, Repetitive Addition, Lattice Method, and Egyptian Multiplication.
M1: Standard Multiplication In standard multi-plication, we multiply each digit of one number byeach digit of the other number, and then sum theresults appropriately:12 × 34 = 12 × (30 + 4) = 12 × 30 + 12 × 4= 360 + 48 = 408
M2: Repetitive Addition Multiplication can be interpreted as repeated addition.For 12 × 34, we add 12 thirty-four times:12 × 34 = 12 + 12 + 12 + • • • + 12 (34 times) = 408</p>
<p>Table 1 :
1
Inductive and deductive accuracy difference ∆.
Gemma-2-2BLlama-3.1-8BStandardLatticeRepetitive Egyptian StandardLatticeRepetitive EgyptianTask → Partial P.+4.1%+6.8%−29.0%+3.6%+40.6% +40.8%−59.0%+29.6%Partial P. → Task−6.1%−10.7%−20.3%−9.6%−3.7%−0.2%−0.9%−2.7%</p>
<p>Table 2 :
2
Diagnostic sets with four calculation methods.</p>
<p>Table 3 :
3
Label space statistics with different rule perturbations.H(L) represents the entropy of the label space, and |L| is the size of the label space.{C j } n i=1 represents all positions in output digits.</p>
<p>Table 6 :
6
Test Accuracy difference ∆ on perturbed addition and multiplication.</p>
<p>Unsloth AI is at: https://unsloth.ai/
A AppendixA.1 Training DetailWe carefully tuned the hyperparameters in our experiments.The learning rate for Gemma-2-2B was set to 1e − 4, while for Llama-3.1-8B it was 2e − 4. Both models used a warm-up of 5 steps and a weight decay of 0.01.We trained for 12 epochs, splitting the dataset into 80% for training, 10% for validation, and 10% for testing.Evaluation was conducted at the end of each epoch, with checkpoints saved based on the best performance on the validation set.LoRA finetuning is used for both models with same setting, lora_rank = 64, lora_alpha = 16, lora_dropout = 0, and we enable rank stabalized lora(Kalajdzievski, 2023)during training.We use unsloth 1 to fasten our training process and vLLM(Kwon et al., 2023)to increase our inference process.A.2 Format Perturbations in Arithmetic TasksIn this section, we apply three types of format perturbations to basic addition and multiplication tasks to evaluate the symbolic reasoning capabilities of large language models (LLMs).Our experiments utilize Gemma-2-2B and Llama-3.1-8Bmodels.The primary objective of varying the input formatA.3.3 Lattice MethodThe lattice method (or grid method) organizes the multiplication of each digit pair in a grid and sums along diagonals.Diagnostic Set:Explanation:• A 1 × B 1 : Tens digit of a times tens digit of b.• A 1 × B 2 : Tens digit of a times units digit of b.• A 2 × B 1 : Units digit of a times tens digit of b.• A 2 × B 2 : Units digit of a times units digit of b.These products fill the cells of the lattice grid:Summing along the diagonals yields the final product.Including these partial products in P lattice covers all the necessary computations in the lattice method.A.3.4 Egyptian MultiplicationEgyptian multiplication involves doubling the multiplicand and adding specific results based on the binary representation of the multiplier.Diagnostic Set:Explanation:• Binary Representation of b: Express b as a sum of powers of two:• Doubling a: Compute successive doublings of a:• Selection and Summation: Identify which 2 k × a correspond to C k = 1 in b's binary representation and sum them:Including all 2 k × a up to n in P egyptian ensures that we have the necessary partial products for any b, allowing us to reconstruct ab by selecting and summing the appropriate terms.Example: If b = 13, its binary representation is 1101, so b = 2 3 + 2 2 + 2 0 .The partial products are:Select 2 0 × a, 2 2 × a, and 2 3 × a (since C 0 = C 2 = C 3 = 1) and sum: ab = a + 4a + 8a = 13a.By formulating the diagnostic sets P as above for each multiplication algorithm, we encapsulate all intermediary computational steps inherent to each method.
. Anthropic, 2024Claude</p>
<p>Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, Mausam , ArXiv, abs/2305.150742023</p>
<p>Models can and should embrace the communicative nature of human-generated math. Sasha Boguraev, Ben Lipkin, Leonie Weissweiler, Kyle Mahowald, arXiv:2409.170052024Preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.141682021Preprint</p>
<p>From explicit cot to implicit cot: Learning to internalize cot step by step. Yuntian Deng, Yejin Choi, Stuart Shieber, arXiv:2405.148382024Preprint</p>
<p>Implicit chain of thought reasoning via knowledge distillation. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber, arXiv:2311.014602023Preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Roziere, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Bi, arXiv:2407.217832024Preprint</p>
<p>Learning beyond pattern matching? assaying mathematical understanding in llms. Siyuan Guo, Aniket Didolkar, Nan Rosemary Ke, Anirudh Goyal, Ferenc Huszár, Bernhard Schölkopf, arXiv:2405.154852024Preprint</p>
<p>How does gpt-2 compute greater-than?. Michael Hanna, Ollie Liu, Alexandre Variengien, arXiv:2305.00586Interpreting mathematical abilities in a pre-trained language model. 2023Preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021Preprint</p>
<p>How well do computers solve math word problems? large-scale dataset construction and evaluation. Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, Wei-Ying Ma, 10.18653/v1/P16-1084Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany20161Association for Computational Linguistics</p>
<p>A rank stabilization scaling factor for fine-tuning with lora. Damjan Kalajdzievski, arXiv:2312.037322023Preprint</p>
<p>Andrej karpathy's lightweight implementation of medium-sized gpts. Andrej Karpathy, 2022</p>
<p>Learning to automatically solve algebra word problems. Nate Kushman, Yoav Artzi, Luke Zettlemoyer, Regina Barzilay, 10.3115/v1/P14-1026Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, MarylandAssociation for Computational Linguistics20141</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Teaching arithmetic to small transformers. Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, arXiv:2307.033812023Preprint</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, Advances in Neural Information Processing Systems. 2022</p>
<p>Improving large language model fine-tuning for solving math problems. Yixin Liu, Avi Singh, C Daniel Freeman, John D Co-Reyes, Peter J Liu, arXiv:2310.100472023Preprint</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Brockman, arXiv:2303.08774Gpt-4 technical report. 2024Preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, 10.18653/v1/2023.emnlp-main.435Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Why are nlp models fumbling at elementary math? a survey of deep learning based word problem solvers. S Sowmya, Sairam Sundaram, Marco Gurajada, Fisichella, P Deepak, Savitha Sam, Abraham , ArXiv, abs/2205.156832022</p>
<p>Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, arXiv:2408.00118Gemma. 22024Preprint</p>
<p>Representing numbers in NLP: a survey and a vision. Avijit Thawani, Jay Pujara, Filip Ilievski, Pedro Szekely, 10.18653/v1/2021.naacl-main.53Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnline. Association for Computational Linguistics2021</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712023Preprint</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, 10.18653/v1/D17-1088Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>Mathchat: Converse to tackle challenging math problems with llm agents. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang, arXiv:2306.013372024aPreprint</p>
<p>Interpretability at scale: Identifying causal mechanisms in alpaca. Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, Noah D Goodman, arXiv:2305.088092024bPreprint</p>
<p>Adversarial math word problem generation. Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra, arXiv:2402.179162024Preprint</p>
<p>Gpt can solve mathematical problems without a calculator. Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang, arXiv:2309.032412023Preprint</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, ArXiv, abs/2309.056532023</p>
<p>Interpreting and improving large language models in arithmetic calculation. Wei Zhang, Chaoqun Wan, Yonggang Zhang, Xinmei Yiu Ming Cheung, Xu Tian, Jieping Shen, Ye, arXiv:2409.016592024Preprint</p>
<p>Reverse that number! decoding order matters in arithmetic learning. Daniel Zhang-Li, Nianyi Lin, Jifan Yu, Zheyuan Zhang, Zijun Yao, Xiaokang Zhang, Lei Hou, Jing Zhang, Juanzi Li, arXiv:2403.058452024Preprint</p>
<p>Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, Arman Cohan, arXiv:2311.09797Financemath: Knowledge-intensive math reasoning in finance domains. 2024aPreprint</p>
<p>Docmath-eval: Evaluating math reasoning capabilities of llms in understanding long and specialized documents. Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan, arXiv:2311.098052024bPreprint</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, ArXiv, abs/2310.044062023a</p>
<p>Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang, arXiv:2309.01686Mathattack: Attacking large language models towards math solving ability. 2023bPreprint</p>            </div>
        </div>

    </div>
</body>
</html>
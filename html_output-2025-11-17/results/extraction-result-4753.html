<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4753 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4753</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4753</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-270560836</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11698v1.pdf" target="_blank">Meta Reasoning for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system prompting method for large language models (LLMs) inspired by human meta-reasoning. Traditional in-context learning-based reasoning techniques, such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art performance across diverse tasks due to their specialized nature. MRP addresses this limitation by guiding LLMs to dynamically select and apply different reasoning methods based on the specific requirements of each task, optimizing both performance and computational efficiency. With MRP, LLM reasoning operates in two phases. Initially, the LLM identifies the most appropriate reasoning method using task input cues and objective descriptions of available methods. Subsequently, it applies the chosen method to complete the task. This dynamic strategy mirrors human meta-reasoning, allowing the model to excel in a wide range of problem domains. We evaluate the effectiveness of MRP through comprehensive benchmarks. The results demonstrate that MRP achieves or approaches state-of-the-art performance across diverse tasks. MRP represents a significant advancement in enabling LLMs to identify cognitive challenges across problems and leverage benefits across different reasoning approaches, enhancing their ability to handle diverse and complex problem domains efficiently. Every LLM deserves a Meta-Reasoning Prompting to unlock its full potential and ensure adaptability in an ever-evolving landscape of challenges and applications.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4753.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4753.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Reasoning Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system prompt that guides an LLM to evaluate a set of reasoning-method descriptions and dynamically select the single most appropriate method for each input, then execute that method to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-4-turbo via Azure OpenAI; reported as a larger-capacity LLM with superior meta-reasoning capability compared to gpt-3.5 in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Meta-Reasoning Prompting (MRP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The model is given a 'reasoning pool' of prompt descriptions for multiple reasoning methods; it scores each method for the current input using a meta-reasoning prompt and selects the highest-scoring method to execute. Diversity is achieved by selection from multiple heterogeneous methods rather than sampling/ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Macro average across 7 benchmarks (GSM8K, Gameof24, Trivia CW, HotpotQA, BigToM, Code, MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate evaluation across seven diverse reasoning tasks (arithmetic, complex math puzzle, creative writing, multi-hop QA, social reasoning, code readability, STEM multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>macro average accuracy 77.2% (0.772) as reported in Table 1</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Best competitor macro average: Tree-of-Thoughts (TOT) 72.5% (0.725); individual baselines range lower</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MRP (dynamic method selection) attains the highest overall (macro) performance across the seven diverse tasks with GPT-4, showing that dynamically choosing among heterogeneous methods yields more consistent, strong performance than any single fixed method.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MRP is not always the top performer on every single task — e.g., TOT outperforms MRP on some specific tasks (see Gameof24 and GSM8K cases). MRP also provides less advantage on very simple tasks where many methods already achieve near-ceiling performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4753.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4753.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TOT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning method that explores multiple branching reasoning paths and self-evaluates/prunes them to find solutions to complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (TOT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generates multiple candidate reasoning branches (diverse reasoning paths) and searches/evaluates them to decide which branches to pursue, enabling exploration of different reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (arithmetic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring step-by-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 94.2% (0.942) on GSM8K (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MRP accuracy 92.1% (0.921) on GSM8K (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>On GSM8K, a task where exploration of multiple reasoning paths helps, TOT attains the highest reported single-method accuracy; however, the overall differences among good methods are small on this relatively simple benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although TOT is top for GSM8K (and strong on Gameof24), it underperforms relative to MRP on other, more heterogeneous tasks (e.g., BigToM, Code), showing that a single diverse-search method does not dominate across all problem types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4753.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4753.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TOT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (TOT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Gameof24 (complex mathematical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given 4 numbers, use arithmetic operations to reach the value 24 — requires combinatorial exploration of operations and orders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 41.0% (0.410) on Gameof24 (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MRP accuracy 31.0% (0.310) on Gameof24 (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TOT (a diverse-path search approach) outperforms MRP on Gameof24, indicating that explicit exploration/branching strategies can be particularly effective on combinatorial arithmetic puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>TOT's advantage on Gameof24 does not generalize to many other task types; thus diversity via branching helps specific structured search problems but is not a universal solution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4753.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4753.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based approach that elicits a single sequential chain of intermediate reasoning steps leading to the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thoughts (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Produces one coherent step-by-step reasoning trace (a single reasoning path) rather than exploring multiple alternative paths or ensembling; similarity arises since repeated runs produce similar chains absent sampling/ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (arithmetic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring step-by-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 91.4% (0.914) on GSM8K (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>TOT 94.2% (0.942) and MRP 92.1% (0.921) on GSM8K (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT (a single-chain approach) achieves high accuracy on GSM8K, close to the best methods; on relatively simple arithmetic reasoning tasks, single-chain methods suffice and differences between diverse and similar approaches are small.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT underperforms on several heterogeneous/complex benchmarks compared to MRP and other methods that use different strategies or dynamic selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4753.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4753.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solo Performance Prompting (multi-persona self-collaboration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulates multiple personas that interact/collaborate to solve complex tasks, providing varied perspectives and solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Solo Performance Prompting (SPP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Simulates multiple collaborating agents/personas (diverse perspectives) that propose and evaluate solutions, creating diversity via role-based variation and cross-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Readability</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assessment or rewriting of code snippets to improve readability; requires both semantic understanding and style judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 67.2% (0.672) on Code (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MRP accuracy 86.7% (0.867) on Code (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Despite being a diverse/multi-agent approach, SPP is much weaker than MRP on code readability in these experiments; dynamic selection of specialized methods via MRP provided a large performance advantage on Code.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SPP's multi-persona diversity did not yield better results on Code; demonstrates that diversity alone (simulating personas) does not guarantee superior performance on all task types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4753.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4753.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that self-generates few-shot examples by retrieving or creating analogies from past problems similar to the current query and uses them as in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Analogical Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generates or retrieves related example problems (analogies) and uses those few-shot exemplars to guide reasoning, achieving diversity by varying exemplar selection and leveraging analogical transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (selected STEM subjects: Physics, Chemistry, Biology, Math)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>High-school level STEM multiple-choice questions from the MMLU benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 94.7% (0.947) on MMLU (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MRP accuracy 85.4% (0.854) on MMLU (Table 1); Self-Refine 86.1% (0.861), CoT 89.4% (0.894)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Analogical prompting achieves very high performance on MMLU in this reported experiment, indicating that analogical few-shot exemplars can be highly effective for some knowledge-intensive multiple-choice tasks. However, MRP's dynamic selection strategy targets robustness across tasks rather than maximizing a single-task score.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Analogical prompting is not uniformly best across all tasks; its high MMLU performance contrasts with weaker results on some other benchmarks (e.g., Gameof24).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4753.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4753.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Reasoning Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>See above; system prompting to select among reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-3.5-turbo via Azure OpenAI; smaller-capacity LLM compared to GPT-4, reported to have weaker meta-reasoning and suffers scoring/self-opinion/factual/reasoning errors in MRP pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Meta-Reasoning Prompting (MRP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>See above; uses model-internal scoring to pick a method from a heterogenous pool before executing that method.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Macro average across same 7 benchmarks (GSM8K, Gameof24, Trivia CW, HotpotQA, BigToM, Code, MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate evaluation across seven diverse reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>macro average accuracy 43.3% (0.433) reported in Table 2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Best competitor macro average in GPT-3.5 experiments: Analogical 43.3% (0.433) — several methods have comparable macro averages; Tree-of-Thoughts 35.2% (0.352)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MRP's benefits depend on base model capability: while MRP substantially helps GPT-4, its effectiveness is limited with GPT-3.5 due to meta-reasoning failures (scoring errors, self-opinion, factual and reasoning errors) during method selection, producing only modest gains or parity with some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On GPT-3.5, MRP is not consistently superior; the paper reports that scoring and self-evaluation errors in smaller models reduce the benefit of dynamic selection, demonstrating a key limitation when applying MRP on weaker LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4753.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4753.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>gpt-3.5-turbo via Azure OpenAI; smaller-capacity LLM used in paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thoughts (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (arithmetic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring step-by-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 83.1% (0.831) on GSM8K (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MRP accuracy 78.1% (0.781) and TOT 81.0% (0.810) on GSM8K with GPT-3.5 (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>With a smaller model (GPT-3.5), a simple CoT approach still attains high performance on GSM8K, sometimes exceeding MRP; indicates that similar single-chain methods can be competitive on arithmetic tasks even when meta-selection is noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MRP did not outperform CoT on GSM8K with GPT-3.5; smaller models' poor meta-evaluation reduces benefit of dynamic selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts <em>(Rating: 2)</em></li>
                <li>Mixture of prompts for llm task adaptation (Sweeping heterogeneity with smart mops: Mixture of prompts for llm task adaptation) <em>(Rating: 1)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4753",
    "paper_id": "paper-270560836",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "MRP",
            "name_full": "Meta-Reasoning Prompting",
            "brief_description": "A system prompt that guides an LLM to evaluate a set of reasoning-method descriptions and dynamically select the single most appropriate method for each input, then execute that method to produce the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo)",
            "model_description": "gpt-4-turbo via Azure OpenAI; reported as a larger-capacity LLM with superior meta-reasoning capability compared to gpt-3.5 in the paper's experiments.",
            "reasoning_method_name": "Meta-Reasoning Prompting (MRP)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "The model is given a 'reasoning pool' of prompt descriptions for multiple reasoning methods; it scores each method for the current input using a meta-reasoning prompt and selects the highest-scoring method to execute. Diversity is achieved by selection from multiple heterogeneous methods rather than sampling/ensembling.",
            "task_name": "Macro average across 7 benchmarks (GSM8K, Gameof24, Trivia CW, HotpotQA, BigToM, Code, MMLU)",
            "task_description": "Aggregate evaluation across seven diverse reasoning tasks (arithmetic, complex math puzzle, creative writing, multi-hop QA, social reasoning, code readability, STEM multiple-choice).",
            "performance": "macro average accuracy 77.2% (0.772) as reported in Table 1",
            "comparison_with_other_method": true,
            "performance_other_method": "Best competitor macro average: Tree-of-Thoughts (TOT) 72.5% (0.725); individual baselines range lower",
            "key_findings": "MRP (dynamic method selection) attains the highest overall (macro) performance across the seven diverse tasks with GPT-4, showing that dynamically choosing among heterogeneous methods yields more consistent, strong performance than any single fixed method.",
            "counter_examples_or_negative_results": "MRP is not always the top performer on every single task — e.g., TOT outperforms MRP on some specific tasks (see Gameof24 and GSM8K cases). MRP also provides less advantage on very simple tasks where many methods already achieve near-ceiling performance.",
            "uuid": "e4753.0",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "TOT",
            "name_full": "Tree-of-Thoughts",
            "brief_description": "A reasoning method that explores multiple branching reasoning paths and self-evaluates/prunes them to find solutions to complex problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo)",
            "model_description": "gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.",
            "reasoning_method_name": "Tree-of-Thoughts (TOT)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generates multiple candidate reasoning branches (diverse reasoning paths) and searches/evaluates them to decide which branches to pursue, enabling exploration of different reasoning trajectories.",
            "task_name": "GSM8K (arithmetic reasoning)",
            "task_description": "Grade-school math word problems requiring step-by-step arithmetic reasoning.",
            "performance": "accuracy 94.2% (0.942) on GSM8K (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "MRP accuracy 92.1% (0.921) on GSM8K (Table 1)",
            "key_findings": "On GSM8K, a task where exploration of multiple reasoning paths helps, TOT attains the highest reported single-method accuracy; however, the overall differences among good methods are small on this relatively simple benchmark.",
            "counter_examples_or_negative_results": "Although TOT is top for GSM8K (and strong on Gameof24), it underperforms relative to MRP on other, more heterogeneous tasks (e.g., BigToM, Code), showing that a single diverse-search method does not dominate across all problem types.",
            "uuid": "e4753.1",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "TOT",
            "name_full": "Tree-of-Thoughts",
            "brief_description": "See above.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo)",
            "model_description": "gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.",
            "reasoning_method_name": "Tree-of-Thoughts (TOT)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "See above.",
            "task_name": "Gameof24 (complex mathematical reasoning)",
            "task_description": "Given 4 numbers, use arithmetic operations to reach the value 24 — requires combinatorial exploration of operations and orders.",
            "performance": "accuracy 41.0% (0.410) on Gameof24 (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "MRP accuracy 31.0% (0.310) on Gameof24 (Table 1)",
            "key_findings": "TOT (a diverse-path search approach) outperforms MRP on Gameof24, indicating that explicit exploration/branching strategies can be particularly effective on combinatorial arithmetic puzzles.",
            "counter_examples_or_negative_results": "TOT's advantage on Gameof24 does not generalize to many other task types; thus diversity via branching helps specific structured search problems but is not a universal solution.",
            "uuid": "e4753.2",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thoughts",
            "brief_description": "A prompt-based approach that elicits a single sequential chain of intermediate reasoning steps leading to the answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo)",
            "model_description": "gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.",
            "reasoning_method_name": "Chain-of-Thoughts (CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Produces one coherent step-by-step reasoning trace (a single reasoning path) rather than exploring multiple alternative paths or ensembling; similarity arises since repeated runs produce similar chains absent sampling/ensembling.",
            "task_name": "GSM8K (arithmetic reasoning)",
            "task_description": "Grade-school math word problems requiring step-by-step arithmetic reasoning.",
            "performance": "accuracy 91.4% (0.914) on GSM8K (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "TOT 94.2% (0.942) and MRP 92.1% (0.921) on GSM8K (Table 1)",
            "key_findings": "CoT (a single-chain approach) achieves high accuracy on GSM8K, close to the best methods; on relatively simple arithmetic reasoning tasks, single-chain methods suffice and differences between diverse and similar approaches are small.",
            "counter_examples_or_negative_results": "CoT underperforms on several heterogeneous/complex benchmarks compared to MRP and other methods that use different strategies or dynamic selection.",
            "uuid": "e4753.3",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SPP",
            "name_full": "Solo Performance Prompting (multi-persona self-collaboration)",
            "brief_description": "Simulates multiple personas that interact/collaborate to solve complex tasks, providing varied perspectives and solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo)",
            "model_description": "gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.",
            "reasoning_method_name": "Solo Performance Prompting (SPP)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Simulates multiple collaborating agents/personas (diverse perspectives) that propose and evaluate solutions, creating diversity via role-based variation and cross-evaluation.",
            "task_name": "Code Readability",
            "task_description": "Assessment or rewriting of code snippets to improve readability; requires both semantic understanding and style judgment.",
            "performance": "accuracy 67.2% (0.672) on Code (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "MRP accuracy 86.7% (0.867) on Code (Table 1)",
            "key_findings": "Despite being a diverse/multi-agent approach, SPP is much weaker than MRP on code readability in these experiments; dynamic selection of specialized methods via MRP provided a large performance advantage on Code.",
            "counter_examples_or_negative_results": "SPP's multi-persona diversity did not yield better results on Code; demonstrates that diversity alone (simulating personas) does not guarantee superior performance on all task types.",
            "uuid": "e4753.4",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Analogical",
            "name_full": "Analogical Prompting",
            "brief_description": "A method that self-generates few-shot examples by retrieving or creating analogies from past problems similar to the current query and uses them as in-context demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo)",
            "model_description": "gpt-4-turbo via Azure OpenAI; large-capacity LLM used in paper experiments.",
            "reasoning_method_name": "Analogical Prompting",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generates or retrieves related example problems (analogies) and uses those few-shot exemplars to guide reasoning, achieving diversity by varying exemplar selection and leveraging analogical transfer.",
            "task_name": "MMLU (selected STEM subjects: Physics, Chemistry, Biology, Math)",
            "task_description": "High-school level STEM multiple-choice questions from the MMLU benchmark.",
            "performance": "accuracy 94.7% (0.947) on MMLU (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "MRP accuracy 85.4% (0.854) on MMLU (Table 1); Self-Refine 86.1% (0.861), CoT 89.4% (0.894)",
            "key_findings": "Analogical prompting achieves very high performance on MMLU in this reported experiment, indicating that analogical few-shot exemplars can be highly effective for some knowledge-intensive multiple-choice tasks. However, MRP's dynamic selection strategy targets robustness across tasks rather than maximizing a single-task score.",
            "counter_examples_or_negative_results": "Analogical prompting is not uniformly best across all tasks; its high MMLU performance contrasts with weaker results on some other benchmarks (e.g., Gameof24).",
            "uuid": "e4753.5",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MRP",
            "name_full": "Meta-Reasoning Prompting",
            "brief_description": "See above; system prompting to select among reasoning methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "gpt-3.5-turbo via Azure OpenAI; smaller-capacity LLM compared to GPT-4, reported to have weaker meta-reasoning and suffers scoring/self-opinion/factual/reasoning errors in MRP pipeline.",
            "reasoning_method_name": "Meta-Reasoning Prompting (MRP)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "See above; uses model-internal scoring to pick a method from a heterogenous pool before executing that method.",
            "task_name": "Macro average across same 7 benchmarks (GSM8K, Gameof24, Trivia CW, HotpotQA, BigToM, Code, MMLU)",
            "task_description": "Aggregate evaluation across seven diverse reasoning tasks.",
            "performance": "macro average accuracy 43.3% (0.433) reported in Table 2",
            "comparison_with_other_method": true,
            "performance_other_method": "Best competitor macro average in GPT-3.5 experiments: Analogical 43.3% (0.433) — several methods have comparable macro averages; Tree-of-Thoughts 35.2% (0.352)",
            "key_findings": "MRP's benefits depend on base model capability: while MRP substantially helps GPT-4, its effectiveness is limited with GPT-3.5 due to meta-reasoning failures (scoring errors, self-opinion, factual and reasoning errors) during method selection, producing only modest gains or parity with some baselines.",
            "counter_examples_or_negative_results": "On GPT-3.5, MRP is not consistently superior; the paper reports that scoring and self-evaluation errors in smaller models reduce the benefit of dynamic selection, demonstrating a key limitation when applying MRP on weaker LLMs.",
            "uuid": "e4753.6",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thoughts",
            "brief_description": "See above.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "gpt-3.5-turbo via Azure OpenAI; smaller-capacity LLM used in paper experiments.",
            "reasoning_method_name": "Chain-of-Thoughts (CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "See above.",
            "task_name": "GSM8K (arithmetic reasoning)",
            "task_description": "Grade-school math word problems requiring step-by-step arithmetic reasoning.",
            "performance": "accuracy 83.1% (0.831) on GSM8K (Table 2)",
            "comparison_with_other_method": true,
            "performance_other_method": "MRP accuracy 78.1% (0.781) and TOT 81.0% (0.810) on GSM8K with GPT-3.5 (Table 2)",
            "key_findings": "With a smaller model (GPT-3.5), a simple CoT approach still attains high performance on GSM8K, sometimes exceeding MRP; indicates that similar single-chain methods can be competitive on arithmetic tasks even when meta-selection is noisy.",
            "counter_examples_or_negative_results": "MRP did not outperform CoT on GSM8K with GPT-3.5; smaller models' poor meta-evaluation reduces benefit of dynamic selection.",
            "uuid": "e4753.7",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts",
            "rating": 2,
            "sanitized_title": "plan_verify_and_switch_integrated_reasoning_with_diverse_xofthoughts"
        },
        {
            "paper_title": "Mixture of prompts for llm task adaptation (Sweeping heterogeneity with smart mops: Mixture of prompts for llm task adaptation)",
            "rating": 1,
            "sanitized_title": "mixture_of_prompts_for_llm_task_adaptation_sweeping_heterogeneity_with_smart_mops_mixture_of_prompts_for_llm_task_adaptation"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.014934,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Meta Reasoning for Large Language Models
17 Jun 2024</p>
<p>Peizhong Gao 
Tsinghua University</p>
<p>Ao Xie 
Tsinghua University</p>
<p>Shaoguang Mao 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Wenshan Wu 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Yan Xia 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Haipeng Mi 
Tsinghua University</p>
<p>Furu Wei 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Microsoft Research Asia</p>
<p>Meta Reasoning for Large Language Models
17 Jun 202442DD103A538B1115489F7101D858B0EDarXiv:2406.11698v1[cs.CL]
We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system prompting method for large language models (LLMs) inspired by human metareasoning.Traditional in-context learning-based reasoning techniques, such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art performance across diverse tasks due to their specialized nature.MRP addresses this limitation by guiding LLMs to dynamically select and apply different reasoning methods based on the specific requirements of each task, optimizing both performance and computational efficiency.With MRP, LLM reasoning operates in two phases.Initially, the LLM identifies the most appropriate reasoning method using task input cues and objective descriptions of available methods.Subsequently, it applies the chosen method to complete the task.This dynamic strategy mirrors human meta-reasoning, allowing the model to excel in a wide range of problem domains.We evaluate the effectiveness of MRP through comprehensive benchmarks.The results demonstrate that MRP achieves or approaches state-of-the-art performance across diverse tasks.MRP represents a significant advancement in enabling LLMs to identify cognitive challenges across problems and leverage benefits across different reasoning approaches, enhancing their ability to handle diverse and complex problem domains efficiently.Every LLM deserves a Meta-Reasoning Prompting to unlock its full potential and ensure adaptability in an ever-evolving landscape of challenges and applications.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown remarkable capabilities in natural language understanding and generation, making significant strides in various reasoning tasks.However, the diversity and complexity of real-world problems require advanced reasoning methods that surpass the capabilities of a single, static approach.While existing reasoning techniques, such as Chain-of-Thoughts [27], Tree-of-Thoughts [32], Analogical Prompting [33], and Solo Performance Prompting [26], offer valuable tools for enhancing reasoning, they often fall short in consistently achieving state-of-the-art performance across different tasks.</p>
<p>These challenges highlight the need for a more adaptive and flexible approach to reasoning in LLMs.In human cognition, meta-reasoning involves monitoring and regulating reasoning and problemsolving activities, adjusting strategies based on the context and specific task requirements [5,4].This adaptive capability allows humans to efficiently allocate cognitive resources, balancing trade-offs between accuracy, complexity, and computational cost.Inspired by this, we propose Meta-Reasoning Prompting (MRP) to endow LLMs with similar adaptive reasoning capabilities.</p>
<p>Meta-Reasoning Prompting (MRP) is a simple yet effective system prompt designed to guide LLMs in dynamically selecting and applying the most suitable reasoning method for a specific task.By incorporating meta-reasoning principles, MRP transforms task-specific prompt engineering into a more general and flexible approach.Under the guidance of MRP, the LLM evaluates the task input and selects an appropriate reasoning method from a predefined set (Reasoning Pool).This selection is informed by objective descriptions and evaluations of the available methods.The chosen method is then applied to complete the task, ensuring the model uses the most effective strategy for the given problem.</p>
<p>Recent advances in reasoning techniques, such as those described in [30,24], introduce a meta-buffer for storing high-level thoughts or use ensemble mechanisms to improve model generalizability.While some of these approaches align with the inherent logic of meta-reasoning, our proposed MRP achieves simple and efficient meta-cognitive effects by directly leveraging the meta-reasoning capabilities of LLMs through prompts, without introducing complex mechanisms.</p>
<p>To evaluate the effectiveness of MRP, we conducted experiments using multiple widely used benchmarks.These benchmarks cover different knowledge and reasoning abilities, providing a comprehensive test of the LLM's performance across various reasoning tasks.Our findings demonstrate that MRP not only approaches state-of-the-art performance across these benchmarks but also excels in tasks requiring a blend of different reasoning strategies.Additionally, we observe that larger models, such as GPT-4, exhibit superior meta-reasoning capabilities compared to smaller models like GPT-3.5.</p>
<p>As models improve, their understanding of problems and methods-i.e., their meta-reasoning abilities-also enhances.MRP utilizes the inherent meta-cognitive abilities of LLMs, providing a straightforward and effective method that enhances their generality across different tasks.Experimental and analytical results indicate the significant potential of MRP in boosting LLM performance.Future work could explore the broader application of MRP, such as constructing training data to enhance the meta-cognitive and general reasoning abilities of LLMs during the training process.</p>
<p>Our key contributions are as follows:</p>
<ol>
<li>
<p>We propose Meta-Reasoning Prompting (MRP), a system prompt that enables LLMs to dynamically select the most suitable reasoning method for specific tasks, enhancing their flexibility and effectiveness.2. Experiments on multiple benchmarks show that MRP approaches state-of-the-art performance and excels in tasks requiring diverse reasoning strategies, particularly in larger models like GPT-4.</p>
</li>
<li>
<p>MRP leverages LLMs' inherent meta-cognitive abilities, improving their generality and performance across tasks.Future work could further enhance these abilities through targeted training data.</p>
</li>
</ol>
<p>Meta-Reasoning Prompting</p>
<p>The Meta-Reasoning Prompting (MRP) is designed to guide a Language Learning Model (LLM) in selecting the most suitable reasoning method from a pool of available methods, thereby enhancing the overall reasoning performance of the model.Detailed prompts can be found in Fig. 2.</p>
<p>Guided by the Meta-Reasoning Prompting, the LLM (M ) begins with an input x 0 and a set of available reasoning methods α 1 , α 2 , . . ., α n .A reasoning pool contains descriptions of each reasoning method in the form of prompts p 1 , p 2 , . . ., p n , with these descriptions extracted from the abstracts of corresponding papers.A Meta-Reasoning Prompting p M R is defined to guide the selection process.For each reasoning method α i (i ranging from 1 to n), the model M evaluates the combined prompt (p i |p M R |x 0 ).This evaluation yields a score s i indicating the effectiveness of method α i for the given input x 0 .
s i = M (p i ∥p M R ∥x 0 ) for i = 1, 2, . . . , n.(1)
The algorithm identifies the reasoning method α k that receives the highest score s i by finding the index k that maximizes the set s 1 , s 2 , . . ., s n .
k = arg max i {s 1 , s 2 , . . . , s n }(2)
Once the best reasoning method α k is determined, it is executed on the input x 0 .The model M generates the final output y 0 using the prompt (p k |x 0 ), which combines the description of the chosen reasoning method with the original input.
y 0 = α k (x 0 )(3)s i = M (p i ∥p M R ∥x 0 ) 3: end for 4: k = arg max i {s 1 , s 2 , . . . , s n } 5:
Determine k for which α k is executed and reason with the chosen method.6: y 0 = α k (x 0 ) Return y 0 3 Experiments</p>
<p>Setup</p>
<p>Implementation of Meta-Reasoning Prompting We implement MRP with seven popular and distinct in-context learning reasoning methods, which also serve as our baseline for comparison.We prompt descriptions for each method, allowing the LLM to understand.</p>
<p>Tasks We experiment with seven diverse tasks, Details about the dataset and its construction are provided in Appendix A.1:</p>
<ol>
<li>Arithmetic Reasoning: GSM8K [3], 1319 basic math questions. 2. Complex Mathematical Reasoning: Game of 24 [32], a game using 4 numbers and basic arithmetic four operations to obtain 24. 3. Creative Writing: Trivia Creative Writing (Trivia CW) [26,14], necessitating the model to assimilate and combine heterogeneous information from multiple domains internally. 4. Multi-Hop Reasoning: HotpotQA, [31], requiring models to connect pieces of information from multiple documents to answer a question.5. Social Reasoning: BigToM [8], to evaluate social situations understanding and the theory of mind.6.Computer Code: Code Readability (Code) [19], to enhance the readability of given code snippets.7. STEM: MMLU [11], Physics, Chemistry, Biology, and Math problems of high school domain.Metrics To prevent any method from skewing the results due to exceptional performance on a specific task, we reported both the arithmetic mean accuracy and the harmonic mean accuracy of each method across all benchmarks.</li>
</ol>
<p>Models We used gpt-3.5-turbo2and gpt-4-turbo 3 with identical prompts to compare the effect of model size on meta-reasoning ability.</p>
<p>Baselines We select seven popular reasoning methods as baselines.These methods include:</p>
<ol>
<li>
<p>Chain-of-Thoughts: breaking down problems into a series of coherent reasoning steps [27].</p>
</li>
<li>
<p>Tree-of-Thoughts: exploring multiple reasoning paths and self-evaluating choices to solve complex problems [32].</p>
</li>
<li>
<p>Analogical prompting: self-generating few-shots based on past experiences and related problems [33].</p>
</li>
<li>
<p>Self-Refine: self-evaluating for refinement and continuously improving the output [17].</p>
</li>
<li>
<p>Solo Performance Prompting: simulating multiple personas to collaboratively solve complex tasks [26].</p>
</li>
</ol>
<p>6.</p>
<p>Step-Back Prompting: abstract high-level concepts and principles to guide the reasoning process [38].</p>
<ol>
<li>SimToM: enabling perspective-taking to understand the character's beliefs and goals [28]</li>
</ol>
<p>Main Results</p>
<p>Meta-Reasoning Prompting performs best on comprehensive tasks As shown in table 1, MRP consistently exhibits robust performance across multiple benchmarks.MRP achieves the second-best in 4 of 7 tasks, including Gameof24, TriviaQA, BigToM and Code.This impressive performance across a wide range of tasks demonstrates MRP's ability to effectively select and apply appropriate reasoning methods tailored to the specific requirements of each task.In terms of overall performance, MRP attains the highest across the 7 tasks, with an average of 0.772.In contrast, although TOT excels in certain tasks such as GSM8K and Gameof24, it performs less impressively in others.We observe noticeable performance gaps compared with MRP in tasks such as BigToM (0.43 VS 0.57) and Code (0.765 VS 0.867).This consistent excellence across all benchmarks underscores MRP's advantages, demonstrating its ability to maintain impressive performance across diverse task domains (as shown in figure 4).</p>
<p>Meta-reasoning capability is influenced by the base model capability As illustrated in table 2, while the performance with GPT-4 is satisfactory, the experimental results with GPT-3.5 indicate that the effectiveness of MRP is suboptimal.Error analysis revealed the main issues: Scoring Error, Self-opinion, Factual Error, and Reasoning Error.This indicates that when the model's capabilities are limited, it cannot have sufficient awareness of its own reasoning abilities and the meta-issues behind the reasoning problems.This performance drop also appears in other reasoning methods, which also indicates that the capability of meta-reasoning, like other reasoning abilities, improves as the model becomes more powerful.</p>
<p>Figure 5: Performance of methods on GSM8K benchmark</p>
<p>Meta-Reasoning Prompting is less effective for simple tasks but significantly improved for more differentiated tasks From the experimental results (see figure 5), it can be seen that MRP and other methods show equal competitiveness on GSM8K, the accuracy of all the reasoning methods is above 90%, but the differentiation between the accuracy of each method is not very high, it can be seen that when the task is simpler, it is harder for MRP to reflect its own advantages, but MRP method is better than each method on the more difficult and comprehensive But the MRP method is significantly better than the other methods in the more difficult and comprehensive tasks.</p>
<p>4 Related Works</p>
<p>Reasoning with LLMs</p>
<p>Prompt-based reasoning methods have become a key technology for enhancing the capabilities of pretrained large language models (LLMs).The Chain-of-Thought (CoT) prompting [27], and its Figure 4: The inference process of large language models (LLMs) under meta-reasoning prompting.</p>
<p>variants [37,39,2,12,25], such as Tree of thoughts (TOT) [32], Graph of thoughts (GOT) [1], enhances LLMs' ability to decompose complex tasks into smaller, manageable tasks, utilizing structured approaches to explore problem-solving pathways.Numerous studies have demonstrated the exceptional performance of prompt-based reasoning methods across various domains and benchmarks.[17,28,38,20,23] Some researchers have even employed analogical reasoning [34,7,33], enabling large models to generate similar questions based on user queries and subsequently summarize solutions based on the answers to these questions.While independent reasoning methods have been proven to improve LLM performance from different perspectives, they still fail to meet integrated problems.</p>
<p>There are also some methods to enhance LLM reasoning through ensemble mechanisms or tuning.X-of-Thoughts improves the success rate of LLM on arithmetic problems by integrating three methods [15].It proposes a trial-and-error iterative mechanism that allows LLM to autonomously repeat attempts to find a final solution.Ni et al.blending off-the-shelf benchmarks to create a comprehensive, integrated LLM assessment [18].Mixtural-Of-Prompts (MoP) dynamically manage and optimize prompt tuning across heterogeneous tasks and data distributions, significantly reducing perplexity and mitigating interference in multi-task scenarios [6].Some researchers fine-tune smaller models with a well-prepared dataset inspired by preference learning to achieve reasoning power comparable to a larger model [35,22,29].They present problem-method coupled datasets and show how to improve the model's grasp of inference skills at the data level.However, there is still a lack of research to explore the meta-reasoning ability of LLMs to choose reasoning methods.</p>
<p>Meta Reasoning</p>
<p>Meta-reasoning is a crucial cognitive process in human intelligence, involving the recognition and interpretation of reasoning to select optimal methods based on past experiences [9].In artificial intelligence, it refers to efficiently deploying computational resources for informed decision-making in specific situations [4,5].Recently, some works develop routing or buffer systems to improve performance, using supervised learning algorithms [21], reward model-based techniques, and other methods [10,16,24].Hu et al. created a benchmark to evaluate these methods' effectiveness [13].</p>
<p>Zeng et al. noted the neglect of meta-reasoning in independent LLMs and proposed a benchmark to evaluate reasoning rationality [36].In [30], the authors introduce a meta-buffer to store a series of high-level thoughts distilled from problem-solving processes across various tasks.This approach aligns with the inherent logic of meta reasoning.However, MRP achieves simple and efficient metacognitive effects by directly unleashing the meta reasoning capabilities of LLM through prompts, without introducing complicated mechanisms.</p>
<p>Conclusions and Outlook</p>
<p>This paper introduces Meta-Reasoning Prompting (MRP), a novel and efficient approach inspired by human meta-reasoning, designed to enhance the adaptability and efficiency of large language models (LLMs).By dynamically selecting and applying the most suitable reasoning method for each task, MRP enables LLMs to optimize performance across diverse problem domains, achieving near state-of-the-art results in comprehensive benchmarks.</p>
<p>Our experiments demonstrate that MRP significantly improves LLMs' ability to handle tasks requiring a blend of different reasoning strategies, particularly in larger models like GPT-4.This dynamic adaptability highlights MRP's potential to address the limitations of traditional reasoning techniques, offering a more flexible and effective solution for varied and complex tasks.</p>
<p>Looking ahead, future research could explore the integration of MRP into training datasets to further enhance LLMs' general reasoning abilities.Additionally, combining MRP with other advanced reasoning techniques could yield further improvements in model performance.Overall, MRP represents a significant step forward in developing more intelligent, efficient, and adaptable AI systems, capable of meeting the diverse demands of real-world problem-solving.</p>
<p>Limitations</p>
<p>Our study investigates the meta-reasoning mechanisms of LLMs by dynamically selecting suitable methods to enhance their performance across various reasoning tasks without introducing new knowledge or training efforts.Currently, Meta-Reasoning Prompting (MRP) selects the highestscoring method for each task.However, drawing from human cognitive processes, tackling complex problems often involves combining multiple reasoning methods.Future research will explore mechanisms such as Top-Probability (Top-P) or Top-K to allow models to ensemble relevant methods, potentially achieving better performance.</p>
<p>Our experimental results indicate that the meta-reasoning ability of LLMs is influenced by the capabilities of the models themselves.For instance, GPT-4's Meta-Reasoning Prompting shows significantly greater improvement compared to GPT-3.5, which aligns with our expectations.Nonetheless, we can further enhance the smaller model's meta-reasoning capabilities through instruction tuning in future works.</p>
<p>Due to space constraints and limited resources, our experiments primarily tested the most representative LLMs (GPT-4 and GPT-3.5).We did not fully cover the performance of other open-source or closed-source models.However, we believe that the experimental results on these representative LLMs provide sufficient insights and implications.</p>
<p>Figure 1 :
1
Figure 1: Illustration of Meta-Reasoning Prompting (MRP) and the difference compared to standard reasoning and traditional reasoning methods.</p>
<p>Figure 2 :
2
Figure 2: Meta-Reasoning Prompt.</p>
<p>Figure 3 :
3
Figure3: (a) Comparison of methods on different benchmarks reveals that guiding LLM to dynamically choose the appropriate reasoning method enables MRP to achieve consistently better performance across all tasks.(b) The arithmetic and harmonic average performances of applying a specific reasoning approach to all benchmarks demonstrate that MRP consistently excels in overall evaluation.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Prompt of COT</p>
<p>Figure 8 :Figure 9 : 13 Figure 10 : 14 Figure 11 :
8913101411
Figure 8: Prompt of TOT</p>
<p>Table 1 :
1
Experiments with GPT4: Comparison of performance on benchmarks using Meta-Reasoning Prompting versus using other methods independently.Bold represents the best performance, and underline represents the second-best performance.
MethodGSM8K Gameof24 Trivia CW HotpotQA BigToMCodeMMLU Macro Avg.COT0.9140.0500.7620.8000.4700.6850.8940.654TOT0.9420.4100.7860.7160.4300.7650.8150.725Analogical0.9240.0400.7350.7770.5000.6140.9470.648Self-Refine0.9290.0800.7640.7630.4700.8720.8610.677SPP0.9290.1700.8610.7630.5500.6720.8740.688STEP-BACK0.9330.0900.7870.8100.4200.8090.8410.670SimTom0.9380.0400.7390.6670.5900.6940.8150.640MRP (our)0.9210.3100.7960.7970.5700.8670.8540.772</p>
<p>Table 2 :
2
Experiments with GPT3.5:Comparison of performance on benchmarks using Meta-Reasoning Prompting versus using other methods independently.Bold represents the best performance, and underline represents the second-best performance.
MethodGSM8K Gameof24 Trivia CW HotpotQA BigToMCodeMMLU Macro Avg.COT0.8310.0300.4140.1870.6100.5780.6750.416TOT0.8100.1000.1550.3600.4300.7970.7350.352Analogical0.8250.0600.3240.1970.6600.7290.7210.433Self-Refine0.7160.0300.2130.1670.6500.7960.5430.372SPP0.8230.1600.5360.2170.5400.6840.6890.469STEP-BACK0.8170.0100.5360.1900.5700.6420.7880.452SimTom0.5860.0400.2400.1770.4600.5990.5030.315MRP (our)0.7810.0500.3460.1870.6000.7590.7220.433
Azure OpenAI, Model Name: gpt-35-turbo, API Version: 0301
Azure OpenAI, Model Name: gpt-4, API Version: 1106-Preview
A Implementation DetailsA.1 Dataset Details Table3shows the split and number of examples used for evaluations in GSM8K, Game of 24, Trivia Creative Writing, HotpotQA, BigTOM, Code Readability and MMLU.The dataset sizes of GSM8K, Gameof24, Trivia Creative Writing are consistent with the size used in the references.To control cost, we randomly tested 100-300 sample of data from HotpotQA, BigTOM, and Code Readability and MMLU.Despite of the economic consideration, we found that on this data scale, MRP has achieved significant results.
Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Boosting of thoughts: Trial-and-error problem solving with large language models. Sijia Chen, Baochun Li, Di Niu, arXiv:2402.111402024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Metareasoning: An Introduction. Metareasoning. T Michael, Anita Cox, Raja, 2011</p>
<p>Metareasoning: Thinking about thinking. T Michael, Anita Cox, Raja, 2011MIT Press</p>
<p>Sweeping heterogeneity with smart mops: Mixture of prompts for llm task adaptation. Chen Dun, Mirian Del Carmen, Hipolito Garcia, Guoqing Zheng, Ahmed Hassan Awadallah, Anastasios Kyrillidis, Robert Sim, arXiv:2310.028422023arXiv preprint</p>
<p>Thought-retriever: Don't just retrieve raw data, retrieve thoughts. Tao Feng, Pengrui Han, Guanyu Lin, Ge Liu, Jiaxuan You, ICLR 2024 Workshop: How Far Are We From AGI. 2024</p>
<p>Understanding social reasoning in language models with language models. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah Goodman, Advances in Neural Information Processing Systems. 362024</p>
<p>Doing more with less: meta-reasoning and meta-learning in humans and machines. Frederick Thomas L Griffiths, Michael B Callaway, Erin Chang, Paul M Grant, Falk Krueger, Lieder, Current Opinion in Behavioral Sciences. 292019</p>
<p>Surya Narayanan, Hari , Matt Thomson, arXiv:2308.11601Tryage: Real-time, intelligent routing of user prompts to large language model. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Chain-ofsymbol prompting elicits planning in large langauge models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang, arXiv:2305.102762023arXiv preprint</p>
<p>Mars: A benchmark for multi-llm algorithmic routing system. Jason Qitian, Jacob Hu, Xiuyu Bieker, Nan Li, Benjamin Jiang, Gaurav Keigwin, Kurt Ranganath, Shriyash Keutzer, Upadhyay Kaustubh, ICLR 2024 Workshop: How Far Are We From AGI. 2024</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017arXiv preprint</p>
<p>Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts. Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Blending is all you need: Cheaper, better alternative to trillion-parameters llm. Xiaoding Lu, Adian Liusie, Raina Vyas, Yuwen Zhang, William Beauchamp, arXiv:2401.029942024arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, Yang You, arXiv:2406.06565Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. 2024arXiv preprint</p>
<p>Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, arXiv:2105.126552021arXiv preprint</p>
<p>A systematic survey of prompt engineering in large language models. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.079272024Techniques and applications. arXiv preprint</p>
<p>Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, Mikhail Yurochkin, arXiv:2309.15789Large language model routing with benchmark datasets. 2023arXiv preprint</p>
<p>Branch-train-mix: Mixing expert llms into a mixture-of-experts llm. Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-Tau Yih, Jason Weston, arXiv:2403.078162024arXiv preprint</p>
<p>Meta-prompting: Enhancing language models with task-agnostic scaffolding. Mirac Suzgun, Adam Tauman, Kalai , arXiv:2401.129542024arXiv preprint</p>
<p>Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou, arXiv:2406.04692Mixture-of-agents enhances large language model capabilities. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.05300202313arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Alex Wilf, Shawn Sihyun, Paul Pu Lee, Louis-Philippe Liang, Morency, arXiv:2311.10227Think twice: Perspective-taking improves large language models' theory-of-mind capabilities. 2023arXiv preprint</p>
<p>Mixture-of-instructions: Comprehensive alignment of a large language model through the mixture of diverse system prompting instructions. Bowen Xu, Shaoyu Wu, Kai Liu, Lulu Hu, arXiv:2404.184102024arXiv preprint</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.04271Buffer of thoughts: Thought-augmented reasoning with large language models. 2024arXiv preprint</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, Denny Zhou, arXiv:2310.01714Large language models as analogical reasoners. 2023arXiv preprint</p>
<p>Thought propagation: An analogical approach to complex reasoning with large language models. Junchi Yu, Ran He, Rex Ying, arXiv:2310.039652023arXiv preprint</p>
<p>Advancing llm reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, arXiv:2404.020782024arXiv preprint</p>
<p>Mr-gsm8k: A meta-reasoning revolution in large language model evaluation. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia, 2024</p>
<p>Enhancing zero-shot chain-of-thought reasoning in large language models through logic. Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter, arXiv:2309.133392023arXiv preprint</p>
<p>Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Quoc V Chi, Denny Le, Zhou, arXiv:2310.06117Take a step back: Evoking reasoning via abstraction in large language models. 2023arXiv preprint</p>
<p>Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen, arXiv:2311.08734Thread of thought unraveling chaotic contexts. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
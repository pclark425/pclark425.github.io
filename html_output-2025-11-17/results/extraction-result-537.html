<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-537 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-537</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-537</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-70671018d4597b6d2d0c99b38b1f1a3f1271eaec</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/70671018d4597b6d2d0c99b38b1f1a3f1271eaec" target="_blank">Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found that both CNN (convolutional neural network) features and word embeddings predict human judgments of similarity well and that these vectors can be further specialized in spatial knowledge if updated when training the model that predicts spatial arrangements of objects.</p>
                <p><strong>Paper Abstract:</strong> Spatial understanding is crucial in many real-world problems, yet little progress has been made towards building representations that capture spatial knowledge. Here, we move one step forward in this direction and learn such representations by leveraging a task consisting in predicting continuous 2D spatial arrangements of objects given object-relationship-object instances (e.g., “cat under chair”) and a simple neural network model that learns the task from annotated images. We show that the model succeeds in this task and, furthermore, that it is capable of predicting correct spatial arrangements for unseen objects if either CNN features or word embeddings of the objects are provided. The differences between visual and linguistic features are discussed. Next, to evaluate the spatial representations learned in the previous task, we introduce a task and a dataset consisting in a set of crowdsourced human ratings of spatial similarity for object pairs. We find that both CNN (convolutional neural network) features and word embeddings predict human judgments of similarity well and that these vectors can be further specialized in spatial knowledge if we update them when training the model that predicts spatial arrangements of objects. Overall, this paper paves the way towards building distributed spatial representations, contributing to the understanding of spatial expressions in language.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e537.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e537.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialRegNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-based spatial regression feedforward network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feedforward neural model that maps a textual triplet (Subject, Relationship, Object) represented as continuous embeddings plus the Subject's 2D center and size to a prediction of the Object's 2D center coordinates and bounding-box half-sizes, trained with a mean-squared-error regression loss on annotated image coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embedding-based feedforward regression model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-layer MLP (two hidden layers of 100 ReLU units) that takes concatenated embeddings for S, R, O (either pre-trained GloVe or VGG-derived visual centroids, or one-hot / random initializations) plus Subject center S^c and size S^b, and outputs continuous predictions for Object center O^c and size O^b. Trained supervised with RMSprop, learning rate 1e-4, MSE objective; embeddings can be kept fixed or updated by backpropagation to specialize them in spatial knowledge. The model discards image pixels and learns exclusively from image coordinates and embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prediction task (S,R,O -> object 2D location and size)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a triple (Subject, Relationship, Object), predict the Object's normalized 2D bounding-box center coordinates (O^c in [0,1]^2) and bounding-box half-sizes (O^b) relative to the Subject, using training data of image instances with annotated boxes (Visual Genome). The model uses mirroring to remove left/right arbitrariness and receives Subject center and size as additional inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational + spatial prediction</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised training on image-coordinate annotations (Visual Genome) combined with pre-trained embeddings (GloVe for language, VGG-derived visual centroids) or random/one-hot initialization; learned by backpropagation when embeddings are updated</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning / regression (training on (S,R,O) -> coordinates), probing via evaluation on held-out and unseen-word splits</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>continuous distributed embeddings (embedding matrices W_S, W_R, W_O updated optionally) whose activations are composed by MLP layers; final outputs are explicit continuous spatial coordinates and sizes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean Squared Error (MSE) on predicted (O^c, O^b), R^2 (coefficient of determination), Pearson r for x/y, above/below classification accuracy and F1, Intersection-over-Union (IoU) for predicted boxes, Spearman rho for similarity task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Representative results (Original test set, lang embeddings initialized with GloVe and updated): MSE=0.011, R^2=0.654, above/below accuracy acc_y=0.773, Pearson r_y=0.832, IoU=0.283. Under unseen-word conditions (Unseen Words split) best non-updating/lang-INItialized: MSE≈0.009, R^2≈0.663, acc_y≈0.770, r_x≈0.888, r_y≈0.835, IoU≈0.164. Random predictor baseline: MSE~0.79 and R^2≈-27.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully predicts coarse 2D object center locations (especially vertical relations) and relative object sizes; generalizes to unseen objects when initialized with semantically or visually informative embeddings (GloVe or VGG centroids); concatenation of visual+linguistic signals improves correspondence with human spatial similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fine-grained lateral (x-axis) placement and precise bounding-box overlap (IoU) remain limited (IoU relatively low); predictions degrade markedly with random embeddings; some errors arise from noisy or irrelevant (S,R,O) combinations in Visual Genome and from ambiguous cases (e.g., separable instances of the same class); updating embeddings can cause overfitting and reduce generalization to unseen embeddings if those embeddings are not co-updated.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random predictor: MSE ~0.79, R^2 << 0; one-hot non-updated (NU-1H) sometimes matches or slightly outperforms in seen-word conditions (e.g., Original set acc_y up to ~0.788 and IoU up to ~0.308), but fails to generalize to unseen words; INI (pre-trained) embeddings (GloVe or VGG) outperform random initializations substantially under unseen-word evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key comparisons served as ablations: INI (pre-trained) vs RND (random) embeddings shows large improvement for unseen words; updating (U) vs non-updating (NU) embeddings affects generalization—non-updated often better for unseen-word tests because updated embedding space shifts; vis vs lang vs concat: concatenated visual+linguistic gives best correlation with human spatial similarity (concatenated updated embeddings Spearman rho ~0.60 vs GloVe ~0.543 and VGG ~0.459); removing Subject size input still yields learning of average sizes but inclusion improves interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A text-input model with continuous embeddings can encode and use spatial and object-relational knowledge to predict 2D object arrangements without direct sensory (pixel) input; pre-trained linguistic and visual embeddings contain spatial cues (size, typical location/affordance) that enable generalization to unseen objects, and supervised training on (S,R,O)->spatial coordinates further specializes embeddings to better match human spatial similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e537.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e537.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GloVe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GloVe (Global Vectors for Word Representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>300-dimensional pre-trained word embeddings trained on Common Crawl used as linguistic object representations that encode semantic attributes (size, functionality) informative for predicting typical spatial arrangements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GloVe: Global Vectors for Word Representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GloVe</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>300-dimensional dense vectors pre-trained on Common Crawl (840B tokens) representing words' distributional semantics; used here to initialize the Subject/Relationship/Object embeddings and optionally fine-tuned via backpropagation on the spatial prediction task.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used in Prediction and Spatial Similarity tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides linguistic feature vectors for S, R, O that the regression model composes to predict object 2D location/size; cosine similarity of (possibly updated) GloVe vectors is used to predict human spatial similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational + spatial (as encoded by language)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (Common Crawl), optionally fine-tuned on supervised spatial regression data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>used as fixed input embeddings (NU) or fine-tuned by supervised learning (U) — no prompting; evaluated by regression outputs and Spearman correlation to human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>distributed dense vectors encoding lexical semantics and attributes (size, function) that correlate with spatial behavior</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman rho against human spatial similarity ratings; regression metrics when used in prediction model</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GloVe baseline Spearman rho with human spatial ratings: 0.543 (LANG column). When used and updated in the Prediction task (U-INI_lang) Spearman rho increases to ~0.557; in Original prediction set: MSE≈0.011, R^2≈0.654, acc_y≈0.773, IoU≈0.283 (U-INI_lang).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Encodes object attributes (size, function) that allow correct prediction of typical spatial relations and good correlation with human spatial-similarity judgments; enables generalization to unseen objects in the regression model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Less effective than the visual+linguistic concatenation; can be misled when distributional similarity is driven by non-spatial factors (resulting in some false similarity associations); updating excessively can lead to overfitting to training spatial contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to VGG visual centroids (VGG-128) GloVe yields higher correlation with human spatial ratings (0.543 vs ~0.459); concatenation of GloVe+VGG improves further (~0.582) and updated concatenation up to ~0.60.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Updating GloVe on the spatial prediction task yields a statistically significant improvement in Spearman correlation (U-INI_lang rho ~0.557 vs baseline GloVe 0.543); replacing GloVe with random embeddings yields large performance drop (U-RND rho ~0.15).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pre-trained linguistic embeddings encode spatially relevant object knowledge (size, typical configurations) that can be exploited by a model operating without pixels to predict object placements; supervised spatial training can further specialize these embeddings to better align with human spatial judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e537.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e537.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VGG-128 centroids</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VGG-128 visual centroid embeddings (ImageNet-derived visual features averaged per concept)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>128-dimensional visual feature vectors obtained by averaging VGG-derived CNN activations across ImageNet images for each object concept; used as visual object representations that carry visual cues relevant to spatial arrangements (size, typical viewpoint).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Imagined Visual Representations as Multimodal Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VGG-128 visual centroid embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>128-dim feature vectors produced by a VGG-style CNN (VGG-128) forward pass on ImageNet images, averaged per concept to form a centroid representing typical visual appearance; used to initialize Subject/Object embeddings in the regression model (Relationships use one-hot when visual embeddings are unavailable).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used in Prediction and Spatial Similarity tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides visual priors (e.g., object size cues, canonical poses) that the regression model uses to predict object 2D location/size and that are correlated with human spatial-similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (visual)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on ImageNet visual data; centroid computed across ImageNet images per concept; optionally fine-tuned in the Prediction task</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>used as fixed input embeddings (NU) or fine-tuned by supervised spatial regression (U); similarity evaluated by cosine and Spearman rho to human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>distributed visual feature vectors (centroids) encoding average appearance statistics correlated with spatial attributes (apparent size, pose, typical context)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman rho for spatial similarity; regression metrics for spatial prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>VGG-128 baseline Spearman rho with human spatial ratings: ~0.459 (V&L column); updated visual embeddings (U-INI_vis) improve to ~0.48; combined updated visual+linguistic concatenation reaches ~0.60. In prediction tasks U-INI_vis Original set: MSE ~0.011, R^2 ~0.627, acc_y ~0.766, r_y ~0.820, IoU ~0.266; unseen-word conditions show degradation but still better than random.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Capture visual size/pose cues helpful for spatial arrangement prediction and for matching some human spatial similarities; contribute complementary information to linguistic embeddings, improving overall spatial judgments when concatenated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Less predictive alone than linguistic embeddings for human spatial similarity; visual centroids do not exist for Relationships (verbs/prepositions) so Relationships used one-hot vectors in vis models; visual similarity sometimes groups unrelated objects (causing mistaken spatial predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed by GloVe alone on spatial-similarity (GloVe 0.543 vs VGG ~0.459); concatenation of both modalities yields the best performance (concatenated pre-trained ~0.582, concatenated updated ~0.60). Random visual embeddings produce very poor performance (U-RND rho ~0.174).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Updating visual embeddings (U-INI_vis) significantly improves Spearman correlation over raw VGG-128; concatenating updated visual+linguistic embeddings gives the largest improvement, indicating complementary contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Averaged CNN visual centroids encode spatially relevant visual cues (size, typical placement) that, although less predictive than language embeddings alone, complement linguistic information and when specialized via supervised training improve alignment with human spatial knowledge; importantly these visual features can be used by a model operating without pixel inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e537.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e537.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding specialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Backpropagation-based specialization of object embeddings for spatial knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The process of updating pre-trained word or visual embeddings by backpropagating the spatial prediction loss so that embeddings become more specialized in encoding spatial relationships and better correlate with human spatial similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embedding specialization via supervised spatial training</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embeddings (GloVe or VGG centroids) are used to initialize the embedding matrices W_S, W_R, W_O and are optionally fine-tuned during training of the spatial regression model; the updated embeddings are then evaluated directly via cosine similarity against crowdsourced human spatial-similarity ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial Similarity evaluation (and Prediction task during training)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>After training the prediction model with embedding updates, the specialized embeddings are evaluated by computing cosine similarities between object vectors and measuring Spearman correlation with human spatial-similarity ratings (1,016 word pairs collected).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation specialization for spatial knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>initial embeddings from pre-training (text or vision) refined by supervised loss on (S,R,O)->spatial coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning of embeddings by backpropagation during supervised regression training; evaluation via probing (cosine similarity correlation with human ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>modified continuous embedding vectors whose geometry is altered to bring spatially similar objects closer in embedding space (explicitly measurable via cosine similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman rho between embedding cosine similarity and human spatial-similarity ratings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Updating embeddings in the Prediction task yielded statistically significant improvements: U-INI_lang Spearman rho ~0.557 (vs GloVe 0.543), U-INI_vis rho ~0.48 (vs VGG-128 ~0.459); concatenated updated embeddings reach ~0.60 (best reported).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Specialized embeddings show increased correlation with human judgments of spatial similarity and improved ability to generalize spatial predictions to unseen objects; both modalities benefit from specialization, and multimodal fusion benefits the most.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Overfitting risk observed for updated linguistic embeddings with excessive epochs (performance can worsen), and updated spaces may reduce compatibility with unseen, fixed pre-trained vectors unless those are also updated.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Un-updated pre-trained embeddings (GloVe/VGG) and random initializations: updated embeddings outperform both (random embeddings show drastic underperformance, U-RND rho ~0.15).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Comparing updated vs non-updated embeddings serves as an ablation: non-updated (NU) often better for some unseen-word generalization scenarios because updated embedding space may shift; however, for spatial-similarity evaluation the updated embeddings consistently improved Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Backpropagating spatial prediction supervision into pre-trained embeddings produces distributed representations more specialized for spatial knowledge, improving alignment to human spatial similarity judgments and enabling better textual-only generalization for spatial arrangement prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation <em>(Rating: 2)</em></li>
                <li>Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates <em>(Rating: 2)</em></li>
                <li>Grounding Spatial Relations for Human-Robot Interaction <em>(Rating: 2)</em></li>
                <li>Imagined Visual Representations as Multimodal Embeddings <em>(Rating: 2)</em></li>
                <li>The Human Semantic Potential: Spatial Language and Constrained Connectionism <em>(Rating: 1)</em></li>
                <li>Don't Just Listen, Use your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-537",
    "paper_id": "paper-70671018d4597b6d2d0c99b38b1f1a3f1271eaec",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "SpatialRegNet",
            "name_full": "Embedding-based spatial regression feedforward network",
            "brief_description": "A feedforward neural model that maps a textual triplet (Subject, Relationship, Object) represented as continuous embeddings plus the Subject's 2D center and size to a prediction of the Object's 2D center coordinates and bounding-box half-sizes, trained with a mean-squared-error regression loss on annotated image coordinates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Embedding-based feedforward regression model",
            "model_size": null,
            "model_description": "Two-layer MLP (two hidden layers of 100 ReLU units) that takes concatenated embeddings for S, R, O (either pre-trained GloVe or VGG-derived visual centroids, or one-hot / random initializations) plus Subject center S^c and size S^b, and outputs continuous predictions for Object center O^c and size O^b. Trained supervised with RMSprop, learning rate 1e-4, MSE objective; embeddings can be kept fixed or updated by backpropagation to specialize them in spatial knowledge. The model discards image pixels and learns exclusively from image coordinates and embeddings.",
            "task_name": "Prediction task (S,R,O -&gt; object 2D location and size)",
            "task_description": "Given a triple (Subject, Relationship, Object), predict the Object's normalized 2D bounding-box center coordinates (O^c in [0,1]^2) and bounding-box half-sizes (O^b) relative to the Subject, using training data of image instances with annotated boxes (Visual Genome). The model uses mirroring to remove left/right arbitrariness and receives Subject center and size as additional inputs.",
            "task_type": "object-relational + spatial prediction",
            "knowledge_type": "spatial + object-relational",
            "knowledge_source": "supervised training on image-coordinate annotations (Visual Genome) combined with pre-trained embeddings (GloVe for language, VGG-derived visual centroids) or random/one-hot initialization; learned by backpropagation when embeddings are updated",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised fine-tuning / regression (training on (S,R,O) -&gt; coordinates), probing via evaluation on held-out and unseen-word splits",
            "knowledge_representation": "continuous distributed embeddings (embedding matrices W_S, W_R, W_O updated optionally) whose activations are composed by MLP layers; final outputs are explicit continuous spatial coordinates and sizes",
            "performance_metric": "Mean Squared Error (MSE) on predicted (O^c, O^b), R^2 (coefficient of determination), Pearson r for x/y, above/below classification accuracy and F1, Intersection-over-Union (IoU) for predicted boxes, Spearman rho for similarity task",
            "performance_result": "Representative results (Original test set, lang embeddings initialized with GloVe and updated): MSE=0.011, R^2=0.654, above/below accuracy acc_y=0.773, Pearson r_y=0.832, IoU=0.283. Under unseen-word conditions (Unseen Words split) best non-updating/lang-INItialized: MSE≈0.009, R^2≈0.663, acc_y≈0.770, r_x≈0.888, r_y≈0.835, IoU≈0.164. Random predictor baseline: MSE~0.79 and R^2≈-27.",
            "success_patterns": "Successfully predicts coarse 2D object center locations (especially vertical relations) and relative object sizes; generalizes to unseen objects when initialized with semantically or visually informative embeddings (GloVe or VGG centroids); concatenation of visual+linguistic signals improves correspondence with human spatial similarity judgments.",
            "failure_patterns": "Fine-grained lateral (x-axis) placement and precise bounding-box overlap (IoU) remain limited (IoU relatively low); predictions degrade markedly with random embeddings; some errors arise from noisy or irrelevant (S,R,O) combinations in Visual Genome and from ambiguous cases (e.g., separable instances of the same class); updating embeddings can cause overfitting and reduce generalization to unseen embeddings if those embeddings are not co-updated.",
            "baseline_comparison": "Random predictor: MSE ~0.79, R^2 &lt;&lt; 0; one-hot non-updated (NU-1H) sometimes matches or slightly outperforms in seen-word conditions (e.g., Original set acc_y up to ~0.788 and IoU up to ~0.308), but fails to generalize to unseen words; INI (pre-trained) embeddings (GloVe or VGG) outperform random initializations substantially under unseen-word evaluation.",
            "ablation_results": "Key comparisons served as ablations: INI (pre-trained) vs RND (random) embeddings shows large improvement for unseen words; updating (U) vs non-updating (NU) embeddings affects generalization—non-updated often better for unseen-word tests because updated embedding space shifts; vis vs lang vs concat: concatenated visual+linguistic gives best correlation with human spatial similarity (concatenated updated embeddings Spearman rho ~0.60 vs GloVe ~0.543 and VGG ~0.459); removing Subject size input still yields learning of average sizes but inclusion improves interpretability.",
            "key_findings": "A text-input model with continuous embeddings can encode and use spatial and object-relational knowledge to predict 2D object arrangements without direct sensory (pixel) input; pre-trained linguistic and visual embeddings contain spatial cues (size, typical location/affordance) that enable generalization to unseen objects, and supervised training on (S,R,O)-&gt;spatial coordinates further specializes embeddings to better match human spatial similarity judgments.",
            "uuid": "e537.0",
            "source_info": {
                "paper_title": "Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "GloVe",
            "name_full": "GloVe (Global Vectors for Word Representation)",
            "brief_description": "300-dimensional pre-trained word embeddings trained on Common Crawl used as linguistic object representations that encode semantic attributes (size, functionality) informative for predicting typical spatial arrangements.",
            "citation_title": "GloVe: Global Vectors for Word Representation",
            "mention_or_use": "use",
            "model_name": "GloVe",
            "model_size": null,
            "model_description": "300-dimensional dense vectors pre-trained on Common Crawl (840B tokens) representing words' distributional semantics; used here to initialize the Subject/Relationship/Object embeddings and optionally fine-tuned via backpropagation on the spatial prediction task.",
            "task_name": "Used in Prediction and Spatial Similarity tasks",
            "task_description": "Provides linguistic feature vectors for S, R, O that the regression model composes to predict object 2D location/size; cosine similarity of (possibly updated) GloVe vectors is used to predict human spatial similarity judgments.",
            "task_type": "object-relational + spatial (as encoded by language)",
            "knowledge_type": "object-relational + spatial",
            "knowledge_source": "pre-training on large text corpora (Common Crawl), optionally fine-tuned on supervised spatial regression data",
            "has_direct_sensory_input": false,
            "elicitation_method": "used as fixed input embeddings (NU) or fine-tuned by supervised learning (U) — no prompting; evaluated by regression outputs and Spearman correlation to human ratings",
            "knowledge_representation": "distributed dense vectors encoding lexical semantics and attributes (size, function) that correlate with spatial behavior",
            "performance_metric": "Spearman rho against human spatial similarity ratings; regression metrics when used in prediction model",
            "performance_result": "GloVe baseline Spearman rho with human spatial ratings: 0.543 (LANG column). When used and updated in the Prediction task (U-INI_lang) Spearman rho increases to ~0.557; in Original prediction set: MSE≈0.011, R^2≈0.654, acc_y≈0.773, IoU≈0.283 (U-INI_lang).",
            "success_patterns": "Encodes object attributes (size, function) that allow correct prediction of typical spatial relations and good correlation with human spatial-similarity judgments; enables generalization to unseen objects in the regression model.",
            "failure_patterns": "Less effective than the visual+linguistic concatenation; can be misled when distributional similarity is driven by non-spatial factors (resulting in some false similarity associations); updating excessively can lead to overfitting to training spatial contexts.",
            "baseline_comparison": "Compared to VGG visual centroids (VGG-128) GloVe yields higher correlation with human spatial ratings (0.543 vs ~0.459); concatenation of GloVe+VGG improves further (~0.582) and updated concatenation up to ~0.60.",
            "ablation_results": "Updating GloVe on the spatial prediction task yields a statistically significant improvement in Spearman correlation (U-INI_lang rho ~0.557 vs baseline GloVe 0.543); replacing GloVe with random embeddings yields large performance drop (U-RND rho ~0.15).",
            "key_findings": "Pre-trained linguistic embeddings encode spatially relevant object knowledge (size, typical configurations) that can be exploited by a model operating without pixels to predict object placements; supervised spatial training can further specialize these embeddings to better align with human spatial judgments.",
            "uuid": "e537.1",
            "source_info": {
                "paper_title": "Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "VGG-128 centroids",
            "name_full": "VGG-128 visual centroid embeddings (ImageNet-derived visual features averaged per concept)",
            "brief_description": "128-dimensional visual feature vectors obtained by averaging VGG-derived CNN activations across ImageNet images for each object concept; used as visual object representations that carry visual cues relevant to spatial arrangements (size, typical viewpoint).",
            "citation_title": "Imagined Visual Representations as Multimodal Embeddings",
            "mention_or_use": "use",
            "model_name": "VGG-128 visual centroid embeddings",
            "model_size": null,
            "model_description": "128-dim feature vectors produced by a VGG-style CNN (VGG-128) forward pass on ImageNet images, averaged per concept to form a centroid representing typical visual appearance; used to initialize Subject/Object embeddings in the regression model (Relationships use one-hot when visual embeddings are unavailable).",
            "task_name": "Used in Prediction and Spatial Similarity tasks",
            "task_description": "Provides visual priors (e.g., object size cues, canonical poses) that the regression model uses to predict object 2D location/size and that are correlated with human spatial-similarity judgments.",
            "task_type": "object-relational + spatial",
            "knowledge_type": "spatial + object-relational (visual)",
            "knowledge_source": "pre-training on ImageNet visual data; centroid computed across ImageNet images per concept; optionally fine-tuned in the Prediction task",
            "has_direct_sensory_input": false,
            "elicitation_method": "used as fixed input embeddings (NU) or fine-tuned by supervised spatial regression (U); similarity evaluated by cosine and Spearman rho to human judgments",
            "knowledge_representation": "distributed visual feature vectors (centroids) encoding average appearance statistics correlated with spatial attributes (apparent size, pose, typical context)",
            "performance_metric": "Spearman rho for spatial similarity; regression metrics for spatial prediction",
            "performance_result": "VGG-128 baseline Spearman rho with human spatial ratings: ~0.459 (V&L column); updated visual embeddings (U-INI_vis) improve to ~0.48; combined updated visual+linguistic concatenation reaches ~0.60. In prediction tasks U-INI_vis Original set: MSE ~0.011, R^2 ~0.627, acc_y ~0.766, r_y ~0.820, IoU ~0.266; unseen-word conditions show degradation but still better than random.",
            "success_patterns": "Capture visual size/pose cues helpful for spatial arrangement prediction and for matching some human spatial similarities; contribute complementary information to linguistic embeddings, improving overall spatial judgments when concatenated.",
            "failure_patterns": "Less predictive alone than linguistic embeddings for human spatial similarity; visual centroids do not exist for Relationships (verbs/prepositions) so Relationships used one-hot vectors in vis models; visual similarity sometimes groups unrelated objects (causing mistaken spatial predictions).",
            "baseline_comparison": "Outperformed by GloVe alone on spatial-similarity (GloVe 0.543 vs VGG ~0.459); concatenation of both modalities yields the best performance (concatenated pre-trained ~0.582, concatenated updated ~0.60). Random visual embeddings produce very poor performance (U-RND rho ~0.174).",
            "ablation_results": "Updating visual embeddings (U-INI_vis) significantly improves Spearman correlation over raw VGG-128; concatenating updated visual+linguistic embeddings gives the largest improvement, indicating complementary contributions.",
            "key_findings": "Averaged CNN visual centroids encode spatially relevant visual cues (size, typical placement) that, although less predictive than language embeddings alone, complement linguistic information and when specialized via supervised training improve alignment with human spatial knowledge; importantly these visual features can be used by a model operating without pixel inputs.",
            "uuid": "e537.2",
            "source_info": {
                "paper_title": "Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Embedding specialization",
            "name_full": "Backpropagation-based specialization of object embeddings for spatial knowledge",
            "brief_description": "The process of updating pre-trained word or visual embeddings by backpropagating the spatial prediction loss so that embeddings become more specialized in encoding spatial relationships and better correlate with human spatial similarity judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Embedding specialization via supervised spatial training",
            "model_size": null,
            "model_description": "Embeddings (GloVe or VGG centroids) are used to initialize the embedding matrices W_S, W_R, W_O and are optionally fine-tuned during training of the spatial regression model; the updated embeddings are then evaluated directly via cosine similarity against crowdsourced human spatial-similarity ratings.",
            "task_name": "Spatial Similarity evaluation (and Prediction task during training)",
            "task_description": "After training the prediction model with embedding updates, the specialized embeddings are evaluated by computing cosine similarities between object vectors and measuring Spearman correlation with human spatial-similarity ratings (1,016 word pairs collected).",
            "task_type": "representation specialization for spatial knowledge",
            "knowledge_type": "spatial + object-relational",
            "knowledge_source": "initial embeddings from pre-training (text or vision) refined by supervised loss on (S,R,O)-&gt;spatial coordinates",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning of embeddings by backpropagation during supervised regression training; evaluation via probing (cosine similarity correlation with human ratings)",
            "knowledge_representation": "modified continuous embedding vectors whose geometry is altered to bring spatially similar objects closer in embedding space (explicitly measurable via cosine similarity)",
            "performance_metric": "Spearman rho between embedding cosine similarity and human spatial-similarity ratings",
            "performance_result": "Updating embeddings in the Prediction task yielded statistically significant improvements: U-INI_lang Spearman rho ~0.557 (vs GloVe 0.543), U-INI_vis rho ~0.48 (vs VGG-128 ~0.459); concatenated updated embeddings reach ~0.60 (best reported).",
            "success_patterns": "Specialized embeddings show increased correlation with human judgments of spatial similarity and improved ability to generalize spatial predictions to unseen objects; both modalities benefit from specialization, and multimodal fusion benefits the most.",
            "failure_patterns": "Overfitting risk observed for updated linguistic embeddings with excessive epochs (performance can worsen), and updated spaces may reduce compatibility with unseen, fixed pre-trained vectors unless those are also updated.",
            "baseline_comparison": "Un-updated pre-trained embeddings (GloVe/VGG) and random initializations: updated embeddings outperform both (random embeddings show drastic underperformance, U-RND rho ~0.15).",
            "ablation_results": "Comparing updated vs non-updated embeddings serves as an ablation: non-updated (NU) often better for some unseen-word generalization scenarios because updated embedding space may shift; however, for spatial-similarity evaluation the updated embeddings consistently improved Spearman correlation.",
            "key_findings": "Backpropagating spatial prediction supervision into pre-trained embeddings produces distributed representations more specialized for spatial knowledge, improving alignment to human spatial similarity judgments and enabling better textual-only generalization for spatial arrangement prediction.",
            "uuid": "e537.3",
            "source_info": {
                "paper_title": "Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision",
                "publication_date_yy_mm": "2018-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation",
            "rating": 2
        },
        {
            "paper_title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates",
            "rating": 2
        },
        {
            "paper_title": "Grounding Spatial Relations for Human-Robot Interaction",
            "rating": 2
        },
        {
            "paper_title": "Imagined Visual Representations as Multimodal Embeddings",
            "rating": 2
        },
        {
            "paper_title": "The Human Semantic Potential: Spatial Language and Constrained Connectionism",
            "rating": 1
        },
        {
            "paper_title": "Don't Just Listen, Use your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks",
            "rating": 1
        }
    ],
    "cost": 0.01609175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Representations Specialized in Spatial Knowledge: Leveraging Language and Vision</h1>
<p>Guillem Collell<br>Department of Computer Science<br>KU Leuven<br>3001 Heverlee, Belgium<br>gcollell@kuleuven.be</p>
<h2>Marie-Francine Moens</h2>
<p>Department of Computer Science
KU Leuven
3001 Heverlee, Belgium
sien.moens@cs.kuleuven.be</p>
<h4>Abstract</h4>
<p>Spatial understanding is crucial in many realworld problems, yet little progress has been made towards building representations that capture spatial knowledge. Here, we move one step forward in this direction and learn such representations by leveraging a task consisting in predicting continuous 2D spatial arrangements of objects given object-relationship-object instances (e.g., "cat under chair") and a simple neural network model that learns the task from annotated images. We show that the model succeeds in this task and, furthermore, that it is capable of predicting correct spatial arrangements for unseen objects if either CNN features or word embeddings of the objects are provided. The differences between visual and linguistic features are discussed. Next, to evaluate the spatial representations learned in the previous task, we introduce a task and a dataset consisting in a set of crowdsourced human ratings of spatial similarity for object pairs. We find that both CNN (convolutional neural network) features and word embeddings predict human judgments of similarity well and that these vectors can be further specialized in spatial knowledge if we update them when training the model that predicts spatial arrangements of objects. Overall, this paper paves the way towards building distributed spatial representations, contributing to the understanding of spatial expressions in language.</p>
<h2>1 Introduction</h2>
<p>Representing spatial knowledge is instrumental in any task involving text-to-scene conversion such as
robot understanding of natural language commands (Guadarrama et al., 2013; Moratz and Tenbrink, 2006) or a number of robot navigation tasks. Despite recent advances in building specialized representations in domains such as sentiment analysis (Tang et al., 2014), semantic similarity/relatedness (Kiela et al., 2015) or dependency parsing (Bansal et al., 2014), little progress has been made towards building distributed representations (a.k.a. embeddings) specialized in spatial knowledge.</p>
<p>Intuitively, one may reasonably expect that the more attributes two objects share (e.g., size, functionality, etc.), the more likely they are to exhibit similar spatial arrangements with respect to other objects. Leveraging this intuition, we foresee that visual and linguistic representations can be spatially informative about unseen objects as they encode features/attributes of objects (Collell and Moens, 2016). For instance, without having ever seen an "elephant" before, but only a "horse", one would probably devise the "elephant" carrying the "human" than otherwise, just by considering their size attribute. Similarly, one can infer that a "tablet" and a "book" will show similar spatial patterns (usually on a table, in someone's hands, etc.) although they barely show any visual resemblance-yet they are similar in size and functionality. In this paper we systematically study how informative visual and linguistic features-in the form of convolutional neural network (CNN) features and word embeddings-are about the spatial behavior of objects.</p>
<p>An important goal of this work is to learn distributed representations specialized in spatial knowledge. As a vehicle to learn spatial representations,</p>
<p>we leverage the task of predicting the 2D spatial arrangement for two objects under a relationship expressed by either a preposition (e.g., "below" or "on") or a verb (e.g., "riding", "jumping", etc.). For that, we make use of images where both objects are annotated with bounding boxes. For instance, in an image depicting (horse, jumping, fence) we reasonably expect to find the "horse" above the "fence". To learn the task, we employ a feed forward network that represents objects as continuous (spatial) features in an embedding layer and guides the learning with a distance-based supervision on the objects' coordinates. We show that the model fares well in this task and that by informing it with either word embeddings or CNN features it is able to output accurate predictions about unseen objects, e.g., predicting the spatial arrangement of (man, riding, bike) without having ever been exposed to a "bike" before. This result suggests that the semantic and visual knowledge carried by the visual and linguistic features correlates to a certain extent with the spatial properties of words, thus providing predictive power for unseen objects.</p>
<p>To evaluate the quality of the spatial representations learned in the previous task, we introduce a task consisting in a set of 1,016 human ratings of spatial similarity between object pairs. It is thus desirable for spatial representations that "spatially similar" objects (i.e., objects that are arranged spatially similar in most situations and relative to other objects) have similar embeddings. In these ratings we show, first, that both CNN features and word embeddings are good predictors of human judgments, and second, that these vectors can be further specialized in spatial knowledge if we update them by backpropagation when learning the model in the task of predicting spatial arrangements of objects.</p>
<p>The rest of the paper is organized as follows. In Sect. 2 we review related research. In Sect. 3 we describe two spatial tasks and a model. In Sect. 4 we describe our experimental setup. In Sect. 5 we present and discuss our results. Finally, in Sect. 6 we summarize our contributions.</p>
<h2>2 Related Work</h2>
<p>Contrary to earlier rule-based approaches to spatial understanding (Kruijff et al., 2007; Moratz and Ten-
brink, 2006), Malinowski and Fritz (2014) propose a learning-based method that learns the parameters of "spatial templates" (or regions of acceptability of an object under a spatial relation) using a pooling approach. They show improved performance in image retrieval and image annotation (i.e., retrieving sentences given a query image) over previous rule-based systems and methods that rely on handcrafted templates. Contrary to us, they restrict to relationships expressed by explicit spatial prepositions (e.g., "on" or "below") while we also consider actions (e.g., "jumping"). Furthermore, they do not build spatial representations for objects.</p>
<p>Other approaches have shown the value of properly integrating spatial information into a variety of tasks. For example, Shiang et al. (2017) improve over the state-of-the-art object recognition by leveraging previous knowledge of object co-occurrences and relative positions of objects-which they mine from text and the web-in order to rank possible object detections. In a similar fashion, Lin and Parikh (2015) leverage common sense visual knowledge (e.g., object locations and co-occurrences) in two tasks: fill-in-the-blank and visual paraphrasing. They compute the likelihood of a scene to identify the most likely answer to multiple-choice textual scene descriptions. In contrast, we focus solely on spatial information rather than semantic plausibility. Moreover, our primary target is to build (spatial) representations. Alternatively, Elliott and Keller (2013) annotate geometric relationships between objects in images (e.g., they add an "on" link between "man" and "bike" in an image of a "man" "riding" a "bike") to better infer the action present in the image. For instance, if the "man" is next to the bike one can infer that the action "repairing" is more likely than "riding" in this image. Accounting for this extra spatial structure allows them to outperform bag-offeatures methods in an image captioning task. In contrast with those who restrict to a small domain of 10 actions (e.g., "taking a photo", "riding", etc.), our goal is to generalize to any unseen/rare objects and actions by learning from frequent spatial configurations and objects, and critically, leveraging representations of objects. Recent work (Collell et al., 2018) tackles the research question of whether relative spatial arrangements can be predicted equally well from actions (e.g., "riding") than from spatial</p>
<p>prepositions (e.g., "below"), and how to interpret the learned weights of the network. In contrast, our research questions concern spatial representations. Crucially, none of the studies above have considered or attempted to learn distributed spatial representations of objects, nor studied how much spatial knowledge is contained in visual and linguistic representations.</p>
<p>The existence of quantitative, continuous spatial representations of objects has been formerly discussed, yet to our knowledge, not systematically investigated before. For instance, Forbus et al. (1991) conjectured that "there is no purely qualitative, general purpose representation of spatial properties", further emphasizing that the quantitative component is strictly necessary.</p>
<p>It is also worth commenting on early work aimed at enhancing the understanding of natural spatial language such as the $\mathrm{L}_{0}$ project (Feldman et al., 1996). In the context of this project, Regier (1996) proposed a connectionist model that learns to predict a few spatial prepositions ("above", "below", "left", "right", "in", "out", "on", and "off") from low resolution videos containing a limited set of toy objects (circle, square, etc.). In contrast, we consider an unlimited vocabulary of real-world objects, and we do not restrict to spatial prepositions but we include actions, as well. Hence, Regier's (1996) setting does not seem plausible to deal with actions given that, in contrast to the spatial prepositions that they use, which are mutually exclusive (an object cannot be "above" and simultaneously "below" another object), actions are not. In particular, actions exhibit large spatial overlap and, therefore, attempt to predict thousands of different actions from the relative locations of the objects seems infeasible. Additionally, Regier's (1996) architecture does not allow to meaningfully extract representations of objects from the visual input-which yields rather visual features.</p>
<p>Here, we propose an ad hoc setting for both, learning and evaluating spatial representations. In particular, instead of learning to predict spatial relations from visual input as in Regier's (1996) work, we learn the reverse direction, i.e., to map the relation (and two objects) to their visual spatial arrangement. By backpropagating the embeddings of the objects while learning the task, we enable learning spatial representations. As a core finding, we show
in an ad hoc task, namely our collected human ratings of spatial similarity, that the learned features are more specialized in spatial knowledge than the CNN features and word embeddings that were used to initialize the parameters of the embeddings.</p>
<h2>3 Tasks and Model</h2>
<p>Here, we first describe the Prediction task and model that we use to learn the spatial representations. We subsequently present the spatial Similarity task which is employed to evaluate the quality of the learned representations.</p>
<h3>3.1 Prediction Task</h3>
<p>To evaluate the ability of a model or embeddings to learn spatial knowledge, we employ the task of predicting the spatial location of an Object (" $O$ ") relative to a Subject (" $S$ ") under a Relationship (" $R$ "). Let $O^{c}=\left(O_{x}^{c}, O_{y}^{c}\right)$ denote the coordinates of the center (" $c$ ") of the Object's bounding box, where $O_{x}^{c} \in \mathbb{R}$ and $O_{y}^{c} \in \mathbb{R}$ are its $x$ and $y$ components. Let $O^{b}=\left(O_{x}^{b}, O_{y}^{b}\right)$ be one half of the vertical $\left(O_{y}^{b} \in \mathbb{R}\right)$ and horizontal $\left(O_{x}^{b} \in \mathbb{R}\right)$ sizes of the Object's bounding box (" $b$ "). A similar notation applies to the Subject (i.e., $S^{c}$ and $S^{b}$ ), and we denote model predictions with a hat $\widehat{O^{c}}, \widehat{O^{b}}$. The task is to learn a mapping from the structured textual input (Subject, Relation, Object)—abbreviated by ( $S, R$, $O$ )-to the output consisting of the Object's center coordinates $O^{c}$ and its size $O^{b}$ (see Fig. 1).</p>
<p>We notice that a "Subject" is not necessarily a syntactic subject but simply a convenient notation to accommodate the case where the Relationship $(R)$ is an action (e.g., "riding" or "wearing"), while when $R$ is a spatial preposition (e.g., "below" or "on") the Subject simply denotes the referent object. Similarly, the Object is not necessarily a direct object. ${ }^{1}$</p>
<h3>3.2 Regression Model</h3>
<p>Following the task above (Sect. 3.1), we consider a model (Fig. 1) that takes a triplet of words $(S, R, O)$ as input and maps their one-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the model (right) and the image pre-processing setting (left).
hot ${ }^{2}$ vectors $w_{S}, w_{R}, w_{O}$ to $d$-dimensional dense vectors $w_{S} W_{S}, w_{R} W_{R}, w_{O} W_{O}$ via dot product with their respective embedding matrices $W_{S} \in$ $\mathbb{R}^{d \times\left|V_{S}\right|}, W_{R} \in \mathbb{R}^{d \times\left|V_{R}\right|}, W_{O} \in \mathbb{R}^{d \times\left|V_{O}\right|}$, where $\left|V_{S}\right|,\left|V_{R}\right|,\left|V_{O}\right|$ are the vocabulary sizes. The embedding layer models our intuition that spatial properties of objects can be, to a certain extent, encoded with a vector of continuous features. In this work we test two types of embeddings, visual and linguistic. The next layer simply concatenates the three embeddings together with the Subject's size $S^{b}$ and Subject center $S^{c}$. The inclusion of the Subject's size is aimed at providing a reference size to the model in order to predict the size of the Object $O^{b} .{ }^{3}$ The resulting concatenated vector $\left[w_{S} W_{S}, w_{R} W_{R}, w_{O} W_{O}, S^{c}, S^{b}\right]$ is then fed into a hidden layer(s) which acts as a composition function for the triplet $(S, R, O)$ :</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$$
z=f\left(W_{h}\left[w_{S} W_{S}, w_{R} W_{R}, w_{O} W_{O}, S^{c}, S^{b}\right]+b_{h}\right)
$$</p>
<p>where $f(\cdot)$ is the non-linearity and $W_{h}$ and $b_{h}$ the parameters of the layer. These "composition layers" allow to distinguish between e.g., (man, walks, horse) which is spatially distinct from (man, rides, horse). We find that adding more layers generally improves performance, so the output $z$ above can simply be composed with more layers, i.e., $f\left(W_{h_{2}} z+b_{h_{2}}\right)$. Finally, a linear output layer tries to match the ground truth targets $y=\left(O^{c}, O^{b}\right)$ using a mean squared error (MSE) loss function:</p>
<p>$$
\operatorname{Loss}(y, \hat{y})=|\hat{y}-y|^{2}
$$</p>
<p>where $\hat{y}=\left(\widehat{O^{c}}, \widehat{O^{b}}\right)$ is the model prediction and $|\cdot|$ denotes the Euclidean norm. Critically, unlike CNNs, the model does not make use of the pixels (which are discarded during the image preprocessing (Fig. 1 and Sect. 3.2.1)), but learns exclusively from image coordinates, yielding a simpler model focused solely on spatial information.</p>
<h3>3.2.1 Image Pre-Processing</h3>
<p>We perform the following pre-processing steps to the images before feeding them to the model.
(i) Normalize the image coordinates by the number of pixels of each axis (vertical and horizontal). This step guarantees that coordinates are independent of the resolution of the image and always lie within the $[0,1] \times[0,1]$ square, i.e., $S^{c}, O^{c} \in[0,1]^{2}$.
(ii) Mirror the image (when necessary). We notice that the distinction between right and left is arbitrary in images since a mirrored image completely preserves its spatial meaning. For instance, a "man" "feeding" an "elephant" can be arbitrarily at either side of the "elephant", while a "man" "riding" an "elephant" cannot be either below or above the "elephant". This left/right arbitrariness has also been acknowledged in prior work (Singhal et al., 2003). Thus, to enable a more meaningful learning, we mirror the image when (and only when) the Object is at the left-hand side of the Subject. ${ }^{4}$ The choice of leaving the Object always to the right-hand side is arbitrary and does not entail a loss of generality, i.e., we can consider left/right symmetrically reflected predictions as equiprobable. Mirroring provides thus a more realistic performance evaluation in the Prediction task and enables learning representations independent of the right/left distinction which is irrelevant for the spatial semantics.</p>
<h3>3.3 Spatial Similarity Task</h3>
<p>To evaluate how well our embeddings match human mental representations of spatial knowledge about objects, we collect ratings for 1,016 word pairs $\left(w_{1}, w_{2}\right)$ asking annotators to rate them by their spatial similarity. That is, objects that exhibit similar locations in most situations and are placed similarly relative to other objects would receive a high score, and lower otherwise. For example (cap, sunglasses) would receive a high score as they are usually at the top of the human body, while following a similar logic, (cap, shoes) would receive a lower score. Our collected ratings establish the spatial counterpart to other existing similarity ratings such as semantic similarity (Silberer and Lapata, 2014), vi-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>sual similarity (Silberer and Lapata, 2014) or general relatedness (Agirre et al., 2009). A few exemplars of ratings are shown in Tab. 1. Following standard practices (Pennington et al., 2014), we compute the prediction of similarity between two embeddings $s_{w_{1}}$ and $s_{w_{2}}$ (representing words $w_{1}$ and $w_{2}$ ) with their cosine similarity:</p>
<p>$$
\cos \left(s_{w_{1}}, s_{w_{2}}\right)=\frac{s_{w_{1}} s_{w_{2}}}{\left|s_{w_{1}}\right|\left|s_{w_{2}}\right|}
$$</p>
<p>We notice that this spatial Similarity task does not involve learning and its main purpose is to evaluate the quality of the representations learned in the Prediction task (Sect. 3.1) and the spatial informativeness of visual and linguistic features.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Word pair</th>
<th style="text-align: center;">Rating</th>
<th style="text-align: left;">Word pair</th>
<th style="text-align: center;">Rating</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">(snowboard, feet)</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: left;">(horns, backpack)</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: left;">(ears, eye)</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: left;">(baby, bag)</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">(cockpit, table)</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: left;">(hair, laptop)</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: left;">(cap, hair)</td>
<td style="text-align: center;">9</td>
<td style="text-align: left;">(earring, racket)</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">(frisbee, food)</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: left;">(ears, hat)</td>
<td style="text-align: center;">5.6</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of our collected similarity ratings.</p>
<h2>4 Experimental Setup</h2>
<p>In this section we describe the experimental settings employed in the tasks and the model.</p>
<h3>4.1 Visual Genome Data Set</h3>
<p>We obtain our annotated data from Visual Genome (Krishna et al., 2017). This dataset contains 108,077 images and over 1.5 M human-annotated object-relationship-object instances $(S, R, O)$ with their corresponding boxes for the Object and Subject. We keep only those examples for which we have embeddings available (see Sect. 4.3). This yields $\sim 1.1 \mathrm{M}$ instances of the form $(S, R, O), 7,812$ unique image objects and 2,214 unique Relationships $(R)$ for our linguistic embeddings; and $\sim 920 \mathrm{~K}(S, R, O)$ instances, 4,496 unique image objects and 1,831 unique Relationships for our visual embeddings. We notice that visual representations do not exist for Relationships $R$ (i.e., either prepositions or verbs) and therefore we only require visual embeddings for the pair $(S, O)$ instead of the complete triplet $(S$, $R, O)$ required in language. Notice that since we</p>
<p>do not restrict to any particular domain (e.g., furniture or landscapes) the combinations $(S, R, O)$ are markedly sparse, which makes learning our Prediction task especially challenging.</p>
<h3>4.2 Evaluation Sets in the Prediction Task</h3>
<p>In the Prediction task, we consider the following subsets of Visual Genome (Sect. 4.1) for evaluation purposes:
(i) Original set: a test split from the original data which contains instances unseen at training time. That is, the test combinations $(S, R, O)$ might have been seen at training time, yet in different instances (e.g., in different images). This set contains a large number of noisy combinations such as (people, walk, funny) or (metal, white, chandelier).
(ii) Unseen Words set: We randomly select a list of 25 objects (e.g., "wheel", "camera", "elephant", etc.) among the 100 most frequent objects in Visual Genome. ${ }^{5}$ We choose them among the most frequent ones in order to avoid meaningless objects such as "gate 2", "number 40 " or " $2: 10 \mathrm{pm}$ " which are not infrequent in Visual Genome. We then take all instances of combinations that contain any of these words, yielding $\sim 123 \mathrm{~K}$ instances. For example, since "cap" is in our list, (girl, wears, cap) is included in this set. When we enforce "unseen" conditions, we remove all these instances from the training set, using them only for testing.</p>
<h3>4.3 Visual and Linguistic Features</h3>
<p>As our linguistic representations, we employ 300dimensional GloVe vectors (Pennington et al., 2014) trained on the Common Crawl corpus with 840Btokens and a 2.2 M words vocabulary. ${ }^{6}$</p>
<p>We use the publicly available visual representations from Collell et al. (2017). ${ }^{7}$ They extract 128dimensional visual features with the forward pass of a VGG-128 (Visual Geometry Group) CNN model (Chatfield et al., 2014) pre-trained in ImageNet (Russakovsky et al., 2015). The representation of a word is the averaged feature vector (centroid) of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>all images in ImageNet for this concept. They only keep words with at least 50 images available. We notice that although we employ visual features from an external source (ImageNet), these could be alternatively obtained in the Visual Genome dataalthough ImageNet generally provides a larger number of images per concept.</p>
<h3>4.4 Method Comparison</h3>
<p>We consider two types of models, those that update the parameters of the embeddings ( $\boldsymbol{U} \sim$ "Update") and those that keep them fixed ( $\boldsymbol{N} \boldsymbol{U} \sim$ "No Update") when learning the Prediction task. For each type ( $U$ and $N U$ ) we consider two conditions, embeddings initialized with pre-trained vectors (INI) and random embeddings ( $\boldsymbol{R N D}$ ) randomly drawn from a component-wise normal distribution of mean and standard deviation equal to those of the original embeddings. For example, $U-R N D$ corresponds to a model with updated, random embeddings. For the INI methods we also add a subindex indicating whether the embeddings are visual (vis) or linguistic (lang), as described in Sect. 4.3. ${ }^{8}$ For the $N U$ type we additionally consider one-hot embeddings ( $\boldsymbol{I H}$ ). We also include a control method (rand-pred) that outputs random uniform predictions.</p>
<h3>4.5 Implementation Details and Validation</h3>
<p>To validate results in our Prediction task we employ a 10 -fold cross-validation (CV) scheme. That is, we split the data into 10 parts and employ $90 \%$ of the data for training and $10 \%$ for testing. This yields 10 embeddings (for each " $U$ " method), which are then evaluated in our Similarity task. In both tasks, we report results averaged across the 10 folds.</p>
<p>Model hyperparameters are first selected by cross-validation in 10 initial splits and results are reported in 10 new splits. All models employ a learning rate of 0.0001 and are trained for 10 epochs by backpropagation with the RMSprop optimizer. The dimensionality of the embeddings is the original one, i.e., $d=300$ for GloVe and $d=128$ for VGG128 (Sect. 4.3), which is preserved for the random-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>embedding methods $R N D$ (Sect. 4.4). Models employ 2 hidden layers with 100 Rectified Linear Units (ReLu), followed by an output layer with a linear activation. Early stopping is employed as a regularizer. We implement our models with Keras deep learning framework in Python 2.7 (Chollet and others, 2015).</p>
<h3>4.6 Spatial Similarity Task</h3>
<p>To build the word pairs, we randomly select a list of objects from Visual Genome and from these we randomly chose 1,016 non-repeated word pairs $\left(w_{1}, w_{2}\right)$. Ratings are collected with the Crowdflower ${ }^{9}$ platform and correspond to averages of at least 5 reliable annotators ${ }^{10}$ that provided ratings in a discrete scale from 1 to 10 . The median similarity rating is 3.3 and the mean variance between annotators per word pair is $\sim 1.2$.</p>
<h3>4.7 Evaluation Metrics</h3>
<h3>4.7.1 Prediction Task</h3>
<p>We evaluate model predictions with the following metrics.
(I) Regression metrics.
(i) Mean Squared Error (MSE) between the predicted $\hat{y}=\left(\widehat{O^{c}}, \widehat{O^{b}}\right)$ and the true $y=\left(O^{c}, O^{b}\right) \mathrm{Ob}$ ject center coordinates and Object size. Notice that since $O^{c}, O^{b}$ are within $[0,1]^{2}$, the MSE is easily interpretable, ranging between 0 and 1.
(ii) Pearson Correlation ( $\mathbf{r}$ ) between the predicted $\widehat{O^{c}}$ and the true $O^{c}$ Object center coordinates. We consider the vertical $\left(\mathbf{r}<em x="x">{y}\right)$ and horizontal $\left(\mathbf{r}</em>$ ).
(iii) Coefficient of Determination $\left(\mathbf{R}^{2}\right)$ of the predictions $\hat{y}=\left(\widehat{O^{c}}, \widehat{O^{b}}\right)$ and the target $y=\left(O^{c}, O^{b}\right)$. $\mathrm{R}^{2}$ is employed to evaluate goodness of fit of a regression model and is related to the percentage of variance of the target explained by the predictions. The best possible score is 1 and it can be arbitrarily negative for bad predictions. A model that outputs either random or constant predictions would obtain scores close to 0 and exactly 0 respectively.}\right)$ components separately (i.e., $O_{x}^{c}$ and $O_{y}^{c</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(II) Classification. Additionally, given the semantic distinction between the vertical and horizontal axis noted above (Sect. 3.2.1), we consider the classification problem of predicting above/below relative locations. That is, if the predicted $y$-coordinate for the Object center $\widehat{O}<em y="y">{y}^{c}$ falls below the $y$-coordinate of the Subject center $S</em>}^{c}$ and the actual Object center $O_{y}^{c}$ is below the Subject center $S_{y}^{c}$, we count it as a correct prediction, and as incorrect otherwise. Likewise for above predictions. We compute both macro-averaged ${ }^{11}$ accuracy $\left(\mathbf{a c c<em y="y">{y}\right)$ and macroaveraged $\mathrm{F} 1\left(\mathbf{F 1}</em>\right)$ metrics.
(III) Intersection over Union (IoU). We consider the bounding box overlap (IoU) from the VOC detection task (Everingham et al., 2015): $\mathrm{IoU}=$ $\operatorname{area}\left(\widehat{B_{O}} \cap B_{O}\right) / \operatorname{area}\left(\widehat{B_{O}} \cup B_{O}\right)$ where $\widehat{B_{O}}$ and $B_{O}$ are predicted and ground truth Object boxes respectively. A prediction is counted as correct if the IoU is larger than $50 \%$. Crucially, we notice that our setting and results are not comparable to object detection as we employ text instead of images as input and thus we cannot leverage the pixels to locate the Object, unlike in detection.</p>
<h3>4.7.2 Similarity Task</h3>
<p>Following standard practices (Pennington et al., 2014), the performance of the predictions of (cosine) similarity from the embeddings (described in Sect. 3.3) is evaluated with the Spearman correlation $\rho$ against the crowdsourced human ratings.</p>
<h2>5 Results and Discussion</h2>
<p>We consider the notation of the methods from Sect. 4.4 and the evaluation subsets described in Sect. 4.2 for the Prediction task. To test statistical significance we employ a Friedman rank test and post hoc Nemeny tests on the results of the 10 folds.</p>
<h3>5.1 Prediction Task</h3>
<p>Table 2 shows that the $I N I$ and $R N D^{12}$ methods perform similarly in the Original test set, arguably be-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>cause a large part of the learning takes place in the parameters of the layers subsequent to the embedding layer. However, in the next section we show that this is no longer the case when unseen words are present. We also observe that the one-hot embeddings $N U-1 H$ perform slightly better than the rest of methods when no unseen words are present (Tab. 2 and Tab. 3 right).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSE</th>
<th style="text-align: center;">$\mathrm{R}^{2}$</th>
<th style="text-align: center;">$\mathrm{acc}_{y}$</th>
<th style="text-align: center;">$\mathrm{Fl}_{y}$</th>
<th style="text-align: center;">$\mathrm{r}_{s}$</th>
<th style="text-align: center;">$\mathrm{r}_{y}$</th>
<th style="text-align: center;">IoU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$U-I N I_{\text {lang }}$</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.283</td>
</tr>
<tr>
<td style="text-align: center;">$U-R N D_{\text {lang }}$</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.279</td>
</tr>
<tr>
<td style="text-align: center;">$N U-I N I_{\text {lang }}$</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.275</td>
</tr>
<tr>
<td style="text-align: center;">$N U-R N D_{\text {lang }}$</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.268</td>
</tr>
<tr>
<td style="text-align: center;">$N U-1 H$</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.297</td>
</tr>
<tr>
<td style="text-align: center;">rand-pred</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">$-27.61$</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.010</td>
</tr>
<tr>
<td style="text-align: center;">$U-I N I_{v i s}$</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.266</td>
</tr>
<tr>
<td style="text-align: center;">$U-R N D_{v i s}$</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.244</td>
</tr>
<tr>
<td style="text-align: center;">$N U-I N I_{v i s}$</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.246</td>
</tr>
<tr>
<td style="text-align: center;">$N U-R N D_{v i s}$</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.237</td>
</tr>
<tr>
<td style="text-align: center;">$N U-1 H$</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.308</td>
</tr>
<tr>
<td style="text-align: center;">rand-pred</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">$-27.51$</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.010</td>
</tr>
</tbody>
</table>
<p>Table 2: Results in the Original test set (Sect. 4.2). Boldface indicates best performance within the corresponding block of methods (lang above, and vis below).</p>
<p>It is also worth noting that the results of the Prediction task are, in fact, conservative. First, the Original test data contains a considerable number of meaningless (e.g., (giraffe, a, animal)), and irrelevant combinations (e.g., (clock, has, numbers) or (sticker, identifies, apple)). Second, even when only meaningful examples are considered, we are inevitably penalizing for plausible predictions. For instance, in (man, watching, man) we expect both men to be reasonably separated on the $x$-axis yet the one with the highest $y$ coordinate is generally not predictable as it depends on their height and their distance to the camera. This yields above/below classification performance and correlations. Regardless, all methods (except rand-pred) exhibit reasonably high performance in all measures.</p>
<h3>5.1.1 Evaluation on Unseen Words</h3>
<p>Table 3 evidences that both visual and linguistic embeddings ( $I N I_{v i s}$ and $I N I_{\text {lang }}$ ) significantly outperform their random-embedding counterparts $R N D$ by a large margin when unseen words are present. The improvement occurs for both, updated $(U)$ and non-updated $(N U)$ embeddings-although it is expected that the updated methods perform slightly
worse than the non-updated ones since the original embeddings will have "moved" during training and therefore an unseen embedding (which has not been updated) might no longer be close to other semantically similar vectors in the updated space.</p>
<p>Besides statistical significance, it is worth mentioning that the $I N I$ methods consistently outperformed both their $R N D$ counterparts and $N U-1 H$ in each of the 10 folds (not shown here) by a steadily large margin. In fact, results are markedly stable across folds, in part due to the large size of the training and test sets ( $&gt;0.9 \mathrm{M}$ and $&gt;120 \mathrm{~K}$ examples respectively). Additionally, to ensure that "unseen" results are not dependent on our particular list of objects, we repeated the experiment with two additional lists of randomly selected objects, obtaining very similar results.</p>
<p>Remarkably, the $I N I$ methods experience only a small performance drop under unseen conditions (Tab. 3, left) compared to when we allow them to train with these words (Tab. 3, right), and this difference might be partially attributed to the reduction of the training data under "unseen" conditions, where at least $10 \%$ of the training data are left out.</p>
<p>Altogether, these results on unseen words show that semantic and visual similarities between concepts, as encoded by word and visual embeddings, can be leveraged by the model in order to predict spatial knowledge about unseen words. ${ }^{13}$</p>
<h3>5.1.2 Qualitative Insight</h3>
<p>Visual inspection of model predictions is instructive in order to gain insight on the spatial informativeness of visual and linguistic representations on unseen words. Figure 2 shows heat maps of low (black) and high (white) probability regions for the objects. The "heat" for the Object is assumed to be normally distributed with mean $(\mu)$ equal to the predicted Object center $\widetilde{O}^{c}$ and standard deviation $(\sigma)$ equal to the predicted Object size $\widetilde{O}^{h}$ (assuming independence of the $x$ and $y$ components, which yields the product of two Gaussians, one for each component $x$ and $y$ ). The "heat" for the Subject is computed similarly, although with $\mu$ and $\sigma$ equal to the</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unseen words condition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Seen words condition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">$\mathrm{R}^{2}$</td>
<td style="text-align: center;">$\mathrm{acc}_{y}$</td>
<td style="text-align: center;">$\mathrm{F1}_{y}$</td>
<td style="text-align: center;">$\mathrm{r}_{x}$</td>
<td style="text-align: center;">$\mathrm{r}_{y}$</td>
<td style="text-align: center;">IoU</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">$\mathrm{R}^{2}$</td>
<td style="text-align: center;">$\mathrm{acc}_{y}$</td>
<td style="text-align: center;">$\mathrm{F1}_{y}$</td>
<td style="text-align: center;">$\mathrm{r}_{x}$</td>
<td style="text-align: center;">$\mathrm{r}_{y}$</td>
</tr>
<tr>
<td style="text-align: center;">$U-I N I_{\text {lang }}$</td>
<td style="text-align: center;">$0.011^{<em> </em>}$</td>
<td style="text-align: center;">$0.584^{<em> </em>}$</td>
<td style="text-align: center;">$0.712^{<em> </em>}$</td>
<td style="text-align: center;">$0.710^{<em> </em>}$</td>
<td style="text-align: center;">$0.877^{<em> </em>}$</td>
<td style="text-align: center;">$0.770^{*}$</td>
<td style="text-align: center;">$0.131^{<em> </em>}$</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">$0.736^{*}$</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.876</td>
</tr>
<tr>
<td style="text-align: center;">$U-R N D_{\text {lang }}$</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.874</td>
</tr>
<tr>
<td style="text-align: center;">$N U-I N I_{\text {lang }}$</td>
<td style="text-align: center;">$0.009^{<em> </em>}$</td>
<td style="text-align: center;">$0.663^{<em> </em>}$</td>
<td style="text-align: center;">$0.770^{<em> </em>}$</td>
<td style="text-align: center;">$0.770^{<em> </em>}$</td>
<td style="text-align: center;">$0.888^{<em> </em>}$</td>
<td style="text-align: center;">$0.835^{<em> </em>}$</td>
<td style="text-align: center;">$0.164^{<em> </em>}$</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">$0.734^{*}$</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.875</td>
</tr>
<tr>
<td style="text-align: center;">$N U-R N D_{\text {lang }}$</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.871</td>
</tr>
<tr>
<td style="text-align: center;">$N U-1 H$</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.098</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.814</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.877</td>
</tr>
<tr>
<td style="text-align: center;">rand-pred</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">$-38.32$</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">$-38.42$</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">$-0.002$</td>
<td style="text-align: center;">$-0.001$</td>
</tr>
<tr>
<td style="text-align: center;">$U-I N I_{\text {vis }}$</td>
<td style="text-align: center;">$0.010^{<em> </em>}$</td>
<td style="text-align: center;">$0.599^{<em> </em>}$</td>
<td style="text-align: center;">$0.775^{<em> </em>}$</td>
<td style="text-align: center;">$0.774^{<em> </em>}$</td>
<td style="text-align: center;">$0.887^{<em> </em>}$</td>
<td style="text-align: center;">$0.801^{<em> </em>}$</td>
<td style="text-align: center;">$0.123^{<em> </em>}$</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.874</td>
</tr>
<tr>
<td style="text-align: center;">$U-R N D_{\text {vis }}$</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.864</td>
</tr>
<tr>
<td style="text-align: center;">$N U-I N I_{\text {vis }}$</td>
<td style="text-align: center;">$0.010^{<em> </em>}$</td>
<td style="text-align: center;">$0.602^{<em> </em>}$</td>
<td style="text-align: center;">$0.777^{<em> </em>}$</td>
<td style="text-align: center;">$0.775^{<em> </em>}$</td>
<td style="text-align: center;">$0.887^{<em> </em>}$</td>
<td style="text-align: center;">$0.803^{<em> </em>}$</td>
<td style="text-align: center;">$0.123^{<em> </em>}$</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.868</td>
</tr>
<tr>
<td style="text-align: center;">$N U-R N D_{\text {vis }}$</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.706</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.862</td>
</tr>
<tr>
<td style="text-align: center;">$N U-1 H$</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.885</td>
</tr>
<tr>
<td style="text-align: center;">rand-pred</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">$-40.34$</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">$-0.001$</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">$-40.37$</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">$-0.002$</td>
<td style="text-align: center;">$-0.001$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results in the Unseen Words set (Sect. 4.2). Left table: results of enforcing "unseen" conditions, i.e., leaving out all words of the Unseen Words set from our training data. Right table: the models are evaluated in the same set but we allow them to train with the words from this set. Asterisks ( ${ }^{<em>}$ ) in an $I N I$ method indicate significantly better performance ( $\mathrm{p}&lt;0.05$ ) than its RND counterpart (i.e., $U-I N I_{\text {emb_type }}$ is compared against $U-R N D$, and $N U$ $I N I_{\text {emb_type }}$ against $N U-R N D$ ). Diamonds $\left({ }^{</em>}\right)$ indicate significantly better performance than $N U-1 H$.
actual Subject center $S^{v}$ and size $S^{b}$, respectively.
The $I N I$ methods in Figure 2 illustrate the contribution of the embeddings to the spatial understanding of unseen objects. In general, both visual and linguistic embeddings enabled predicting meaningful spatial arrangements, yet for the sake of space we have only included three examples where: vis performs better than lang (third column), where lang performs better than vis (second column), and where both perform well (first column). We notice that the embeddings enable the model to infer that e.g., since "camera" (unseen) is similar to "camcorder" (seen at training time), both must behave spatially similarly. Likewise, the embeddings enable predicting correctly the relative sizes of unseen objects. We also observe that when the embeddings are not informative enough, model predictions become less accurate. For instance, in $N U-I N I_{\text {lang }}$, some unrelated objects (e.g., "ipod") have embeddings similar to "apple", and analogously for $N U$ $I N I_{\text {vis }}$ and "tail". We finally notice that predictions on unseen objects using random embeddings (RND) are markedly bad.</p>
<h3>5.2 Spatial Similarity Task</h3>
<p>Table 4 shows the results of evaluating the embeddings, including those learned in the Prediction task, against the human ratings of spatial similarity (Sect. 3.3). Hence, only the "updated" methods $(U)$ are shown and we additionally in-
clude the concatenation of visual and linguistic embeddings $C O N C_{\text {GloVe }+V G G-128}$ and the concatenation of the corresponding updated embeddings $\operatorname{CONC}<em _lang="{lang" _text="\text">{U-I N I</em>$.}}+U-I N I_{\text {vis }}</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LANG</th>
<th style="text-align: center;">V\&amp;L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GloVe</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.535</td>
</tr>
<tr>
<td style="text-align: left;">VGG-128</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.459</td>
</tr>
<tr>
<td style="text-align: left;">$\operatorname{CONC}_{\text {GloVe }+V G G-128}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.582</td>
</tr>
<tr>
<td style="text-align: left;">$U-I N I_{\text {lang }}$</td>
<td style="text-align: center;">$0.557 \pm 0.0015^{*}$</td>
<td style="text-align: center;">$0.558 \pm 0.002^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">$U-I N I_{\text {vis }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.48 \pm 0.0012^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">$\operatorname{CONC}<em _lang="{lang" _text="\text">{U-I N I</em>$}}+U-I N I_{\text {vis }}</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.6 \pm 0.0015^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">$U-R N D$</td>
<td style="text-align: center;">$0.15 \pm 0.0075$</td>
<td style="text-align: center;">$0.174 \pm 0.0078$</td>
</tr>
<tr>
<td style="text-align: left;"># word pairs</td>
<td style="text-align: center;">1016</td>
<td style="text-align: center;">839</td>
</tr>
</tbody>
</table>
<p>Table 4: Spearman correlations between model predictions and human ratings. Standard errors across folds are shown for the methods that involve learning (second block). Columns correspond to the word pairs for which both embeddings (vis and lang) are available (V\&amp;L) and those for which only the linguistic embeddings are available (LANG). Asterisk ( ${ }^{*}$ ) indicates significant improvement ( $\mathrm{p}&lt;0.05$ ) of a $U-I N I$ method of the second block $\left(U-I N I_{\text {vis }}\right.$ and $\left.U-I N I_{\text {lang }}\right)$ over its corresponding untrained embedding (i.e., VGG-128 or GloVe respectively) from the first block.</p>
<p>The first thing to notice in Tab. 4 is that both visual and linguistic embeddings show good correlations with human spatial ratings ( $\rho&gt;0.45$ and $\rho&gt;0.53$ respectively), suggesting that visual and linguistic features carry significant knowledge about</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Heat maps of predictions of the $N U-I N I_{\text {lang }}$, $U-I N I_{v i s}$ and $N U-R N D$ methods. The unseen Objects are underlined (top of the image) and their corresponding four (cosine-based) nearest neighbors are shown below with their respective cosine similarities.
spatial properties of objects. In particular, linguistic features seem to be more spatially informative than visual features.</p>
<p>Crucially, we observe a significant improvement of the $U-I N I_{v i s}$ over the original visual vectors ( $V G G-128)(\mathrm{p}&lt;0.05)$ and of the $U-I N I_{\text {lang }}$ over the original linguistic embeddings (GloVe) ( $\mathrm{p}&lt;0.05$ ), which evidence the effectiveness of training in the Prediction task as a method to further specialize embeddings in spatial knowledge. It is worth mentioning that these improvements are consistent in each of the 10 folds (not shown here) and markedly stable (see standard errors in Tab. 4).</p>
<p>We additionally observe that the concatenation of visual and linguistic embeddings $C O N C_{\text {GloVe }+V G G-128}$ outperforms all unimodal embeddings by a margin, suggesting that the fusion of visual and linguistic features provides a more complete description of spatial properties of objects. Remarkably, the improvement is even larger for the concatenation of the embeddings updated during training $C O N C_{U-I N I_{\text {lang }}+U-I N I_{v i s}}$, which obtains the highest performance overall.</p>
<p>Figure 3 illustrates the progressive specialization of our embeddings in spatial knowledge as we train them in our Prediction task. We notice that all embeddings improve, yet $U-I N I_{\text {lang }}$ seem to worsen their quality when we over-train them-likely due to overfitting, as we do not use any regularizer besides early stopping. We also observe that although the random embeddings $(R N D)$ are the ones that benefit the most from the training, their performance is still far from that of $U-I N I_{v i s}$ and $U-I N I_{\text {lang }}$, suggesting the importance of visual and linguistic features to represent spatial properties of objects.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Correlation between human ratings and embedding cosine similarities at each number of epochs.</p>
<p>It is relevant to mention that in a pilot study we crowdsourced a different list of 1,016 object pairs where we employed 3 instead of 5 annotators per row. Results stayed remarkably consistent with those presented here-the improvement for the updated embeddings was in fact even larger.</p>
<p>Limitations of the current approach and future work In order to keep the design clean in this first paper on distributed spatial representations we employ a fully supervised setup. However, we notice that methods to automatically parse images (e.g., object detectors) and sentences are available.</p>
<p>A second limitation is the 2D simplification of the actual 3D world that our approach and the current spatial literature generally employs. Even though methods that infer 3D structure from 2D images exist, this is beyond the scope of this paper which shows that a 2D treatment already enhances the learned spatial representations. It is also worth noting that the proposed regression setting trivially generalizes to 3D if suitable data are available, and in fact, we believe that the learned representations could further benefit from such extension.</p>
<h2>6 Conclusions</h2>
<p>Altogether, this paper sheds light on the problem of learning distributed spatial representations of objects. To learn spatial representations we have leveraged the task of predicting the continuous 2D relative spatial arrangement of two objects under a relationship, and a simple embedding-based neural model that learns this task from annotated images. In the same Prediction task we have shown that both word embeddings and CNN features endow the model with great predictive power when is presented with unseen objects. Next, in order to assess the spatial content of distributed representations, we have collected a set of 1,016 object pairs rated by spatial similarity. We have shown that both word embeddings and CNN features are good predictors of human spatial judgments. More specifically, we find that word embeddings ( $\rho=0.535$ ) tend to perform better than visual features ( $\rho \sim 0.46$ ), and that their combination ( $\rho \sim 0.6$ ) outperforms both modalities separately. Crucially, in the same ratings we have shown that by training the embeddings in the Prediction task we can further specialize them in spatial knowledge, making them more akin to human spatial judgments. To benchmark the task, we make the Similarity dataset and our trained spatial representations publicly available. ${ }^{14}$</p>
<p>Lastly, this paper contributes to the automatic un-</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>derstanding of spatial expressions in language. The lack of common sense knowledge has been recurrently argued as one of the main reasons why machines fail at exhibiting more "human-like" behavior in tasks (Lin and Parikh, 2015). Here, we have provided a means of compressing and encoding such common sense spatial knowledge about objects into distributed representations, further showing that these specialized representations correlate well with human judgments. In future work, we will also explore the application of our trained spatial embeddings in extrinsic tasks in which representing spatial knowledge is essential such as robot navigation or robot understanding of natural language commands (Guadarrama et al., 2013; Moratz and Tenbrink, 2006). Robot navigation tasks such as assisting people with special needs (blind, elderly, etc.) are in fact becoming increasingly necessary (Ye et al., 2015) and require great understanding of spatial language and spatial connotations of objects.</p>
<h2>Acknowledgments</h2>
<p>This work has been supported by the CHIST-ERA EU project MUSTER ${ }^{15}$ and by the KU Leuven grant RUN/15/005. We additionally thank our anonymous reviewers for their insightful comments which helped to improve the overall quality of the paper, and the action editors for their helpful assistance.</p>
<h2>References</h2>
<p>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Paşca, and Aitor Soroa. 2009. A Study on Similarity and Relatedness Using Distributional and WordNet-Based Approaches. In NAACL, pages 19-27. ACL.
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. In ACL, pages 809-815.
Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Return of the Devil in the Details: Delving Deep into Convolutional Nets. In $B M V C$.
François Chollet et al. 2015. Keras. https:// github.com/fchollet/keras.
Guillem Collell and Marie-Francine Moens. 2016. Is an Image Worth More than a Thousand Words? On the Fine-Grain Semantic Differences between Visual and</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Linguistic Representations. In COLING, pages 28072817. ACL.</p>
<p>Guillem Collell, Teddy Zhang, and Marie-Francine Moens. 2017. Imagined Visual Representations as Multimodal Embeddings. In AAAI, pages 4378-4384. AAAI.</p>
<p>Guillem Collell, Luc Van Gool, and Marie-Francine Moens. 2018. Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates. In AAAI. AAAI.
Desmond Elliott and Frank Keller. 2013. Image Description Using Visual Dependency Representations. In EMNLP, volume 13, pages 1292-1302.
Mark Everingham, S.M. Ali Eslami, Luc Van Gool, Christopher K.I. Williams, John Winn, and Andrew Zisserman. 2015. The PASCAL Visual Object Classes Challenge: A Retrospective. International Journal of Computer Vision, 111(1):98-136.
Jerome Feldman, George Lakoff, David Bailey, Srini Narayanan, Terry Regier, and Andreas Stolcke. 1996. $L_{0}$-The First Five Years of an Automated Language Acquisition Project. Integration of Natural Language and Vision Processing, 10:205.
Kenneth D. Forbus, Paul Nielsen, and Boi Faltings. 1991. Qualitative spatial reasoning: The CLOCK project. Artificial Intelligence, 51(1-3):417-471.
Sergio Guadarrama, Lorenzo Riano, Dave Golland, Daniel Go, Yangqing Jia, Dan Klein, Pieter Abbeel, Trevor Darrell, et al. 2013. Grounding Spatial Relations for Human-Robot Interaction. In IROS, pages 1640-1647. IEEE.
Douwe Kiela, Felix Hill, and Stephen Clark. 2015. Specializing Word Embeddings for Similarity or Relatedness. In EMNLP, pages 2044-2048.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, et al. 2017. Visual Genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32-73.
Geert-Jan M. Kruijff, Hendrik Zender, Patric Jensfelt, and Henrik I. Christensen. 2007. Situated Dialogue and Spatial Organization: What, Where and Why? International Journal of Advanced Robotic Systems, 4(1):16.
Xiao Lin and Devi Parikh. 2015. Don't Just Listen, Use your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks. In CVPR, pages 2984-2993.
Mateusz Malinowski and Mario Fritz. 2014. A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation. arXiv preprint arXiv:1411.5190v2.</p>
<p>Reinhard Moratz and Thora Tenbrink. 2006. Spatial Reference in Linguistic Human-Robot Interaction: Iterative, Empirically Supported Development of a Model of Projective Relations. Spatial Cognition and Computation, 6(1):63-107.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP, volume 14, pages 15321543.</p>
<p>James Pustejovsky, Jessica Moszkowicz, and Marc Verhagen. 2012. A Linguistically Grounded Annotation Language for Spatial Information. Traitement $A u$ tomatique des Langues, 53(2):87-113.
Terry Regier. 1996. The Human Semantic Potential: Spatial Language and Constrained Connectionism. MIT Press.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211-252.
Sz-Rung Shiang, Stephanie Rosenthal, Anatole Gershman, Jaime Carbonell, and Jean Oh. 2017. VisionLanguage Fusion for Object Recognition. In AAAI, pages 4603-4610. AAAI.
Carina Silberer and Mirella Lapata. 2014. Learning Grounded Meaning Representations with Autoencoders. In $A C L$, pages 721-732.
Amit Singhal, Jiebo Luo, and Weiyu Zhu. 2003. Probabilistic Spatial Context Models for Scene Content Understanding. In CVPR, volume 1, pages 235-241. IEEE.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification. In $A C L$, pages 1555-1565.
Cang Ye, Soonhac Hong, and Amirhossein Tamjidi. 2015. 6-DOF Pose Estimation of a Robotic Navigation Aid by Tracking Visual and Geometric Features. IEEE Transactions on Automation Science and Engineering, 12(4):1169-1180.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ https://github.com/gcollell/spatial-representations&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{15}$ http://www.chistera.eu/projects/muster&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
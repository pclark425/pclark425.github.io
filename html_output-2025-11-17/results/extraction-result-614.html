<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-614 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-614</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-614</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-23e02374c7b2d517c27ad0cf891c02b85fed61b6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/23e02374c7b2d517c27ad0cf891c02b85fed61b6" target="_blank">Object-Centric Scene Representations Using Active Inference</a></p>
                <p><strong>Paper Venue:</strong> Neural Computation</p>
                <p><strong>Paper TL;DR:</strong> A novel approach for scene understanding is proposed, leveraging an object-centric generative model that enables an agent to infer object category and pose in an allocentric reference frame using active inference, a neuro-inspired framework for action and perception.</p>
                <p><strong>Paper Abstract:</strong> Abstract Representing a scene and its constituent objects from raw sensory data is a core ability for enabling robots to interact with their environment. In this letter, we propose a novel approach for scene understanding, leveraging an object-centric generative model that enables an agent to infer object category and pose in an allocentric reference frame using active inference, a neuro-inspired framework for action and perception. For evaluating the behavior of an active vision agent, we also propose a new benchmark where, given a target viewpoint of a particular object, the agent needs to find the best matching viewpoint given a workspace with randomly positioned objects in 3D. We demonstrate that our active inference agent is able to balance epistemic foraging and goal-driven behavior, and quantitatively outperforms both supervised and reinforcement learning baselines by more than a factor of two in terms of success rate.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e614.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e614.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SceneCCN + AIF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scene Cortical Column Network with Active Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system that combines an explicit hierarchical object-centric probabilistic generative model (declarative) with amortized neural inference (imperative) and particle-filtered position beliefs, using expected free energy minimization to select actions for active vision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SceneCCN + Active Inference</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular hybrid architecture where a top-down hierarchical generative model factorizes a 3D scene into K object-entities (each with identity i_k, allocentric translation t_k, and object-centric pose p_{k,t}) and an explicit camera/viewpoint/action model; the likelihoods and amortized posteriors are implemented with neural networks (per-object Cortical Column Networks (CCNs) for pose, a fully-convolutional mask predictor, a spatial-transformer-based crop extractor, and a pose-transition network). Object translations are represented as probabilistic beliefs in allocentric space maintained by a particle filter. Action selection is performed by Monte Carlo sampling of candidate viewpoints scored by expected free energy G (utility + expected information gain) computed from the current probabilistic beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>An explicit probabilistic generative model / graphical model (factorized joint P(s, o~, a~)) that defines prior distributions over object identity, pose transitions P(p_{k,t}|p_{k,t-1},a_{t-1}), object translation priors P(t_k), and an observation composition model P(o_t | {o_{k,t}, u_{k,t}, sigma_{k,t}}); plus a particle-filter representation of posterior beliefs over object translations and the active inference objective (variational free energy / expected free energy).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Multiple neural network modules trained with gradient descent: per-category Cortical Column Networks (CCNs) — encoder/decoder pose models and pose-transition network (MLP), a fully-convolutional mask network, a spatial transformer-based crop estimator, identity (Bernoulli) predictor, and amortized posterior networks; Monte Carlo sampling for viewpoint/action selection; procedural particle filter for t_k belief updates.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular combination: the declarative generative model defines latent variables and training/objective (variational free energy / expected free energy). Neural networks amortize inference and implement likelihoods/transition models and are trained to minimize losses that correspond to terms in the variational free energy (reconstruction MSE, KL regularizers, BCE for identity). The particle filter provides a non-parametric posterior for object positions that feeds into expected free energy computations for action selection. Action selection integrates neural-inferred factors (pose, identity, scale) and particle-filter beliefs to compute G via Monte Carlo importance sampling; training of neural components is separate (two-phase) and not end-to-end with the particle filter/action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Balanced epistemic-foraging and goal-directed behavior emerging from expected free energy: the agent actively explores (drives actions to reduce uncertainty over object positions) when the goal object is not seen, and exploits goal-directed moves when confident; robustness to occlusions because the info-gain term causes active search rather than premature failure; improved sample-efficiency for active search tasks compared to purely supervised pose regression or model-based RL baselines; interpretable belief dynamics (inspectable particle distributions and pose/posterior factors) enabling visualizing how beliefs converge over timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Active Search / Active Vision benchmark in a 3D simulated workspace with 1-5 objects from the YCB dataset (master chef can, cracker box, sugar box, tomato soup can, mustard bottle); task: reach a provided object-centric goal observation by moving a pinhole camera (continuous 6-DOF actions), measured by success (translation < 7.5 cm and rotation < 0.5 rad) and azimuth/elevation/range error to goal.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Average success rate 69.0% on 500 evaluation scenes (100 per target object); mean azimuth error Δφ = 0.569 ± 0.034 rad; mean elevation error Δθ = 0.231 ± 0.014 rad; mean range error Δr = 0.067 ± 0.004 m (Table 1, total). On the single LEXA environment (100 goals) SceneCCN + AIF success rate = 62% with Δφ = 1.050 (±0.470), Δθ = 0.187 (±0.083), Δr = 0.076 (±0.039) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>PoseCNN (supervised, depth-enabled) baseline: total success rate 16.2% on the 500-scene benchmark; mean azimuth error Δφ = 1.277 ± 0.042 rad; Δθ = 0.431 ± 0.015 rad; Δr = 0.186 ± 0.007 m (Table 1, PoseCNN totals).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Partial/generalization trade-offs reported: per-object CCNs are trained per category (500 views each), which yields good within-category performance but poor generalization to unseen object categories; the paper reports that SceneCCN outperforms static supervised approaches in the active setting and is more robust under occlusion, but scaling to many categories or many instances of the same category is a limitation. Out-of-distribution/zero-shot generalization to novel categories is not demonstrated and is identified as a limitation (need to train a CCN per new category). Specific failure on the tomato soup can is attributed to smaller rendering scale and weaker CCN accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability relative to monolithic neural systems: the declarative generative model exposes explicit latent factors (identity, pose, translation) and a particle-filter belief over translations that can be visualized and inspected; expected free energy components (preference term and expected information gain) are explicit and interpretable; intermediate outputs (masks, spatial-transformer crops, Bernoulli identity) are human-interpretable. However, CCNs and other neural modules remain parametric and less transparent internally.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires one CCN per object category (scalability issue); difficulty scaling to scenes with many instances of the same category or heavily cluttered scenes of similar categories; currently considers static environments only (no object dynamics beyond learned pose transitions conditioned on camera action); some objects (e.g., tomato soup can) had worse performance due to dataset/rendering scale issues and corresponding weaker per-object CCN models; integration is modular rather than end-to-end, so joint optimization across particle filter and neural modules is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Active inference / variational free energy principle: perception and action are cast as approximate Bayesian inference and expected free energy minimization; the paper shows how specific loss terms for neural modules correspond to variational free energy terms of the declared generative model, providing a principled probabilistic basis for the hybrid decomposition (division of labor between explicit probabilistic model and neural amortized inference).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Object-Centric Scene Representations Using Active Inference', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e614.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e614.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoseCNN + Infogain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PoseCNN with Particle-Filter Information Gain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline hybrid where a supervised neural pose estimator (PoseCNN) is combined with a particle-filter over object positions and an information-gain-driven exploration term for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PoseCNN + Infogain</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular hybrid baseline that uses PoseCNN (a supervised deep network for segmentation and 6-DOF pose estimation from RGB-D) to produce per-frame pose and identity estimates; these outputs feed a particle filter that maintains beliefs over object positions; when the target object is not observed, action selection includes an information-gain (epistemic) term over object position (similar to SceneCCN's info-gain) to guide active search.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Particle filter representation of posterior beliefs over object translations and an information-gain scoring mechanism (expected reduction in uncertainty over object position) used to influence action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>PoseCNN — a convolutional neural network for segmentation and 6-DOF pose estimation trained in a supervised manner on RGB-D data (Xiang et al., 2018).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular pipeline: PoseCNN provides pose/identity and depth when available; a particle filter integrates these observations over time; expected information gain over particle-filter beliefs is computed and combined with the pose-based control heuristic to produce actions. Integration is not end-to-end; components operate sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved active search over pure supervised PoseCNN: adding the information-gain particle-filter component increases the success rate by encouraging exploration when the object is not seen, thereby reducing premature stopping; however, it does not achieve the same robustness or success rates as the full SceneCCN hybrid because PoseCNN lacks the object-centric generative/pose-likelihood modeling and per-category CCNs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same Active Search / Active Vision benchmark as SceneCCN (3D pinhole camera agent seeking goal observation among YCB objects).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>PoseCNN + Infogain total success rate 28.4% on the 500-scene benchmark; mean azimuth error Δφ = 1.119 ± 0.045 rad; mean elevation error Δθ = 0.367 ± 0.015 rad; mean range error Δr = 0.150 ± 0.007 m (Table 1 totals). On the LEXA single-environment test, PoseCNN + Infogain success rate = 30% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>PoseCNN (imperative-only baseline) total success rate 16.2% on the 500-scene benchmark; Δφ = 1.277 ± 0.042 rad; Δθ = 0.431 ± 0.015 rad; Δr = 0.186 ± 0.007 m (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>PoseCNN is a supervised estimator that generalizes according to its training coverage; adding the particle-filter info-gain improves search in scenes where the object is initially out-of-view, but no claims about improved generalization to novel object categories are made. The hybrid improves robustness to occlusion/initial non-visibility relative to pure PoseCNN.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Interpretability arises mainly from the particle-filter belief visualization (inspectable position distributions) and the explicit info-gain objective; PoseCNN’s internal representations remain opaque. Overall, the hybrid gives some interpretable belief signals but limited introspection into the learned pose estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still substantially lower overall success rates than SceneCCN + AIF; relies on depth input (PoseCNN uses depth whereas SceneCCN does not); integrating a supervised pose regressor with a particle filter does not recover the benefits of learning an object-centric generative model tailored for active inference; performance depends on PoseCNN accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Practical combination of supervised pose estimation with a probabilistic belief tracker and an information-gain-driven action heuristic; lacks the variational free-energy derivation that underpins SceneCCN's losses, but uses the general principle that actions that reduce position uncertainty are desirable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Object-Centric Scene Representations Using Active Inference', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e614.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e614.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Inference + Amortized Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Inference with Amortized Neural Inference (object-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general hybrid paradigm used in the paper: explicit probabilistic generative models (active inference framework) coupled with amortized neural networks (CCNs, spatial transformers) to implement approximate posteriors and likelihoods for perception and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active Inference with Amortized Neural Inference (object-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid methodological pattern where an explicit Bayesian generative model (declarative specification of latent variables, priors, and likelihoods) is inverted using amortized neural inference: neural encoders predict variational posterior parameters for latent states (pose, identity, crop parameters), neural decoders implement the likelihood model (object-centric rendering), and particle filters or other procedural techniques maintain non-parametric posteriors for select variables (object translations). Actions are chosen by minimizing expected free energy computed using the combination of neural-inferred posteriors and procedural beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Principled probabilistic generative model (factorized joint and prior terms) and active inference objective (variational free energy and expected free energy) governing perception and action.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Amortized neural networks (encoders/decoders/transition networks), spatial transformers, and procedural particle filters; neural modules are trained with MSE/KL/BCE losses corresponding to free-energy terms.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Loss-level correspondence + modular pipeline: the paper shows explicit mapping of neural training losses (reconstructions, KL terms, BCE) to variational free energy terms; inference uses neural outputs to form proposals/likelihoods which are integrated with particle filters; action selection is computed using beliefs from both neural and particle-filter components via Monte Carlo estimation of expected free energy.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Principled trade-off between exploration and exploitation (via expected free energy decomposition) that yields active search behavior; improved robustness to occlusion; interpretable factorized beliefs enabling targeted information-seeking actions; practical sample-efficiency gains in active tasks compared to naive supervised or pure RL approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Applied to the Active Search / Active Vision benchmark described in the paper (goal-reaching with a moving camera in YCB-object scenes).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Performance is expressed through the SceneCCN + AIF system results (see SceneCCN + AIF entry): success rate 69.0% on the 500-scene benchmark; other baselines reported in paper for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Comparable imperative-only baseline: PoseCNN success rate 16.2% (500-scene benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>The paradigm yields better active-search behavior when the learned neural components capture object-centric likelihoods and transitions, but generalization across object categories is limited when per-category neural modules are used; the method generalizes better to occlusion/noisy observations within trained categories due to explicit uncertainty modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High-level interpretability via explicit probabilistic factors and free-energy decomposition; intermediate neural outputs (masks, pose distributions) provide human-interpretable signals; expected free energy components (preference vs info-gain) give an interpretable policy objective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability concerns when amortized modules are trained per category; non-end-to-end modular training means mismatches between neural approximators and declarative beliefs can persist; particle filter approximations and Monte Carlo action selection introduce computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Variational Bayesian inference and the Free Energy Principle / Active Inference (minimization of variational free energy and expected free energy) underpins why declarative probabilistic structure combined with neural amortized inference yields emergent active perception and exploratory behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Object-Centric Scene Representations Using Active Inference', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embodied Object Representation Learning and Recognition <em>(Rating: 2)</em></li>
                <li>Active Inference: The Free Energy Principle in Mind, Brain, and Behavior <em>(Rating: 2)</em></li>
                <li>Generative Models for Active Vision <em>(Rating: 2)</em></li>
                <li>Object-based active inference <em>(Rating: 2)</em></li>
                <li>PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes <em>(Rating: 1)</em></li>
                <li>Discovering and achieving goals via world models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-614",
    "paper_id": "paper-23e02374c7b2d517c27ad0cf891c02b85fed61b6",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "SceneCCN + AIF",
            "name_full": "Scene Cortical Column Network with Active Inference",
            "brief_description": "A hybrid system that combines an explicit hierarchical object-centric probabilistic generative model (declarative) with amortized neural inference (imperative) and particle-filtered position beliefs, using expected free energy minimization to select actions for active vision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SceneCCN + Active Inference",
            "system_description": "A modular hybrid architecture where a top-down hierarchical generative model factorizes a 3D scene into K object-entities (each with identity i_k, allocentric translation t_k, and object-centric pose p_{k,t}) and an explicit camera/viewpoint/action model; the likelihoods and amortized posteriors are implemented with neural networks (per-object Cortical Column Networks (CCNs) for pose, a fully-convolutional mask predictor, a spatial-transformer-based crop extractor, and a pose-transition network). Object translations are represented as probabilistic beliefs in allocentric space maintained by a particle filter. Action selection is performed by Monte Carlo sampling of candidate viewpoints scored by expected free energy G (utility + expected information gain) computed from the current probabilistic beliefs.",
            "declarative_component": "An explicit probabilistic generative model / graphical model (factorized joint P(s, o~, a~)) that defines prior distributions over object identity, pose transitions P(p_{k,t}|p_{k,t-1},a_{t-1}), object translation priors P(t_k), and an observation composition model P(o_t | {o_{k,t}, u_{k,t}, sigma_{k,t}}); plus a particle-filter representation of posterior beliefs over object translations and the active inference objective (variational free energy / expected free energy).",
            "imperative_component": "Multiple neural network modules trained with gradient descent: per-category Cortical Column Networks (CCNs) — encoder/decoder pose models and pose-transition network (MLP), a fully-convolutional mask network, a spatial transformer-based crop estimator, identity (Bernoulli) predictor, and amortized posterior networks; Monte Carlo sampling for viewpoint/action selection; procedural particle filter for t_k belief updates.",
            "integration_method": "Modular combination: the declarative generative model defines latent variables and training/objective (variational free energy / expected free energy). Neural networks amortize inference and implement likelihoods/transition models and are trained to minimize losses that correspond to terms in the variational free energy (reconstruction MSE, KL regularizers, BCE for identity). The particle filter provides a non-parametric posterior for object positions that feeds into expected free energy computations for action selection. Action selection integrates neural-inferred factors (pose, identity, scale) and particle-filter beliefs to compute G via Monte Carlo importance sampling; training of neural components is separate (two-phase) and not end-to-end with the particle filter/action selection.",
            "emergent_properties": "Balanced epistemic-foraging and goal-directed behavior emerging from expected free energy: the agent actively explores (drives actions to reduce uncertainty over object positions) when the goal object is not seen, and exploits goal-directed moves when confident; robustness to occlusions because the info-gain term causes active search rather than premature failure; improved sample-efficiency for active search tasks compared to purely supervised pose regression or model-based RL baselines; interpretable belief dynamics (inspectable particle distributions and pose/posterior factors) enabling visualizing how beliefs converge over timesteps.",
            "task_or_benchmark": "Active Search / Active Vision benchmark in a 3D simulated workspace with 1-5 objects from the YCB dataset (master chef can, cracker box, sugar box, tomato soup can, mustard bottle); task: reach a provided object-centric goal observation by moving a pinhole camera (continuous 6-DOF actions), measured by success (translation &lt; 7.5 cm and rotation &lt; 0.5 rad) and azimuth/elevation/range error to goal.",
            "hybrid_performance": "Average success rate 69.0% on 500 evaluation scenes (100 per target object); mean azimuth error Δφ = 0.569 ± 0.034 rad; mean elevation error Δθ = 0.231 ± 0.014 rad; mean range error Δr = 0.067 ± 0.004 m (Table 1, total). On the single LEXA environment (100 goals) SceneCCN + AIF success rate = 62% with Δφ = 1.050 (±0.470), Δθ = 0.187 (±0.083), Δr = 0.076 (±0.039) (Table 2).",
            "declarative_only_performance": null,
            "imperative_only_performance": "PoseCNN (supervised, depth-enabled) baseline: total success rate 16.2% on the 500-scene benchmark; mean azimuth error Δφ = 1.277 ± 0.042 rad; Δθ = 0.431 ± 0.015 rad; Δr = 0.186 ± 0.007 m (Table 1, PoseCNN totals).",
            "has_comparative_results": true,
            "generalization_properties": "Partial/generalization trade-offs reported: per-object CCNs are trained per category (500 views each), which yields good within-category performance but poor generalization to unseen object categories; the paper reports that SceneCCN outperforms static supervised approaches in the active setting and is more robust under occlusion, but scaling to many categories or many instances of the same category is a limitation. Out-of-distribution/zero-shot generalization to novel categories is not demonstrated and is identified as a limitation (need to train a CCN per new category). Specific failure on the tomato soup can is attributed to smaller rendering scale and weaker CCN accuracy.",
            "interpretability_properties": "High interpretability relative to monolithic neural systems: the declarative generative model exposes explicit latent factors (identity, pose, translation) and a particle-filter belief over translations that can be visualized and inspected; expected free energy components (preference term and expected information gain) are explicit and interpretable; intermediate outputs (masks, spatial-transformer crops, Bernoulli identity) are human-interpretable. However, CCNs and other neural modules remain parametric and less transparent internally.",
            "limitations_or_failures": "Requires one CCN per object category (scalability issue); difficulty scaling to scenes with many instances of the same category or heavily cluttered scenes of similar categories; currently considers static environments only (no object dynamics beyond learned pose transitions conditioned on camera action); some objects (e.g., tomato soup can) had worse performance due to dataset/rendering scale issues and corresponding weaker per-object CCN models; integration is modular rather than end-to-end, so joint optimization across particle filter and neural modules is not provided.",
            "theoretical_framework": "Active inference / variational free energy principle: perception and action are cast as approximate Bayesian inference and expected free energy minimization; the paper shows how specific loss terms for neural modules correspond to variational free energy terms of the declared generative model, providing a principled probabilistic basis for the hybrid decomposition (division of labor between explicit probabilistic model and neural amortized inference).",
            "uuid": "e614.0",
            "source_info": {
                "paper_title": "Object-Centric Scene Representations Using Active Inference",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "PoseCNN + Infogain",
            "name_full": "PoseCNN with Particle-Filter Information Gain",
            "brief_description": "A baseline hybrid where a supervised neural pose estimator (PoseCNN) is combined with a particle-filter over object positions and an information-gain-driven exploration term for action selection.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "PoseCNN + Infogain",
            "system_description": "A modular hybrid baseline that uses PoseCNN (a supervised deep network for segmentation and 6-DOF pose estimation from RGB-D) to produce per-frame pose and identity estimates; these outputs feed a particle filter that maintains beliefs over object positions; when the target object is not observed, action selection includes an information-gain (epistemic) term over object position (similar to SceneCCN's info-gain) to guide active search.",
            "declarative_component": "Particle filter representation of posterior beliefs over object translations and an information-gain scoring mechanism (expected reduction in uncertainty over object position) used to influence action selection.",
            "imperative_component": "PoseCNN — a convolutional neural network for segmentation and 6-DOF pose estimation trained in a supervised manner on RGB-D data (Xiang et al., 2018).",
            "integration_method": "Modular pipeline: PoseCNN provides pose/identity and depth when available; a particle filter integrates these observations over time; expected information gain over particle-filter beliefs is computed and combined with the pose-based control heuristic to produce actions. Integration is not end-to-end; components operate sequentially.",
            "emergent_properties": "Improved active search over pure supervised PoseCNN: adding the information-gain particle-filter component increases the success rate by encouraging exploration when the object is not seen, thereby reducing premature stopping; however, it does not achieve the same robustness or success rates as the full SceneCCN hybrid because PoseCNN lacks the object-centric generative/pose-likelihood modeling and per-category CCNs.",
            "task_or_benchmark": "Same Active Search / Active Vision benchmark as SceneCCN (3D pinhole camera agent seeking goal observation among YCB objects).",
            "hybrid_performance": "PoseCNN + Infogain total success rate 28.4% on the 500-scene benchmark; mean azimuth error Δφ = 1.119 ± 0.045 rad; mean elevation error Δθ = 0.367 ± 0.015 rad; mean range error Δr = 0.150 ± 0.007 m (Table 1 totals). On the LEXA single-environment test, PoseCNN + Infogain success rate = 30% (Table 2).",
            "declarative_only_performance": null,
            "imperative_only_performance": "PoseCNN (imperative-only baseline) total success rate 16.2% on the 500-scene benchmark; Δφ = 1.277 ± 0.042 rad; Δθ = 0.431 ± 0.015 rad; Δr = 0.186 ± 0.007 m (Table 1).",
            "has_comparative_results": true,
            "generalization_properties": "PoseCNN is a supervised estimator that generalizes according to its training coverage; adding the particle-filter info-gain improves search in scenes where the object is initially out-of-view, but no claims about improved generalization to novel object categories are made. The hybrid improves robustness to occlusion/initial non-visibility relative to pure PoseCNN.",
            "interpretability_properties": "Interpretability arises mainly from the particle-filter belief visualization (inspectable position distributions) and the explicit info-gain objective; PoseCNN’s internal representations remain opaque. Overall, the hybrid gives some interpretable belief signals but limited introspection into the learned pose estimator.",
            "limitations_or_failures": "Still substantially lower overall success rates than SceneCCN + AIF; relies on depth input (PoseCNN uses depth whereas SceneCCN does not); integrating a supervised pose regressor with a particle filter does not recover the benefits of learning an object-centric generative model tailored for active inference; performance depends on PoseCNN accuracy.",
            "theoretical_framework": "Practical combination of supervised pose estimation with a probabilistic belief tracker and an information-gain-driven action heuristic; lacks the variational free-energy derivation that underpins SceneCCN's losses, but uses the general principle that actions that reduce position uncertainty are desirable.",
            "uuid": "e614.1",
            "source_info": {
                "paper_title": "Object-Centric Scene Representations Using Active Inference",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Active Inference + Amortized Inference",
            "name_full": "Active Inference with Amortized Neural Inference (object-centric)",
            "brief_description": "A general hybrid paradigm used in the paper: explicit probabilistic generative models (active inference framework) coupled with amortized neural networks (CCNs, spatial transformers) to implement approximate posteriors and likelihoods for perception and planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Active Inference with Amortized Neural Inference (object-centric)",
            "system_description": "A hybrid methodological pattern where an explicit Bayesian generative model (declarative specification of latent variables, priors, and likelihoods) is inverted using amortized neural inference: neural encoders predict variational posterior parameters for latent states (pose, identity, crop parameters), neural decoders implement the likelihood model (object-centric rendering), and particle filters or other procedural techniques maintain non-parametric posteriors for select variables (object translations). Actions are chosen by minimizing expected free energy computed using the combination of neural-inferred posteriors and procedural beliefs.",
            "declarative_component": "Principled probabilistic generative model (factorized joint and prior terms) and active inference objective (variational free energy and expected free energy) governing perception and action.",
            "imperative_component": "Amortized neural networks (encoders/decoders/transition networks), spatial transformers, and procedural particle filters; neural modules are trained with MSE/KL/BCE losses corresponding to free-energy terms.",
            "integration_method": "Loss-level correspondence + modular pipeline: the paper shows explicit mapping of neural training losses (reconstructions, KL terms, BCE) to variational free energy terms; inference uses neural outputs to form proposals/likelihoods which are integrated with particle filters; action selection is computed using beliefs from both neural and particle-filter components via Monte Carlo estimation of expected free energy.",
            "emergent_properties": "Principled trade-off between exploration and exploitation (via expected free energy decomposition) that yields active search behavior; improved robustness to occlusion; interpretable factorized beliefs enabling targeted information-seeking actions; practical sample-efficiency gains in active tasks compared to naive supervised or pure RL approaches.",
            "task_or_benchmark": "Applied to the Active Search / Active Vision benchmark described in the paper (goal-reaching with a moving camera in YCB-object scenes).",
            "hybrid_performance": "Performance is expressed through the SceneCCN + AIF system results (see SceneCCN + AIF entry): success rate 69.0% on the 500-scene benchmark; other baselines reported in paper for comparison.",
            "declarative_only_performance": null,
            "imperative_only_performance": "Comparable imperative-only baseline: PoseCNN success rate 16.2% (500-scene benchmark).",
            "has_comparative_results": true,
            "generalization_properties": "The paradigm yields better active-search behavior when the learned neural components capture object-centric likelihoods and transitions, but generalization across object categories is limited when per-category neural modules are used; the method generalizes better to occlusion/noisy observations within trained categories due to explicit uncertainty modeling.",
            "interpretability_properties": "High-level interpretability via explicit probabilistic factors and free-energy decomposition; intermediate neural outputs (masks, pose distributions) provide human-interpretable signals; expected free energy components (preference vs info-gain) give an interpretable policy objective.",
            "limitations_or_failures": "Scalability concerns when amortized modules are trained per category; non-end-to-end modular training means mismatches between neural approximators and declarative beliefs can persist; particle filter approximations and Monte Carlo action selection introduce computational cost.",
            "theoretical_framework": "Variational Bayesian inference and the Free Energy Principle / Active Inference (minimization of variational free energy and expected free energy) underpins why declarative probabilistic structure combined with neural amortized inference yields emergent active perception and exploratory behavior.",
            "uuid": "e614.2",
            "source_info": {
                "paper_title": "Object-Centric Scene Representations Using Active Inference",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embodied Object Representation Learning and Recognition",
            "rating": 2
        },
        {
            "paper_title": "Active Inference: The Free Energy Principle in Mind, Brain, and Behavior",
            "rating": 2
        },
        {
            "paper_title": "Generative Models for Active Vision",
            "rating": 2
        },
        {
            "paper_title": "Object-based active inference",
            "rating": 2
        },
        {
            "paper_title": "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes",
            "rating": 1
        },
        {
            "paper_title": "Discovering and achieving goals via world models",
            "rating": 1
        }
    ],
    "cost": 0.01650875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Object-Centric Scene Representations using Active Inference</h1>
<p>Toon Van de Maele ${ }^{1}$ Tim Verbelen ${ }^{1}$ Pietro Mazzaglia ${ }^{1}$ Stefano Ferraro ${ }^{1}$ Bart Dhoedt ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Representing a scene and its constituent objects from raw sensory data is a core ability for enabling robots to interact with their environment. In this paper, we propose a novel approach for scene understanding, leveraging a hierarchical object-centric generative model that enables an agent to infer object category and pose in an allocentric reference frame using active inference, a neuro-inspired framework for action and perception. For evaluating the behavior of an active vision agent, we also propose a new benchmark where, given a target viewpoint of a particular object, the agent needs to find the best matching viewpoint given a workspace with randomly positioned objects in 3D. We demonstrate that our active inference agent is able to balance epistemic foraging and goal-driven behavior, and outperforms both supervised and reinforcement learning baselines by a large margin.</p>
<h2>1. Introduction</h2>
<p>Spatial scene understanding is a core ability for enabling robots to understand and interact with their environment and has been a long-standing challenge in computer vision. Humans naturally decompose scenes into object-centric representations and infer information about objects, their appearance, their constituent parts, as well as their pose and shape in the 3D space (Hinton, 1979). For example, when seeing a coffee cup, humans immediately know where to reach for the handle, even when the handle is not directly in view.</p>
<p>In the past decade, advances in deep learning have enabled to devise systems that can distinguish objects in images, i.e. predicting segmentation masks (Minaee et al., 2021), or the object pose in 3D (Xiang et al., 2018a; Du et al., 2021) by supervised training on a dataset annotated with predefined object classes and poses. Also, unsupervised methods have been proposed to infer separate objects from one (Eslami</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2016a; Bear et al., 2020) or multiple (Chen et al., 2021) views. In order to have full 3D scene understanding, other approaches learn representations of complete 3D shapes by training on 3D CAD models of objects of interest (Wu et al., 2015). Such representations can then be used to infer objects shape and pose in 3D from one or more RGB-D images (Sucar et al., 2020). However, these methods typically operate on static inputs and do not allow the agent to interact with the scene.</p>
<p>In contrast, humans learn by actively engaging and interacting with the world (James et al., 2014). A prevailing account of human perception is that the brain builds a generative model of the world, constantly explaining its observations (Friston et al., 2017), i.e. active inference. In this regard, vision is cast as inverting a generative model of the scene, in order to infer its constituent objects, their appearance, and pose (Parr et al., 2021). This is consistent with findings in the brain, where visual inputs are processed by a dorsal ("where") stream on the one hand, representing where an object is in the space, and a ventral ("what") stream on the other hand, representing object identity (Mishkin et al., 1983).</p>
<p>In this paper, we propose a novel method for spatial scene understanding, using an agent that can actively move the camera in the scene. We endow the agent with an objectcentric generative model, which is optimized by minimizing free energy. By also inferring actions that minimize expected free energy, the agent engages in active inference and balances goal-directed with explorative behavior. To evaluate our approach, we present a novel benchmark that casts active vision as a task in which an artifical agent needs to reach a goal observation. To demonstrate this, our paper proposes the following contributions:</p>
<ul>
<li>We propose a novel hierarchical object-centric generative model that factorizes object identity from a location in both egocentric and allocentric reference frames (Section 3).</li>
<li>
<p>We developed a new benchmark environment for evaluating the active vision problem. In this environment, the agent needs to find a particular object by actively engaging with the environment through moving the camera (Section 4.1).</p>
</li>
<li>
<p>We compare performance both quantitatively and qualitatively against two other methods that adopt a supervised (Xiang et al., 2018a) and a reinforcement learning (Mendonca et al., 2021) approach respectively (Section 4.2) and show that we perform better on the active vision benchmark.</p>
</li>
<li>We demonstrate that driving active vision through the expected free energy is naturally robust against occlusions, as it will actively explore the environment when the target object is initially not in view (Section 4.2).</li>
</ul>
<h2>2. Related Work</h2>
<p>Object-centric representations: One of the early works on decomposing images of scenes into their constituent objects is Attend Infer Repeat (Eslami et al., 2016b), where an observation is decomposed into object-level representations by having a recurrent neural network predicting the parameters of a spatial transformer network (Jaderberg et al., 2015). PSGNets (Bear et al., 2020) introduce graph pooling and vectorization operations that convert image feature maps into object-centric graph structures. In ROOTS (Chen et al., 2021), the authors consider multiple viewpoints from the same scene, crop out the objects of each viewpoint, and then group them together to encode them with a generative query network instance for each object (Eslami et al., 2018). In doing so, the objects can be rendered individually and aggregated in a full observation. Other methods adopt a representation with a fixed number of "slots" that can be used to represent objects in the scene. MONet (Burgess et al., 2019) recurrently predicts a mask for each available slot and then uses a variational autoencoder to encode each masked observation. In IODINE (Greff et al., 2020), a joint decomposition and representation model is learned with fixed slot allocation. To scale this up to more objects, the encoders are adapted for predicting proposals in parallel instead of recurrently (Crawford \&amp; Pineau, 2019; Jiang et al., 2020). GENESIS in addition learns an autoregressive prior for scene generation (Engelcke et al., 2020). Locatello et al. introduce the Slot Attention module, which uses an attention mechanism to bind input features to the set of slots, which proved to be more efficient in terms of both memory consumption and runtime (Locatello et al., 2020). Object Scene Representation Transformer (Sajjadi et al., 2022a) combines slot attention with scene representation transformers (Sajjadi et al., 2022b) to learn object-centric representations from a set of multiple viewpoints of a scene in an end-to-end fashion. When dealing with sequences of observations over time, most models learn to predict the dynamics of each object slot separately (Kosiorek et al., 2018; Jiang et al., 2020).</p>
<p>World Models: In the context of reinforcement learning, world models have been devised to compress observations
into a latent state space, and learn a dynamics model to predict how actions evolve future states (Ha \&amp; Schmidhuber, 2018). These world models can then be used for planning actions (Hafner et al., 2019), or for learning policies in imagination, achieving state-of-the-art performance on various RL benchmarks (Hafner et al., 2020; 2021; Mazzaglia et al., 2022). An increasingly popular paradigm, especially in RL for robotics, is to provide the agent with a goal observation to obtained (Andrychowicz et al., 2017), for which also a world model can be leveraged to discover and achieve novel goals (Mendonca et al., 2021). A recent line of work tries to combine structured latent state spaces, i.e. using different slots, for learning world models, typically for object-related tasks such as block pushing and stacking (Kipf et al., 2019; Watters et al., 2019; Veerapaneni et al., 2020; Lin et al., 2020).</p>
<p>Active Inference: Active inference is a theory that characterizes perception, planning, and action in terms of probabilistic inference (Parr et al., 2022), which is applied in various domains ranging from neuroscience (Smith et al., 2020), neuropsychology (Parr et al., 2018), biology (Pio-Lopez et al., 2022) to robotics and artificial intelligence (Lanillos et al., 2021). In particular, visual foraging can be modeled using active inference agents, although typically limited to simulations with discrete observation or action spaces (Mirza et al., 2016; Daucé, 2018). Van de Maele et al. (2022) adopt the active inference framework to learn objectcentric generative models of 3D objects, called cortical column networks (CCN). Similar to the work on structured world models, van Bergen \&amp; Lanillos (2022) extend the IODINE framework with actions to infer policies in a 2D sprites environment.</p>
<p>Scene Representation Benchmark: There exists a multitude of datasets for learning 3D scene representations, but many of these datasets use a static camera (Johnson et al., 2016; Yan et al., 2021; Kundu et al., 2022) or use prerecorded moving camera frames (Sajjadi et al., 2022b). To the best of our knowledge, none of these benchmarks consider an active camera that can move in the scene.</p>
<h2>3. Method</h2>
<p>In this section, we describe how our agent carries out scene decomposition leveraging a hierarchical generative model. We first discuss the active inference framework and show how optimizing the free energy functional yields a tradeoff between epistemic foraging and goal-directed behavior. Then, we propose a generative model representing a scene as a collection of multiple objects with distinct features, represented as hidden variables. Next, we describe how to instantiate the generative model using deep neural networks, and how the agent can be driven towards goal through expected free energy minimization, updating its beliefs about</p>
<p>the scene representation at every step.</p>
<h3>3.1. Active Inference</h3>
<p>Active inference is a process theory of the brain which states that all neuronal processing and action is driven by the minimization of (a bound on) surprise, i.e. free energy (Parr et al., 2022). This offers a first principles account of understanding perception and action as approximate Bayesian inference on hidden states and actions of a generative model. In general, this generative model is the joint probability distribution over sequences of observations $\tilde{\mathbf{o}}$, actions $\tilde{\mathbf{a}}$ and hidden states $\tilde{\mathbf{s}}$ :</p>
<p>$$
P(\tilde{\mathbf{o}}, \tilde{\mathbf{a}}, \tilde{\mathbf{s}})=\prod_{t} P\left(\mathbf{o}<em t="t">{t} \mid \mathbf{s}</em>}\right) P\left(\mathbf{s<em t-1="t-1">{t} \mid \mathbf{s}</em>}, \mathbf{a<em t-1="t-1">{t-1}\right) P\left(\mathbf{a}</em>\right)
$$</p>
<p>The goal of the agent is then to minimize the variational free energy $F$ (Parr et al., 2022), i.e. the negative evidence lower bound (Rezende et al., 2014; Kingma \&amp; Welling, 2014), by introducing the approximate posterior $Q\left(\mathbf{s}<em t="t">{t} \mid \mathbf{o}</em>\right)$.</p>
<p>$$
\begin{aligned}
F= &amp; \sum_{t} D_{K L}\left[Q\left(\mathbf{s}<em t="t">{t} \mid \mathbf{o}</em>}\right) | P\left(\mathbf{s<em t-1="t-1">{t} \mid \mathbf{s}</em>}, \mathbf{a<em Q_left_mathbf_s="Q\left(\mathbf{s">{t-1}\right)\right] \
&amp; +\mathbb{E}</em><em t="t">{t} \mid \mathbf{o}</em>}\right)}\left[-\log P\left(\mathbf{o<em t="t">{t} \mid \mathbf{s}</em>\right)\right]
\end{aligned}
$$</p>
<p>Crucially, in active inference the agent also selects actions $a_{t}$ that it believes will minimize the so-called expected free energy $G\left(a_{t}\right)$ (Parr et al., 2022):</p>
<p>$$
\begin{aligned}
G\left(a_{t}\right)= &amp; \mathbb{E}<em t_1="t+1">{Q\left(\mathbf{s}</em>}, \mathbf{o<em t_1="t+1">{t+1}\right)}[\underbrace{-\log P\left(\mathbf{o}</em>}\right)<em t_1="t+1">{\text {Realizing Preferences }} \
&amp; +\underbrace{\log Q\left(\mathbf{s}</em>} \mid \mathbf{a<em t_1="t+1">{t}\right)-\log Q\left(\mathbf{s}</em>} \mid \mathbf{a<em t_1="t+1">{t}, \mathbf{o}</em>]
\end{aligned}
$$}\right)}_{\text {Expected Information Gain }</p>
<p>consisting of a utility term based on a preferred distribution of future observations, as well as an information gain term. Hence, minimizing expected free energy balances goal-directed behavior and epistemic foraging.</p>
<h3>3.2. A Generative Model for Vision</h3>
<p>The agent - an active camera - entails a generative model to explain its observations and the effect of its actions thereon. In this case, the model should thus describe a 3D scene consisting of a set of objects in a workspace while observing 2D images from viewpoints reached through moving the camera. We factorize the generative model of a scene $\mathbf{s}$ as a hierarchical composition of $K$ entities $\mathbf{e}_{k}$. In the remainder, we assume the scene is static and therefore these entities do not change. However, this model could also be extended to take the object dynamics into account over time.</p>
<p>The image the agent observes is constructed through a topdown generative model, depicted in Figure 1. Each entity $\mathbf{e}<em k="k">{k}$ consists of an identity $\mathbf{i}</em>$ w.r.t. a global
}$, a translation $\mathbf{t}_{k<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Graphical representation of the agents' generative model for an object-centric factorization of a scene. Observed variables are denoted by blue circles, while unobserved variables are denoted by white circles.
reference frame, and a latent representation of the object pose $\mathbf{p}<em t="t">{k, t}$.
To compose an observation of a particular viewpoint $\mathbf{v}</em>}$, for each entity, an object-centric observation $\mathbf{o<em k_="k," t="t">{k, t}$ is generated from the identity and pose, which renders the specific object in a particular pose. Next, given the object translation and a camera pinhole model, the pixel-coordinates $\mathbf{u}</em>$ the agent receives as input is then a composition of these object-centric observations in a global view. Note that the identity and translation parameters are consistent over the scene, do not change over time, and are therefore not dependent on the action of the agent.}$ and scale $\sigma_{k, t}$ in the full observation can be generated. The resulting observation $\mathbf{o}_{t</p>
<p>The action $\mathbf{a}<em t="t">{t-1}$ represents the relative transform the agent must take in the global reference frame, in order to reach the next viewpoint $\mathbf{v}</em>}$. We parameterize the resulting viewpoint as the combination of the location $\mathbf{l<em t="t">{t}$ in space at which the agent wants to direct its gaze, together with the spherical coordinates with respect to this look-at point: range $\mathbf{r}</em>$.}$, elevation $\phi_{t}$ and azimuth $\theta_{t</p>
<p>The factorization of the described generative model is shown in Figure 1, and a detailed formal description of the model is provided in Appendix A. This kind of decomposition of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Flow of the inference process of observation into the distinct latent variables. For each of the K -considered object categories, the object pixels are extracted from the observation (red) by a neural network that predicts pixel-center, scale, and a masked crop of the object. From this crop, the belief over the pose and identity latent variable is predicted (green) using a CCN. Given the pinhole camera model, pixel-wise center, and scale of the object, an estimate of the object's location are formed which is used to update a particle filter (blue) approximating the belief over the object translation.
a scene in distinct objects with their respective features, as well as the specification of actions in our generative model might also underpin how the visual system of the brain works, and maps to the visual cortices, cerebral attention networks, and the oculomotor system (Parr et al., 2021).</p>
<h3>3.3. Scene Perception with SceneCCN</h3>
<p>In active inference, perception is cast as approximate Bayesian inference over the hidden variables. At each timestep, the agent observes $\mathbf{o}_{t}$ and infers approximate posterior distributions over each hidden variable in a bottom-up process. This inference process is amortized using a collection of neural network models, which is depicted in Figure 2.</p>
<p>We ground our approach on Cortical Column Networks (CCN) (Van de Maele et al., 2022), which are object-centric models that learn the approximate posterior over the latent pose $q_{\theta}\left(\mathbf{p}<em k_="k," t="t">{k, t} \mid \mathbf{o}</em>}\right)$ for views of a particular object, by predicting observations of novel poses. These consist of a separate encoder-decoder model trained separately per object category, which is inspired by cortical columns in the brain (Hawkins et al., 2017), and reflect how toddlers typically interact with one object at a time (James et al., 2014). This allows these to be efficiently trained in parallel on a limited dataset consisting of views of one particular object, at the cost of poor generalization (i.e. for new object categories a new CCN will need to be trained). In addition to the pose representation, the encoder also outputs a Bernoulli variable $q_{\theta}\left(\mathbf{i<em k_="k," t="t">{k} \mid \mathbf{o}</em>\right)$ which indicates the belief that the object of interest is present in the view.</p>
<p>In order to infer object-centric views $\mathbf{o}<em _phi="\phi">{k, t}$, we learn for each object category an object extraction network $q</em>}\left(\mathbf{o<em k_="k," t="t">{k, t}, \mathbf{u}</em>}, \sigma_{k, t} \mid \mathbf{o<em k_="k," t="t">{t}\right)$ with parameters $\phi$, which first produces a mask $\alpha</em>$ which masks anything beside the ob-
ject category of interest. Based on the masked observation $\mathbf{o}<em k_="k," t="t">{t} \odot \alpha</em>}$, we then predict the center pixel $\mathbf{u<em k_="k," t="t">{k, t}$ of where the object is present in the observation, as well as the scale $\sigma</em>$ for the CCN.}$. This is then fed into a spatial transformer network (Jaderberg et al., 2015), which produces an object-centric crop $\mathbf{o}_{k, t</p>
<p>The neural networks comprising these two blocks are optimized in two separate phases. In the first phase, the pose representation is learned by training the CCN on an objectcentric dataset. For each object category, a set of 500 observations and viewpoint pairs is collected. The viewpoints are sampled uniformly on the surface of a sphere, with a fixed radius, while oriented towards the object center. For each step during training, two views are randomly sampled, and the action is computed as the relative displacement in azimuth and elevation. To learn how this action results in a transitioned pose representation, we also train a pose transition network $p_{\theta}\left(\mathbf{p}<em 1="1" k_="k,">{k, 2} \mid \mathbf{p}</em>\right)$ that models these dynamics. For more details about the construction of this dataset, the reader is referred to Appendix B.}, \mathbf{a}_{t-1</p>
<p>The objective is prediction error over transitioned poses, given an action:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em 1="1">{1}= &amp; \underbrace{\lambda</em>}\left|\mathbf{o<em _theta="\theta">{k, 1}-p</em>}\left(\mathbf{o<em 1="1" k_="k,">{k, 1} \mid \mathbf{p}</em>\right)\right|<em 2="2">{2}+\lambda</em>}\left|\mathbf{o<em _theta="\theta">{k, 2}-p</em>}\left(\mathbf{o<em 1="1" k_="k,">{k, 1} \mid \mathbf{p}</em>\right)\right|<em _Reconstruction="{Reconstruction" _text="\text" before="before" error="error" transition="transition">{2}}</em> \
&amp; +\underbrace{\lambda_{3}\left|\mathbf{o}}<em _theta="\theta">{k, 2}-p</em>}\left(\mathbf{o<em 2_="2," _="{" _text="\text" k_="k," trans="trans">{k, 2, \text { trans }} \mid \mathbf{p}</em>\right)\right|}<em _Reconstruction="{Reconstruction" _text="\text" after="after" error="error" transition="transition">{2}}</em> \
&amp; +\underbrace{\lambda_{4} D_{K L}\left[q_{\theta}\left(\mathbf{p}}<em 2="2" k_="k,">{k, 2} \mid \mathbf{o}</em>}\right)\right]\left[p_{\theta}\left(\mathbf{p<em 1="1" k_="k,">{k, 2} \mid \mathbf{p}</em>}, \mathbf{a<em _Complexity="{Complexity" _text="\text" model="model" of="of" transition="transition">{t-1}\right)\right]}</em>
\end{aligned}
$$}</p>
<p>These models are additionally regularized by a KLdivergence term between the outputted distributions and a standard normal distribution.</p>
<p>In the second phase, the object extraction network is trained for extracting object-centric crops. To this end, we do not collect a new dataset, but instead augment the object-centric dataset by scaling and translating the observations, while also adding a scaled and translated object from a different category to simulate occlusion. On top of this, we add random color patches in the background. The model learns to extract object-centric crops by minimizing:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em k_="k," t="t">{2}= &amp; \underbrace{\left|\mathbf{o}</em>}-q_{\phi}\left(\mathbf{o<em k_="k," t="t">{k, t} \mid \mathbf{u}</em>}, \sigma_{k, t}, \mathbf{o<em 2="2">{t}\right)\right|</em>}<em k_="k," t="t">{\text {Cropping error }} \
&amp; +\underbrace{\left|\alpha</em>}-\tilde{\alpha<em 2="2">{k, t}\right|</em>}<em k="k">{\text {Masking error }}+\underbrace{\operatorname{BCE}\left(\mathbf{i}</em>}, q_{\theta}\left(\mathbf{i<em k_="k," t="t">{k} \mid \mathbf{o}</em>
\end{aligned}
$$}\right)\right)}_{\text {Identification error }</p>
<p>where the first two terms are computed as the mean squared error between the predicted masks and crops, and the respective ground truths. The BCE-term is the binary cross entropy loss over the predicted object identity variable. For regularization purposes, the extracted crop is also fed through the (frozen) CCN and the KL-divergence terms from Equation 4 are also optimized.</p>
<p>In Appendix A, we show how these loss terms are in effect consistent with minimizing the variational free energy of the generative model proposed Section 3.2. Also for the exact parameterizations of all neural networks, hyperparameters, and training details, the reader is referred to Appendix C.</p>
<p>To infer posterior beliefs over the object translation $\mathbf{t}<em k_="k," t="t">{k, t}$ in an allocentric reference frame, we use the inferred pixel coordinates $\mathbf{u}</em>$ of the object. Given the absolute viewpoint of our camera and a pinhole camera model, we can backproject the pixel coordinate to a ray in 3D, along which we specify a 3D Gaussian distribution with mean at the estimated depth given the scale, and a fixed covariance matrix forming an ellipsoid density along the ray. To get a better estimate of the object's position over time, this position belief is integrated using a particle filter. When an object is not detected in an observation, we also reduce the particle weights that are in view, to also reflect and integrate the information gained by not observing a particular object.}$ and scale $\sigma_{k, t</p>
<h3>3.4. Action selection</h3>
<p>Setting an agent's goal in active inference is done by specifying a prior preference in observation space, which the agent is expected to obtain. In this case, the preference is the log probability of reaching the goal observation $\mathbf{o}_{\text {goal }}$. Actions are then selected that minimize the expected free energy G, as defined in Equation 3, i.e. scoring how much expected observations will realize the preferred observations, and how much information these will bring over the hidden variables.</p>
<p>In our case, we mainly focus on the expected information
gain about the object positions $\mathbf{t}<em _goal="{goal" _text="\text">{k}$, which are the main source of uncertainty in the scene. Moreover, since the log likelihood in observation space (i.e. pixel space) is less meaningful, we first infer the goal object identity, scale and pose from $\mathbf{o}</em>$ using our SceneCCN, and then score utility w.r.t. reaching these factor values.}</p>
<p>The expected free energy for a candidate viewpoint $\mathbf{v}_{t+1}$ becomes:</p>
<p>$$
\begin{aligned}
G\left(\mathbf{v}<em Q_left_mathbf_s="Q\left(\mathbf{s">{t+1}\right)= &amp; \mathbb{E}</em>}, \mathbf{o<em k="k">{t+1}\right)}[\underbrace{-\log P\left(\mathbf{t}</em>} \mid \mathbf{v<em _goal="{goal" _text="\text">{t+1}, \mathbf{o}</em>}}\right)<em k_="k," t_1="t+1">{\text {Look at goal object position }} \
&amp; -\underbrace{\log P\left(\sigma</em>} \mid \mathbf{v<em _goal="{goal" _text="\text">{t+1}, \mathbf{o}</em>}}\right)<em k_="k," t_1="t+1">{\text {at the desired scale }} \
&amp; -\underbrace{\log P\left(\mathbf{p}</em>} \mid \mathbf{v<em _goal="{goal" _text="\text">{t+1}, \mathbf{o}</em>}}\right)<em k_="k," t_1="t+1">{\text {and desired pose }} \
&amp; +\underbrace{\log Q\left(\mathbf{t}</em>} \mid \mathbf{v<em k_="k," t_1="t+1">{t+1}\right)-\log Q\left(\mathbf{t}</em>} \mid \mathbf{v<em t_1="t+1">{t+1}, \mathbf{o}</em>
\end{aligned}
$$}\right)}_{\text {while searching for the object position. }</p>
<p>This boils down to directing the agent towards looking at the goal object at the right pose and scale, while also searching for where the object is positioned. We use Monte Carlo sampling to obtain the best next viewpoint, by sampling 5000 targets and evaluating G. Instead of sampling uniformly in the workspace, we use importance sampling, putting more weight on viewpoints that look at positions where the target object is more likely positioned given the current beliefs of the particle filter. Once $\mathbf{v}_{t+1}$ is determined, we find the next action, by moving the camera a step of maximum 5 cm towards the target view.</p>
<h2>4. Experiments</h2>
<p>We aim to evaluate whether our proposed generative model entails a representation that enables the agent to understand the scene and to infer: (i) what the different objects are and (ii) where these different objects are located. To this end, we design a new environment in which objects from the YCB dataset (Calli et al., 2015) are spawned in random positions, and the agent can move a camera in the workspace.</p>
<h3>4.1. Active Search Benchmark</h3>
<p>In order to evaluate an active vision agent, we designed a benchmark that casts the scene perception problem as a problem in which a goal observation has to be reached by a virtual camera. We created a simulation environment in which between one and five objects from the YCB dataset (Calli et al., 2015) are spawned. We consider the master chef can, the cracker box, the sugar box, the tomato soup can, and the mustard bottle. The positions of the objects are randomly generated to be on a table of size $1 \mathrm{~m} \times 1 \mathrm{~m}$ with a uniformly randomly sampled color. The goal image is an object-centric observation of a target object. In this observation, the other</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. An example of the active vision environment. The left image shows the scene, a table with 5 objects from the YCB dataset. The green camera shows the current pose of the agent and the blue camera shows the target pose of the agent in the environment. The middle image shows the object-centric target goal observation in which the other objects are not rendered. The right image shows the current observation the agent sees.
objects in the scene are not rendered to have no ambiguity on what the target is.</p>
<p>The agent can move a pinhole camera, and the action space is defined as continuous in 6 dimensions, representing the relative displacement in the three-dimensional space and the Euler angles for the relative rotation. Values in the action space lie in the range $[-0.5,0.5]$. The agent can only move over the table with a max height of 0.6 cm . If the action moves the camera outside of the environment or inside of an object, the resulting pose is clipped.</p>
<p>The task is considered successful if the agent reaches the goal pose within a translation error below 7.5 cm , and a rotation error lower than 0.5 rad . The agent starts at an initial position with 0 rad azimuth, $\frac{\pi}{4}$ rad elevation, and a range of 0.65 m , with respect to the table center. A visual representation of the environment, a goal observation, and the initial observation of a randomly generated scene is shown in Figure 3.</p>
<p>We benchmark the performance in this environment using the following metrics: the success rate (i.e. when the agent reaches the goal in less than 350 steps), the azimuth, elevation, and range error of the goal camera position with respect to the goal object position. As symmetry in object shape can cause a high overall translation error while actually having an accurate final observation, opted for these metrics instead of an error in allocentric 6DOF space.</p>
<p>We implemented this environment adopting the structure of OpenAI gym (Brockman et al., 2016), and the code is supplied in the supplementary material.</p>
<h3>4.2 Results</h3>
<p>Baselines: We compare our approach with several baselines for solving the benchmark:</p>
<ul>
<li>SceneCCN + AIF: Our agent uses the proposed SceneCCN to decompose scene observations into be- liefs over separate factors (Section 3.3), and adopts the expected free energy objective for action selection (Section 3.4).</li>
<li>PoseCNN: A supervised model for object segmentation and pose estimation of RGBD data (Xiang et al., 2018b) trained on the YCB dataset. The agent action is acquired by first estimating the identity and pose directly from both the goal observation and the agents' current observation. From these two estimated poses, the relative transform is computed, and a 5 cm step according to this transform is taken as action. The agent executes this process every step until the target pose is reached. When the agent does not observe the preference the agent stops. Note that PoseCNN has access to the depth information, whereas our approach has not.</li>
<li>PoseCNN + Infogain: The PoseCNN baseline, but adding our object position particle filter. When the target object is not in view, we use the information gain term over object position similar to our approach.</li>
<li>LEXA: A model-based RL agent that adopts unsupervised exploration, to learn an accurate world model of the environment, and goal-conditioned RL, to learn to reach goals sampled from the agent experience buffer (Mendonca et al., 2021).</li>
</ul>
<p>Benchmark Performance: The benchmark consists of 500 evaluation scenes: 100 for each target object with 1-5 objects randomly positioned in the workspace, which has a random color. The exact scenes on which we evaluated are provided in the supplementary material. The error in spherical coordinates with respect to the target object center is plotted in Figure 4. Our approach outperforms the PoseCNN baseline, whereas adding information gain also improves the vanilla PoseCNN baseline.</p>
<p>A more detailed breakdown per object is provided in Table 1. Our model consistently outperforms the baselines in both in terms of pose error and success rate of reaching the target pose according to the environment stop criteria. We
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Box plot representing the azimuth, elevation, and range error of the reached position for our approach and the two PoseCNN baselines over all scenes of the 25 configurations.</p>
<p>Table 1. Comparison of SceneCCN with PoseCNN on the whole benchmark. We compare over the success rate ( $\%$ s), azimuth error $\Delta \phi$, elevation error $\Delta \theta$ and range error $\Delta \mathbf{r}$. For each target object, 100 scenes are evaluated. The values are represented as mean $\pm$ standard error. The best performances are marked in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">\% s</th>
<th style="text-align: center;">$\Delta \phi$</th>
<th style="text-align: center;">$\Delta \theta$</th>
<th style="text-align: center;">$\Delta \mathbf{r}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PoseCNN</td>
<td style="text-align: center;">Master chef can</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">$1.333 \pm 0.088$</td>
<td style="text-align: center;">$0.418 \pm 0.029$</td>
<td style="text-align: center;">$0.203 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cracker box</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">$1.219 \pm 0.096$</td>
<td style="text-align: center;">$0.387 \pm 0.031$</td>
<td style="text-align: center;">$0.148 \pm 0.013$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sugar box</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">$1.092 \pm 0.087$</td>
<td style="text-align: center;">$0.447 \pm 0.035$</td>
<td style="text-align: center;">$0.185 \pm 0.015$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mustard bottle</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">$1.466 \pm 0.091$</td>
<td style="text-align: center;">$0.508 \pm 0.041$</td>
<td style="text-align: center;">$0.175 \pm 0.016$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tomato soup can</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">$1.276 \pm 0.100$</td>
<td style="text-align: center;">$0.395 \pm 0.030$</td>
<td style="text-align: center;">$0.218 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">$1.277 \pm 0.042$</td>
<td style="text-align: center;">$0.431 \pm 0.015$</td>
<td style="text-align: center;">$0.186 \pm 0.007$</td>
</tr>
<tr>
<td style="text-align: center;">PoseCNN + Infogain</td>
<td style="text-align: center;">Master chef can</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">$1.198 \pm 0.099$</td>
<td style="text-align: center;">$0.419 \pm 0.034$</td>
<td style="text-align: center;">$0.171 \pm 0.017$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cracker box</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">$1.115 \pm 0.101$</td>
<td style="text-align: center;">$0.380 \pm 0.036$</td>
<td style="text-align: center;">$0.133 \pm 0.011$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sugar box</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">$0.978 \pm 0.098$</td>
<td style="text-align: center;">$0.330 \pm 0.032$</td>
<td style="text-align: center;">$0.149 \pm 0.015$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mustard bottle</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">$1.241 \pm 0.100$</td>
<td style="text-align: center;">$0.381 \pm 0.037$</td>
<td style="text-align: center;">$0.138 \pm 0.013$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tomato soup can</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">$1.065 \pm 0.097$</td>
<td style="text-align: center;">$0.327 \pm 0.031$</td>
<td style="text-align: center;">$0.157 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">$1.119 \pm 0.045$</td>
<td style="text-align: center;">$0.367 \pm 0.015$</td>
<td style="text-align: center;">$0.150 \pm 0.007$</td>
</tr>
<tr>
<td style="text-align: center;">SceneCCN + AIF</td>
<td style="text-align: center;">Master chef can</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">$0.491 \pm 0.067$</td>
<td style="text-align: center;">$0.138 \pm 0.014$</td>
<td style="text-align: center;">$0.042 \pm 0.005$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cracker box</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">$0.532 \pm 0.075$</td>
<td style="text-align: center;">$0.290 \pm 0.039$</td>
<td style="text-align: center;">$0.058 \pm 0.007$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sugar box</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">$0.508 \pm 0.076$</td>
<td style="text-align: center;">$0.232 \pm 0.032$</td>
<td style="text-align: center;">$0.058 \pm 0.006$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mustard bottle</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">$0.420 \pm 0.064$</td>
<td style="text-align: center;">$0.275 \pm 0.041$</td>
<td style="text-align: center;">$0.057 \pm 0.008$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tomato soup can</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">$0.896 \pm 0.085$</td>
<td style="text-align: center;">$0.220 \pm 0.019$</td>
<td style="text-align: center;">$0.120 \pm 0.013$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">$0.569 \pm 0.034$</td>
<td style="text-align: center;">$0.231 \pm 0.014$</td>
<td style="text-align: center;">$0.067 \pm 0.004$</td>
</tr>
</tbody>
</table>
<p>obtain an average success rate of $69 \%$, which is more than double than the PoseCNN models with and without info gain ( $28.4 \%$ and $16.2 \%$ respectively). This shows that using an infogain exploration strategy clearly helps in finding the target object, but the object-centric model is also an important success factor. This acknowledges that representations learnt by actively interacting with objects might be better suited for object understanding than supervised training of 6DOF pose regression on static views.</p>
<p>When we disentangle these features separately for the different objects, shown in Table 1, we observe that the azimuth error is the highest for the object with the most symmetry (i.e. tomato soup can), and the lowest for the object with the least symmetry (i.e. the mustard bottle), while this is not necessarily reflected by the elevation. We also to note that the performance of our SceneCCN on the tomato soup can underperform compared to the other objects. We attribute this to the fact that the CCN model of the tomato soup can was less accurate than the ones of the other objects. This is probably due to the fact that the tomato soup can was rendered in a smaller scale than the others (see Figure 7), which made it harder for the models to accurately infer the position and pose.</p>
<p>Comparison with model-based RL: We also evaluated Latent Explorer Achiever (LEXA) (Mendonca et al., 2021) on a single environment instance with 5 objects in a fixed configuration, but with varying goals to reflect the origi-
nal LEXA setup. LEXA sets goals by randomly sampling them from a replay buffer acquired through exploration. We trained this agent for $\sim 7 \mathrm{M}$ steps $^{1}$ and compare our approach and both PoseCNN approaches on this environment. Qualitative results are shown in Figure 5 and show the goal observation, and the final reached frame for all the baselines as well as for our approach. We can see that the LEXA agent sometimes is able to reach the goal almost perfectly (second and last column) but in other cases, it picks the wrong target object.</p>
<p>The quantitative performance of all approaches in this environment is shown in Table 2. We observe that the LEXA baseline is only able to solve the task in 11 out of 100 cases, while our approach was able to solve this 62 times. PoseCNN was only able to solve it 17 times and 30 times with the added infogain term. Further, we notice the same trends as observed in Table 1. While LEXA performs worse than the PoseCNN baselines in terms of success rate, the range and elevation error are lower. It also has the lowest azimuth error of all approaches.</p>
<p>Exploration vs Exploitation: Finally. we investigate the emerging behavior when the agent does not have a goal image. The function optimized by the agent then becomes</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2. Comparison with LEXA on the single environment LEXA was trained on. 100 goals were randomly selected and used to evaluate the performance of the agents on the success rate ( $\% \mathbf{s}$ ), azimuth error $(\Delta \phi)$, elevation error $(\Delta \theta)$ and range error $(\Delta \mathbf{r})$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">\% s</th>
<th style="text-align: left;">$\Delta \phi$</th>
<th style="text-align: left;">$\Delta \theta$</th>
<th style="text-align: left;">$\Delta \mathbf{r}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PoseCNN</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">1.824</td>
<td style="text-align: left;">0.425</td>
<td style="text-align: left;">0.238</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\pm 0.494$</td>
<td style="text-align: left;">$\pm 0.082$</td>
<td style="text-align: left;">$\pm 0.053$</td>
</tr>
<tr>
<td style="text-align: left;">PoseCNN + Infogain</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">1.195</td>
<td style="text-align: left;">0.460</td>
<td style="text-align: left;">0.201</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\pm 0.398$</td>
<td style="text-align: left;">$\pm 0.118$</td>
<td style="text-align: left;">$\pm 0.057$</td>
</tr>
<tr>
<td style="text-align: left;">SceneCCN + AIF</td>
<td style="text-align: left;">$\mathbf{6 2}$</td>
<td style="text-align: left;">1.050</td>
<td style="text-align: left;">$\mathbf{0 . 1 8 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 7 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\pm 0.470$</td>
<td style="text-align: left;">$\pm 0.083$</td>
<td style="text-align: left;">$\pm 0.039$</td>
</tr>
<tr>
<td style="text-align: left;">LEXA</td>
<td style="text-align: left;">11</td>
<td style="text-align: left;">$\mathbf{1 . 0 2 5}$</td>
<td style="text-align: left;">0.368</td>
<td style="text-align: left;">0.138</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\pm 0.300$</td>
<td style="text-align: left;">$\pm 0.103$</td>
<td style="text-align: left;">$\pm 0.030$</td>
</tr>
</tbody>
</table>
<p>the expected free energy without the instrumental terms, which boils down to the (negative) infogain over the object position (see Equation (6)). In Figure 6, we show this for a particular scene in which the five YCB objects are present. At the first step, it is clear that the belief over object positions has a large variance, but over time the agent iterates and attends to different objects, narrowing the variance over these specific object instances. This can be seen by the camera's trajectory shown in black. We observe that over multiple timesteps the variance over the object position reduces, and the particles mean lies closer to the ground truth position of the objects. In this visualization, we show that driving behavior through the minimization of $G$ actively reduces the ambiguity over the scene.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Five goals in the environment on which the LEXA agent was trained. We show the finally reached frame for all the baselines (PoseCNN, PoseCNN + Infogain, SceneCCN + AIF and LEXA).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. 2D projection of the position belief evolution through the minimizing of the expected free energy $G$ using the SceneCCN generative model. The colors indicate the object over which the belief is formed. The black circles represent the ground truth positions of these objects and the black line shows the trajectory taken by the agent.</p>
<h2>5. Conclusion</h2>
<p>In this paper, we proposed a novel object-centric generative model that builds a representation for scenes with multiple objects using active inference. We developed a new benchmark for evaluating performance on active vision tasks using a pinhole camera agent in a 3D environment with a specific move-to goal. Our approach outperforms both a supervised pose estimation model and an unsupervised model-based RL baseline.</p>
<p>The main limitation of SceneCCN is that we require a single neural network per object type, which will be difficult to scale to a world with hundreds of object categories with even more different appearances. A potential mitigation would be to extend our CCN models to share weights of the different encoders, and/or also infer a latent representation for shape and appearance as proposed by Ferraro et al. (2022).</p>
<p>Compared to slot-attention models, we entail a separate slot per object entity instead of a slot per object instance. This ensures that we require less data to train a single category compared to an end-to-end system that has to learn to map either of the categories to any slot. However, our approach comes the downside that it is more difficult to scale towards scenes with multiple objects of a single instance, or to deal with heavily cluttered environments of similar categories.</p>
<p>Currently, our method only considers static environments. In future work, we plan to also consider object dynamics by adding a dynamics model for each object category that given the history of poses and action could predict the next pose.</p>
<h2>References</h2>
<p>Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J.,</p>
<p>Pieter Abbeel, O., and Zaremba, W. Hindsight experience replay. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings. neurips.cc/paper/2017/file/ 453fadbd8a1a3af50a9df4df899537b5-Paper. pdf.</p>
<p>Bear, D., Fan, C., Mrowca, D., Li, Y., Alter, S., Nayebi, A., Schwartz, J., Fei-Fei, L. F., Wu, J., Tenenbaum, J., and Yamins, D. L. Learning physical graph representations from visual scenes. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 6027-6039. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 4324e8d0d37b110ee1a4f1633ac52df5-Paper. pdf.</p>
<p>Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., and Lerchner, A. MONet: Unsupervised Scene Decomposition and Representation. arXiv:1901.11390 [cs, stat], January 2019. URL http://arxiv.org/abs/1901.11390. arXiv: 1901.11390.</p>
<p>Calli, B., Singh, A., Walsman, A., Srinivasa, S., Abbeel, P., and Dollar, A. M. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 International Conference on Advanced Robotics (ICAR), pp. 510-517, 2015. doi: 10.1109/ICAR.2015.7251504.</p>
<p>Chen, C., Deng, F., and Ahn, S. ROOTS: ObjectCentric Representation and Rendering of 3D Scenes. arXiv:2006.06130 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.06130. arXiv: 2006.06130.</p>
<p>Crawford, E. and Pineau, J. Exploiting Spatial Invariance for Scalable Unsupervised Object Tracking. arXiv:1911.09033 [cs, stat], November 2019. URL http://arxiv.org/abs/1911.09033. arXiv: 1911.09033.</p>
<p>Daucé, E. Active Fovea-Based Vision Through Computationally-Effective Model-Based Prediction. Frontiers in Neurorobotics, 12:76, December 2018. ISSN 1662-5218. doi: 10.3389/fnbot.2018.00076. URL https://www.frontiersin.org/article/ 10.3389/fnbot.2018.00076/full.</p>
<p>Du, G., Wang, K., Lian, S., and Zhao, K. Vision-based Robotic Grasping From Object Localization, Object Pose Estimation to Grasp Estimation for Parallel Grippers: A Review. Artificial Intelligence Review, 54(3):1677-1734, March 2021.</p>
<p>Engelcke, M., Kosiorek, A., Parker Jones, O., and Posner, H. Genesis: Generative scene inference and sampling of object-centric latent representations. OpenReview, 2020.</p>
<p>Eslami, S. M. A., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., and Hinton, G. E. Attend, infer, repeat: Fast scene understanding with generative models. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16, pp. 3233-3241. Curran Associates Inc., 2016a.</p>
<p>Eslami, S. M. A., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., and Hinton, G. E. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models. arXiv:1603.08575 [cs], August 2016b. URL http://arxiv.org/abs/1603.08575. arXiv: 1603.08575.</p>
<p>Eslami, S. M. A., Jimenez Rezende, D., Besse, F., Viola, F., Morcos, A. S., Garnelo, M., Ruderman, A., Rusu, A. A., Danihelka, I., Gregor, K., Reichert, D. P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N., King, H., Hillier, C., Botvinick, M., Wierstra, D., Kavukcuoglu, K., and Hassabis, D. Neural scene representation and rendering. Science, 360(6394):1204-1210, June 2018. ISSN 0036-8075, 1095-9203. doi: 10.1126/ science.aar6170. URL https://www.science. org/doi/10.1126/science.aar6170.</p>
<p>Ferraro, S., Van de Maele, T., Mazzaglia, P., Verbelen, T., and Dhoedt, B. Disentangling shape and pose for objectcentric deep active inference models, 2022. URL https : //arxiv.org/abs/2209.09097.</p>
<p>Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo, G. Active inference: A process theory. Neural Comput., 29(1):1-49, jan 2017. ISSN 0899-7667. doi: 10.1162/NECO_a_00912.</p>
<p>Greff, K., Kaufman, R. L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick, M., and Lerchner, A. Multi-Object Representation Learning with Iterative Variational Inference. arXiv:1903.00450 [cs, stat], July 2020. URL http://arxiv.org/abs/1903. 00450. arXiv: 1903.00450.</p>
<p>Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.</p>
<p>Hafner, D., Lillicrap, T. P., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 2555-2565, 2019. URL http://proceedings. mlr.press/v97/hafner19a.html.</p>
<p>Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020.</p>
<p>Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. Mastering atari with discrete world models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https: //openreview.net/forum?id=0oabwyZbOu.</p>
<p>Hawkins, J., Ahmad, S., and Cui, Y. A Theory of How Columns in the Neocortex Enable Learning the Structure of the World. Frontiers in Neural Circuits, 11:81, October 2017. ISSN 1662-5110. doi: 10.3389/fncir.2017.00081. URL http://journal.frontiersin.org/ article/10.3389/fncir.2017.00081/full.</p>
<p>Hinton, G. E. Some demonstrations of the effects of structural descriptions in mental imagery. Cognitive Science (COGSCI), 3(3):231-250, 1979.</p>
<p>Jaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K. Spatial transformer networks, 2015. URL https://arxiv.org/abs/1506.02025.</p>
<p>James, K. H., Jones, S. S., Smith, L. B., and Swain, S. N. Young children's self-generated object views and object recognition. Journal of Cognition and Development, 15(3):393-401, 2014. doi: 10.1080/15248372. 2012.749481 .</p>
<p>Jiang, J., Janghorbani, S., de Melo, G., and Ahn, S. Scalor: Generative world models with scalable object representations. In Proceedings of ICLR 2020, 2020. URL https: //openreview.net/pdf?id=SJxrKgStDH.</p>
<p>Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., and Girshick, R. B. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. CoRR, abs/1612.06890, 2016. URL http: //arxiv.org/abs/1612.06890.</p>
<p>Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs], January 2017. URL http://arxiv.org/abs/1412. 6980. arXiv: 1412.6980.</p>
<p>Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. arXiv:1312.6114 [cs, stat], May 2014. URL http://arxiv.org/abs/1312.6114. arXiv: 1312.6114 .</p>
<p>Kipf, T., van der Pol, E., and Welling, M. Contrastive learning of structured world models. arXiv preprint arXiv:1911.12247, 2019.</p>
<p>Kosiorek, A. R., Kim, H., Posner, I., and Teh, Y. W. Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects. arXiv:1806.01794 [cs, stat], November 2018. URL http://arxiv.org/abs/1806. 01794. arXiv: 1806.01794.</p>
<p>Kundu, A., Tagliasacchi, A., Mak, A. Y., Stone, A., Doersch, C., Oztireli, C., Herrmann, C., Gnanapragasam, D., Duckworth, D., Rebain, D., Fleet, D. J., Sun, D., Nowrouzezahrai, D., Lagun, D., Pot, E., Zhong, F., Golemo, F., Belletti, F., Meyer, H., Liu, H.-T. D., Laradji, I., Greff, K., Yi, K. M., Beyer, L., Sela, M., Sajjadi, M. S. M., Radwan, N., Sabour, S., Vora, S., Kipf, T., Wu, T., Sitzmann, V., Du, Y., and Miao, Y. (eds.). Kubric: A scalable dataset generator, 2022.</p>
<p>Lanillos, P., Meo, C., Pezzato, C., Meera, A. A., Baioumy, M., Ohata, W., Tschantz, A., Millidge, B., Wisse, M., Buckley, C. L., and Tani, J. Active inference in robotics and artificial agents: Survey and challenges. CoRR, abs/2112.01871, 2021. URL https://arxiv.org/ abs/2112.01871.</p>
<p>Lin, Z., Wu, Y.-F., Peri, S., Fu, B., Jiang, J., and Ahn, S. Improving generative imagination in object-centric world models. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org, 2020.</p>
<p>Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf, T. Object-centric learning with slot attention. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 11525-11538. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 8511df98c02ab60aea1b2356c013bc0f-Paper. pdf.</p>
<p>Mazzaglia, P., Verbelen, T., Dhoedt, B., Lacoste, A., and Rajeswar, S. Choreographer: Learning and adapting skills in imagination. 2022. URL https://openreview. net/forum?id=BxYsP-7ggf.</p>
<p>Mendonca, R., Rybkin, O., Daniilidis, K., Hafner, D., and Pathak, D. Discovering and achieving goals via world models. In Advances in Neural Information</p>
<p>Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 2437924391, 2021. URL https://proceedings. neurips.cc/paper/2021/hash/ cc4af25fa9d2d5c953496579b75f6f6c-Abstract html.</p>
<p>Minaee, S., Boykov, Y. Y., Porikli, F., Plaza, A. J., Kehtarnavaz, N., and Terzopoulos, D. Image segmentation using deep learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1, 2021. doi: 10.1109/TPAMI.2021.3059968.</p>
<p>Mirza, M. B., Adams, R. A., Mathys, C. D., and Friston, K. J. Scene Construction, Visual Foraging, and Active Inference. Frontiers in Computational Neuroscience, 10, June 2016. ISSN 1662-5188. doi: 10.3389/fncom.2016.00056. URL http: //journal.frontiersin.org/Article/10. 3389/fncom.2016.00056/abstract.</p>
<p>Mishkin, M., Ungerleider, L. G., and Macko, K. A. Object vision and spatial vision: two cortical pathways. Trends in Neurosciences, 6:414-417, January 1983. doi: 10.1016/ 0166-2236(83)90190-x. URL https://doi.org/ 10.1016/0166-2236(83)90190-x.</p>
<p>Parr, T., Rees, G., and Friston, K. J. Computational neuropsychology and bayesian inference. Frontiers in Human Neuroscience, 12, 2018. ISSN 1662-5161. doi: 10.3389/fnhum.2018.00061. URL https://www.frontiersin.org/articles/ 10.3389/fnhum.2018.00061.</p>
<p>Parr, T., Sajid, N., Da Costa, L., Mirza, M. B., and Friston, K. J. Generative Models for Active Vision. Frontiers in Neurorobotics, 15:651432, April 2021. ISSN 1662-5218. doi: 10.3389/fnbot.2021.651432. URL https://www.frontiersin.org/articles/ 10.3389/fnbot.2021.651432/full.</p>
<p>Parr, T., Pezzulo, G., and Friston, K. J. Active Inference: The Free Energy Principle in Mind, Brain, and Behavior. The MIT Press, March 2022. ISBN 978-0-262-36997-8. doi: 10.7551/mitpress/12441. 001.0001. URL https://doi.org/10.7551/ mitpress/12441.001.0001.</p>
<p>Pio-Lopez, L., Kuchling, F., Tung, A., Pezzulo, G., and Levin, M. Frontiers in Computational Neuroscience, 16, 2022. ISSN 1662-5188. doi: 10.3389/fncom. 2022. 988977. URL https://www.frontiersin.org/ articles/10.3389/fncom.2022.988977.</p>
<p>Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic Backpropagation and Approximate Inference in</p>
<p>Deep Generative Models. arXiv:1401.4082 [cs, stat], May 2014. URL http://arxiv.org/abs/1401. 4082. arXiv: 1401.4082.</p>
<p>Sajjadi, M. S. M., Duckworth, D., Mahendran, A., van Steenkiste, S., Pavetić, F., Lučić, M., Guibas, L. J., Greff, K., and Kipf, T. Object scene representation transformer. In Advances in Neural Information Processing Systems, 2022a.</p>
<p>Sajjadi, M. S. M., Meyer, H., Pot, E., Bergmann, U., Greff, K., Radwan, N., Vora, S., Lucic, M., Duckworth, D., Dosovitskiy, A., Uszkoreit, J., Funkhouser, T., and Tagliasacchi, A. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. CVPR, 2022b. URL https://srt-paper.github.io/.</p>
<p>Smith, E. J., Meger, D., Pineda, L., Calandra, R., Malik, J., Romero, A., and Drozdzal, M. Active 3d shape reconstruction from vision and touch. CoRR, abs/2107.09584, 2021. URL https://arxiv.org/ abs/2107.09584.</p>
<p>Smith, R., Badcock, P., and Friston, K. J. Recent advances in the application of predictive coding and active inference models within clinical neuroscience. Psychiatry and Clinical Neurosciences, 75(1):3-13, September 2020. doi: 10.1111/pcn.13138. URL https: //doi.org/10.1111/pcn. 13138.</p>
<p>Sucar, E., Wada, K., and Davison, A. Nodeslam: Neural object descriptors for multi-view shape reconstruction. In 2020 International Conference on 3D Vision (3DV), pp. 949-958, nov 2020. doi: 10.1109/3DV50981.2020. 00105 .
van Bergen, R. S. and Lanillos, P. L. Object-based active inference, 2022. URL https://arxiv.org/abs/ 2209.01258.</p>
<p>Van de Maele, T., Verbelen, T., Çatal, O., and Dhoedt, B. Embodied Object Representation Learning and Recognition. Frontiers in Neurorobotics, 16:840658, April 2022. ISSN 1662-5218. doi: 10.3389/fnbot.2022.840658. URL https://www.frontiersin.org/articles/ 10.3389/fnbot.2022.840658/full.</p>
<p>Veerapaneni, R., Co-Reyes, J. D., Chang, M., Janner, M., Finn, C., Wu, J., Tenenbaum, J., and Levine, S. Entity abstraction in visual model-based reinforcement learning. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (eds.), Proceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pp. 1439-1456. PMLR, 30 Oct-01 Nov 2020. URL https://proceedings.mlr.press/ v100/veerapaneni20a.html.</p>
<p>Watters, N., Matthey, L., Bosnjak, M., Burgess, C. P., and Lerchner, A. COBRA: data-efficient model-based RL through unsupervised object discovery and curiositydriven exploration. CoRR, abs/1905.09275, 2019. URL http://arxiv.org/abs/1905.09275.</p>
<p>Wu, Z., Song, S., Khosla, A., Zhang, L., Tang, X., and Xiao, J. 3d shapenets: A deep representation for volumetric shape modeling. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, USA, June 2015.</p>
<p>Xiang, Y., Schmidt, T., Narayanan, V., and Fox, D. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Robotics: Science and Systems (RSS), 2018a.</p>
<p>Xiang, Y., Schmidt, T., Narayanan, V., and Fox, D. PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes. arXiv:1711.00199 [cs], May 2018b. URL http://arxiv.org/abs/1711.00199. arXiv: 1711.00199.</p>
<p>Yan, X., Yuan, Z., Du, Y., Liao, Y., Guo, Y., Li, Z., and Cui, S. Clevr3d: Compositional language and elementary visual reasoning for question answering in 3d realworld scenes, 2021. URL https://arxiv.org/ abs/2112.11691.</p>
<h1>A. The Generative Model and Variational Free Energy</h1>
<p>The generative model shown in Figure 1 can be formalized as:</p>
<p>$$
\begin{aligned}
P(\mathbf{s}, \tilde{\mathbf{o}}, \tilde{\mathbf{a}})= &amp; P\left(\mathbf{v}<em t="1">{0}\right) \prod</em>}^{T} P\left(\mathbf{v<em t-1="t-1">{t} \mid \mathbf{v}</em>}, \mathbf{a<em t-1="t-1">{t-1}\right) P\left(\mathbf{a}</em>\right) \
&amp; \prod_{k} P\left(\mathbf{o}<em k_="k," t="t">{t} \mid \mathbf{o}</em>}, \mathbf{u<em k_="k," t="t">{k, t}, \sigma</em>}\right) P\left(\mathbf{o<em k="k">{k, t} \mid \tilde{\mathbf{i}}</em>}, \mathbf{p<em k_="k," t="t">{k, t}\right) P\left(\mathbf{p}</em>} \mid \mathbf{p<em t-1="t-1">{k, t-1}, \mathbf{a}</em>}\right) P\left(\mathbf{u<em k="k">{k, t} \mid \mathbf{t}</em>}, \mathbf{v<em k_="k," t="t">{t}\right) P\left(\sigma</em>} \mid \mathbf{t<em t="t">{k}, \mathbf{v}</em>}\right) P\left(\tilde{\mathbf{i}<em k="k">{k}\right) P\left(\mathbf{t}</em>\right)
\end{aligned}
$$</p>
<p>where the tilde represents a sequence of the variable over time, and $P(\mathbf{s})=\prod_{k} P\left(\tilde{\mathbf{i}}<em k="k">{k}, \mathbf{t}</em>\right)$.
The approximate posterior $Q(\mathbf{s} \mid \tilde{\mathbf{o}})$ is factorized through the following mean field approximation:}, \tilde{\mathbf{p}}_{k</p>
<p>$$
Q(\mathbf{s} \mid \tilde{\mathbf{o}})=\prod_{t} \prod_{k} Q\left(\mathbf{u}<em t="t">{k, t} \mid \mathbf{o}</em>}\right) Q\left(\sigma_{k, t} \mid \mathbf{o<em k="k">{t}\right) Q\left(\mathbf{t}</em>} \mid \mathbf{u<em k_="k," t="t">{k, t}, \sigma</em>}\right) Q\left(\mathbf{p<em k_="k," t="t">{k, t} \mid \mathbf{o}</em>}\right) Q\left(\tilde{\mathbf{i}<em k_="k," t="t">{k} \mid \mathbf{o}</em>\right)
$$</p>
<p>For the generative model and approximate posterior described above, the free energy $F$ is defined as:</p>
<p>$$
\begin{aligned}
F= &amp; \mathbb{E}<em t="t">{Q(\mathbf{s} \mid \tilde{\mathbf{o}})}[\log Q(\mathbf{s} \mid \tilde{\mathbf{o}})-\log P(\mathbf{s}, \tilde{\mathbf{o}}, \tilde{\mathbf{a}})] \
= &amp; \sum</em>} \sum_{k} \underbrace{\mathbb{E<em t="t">{Q(\mathbf{s} \mid \tilde{\mathbf{o}})}\left[-\log P\left(\mathbf{v}</em>} \mid \mathbf{v<em t-1="t-1">{t-1}, \mathbf{a}</em>}\right)-\log P\left(\mathbf{a<em _Constant="{Constant" _text="\text">{t-1}\right)\right]}</em> \
&amp; +\underbrace{D_{K L}\left[Q\left(\mathbf{p}}<em k_="k," t="t">{k, t} \mid \mathbf{o}</em>}\right) | P\left(\mathbf{p<em k_="k," t-1="t-1">{k, t} \mid \mathbf{p}</em>}, \mathbf{a<em>{t-1}\right)\right)\right]}</em>{\text {Pose Transition Model<em> }} \
&amp; +\underbrace{D_{K L}\left[Q\left(\tilde{\mathbf{i}}<em k_="k," t="t">{k} \mid \mathbf{o}</em>}\right) | P\left(\tilde{\mathbf{i}<em>{k}\right)\right]}</em>{\text {Object category classification</em><em> }} \
&amp; +\underbrace{D_{K L}\left[Q\left(\sigma_{k, t} \mid \mathbf{o}<em k_="k," t="t">{t}\right) | P\left(\sigma</em>} \mid \mathbf{t<em t="t">{t}, \mathbf{v}</em>}\right)\right]+D_{K L}\left[Q\left(\mathbf{u<em t="t">{k, t} \mid \mathbf{o}</em>}\right) | P\left(\mathbf{u<em t="t">{k, t} \mid \mathbf{t}</em>}, \mathbf{v<em>{t}\right)\right]}</em>{\text {Object-centric cropping</em><em> }} \
&amp; +\underbrace{D_{K L}\left[Q\left(\mathbf{t}<em t="t">{k} \mid \mathbf{o}</em>}\right) | P\left(\mathbf{t<em>{k}\right)\right]}</em>{\text {Object positions</em>*<em> }} \
&amp; +\underbrace{\mathbb{E}<em k_="k," t="t">{Q(\mathbf{s} \mid \tilde{\mathbf{o}})}\left[-\log P\left(\mathbf{o}</em>} \mid \tilde{\mathbf{i}<em k_="k," t="t">{t}, \mathbf{p}</em>_{\text {Object-centric observation likelihood}\right)\right)\right]</em> }} \
&amp; +\underbrace{\mathbb{E}<em t="t">{Q(\mathbf{s} \mid \tilde{\mathbf{o}})}\left[-\log P\left(\mathbf{o}</em>} \mid \mathbf{o<em k_="k," t="t">{k, t}, \mathbf{u}</em>
\end{aligned}
$$}, \sigma_{k, t}\right)\right]}_{\text {Full observation likelihood }</p>
<p>This decomposes in a number of terms that are optimized in our two training phases. In the first phase, we train object-centric CCNs, which jointly optimize an object-centric likelihood model $p_{\theta}\left(\mathbf{o}<em k="k">{k, t} \mid \tilde{\mathbf{i}}</em>}, \mathbf{p<em _theta="\theta">{k, t}\right)$, a pose transition model $p</em>}\left(\mathbf{p<em k_="k," t-1="t-1">{k, t} \mid \mathbf{p}</em>}, \mathbf{a<em _phi="\phi">{t-1}\right)$ and an approximate pose posterior model $q</em>}\left(\mathbf{p<em k_="k," t="t">{k, t} \mid \mathbf{o}</em>\right)$ covering the terms denoted with <em>. In the second phase, we additionally train a model outputting the object category $q_{\phi}\left(\tilde{\mathbf{i}}<em k_="k," t="t">{k} \mid \mathbf{o}</em>}\right)$ as well as a model outputting object-centric crop parameters $q_{\phi}\left(\mathbf{u<em k_="k," t="t">{k, t}, \sigma</em>^{} \mid \mathbf{o}_{t}\right)$. This is done by optimizing the terms denoted with ${ </em> <em>}$, and noting that generating bounding box parameters $\mathbf{u}<em k_="k," t="t">{k, t}$ and $\sigma</em>^{}$ from object position $\mathbf{t}$ and camera viewpoint $\mathbf{v}$ is determined by the pinhole camera model. Finally, the posterior over positions (denoted with ${ </em> * *}$ ) is implemented using a particle filter, and has no parameters to optimize. In a similar vein, the remaining terms have no influence over the learnable parameters or are constant terms under the expectation.</p>
<p>In practice, optimizing the negative log likelihood terms is equivalent to minimizing the mean squared error between the reconstructed observation and the ground truth observation. Similarly, optimizing the KL-divergence for a Bernoulli variable is equivalent to minimizing the binary cross entropy error over this variable. For this reason, we implemented the loss using</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. A few examples from the object-centric dataset (left) and the augmented dataset (right).
these terms. Instead of regressing the bounding box parameters, we opted for a spatial transformer that directly transforms the input image into an object-centric crop from these parameters, and add a supervised loss to the intermediately computed object-specific mask as this resulted in more stable results.</p>
<h1>B. Object-Centric Datasets</h1>
<p>For each of the considered objects in the YCB dataset, a dataset is created consisting of observation-viewpoint pairs. In this work we consider the following objects: master chef can, cracker box, sugar box, tomato soup can, and mustard bottle.</p>
<p>We only consider observing objects from a fixed distance, as this will allow an agent to use the scale of an object for estimating the distance to the object. We then sample all observations from a fixed distance of 40 cm from the object center. Points are sampled uniformly on a sphere around around the object center, considering a fixed radius of 40 cm . Both the RGB observation and the viewpoints are recorded. For each object category, a dataset of 500 samples is created. Some samples from this dataset are shown on the left of Figure 7.</p>
<p>For training the second phase that considers scene (not object-centric) observations, we do not create a novel dataset. Instead, we augment and combine our object-centric observations to lie in a similar distribution to full scenes. To this end, we randomly scale and translate the object in the observation to simulate a sense of translation. Then two randomly oriented rectangles in different colors are used as a background, which increases robustness to different environments. On top of this, we also add a different random object from one of the other categories that is also scaled and translated according to the same distribution as the main object. This sometimes occludes the object of interest and makes our model more robust. Finally, to add augmentation for training the object identity prediction, in $20 \%$ of the cases we remove the object from the observation. Some samples from this dataset are shown in Figure 7.</p>
<h2>C. Neural Network Parameters</h2>
<p>There are multiple neural networks working together for the approximate inference of the hidden variables. When the agent observes the scene, first a fully convolutional neural network estimates which pixels belong to the object of category k , this is the observation mask $\alpha_{k, t}$. The parameterization of this layer is shown in Table 3.</p>
<p>The observation is then masked using a pixel-wise product $\mathbf{o}<em k_="k," t="t">{t} \odot \alpha</em>}$ and further processed to predict object identity $\mathbf{i<em k_="k," t="t">{k}$, i.e. whether the object of interest is present in the observation. This neural network predicts the parameters for a spatial transformer, i.e. the center of the object in pixel space and the object scale directly. Its parameterization is shown in Table 4. This directly predicts the scale and normalized center of the object of interest. To get the predicted scale in a range for the model to optimize we scale and invert this output $h$ of the neural network using the following transform: $\sigma</em>=1 /(3.8 \cdot h+0.2)$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Table 3. configuration of the Masking Neural Network.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Layer</td>
<td style="text-align: center;">Output Neurons/Filters</td>
</tr>
<tr>
<td style="text-align: left;">Interpolate to 64x64</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (1x1) + Leaky ReLU</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (3x3) + Leaky ReLU</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (3x3) + Leaky ReLU</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (3x3)</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Median Pool (5x5)</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Sigmoid</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Interpolate to 480x480</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Table 4. Configuration of the Spatial Transformer Network.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Layer</td>
<td style="text-align: center;">Output Neurons/Filters</td>
</tr>
<tr>
<td style="text-align: left;">Interpolate to 32x32</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Flatten</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Linear + Leaky ReLU</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">Linear + Leaky ReLU</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Linear + Leaky ReLU</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Linear + ELU</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Using the output of the spatial transformer network, the masked observation can be processed into an object-centric observation after which this is processed by two neural networks. One predicts the object identity $\mathbf{i}_{k}$ or whether the object of interest is present in the observation. The parameterization is shown in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Table 5. Configuration of the Identity Estimation Network.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Layer</td>
<td style="text-align: center;">Output Neurons/Filters</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (4x4) + Leaky ReLU</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (4x4) + Leaky ReLU</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (4x4) + Leaky ReLU</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Convolutional (4x4) + Leaky ReLU</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Flatten</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Linear + Sigmoid</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Finally, a CCN is able to estimate a latent variable describing the object pose. This model consists of three neural networks. First, an encoder takes an object-centric observation as an input and predicts the parameters of a multivariate Gaussian with 8 dimensions and with a diagonal covariance matrix. This variable represents the object pose in an object-centric reference frame. The details of this model can be found in Table 6. The final output of this model is a vector of twice the latent size, representing the mean and standard deviation of the multivariate Gaussian. A decoder or likelihood model takes as input a sample from this pose distribution and decodes it into a pixel-based observation. Using the transition model, the distribution over the pose can be estimated for future viewpoints. The model takes as input the concatenated vector of a sample from the current belief over pose and the action. The action in this case is a two-dimensional vector representing the displacement in azimuth and elevation, as we only consider observations from a fixed distance.</p>
<p>These neural networks are trained in two distinct steps, using the same object-centric dataset of 500 observation-viewpoint pairs per object. Drawing inspiration from object-centric learning in infants (Smith et al., 2021), the first phase focuses on learning object-centric representations. While the second phase focuses on learning to decompose the scene into object-centric representations. In the first phase, the CCN models is optimized on the $\mathcal{L}<em 2="2">{1}$-loss, described in Equation 4. This is done using the Adam (Kingma \&amp; Ba, 2017) optimizer with the configuration parameters shown in Table 9. In the second phase, the other models are optimized by minimizing the $\mathcal{L}</em>$-loss (Equation 5), again using the Adam (Kingma \&amp; Ba, 2017)</p>
<p>Table 6. Configuration of the CCN Encoder.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Layer</th>
<th style="text-align: center;">Output Neurons/Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Convolution (4x4) + Leaky ReLU</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (4x4) + Leaky ReLU</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (4x4) + Leaky ReLU</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (4x4) + Leaky ReLU</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Flatten</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Linear</td>
<td style="text-align: center;">$2 \cdot 8$</td>
</tr>
<tr>
<td style="text-align: left;">Softplus for $\sigma$ output</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 7. Configuration of the CCN Decoder.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Layer</th>
<th style="text-align: center;">Output Neurons/Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Linear</td>
<td style="text-align: center;">4096</td>
</tr>
<tr>
<td style="text-align: left;">Unflatten to 64x8x8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (5x5) + Leaky ReLU</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Interpolate to 17x17</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (5x5) + Leaky ReLU</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Interpolate to 35x35</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (6x6) + Leaky ReLU</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Interpolate to 69x69</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (6x6) + Leaky ReLU</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Convolution (1x1)</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>optimizer, but now with a learning rate of $1 \cdot 10^{-4}$.</p>
<h1>D. Details on Active Agents</h1>
<p>When we consider the agent driven through expected free energy. A new target viewpoint $\mathbf{v}_{t+1}$ is computed every 10 steps, or unless the target is reached.</p>
<p>When estimating the 3D multivariate Gaussian, the mean is placed at the estimated depth along the negative z-direction of the camera pose in OpenGL format. The depth is acquired using the following relation with the scale: $d=0.4 / \sigma_{k, t}$, as observations are trained for a distance $d=0.4$. The variance is then set at a value of $0.1973 / 2$ along the depth dimension, and 0.02 along the other dimensions.</p>
<p>The particle filters for estimating the position of the individual objects are initialized with 10k particles. At each step, the particles are resampled with a standard deviation of $0.025 \mathrm{~cm}$. When the agent does not observe the object in question, all particles that are in view of the camera and closer than the nearest (estimated) object, are set to a low value of $10^{-5}$ before normalization.</p>
<p>For the implementation, the reader is referred to the supplementary materials.</p>
<p>Table 8. Configuration of the CCN Transition Model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Layer</th>
<th style="text-align: center;">Output Neurons/Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Linear + Leaky ReLU</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Linear + Leaky ReLU</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">Linear + Leaky ReLU</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">Linear</td>
<td style="text-align: center;">$2 \cdot 8$</td>
</tr>
<tr>
<td style="text-align: left;">Softplus for $\sigma$ output</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 9. Hyperparameters for optimization of the CCN in the first stage of training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Initial Value</th>
<th style="text-align: center;">Range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\lambda_{1}$</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$[80,100]$</td>
</tr>
<tr>
<td style="text-align: left;">$\lambda_{2}$</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$[80,100]$</td>
</tr>
<tr>
<td style="text-align: left;">$\lambda_{3}$</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$[80,100]$</td>
</tr>
<tr>
<td style="text-align: left;">$\lambda_{4}$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">fixed</td>
</tr>
<tr>
<td style="text-align: left;">Adjust frequency</td>
<td style="text-align: center;">500 steps</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Adjust factor</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$5 \cdot 10^{-4}$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h1>E. Additional Results</h1>
<p>All approaches are evaluated on a set of ten goals from the environment in which the LEXA agent was trained. The qualitative results can be observed in Figure 8. The goal observation is displayed in the top row, while the other goals show the final reached frame of the agent with a maximum of 350 steps.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Qualitative results of the scene in which LEXA was trained. In this configuration, five objects are randomly placed on a table, and the agent must reach a goal observation. The top row shows the goal observation, while the other rows show the final reached observation for each agent. The simulation is stopped after 350 steps.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For sample-efficiency comparison, we trained SceneCCN using 500k samples per object, for a total of 2.5 M frames on the augmented dataset.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
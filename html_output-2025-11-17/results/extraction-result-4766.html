<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4766 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4766</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4766</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-264833001</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.00694v2.pdf" target="_blank">Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer. Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset. Code will be released at https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4766.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4766.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought sampling with GPT-3.5 (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach that samples many detailed low-level chain-of-thought reasoning traces from GPT-3.5 and uses majority voting to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 family model (version gpt-3.5-turbo-0613) used as the low-level follower to generate detailed chain-of-thought reasoning traces via sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) sampling + Majority Voting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Repeatedly sample detailed low-level step-by-step reasoning chains for a question from the same LLM prompt distribution (no explicit high-level tactic conditioning), then aggregate answers using majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard subset of MATH dataset (Level-5); 140 questions sampled evenly across categories for cost-limited evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>final-answer accuracy 22.14% (majority voting over 64 CoT samples)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Hierarchical (retrieval) + tournament: 27.85% (same evaluation set, GPT-3.5 as follower)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT sampling attains modest recall of correct answers as the number of samples increases, but majority-voted final accuracy plateaus; when compared with hierarchical (diverse) exploration, CoT sampling yields lower grouped-majority recall and lower final-answer accuracy for GPT-3.5 on MATH Level-5.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Tree-of-Thoughts (ToT) as implemented produced substantially fewer chains (≈8.41 on average) and underperformed CoT sampling; CoT majority voting fails to surface correct but rare solution modes that hierarchical grouping can reveal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4766.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4766.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought sampling with GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach that samples many low-level chain-of-thought reasoning traces from GPT-4 and uses majority voting to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 model (version gpt-4-0613) used as the low-level follower to generate detailed chain-of-thought reasoning traces via sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) sampling + Majority Voting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Repeatedly sample detailed low-level step-by-step reasoning chains from GPT-4 without conditioning on diverse high-level tactics; aggregate answers with majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard subset of the MATH dataset (Level-5); 140 questions sampled evenly across categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>final-answer accuracy 40.89% (majority voting over 64 CoT samples)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Hierarchical (prompt-hints) + tournament: 44.29% (same evaluation set, GPT-4 as follower)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 CoT sampling is substantially stronger than GPT-3.5 but still benefits from hierarchical/diverse high-level tactic conditioning; hierarchical prompt-based hints increased grouped-majority recall and final-answer accuracy over plain CoT sampling for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>When retrieval-based high-level hints were used, gains were smaller for GPT-4 compared to prompt-based hints (retrieval: ≈51.36% grouped-majority recall vs prompt: 54.46%), indicating that the form of diversity matters and interaction with follower strength is important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4766.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4766.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hier-Prompt-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical policy (prompt-based high-level tactics) with GPT-3.5 follower</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diverse hierarchical approach where a high-level leader (prompted LLM) proposes concise technique hints and GPT-3.5 (follower) generates multiple low-level chains per hint; groups are then selected via a tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (follower); GPT-4 sometimes used as leader/selector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Follower: GPT-3.5 (gpt-3.5-turbo-0613) generates m = 16 chains per provided high-level hint; leader: GPT-4 or same LLM prompted to propose up to n = 4 high-level tactics (prompt-based).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Hierarchical policy — Prompt-for-tactics (leader) + grouped sampling + tournament selection</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Leader prompts produce n concise, diverse high-level techniques/hints; for each hint the follower samples multiple low-level reasoning chains producing n groups; group-majorities are computed and a small tournament (pairwise comparisons using GPT-4 or GPT-3.5) selects the final reasoning chain/answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard MATH problems used to evaluate exploring distinct high-level strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>final-answer accuracy ≈ 25.00% (tournament selection; GPT-3.5 follower, prompt-hints), grouped-majority recall 32.69%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT sampling baseline: final-answer accuracy 22.14% ; grouped-majority recall 31.19%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For GPT-3.5 follower, prompt-based hierarchical exploration yields modest improvements in grouped-majority recall and final-answer accuracy over CoT sampling, demonstrating improved discovery/visibility of correct solutions when high-level tactic diversity is introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Prompt-based hints underperform retrieval-based hints when the follower is weaker (GPT-3.5); failure modes include the follower ignoring or failing to exploit the high-level hints, leading to limited gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4766.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4766.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hier-Prompt-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical policy (prompt-based high-level tactics) with GPT-4 follower</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diverse hierarchical approach where a high-level leader proposes concise technique hints and GPT-4 generates multiple low-level chains per hint; a tournament selects the final chain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as the low-level follower; leader prompts (also GPT-4) produce up to n = 4 high-level tactics; follower generates m = 16 chains per tactic; tournament performed (usually with GPT-4) to select final chain.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Hierarchical policy — Prompt-for-tactics (leader) + grouped sampling + tournament selection</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Leader is prompted (few-shot/zero-shot) to propose diverse concise techniques; follower conditioned on each technique samples multiple detailed chains per tactic; groups are reduced via group-majority and a tournament of pairwise comparisons selects the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging math reasoning problems requiring creative high-level strategy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>final-answer accuracy 44.29% (tournament selection with hierarchical prompt-hints), grouped-majority recall 54.46%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT sampling baseline: final-answer accuracy 40.89%, grouped-majority recall 50.78%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When the follower is strong (GPT-4), prompt-based high-level tactic generation yields larger gains: higher grouped-majority recall and improved final-answer accuracy compared to CoT sampling; GPT-4 follower better understands and exploits concise technique hints.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Retrieved problem-solution hints (retrieval-based hierarchy) performed slightly worse than prompt-based hints for GPT-4 in grouped-majority recall and final accuracy, showing that the nature of the high-level hints matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4766.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4766.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hier-Retrieval-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical policy (retrieved problem-solution hints) with GPT-3.5 follower</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diverse hierarchical approach where a SentenceBERT retrieval returns similar problems and solutions as high-level guidance; GPT-3.5 then generates multiple low-level chains per retrieved hint group.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (follower); SentenceBERT for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Follower: GPT-3.5 (gpt-3.5-turbo-0613). High-level hints are generated by retrieving n similar problems+solutions using a sentence-embedding model (SentenceBERT), and those are used as in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Hierarchical policy — Retrieval-of-similar QA + grouped sampling + tournament selection</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>For each retrieved similar problem and its solution, the follower is prompted to follow analogous tactics and produces multiple low-level reasoning chains per retrieved exemplar; groups are aggregated and a tournament selects the final chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset) and larger 1047-question set (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging math reasoning problems; retrieval provides concrete solved examples as high-level guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>final-answer accuracy 27.85% (tournament selection; GPT-3.5 follower, retrieval-based hints) ; grouped-majority recall 34.23%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT sampling baseline: final-answer accuracy 22.14% ; grouped-majority recall 31.19%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-based high-level hints produced the largest gains for the weaker follower (GPT-3.5), improving both grouped-majority recall and final-answer accuracy more than prompt-based hints, suggesting concrete retrieved demonstrations are easier for a weaker follower to exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For the stronger follower (GPT-4), retrieval-based hints were less advantageous than concise prompt-based techniques; some retrieved examples/hints can be irrelevant and mislead the follower if not carefully filtered.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4766.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4766.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hier-Retrieval-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical policy (retrieved problem-solution hints) with GPT-4 follower</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diverse hierarchical approach using retrieved problem-solution exemplars as high-level guidance, with GPT-4 generating multiple chains per retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-0613 (follower); SentenceBERT for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 follower (gpt-4-0613); high-level hints are retrieved similar problems + solutions via SentenceBERT and used as in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Hierarchical policy — Retrieval-of-similar QA + grouped sampling + tournament selection</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Retrieved solved problems are presented as high-level guidance; GPT-4 generates multiple low-level chains per retrieved exemplar; groups are majority-aggregated and a tournament selects the final chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard math problems where retrieved solved examples can encourage strategy transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>final-answer accuracy 42.14% (tournament selection; GPT-4 follower, retrieval-based hints) ; grouped-majority recall ≈ 51.36%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT sampling baseline: final-answer accuracy 40.89% ; grouped-majority recall 50.78%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-based hierarchical hints improve visibility and answer accuracy for GPT-4 versus CoT baseline, but are slightly less effective than concise prompt-based tactic hints for GPT-4; the effectiveness of hint type depends on follower capability.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Retrieved similar exemplars occasionally caused the follower to follow incorrect analogies or produce consistent but incorrect answers; tournament-based selection mitigates but does not eliminate such failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4766.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4766.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT) implemented with GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Implementation of Tree-of-Thoughts breadth-first search with GPT-3.5 as the reasoning engine; included as a comparative method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613) used to expand nodes in a ToT search; BFS expansion with branching/beam constraints as in the referenced ToT method.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT) BFS search + Voting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Perform tree-based search over intermediate reasoning states (ToT), expanding multiple child thoughts per node and keeping a beam of candidates; final answers aggregated by voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging math problems; ToT designed to search over intermediate thoughts rather than sampling flat chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>final-answer accuracy 13.45% (ToT + Voting with GPT-3.5 on the 140-question set)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT sampling baseline: 22.14% (same evaluation set and model), Hierarchical retrieval: 27.85%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The implemented ToT produced far fewer total reasoning chains on average (~8.41 chains) than CoT sampling (64 chains) and underperformed the CoT baseline in this setup; limited chain budget / fewer end candidates harmed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>ToT underperformed here — authors attribute this to producing far fewer final chains (search budget) relative to flat sampling, suggesting that naive ToT instantiations can be less effective under tight computation/sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4766.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4766.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tournament-Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tournament-based reasoning-chain selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Efficient selection procedure that compares group-majority reasoning chains pairwise (with LLM comparisons) to select a final high-quality reasoning chain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 or GPT-3.5 used as the selector (authors report GPT-4 as stronger selector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Tournament comparisons are performed by prompting an LLM (GPT-4 preferred) to compare two candidate reasoning chains and decide which is better; repeated pairwise comparisons (k repeats) are optionally majority voted.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tournament-based selection (pairwise LLM comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>From each high-level hint group, pick a representative chain that matches the group-majority, build a small set S of n candidates, then run n-1 pairwise GPT comparisons (the 'tournament') to find the best chain; uses far fewer LLM calls than exhaustive verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH Level-5 (140-question evaluation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selection mechanism applied after diverse-group chain generation to choose the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When used with hierarchical retrieval + GPT-3.5 follower, tournament selection (GPT-4 as selector, k=1) increased final-answer accuracy from 22.14% (CoT majority) to 27.85% on the 140-question set; selection temperature had little effect for 0<T<1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Majority voting over all samples: 22.14% (CoT baseline with GPT-3.5 on same set); majority voting over groups yields smaller gains than tournament when combined with hierarchical methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tournament selection effectively filters out misleading tactic-specific consistency (groups that are consistently wrong) by pairwise LLM comparison, improving final-answer accuracy with very few extra LLM calls; GPT-4 is especially effective as the selector even with k=1.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Tournament selection alone does not make CoT baseline beat hierarchical approaches — hierarchical diversity is an essential component; tournament's gains depend on having diverse high-quality groups to choose from.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 1)</em></li>
                <li>Reasoning and Exploration with Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4766",
    "paper_id": "paper-264833001",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "CoT-GPT3.5",
            "name_full": "Chain-of-Thought sampling with GPT-3.5 (gpt-3.5-turbo-0613)",
            "brief_description": "Baseline approach that samples many detailed low-level chain-of-thought reasoning traces from GPT-3.5 and uses majority voting to produce the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "OpenAI GPT-3.5 family model (version gpt-3.5-turbo-0613) used as the low-level follower to generate detailed chain-of-thought reasoning traces via sampling.",
            "reasoning_method_name": "Chain-of-Thought (CoT) sampling + Majority Voting",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Repeatedly sample detailed low-level step-by-step reasoning chains for a question from the same LLM prompt distribution (no explicit high-level tactic conditioning), then aggregate answers using majority voting.",
            "task_name": "MATH Level-5 (140-question evaluation subset)",
            "task_description": "Hard subset of MATH dataset (Level-5); 140 questions sampled evenly across categories for cost-limited evaluation.",
            "performance": "final-answer accuracy 22.14% (majority voting over 64 CoT samples)",
            "comparison_with_other_method": true,
            "performance_other_method": "Hierarchical (retrieval) + tournament: 27.85% (same evaluation set, GPT-3.5 as follower)",
            "key_findings": "CoT sampling attains modest recall of correct answers as the number of samples increases, but majority-voted final accuracy plateaus; when compared with hierarchical (diverse) exploration, CoT sampling yields lower grouped-majority recall and lower final-answer accuracy for GPT-3.5 on MATH Level-5.",
            "counter_examples_or_negative_results": "Tree-of-Thoughts (ToT) as implemented produced substantially fewer chains (≈8.41 on average) and underperformed CoT sampling; CoT majority voting fails to surface correct but rare solution modes that hierarchical grouping can reveal.",
            "uuid": "e4766.0",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CoT-GPT4",
            "name_full": "Chain-of-Thought sampling with GPT-4 (gpt-4-0613)",
            "brief_description": "Baseline approach that samples many low-level chain-of-thought reasoning traces from GPT-4 and uses majority voting to produce the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-0613",
            "model_description": "OpenAI GPT-4 model (version gpt-4-0613) used as the low-level follower to generate detailed chain-of-thought reasoning traces via sampling.",
            "reasoning_method_name": "Chain-of-Thought (CoT) sampling + Majority Voting",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Repeatedly sample detailed low-level step-by-step reasoning chains from GPT-4 without conditioning on diverse high-level tactics; aggregate answers with majority voting.",
            "task_name": "MATH Level-5 (140-question evaluation subset)",
            "task_description": "Hard subset of the MATH dataset (Level-5); 140 questions sampled evenly across categories.",
            "performance": "final-answer accuracy 40.89% (majority voting over 64 CoT samples)",
            "comparison_with_other_method": true,
            "performance_other_method": "Hierarchical (prompt-hints) + tournament: 44.29% (same evaluation set, GPT-4 as follower)",
            "key_findings": "GPT-4 CoT sampling is substantially stronger than GPT-3.5 but still benefits from hierarchical/diverse high-level tactic conditioning; hierarchical prompt-based hints increased grouped-majority recall and final-answer accuracy over plain CoT sampling for GPT-4.",
            "counter_examples_or_negative_results": "When retrieval-based high-level hints were used, gains were smaller for GPT-4 compared to prompt-based hints (retrieval: ≈51.36% grouped-majority recall vs prompt: 54.46%), indicating that the form of diversity matters and interaction with follower strength is important.",
            "uuid": "e4766.1",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Hier-Prompt-GPT3.5",
            "name_full": "Hierarchical policy (prompt-based high-level tactics) with GPT-3.5 follower",
            "brief_description": "Diverse hierarchical approach where a high-level leader (prompted LLM) proposes concise technique hints and GPT-3.5 (follower) generates multiple low-level chains per hint; groups are then selected via a tournament.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613 (follower); GPT-4 sometimes used as leader/selector",
            "model_description": "Follower: GPT-3.5 (gpt-3.5-turbo-0613) generates m = 16 chains per provided high-level hint; leader: GPT-4 or same LLM prompted to propose up to n = 4 high-level tactics (prompt-based).",
            "reasoning_method_name": "Hierarchical policy — Prompt-for-tactics (leader) + grouped sampling + tournament selection",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Leader prompts produce n concise, diverse high-level techniques/hints; for each hint the follower samples multiple low-level reasoning chains producing n groups; group-majorities are computed and a small tournament (pairwise comparisons using GPT-4 or GPT-3.5) selects the final reasoning chain/answer.",
            "task_name": "MATH Level-5 (140-question evaluation subset)",
            "task_description": "Hard MATH problems used to evaluate exploring distinct high-level strategies.",
            "performance": "final-answer accuracy ≈ 25.00% (tournament selection; GPT-3.5 follower, prompt-hints), grouped-majority recall 32.69%",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT sampling baseline: final-answer accuracy 22.14% ; grouped-majority recall 31.19%",
            "key_findings": "For GPT-3.5 follower, prompt-based hierarchical exploration yields modest improvements in grouped-majority recall and final-answer accuracy over CoT sampling, demonstrating improved discovery/visibility of correct solutions when high-level tactic diversity is introduced.",
            "counter_examples_or_negative_results": "Prompt-based hints underperform retrieval-based hints when the follower is weaker (GPT-3.5); failure modes include the follower ignoring or failing to exploit the high-level hints, leading to limited gains.",
            "uuid": "e4766.2",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Hier-Prompt-GPT4",
            "name_full": "Hierarchical policy (prompt-based high-level tactics) with GPT-4 follower",
            "brief_description": "Diverse hierarchical approach where a high-level leader proposes concise technique hints and GPT-4 generates multiple low-level chains per hint; a tournament selects the final chain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-0613",
            "model_description": "GPT-4 used as the low-level follower; leader prompts (also GPT-4) produce up to n = 4 high-level tactics; follower generates m = 16 chains per tactic; tournament performed (usually with GPT-4) to select final chain.",
            "reasoning_method_name": "Hierarchical policy — Prompt-for-tactics (leader) + grouped sampling + tournament selection",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Leader is prompted (few-shot/zero-shot) to propose diverse concise techniques; follower conditioned on each technique samples multiple detailed chains per tactic; groups are reduced via group-majority and a tournament of pairwise comparisons selects the final answer.",
            "task_name": "MATH Level-5 (140-question evaluation subset)",
            "task_description": "Challenging math reasoning problems requiring creative high-level strategy exploration.",
            "performance": "final-answer accuracy 44.29% (tournament selection with hierarchical prompt-hints), grouped-majority recall 54.46%",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT sampling baseline: final-answer accuracy 40.89%, grouped-majority recall 50.78%",
            "key_findings": "When the follower is strong (GPT-4), prompt-based high-level tactic generation yields larger gains: higher grouped-majority recall and improved final-answer accuracy compared to CoT sampling; GPT-4 follower better understands and exploits concise technique hints.",
            "counter_examples_or_negative_results": "Retrieved problem-solution hints (retrieval-based hierarchy) performed slightly worse than prompt-based hints for GPT-4 in grouped-majority recall and final accuracy, showing that the nature of the high-level hints matters.",
            "uuid": "e4766.3",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Hier-Retrieval-GPT3.5",
            "name_full": "Hierarchical policy (retrieved problem-solution hints) with GPT-3.5 follower",
            "brief_description": "Diverse hierarchical approach where a SentenceBERT retrieval returns similar problems and solutions as high-level guidance; GPT-3.5 then generates multiple low-level chains per retrieved hint group.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613 (follower); SentenceBERT for retrieval",
            "model_description": "Follower: GPT-3.5 (gpt-3.5-turbo-0613). High-level hints are generated by retrieving n similar problems+solutions using a sentence-embedding model (SentenceBERT), and those are used as in-context examples.",
            "reasoning_method_name": "Hierarchical policy — Retrieval-of-similar QA + grouped sampling + tournament selection",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "For each retrieved similar problem and its solution, the follower is prompted to follow analogous tactics and produces multiple low-level reasoning chains per retrieved exemplar; groups are aggregated and a tournament selects the final chain.",
            "task_name": "MATH Level-5 (140-question evaluation subset) and larger 1047-question set (GPT-3.5)",
            "task_description": "Challenging math reasoning problems; retrieval provides concrete solved examples as high-level guidance.",
            "performance": "final-answer accuracy 27.85% (tournament selection; GPT-3.5 follower, retrieval-based hints) ; grouped-majority recall 34.23%",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT sampling baseline: final-answer accuracy 22.14% ; grouped-majority recall 31.19%",
            "key_findings": "Retrieval-based high-level hints produced the largest gains for the weaker follower (GPT-3.5), improving both grouped-majority recall and final-answer accuracy more than prompt-based hints, suggesting concrete retrieved demonstrations are easier for a weaker follower to exploit.",
            "counter_examples_or_negative_results": "For the stronger follower (GPT-4), retrieval-based hints were less advantageous than concise prompt-based techniques; some retrieved examples/hints can be irrelevant and mislead the follower if not carefully filtered.",
            "uuid": "e4766.4",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Hier-Retrieval-GPT4",
            "name_full": "Hierarchical policy (retrieved problem-solution hints) with GPT-4 follower",
            "brief_description": "Diverse hierarchical approach using retrieved problem-solution exemplars as high-level guidance, with GPT-4 generating multiple chains per retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-0613 (follower); SentenceBERT for retrieval",
            "model_description": "GPT-4 follower (gpt-4-0613); high-level hints are retrieved similar problems + solutions via SentenceBERT and used as in-context demonstrations.",
            "reasoning_method_name": "Hierarchical policy — Retrieval-of-similar QA + grouped sampling + tournament selection",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Retrieved solved problems are presented as high-level guidance; GPT-4 generates multiple low-level chains per retrieved exemplar; groups are majority-aggregated and a tournament selects the final chain.",
            "task_name": "MATH Level-5 (140-question evaluation subset)",
            "task_description": "Hard math problems where retrieved solved examples can encourage strategy transfer.",
            "performance": "final-answer accuracy 42.14% (tournament selection; GPT-4 follower, retrieval-based hints) ; grouped-majority recall ≈ 51.36%",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT sampling baseline: final-answer accuracy 40.89% ; grouped-majority recall 50.78%",
            "key_findings": "Retrieval-based hierarchical hints improve visibility and answer accuracy for GPT-4 versus CoT baseline, but are slightly less effective than concise prompt-based tactic hints for GPT-4; the effectiveness of hint type depends on follower capability.",
            "counter_examples_or_negative_results": "Retrieved similar exemplars occasionally caused the follower to follow incorrect analogies or produce consistent but incorrect answers; tournament-based selection mitigates but does not eliminate such failures.",
            "uuid": "e4766.5",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ToT-GPT3.5",
            "name_full": "Tree-of-Thoughts (ToT) implemented with GPT-3.5",
            "brief_description": "Implementation of Tree-of-Thoughts breadth-first search with GPT-3.5 as the reasoning engine; included as a comparative method.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "GPT-3.5 (gpt-3.5-turbo-0613) used to expand nodes in a ToT search; BFS expansion with branching/beam constraints as in the referenced ToT method.",
            "reasoning_method_name": "Tree-of-Thoughts (ToT) BFS search + Voting",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Perform tree-based search over intermediate reasoning states (ToT), expanding multiple child thoughts per node and keeping a beam of candidates; final answers aggregated by voting.",
            "task_name": "MATH Level-5 (140-question evaluation subset)",
            "task_description": "Challenging math problems; ToT designed to search over intermediate thoughts rather than sampling flat chains.",
            "performance": "final-answer accuracy 13.45% (ToT + Voting with GPT-3.5 on the 140-question set)",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT sampling baseline: 22.14% (same evaluation set and model), Hierarchical retrieval: 27.85%",
            "key_findings": "The implemented ToT produced far fewer total reasoning chains on average (~8.41 chains) than CoT sampling (64 chains) and underperformed the CoT baseline in this setup; limited chain budget / fewer end candidates harmed performance.",
            "counter_examples_or_negative_results": "ToT underperformed here — authors attribute this to producing far fewer final chains (search budget) relative to flat sampling, suggesting that naive ToT instantiations can be less effective under tight computation/sample budgets.",
            "uuid": "e4766.6",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Tournament-Selection",
            "name_full": "Tournament-based reasoning-chain selection",
            "brief_description": "Efficient selection procedure that compares group-majority reasoning chains pairwise (with LLM comparisons) to select a final high-quality reasoning chain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 or GPT-3.5 used as the selector (authors report GPT-4 as stronger selector)",
            "model_description": "Tournament comparisons are performed by prompting an LLM (GPT-4 preferred) to compare two candidate reasoning chains and decide which is better; repeated pairwise comparisons (k repeats) are optionally majority voted.",
            "reasoning_method_name": "Tournament-based selection (pairwise LLM comparisons)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "From each high-level hint group, pick a representative chain that matches the group-majority, build a small set S of n candidates, then run n-1 pairwise GPT comparisons (the 'tournament') to find the best chain; uses far fewer LLM calls than exhaustive verification.",
            "task_name": "MATH Level-5 (140-question evaluation subset)",
            "task_description": "Selection mechanism applied after diverse-group chain generation to choose the final answer.",
            "performance": "When used with hierarchical retrieval + GPT-3.5 follower, tournament selection (GPT-4 as selector, k=1) increased final-answer accuracy from 22.14% (CoT majority) to 27.85% on the 140-question set; selection temperature had little effect for 0&lt;T&lt;1.",
            "comparison_with_other_method": true,
            "performance_other_method": "Majority voting over all samples: 22.14% (CoT baseline with GPT-3.5 on same set); majority voting over groups yields smaller gains than tournament when combined with hierarchical methods.",
            "key_findings": "Tournament selection effectively filters out misleading tactic-specific consistency (groups that are consistently wrong) by pairwise LLM comparison, improving final-answer accuracy with very few extra LLM calls; GPT-4 is especially effective as the selector even with k=1.",
            "counter_examples_or_negative_results": "Tournament selection alone does not make CoT baseline beat hierarchical approaches — hierarchical diversity is an essential component; tournament's gains depend on having diverse high-quality groups to choose from.",
            "uuid": "e4766.7",
            "source_info": {
                "paper_title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 1,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Reasoning and Exploration with Language Models",
            "rating": 1,
            "sanitized_title": "reasoning_and_exploration_with_language_models"
        }
    ],
    "cost": 0.01697775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>UNLEASHING THE CREATIVE MIND: LANGUAGE MODEL AS HIERARCHICAL POLICY FOR IMPROVED EXPLORATION ON CHALLENGING PROBLEM SOLVING
5 Dec 2023</p>
<p>Zhan Ling 
UC San Diego</p>
<p>Yunhao Fang 
UC San Diego</p>
<p>Xuanlin Li 
Tongzhou Mu 
UC San Diego</p>
<p>Mingu Lee 
UC San Diego</p>
<p>Qualcomm AI Research *</p>
<p>Reza Pourreza 
Qualcomm AI Research *</p>
<p>Roland Memisevic 
Qualcomm AI Research *</p>
<p>Hao Su 
UC San Diego</p>
<p>Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc</p>
<p>UNLEASHING THE CREATIVE MIND: LANGUAGE MODEL AS HIERARCHICAL POLICY FOR IMPROVED EXPLORATION ON CHALLENGING PROBLEM SOLVING
5 Dec 2023ADBB8A377B5B0A3129B207BFFB4E685AarXiv:2311.00694v2[cs.AI]
Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems.Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains.However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space.In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning.This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction.The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal.Additionally, we propose an effective and efficient tournamentbased approach to select among these explored solution groups to reach the final answer.Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging mathematic datasets like MATH.Code will be released at https: //github.com/lz1oceani/LLM-As-Hierarchical-Policy.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023;OpenAI, 2023) have demonstrated remarkable potential across a myriad of disciplines such as common sense understanding (Hendrycks et al., 2020;Srivastava et al., 2022) and code generation (Chen et al., 2021;Li et al., 2023).Yet, LLMs often struggle with challenging reasoning tasks, such as writing mathematical proof and solving advanced mathematics problems.These tasks are inherently creative, as the path to a solution isn't immediately evident, requiring the exploration of numerous problem-solving tactics before discovering a successful path towards the end goal.</p>
<p>While recent works have investigated enhancing LLMs' exploration ability in problem solving through sampling and search (Wang et al., 2022;Yao et al., 2023;Besta et al., 2023), these approaches still exhibit considerable limitations.Before we describe such limitations, let's think of how humans approach mathematical proofs: one typical methodology is that we begin by connecting the target proof statement to our prior experiences such as proofs with similar routines (e.g., divide-and-conquer) or relevant techniques (e.g., root-finding).From this reservoir of knowledge and familiarity, humans try multiple "high-level" proof tactics and directions that possess the potential to reach the goal, and subsequently develop detailed, "low-level" proof details based on these directions.It should be noted that the quality of "high-level" strategies and thinking processes can exert a substantial impact on the effectiveness, efficiency, and likelihood of successfully solving these problems, as illustrated in Tab. 1.In cognitive science, such advanced higher-order thinking skills are referred to as Metacognition (Davidson et al., 1994;Metcalfe &amp; Shimamura, 1994;Berardi-Coletta et al., 1995).It is widely acknowledged that metacognition ability leads peo- Figure 1: Overview of our approach, which frames language models as a hierarchical policy for exploration.The visionary high-level leader policy connects the target problem with the language model's knowledge, proposing multiple diverse tactics and hints for exploration.The low-level follower policy leverages these hints as in-context guidance to execute detailed problem-solving processes.Finally, we employ an effective and efficient tournament-based approach to select the desired reasoning chains and reach the final answer.</p>
<p>ple to effective problem-solving strategies and successful task completion (Swanson, 1992;Alexander et al., 1995).</p>
<p>Compared to human exploration of complex problem solution spaces, the aforementioned sampling and search methods in NLP have primarily focused on delving into the detailed, "low-level" reasoning steps, often overlooking the "high-level" strategies and cognitive processes.We, therefore, aspire to unleash LLMs' potential for creative exploration of high-level tactics and hints, enabling them to tackle challenging reasoning problems with similar ingenuity and proficiency as humans.</p>
<p>To this end, we draw inspiration from the concept of a "hierarchical policy" in the decision-making literature (Bacon et al., 2017;Li et al., 2017;Kulkarni et al., 2016), and we propose to define LLM as a hierarchical policy for problem solving, which consists of a visionary high-level leader policy and a low-level follower policy.In our framework, the high-level leader establishes connections between the target problem and the LLM's extensive knowledge and prior problem-solving experiences.It leverages this information to propose various high-level problem-solving tactics and directions for exploration.The low-level follower policy then utilizes each of these hints as an in-context guidance throughout the detailed step-by-step problem-solving processes.Furthermore, we desire implementations of this idea to be achieved through minimal effort.Indeed, this can be achieved by leveraging off-the-shelf pretrained language models and in-context learning.Finally, after we obtain an array of diverse reasoning chains through LLM's creative exploration process, we propose an effective and efficient tournament-based method to select among these chains to arrive at the final answer.We illustrate an overview of our approach in Fig. 1.</p>
<p>Experimentally, we demonstrate that our high-level leader policy is capable of exploring and producing meaningful and inspiring hints and guidance for the low-level follower policy, thereby making it easier for correct reasoning chains and answers to stand out.Our reasoning chain selection approach effectively identifies desired reasoning chains, enhancing the final answer accuracy on challenging mathematical reasoning tasks.Our key contributions are as follows:</p>
<ol>
<li>
<p>To effectively explore expansive solution spaces in complex problems, we propose framing large language models (LLMs) as a hierarchical policy, encompassing both "high-level" and "lowlevel" cognitive processes, facilitating a more efficient exploration of distinct high-level ideas.</p>
</li>
<li>
<p>Within our hierarchical policy, we present two effective approaches for the visionary high-level leader policy to generate a diverse set of tactics and hints that guide the low-level follower policy during exploration.</p>
</li>
<li>
<p>We propose an effective and efficient tournament-based approach for selecting desired reasoning chains among those generated during exploration, facilitating the attainment of the final answer.</p>
</li>
<li>
<p>Experimentally, we demonstrate that our approach produces high-level hints and guidance that are meaningful and inspiring, enhances problem-solving strategy exploration, leads to better discovery Question: For how many integer values of a does the equation x 2 + ax + 5a = 0 have integer solutions for x? Ground Truth Solution (High level approach: Vieta's Formulas + Factorizing Trick): Suppose the roots of the quadratic are given by m and n.Note that
(x − m)(x − n) = x 2 − (m + n)x + mn = x 2 + ax + 5a
, and setting coefficients equal, it follows that m + n = −a and mn = 5a (This also follows directly from Vieta's formulas.)Notice that the a can be canceled by either dividing or noting that 0 = 5a +5 • (−a) = mn + 5(m + n).Simon's Favorite Factoring Trick can now be applied: mn + 5m + 5n + 25 = (m + 5)(n + 5) = 25.It follows that m + 5 and n + 5 are divisors of 25, whose pairs of divisors are given by ±{(1, 25), (5, 5), (25, 1)}.Solving, we see that (m, n) is in the set {(−4, 20), (0, 0), (20, −4), (−6, −30), (−10, −10), (−30, −6)}.However, the two pairs of symmetric solutions yield redundant values for a, so it follows that the answer is 4 .</p>
</li>
</ol>
<p>Example Reasoning from GPT-4 (High level approach: Discriminant is a perfect square): First, we note that the discriminant of a quadratic equation must be a perfect square for the roots of the equation to be integers.The discriminant of our equation x 2 + ax + 5a = 0 is a 2 − 20a.Setting this equal to k 2 for some integer k, we get a 2 − 20a − k 2 = 0.This is a quadratic equation in a, and its roots are a = 10 ± √ 100 + k 2 .Again, for a to be an integer, √ 100 + k 2 must be an integer.Let
√ 100 + k 2 = m. Then m 2 = 100 + k 2 =⇒ k 2 = m 2 − 100.
This means that m 2 − 100 must be a perfect square.The possible values of m which satisfy this condition are m = 10, 11, 12, 13, 14, 15, 20.For each of these 7 values of m, we get 2 values of a (one with the + sign and one with the − sign).However, we have to be careful with m = 10, because in this case k = 0, and we get only one value of a = 10.So, the total number of integer values of a for which the equation x 2 + ax + 5a = 0 has integer solutions for x is 2 × 6 + 1 = 13 .Table 1: An example problem from the MATH dataset, which can be solved through two distinct high-level approaches: (1) Identify when the discriminant is a perfect square; (2) Apply Vieta's formulas and the factorization trick.The first high-level approach does not effectively reduce the problem into a simpler form, as solving for all a where a 2 − 20a is a perfect square remains a highly challenging task, which causes the problem solving process to become stuck.Conversely, adopting the second strategy leads to an elegant and concise solution.</p>
<p>and visibility of correct solutions, and improves the final answer accuracy on challenging problems in the MATH dataset.</p>
<p>RELATED WORK</p>
<p>Reasoning and Exploration with Language Models.Recent large language models (LLMs) have demonstrated remarkable potentials in solving complex reasoning tasks.A key strategy is to encourage LLMs to generate detailed step-by-step reasoning processes through in-context learning techniques, including but not limited to Chain-of-Thought (CoT) prompting (Wei et al., 2022) and numerous other approaches (Kojima et al., 2022;Zhou et al., 2022a;Zhang et al., 2022;Si et al., 2022;Wang et al., 2022;Zhou et al., 2022b;Liu et al., 2023;Zhou et al., 2023;Yao et al., 2022).For many challenging reasoning tasks such as mathematical problem solving, proof writing, and inductive reasoning, it is often challenging for LLMs to obtain the correct solution in a single attempt.Therefore, to further enhance LLMs' problem solving capabilities, it is highly beneficial to encourage LLMs to search and explore over diverse problem-solving strategies and reasoning steps.2023) introduce verification filtering to the reasoning chain exploration process.Yao et al. (2023) and Besta et al. (2023) perform tree-based and graph-based search over reasoning steps with backtracking and refinement.However, much prior work limits reasoning exploration to specific and detailed reasoning steps, and the high-level strategies and thinking processes are often overlooked.In this study, we frame LLMs as hierarchical policies, enhancing problem-solving by exploring diverse high-level strategies and tactics.</p>
<p>Hierarchical Policy.The concept of hierarchical policy was originally proposed in reinforcement learning (RL) and imitation learning (IL) as a multi-level decision-making approach to tackle complex, long-horizon tasks (Bacon et al., 2017;Li et al., 2017;Kulkarni et al., 2016;Nachum et al., 2018;Gupta et al., 2020;Pertsch et al., 2021).In hierarchical RL, a high-level policy proposes latent features and subgoals that guide low-level policy actions.Prior work has also investigated enhancing the exploration abilities of hierarchical policies (Li et al., 2020;Gehring et al., 2021</p>
<p>Prompt</p>
<p>To find the number of whole numbers less than 18632 that are congruent to 23 (mod 37), we need to find the largest multiple of 37 that is less than 18632 and then count the number of multiples of 37 up to that number.</p>
<p>The largest multiple of 37 less than 18632 is 37 ⋅ 503 = 18611.</p>
<p>To find the number of multiples of 37 up to $18611$, we can divide 18611 by 37. We get
!"#!! $% = 503.
Therefore, there are 123 whole numbers less than 18632 that are congruent to 23 (mod 37).</p>
<p>Low-level Reasoning Chain 1</p>
<p>To find the whole numbers less than 18632 that are congruent to 23 (mod 37) , we need to find the largest multiple of 37 that is less than 18632 and then count the number of multiples of 37 that are less than or equal to this largest multiple.</p>
<p>The largest multiple of $37$ that is less than 18632 is 37×502 = 18674, since 37×503 = 18611 is less than 18632.Now we need to count the number of multiples of $37$ that are less than or equal to 18674 .To do this, we divide 18674 by 37 and round down to the nearest whole number:
!"#%&amp; $% = 502 .
Therefore, there are 126 whole numbers less than $18632$ that are congruent to 23 (mod 37).</p>
<p>Low-level Reasoning Chain 2</p>
<p>We want to count the number of whole numbers less than $18632$ that are congruent to 23 (mod 37).</p>
<p>To solve this, we can start by finding the smallest whole number that is congruent to 23 (mod 37).This can be done by finding the remainder when 23 is divided by 37. Since 23 &lt; 37 , the number itself is congruent to 23 (mod 37).</p>
<p>...... (too long)</p>
<p>Therefore, there are 89: whole numbers less than 18632 that are congruent to 23 (mod 37).</p>
<p>Low-level Reasoning Chain 1</p>
<p>To find the whole numbers less than 18632 that are congruent to 23 (mod 37), we can start by finding the smallest whole number that is congruent to 23 (mod 37).This can be done by adding multiples of 37 to 23 until we reach a number greater than 18632.</p>
<p>...... (too long)</p>
<p>Therefore, there are 8;9 whole numbers less than 18632 that are congruent to 23 (mod 37).</p>
<p>Low-level Reasoning Chain 2</p>
<p>Question: How many three-digit positive integers &lt; satisfy 39=8 + 89: ≡ (mod 63)?</p>
<p>Answer: {Ground truth answer from MATH training dataset}</p>
<p>High-Level Thought X High-level thoughts can also be a retrieved QA example</p>
<p>Substitution</p>
<p>Figure 2: A detailed illustration of our approach that frames language models as a hierarchical policy for exploration (zoom in for better view).The high-level leader proposes diverse hints that guide the low-level follower in problem-solving exploration.These hints can take the form of concise techniques and theorems, or a set of retrieved similar problems and solutions.In this example, "Modular Arithmatic" is highly relevant to the target question, and the low-level follower successfully finds the correct answer in some generated reasoning chains.On the other hand, "Counting Principles" is irrelevant to the target question, and the low-level follower struggles to reach the correct solution.</p>
<p>Later, we propose an effective and efficient approach to select the desired reasoning chains from those generated by the low-level follower.et al., 2022;Huang et al., 2023).However, few prior works has framed large language models as hierarchical policies through in-context learning to improve their exploration capabilities in problem solving, which is the main focus of our work.</p>
<p>LANGUAGE MODEL AS A HIERARCHICAL POLICY FOR EXPLORATION</p>
<p>A natural language reasoning process can be defined as a tuple (Q, T, A), where Q is the target question; A is the ground-truth answer in the format of a number in math word problems or a statement to proof or conclude; T is the set of locally-valid reasoning chains that reach the groundtruth answer, i.e.,
T = {τ = (τ 1 , τ 2 , . . . , τ s ) : valid(τ ) = 1, τ 1 = Q, τ s = A}.
A large language model (LLM), denoted as π, takes a question Q and a prompt P as input to generate a step-bystep reasoning chain τ = π(P, Q) that attempts to solve the problem.In the quest to improve LLMs' exploration abilities in problem solving, much prior work focuses on exploring, sampling, and searching for specific reasoning steps (Wang et al., 2022;Yao et al., 2023;Besta et al., 2023).</p>
<p>Yet, these methods tend to neglect the higher-order cognitive processes inherent in human problem solving.Successful problem-solving often relies on a guiding strategy and hint, and overlooking this aspect could potentially lead to inefficient and ineffective exploration.</p>
<p>To address these limitations, we propose to formulate LLM as a hierarchical policy π = (π high , π low ) for problem solving through in-context learning.Following the convention of Markov Decision Process, π, π high , and π low are probabilities over token sequences.The visionary highlevel leader policy π high takes in a question Q and a prompt P high as input and returns a set of possible high-level tactics and hints H = {h 1 . . .h n }, where H ∼ π high (P high , Q) (we emphasize that a sample from π high returns a tactic set instead of a single tactic; we will also use tactic / hint interchangably from here on).Then, the low-level follower policy π low utilizes each h i as an incontext guidance to execute specific problem-solving processes by sampling or searching reasoning chains, yielding a group of reasoning chains
T i = {t i,1 , . . . , t i,m }, where t i,j ∼ π low (h i , Q).</p>
<p>Preprint</p>
<p>To successfully instantiate our hierarchical approach, there are two crucial design components we need to address: (1) How to encourage the leader π high to generate appropriate tactics and hints H that serve as effective guidance for the follower π low ;</p>
<p>(2) Given groups of reasoning chains {T i } n i=1 generated by π low , how to effectively and efficiently choose the desired reasoning chains to obtain the final answer.</p>
<p>Generating high-level tactics and hints for exploration.Given a question Q, our goal is to encourage the leader π high to effectively establish the connection between Q and relevant language model knowledge, from which it proposes high-level hints and directions that holds significant potential for reaching the goal.We would also like to limit irrelevant hints that potentially mislead the low-level policy, since previous work (Shi et al., 2023) has shown that language models can be brittle to irrelevant context.We therefore propose two approaches for π high to generate high-level problem-solving tactics and hints (an illustration in Fig. 2):</p>
<p>(a) Prompt an LLM, such as GPT-4, to generate a list of relevant techniques and concepts for a target question.We aim for these hints to be clear, concise, and easily interpretable, e.g., "Angle Bisector Theorem", "Minimization with Derivatives", such that they can serve as effective guidance for the low-level policy π low .See Appendix for detailed prompts.(b) Use a sequence-embedding language model, such as SentenceBERT (Reimers &amp; Gurevych, 2019), to retrieve a set of relevant problems with their step-by-step solutions.Each relevant problem and solution then inspires π low to utilize similar tactics and strategies when exploring and generating reasoning chains.</p>
<p>Reasoning Chain Space Probablistic Interpretation: Next, we build a connection between our method and hierarchcial policies in the Markov Decision Process (MDP) framework, and we use the MDP framework to explain the improved exploration ability.To make it intuitive, we use Fig. 3 to illustrate the idea.In the low-level reasoning chain space T = {t 1 , t 2 , . ..} given Q, we group reasoning chains based on the high-level tactics they employ.For example, t 1,1 and t 1,2 both employ tactic h 1 .Here, the size of the region corresponds to the marginal conditional probability of h given Q when we sample the low-level reasoning chain without providing a high-level tactic prompt.We denote this marginal probability as Pr(h|Q).Note that the tactic with the highest Pr(h|Q) may not necessarily lead to the correct reasoning chains.This should not be counter-intuitive, especially for hard math problems that require out-of-the-box thinking.In practice, for a specific math question that receives an incorrect answer from GPT-3.5/4, we have observed that the generated reasoning chains frequently rely predominantly on a single tactic.Instead, our leader-follower strategy takes two steps to generate the low-level reasoning chain, which can be formulated as below using the marginal probability formula:
h 1 h 2 h 3 … t 1,1 t 1,2 t 2,1 t 2,2 t ,1 t ,23Pr(A|Q) = h Pr(A|h, Q) • Unif(h ∈ H) • Pr(H|Q)(1)
Here, Pr(A|h, Q) corresponds to π low , Pr(H|Q) corresponds to π high , and Unif(h ∈ H) denotes a unform distribution among h ∈ H.Note that Unif(h ∈ H) • Pr(H|Q) ̸ = Pr(h|Q).To illustrate with Fig. 3, suppose H = {h 1 , h 2 , h 3 } and Pr(H; π high ) = 1, then sampling by Pr(h i |Q) corresponds to sampling by the area of h i , whereas sampling by Unif(h ∈ H) • Pr(H|Q) corresponds to uniformly sampling among h 1 , h 2 , and h 3 , i.e., regarding them as if they were having the same area.</p>
<p>For general cases, our strategy samples all the different hints returned by π high with equal probabilities.Our strategy, therefore, aligns with the spirit of common practice in reinforcement learning to encourage the exploration behavior by making the density of actions more uniform (e.g., ϵ-greedy policy reduces the chance of the best action and increases the chance of worse actions).</p>
<p>Effectively and efficiently selecting final reasoning chains.Given n high-level tactics and hints {h i } n i=1 proposed by the leader π high , the low-level follower policy π low produces n groups of Preprint reasoning chains {T i } n i=1 .Throughout our experiments, we maintain a constant size of reasoning chains for all groups, i.e., ∀i, |T i | = m.As it is a common scenario that not all of the suggested tactics are relevant to problem solving, and irrelevant hints could make the low-level policy more susceptible to reasoning mistakes, we would like to effectively select the desired reasoning chains among those generated by the low-level policy.We would also like to make the selection process efficient, reducing the need to invoke a large number of language model calls.</p>
<p>To this end, we propose the following tournament-based approach to select reasoning chains: Within each group of reasoning chains T i , we conduct majority voting to establish the groupmajority answer A i .Then, for every group, we randomly select a single reasoning chain from those that reach the group-majority answer, and we add it to a "selection" set S (as a result, |S| = n).Next, denote the "final" reasoning chain as τ final , which we initialize as first reasoning chain of S.</p>
<p>We then initiate a "tournament": Over n − 1 iterations, for each iteration i, we prompt GPT-4 to compare the current τ final with the (i + 1)-th reasoning chain in S to determine which is better.If the latter is better, we set it as τ final .Empirically, we also gather comparison results through majority voting conducted over k repetitions.</p>
<p>The above approach requires a small number of n × k language model calls to select a desired reasoning chain.For instance, when we have n = 4 reasoning groups, each containing m = 16 reasoning chains, and we perform k = 3 comparison repetitions, it takes only 12 calls to GPT-4.Later, we will also demonstrate the effectiveness of our reasoning chain selection approach in improving final answer accuracy.</p>
<p>EXPERIMENTS</p>
<p>In this section, we perform quantitative and qualitative evaluations to demonstrate the effectiveness of our approach.We first investigate whether our approach successfully enhances the discovery and visibility of correct solutions, introducing a quantitative metric for this assessment.Subsequently, we assess whether our approach improves final answer accuracy in challenging mathematical reasoning problems.Lastly, we further analyze the success and failures our approach, as well as discuss its limitations.</p>
<p>Experiment setup.Unless otherwise specified, we adopt the MATH dataset (Hendrycks et al., 2020) and use its Level-5 test set, i.e., the hardest subset of questions, to evaluate different approaches.For the high-level policy π high , we adopt the two approaches outlined in Sec.3: (1) prompt GPT-4 to generate at most n relevant hints and tactics for a target question; (2) use SentenceBERT to retrieve n most relevant problems from the MATH training set.For the low-level policy π low , we adopt either GPT-3.5 or GPT-4.Unless otherwise specified, we establish the following parameters: n = 4 high-level hints (or reasoning groups) per question, m = 16 generated reasoning chains per group, and k = 1 comparison repeats for our tournament-based reasoning chain selection process (which is performed using GPT-4).We set temperature to be 0.3 during reasoning chain selection and 0.7 otherwise.</p>
<p>Our evaluation set of questions is constructed as follows: For experiments in Sec.4.1 and Sec.4.2, we randomly sample 20 questions for each of the 7 categories in the MATH Level-5 test set, resulting in an evaluation set of 140 questions.We opted for this smaller evaluation set due to the cost associated with evaluating our approach when either GPT-3.5 or GPT-4 serves as π low , which amounted to approximately $150 for these 140 questions.Evaluating GPT-4 as π low on the full test set would have been expensive.Later, when we conduct ablations in Sec.4.3, we employ the entire subset of MATH Level-5 questions (excluding those requiring visual understanding in vector graphics) to assess our approach when GPT-3.5 serves as π low .The findings remain consistent.</p>
<p>DO WE ENHANCE THE DISCOVERY AND VISIBILITY OF CORRECT SOLUTIONS?</p>
<p>In this section, we investigate whether by framing LLMs as a hierarchical policy and encouraging them to explore multiple diverse problem-solving tactics and hints, we improve the discovery and visibility of desired solutions that reach the correct answers.We compare our approach with the Chain-of-Thought (CoT) Sampling (Wei et al., 2022) + Majority Voting (Wang et al., 2022) baseline, which samples the same amount of detailed reasoning chains as ours and performs majority voting to obtain the final answer.It is a common scenario that even if LLM identifies correct solutions by a reasonable chance (and does so multiple times), these correct solutions are not effectively discovered and remain hidden during majority voting.Such scenarios are not reflected in the standard accuracy metric.Therefore, standard accuracy and recall are not ideal metrics to assess the visibility of correct answers.</p>
<p>We would like to assess the "visibility" of the correct answers among solutions generated along the exploration process.A correct answer is "visible" if it not only exists in at least one of reasoning chains, but also occupies a substantial proportion of them, even though it might not be the majority answer.An intuitive way of measurement is to compare the accuracy and recall of correct answers between our approach and the baseline.However, we find that standard metrics like accuracy and recall do not perfectly align with our goal.In particular, for our CoT Sampling baseline, as the number of reasoning chain samples goes up, the recall steadily improves, but the final-answer accuracy after majority voting plateaus after a few reasoning chain samples, as illustrated in Fig. 4a.This suggests that the standard recall metric poorly correlates the prominence of correct answers.On the other hand, the standard accuracy metric does not reflect the scenarios where LLM identifies correct solutions by a reasonable chance (and does so multiple times), but these correct answers become submerged during majority voting.Such scenarios often occur, and we illustrate this phenomenon in Fig. 4b.Therefore, the visibility of desired solutions is not adequately captured by the standard accuracy or recall metric.</p>
<p>We invent a "microscope" to inspect the visiblity of the correct answers.We propose the following "Grouped-Majority Recall" metric to better quantify the visibility of correct solutions.The calculation takes two steps.First of all, recall that our proposed method produces n groups each containing m reasoning chains.The CoT Sampling baseline can be viewed as a special case of our method with a single group (n = 1) of empty high-level tactics.To calculate the Grouped-Majority Recall metric, we first perform majority voting in each group to obtain n majority answers, and then we calculate the percentage of questions whose ground truth answer exist in at least one of the n group-majority answers.Sometimes, a group will contain multiple majority answers, in which case we calculate the expected value of our metric.To compare our approach with the CoT Sampling baseline, we randomly partition n × m CoT samples into n groups and calculate our metric in the same manner.</p>
<p>Intuition behind the Grouped-Majority Recall: In contrast to the standard accuracy metric, rare correct answers that might be obscured amidst all the n • m samples could emerge as the majority in one of the groups, thereby being recognized by our new metric.This occurrence, as noted in our observations, is not uncommon: when a high-level tactic is accurate yet seldom sampled, it frequently yields a substantial number of correct low-level solutions that can dominate in a group, despite remaining a minority among all samples.Our new metric aptly acknowledges such instances.Additionally, in contrast to the standard recall metric, for a correct answer to be recognized by our new metric, it should not only appear in at least one reasoning chain, but also take up the majority in a group, necessitating its appearance in multiple reasoning chains.</p>
<p>Results.We report the Grouped-Majority Recall metrics in Tab. 2. We find that our exploration approaches effectively enhance Grouped-Majority Recall when either GPT-3.5 or GPT-4 serves as Table 3: Comparison of final answer accuracy between the Chain-of-Thought sampling baseline and our two exploration approaches outlined in Sec. 3. We also report Tree-of-Thoughts results for reference.We use GPT-3.5 or GPT-4 as the language model for CoT &amp; ToT, along with the low-level policy in our approaches.Results are reported on our 140-question MATH Level-5 evaluation set.</p>
<p>our low-level policy, demonstrating that our methods improve the discovery and visibility of solutions leading to the correct answers.Additionally, we observe that among our two exploration approaches, using concise technique-based hints underperforms using retrieved problem-solution hints when GPT-3.5 is used as the follower, but outperforms the latter for GPT-4.We conjecture that this is caused by the stronger ability for the GPT-4 follower to understand and effectively utilize the hints in specific problem-solving processes.On the other hand, for the weaker GPT-3.5 follower, even if the high-level hints are already inspiring and meaningful, it may not effectively utilize the hint to solve the target problem.An example is illustrated in Tab. 9.</p>
<p>DO WE IMPROVE FINAL ANSWER ACCURACY FOR CHALLENGING MATH PROBLEMS?</p>
<p>Next, we investigate whether our exploration and tournament-based reasoning chain selection approach enhance the final answer accuracy on the Level-5 test set of the MATH dataset.We compare our approach with the CoT Sampling + Majority Voting baseline in Tab. 3. We find that both of our exploration approaches successfully improve the final answer accuracy, demonstrating that our approach effectively selects among the explored reasoning chains to retain the high-quality ones.</p>
<p>We also implement Tree-of-Thoughts (ToT) (Yao et al., 2023) for mathematics problem solving following the original paper, and present its results on GPT-3.5 as a reference1 .We perform breadthfirst search (BFS) in ToT, where at each step we expand 8 children and keep the best 5 candidates at each depth level.We limit the tree depth to 16.However, we observe that the final accuracy for ToT is significantly worse than the CoT Sampling + Voting baseline.Upon further analysis, we find that  Recall and final answer accuracy on our 1047-question evaluation set using GPT-3.5 as the language model for the CoT Sampling Baseline along with the low-level follower policy in our approaches.</p>
<p>the average number of reasoning chains ToT produces is 8.41, which is significantly fewer than the 64 reasoning chains in our baseline, potentially harming its performance.</p>
<p>MORE RESULTS AND ABLATION STUDY</p>
<p>Evaluation on a larger set of MATH questions.As stated in our experiment setup, our experiments in Sec.4.1 and Sec.4.2 were conducted using a smaller sample of 140 questions from the MATH Level-5 test-set due to the cost associated with evaluating GPT-4.In this section, we expand our evaluation by using GPT-3.5 as the low-level follower π low and evaluating on a larger set of 1047 questions from the MATH Level-5 test set.This evaluation set encompasses all questions from the MATH Level-5 test set, except those that include Asymptote (a Vector Graphics Language) code in the question or answer, as these questions require visual comprehension.Additionally, unlike the 140-question evaluation set, which features an equal distribution of 20 questions from each of the 7 categories, the 1047 questions evaluated in this section exhibit an uneven distribution among different categories, with 429 questions coming from algebra and prealgebra, which are considered easier compared to the rest.</p>
<p>We present the Grouped-Majority Recall evaluation results in Tab.4a and the final answer accuracy results in Tab.4b.The findings are consistent with those we obtained in Sec.4.1 and Sec.4.2.We also observe that the overall Grouped-Majority Recall and the final answer accuracy are higher than those we obtained on the 140-question evaluation set, which is due to the higher portion of algebra and prealgebra questions in our 1047-question evaluation set, and these questions are considered easier.</p>
<p>Ablation on tournament-based reasoning chain selection.Additionally, we perform an ablation on different design decisions of our tournament-based reasoning chain selection approach.We conduct an experiment where we use either GPT-3.5 or GPT-4 to perform our tournament selection process, while varying the value of k, the number of comparison repetitions, in each of iteration of the tournament.Results are shown in Tab. 5. We find that when k is small, using GPT-4 for tournament-based reasoning chain selection leads to a significant improvement compared to using GPT-3.5.Notably, GPT-4 demonstrates strong performance as a reasoning chain selector even with k = 1, resulting in a noteworthy enhancement of final answer accuracy over the CoT Sampling baseline.</p>
<p>We also conduct an experiment where we vary the decoding temperature (T ) of GPT-4 during our tournament-based reasoning chain selection process (the temperature was set to 0.3 in our previous experiments).Results are shown in Tab. 6.We observe that when 0 &lt; T &lt; 1, different temperatures have little effect on the final answer accuracy.Answer Accuracy 21.43 22.86 27.14 27.85 27.85 27.85 22.14 Table 5: Ablation on using different models to conduct our tournament-based reasoning chain selection process, along with using different k, i.e., different numbers of comparison repetitions, during this process.We compare our retrieval-based method with the CoT Sampling baseline.Results are obtained using our 140-question evaluation set.We use GPT-3.5 as the language model for our lowlevel follower and the CoT Sampling Baseline.
= 1 k = 3 k = 5 k = 1 k = 3 k = 5 N/A
Temperature T = 0 T = 0.3 T = 0.7 T = 1.0</p>
<p>Answer Accuracy 27.14 27.85 27.85 27.85</p>
<p>Table 6: Ablation on using different temperature (T ) in our tournament-based reasoning chain selection process.Results are obtained using GPT-4 as our tournament selection model with k = 1 comparison repetition on our 140-question evaluation set.We use GPT-3.5 as the language model for our low-level follower and the CoT Sampling Baseline.</p>
<p>Does final answer accuracy improvement come from our hierarchical policy framework and / or our tournament selection process?We perform an ablation experiment where we investigate the role of our hierarchical policy framework and our tournament-based reasoning chain selection process in improving the final answer accuracy.Results are shown in Tab. 7. We find that</p>
<p>• For our hierarchical policy approaches, adopting our tournament-based reasoning chain selection process yields better final answer accuracy than using majority voting, demonstrating the effectiveness of our tournament selection process.Intuitively, this is because not all of the tactics and hints produced by our high-level leader are helpful, and some of them might mislead the low-level follower, potentially causing it to generate a consistent but wrong answer under a misleading high-level guidance.By evaluating reasoning chains using our tournament-based selection approach, we effectively remove those that exhibit reasoning mistakes and keep those that are more trustful.• Additionally, for the CoT + Sampling baseline, adopting our tournament-based reasoning chain selection process does not ourperform our approaches that employ the hierarchicalpolicy framework.This demonstrates that our hierarchical policy plays a significant role in enhancing LLM's ability to solve challenging problems.</p>
<p>Evaluation on GSM8K.Previously, our experiments were conducted on the Level-5 subset of the MATH dataset.In this experiment, we investigate the efficacy of our approach on a wider range of datasets such as GSM8K.Results are shown in Tab. 8. We find that while GSM8k features easier mathematics problems than the MATH Level-5 questions used in our previous experiments, our approach continues to achieve better final-answer accuracy.This suggests that our approach retains its effectiveness across datasets with varying degrees of difficulty.</p>
<p>FURTHER ANALYSIS AND LIMITATIONS</p>
<p>In this section, we provide a further analysis into the success and failures of our approach, and we discuss the potential limitations of our approach along this process.</p>
<p>We observe that our high-level leader policy is capable of producing many insightful and inspiring hints, such as the "High-Level Thought 1" in Fig. 2, even though it sometimes produces irrelevant hints (e.g., "High-Level Thought 2" in Fig. 2).To further analyze the quality of generated high-level hints, we perform an experiment, where we aggregate all the high-level techniques and concepts produced by our first hint generation approach into a set.For each hint, we obtain its corresponding problem and ground truth answer, and then prompt GPT-4 to generate 10 "ground-truth hints" given both information.Subsequently, we calculate the percentage of hints that match at least one of its Table 8: Final answer accuracy on GSM8K.We adopt GPT-3.5 as the low-level follower, and we sample 32 reasoning chains per problem.For "Majority Voting", we directly perform majority finalanswer voting over the 32 reasoning chains.For tournament, we adopt n = 4 groups, each having m = 8 reasoning chains.</p>
<p>corresponding "ground-truth hints", serving as a measure of hint quality.Our findings reveal that 94.3% of hints are among the ground-truth hints, highlighting the effectiveness of our approach in proposing pertinent, insightful, and inspiring hints.</p>
<p>Next, we delve into the analysis of common sources of failure cases to gain a deeper understanding of our approach's behavior.One significant factor contributing to these failures is that, despite the high-level leader producing hints being both inspiring and meaningful, the low-level follower may not closely follow these hints to solve the target problem, resulting in reasoning errors.Sometimes, it might even ignore the hints.An illustrative example is presented in Tab. 9. We also empirically observe that such phenomenon occurs more often for a weaker follower language model compared to a stronger follower language model (e.g., GPT-3.5 vs. GPT-4).Furthermore, even when the follower effectively incorporates the hints into its problem-solving processes, reasoning errors can still occur, as illustrated in Tab.10.The inconsistent adherence to high-level hints and the dependence on the capabilities of the follower model highlight the current limitations of our approach.</p>
<p>CONCLUSION</p>
<p>In this work, we propose to frame large language models (LLMs) as a hierarchical policy to effectively explore the expansive solution spaces in challenging mathematical reasoning problems.Our hierarchical policy framework consists of a visionary "high-level" leader policy, which establishes connections between the target problem and the language model knowledge to generate hints, along with a "low-level" follower policy that leverages these hints as an in-context guidance throughout detailed problem-solving processes.Within this framework, we introduce two effective approach for the high-level leader policy to generate a diverse set of problem-solving tactics and hints for exploration.Additionally, we present an effective and efficient tournament-based method to select desired reasoning chains among the explored ones to attain the final answer.Through experiments, we demonstrate that our approach enhances problem-solving strategy exploration, improves the discovery and visibility of correct solutions, and enhances the final answer accuracy on challenging problems in the MATH dataset.</p>
<p>Preprint</p>
<p>Question: {question}</p>
<p>Use the methods related to "{technique or concept}" to derive your answer!Detail your reasoning step-by-step.Finally, present your final answer using the LaTeX command "\boxed{".</p>
<p>Answer:</p>
<p>Table 14: Zero-shot prompt for following a concise technique and concept produced by the highlevel leader policy.</p>
<p>Question: {demo-question}</p>
<p>Answer: {demo-step-by-step-answer}</p>
<p>Please refer to the above example as a demonstration.If it is not relevant to the current question, you may disregard them.</p>
<p>Question: {question} Answer: Please provide step-by-step reasoning, and present the final answer in the latex environment starting with \boxed{, without using a diagram.Table 16: Few-shot prompt for the high-level leader policy to propose diverse techniques and concepts that serve as hints to steer the problem-solving process of the low-level follower.Examine the difference between two answers thoroughly.Which one do you consider to be the most accurate?Support your decision with a comprehensive explanation.Provide your preference by stating with "Preferred Answer Index:" and the best answer.</p>
<p>Recent works like Bostrom et al. (2022); Tafjord et al. (2021); Yang et al. (2022); Creswell et al. (2022) perform step-by-step search to construct deductive natural language proofs given premises.Wang et al. (2022) samples and explores multiple reasoning chains, then performs majority voting to obtain the final answer.Weng et al. (2022); Lightman et al. (2023); Ling et al. (</p>
<p>3 Figure 3 :
33
Figure 3: Illustration of the partitioning of the reasoning chain space based on the high-level tactics and hints employed in the solution.</p>
<p>Figure 4 :
4
Figure 4: Statistics for the "CoT Sampling + Majority Voting" baseline.(a) Recall of correct answer steadily improves as the number of sampled reasoning chains increases, but the final answer accuracy after majority voting plateaus after a few reasoning chain samples.This suggests that the recall metric poorly correlates with the prominence of correct answers.(b) Percentage of questions where x reasoning chains have correct answers among the 64 reasoning chain samples (x ∈ N).It is a common scenario that even if LLM identifies correct solutions by a reasonable chance (and does so multiple times), these correct solutions are not effectively discovered and remain hidden during majority voting.Such scenarios are not reflected in the standard accuracy metric.Therefore, standard accuracy and recall are not ideal metrics to assess the visibility of correct answers.</p>
<p>Trapezoid ABCD has sides AB = 92, BC = 50, CD = 19, and AD = 70, with AB parallel to CD.A circle with center P on AB is drawn tangent to BC and AD.Given that AP = m n , where m and n are relatively prime positive integers, find m + n.Twenty switches in an office computer network are to be connected so that each switch has a direct connection to exactly three other switches.How many connections will be necessary?Response: 1: Handshaking Lemma in Graph Theory 2: Combinatorial counting principles Example 4: Question: What is the sum of the x-values that satisfy the equation 5 = x 3 −2x 2 −8x x+2 ?</p>
<p>Table 2 :
2
Comparison of the Grouped-Majority Recall metric between the Chain-of-Thought sampling baseline and our two exploration approaches outlined in Sec. 3. We use GPT-3.5 or GPT-4 as the language model for the CoT &amp; Sampling Baseline, along with the low-level follower policy in our approaches.Metrics are obtained using n = 4 reasoning groups and m = 16 reasoning chains per group on our 140-question MATH Level-5 evaluation set.
PreprintModelMethodAlg.Count. &amp; Prob.Geom.Interm. Alg.Num. TheoryPrealg. Precal. OverallCoT &amp; Sampling46.67 32.83 11.67 17.50 29.84 69.58 10.31 31.19GPT-3.5Ours -Prompt for Tactics 41.67 32.83 19.38 15.00 35.00 75.00 10.00 32.69Ours -Retrieval49.48 31.98 15.31 15.00 45.31 72.50 10.00 34.23CoT &amp; Sampling62.50 55.67 50.00 24.83 61.67 88.75 12.00 50.78GPT-4Ours -Prompt for Tactics 65.00 52.83 51.25 29.31 79.08 91.25 12.50 54.46Ours -Retrieval67.50 44.58 53.50 31.00 60.17 95.007.8351.36ModelMethodAlg.Count. &amp; Prob.Geom.Interm. Alg.Num. TheoryPrealg. Precal. OverallCoT Sampling + Voting 30.00 25.00 10.00 15.005.0060.00 10.00 22.14GPT-3.5ToT + Voting15.00 0.005.0020.000.0049.170.0013.45Ours -Prompt for Tactics 40.00 30.00 15.000.0020.00 65.005.0025.00Ours -Retrieval50.00 25.00 15.000.0030.00 70.005.0027.85CoT Sampling + Voting 50.00 36.25 47.50 10.00 55.00 75.00 12.50 40.89GPT-4Ours -Prompt for Tactics 65.00 35.00 40.00 15.00 70.00 80.005.0044.29Ours -Retrieval50.00 45.00 40.00 30.00 50.00 70.00 10.00 42.14</p>
<p>Table 4 :
4
Comparison of Grouped-Majority</p>
<p>Table 7 :
7
Effect of majority voting and our tournament-based reasoning chain selection on the finalanswer accuracy of the CoT + Sampling baseline and our hierarchical policy approaches.For "Majority Voting", we directly perform majority voting over the 64 sampled reasoning chains per problem.For "Majority Voting over Groups" and "Tournament", we adopt n = 4 groups, each having m = 16 reasoning chains.We use GPT-3.5 as the language model for our low-level follower and the CoT Sampling Baseline.
PreprintMethodBaseline CoT + Sampling Ours -Tactic Ours -RetrievalMajority Voting22.1421.7923.57Majority Voting over Groups19.7020.2423.39Tournament22.8625.0027.85MethodCoT + Sampling Baseline Majority VotingCoT + Sampling Baseline TournamentOurs -Retrieval (w/ Tournament)Answer Accuracy88.4588.8589.91</p>
<p>Table 15 :
15
Zero-shot prompt for following a question-answer demonstration retrieved by the highlevel leader policy.Preprint D.3 PROMPTS FOR HIGH-LEVEL LEADER TO GENERATE TACTICS AND HINTSWhen given a mathematical problem, your task is to list high-level mathematical techniques or concepts that can potentially lead to its solution.Do not provide detailed explanations, only name the techniques.Each technique you list should have the potential to guide towards a solution on its own.For clarity, here are some examples:</p>
<p>Table 17 :
17
Zero-shot prompt for comparing two reasoning chains in our tournament-based reasoning chain selection process.</p>
<p>Running ToT is especially expensive and costs over $100 on GPT-3.5. Running ToT on GPT-4 would incur thousands of dollars of expense, so we would like to leave it for future work.
PreprintQuestion: Carson flips over the cards of a standard 52-card deck one at a time.What is the probability that he flips over the ace of spades before any face card (jack, queen or king)?High-Level Hint: Conditional Probability Guidance: Use principles of "Conditional Probability" to approach this question.Ensure you think step-by-step, presenting a clear, logical solution pathway.Conclude with the final answer enclosed in the \boxed command.GPT-3.5 Follower Output:To solve this problem, we can consider the probability of flipping over the ace of spades first and the probability of flipping over any face card first, and then subtract the probability of flipping over any face card first from 1.There is only one ace of spades in the deck, so the probability of flipping it over first is 1 52 .There are 12 face cards (four jacks, four queens, and four kings) in the deck.So the probability of flipping over any face card first is 12 52 .Therefore, the probability of flipping over the ace of spades before any face card is 1 − 12 52 = 5 13 .Table9: A failure case in which the GPT-4 high-level leader generates a highly-relevant hint for the question, but the GPT-3.5 low-level follower fails to utilize the hint effectively during problemsolving, resulting in wrong answer.Retrieved Question: For some constants a and b, letQuestion: For some constants a and b, letThe function f has the property that f (f (x)) = x for all x.What is a + b?GPT-4 Follower Output:Table10: A failure case in which a very similar question along with its ground truth solution (from the MATH training set) are retrieved and padded as prompt for the GPT-4 low-level follower.Even though the model effectively incorporates the provided demonstration into its problem-solving process by following similar reasoning steps, the model still fails to produce the correct answer.PreprintA ANSWER EXTRACTIONWe obtain the final answer from a generated reasoning chain by identifying the content within the LaTeX environment \boxed{}, as specified in the prompt shown in Tab. 13.B GPT 3.5/4 MODEL VERSION AND HYPERPARAMETER DETAILSWe use "gpt-3.5-turbo-0613''and "gpt-4-0613" as the GPT-3.5 and GPT-4 version used throughout the experiments in our paper.We set the decoding temperature to 0.3 for tournament-based reasoning chain selection and 0.7 otherwise (i.e., for hint generation from the high-level leader, reasoning chain generation from the low-level follower, along with the baselines).C COST ANALYSISD PROMPTS D.1 CHAIN-OF-THOUGHT (COT) BASELINE PROMPTQuestion: {question}Please provide step-by-step reasoning, and present the final answer in the LaTeX environment starting with \boxed{.Answer:Table13: Zero-shot prompt for reasoning chain generation using the baseline CoT Sampling + Majority Voting approach on the MATH dataset.We need the model to present the final answer within a LaTeX boxed environment, which is capable of effectively handling complex output formats, such as intervals or matrices.D.2 PROMPTS FOR LOW-LEVEL FOLLOWER Preprint D.5 PROMPT FOR TREE-OF-THOUGHTS (TOT)Answering Guidelines1. Please answer the question step by step.2. Organize each step into two lines:-The first line starts with "#" followed by the step number and then succinctly describe the key action or result of the step.-The second line begins with "-" and provides a brief reasoning or explanation for the particular step.3. The first line of the final step is "What is the final answer?", and the second line provides the final answer using the latex format \boxed{} in this step.Example Question:By starting with a million and alternatively dividing by 2 and multiplying by 5, Anisha created a sequence of integers that starts 1000000, 500000, 2500000, 1250000, and so on.What is the last integer in her sequence?Express your answer in the form a b , where a and b are positive integers and a is as small as possible.Answer:Let's think step by step! #1.How can we express a million in terms of its prime factors?-A million can be expressed as 10 6 .Further breaking it down gives: 10 6 = (2 6 )(5 6 ).#2.What happens to the sequence as we proceed?-Each time she divides by 2, she removes one factor of 2. Each time she multiplies by 5, she adds one factor of 5. Since she starts with 6 factors of 2, after 6 divisions by 2, all factors of 2 are removed.#3.How many factors of 5 does she have at the end of the process?-Initially, there are 5 6 .After 6 steps of multiplication by 5, 6 more factors of 5 are added.This gives a total of 5 6 × 5 6 = 5 1 2.#4.What is the last integer in her sequence?-At the end of 12 steps, every factor of 2 has been replaced with a factor of 5. Thus, the integer becomes 5 6 × 5 6 = 5 12 .#5.What is the final answer?-5 12 .Please follow the Answering Guidelines and use the format shown in the above example to answer the following question!! Question: {question} Answer:Let's think step by step! {current-steps}Table18: One-shot prompt for generating the next reasoning step in ToT.PreprintInstruction: Given a problem statement and its current reasoning steps, determine if the proposed additional reasoning step is useful for solving the problem.Note: a useful reasoning step does not need to be directly related to the final question.Question:Let x, y, and z be real numbers such that x − 2y + 2z = 3 and 2x + y − z = 6.Find x 2 + z 2 − y 2 .New Reasoning Step: #1.Solve the system of equations to find the values of x, y, and z.Is the new reasoning step necessary or useful?No. We can't find unique values for x, y, z with only two linear equations for three unknowns.Question:Let P be a point on the lineand let Q be a point on the line. Find the shortest possible distance P Q.#1. Find the vector connecting the initial points of the two lines, which corresponds to the direction from point P to point Q.New Reasoning Step: #2.Calculate the dot product of the direction vector of the first line with the direction vector of the second line.Is the new reasoning step necessary or useful?Yes.The dot product will help determine if the lines are parallel, which is important in calculating the shortest distance between them.Evaluate the reasoning step in the new question by following the provided instruction, as shown in the previous examples.Question: {question} {current-steps} New Reasoning Step: {new-reasoning-step} Is the new reasoning step necessary or useful?Table19: Few-shot prompt for scoring the generated next reasoning step in ToT.
Development of metacognition in gifted children: Directions for future research. Martha Joyce M Alexander, Paula J Carr, Schwanenflugel, Developmental review. 1511995</p>
<p>The option-critic architecture. Pierre-Luc Bacon, Jean Harb, Doina Precup, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Metacognition and problem solving: A process-oriented approach. Bernadette Berardi-Coletta, Linda S Buyer, Roger L Dominowski, Elizabeth R Rellinger, Journal of Experimental Psychology: Learning, Memory, and Cognition. 2112051995</p>
<p>Preprint Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Natural language deduction through search over statement compositions. Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, Greg Durrett, arXiv:2201.060282022arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>The role of metacognition in problem solving. Janet E Davidson, Rebecca Deuser, Robert J Sternberg, Metacognition: Knowing about knowing. 1994207226</p>
<p>Hierarchical skills for efficient exploration. Jonas Gehring, Gabriel Synnaeve, Andreas Krause, Nicolas Usunier, Advances in Neural Information Processing Systems. 202134</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol Hausman, Proceedings of the Conference on Robot Learning. Leslie Pack Kaelbling, Danica Kragic, Komei Sugiura, the Conference on Robot LearningPMLR30 Oct-01 Nov 2020100</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Reparameterized policy learning for multimodal trajectory optimization. Zhiao Huang, Litian Liang, Zhan Ling, Xuanlin Li, Chuang Gan, Hao Su, 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Karthik Tejas D Kulkarni, Ardavan Narasimhan, Josh Saeedi, Tenenbaum, Advances in neural information processing systems. 292016</p>
<p>Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators. Chengshu Li, Fei Xia, Roberto Martin-Martin, Silvio Savarese, Conference on Robot Learning. PMLR2020</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023arXiv preprint</p>
<p>Infogail: Interpretable imitation learning from visual demonstrations. Yunzhu Li, Jiaming Song, Stefano Ermon, 201730Advances in neural information processing systems</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Deductive verification of chain-of-thought reasoning. Ling Preprint Zhan, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, arXiv:2306.038722023arXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Janet Metcalfe, Arthur P Shimamura, Metacognition: Knowing about knowing. MIT press1994</p>
<p>Data-efficient hierarchical reinforcement learning. Ofir Nachum, Shane Shixiang, Honglak Gu, Sergey Lee, Levine, Advances in neural information processing systems. 201831</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, Sanja Fidler, ACM Transactions On Graphics (TOG). 4142022</p>
<p>Accelerating reinforcement learning with learned skill priors. Karl Pertsch, Youngwoon Lee, Joseph Lim, Conference on robot learning. PMLR2021</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bertnetworks. 2019arXiv preprint</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou, arXiv:2302.000932023arXiv preprint</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang, arXiv:2210.09150Prompting gpt-3 to be reliable. 2022arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>The relationship between metacognition and problem solving in gifted children. Lee Swanson, Roeper Review. 1511992</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, Association for Computational LinguisticsAugust 1-6, 20212021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, arXiv:2205.124432022arXiv preprint</p>
<p>Shunyu Preprint, Jeffrey Yao, Dian Zhao, Nan Yu, Izhak Du, Karthik Shafran, Yuan Narasimhan, Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, arXiv:2308.079212023arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, arXiv:2205.10625Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. 2022aarXiv preprint</p>
<p>Teaching algorithmic reasoning via in-context learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, arXiv:2211.090662022barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
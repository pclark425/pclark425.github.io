<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-834 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-834</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-834</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-265221315</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.09721v1.pdf" target="_blank">On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art GPT-4 model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e834.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e834.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art proprietary large language model used in this study as the main high-capability LLM for evaluating long-form database question answering with and without SQL-module interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large transformer-based chat model (used via chat API). Employed with prompting to (a) plan retrieval, (b) generate SQL, and (c) synthesize long-form answers; evaluation also used GPT-4 as the multi-agent evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form database question answering (conclusive and interpretive) over synthetic Spider-derived databases</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Correct on ~30% of conclusive questions when using a better interaction strategy; average interpretive score 2.34/5 (reported across the dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA with SQL code interpreter (agent generates multiple SQL queries and synthesizes results)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning; multi-query retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Sub-task meta-review perfect rates reported: Interaction Planning (IP) ~28%, Tool Employment (SQL gen, TE) ~28%, Information Synthesis (IS) ~53% (meta-reviewer rates from Table 4). Overall conclusive-question correctness ~30% with interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No architectural retraining; augmented at inference with a SQL interpreter (tool-use interface). Workflow decomposition: natural-language planning, SQL generation, synthesis (chain-of-thought style prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / in-context use of the chat model; no new supervised fine-tuning or RL applied in these experiments (GPT-4 used off-the-shelf).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool augmentation and prompting/interaction strategy (sequential vs iterative); multi-agent evaluation (evaluation-side intervention).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Augmented the base LLM with a symbolic SQL interpreter (allowing generation and execution of SQL). Two interaction strategies were evaluated: Sequential-Interaction (plan -> generate SQLs -> synthesize) and Iterative-Interaction (plan + generate SQL repeatedly until stop). A multi-agent (reviewer/meta-reviewer) evaluation framework was also introduced to improve assessment reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Interaction with SQL modules produced a significant improvement on conclusive questions compared to no-interaction baselines (exact numeric delta not fully enumerated in text), but absolute performance remained low: ~30% correct on conclusive questions for GPT-4 and average 2.34/5 on interpretive questions. Sequential strategy generally outperformed iterative for most LLMs including GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper attributes the gap to failures in interaction planning and tool employment (i.e., difficulty generating multiple valid SQL queries and coordinating multi-step retrieval), rather than information synthesis (agents generally synthesize retrieved data well). Additional factors include limited interaction depth (agents produce few valid queries) and smaller models resorting to guess-based answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e834.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary chat-oriented transformer model evaluated alongside GPT-4 to measure generalizability of interaction findings across proprietary LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary chat model (smaller/capability-wise than GPT-4) used via prompting for the same interaction paradigms (no-interaction, sequential, iterative).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form database question answering (same dataset as GPT-4 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA with SQL interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Sub-task meta-review perfect rates reported (Table 4) roughly: IP ~9%, TE ~13%, IS ~22% (meta-review rates); reviewer rates higher but meta-review considered authoritative.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Augmented at inference with SQL interpreter; natural-language planning and SQL generation pipeline; no model architectural change.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / in-context use; used as-is without additional fine-tuning in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool augmentation and prompting/interaction strategies (sequential vs iterative).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same intervention as for GPT-4: SQL interpreter augmentation and evaluation of sequential vs iterative interaction strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Paper reports improvement from SQL-module interaction on conclusive questions for capable models including GPT-3.5, but absolute success rates and subtask perfect rates remain substantially lower than ideal (meta-review subtask perfect rates low as above).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Same causes as GPT-4: difficulty in planning and SQL generation; limited number/validity of interactions; agents often fail to formulate sufficient queries to retrieve all necessary evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e834.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-Llama-34b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code-Llama (34B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source code-specialized LLM (34B) used in experiments; showed some capacity to engage with the SQL module but less consistently than proprietary LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Code-Llama-34b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer model with code specialization and instruction-finetuned variants used for instruction-following; evaluated in the same interaction strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form database question answering (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA with SQL interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; SQL generation; planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported to show some of the interaction barriers (planning/tool employment) but to a lesser extent than the proprietary LLMs; exact numeric performance not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Code-specialized model; used instruction-following variant; SQL tool interface at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Instruction-following fine-tuned checkpoint used; experiments conducted via prompting (no new fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool augmentation; interaction strategy evaluation (sequential vs iterative).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same SQL-interpreter augmentation and sequential/iterative agent designs applied.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Some benefit observed from interaction relative to non-interactive baselines, but the interaction barriers (planning and SQL generation) remained noticeable.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Limited planning and SQL generation capabilities relative to need for multi-query retrieval; the model sometimes under-engaged with the SQL tool leading to poor retrieval coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e834.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (13B chat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source chat-capable LLM tested in sequential and iterative interaction strategies; showed limited engagement but some nuance in behavior across strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Llama-2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter open-source chat model; instruction/chat variant used to attempt planning, SQL generation, and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form database question answering (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA with SQL interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper reports Llama-2-13b sometimes favored iterative strategies and engaged minimally with SQL in iterative mode; overall interaction effectiveness low (many instances with near-zero valid interactions). Exact numeric metrics not provided beyond Table 4 low rates.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Chat/instruction-following variant; inference-time SQL-tool interface; no architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Used pre-existing instruction/chat-tuned checkpoint; experiments relied on prompting and in-context behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool augmentation and prompting/strategy interventions (sequential vs iterative).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated sequential vs iterative interaction; observed that in iterative mode this model planned less and generated fewer SQL queries, reducing effective retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Sequential strategy generally outperformed iterative for this class of models; iterative engagement produced lower interaction depth and poorer retrieval coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Under-planning and minimal SQL engagement (fewer valid queries) caused the interactive performance gap; synthesis ability remained comparatively stronger but was limited by lack of retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e834.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (7B chat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller open-source chat model (7B) evaluated and observed to rarely engage meaningfully with the SQL module in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Llama-2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter open-source chat model used in instruction-following mode for the interaction strategies; evaluated for planning, SQL generation, and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form database question answering (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA with SQL interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported near-zero meaningful interaction for many instances (Table 4 shows very low perfect rates across subtasks); often failed to generate valid SQL queries or engage in planning.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Instruction/chat variant with SQL tool interface; no model-level architectural change.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Instruction-following checkpoint used; no additional fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool augmentation and prompting strategies evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same interventions tested; smaller model size limited ability to perform planning and SQL generation at the required level.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Little to no effective benefit from interaction strategies due to lack of meaningful engagement; performance remained close to guess-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Insufficient model capacity for complex planning and multi-query generation; results indicate the gap is primarily in action/planning rather than synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e834.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B-parameter model evaluated; in iterative settings it often produced empty plans and did not engage with the SQL tool, effectively guessing answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter instruction-following model used for planning, SQL generation, and synthesis experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form database question answering (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA with SQL interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>In iterative strategy, Mistral-7b frequently produced empty plans and did not query the database at all; its performance is used as a baseline for guess-based answers in figures. Subtask perfect rates near zero in sequential settings as well (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Instruction-following model; used with external SQL interpreter; no architectural modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Instruction-finetuned variant used; experiments done via prompting only.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool augmentation and prompting/interaction strategy comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Augmented with SQL interpreter and evaluated under sequential/iterative strategies. Observed minimal to no engagement (empty plans) under iterative strategy leading to guess-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>No observable improvement when the agent did not engage with the tool â€” iterative mode sometimes produced no queries and no benefit; sequential sometimes better but still poor overall.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Model did not perform planning or tool employment; result is guess-driven answers rather than retrieval-driven synthesis, pointing to deficits in eliciting tool use via prompting for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e834.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-Interaction Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No-Interaction (Non-interactive LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline setup where the LLM does not generate SQL or interact with the SQL module; instead, the entire database records (when small enough) are provided in-context and the model answers directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>No-Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent configuration rather than a new model: LLM is given full database records in the prompt and asked to answer via chain-of-thought style prompting without generating SQL/using an external tool.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form database question answering (non-interactive baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / in-context demonstration; full DB contents included in prompt when databases small enough.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as a fairness baseline to compare against interactive agents; when complete DB fits into context, this baseline can sometimes match or exceed retrieval-based agents for interpretive questions but underperforms on conclusive (precision-demanding) questions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Paper reports that SQL-module interaction significantly improves performance on conclusive questions relative to this baseline (for capable LLMs), indicating benefits of tool-augmented retrieval for precision-demanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e834.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequential-Interaction Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential-Interaction (plan -> SQL -> synthesize)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent interaction strategy defined in this paper where the agent first plans in natural language, then generates SQL queries to retrieve information, then synthesizes the gathered data into a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Sequential-Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interaction/agent workflow rather than a model: linear sequence of sub-tasks â€” Interaction Planning, Tool Employment (SQL generation and execution), then Information Synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form DB QA with multi-query retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; tool use; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Generally yielded better results across most LLMs in the study compared to iterative; however, degree of improvement depended on model capability (proprietary LLMs benefited more). Exact aggregate % improvements not fully enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Workflow-level design: explicit planning step, then SQL generation, then synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / control of agent behavior via prompts and procedure templates.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting/interaction strategy (workflow design).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Defined and implemented sequential interaction as an explicit three-stage process, tested across multiple LLMs to compare against iterative approach.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Sequential strategy generally outperformed iterative for most LLMs (improved retrieval/use of SQL and final answer quality), except a few open-source models (Llama-2-13b, Mistral-7b) where iterative was observed to be favored but those implementations often minimally engaged the SQL module.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e834.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e834.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative-Interaction Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative-Interaction (plan+act loop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent interaction strategy where the agent alternates between planning and tool employment in a loop, deciding after each retrieval whether to continue or stop, inspired by self-ask prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Iterative-Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interaction/agent workflow: repeated cycle of planning (what to retrieve next) and SQL generation, terminating when sufficient evidence is gathered, followed by final synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Long-form DB QA requiring multi-query retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Interactive long-form DB QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; iterative multi-step retrieval; tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>In practice, iterative strategies often led to shorter/shallower plans and fewer valid SQL queries for many open-source models; iterative performed worse than sequential in aggregate except for some models which still under-engaged the tool.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Workflow-level iterative loop; relies on the model to decide to continue or stop interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompt/strategy-level (no additional training reported).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting/interaction strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Modeled after self-ask prompting: alternate plan and action until termination condition met; tested to see if iterative retrieval improves comprehensiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Often worse than sequential: iterative agents tended to plan less and interact minimally with the SQL module producing fewer valid interactions; for some models (Mistral-7b) iterative produced empty plans and essentially guessing behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question answering with human feedback <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Augmented language models: a survey <em>(Rating: 2)</em></li>
                <li>ToolQA: A dataset for LLM question answering with external tools <em>(Rating: 1)</em></li>
                <li>ToolWriter / Generate, transform, answer: Question specific tool synthesis for tabular data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-834",
    "paper_id": "paper-265221315",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "State-of-the-art proprietary large language model used in this study as the main high-capability LLM for evaluating long-form database question answering with and without SQL-module interaction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4",
            "model_description": "Proprietary large transformer-based chat model (used via chat API). Employed with prompting to (a) plan retrieval, (b) generate SQL, and (c) synthesize long-form answers; evaluation also used GPT-4 as the multi-agent evaluator.",
            "model_size": null,
            "qa_task_name": "Long-form database question answering (conclusive and interpretive) over synthetic Spider-derived databases",
            "qa_performance": "Correct on ~30% of conclusive questions when using a better interaction strategy; average interpretive score 2.34/5 (reported across the dataset).",
            "interactive_task_name": "Interactive long-form DB QA with SQL code interpreter (agent generates multiple SQL queries and synthesizes results)",
            "interactive_task_type": "tool use; planning; multi-step reasoning; multi-query retrieval",
            "interactive_performance": "Sub-task meta-review perfect rates reported: Interaction Planning (IP) ~28%, Tool Employment (SQL gen, TE) ~28%, Information Synthesis (IS) ~53% (meta-reviewer rates from Table 4). Overall conclusive-question correctness ~30% with interaction.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "No architectural retraining; augmented at inference with a SQL interpreter (tool-use interface). Workflow decomposition: natural-language planning, SQL generation, synthesis (chain-of-thought style prompting).",
            "training_method": "Prompting / in-context use of the chat model; no new supervised fine-tuning or RL applied in these experiments (GPT-4 used off-the-shelf).",
            "intervention_type": "tool augmentation and prompting/interaction strategy (sequential vs iterative); multi-agent evaluation (evaluation-side intervention).",
            "intervention_description": "Augmented the base LLM with a symbolic SQL interpreter (allowing generation and execution of SQL). Two interaction strategies were evaluated: Sequential-Interaction (plan -&gt; generate SQLs -&gt; synthesize) and Iterative-Interaction (plan + generate SQL repeatedly until stop). A multi-agent (reviewer/meta-reviewer) evaluation framework was also introduced to improve assessment reliability.",
            "intervention_effect": "Interaction with SQL modules produced a significant improvement on conclusive questions compared to no-interaction baselines (exact numeric delta not fully enumerated in text), but absolute performance remained low: ~30% correct on conclusive questions for GPT-4 and average 2.34/5 on interpretive questions. Sequential strategy generally outperformed iterative for most LLMs including GPT-4.",
            "hypothesized_cause_of_gap": "Paper attributes the gap to failures in interaction planning and tool employment (i.e., difficulty generating multiple valid SQL queries and coordinating multi-step retrieval), rather than information synthesis (agents generally synthesize retrieved data well). Additional factors include limited interaction depth (agents produce few valid queries) and smaller models resorting to guess-based answers.",
            "uuid": "e834.0",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "Proprietary chat-oriented transformer model evaluated alongside GPT-4 to measure generalizability of interaction findings across proprietary LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-3.5-turbo",
            "model_description": "Proprietary chat model (smaller/capability-wise than GPT-4) used via prompting for the same interaction paradigms (no-interaction, sequential, iterative).",
            "model_size": null,
            "qa_task_name": "Long-form database question answering (same dataset as GPT-4 experiments)",
            "qa_performance": null,
            "interactive_task_name": "Interactive long-form DB QA with SQL interpreter",
            "interactive_task_type": "tool use; planning; multi-step reasoning",
            "interactive_performance": "Sub-task meta-review perfect rates reported (Table 4) roughly: IP ~9%, TE ~13%, IS ~22% (meta-review rates); reviewer rates higher but meta-review considered authoritative.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Augmented at inference with SQL interpreter; natural-language planning and SQL generation pipeline; no model architectural change.",
            "training_method": "Prompting / in-context use; used as-is without additional fine-tuning in experiments.",
            "intervention_type": "tool augmentation and prompting/interaction strategies (sequential vs iterative).",
            "intervention_description": "Same intervention as for GPT-4: SQL interpreter augmentation and evaluation of sequential vs iterative interaction strategies.",
            "intervention_effect": "Paper reports improvement from SQL-module interaction on conclusive questions for capable models including GPT-3.5, but absolute success rates and subtask perfect rates remain substantially lower than ideal (meta-review subtask perfect rates low as above).",
            "hypothesized_cause_of_gap": "Same causes as GPT-4: difficulty in planning and SQL generation; limited number/validity of interactions; agents often fail to formulate sufficient queries to retrieve all necessary evidence.",
            "uuid": "e834.1",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Code-Llama-34b",
            "name_full": "Code-Llama (34B)",
            "brief_description": "A large open-source code-specialized LLM (34B) used in experiments; showed some capacity to engage with the SQL module but less consistently than proprietary LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Code-Llama-34b",
            "model_description": "Open-source transformer model with code specialization and instruction-finetuned variants used for instruction-following; evaluated in the same interaction strategies.",
            "model_size": "34B",
            "qa_task_name": "Long-form database question answering (this paper's dataset)",
            "qa_performance": null,
            "interactive_task_name": "Interactive long-form DB QA with SQL interpreter",
            "interactive_task_type": "tool use; SQL generation; planning",
            "interactive_performance": "Reported to show some of the interaction barriers (planning/tool employment) but to a lesser extent than the proprietary LLMs; exact numeric performance not provided in text.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Code-specialized model; used instruction-following variant; SQL tool interface at inference time.",
            "training_method": "Instruction-following fine-tuned checkpoint used; experiments conducted via prompting (no new fine-tuning reported).",
            "intervention_type": "tool augmentation; interaction strategy evaluation (sequential vs iterative).",
            "intervention_description": "Same SQL-interpreter augmentation and sequential/iterative agent designs applied.",
            "intervention_effect": "Some benefit observed from interaction relative to non-interactive baselines, but the interaction barriers (planning and SQL generation) remained noticeable.",
            "hypothesized_cause_of_gap": "Limited planning and SQL generation capabilities relative to need for multi-query retrieval; the model sometimes under-engaged with the SQL tool leading to poor retrieval coverage.",
            "uuid": "e834.2",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Llama-2-13b",
            "name_full": "Llama 2 (13B chat)",
            "brief_description": "Open-source chat-capable LLM tested in sequential and iterative interaction strategies; showed limited engagement but some nuance in behavior across strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Llama-2-13b",
            "model_description": "13B-parameter open-source chat model; instruction/chat variant used to attempt planning, SQL generation, and synthesis.",
            "model_size": "13B",
            "qa_task_name": "Long-form database question answering (this paper's dataset)",
            "qa_performance": null,
            "interactive_task_name": "Interactive long-form DB QA with SQL interpreter",
            "interactive_task_type": "tool use; planning; multi-step reasoning",
            "interactive_performance": "Paper reports Llama-2-13b sometimes favored iterative strategies and engaged minimally with SQL in iterative mode; overall interaction effectiveness low (many instances with near-zero valid interactions). Exact numeric metrics not provided beyond Table 4 low rates.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Chat/instruction-following variant; inference-time SQL-tool interface; no architectural changes.",
            "training_method": "Used pre-existing instruction/chat-tuned checkpoint; experiments relied on prompting and in-context behavior.",
            "intervention_type": "tool augmentation and prompting/strategy interventions (sequential vs iterative).",
            "intervention_description": "Evaluated sequential vs iterative interaction; observed that in iterative mode this model planned less and generated fewer SQL queries, reducing effective retrieval.",
            "intervention_effect": "Sequential strategy generally outperformed iterative for this class of models; iterative engagement produced lower interaction depth and poorer retrieval coverage.",
            "hypothesized_cause_of_gap": "Under-planning and minimal SQL engagement (fewer valid queries) caused the interactive performance gap; synthesis ability remained comparatively stronger but was limited by lack of retrieved evidence.",
            "uuid": "e834.3",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Llama-2-7b",
            "name_full": "Llama 2 (7B chat)",
            "brief_description": "Smaller open-source chat model (7B) evaluated and observed to rarely engage meaningfully with the SQL module in these tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Llama-2-7b",
            "model_description": "7B-parameter open-source chat model used in instruction-following mode for the interaction strategies; evaluated for planning, SQL generation, and synthesis.",
            "model_size": "7B",
            "qa_task_name": "Long-form database question answering (this paper's dataset)",
            "qa_performance": null,
            "interactive_task_name": "Interactive long-form DB QA with SQL interpreter",
            "interactive_task_type": "tool use; planning",
            "interactive_performance": "Reported near-zero meaningful interaction for many instances (Table 4 shows very low perfect rates across subtasks); often failed to generate valid SQL queries or engage in planning.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Instruction/chat variant with SQL tool interface; no model-level architectural change.",
            "training_method": "Instruction-following checkpoint used; no additional fine-tuning reported.",
            "intervention_type": "tool augmentation and prompting strategies evaluated.",
            "intervention_description": "Same interventions tested; smaller model size limited ability to perform planning and SQL generation at the required level.",
            "intervention_effect": "Little to no effective benefit from interaction strategies due to lack of meaningful engagement; performance remained close to guess-based baselines.",
            "hypothesized_cause_of_gap": "Insufficient model capacity for complex planning and multi-query generation; results indicate the gap is primarily in action/planning rather than synthesis.",
            "uuid": "e834.4",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Mistral-7b",
            "name_full": "Mistral (7B)",
            "brief_description": "Open-source 7B-parameter model evaluated; in iterative settings it often produced empty plans and did not engage with the SQL tool, effectively guessing answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Mistral-7b",
            "model_description": "7B-parameter instruction-following model used for planning, SQL generation, and synthesis experiments.",
            "model_size": "7B",
            "qa_task_name": "Long-form database question answering (this paper's dataset)",
            "qa_performance": null,
            "interactive_task_name": "Interactive long-form DB QA with SQL interpreter",
            "interactive_task_type": "tool use; planning; multi-step reasoning",
            "interactive_performance": "In iterative strategy, Mistral-7b frequently produced empty plans and did not query the database at all; its performance is used as a baseline for guess-based answers in figures. Subtask perfect rates near zero in sequential settings as well (Table 4).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Instruction-following model; used with external SQL interpreter; no architectural modifications.",
            "training_method": "Instruction-finetuned variant used; experiments done via prompting only.",
            "intervention_type": "tool augmentation and prompting/interaction strategy comparison.",
            "intervention_description": "Augmented with SQL interpreter and evaluated under sequential/iterative strategies. Observed minimal to no engagement (empty plans) under iterative strategy leading to guess-level performance.",
            "intervention_effect": "No observable improvement when the agent did not engage with the tool â€” iterative mode sometimes produced no queries and no benefit; sequential sometimes better but still poor overall.",
            "hypothesized_cause_of_gap": "Model did not perform planning or tool employment; result is guess-driven answers rather than retrieval-driven synthesis, pointing to deficits in eliciting tool use via prompting for this model.",
            "uuid": "e834.5",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "No-Interaction Agent",
            "name_full": "No-Interaction (Non-interactive LLM baseline)",
            "brief_description": "Baseline setup where the LLM does not generate SQL or interact with the SQL module; instead, the entire database records (when small enough) are provided in-context and the model answers directly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "No-Interaction",
            "model_description": "Agent configuration rather than a new model: LLM is given full database records in the prompt and asked to answer via chain-of-thought style prompting without generating SQL/using an external tool.",
            "model_size": null,
            "qa_task_name": "Long-form database question answering (non-interactive baseline)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": "Prompting / in-context demonstration; full DB contents included in prompt when databases small enough.",
            "intervention_type": null,
            "intervention_description": "Used as a fairness baseline to compare against interactive agents; when complete DB fits into context, this baseline can sometimes match or exceed retrieval-based agents for interpretive questions but underperforms on conclusive (precision-demanding) questions.",
            "intervention_effect": "Paper reports that SQL-module interaction significantly improves performance on conclusive questions relative to this baseline (for capable LLMs), indicating benefits of tool-augmented retrieval for precision-demanding tasks.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e834.6",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Sequential-Interaction Agent",
            "name_full": "Sequential-Interaction (plan -&gt; SQL -&gt; synthesize)",
            "brief_description": "Agent interaction strategy defined in this paper where the agent first plans in natural language, then generates SQL queries to retrieve information, then synthesizes the gathered data into a final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Sequential-Interaction",
            "model_description": "Interaction/agent workflow rather than a model: linear sequence of sub-tasks â€” Interaction Planning, Tool Employment (SQL generation and execution), then Information Synthesis.",
            "model_size": null,
            "qa_task_name": "Long-form DB QA with multi-query retrieval",
            "qa_performance": null,
            "interactive_task_name": "Interactive long-form DB QA",
            "interactive_task_type": "planning; tool use; multi-step reasoning",
            "interactive_performance": "Generally yielded better results across most LLMs in the study compared to iterative; however, degree of improvement depended on model capability (proprietary LLMs benefited more). Exact aggregate % improvements not fully enumerated in text.",
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "Workflow-level design: explicit planning step, then SQL generation, then synthesis.",
            "training_method": "Prompting / control of agent behavior via prompts and procedure templates.",
            "intervention_type": "prompting/interaction strategy (workflow design).",
            "intervention_description": "Defined and implemented sequential interaction as an explicit three-stage process, tested across multiple LLMs to compare against iterative approach.",
            "intervention_effect": "Sequential strategy generally outperformed iterative for most LLMs (improved retrieval/use of SQL and final answer quality), except a few open-source models (Llama-2-13b, Mistral-7b) where iterative was observed to be favored but those implementations often minimally engaged the SQL module.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e834.7",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Iterative-Interaction Agent",
            "name_full": "Iterative-Interaction (plan+act loop)",
            "brief_description": "Agent interaction strategy where the agent alternates between planning and tool employment in a loop, deciding after each retrieval whether to continue or stop, inspired by self-ask prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Iterative-Interaction",
            "model_description": "Interaction/agent workflow: repeated cycle of planning (what to retrieve next) and SQL generation, terminating when sufficient evidence is gathered, followed by final synthesis.",
            "model_size": null,
            "qa_task_name": "Long-form DB QA requiring multi-query retrieval",
            "qa_performance": null,
            "interactive_task_name": "Interactive long-form DB QA",
            "interactive_task_type": "planning; iterative multi-step retrieval; tool use",
            "interactive_performance": "In practice, iterative strategies often led to shorter/shallower plans and fewer valid SQL queries for many open-source models; iterative performed worse than sequential in aggregate except for some models which still under-engaged the tool.",
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "Workflow-level iterative loop; relies on the model to decide to continue or stop interaction.",
            "training_method": "Prompt/strategy-level (no additional training reported).",
            "intervention_type": "prompting/interaction strategy.",
            "intervention_description": "Modeled after self-ask prompting: alternate plan and action until termination condition met; tested to see if iterative retrieval improves comprehensiveness.",
            "intervention_effect": "Often worse than sequential: iterative agents tended to plan less and interact minimally with the SQL module producing fewer valid interactions; for some models (Mistral-7b) iterative produced empty plans and essentially guessing behavior.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e834.8",
            "source_info": {
                "paper_title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question answering with human feedback",
            "rating": 2,
            "sanitized_title": "webgpt_browserassisted_question_answering_with_human_feedback"
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Augmented language models: a survey",
            "rating": 2,
            "sanitized_title": "augmented_language_models_a_survey"
        },
        {
            "paper_title": "ToolQA: A dataset for LLM question answering with external tools",
            "rating": 1,
            "sanitized_title": "toolqa_a_dataset_for_llm_question_answering_with_external_tools"
        },
        {
            "paper_title": "ToolWriter / Generate, transform, answer: Question specific tool synthesis for tabular data",
            "rating": 1,
            "sanitized_title": "toolwriter_generate_transform_answer_question_specific_tool_synthesis_for_tabular_data"
        }
    ],
    "cost": 0.01794075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering
16 Nov 2023</p>
<p>Nan Linyong linyong.nan@yale.edu 
Allen Institute for AI</p>
<p>Ellen Zhang ellen.zhang@yale.edu 
Allen Institute for AI</p>
<p>Weijin Zou 
Allen Institute for AI</p>
<p>Yilun Zhao 
Allen Institute for AI</p>
<p>Wenfei Zhou 
Allen Institute for AI</p>
<p>Arman Cohan 
Allen Institute for AI</p>
<p>Yale University 
Allen Institute for AI</p>
<p>Linkedin 
Allen Institute for AI</p>
<p>Nvidia Corporation 
Allen Institute for AI</p>
<p>On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering
16 Nov 20230FD48759EB582C020899B48F39418765arXiv:2311.09721v1[cs.CL]
This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter.The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative.Our findings highlight that this task poses great challenges even for the state-of-the-art GPT-4 model.We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction.A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries.To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations.This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.</p>
<p>Introduction</p>
<p>Significant advancements in natural language processing have been driven by the development of Large Language Models (LLMs) (Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;OpenAI, 2023), which have become fundamental components of numerous products used by millions, reshaping people's habits on accessing information.Despite their widespread adoption and impact, LLMs face intrinsic limitations due to their design, including limited context window, stochastic nature which makes them less suited for tasks requiring high standards of precision, and extensive computations (Mialon et al., 2023;Ji et al., 2023;Wang et al., 2023a).Many studies have explored ways to mitigate these constraints by augmenting LLMs with modules/tools of complementary features (Nakano et al., 2022;Lewis et al., 2020;Lazaridou et al., 2022;Gao et al., 2023a;Parisi et al., 2022;Schick et al., 2023).In our study, we focus on augmenting LLMs with a symbolic module -a SQL code interpreter -and assess their performance using the long-form database question-answering task that we introduce, illustrated in Figure 1.Such augmentation is inevitable for tasks involving databases, as they often far exceed the size of LLMs' context windows 1 , making information retrieval through any means other than SQL inefficient.Additionally, the use of SQL queries brings transparency to the reasoning process of LLM agents, providing a means to validate the accuracy of their generated responses.</p>
<p>LLMs augmented with external modules/tools possess two primary abilities: the capacity to act, which involves the use of tools, and the capability to reason, which encompasses planning and analyzing the outcomes of actions (Mialon et al., 2023;Madaan et al., 2023;Paul et al., 2023;Yao et al., 2023;Yoran et al., 2023;Shinn et al., 2023).While numerous studies have evaluated these abilities in different contexts, we contend that some of them focus more on evaluating tool selection and tool employment with less focus on evaluating how LLM agents reflect or synthesize the action results (Parisi et al., 2022;Schick et al., 2023;Zhuang et al., 2023;Li et al., 2023).Other research (Shuster et al., 2022;Yao et al., 2023;BehnamGhader et al., 2023) does examine both the action and reasoning capacities of LLM agents, yet the actions' complexity is not as demanding as in studies with a stronger focus on the action aspect.Our goals are twofold: firstly, to introduce a task that places 1 Enterprise databases can easily store hundreds of millions of records for real-world applications.</p>
<p>Database Module</p>
<p>LLM Agent</p>
<p>Planning</p>
<p>We need to extract following information: singers citizenship, singers' net worth from 'singer' table, and sales of each song...</p>
<p>Employing Tools</p>
<p>SELECT Singer_ID, Name, Net_Worth_Millions, Citizenship FROM singer; SELECT Song_ID, Singer_ID, Sales, Highest_Position FROM song; ...</p>
<p>Synthesizing</p>
<p>After analyzing the data, we found the following correlations between singers' citizenship and their net...</p>
<p>"Investigate any correlations between singers' citizenship and their net worth, song sales, or highest chart positions."</p>
<p>Figure 1: Illustration of our long-form database question answering task.The LLM agent is expected to perform a series of tasks requiring reasoning and actions to interact with the database module.</p>
<p>equal emphasis on the complexities of both action and reasoning, requiring a concerted interaction between them, and secondly, to assess the proficiency of various LLM agents merging these dual aspects into a cohesive performance.Here are our main contributions:</p>
<p>â€¢ We introduce a new long-form database question answering task, requiring retrieval, reasoning and synthesis of diverse information from database.We develop a systematic approach for collecting questions, databases, and corresponding answers in a way that ensures the answers are definitive and indisputable, lending greater validity to the evaluation process.The task is challenging in retrieval: on average, it requires the formulation of three SQL queries to gather sufficient information to answer the questions.</p>
<p>â€¢ We explore the benefits of augmenting LLMs with the SQL code interpreter for our task, by comparing the performance of baseline LLMs given the complete database records but without SQL capabilities against LLM agents that are given database schema and SQL generation capacity.</p>
<p>â€¢ In evaluating the performance of agents across all sub-tasks, we identify that planning and tool utilization are the critical challenges in achieving effective coordination.We also delve into the reasons behind their shortcomings.We extend our examination to the generalizability of our results across various LLMs, measuring the disparity in performance between agents using proprietary and open-source LLMs as their foundation.</p>
<p>â€¢ Finally, we introduced a multi-agent evaluation framework aimed at enhancing the precision and consistency of the output assessments using GPT-4 evaluators.</p>
<p>Data Collection</p>
<p>In constructing our evaluation dataset, we prioritize a robust set of desiderata.These include the intensive retrieval of diverse information from the database, the application of rigorous reasoning over the information retrieved, and the synthesis of facts and inferences into a coherent and comprehensive long-form answer.Our methodology employs a hybrid annotation framework: we leverage the capabilities of GPT-4 to generate preliminary annotations, then these annotations are selected and refined through manual intervention to ensure quality and relevance.The specifics and quantitative details of our evaluation dataset are presented in Table 1.We detail the collection of questions in Section 2.1 and the acquisition of answers in Section 2.2.</p>
<p>Question Generation</p>
<p>Our starting point is the databases from the Spider dataset (Yu et al., 2018).We introduce a question generation pipeline designed to generate questions and iteratively refine them, addressing common issues encountered during preliminary experiments with GPT-4 generated queries.This pipeline can be described as Control-Condense-Confirm, it begins by exerting control over the question generation.We direct GPT-4 to generate questions that pertain to specific entities or keywords by using the original questions from the Spider dataset as the basis.These questions are instrumental as they concentrate on distinct column sets from various tables, providing a targeted focus that counters the LLM's propensity to formulate overly broad and indistinct questions.Following the initial control, we often find the questions to be exceedingly detailed.</p>
<p>To address this, we condense the content, removing superfluous information.This pruning process not only ensures the questions remain challenging but also leaves room for the model to demonstrate its inferential capabilities.The final phase is the manual review of questions to confirm they are unambiguous and meet all predefined criteria for the task.This step guarantees that the questions are of high quality and align with the specified desiderata of our dataset.</p>
<p>Answer Annotation</p>
<p>Building upon the question generation strategy outlined in the previous section, the task of annotating answers to questions is generally an effort-intensive task as it requires the formulation of multiple SQL queries.This task is further complicated by the fact that many databases, such as those in the Spider collection, often contain an insufficient number of data records for a comprehensive answer.We propose a method that employs a Conjecture-Construct-Conclude strategy to circumvent these issues.</p>
<p>The process begins by prompting GPT-4 with the question alongside the database schema to conjecture an answer.Subsequently, we construct database records that corroborate this conjectured answer, formatted as INSERT statements.These statements are integrated with the original database's CREATE statements, resulting in a bespoke synthetic database aligned with the question.To ensure the integrity of the synthetic database, we execute the merged statements to confirm the absence of errors and manually inspect the data records' alignment with the conjectured answer.As the final step of our method, we task GPT-4 to conclude with a substantiated answer, ensuring that it aligns with the evidences we injected to the synthetic database.This procedure ensures that each question is matched with a definitive answer, backed by verifiable evidence from the database records.</p>
<p>Finally, we examine the question and its corresponding answer.We noticed that a substantial number of questions allow for multiple plausible answers, each subject to interpretation of certain ab-stract word in the question. 2To refine the fairness of evaluations against a reference answer, we categorize all questions as either "Interpretive" or "Conclusive".This classification is based on whether the question demands a clear-cut outcome or allows various valid responses.We provide demonstrative examples in Figure 6 of the appendix to illustrate the distinction between these categories.The distribution of questions across these categories is detailed in Table 1.</p>
<p>Methods</p>
<p>We aim to evaluate how effective LLMs are at performing a complex task that necessitate working with external modules.We explore five main aspects: (1) the proficiency of LLMs in completing our proposed task through interaction with external modules; (2) the extent of improvement LLMs gain from engaging with external modules; (3) the impact of various interaction strategies on LLM performance and the identification of the most effective one; (4) the challenges that hinder effective interaction; (5) the generalizability of our findings across diverse LLMs and the performance disparities attributed to the usage of different LLMs.We can address the first, second and last aspects by directly evaluating the quality of the final answer generated by LLMs.To delve into the third and fourth aspects, we need to first dissect the "interaction" process within our task into its constituent components.</p>
<p>We propose to decompose the LLMs' expected workflow for our task into three distinct sub-tasks: interaction planning, tool employment, and information synthesis.Interaction planning involves the LLM determining its interaction strategy with the external module, considering the question and past interactions.Tool employment is the phase where the LLM generates module-specific commands for the actual interaction.Information synthesis requires the LLM to review the interaction history and any newly acquired context to compile the key information for the final answer.This framework allows us to refine our second objective into assessing how different compositions of these sub-tasks affect the quality of the final answer, and also to define the most effective composition.The third objective can be addressed by evaluating LLM's execution within each sub-task.</p>
<p>While the potential configurations of these sub-tasks are vast, this study will narrow its focus to two primary interaction strategies for feasibility:</p>
<p>â€¢ Sequential: The LLM agent systematically tackles the sub-tasks in a linear, step-by-step fashion, with predetermined sequence: interaction planning, tool employment, and information synthesis.The agent's focus should be on prioritizing both precision and comprehensiveness throughout each juncture.</p>
<p>â€¢ Iterative: The LLM agent cyclically alternates between interaction planning and tool employment, similar to the self-ask prompting (Press et al., 2023).The key aspect of interaction planning in this context is to identify the most crucial information to extract from the database given the previous interactions.The strategy calls for the agent to ensure precision in every single interaction and to achieve comprehensiveness by deciding when to terminate the interaction cycle.</p>
<p>Equipped with these strategies, we proceed to empirically explore our central questions.</p>
<p>Experiments 4.1 Design</p>
<p>To prove the key areas identified in Section 3, we designed two sets of experiments.The first set evaluates three different types of LLM agents:</p>
<p>â€¢ No-Interaction: This LLM is tasked with</p>
<p>deriving the final answer with a chain-ofthoughts prompting without engaging with the SQL module, i.e. generating SQL queries.</p>
<p>To ensure fairness, we supply the complete database records within the prompt for context.</p>
<p>â€¢ Sequential-Interaction: We implement an LLM agent that utilizes the sequential strategy when working with the SQL module.It begins by devising a plan in natural language to identify the needed information and its sources, proceeds to generate SQL queries to retrieve this information, and concludes by integrating the data into the final answer.</p>
<p>â€¢ Iterative-Interaction: This strategy employs an LLM agent that iteratively determines the most crucial information to retrieve given the interaction history.The agent articulates this in natural language, crafts the corresponding SQL query, and repeats this process until it elects to stop.The final step involves consolidating the gathered information into a conclusive answer.</p>
<p>The second set of experiment focuses on evaluating the generalizability of our findings across various LLMs, as well as comparing their performance.We tested two proprietary LLMs: GPT-4 and GPT-3.5-turbo, and six open-source LLMs of different sizes and capabilities: Llama-2-[7, 13]b, Code-Llama-[7, 13, 34]b, and Mistral-7b.The Llama-2 models were tested using their chat versions, while the Code-llama and Mistral models were evaluated using versions fine-tuned for instruction-following.</p>
<p>Evaluation</p>
<p>To rigorously assess the performance, we implemented two distinct evaluation methods.Both involve using an LLM for the evaluation process, yet they differ in terms of their reliance on a reference answer.Throughout both evaluation methods, we use GPT-4 to ensure consistency.</p>
<p>Reference-based Evaluation</p>
<p>In this method, we utilize an LLM to compare the system-generated answer against a reference answer, whose acquirement is detailed in Section 2.2.The evaluation protocol is adapted to suit the nature of the question: for conclusive questions that demand a specific answer, the LLM evaluator provides a straightforward verdict of either "match" or "not match" and offers a rationale for its decision.For interpretive questions, which permit a spectrum of answers, the LLM assigns a nuanced score ranging from 1 (no match) to 5 (exact match), reflecting the degree of information overlap with the reference.The scoring rubric for this nuanced evaluation is outlined in Figure 7 of the appendix.</p>
<p>Reference-free Evaluation</p>
<p>Assessing LLM performance on individual subtasks is essential, yet the multitude of potential answer pathways complicates the annotation process, making reference-based evaluation impractical.To navigate this challenge, we devised a referencefree evaluation using a multi-agent framework modeled after the academic peer-review system, as illustrated in Figure 2.This framework enlists a group of reviewers and meta-reviewers to evaluate the system outputs.Each reviewer receives the question, database schema, and the LLM agent's output for individual sub-tasks.Their role is to critically assess each output across various dimensions and determine if it is "perfect" or "not perfect".Meta-reviewers are then presented with the reviewers' assessments and verdicts.Their task is to discern consensus or discrepancies among the reviewers' opinions, evaluate the validity of their critiques, and render a final decision of "perfect" or "not perfect".The ultimate evaluation outcome is derived from the majority ruling among the metareviewers.To ensure a diversity of perspectives and avoid uniformity in judgment, we configured each GPT-4 evaluator with a temperature of 0.7.The specific guidelines used to direct reviewers and meta-reviewers are detailed in Figures 8, 9, 10 in the appendix.</p>
<p>Results</p>
<p>The experimental outcomes are presented in Figures 3, 4, 5 and Tables 2, 3, 4, addressing the main research questions posed by our study.</p>
<p>LLM Agent Performance on Proposed Task</p>
<p>Examining Figures 3, 5, and Tables 2, 4, it becomes evident that even the state-of-the-art GPT-4, when utilizing a better interaction strategy, correctly answers only 30% of conclusive questions and achieves an average score of 2.34 on a 1-5 scale.The multi-agent evaluation echoes this, with approximately 30% of instances deemed perfect, indicating considerable room for improvement.</p>
<p>Improvement from SQL Module Interaction</p>
<p>Table 3 compares LLM agents' performances with and without SQL module interaction.A significant improvement is observed in LLMs' performance on conclusive questions when augmented with SQL modules.However, this is not the case for interpretive questions.It is important to note that this result applies to instances with small databases where complete records fit within the context window provided to non-interactive LLMs.This suggests that direct interaction with external modules is particularly beneficial for tasks that demand high precision in retrieval, reinforcing our arguments of LLM limitations in Section 1.</p>
<p>The Impact of Interaction Strategies Figure 3 reveals that, generally, a sequential strategy yields better results across most LLMs, with iterative strategies favoring Llama-2-13b and Mistral-7b.</p>
<p>We investigate this trend by analyzing the length of plans, the number of generated SQL queries, and their validity.Notably, when employing iterative strategies, both Llama-2-13b and Mistral-7b engage minimally with the SQL module, with the Mistral-7b agent does not engage with the database at all (empty plan), indicating that it essentially guesses the answers.We set the performance of Mistral-7b with iterative strategy as a reference point for guess-based answers, marked by vertical lines in the figures, and make a note that performances close to this baseline likely result from guesswork.Additionally, we can also notice that agents using iterative strategies tend to plan less and interact minimally with SQL modules, contributing to their underperformance compared to sequential strategies.Generalizability Across Different LLMs The conclusions regarding interaction strategy appear consistent across various LLMs.However, our hypothesis of interaction barriers primarily holds for the more capable proprietary LLMs like GPT-4 and GPT-3.5-turbo, and to a lesser extent, Code-llama-34b.The remaining LLMs did not engage in any meaningful interaction, thus no discernible patterns were noted.</p>
<p>Barriers to Effective Interaction Figures 5 and</p>
<p>Interaction Depth and Answer Quality Figure 4 suggests a weak correlation between the number of valid interactions (i.e., agent-generated SQL queries that yield non-empty results) and performance, hinting that more successful retrieval aids in generating more precise and comprehensive answers.</p>
<p>Diversity and Consensus in Multi-agent Evaluation Aggregating multiple diverse evaluations and meta-evaluations from LLMs appears to decrease result variance, as evidenced by the increased consensus among meta-reviewer LLMs compared to reviewer LLMs shown in Table 4.</p>
<p>The scores from meta-reviewers are consistently lower than those from reviewers, indicating that the meta-review process critically considers the issues highlighted by reviewers.This multi-tiered review mechanism ensures that our evaluation framework effectively balances both precision and recall.</p>
<p>5 Related Work</p>
<p>Augmented Language Models</p>
<p>The use of external tools to augment language model outputs and mitigate model hallucination has been previously studied in other domains and tasks (Mialon et al., 2023).Models augmented with tools like internet search (Lazaridou et al., 2022), Python interpreters (Gao et al., 2023b), math equation-generating models (Imani et al., 2023), and question-answering models (Guu et al., 2020) have empirically shown improvements in accuracy in comparison to their counterpart baseline models.Other models like TALM (Parisi et al., 2022) and Toolformer (Schick et al., 2023) for question answering and ToolWriter (Gemmell and Dalton, 2023) for tabular question answering have built on top of these to limit reliance on humans to select tools for question answering models by fine-tuning the models to learn how and when to use tools.</p>
<p>Our work differs from these previous works in that we augment language models to use tools in the data-to-text generation domain specifically where the model is expected to not only query from a database with the use of external tools, but also aggregate these results and interpret the data to produce a paragraph-length response to a not necessarily close-ended question.</p>
<p>Reasoning and Action</p>
<p>Our work also draws on elements of frameworks that either prompt the model to repeatedly reason, act upon the reasoning, and update the action plan until the answer is found (Yao et al., 2023) or plan out the different components needed to answer a question before retrieving and generating the answer (Su et al., 2021).Other frameworks have used additional language models as the planner to aggregate information retrieved by a diverse inventory Figure 4: Correlation between answer quality and number of valid interaction (SQL queries that returned nonempty results)</p>
<p>of tools (Lu et al., 2023).Our work builds upon some of these frameworks and investigates these in context of the task of long-form data-to-text generation.</p>
<p>Text-to-SQL</p>
<p>The field of Text-to-SQL has been extensively studied as the standard method for database question answering, with significant contributions from a range of studies (Berant et al., 2013;Zhong et al., 2017;Yu et al., 2018;Yin and Neubig, 2018;Yu et al., 2019;Wang et al., 2020;Yin et al., 2020;Scholak et al., 2021;Ren et al., 2021;Xie et al., 2022;Cheng et al., 2023;Nan et al., 2023, inter alia).Despite this, our study innovates by introducing a question-answering task over databases that demands the generation of multiple SQL queries to formulate a single answer.</p>
<p>Figure 5: Reference-free multi-agent evaluation results for different sub-tasks and LLMs, all employing sequential interaction strategy.</p>
<p>Text Generation Evaluation</p>
<p>Development in automatic evaluation metrics have emerged, utilizing LLMs to evaluate the quality of generated texts (Fu et al., 2023;Liu et al., 2023).These methods have also been adapted for evaluating text pertaining to tabular data (Rebuffel et al., 2021) and hallucination detection (Manakul et al., 2023).(Wang et al., 2023b) introduced self-consistency sampling, which has been shown to improve the reasoning performance of the system.This approach involves generating a set of diverse answers and selecting the most common one through majority vote.In our study, we propose a reference-free multi-agent evaluation framework that synthesizes these ideas.</p>
<p>Conclusion</p>
<p>In conclusion, our investigation reveals the current limitations of LLMs in complex retrieval and reasoning tasks.Augmentation with a SQL module proved beneficial, particularly for conclusive questions, and pointed to the necessity of strategic interaction planning and proficient tool employment.</p>
<p>Our findings stress the need for improvement in these areas to enhance LLM effectiveness.Despite the challenge of varying performance across different models, our multi-agent evaluation framework provides a scalable and rigorous method for assessing agent capabilities.We hope that our proposed task and findings will encourage further investigations in LLMs' capabilities of interacting with external modules, inching towards LLMs capable of handling complex tasks with enhanced precision.</p>
<p>Limitations</p>
<p>This study acknowledges several constraints that much be considered when interpreting the results.First, our evaluation dataset is relatively small, a limitation primarily due to budget constraints.We plan to expand our dataset to enhance the statistical significance of our findings.Second, our current method does not include a rigorous human evaluation to ascertain the alignment between our automatic evaluation methods and human judgment.</p>
<p>We recognize this as a pivotal area for further research and intend to incorporate human evaluation in future.Third, while this study concentrated on tasks that require intensive action and reasoning capabilities, there is room to explore how LLM agents would perform with external modules on similar tasks with less stringent requirements.Fourth, this study's investigation is limited to specific modules, leaving the examination of a broader spectrum of modules unaddressed.Expanding our research to include a more diverse set of modules is a direction we plan to explore in our future work.Lastly, it is important to acknowledge a potential bias in our evaluation methodology stemming from the exclusive use of GPT-4 for generating reference answers as well as for evaluating system-generated responses.This reliance could skew the evaluation in favor of GPT-4 agent's answers.</p>
<p>Given the following inputs: Question: {question} Reference (Gold) Answer: {gold_answer} System Generated Answer: {answer} Evaluation Process:</p>
<p>Read the gold answer carefully to understand the precise information it conveys.</p>
<p>Examine the system-generated answer to identify the information presented.Check for the presence of critical information (such as conclusions) from the gold answer in the system-generated answer.</p>
<p>Evaluation Criteria:</p>
<p>The system-generated answer is considered a "Match" if it contains all the critical information from the gold answer.The presence of additional non-contradictory information in the systemgenerated answer is acceptable, provided that all the information from the gold answer is included.</p>
<p>Output Format:</p>
<p>If the system-generated answer includes all the critical information from the gold answer, the output should be: "Conclusion: Match" If any critical information from the gold answer is missing or misrepresented in the systemgenerated answer, the output should be: "Conclusion: Not Match" Conclusion:</p>
<p>(a) Scoring Metrics for Conclusive Questions</p>
<p>Given the following inputs: Question: {question} Reference (Gold) Answer: {gold_answer} System-Generated Answer: {answer} Evaluation Process: Familiarize yourself with the gold answer to understand the full scope of information it contains.</p>
<p>Analyze the system-generated answer to identify the information that has been captured.Compare the two answers to determine how much of the gold answer's information is reflected in the system-generated answer.</p>
<p>Scoring Metrics: Score 1: The system-generated answer lacks almost all the key points that the comprehensive gold answer provides.Score 2: The system-generated answer includes some key points from the gold answer but misses others, and it may include additional details not found in the gold answer.Score 3: The system-generated answer captures most of the key information from the gold answer, but there are noticeable omissions or additions.Score 4: The system-generated answer encompasses all key points from the gold answer and also introduces more information not covered in the gold answer.Score 5: The system-generated answer perfectly mirrors the gold answer, containing all the information with no omissions or additions.</p>
<p>Output Format: Provide a score between 1 to 5 based on the evaluation.The output should be: "Score: [1/2/3/4/5]" Score:</p>
<p>(b) Scoring Metrics for Interpretive Questions Figure 7: Prompts used for evaluating system generated answers for conclusive and interpretive questions Problem Context: A planning agent has been tasked to devise a solution to a user question related to a database.Given the question and the database's description, the agent proposes a plan detailing the type of information it would retrieve from the database to answer the question effectively.</p>
<p>Your Task: You are to evaluate the plan's relevance and comprehensiveness.Assess whether the plan can indeed retrieve the necessary information to address the user's question.</p>
<p>Inputs:</p>
<p>User Question: {question} Database Description: {database_text} Agent's Proposed Plan: {plan} Evaluation Criteria: Relevance: Does the plan target relevant pieces of information from the database that directly pertain to the user's question?Comprehensiveness: Is the plan exhaustive, ensuring all necessary pieces of information are retrieved to fully answer the user's question?Plan Definitions: Perfect Plan: A plan that is both relevant and comprehensive, ensuring that the user's question can be answered completely without missing any essential data points.Imperfect Plan: A plan that misses out on some relevant information, or includes unnecessary steps, thus not providing a complete or accurate solution to the user's question.</p>
<p>Response Format: Rationale: Begin with a detailed explanation of your evaluation.Discuss the strengths or weaknesses of the plan based on the relevance and comprehensiveness criteria.Final Decision: After providing the rationale, conclude with one of the following decisions:</p>
<p>-Perfect: If you believe the plan meets both the relevance and comprehensiveness criteria effectively.</p>
<p>-Imperfect: If you find the plan lacking in any aspect, be it relevance or comprehensiveness.</p>
<p>(a) Review Criteria for Interaction Planning</p>
<p>Problem Context:</p>
<p>As the "editor-in-chief", you are tasked with evaluating the reviews provided by multiple reviewers on a planning agent's proposed plan to answer a database-related user question.Each review contains a detailed rationale and a final decision.Your Task: Your goal is to compare and assess the rationales provided by the reviewers, and then make a final, conclusive decision about the planning agent's proposal.This decision should be based on a comprehensive understanding of the reviewers' perspectives and the evidence they present.Rationale: Begin with a detailed explanation of your evaluation.Address the SQL queries' correctness, their alignment with the initial plan, and the resulting output's relevance to the user's query.Final Decision: After providing the rationale, conclude with one of the following decisions:</p>
<p>Inputs</p>
<p>-Perfect: If all SQL queries are correct, aligned with the plan, and the results answer the user's question as expected.</p>
<p>-Imperfect: If you find any discrepancies in correctness, alignment, or the execution results of the proposed SQL queries.</p>
<p>(a) Review Criteria for Tool Employment</p>
<p>Problem Context:</p>
<p>As the "editor-in-chief", you are presented with multiple reviews evaluating an agent's capability to generate SQL queries from a given plan to answer a user question using a specified database.Each review contains an in-depth rationale and a final decision regarding the correctness, alignment, and execution results of the SQL queries.Your Task: Your goal is to compare and assess the rationales provided by the reviewers, weighing their evidence and perspectives, and then make a final, conclusive decision regarding the agent's SQL queries based on the aggregated reviews.Perfect Answer: An answer that accurately interprets the SQL queries and results, and addresses the user's question both correctly and comprehensively.Imperfect Answer: An answer that either misinterprets the SQL information, or does not completely and accurately address the user's question.</p>
<p>Response Format: Rationale: Begin with a detailed explanation of your evaluation.Discuss the strengths or weaknesses of the agent's synthesized answer based on the criteria of interpretation accuracy, correctness, and comprehensiveness.</p>
<p>Final Decision: After providing the rationale, conclude with one of the following decisions:</p>
<p>-Perfect: If you believe the agent's answer meets all evaluation criteria effectively.</p>
<p>-Imperfect: If you identify any shortcomings in interpretation accuracy, correctness, or comprehensiveness of the answer.</p>
<p>(a) Review Criteria for Information Synthesis</p>
<p>Problem Context:</p>
<p>As the "editor-in-chief", you are tasked with evaluating multiple reviews that assess an agent's synthesis of an answer based on a user's question, a search plan, and the results of executed SQL queries.Each review contains a detailed rationale and a final decision on the agent's capability to coherently integrate the information and answer the user's question.Your Task: Your role is to compare and evaluate the rationales provided by the reviewers, integrating their insights and perspectives.Based on this aggregated understanding, make a final, conclusive decision about the agent's synthesized answer.Evidence Quality: Do the reviewers present substantial and compelling evidence in their rationales?Final Decision Basis: Does the collective insight of the reviewers lead to a clear, definitive conclusion about the agent's answer?Response Format: Rationale: Start with a comprehensive comparison of the rationales given by the reviewers.Address any commonalities or differences in their evaluations and describe how these factors influenced your final decision.</p>
<p>Final Decision: After dissecting the reviewers' insights, decide on one of the following: -Perfect: If the collective evaluations suggest that the agent's synthesized answer meets all the required criteria.</p>
<p>-Imperfect: If the integrated reviews indicate issues in the agent's interpretation, correctness, or comprehensiveness.</p>
<p>(b) Meta-Review Criteria for Information Synthesis</p>
<p>Figure 2 :
2
Figure 2: Illustration of our multi-agent evaluation framework.It consists of two tiers of evaluation process.</p>
<p>Figure 3: Reference-based evaluation results across various interaction strategies and LLMs, with a vertical line representing the performance achieved by a non-interactive LLM agent lacking database context, serving as the baseline for guessing.</p>
<p>(a) Match Rate vs. Valid Interaction for Instances with Conclusive Questions (b) Match Score vs. Valid Interaction for Instances with Interpretive Questions</p>
<p>Figure 10 :
10
Figure 10: Prompts used for reviewing and meta-reviewing information synthesis</p>
<p>Table 1 :
1PropertyValueEvaluation Dataset Size200-# Conclusive Questions98-# Interpretive Questions102Reference Answer Length-Conclusive Questions (Avg.)132-Interpretive Questions (Avg.)209Database size-# Tables (Med.)4-# Columns (Med.)4-# Data Records (Med.)11
Dataset Statistics.Avg.stands for average and Med.stands for median.</p>
<p>Table 4
4highlight that planning and tool employ-ment (i.e. SQL generation) are the main hurdlespreventing agents from performing well on the pro-posed tasks. Conversely, agents generally excel atsynthesizing retrieved information to produce anaccurate and comprehensive answer. This indicatesthat eliciting better interaction planning via moreeffective prompting and enhancing LLMs' abilityto generate multiple SQL queries in parallel from</p>
<p>Table 2 :
2
Reference-based evaluation results and other measurements of the interaction process.C stands for conclusive and I stands for interpretive.Valid SQL indicate SQL queries that are generated by the LLM agent that have non-empty execution results.</p>
<p>Table 3
3: LLM agents with vs. without interaction withexternal SQL modules. Entire database records areprovided in the context for LLMs that disable interac-tion. We sampled 161 out of 200 questions to reporttheir reference-based evaluation results because somedatabases are too large to fit into the context window ofLLMs.</p>
<p>Table 4 :
4ReviewerMeta-ReviewerLLMSub-TaskPerf. RateAgree.Perf. RateAgree.IP0.480.540.280.88GPT-4TE0.410.690.280.93IS0.610.720.530.93IP0.140.800.090.96GPT-3.5-turboTE0.180.820.131.00IS0.260.790.220.95IP0.020.970.010.99Llama-2-7bTE0.010.990.011.00IS0.010.990.001.00IP0.030.960.020.98Llama-2-13bTE0.010.980.010.99IS0.010.990.010.99IP0.020.930.010.99Code-llama-7bTE0.010.980.010.99IS0.020.980.011.00IP0.010.960.011.00Code-llama-13bTE0.020.980.021.00IS0.020.970.021.00IP0.040.900.010.98Code-llama-34bTE0.020.970.010.99IS0.040.960.040.99IP0.030.960.010.99Mistral-7bTE0.010.980.001.00IS0.050.940.030.98
Reference-free multi-agent evaluation -finegrained results for different LLMs adopting Sequential interaction strategy.IP stands for Interaction Planning, TE stands for Tool Employment, and IS stands for Information Synthesis.Perf.Rate stands for percentage of instances that (meta-)reviewers considers perfect, and Agree.stands for agreement, and it is calculated with the percentage of instances that (meta-)reviewers reach in unanimous agreement.</p>
<p>An agent is given a question, a database for retrieving relevant context, and a plan of how to perform the retrieval.It has been tasked to translate the plan into accurate and executable SQL queries.These queries should correspond to the given plan and effectively retrieve the relevant information from the database to address the user's question, adhering to the database structure provided.Your Task: You are to evaluate the correctness and alignment of the SQL queries generated by the agent based on the plan provided.Also, review the execution results to determine if they fulfill the user's requirements as stipulated in the plan.Correctness: Are the SQL queries syntactically and semantically correct, and do they retrieve the expected data from the database?Alignment: Do the SQL queries align with the steps outlined in the initial plan?Execution Results: Does the outcome of the SQL queries correspond to the desired results based on the user's question and the initial plan?Query Definitions: Perfect Queries: All SQL queries are correct, aligned, and ensure that the user's question is addressed in accordance with the initial plan.Imperfect Queries: There is at least one SQL query that has errors, misalignments, or does not produce the expected results as outlined in the initial plan.
Problem Context:Inputs:User Question: {question}Database Description: {database_text}Search Plan: {plan}Agent's Proposed SQL Queries and Execution Results: {sql_results}Evaluation Criteria:Response Format::User Question:{question}Database Description:{database_text}Agent's Proposed Plan:{plan}Reviewers' Rationales and Decisions:{IP_reviews}Evaluation Criteria:Review Consistency: Are the reviewers' rationales and decisions consistent with each other?Evidence Quality: Is the evidence provided in the rationales substantial and convincing enough to make adefinitive conclusion?Final Decision Basis: Does the aggregated perspective of the reviewers lead to a clear final decision?Response Format:Rationale: Begin with a detailed explanation comparing the rationales provided by the reviewers. Highlightconsistencies or discrepancies among them and discuss how these influenced your final decision.Final Decision: After providing the rationale, conclude with one of the following decisions:-Perfect: If the aggregated insights from reviewers suggest that the planning agent's proposal is bothrelevant and comprehensive.-Imperfect: If the combined reviews indicate that the planning agent's proposal is lacking in eitherrelevance or comprehensiveness.(b) Meta-Review Criteria for Interaction PlanningFigure 8: Prompts used for reviewing and meta-reviewing interaction planning</p>
<p>An agent is presented with a user's question, a plan to extract more context for answering the question, and a search history containing SQL queries used to retrieve this context from the database.The agent's task is to synthesize all the given information to construct a coherent answer to the question.Your Task: You are to evaluate the synthesis produced by the agent.Assess whether the agent's response accurately interprets the SQL queries and their execution results.Furthermore, determine if the synthesized answer addresses the user's question both correctly and comprehensively.Interpretation Accuracy: Does the agent's answer demonstrate a correct understanding of the SQL queries and their execution results?Answer Correctness: Is the agent's synthesized answer accurate in terms of the given information?Comprehensiveness: Does the agent's answer cover all aspects of the user's question based on the context retrieved?Answer Definitions:
Problem Context:Inputs:User Question:{question}Database Description:{database_text}Search Plan:{plan}SQL Queries and Execution Results:{sql_results}Agent's Synthesized Answer:{answer}Evaluation Criteria:Inputs:User Question: {question}Database Description: {database_text}Search Plan: {plan}Agent's Proposed SQL Queries and Execution Results: {sql_results}Reviewers' Rationales and Decisions: {TE_reviews}Evaluation Criteria:Review Consistency: Do the reviewers agree in their evaluations, or are there conflicting perspectives?Evidence Quality: Are the rationales provided by reviewers substantial and convincing?Final Decision Basis: Based on the aggregated insights of the reviewers, is there a clear and justifiablefinal decision?Response Format:Rationale: Begin with a detailed comparison of the rationales provided by the reviewers. Address anyconsistencies or discrepancies in their evaluations, emphasizing how these observations influenced yourfinal decision.Final Decision: After analyzing the rationales, conclude with one of the following decisions:-Perfect: If the collective insights suggest that all the agent's SQL queries are accurate, aligned, andanswer the user's question as stipulated.
-Imperfect: If the combined reviews reveal issues in correctness, alignment, or the execution results of the agent's SQL queries.(b) Meta-Review Criteria for Tool Employment Figure 9: Prompts used for reviewing and meta-reviewing tool employment</p>
<p>Such as "impact", "success", "notable trends", etc.
Figure 6: Examples of Conclusive and Interpretive Questions
Appendixâ€¢ Conclusive questions:1. Do dual-enrolled students tend to perform better or worse than their peers in the same degree programs?2. Analyze the relationship between teachers' experience and their performance based on the grades received in the courses they have taught.3. Investigate any correlations between poker players' performance and factors such as nationality, age, and height.â€¢ Interpretive questions:1. Compare the success metrics between French and non-French singers.2. Analyze the impact of record companies on the success of orchestras based on their performance ratings and attendance.3. Analyze the performance of the TV series by language and country, and identify any notable patterns or trends.
Can retriever-augmented language models reason? the blame game between the retriever and the language model. Parishad Behnamghader, Santiago Miret, Siva Reddy, 2023</p>
<p>Semantic parsing on Freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USA2013Association for Computational Linguistics</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 2023</p>
<p>PAL: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023a202</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Pal: Program-aided language models. 2023b</p>
<p>Generate, transform, answer: Question specific tool synthesis for tabular data. Carlos Gemmell, Jeffrey Dalton, 2023</p>
<p>Realm: Retrievalaugmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, 2020</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, 2023</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 12552023</p>
<p>Internetaugmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, 2022</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-Tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Api-bank: A comprehensive benchmark for tool. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, 2023augmented llms</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, Chameleon: Plug-and-play compositional reasoning with large language models. 2023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 2023</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, 2023</p>
<p>. GrÃ©goire Mialon, Roberto DessÃ¬, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste RoziÃ¨re, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 2023Augmented language models: a survey</p>
<p>Webgpt: Browserassisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2022</p>
<p>Enhancing few-shot text-tosql capabilities of large language models: A study on prompt design strategies. Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, Dragomir Radev, 2023</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, Talm: Tool augmented language models. 2022</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Refiner: Reasoning feedback on intermediate representations. 2023</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Data-questeval: A referenceless metric for data-totext semantic evaluation. ClÃ©ment Rebuffel, Thomas Scialom, Laure Soulier, Benjamin Piwowarski, Sylvain Lamprier, Jacopo Staiano, Geoffrey Scoutheeten, Patrick Gallinari, 2021</p>
<p>Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs. Hanjun Hongyu Ren, Bo Dai, Xinyun Dai, Michihiro Chen, Haitian Yasunaga, Dale Sun, Jure Schuurmans, Denny Leskovec, Zhou, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139of Proceedings of Machine Learning Research</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. Torsten Scholak, Nathan Schucher, Dzmitry Bahdanau, 10.18653/v1/2021.emnlp-main.779Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion. Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston, 2022</p>
<p>Plan-then-generate: Controlled data-to-text generation via planning. Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier, 10.18653/v1/2021.findings-emnlp.76Findings of the Association for Computational Linguistics: EMNLP 2021. Punta CanaDominican Republic. Association for Computational Linguistics2021</p>
<p>RAT-SQL: Relation-aware schema encoding and linking for textto-SQL parsers. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson, 10.18653/v1/2020.acl-main.677Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang, Survey on factuality in large language models: Knowledge, retrieval and domainspecificity. 2023a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023b</p>
<p>UnifiedSKG: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, I Sida, Victor Wang, Bailin Zhong, Chengzu Wang, Connor Li, Ansong Boyle, Ziyu Ni, Dragomir Yao, Caiming Radev, Lingpeng Xiong, Rui Kong, Noah A Zhang, Luke Smith, Tao Zettlemoyer, Yu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, 2023</p>
<p>TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation. Pengcheng Yin, Graham Neubig, 10.18653/v1/D18-2002Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>TaBERT: Pretraining for joint understanding of textual and tabular data. Pengcheng Yin, Graham Neubig, Wen-Tau Yih, Sebastian Riedel, 10.18653/v1/2020.acl-main.745Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Answering questions by meta-reasoning over multiple chains of thought. Tomer Ori Yoran, Ben Wolfson, Uri Bogin, Daniel Katz, Jonathan Deutch, Berant, 2023</p>
<p>CoSQL: A conversational text-to-SQL challenge towards crossdomain natural language interfaces to databases. Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Victoria Xi, Yi Lin, Tianze Chern Tan, Zihan Shi, Youxuan Li, Michihiro Jiang, Sungrok Yasunaga, Tao Shim, Alexander Chen, Zifan Fabbri, Luyao Li, Yuwen Chen, Shreya Zhang, Vincent Dixit, Caiming Zhang, Richard Xiong, Walter Socher, Dragomir Lasecki, Radev, 10.18653/v1/D19-1204Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev, 10.18653/v1/D18-1425Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Seq2sql: Generating structured queries from natural language using reinforcement learning. Victor Zhong, Caiming Xiong, Richard Socher, CoRR, abs/1709.001032017</p>
<p>Toolqa: A dataset for llm question answering with external tools. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang, 2023</p>            </div>
        </div>

    </div>
</body>
</html>